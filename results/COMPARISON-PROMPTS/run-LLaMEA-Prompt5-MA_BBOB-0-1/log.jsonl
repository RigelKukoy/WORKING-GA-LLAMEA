{"id": "e940d8d0-7e5e-41e7-a528-7918af625c9a", "fitness": 0.16001467261320781, "name": "CMAES", "description": "Covariance Matrix Adaptation Evolution Strategy with a restart mechanism and adaptive population size.", "code": "import numpy as np\n\nclass CMAES:\n    def __init__(self, budget=10000, dim=10, popsize=None, cs=0.3, cc=0.3, c_sigma=0.2, c_cov=0.2, restarts=3):\n        self.budget = budget\n        self.dim = dim\n        self.restarts = restarts\n        \n        if popsize is None:\n            self.popsize = 4 + int(3 * np.log(self.dim))\n        else:\n            self.popsize = popsize\n\n        self.mu = self.popsize // 2  # Number of parents/elite solutions\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        \n        self.cs = cs  # Learning rate for step size\n        self.cc = cc  # Learning rate for mean\n        self.c_sigma = c_sigma  # Learning rate for step size adaptation\n        self.c_cov = c_cov  # Learning rate for covariance matrix adaptation\n        \n        self.D = None  # Initialize eigenvalues of covariance matrix\n        self.C = None # Covariance matrix\n        self.m = None # Mean\n        self.sigma = None # Step size\n\n        self.p_sigma = None  # Evolution path for step size\n        self.p_c = None # Evolution path for covariance matrix\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def initialize(self):\n        self.m = np.random.uniform(-5.0, 5.0, self.dim) # Initialize mean within bounds\n        self.sigma = 0.5  # Initial step size, a reasonable value\n        self.C = np.eye(self.dim) # Initialize covariance matrix to identity\n        self.D = np.ones(self.dim) #Eigenvalues of C\n        self.p_sigma = np.zeros(self.dim)\n        self.p_c = np.zeros(self.dim)\n\n    def __call__(self, func):\n        evals = 0\n        for _ in range(self.restarts):\n            self.initialize() # Restart\n\n            while evals < self.budget:\n                # 1. Sample population\n                z = np.random.normal(0, 1, size=(self.popsize, self.dim))\n                y = self.D * z  # Apply scaling from covariance matrix\n                x = self.m + self.sigma * y\n                \n                #Clip to bounds\n                lb = func.bounds.lb\n                ub = func.bounds.ub\n                x = np.clip(x, lb, ub)\n                \n                f = np.array([func(xi) for xi in x])\n                evals += self.popsize\n\n                if evals > self.budget:\n                    break # Ensure budget is not exceeded\n\n                # 2. Selection and Recombination\n                idx = np.argsort(f)\n                x_elite = x[idx[:self.mu]]\n                z_elite = z[idx[:self.mu]]\n                f_elite = f[idx[:self.mu]]\n\n\n                # Update optimal solution\n                if f_elite[0] < self.f_opt:\n                    self.f_opt = f_elite[0]\n                    self.x_opt = x_elite[0]\n\n                # 3. Update Mean\n                m_old = self.m.copy()\n                self.m = np.sum(self.weights[:, None] * x_elite, axis=0)\n\n                # 4. Update Evolution Paths\n                y_mean = np.sum(self.weights[:, None] * z_elite, axis=0)\n                self.p_sigma = (1 - self.cs) * self.p_sigma + np.sqrt(self.cs * (2 - self.cs)) * y_mean\n                hsig = np.linalg.norm(self.p_sigma) / np.sqrt(1 - (1 - self.cs)**(2 * evals / self.popsize)) < 1.4 + 2 / (self.dim + 1)\n                self.p_c = (1 - self.cc) * self.p_c + hsig * np.sqrt(self.cc * (2 - self.cc)) * (self.m - m_old) / self.sigma\n\n                # 5. Update Covariance Matrix\n                self.C = (1 - self.c_cov) * self.C + self.c_cov * (self.p_c[:, None] @ self.p_c[None, :]) + self.c_cov * (1 - hsig) * self.c_cov * self.C\n                self.sigma = self.sigma * np.exp((self.c_sigma / 0.8) * (np.linalg.norm(self.p_sigma) / np.sqrt(self.dim) - 1)) #Ensure positive adaptation\n\n                # Eigen decomposition\n                try:\n                    self.D, B = np.linalg.eigh(self.C)\n                except np.linalg.LinAlgError:\n                    self.C = np.eye(self.dim)\n                    self.D = np.ones(self.dim)\n\n                self.D = np.sqrt(np.abs(self.D))\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm CMAES scored 0.160 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.035754956899595736, 0.06216520779912371, 0.2333973175300137, 0.13692957971597408, 0.15774433546419264, 0.14956539438016592, 0.2072024452215404, 0.1355378569502934, 0.11523089163308509, 0.11497374307948216, 0.1433607861884444, 0.17953888467419343, 0.22457015519223167, 0.13612859371185582, 0.3570850857600266, 0.2085984088343784, 0.17043117950928355, 0.20153819604340828, 0.11401042284561536, 0.11653001083125125]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "47de978c-c17f-4e30-afce-97497a2e0e34", "fitness": 0.6435695612809138, "name": "AdaptiveDE", "description": "Adaptive Differential Evolution with elitist archive and self-adaptation of parameters.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.F = 0.5  # Initial value for differential weight\n        self.CR = 0.7 # Initial value for crossover rate\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        # Initialize archive\n        archive = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.archive_size, self.dim))\n        archive_fitness = np.full(self.archive_size, np.inf)\n\n        # Main loop\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x1, x2, x3 = population[idxs]\n\n                # Add a random vector from archive\n                if np.random.rand() < 0.1: # Probability of 0.1 to use archive member\n                    idx_archive = np.random.choice(self.archive_size)\n                    mutant = x1 + self.F * (x2 - x3) + self.F * (archive[idx_archive] - x1)  # Mutation with archive\n\n                else:\n                    mutant = x1 + self.F * (x2 - x3) # Standard DE mutation\n\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub) # Clip to boundaries\n\n\n                # Crossover\n                crossover_mask = np.random.rand(self.dim) < self.CR\n                trial_vector = np.where(crossover_mask, mutant, population[i])\n\n                # Evaluation\n                f_trial = func(trial_vector)\n                self.budget -= 1\n\n                # Selection\n                if f_trial < fitness[i]:\n                    # Update archive\n                    if f_trial < np.max(archive_fitness):\n                        worst_archive_idx = np.argmax(archive_fitness)\n                        archive[worst_archive_idx] = trial_vector\n                        archive_fitness[worst_archive_idx] = f_trial\n                    \n                    population[i] = trial_vector\n                    fitness[i] = f_trial\n\n                    # Self-adapt F and CR (simplified)\n                    self.F = np.clip(np.random.normal(0.5, 0.1), 0.1, 1.0)  \n                    self.CR = np.clip(np.random.normal(0.7, 0.1), 0.1, 1.0)\n                \n\n                # Update best\n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = trial_vector\n\n                if self.budget <= 0:\n                    break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveDE scored 0.644 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.22752668444497415, 0.5358582844873963, 0.6489178998084921, 0.8486755516272146, 0.7118340142731842, 0.7664803672628772, 0.5995924205452332, 0.6299919338910898, 0.7089296809552369, 0.6382151483413188, 0.7979352303141884, 0.9974135067854243, 0.33520333696308113, 0.6507565352594378, 0.8819584621542166, 0.7728410985303782, 0.5010204872691721, 0.8172859048916908, 0.2851297942497183, 0.5158248835639492]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "6e355f4a-5357-47e2-9b94-4bf1197cc891", "fitness": 0.0, "name": "HybridPSO_DE", "description": "A population-based algorithm that combines the exploration of Particle Swarm Optimization (PSO) with the exploitation of Differential Evolution (DE), enhanced with a restart mechanism.", "code": "import numpy as np\n\nclass HybridPSO_DE:\n    def __init__(self, budget=10000, dim=10, pop_size=20, pso_weight=0.7, de_cross_rate=0.7, restart_prob=0.05):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.pso_weight = pso_weight\n        self.de_cross_rate = de_cross_rate\n        self.restart_prob = restart_prob\n\n\n    def __call__(self, func):\n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        # Initialize personal best positions and values\n        pbest_positions = population.copy()\n        pbest_fitness = fitness.copy()\n\n        # Initialize global best position and value\n        gbest_index = np.argmin(fitness)\n        gbest_position = population[gbest_index].copy()\n        gbest_fitness = fitness[gbest_index]\n\n        self.f_opt = gbest_fitness\n        self.x_opt = gbest_position\n\n        # Main optimization loop\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # PSO update\n                velocity = self.pso_weight * np.random.rand(self.dim) * (pbest_positions[i] - population[i]) + \\\n                           (1 - self.pso_weight) * np.random.rand(self.dim) * (gbest_position - population[i])\n                new_position = population[i] + velocity\n                new_position = np.clip(new_position, func.bounds.lb, func.bounds.ub)  # Ensure bounds are respected\n\n                # DE update\n                if np.random.rand() < self.de_cross_rate:\n                    indices = np.random.choice(self.pop_size, 3, replace=False)\n                    x_r1, x_r2, x_r3 = population[indices]\n                    diff_vector = x_r2 - x_r3\n                    de_position = population[i] + np.random.rand() * diff_vector\n                    de_position = np.clip(de_position, func.bounds.lb, func.bounds.ub)\n                    \n                    j_rand = np.random.randint(self.dim)\n                    for j in range(self.dim):\n                         if np.random.rand() > self.de_cross_rate and j != j_rand:\n                              de_position[j] = new_position[j]\n\n                    new_position = de_position\n                    \n                new_fitness = func(new_position)\n                self.budget -= 1\n                if self.budget <= 0:\n                    break\n\n                # Update personal best\n                if new_fitness < pbest_fitness[i]:\n                    pbest_fitness[i] = new_fitness\n                    pbest_positions[i] = new_position.copy()\n\n                    # Update global best\n                    if new_fitness < gbest_fitness:\n                        gbest_fitness = new_fitness\n                        gbest_position = new_position.copy()\n                        self.f_opt = gbest_fitness\n                        self.x_opt = gbest_position\n\n                # Update population\n                population[i] = new_position.copy()\n                fitness[i] = new_fitness\n\n            # Restart mechanism\n            if np.random.rand() < self.restart_prob:\n                for i in range(self.pop_size):\n                     population[i] = np.random.uniform(func.bounds.lb, func.bounds.ub)\n                     fitness[i] = func(population[i])\n                     self.budget -=1\n                     if self.budget <= 0:\n                          break\n                if self.budget <= 0:\n                     break\n                pbest_positions = population.copy()\n                pbest_fitness = fitness.copy()\n\n                gbest_index = np.argmin(fitness)\n                gbest_position = population[gbest_index].copy()\n                gbest_fitness = fitness[gbest_index]\n                self.f_opt = gbest_fitness\n                self.x_opt = gbest_position\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm HybridPSO_DE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "9f9c16ea-af73-46de-bf5b-a8f732df56c8", "fitness": "-inf", "name": "AdaptivePopulationSearch", "description": "An Adaptive Population-Based Search with orthogonal learning and mutation.", "code": "import numpy as np\n\nclass AdaptivePopulationSearch:\n    def __init__(self, budget=10000, dim=10, pop_size=20, mutation_rate=0.1, orthogonal_learning_rate = 0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.mutation_rate = mutation_rate\n        self.orthogonal_learning_rate = orthogonal_learning_rate\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        \n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n        \n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index]\n        \n        generation = 0\n        while self.budget > 0:\n            generation += 1\n            \n            # Selection (Tournament Selection)\n            indices = np.random.choice(self.pop_size, size=self.pop_size // 2 * 2, replace=False)\n            tournament_fitness = fitness[indices].reshape(-1, 2)\n            tournament_winners = np.argmin(tournament_fitness, axis=1)\n            parents = population[indices.reshape(-1, 2)[np.arange(self.pop_size // 2), tournament_winners]]\n\n            # Crossover (Simulated Binary Crossover - SBX)\n            offspring = parents.copy()\n            for i in range(0, self.pop_size // 2, 2):\n                rand = np.random.rand(self.dim)\n                beta = np.where(rand <= 0.5, (2 * rand)**(1 / (1 + 1)), (1 / (2 * (1 - rand)))**(1 / (1 + 1)))\n                \n                offspring[i] = 0.5 * ((parents[i] + parents[i+1]) - beta * np.abs(parents[i] - parents[i+1]))\n                offspring[i+1] = 0.5 * ((parents[i] + parents[i+1]) + beta * np.abs(parents[i] - parents[i+1]))\n\n                # Clip to bounds\n                offspring[i] = np.clip(offspring[i], func.bounds.lb, func.bounds.ub)\n                offspring[i+1] = np.clip(offspring[i+1], func.bounds.lb, func.bounds.ub)\n            \n            # Mutation (Uniform Mutation)\n            mutation_mask = np.random.rand(self.pop_size // 2, self.dim) < self.mutation_rate\n            offspring[mutation_mask] = np.random.uniform(func.bounds.lb, func.bounds.ub, size=np.sum(mutation_mask))\n            \n            # Orthogonal Learning\n            if np.random.rand() < self.orthogonal_learning_rate:\n                rand_index = np.random.randint(0, self.pop_size // 2)\n                dim_index = np.random.randint(0, self.dim)\n                orthogonal_point = self.x_opt.copy()\n                orthogonal_point[dim_index] = np.random.uniform(func.bounds.lb, func.bounds.ub)\n                offspring[rand_index] = orthogonal_point\n            \n            # Evaluate offspring\n            offspring_fitness = np.array([func(x) for x in offspring])\n            self.budget -= self.pop_size // 2\n            \n            # Replace worst individuals in population\n            worst_indices = np.argsort(fitness)[-self.pop_size // 2:]\n            population[worst_indices] = offspring\n            fitness[worst_indices] = offspring_fitness\n            \n            # Update best solution\n            best_index = np.argmin(fitness)\n            if fitness[best_index] < self.f_opt:\n                self.f_opt = fitness[best_index]\n                self.x_opt = population[best_index]\n        \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "In the code, line 784, in numpy.PyArray_MultiIterNew3, the following error occurred:\nValueError: shape mismatch: objects cannot be broadcast to a single shape.  Mismatch is between arg 0 with shape (0,) and arg 1 with shape (2,).", "error": "In the code, line 784, in numpy.PyArray_MultiIterNew3, the following error occurred:\nValueError: shape mismatch: objects cannot be broadcast to a single shape.  Mismatch is between arg 0 with shape (0,) and arg 1 with shape (2,).", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "7c8db435-46f5-45f7-98ec-7b49ab5ba98b", "fitness": 0.3580629417353454, "name": "CMAES", "description": "CMA-ES with a more aggressive covariance matrix adaptation and dynamic step size control using a moving average of the objective function values for faster convergence.", "code": "import numpy as np\n\nclass CMAES:\n    def __init__(self, budget=10000, dim=10, popsize=None, cs=0.3, cc=0.3, c_sigma=0.2, c_cov=0.5, restarts=3, adapt_popsize=True):\n        self.budget = budget\n        self.dim = dim\n        self.restarts = restarts\n        self.adapt_popsize = adapt_popsize\n        \n        if popsize is None:\n            self.popsize = 4 + int(3 * np.log(self.dim))\n        else:\n            self.popsize = popsize\n\n        self.mu = self.popsize // 2  # Number of parents/elite solutions\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        \n        self.cs = cs  # Learning rate for step size\n        self.cc = cc  # Learning rate for mean\n        self.c_sigma = c_sigma  # Learning rate for step size adaptation\n        self.c_cov = c_cov  # Learning rate for covariance matrix adaptation\n        \n        self.D = None  # Initialize eigenvalues of covariance matrix\n        self.C = None # Covariance matrix\n        self.m = None # Mean\n        self.sigma = None # Step size\n\n        self.p_sigma = None  # Evolution path for step size\n        self.p_c = None # Evolution path for covariance matrix\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.f_history = []\n        self.history_length = min(100, budget // (2 * self.popsize)) # Use a smaller history when budget is smaller\n\n    def initialize(self):\n        self.m = np.random.uniform(-5.0, 5.0, self.dim) # Initialize mean within bounds\n        self.sigma = 0.5  # Initial step size, a reasonable value\n        self.C = np.eye(self.dim) # Initialize covariance matrix to identity\n        self.D = np.ones(self.dim) #Eigenvalues of C\n        self.p_sigma = np.zeros(self.dim)\n        self.p_c = np.zeros(self.dim)\n        self.f_history = []\n\n    def __call__(self, func):\n        evals = 0\n        for _ in range(self.restarts):\n            self.initialize() # Restart\n\n            while evals < self.budget:\n                # 1. Sample population\n                z = np.random.normal(0, 1, size=(self.popsize, self.dim))\n                y = self.D * z  # Apply scaling from covariance matrix\n                x = self.m + self.sigma * y\n                \n                #Clip to bounds\n                lb = func.bounds.lb\n                ub = func.bounds.ub\n                x = np.clip(x, lb, ub)\n                \n                f = np.array([func(xi) for xi in x])\n                evals += self.popsize\n\n                if evals > self.budget:\n                    break # Ensure budget is not exceeded\n\n                # 2. Selection and Recombination\n                idx = np.argsort(f)\n                x_elite = x[idx[:self.mu]]\n                z_elite = z[idx[:self.mu]]\n                f_elite = f[idx[:self.mu]]\n\n\n                # Update optimal solution\n                if f_elite[0] < self.f_opt:\n                    self.f_opt = f_elite[0]\n                    self.x_opt = x_elite[0]\n\n                # 3. Update Mean\n                m_old = self.m.copy()\n                self.m = np.sum(self.weights[:, None] * x_elite, axis=0)\n\n                # 4. Update Evolution Paths\n                y_mean = np.sum(self.weights[:, None] * z_elite, axis=0)\n                self.p_sigma = (1 - self.cs) * self.p_sigma + np.sqrt(self.cs * (2 - self.cs)) * y_mean\n                hsig = np.linalg.norm(self.p_sigma) / np.sqrt(1 - (1 - self.cs)**(2 * evals / self.popsize)) < 1.4 + 2 / (self.dim + 1)\n                self.p_c = (1 - self.cc) * self.p_c + hsig * np.sqrt(self.cc * (2 - self.cc)) * (self.m - m_old) / self.sigma\n\n                # 5. Update Covariance Matrix\n                self.C = (1 - self.c_cov) * self.C + self.c_cov * (self.p_c[:, None] @ self.p_c[None, :]) + self.c_cov * (1 - hsig) * self.c_cov * self.C\n\n                # Dynamic step size adaptation using moving average\n                self.f_history.extend(f_elite)\n                self.f_history = self.f_history[-self.history_length:]  # Keep only the recent history\n\n                if len(self.f_history) > 1:\n                    improvement = np.mean(self.f_history[:-1]) - self.f_history[-1]\n                    if improvement > 0:\n                        self.sigma = self.sigma * np.exp(self.c_sigma * improvement / np.std(self.f_history)) #Increase sigma if improvement is high\n                    else:\n                         self.sigma = self.sigma * np.exp(self.c_sigma * improvement / np.std(self.f_history))#Decrease sigma if improvement is low\n\n                self.sigma = max(self.sigma, 1e-10) #Minimum value for sigma\n                # Eigen decomposition\n                try:\n                    self.D, B = np.linalg.eigh(self.C)\n                except np.linalg.LinAlgError:\n                    self.C = np.eye(self.dim)\n                    self.D = np.ones(self.dim)\n\n                self.D = np.sqrt(np.abs(self.D))\n\n                if self.adapt_popsize:\n                    if evals > self.budget // 2:\n                        self.popsize = max(4, self.popsize // 2)\n                        self.mu = self.popsize // 2\n                        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n                        self.weights = self.weights / np.sum(self.weights)\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "The algorithm CMAES scored 0.358 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["e940d8d0-7e5e-41e7-a528-7918af625c9a"], "operator": null, "metadata": {"aucs": [0.06737550548708404, 0.23294303141434292, 0.5215422334372779, 0.1420039618380089, 0.23152040192479528, 0.911023737753123, 0.21381369393662464, 0.2141203101136382, 0.41705591016776566, 0.07788179630362224, 0.32531840382678934, 0.9999159197071879, 0.22733869362600145, 0.3806359462995095, 0.5665088637067339, 0.24939615919724478, 0.22944746401200744, 0.556025562949902, 0.13034748031591248, 0.467043758689337]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "08015db1-79a4-465f-81bd-022d1006f796", "fitness": "-inf", "name": "GradientAssistedSearch", "description": "A search strategy that uses a combination of global exploration via random sampling and local refinement via gradient estimation.", "code": "import numpy as np\n\nclass GradientAssistedSearch:\n    def __init__(self, budget=10000, dim=10, pop_size=10, exploration_rate=0.7, learning_rate=0.1, gradient_estimation_radius=0.01):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.exploration_rate = exploration_rate\n        self.learning_rate = learning_rate\n        self.gradient_estimation_radius = gradient_estimation_radius\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index]\n\n        while self.budget > 0:\n            # Exploration vs. Exploitation\n            if np.random.rand() < self.exploration_rate:\n                # Global Exploration: Random Sampling\n                new_individual = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                new_fitness = func(new_individual)\n                self.budget -= 1\n\n                if new_fitness < self.f_opt:\n                    self.f_opt = new_fitness\n                    self.x_opt = new_individual\n\n                # Replace worst individual in population if better\n                worst_index = np.argmax(fitness)\n                if new_fitness < fitness[worst_index]:\n                    population[worst_index] = new_individual\n                    fitness[worst_index] = new_fitness\n\n            else:\n                # Local Refinement: Gradient Estimation\n                index = np.random.randint(0, self.pop_size)\n                x = population[index].copy()\n                gradient = np.zeros(self.dim)\n\n                # Estimate gradient using finite differences\n                for i in range(self.dim):\n                    x_plus = x.copy()\n                    x_minus = x.copy()\n\n                    x_plus[i] += self.gradient_estimation_radius\n                    x_minus[i] -= self.gradient_estimation_radius\n\n                    x_plus[i] = np.clip(x_plus[i], func.bounds.lb, func.bounds.ub)\n                    x_minus[i] = np.clip(x_minus[i], func.bounds.lb, func.bounds.ub)\n\n                    fitness_plus = func(x_plus)\n                    fitness_minus = func(x_minus)\n                    self.budget -= 2\n\n                    gradient[i] = (fitness_plus - fitness_minus) / (2 * self.gradient_estimation_radius)\n                    if self.budget <= 0:\n                        break\n\n                # Move in the direction of the negative gradient\n                x_new = x - self.learning_rate * gradient\n                x_new = np.clip(x_new, func.bounds.lb, func.bounds.ub)\n\n                new_fitness = func(x_new)\n                self.budget -= 1\n\n                if new_fitness < self.f_opt:\n                    self.f_opt = new_fitness\n                    self.x_opt = x_new\n\n                # Replace the old individual if the new one is better\n                if new_fitness < fitness[index]:\n                    population[index] = x_new\n                    fitness[index] = new_fitness\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "In the code, line 57, in __call__, the following error occurred:\nValueError: setting an array element with a sequence.\nOn line: x_plus[i] = np.clip(x_plus[i], func.bounds.lb, func.bounds.ub)", "error": "In the code, line 57, in __call__, the following error occurred:\nValueError: setting an array element with a sequence.\nOn line: x_plus[i] = np.clip(x_plus[i], func.bounds.lb, func.bounds.ub)", "parent_ids": ["9f9c16ea-af73-46de-bf5b-a8f732df56c8"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "77501324-766b-45aa-8166-ede5ac3d4390", "fitness": 0.417335463861197, "name": "AdaptiveDE_DynPop", "description": "An Adaptive Differential Evolution with a dynamically adjusted population size based on success rate and orthogonal crossover.", "code": "import numpy as np\n\nclass AdaptiveDE_DynPop:\n    def __init__(self, budget=10000, dim=10, initial_pop_size=50, archive_size=10, success_rate_threshold=0.2):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = initial_pop_size\n        self.initial_pop_size = initial_pop_size\n        self.archive_size = archive_size\n        self.F = 0.5\n        self.CR = 0.7\n        self.success_rate_threshold = success_rate_threshold\n        self.success_count = 0\n        self.generation_count = 0\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        # Initialize archive\n        archive = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.archive_size, self.dim))\n        archive_fitness = np.full(self.archive_size, np.inf)\n\n        while self.budget > 0:\n            self.success_count = 0\n            self.generation_count += 1\n\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x1, x2, x3 = population[idxs]\n\n                # Add a random vector from archive\n                if np.random.rand() < 0.1: # Probability of 0.1 to use archive member\n                    idx_archive = np.random.choice(self.archive_size)\n                    mutant = x1 + self.F * (x2 - x3) + self.F * (archive[idx_archive] - x1)  # Mutation with archive\n\n                else:\n                    mutant = x1 + self.F * (x2 - x3) # Standard DE mutation\n\n\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Orthogonal Crossover\n                orthogonal_basis = np.random.randn(self.dim, self.dim)\n                q, r = np.linalg.qr(orthogonal_basis)  # Generate orthogonal basis\n                chosen_direction = np.random.choice(self.dim)\n                trial_vector = population[i] + self.CR * (mutant - population[i]) + 0.1 * (q[:, chosen_direction]) # Orthogonal Crossover\n\n\n                trial_vector = np.clip(trial_vector, func.bounds.lb, func.bounds.ub)\n\n                # Evaluation\n                f_trial = func(trial_vector)\n                self.budget -= 1\n\n                # Selection\n                if f_trial < fitness[i]:\n                    # Update archive\n                    if f_trial < np.max(archive_fitness):\n                        worst_archive_idx = np.argmax(archive_fitness)\n                        archive[worst_archive_idx] = trial_vector\n                        archive_fitness[worst_archive_idx] = f_trial\n                    \n                    population[i] = trial_vector\n                    fitness[i] = f_trial\n                    self.success_count += 1\n\n                    # Self-adapt F and CR (simplified)\n                    self.F = np.clip(np.random.normal(0.5, 0.1), 0.1, 1.0)\n                    self.CR = np.clip(np.random.normal(0.7, 0.1), 0.1, 1.0)\n\n                # Update best\n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = trial_vector\n\n                if self.budget <= 0:\n                    break\n            \n            # Adjust population size dynamically\n            success_rate = self.success_count / self.pop_size\n            if success_rate > self.success_rate_threshold and self.pop_size < 2*self.initial_pop_size:\n                self.pop_size = min(int(self.pop_size * 1.1), 2*self.initial_pop_size)  # Increase population size\n                new_individuals = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size - len(population), self.dim))\n                new_fitness = np.array([func(x) for x in new_individuals])\n                self.budget -= len(new_individuals)\n                population = np.vstack((population, new_individuals))\n                fitness = np.concatenate((fitness, new_fitness))\n\n            elif success_rate < (self.success_rate_threshold / 2) and self.pop_size > self.initial_pop_size//2:\n                 self.pop_size = max(int(self.pop_size * 0.9), self.initial_pop_size//2) # Decrease population size\n                 population = population[:self.pop_size]\n                 fitness = fitness[:self.pop_size]\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "The algorithm AdaptiveDE_DynPop scored 0.417 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["47de978c-c17f-4e30-afce-97497a2e0e34"], "operator": null, "metadata": {"aucs": [0.2107759145199537, 0.27815436222748346, 0.43691175998777576, 0.3786916558255379, 0.36407817451958313, 0.4497872492694093, 0.2913498518165003, 0.3801286973895225, 0.4291357658150958, 0.22506198139459477, 0.7906003081867793, 0.9983879907839509, 0.2879202050602445, 0.39652152401907204, 0.6927452441219972, 0.3210294147850583, 0.31614260676009986, 0.3549390455892283, 0.2601716093403156, 0.48417591581173725]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "22bf79c6-5912-426f-9325-a6bb3afd12c0", "fitness": 0.15903054913899067, "name": "CMAES", "description": "Simplified CMA-ES with a focus on parameter adaptation and eigenvalue updates for faster convergence.", "code": "import numpy as np\n\nclass CMAES:\n    def __init__(self, budget=10000, dim=10, popsize=None, cs=0.3, cc=0.3, c_sigma=0.2, c_cov=0.2, restarts=3):\n        self.budget = budget\n        self.dim = dim\n        self.restarts = restarts\n        \n        if popsize is None:\n            self.popsize = 4 + int(3 * np.log(self.dim))\n        else:\n            self.popsize = popsize\n\n        self.mu = self.popsize // 2  # Number of parents/elite solutions\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        \n        self.cs = cs  # Learning rate for step size\n        self.cc = cc  # Learning rate for mean\n        self.c_sigma = c_sigma  # Learning rate for step size adaptation\n        self.c_cov = c_cov  # Learning rate for covariance matrix adaptation\n        \n        self.D = None  # Initialize eigenvalues of covariance matrix\n        self.C = None # Covariance matrix\n        self.m = None # Mean\n        self.sigma = None # Step size\n\n        self.p_sigma = None  # Evolution path for step size\n        self.p_c = None # Evolution path for covariance matrix\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def initialize(self):\n        self.m = np.random.uniform(-5.0, 5.0, self.dim) # Initialize mean within bounds\n        self.sigma = 0.5  # Initial step size, a reasonable value\n        self.C = np.eye(self.dim) # Initialize covariance matrix to identity\n        self.D = np.ones(self.dim) #Eigenvalues of C\n        self.p_sigma = np.zeros(self.dim)\n        self.p_c = np.zeros(self.dim)\n\n    def __call__(self, func):\n        evals = 0\n        for _ in range(self.restarts):\n            self.initialize() # Restart\n\n            while evals < self.budget:\n                # 1. Sample population\n                z = np.random.normal(0, 1, size=(self.popsize, self.dim))\n                y = self.D * z  # Apply scaling from covariance matrix\n                x = self.m + self.sigma * y\n                \n                #Clip to bounds\n                lb = func.bounds.lb\n                ub = func.bounds.ub\n                x = np.clip(x, lb, ub)\n                \n                f = np.array([func(xi) for xi in x])\n                evals += self.popsize\n\n                if evals > self.budget:\n                    break # Ensure budget is not exceeded\n\n                # 2. Selection and Recombination\n                idx = np.argsort(f)\n                x_elite = x[idx[:self.mu]]\n                z_elite = z[idx[:self.mu]]\n                f_elite = f[idx[:self.mu]]\n\n\n                # Update optimal solution\n                if f_elite[0] < self.f_opt:\n                    self.f_opt = f_elite[0]\n                    self.x_opt = x_elite[0]\n\n                # 3. Update Mean\n                m_old = self.m.copy()\n                self.m = np.sum(self.weights[:, None] * x_elite, axis=0)\n\n                # 4. Update Evolution Paths\n                y_mean = np.sum(self.weights[:, None] * z_elite, axis=0)\n                self.p_sigma = (1 - self.cs) * self.p_sigma + np.sqrt(self.cs * (2 - self.cs)) * y_mean\n                hsig = np.linalg.norm(self.p_sigma) / np.sqrt(1 - (1 - self.cs)**(2 * evals / self.popsize)) < 1.4 + 2 / (self.dim + 1)\n                self.p_c = (1 - self.cc) * self.p_c + hsig * np.sqrt(self.cc * (2 - self.cc)) * (self.m - m_old) / self.sigma\n\n                # 5. Update Covariance Matrix\n                self.C = (1 - self.c_cov) * self.C + self.c_cov * (self.p_c[:, None] @ self.p_c[None, :])\n                self.sigma = self.sigma * np.exp((self.c_sigma / 0.8) * (np.linalg.norm(self.p_sigma) / np.sqrt(self.dim) - 1)) #Ensure positive adaptation\n\n                # Eigen decomposition\n                try:\n                    D, B = np.linalg.eigh(self.C)\n                    self.D = np.sqrt(np.abs(D))\n                except np.linalg.LinAlgError:\n                    self.C = np.eye(self.dim)\n                    self.D = np.ones(self.dim)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "The algorithm CMAES scored 0.159 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["e940d8d0-7e5e-41e7-a528-7918af625c9a"], "operator": null, "metadata": {"aucs": [0.03873119448082685, 0.06689471273401637, 0.21749831977404854, 0.1471644044080691, 0.16589844100731188, 0.1654770818995729, 0.23130972238316316, 0.06328703841548555, 0.20725812959357148, 0.12710297127070536, 0.11151433496254282, 0.1400449042257238, 0.25153586593373367, 0.20437448126545021, 0.15293564535513782, 0.19512214299785813, 0.17622881860231332, 0.2818332080272796, 0.13327612464276228, 0.10312344080024105]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "f1e4f071-dcb2-474e-a252-e02a74c38190", "fitness": "-inf", "name": "AdaptiveHybridPSO_CMAES", "description": "A self-adaptive hybrid algorithm blending PSO's global search with CMA-ES's local refinement, dynamically adjusting strategy probabilities based on performance feedback and a diversity maintenance scheme.", "code": "import numpy as np\nfrom scipy.linalg import sqrtm\n\nclass AdaptiveHybridPSO_CMAES:\n    def __init__(self, budget=10000, dim=10, pop_size=20, pso_weight=0.7, cma_learning_rate=0.1, restart_prob=0.05, diversity_threshold=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.pso_weight = pso_weight\n        self.cma_learning_rate = cma_learning_rate\n        self.restart_prob = restart_prob\n        self.diversity_threshold = diversity_threshold\n        self.pso_success_rate = 0.5\n        self.cma_success_rate = 0.5\n\n\n    def __call__(self, func):\n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        # Initialize personal best positions and values\n        pbest_positions = population.copy()\n        pbest_fitness = fitness.copy()\n\n        # Initialize global best position and value\n        gbest_index = np.argmin(fitness)\n        gbest_position = population[gbest_index].copy()\n        gbest_fitness = fitness[gbest_index]\n\n        self.f_opt = gbest_fitness\n        self.x_opt = gbest_position\n\n        # CMA-ES parameters initialization\n        mean = gbest_position.copy()\n        sigma = 0.1 * (func.bounds.ub - func.bounds.lb)\n        C = np.eye(self.dim)  # Covariance matrix\n        ps = np.zeros(self.dim) #Evolution Path for Sigma\n        pc = np.zeros(self.dim) #Evolution Path for C\n        mu = self.pop_size // 2 #Select best mu individuals\n        c_sigma = 0.3 #Learning rate for Sigma\n        c_c = 0.3 #Learning rate for C\n        dampsigma = 1+c_sigma\n\n        def CMA_sample(mean,sigma,C):\n            A = sqrtm(C)\n            z = np.random.randn(self.dim)\n            return mean + sigma*np.dot(A,z)\n\n        def update_CMA(x_k, mean, ps, pc, C, sigma, mu):\n            y_k = (x_k - mean) / sigma\n            ps = (1-c_sigma)*ps + np.sqrt(c_sigma*(2-c_sigma))*y_k\n            hsig = np.linalg.norm(ps)/np.sqrt(1-(1-c_sigma)**2) < (1.4 + 2/(self.dim+1))\n            pc = (1-c_c)*pc + hsig*np.sqrt(c_c*(2-c_c))*y_k\n            C = (1-c_c)*C + c_c*(pc[:,None] @ pc[None,:])\n            return ps, pc, C\n\n\n        def is_diverse(population, threshold):\n            distances = np.linalg.norm(population[:, None, :] - population[None, :, :], axis=2)\n            distances = np.triu(distances, k=1)\n            mean_distance = np.sum(distances) / (self.pop_size * (self.pop_size - 1) / 2)\n            return mean_distance > threshold\n        \n        pso_count = 0\n        cmaes_count = 0\n\n        # Main optimization loop\n        while self.budget > 0:\n            new_population = []\n            new_fitness = []\n            for i in range(self.pop_size):\n                if np.random.rand() < self.pso_success_rate: #Adaptive Probability\n                    # PSO update\n                    velocity = self.pso_weight * np.random.rand(self.dim) * (pbest_positions[i] - population[i]) + \\\n                               (1 - self.pso_weight) * np.random.rand(self.dim) * (gbest_position - population[i])\n                    new_position = population[i] + velocity\n                    new_position = np.clip(new_position, func.bounds.lb, func.bounds.ub)  # Ensure bounds are respected\n                    pso_count += 1\n                else:\n                    # CMA-ES sample\n                    new_position = CMA_sample(mean,sigma,C)\n                    new_position = np.clip(new_position, func.bounds.lb, func.bounds.ub)\n                    cmaes_count +=1\n\n                new_fitness_val = func(new_position)\n                self.budget -= 1\n                if self.budget <= 0:\n                    break\n\n                new_population.append(new_position)\n                new_fitness.append(new_fitness_val)\n\n            if self.budget <= 0:\n                 break\n\n            new_population = np.array(new_population)\n            new_fitness = np.array(new_fitness)\n\n\n            # Update personal best\n            for i in range(self.pop_size):\n                if new_fitness[i] < pbest_fitness[i]:\n                    pbest_fitness[i] = new_fitness[i]\n                    pbest_positions[i] = new_population[i].copy()\n\n                    # Update global best\n                    if new_fitness[i] < gbest_fitness:\n                        gbest_fitness = new_fitness[i]\n                        gbest_position = new_population[i].copy()\n                        self.f_opt = gbest_fitness\n                        self.x_opt = gbest_position\n\n            #Update CMA-ES parameters based on best solutions\n            sorted_indices = np.argsort(new_fitness)\n            best_indices = sorted_indices[:mu]\n\n            old_mean = mean.copy()\n            mean = np.mean(new_population[best_indices], axis=0)\n            y_k = (new_population[best_indices] - old_mean) / sigma\n            ps, pc, C = update_CMA(np.mean(new_population[best_indices],axis=0), old_mean, ps, pc, C, sigma, mu)\n            sigma *= np.exp((c_sigma/dampsigma)*((np.linalg.norm(ps)/np.sqrt(self.dim))-1))\n            # Update population\n            population = new_population.copy()\n            fitness = new_fitness.copy()\n\n            #Restart mechanism\n            if np.random.rand() < self.restart_prob or not is_diverse(population, self.diversity_threshold):\n                population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                fitness = np.array([func(x) for x in population])\n                self.budget -= self.pop_size\n                if self.budget <= 0:\n                    break\n                pbest_positions = population.copy()\n                pbest_fitness = fitness.copy()\n\n                gbest_index = np.argmin(fitness)\n                gbest_position = population[gbest_index].copy()\n                gbest_fitness = fitness[gbest_index]\n                self.f_opt = gbest_fitness\n                self.x_opt = gbest_position\n                mean = gbest_position.copy()\n                C = np.eye(self.dim)  # Reset covariance matrix\n                ps = np.zeros(self.dim)\n                pc = np.zeros(self.dim)\n\n            #Adapt Strategy Probabilities\n            if pso_count+cmaes_count > 0:\n                if gbest_fitness < self.f_opt:\n                    self.pso_success_rate = 0.9*self.pso_success_rate + 0.1 * (pso_count/(pso_count+cmaes_count))\n                else:\n                    self.pso_success_rate = 0.9*self.pso_success_rate + 0.1 * (pso_count/(pso_count+cmaes_count))\n\n            if np.isnan(self.pso_success_rate):\n                self.pso_success_rate = 0.5\n            pso_count = 0\n            cmaes_count = 0\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "In the code, line 47, in CMA_sample, the following error occurred:\nNameError: name 'sqrtm' is not defined\nOn line: A = sqrtm(C)", "error": "In the code, line 47, in CMA_sample, the following error occurred:\nNameError: name 'sqrtm' is not defined\nOn line: A = sqrtm(C)", "parent_ids": ["6e355f4a-5357-47e2-9b94-4bf1197cc891"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "9b5cae55-b83e-495a-a890-72ecfacbeae2", "fitness": "-inf", "name": "VoronoiAdaptiveSearch", "description": "A self-organizing search strategy that dynamically adjusts search parameters based on local fitness landscape characteristics using a Voronoi-based partitioning.", "code": "import numpy as np\nfrom scipy.spatial import Voronoi\n\nclass VoronoiAdaptiveSearch:\n    def __init__(self, budget=10000, dim=10, pop_size=20, initial_neighbor_size=0.1, expansion_rate=1.1, contraction_rate=0.9, min_neighbor_size=1e-6):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.initial_neighbor_size = initial_neighbor_size\n        self.expansion_rate = expansion_rate\n        self.contraction_rate = contraction_rate\n        self.min_neighbor_size = min_neighbor_size\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index]\n\n        neighbor_sizes = np.full(self.pop_size, self.initial_neighbor_size)\n\n        generation = 0\n        while self.budget > 0:\n            generation += 1\n\n            # Voronoi Partitioning\n            try:\n                vor = Voronoi(population)\n            except:\n                # Handle degenerate cases where Voronoi computation fails (e.g., all points are the same)\n                population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                fitness = np.array([func(x) for x in population])\n                vor = Voronoi(population)\n            \n            # Exploration and Exploitation within Voronoi Cells\n            new_population = np.zeros_like(population)\n            new_fitness = np.zeros_like(fitness)\n            \n            for i in range(self.pop_size):\n                # Explore around the current point\n                neighbor_vector = np.random.uniform(-neighbor_sizes[i], neighbor_sizes[i], size=self.dim)\n                new_point = population[i] + neighbor_vector\n\n                # Clip to bounds\n                new_point = np.clip(new_point, func.bounds.lb, func.bounds.ub)\n\n                new_fitness_val = func(new_point)\n                self.budget -= 1\n\n                if new_fitness_val < fitness[i]:\n                    new_population[i] = new_point\n                    new_fitness[i] = new_fitness_val\n\n                    # Expand neighbor size if improvement\n                    neighbor_sizes[i] = min(neighbor_sizes[i] * self.expansion_rate, (func.bounds.ub - func.bounds.lb))  #Cap at the size of the bounds.\n                else:\n                    new_population[i] = population[i]\n                    new_fitness[i] = fitness[i]\n                    # Contract neighbor size if no improvement\n                    neighbor_sizes[i] = max(neighbor_sizes[i] * self.contraction_rate, self.min_neighbor_size)\n\n                # Update best solution\n                if new_fitness_val < self.f_opt:\n                    self.f_opt = new_fitness_val\n                    self.x_opt = new_point\n\n            population = new_population\n            fitness = new_fitness\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "In the code, line 40, in __call__, the following error occurred:\nNameError: name 'Voronoi' is not defined\nOn line: vor = Voronoi(population)", "error": "In the code, line 40, in __call__, the following error occurred:\nNameError: name 'Voronoi' is not defined\nOn line: vor = Voronoi(population)", "parent_ids": ["9f9c16ea-af73-46de-bf5b-a8f732df56c8"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "9b0db343-944c-47a7-8ce3-4a61c2b0ec46", "fitness": 0.2827410252654948, "name": "AdaptiveHybridPSO_DE", "description": "An adaptive hybrid algorithm that dynamically adjusts the balance between PSO and DE based on their recent performance, using a simplified DE update and a more robust restart strategy based on population diversity.", "code": "import numpy as np\n\nclass AdaptiveHybridPSO_DE:\n    def __init__(self, budget=10000, dim=10, pop_size=20, initial_pso_weight=0.7, de_cross_rate=0.7):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.pso_weight = initial_pso_weight\n        self.de_cross_rate = de_cross_rate\n        self.pso_success_rate = 0.5\n        self.de_success_rate = 0.5\n        self.success_memory = [] #Memory of previous success of pso vs de\n        self.success_window = 10 #Size of memory window\n        self.adaptive_rate = 0.1 #Rate to adapt pso_weight\n        self.min_pso_weight = 0.2\n        self.max_pso_weight = 0.8\n\n\n    def __call__(self, func):\n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        # Initialize personal best positions and values\n        pbest_positions = population.copy()\n        pbest_fitness = fitness.copy()\n\n        # Initialize global best position and value\n        gbest_index = np.argmin(fitness)\n        gbest_position = population[gbest_index].copy()\n        gbest_fitness = fitness[gbest_index]\n\n        self.f_opt = gbest_fitness\n        self.x_opt = gbest_position\n\n        # Main optimization loop\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Decide whether to use PSO or DE\n                if np.random.rand() < self.pso_weight:\n                    # PSO update\n                    velocity = np.random.rand(self.dim) * (pbest_positions[i] - population[i]) + \\\n                               np.random.rand(self.dim) * (gbest_position - population[i])\n                    new_position = population[i] + velocity\n                    new_position = np.clip(new_position, func.bounds.lb, func.bounds.ub)\n                    method = 'pso'\n\n                else:\n                    # Simplified DE update\n                    indices = np.random.choice(self.pop_size, 3, replace=False)\n                    x_r1, x_r2, x_r3 = population[indices]\n                    diff_vector = x_r2 - x_r3\n                    new_position = population[i] + np.random.rand() * diff_vector\n                    new_position = np.clip(new_position, func.bounds.lb, func.bounds.ub)\n                    method = 'de'\n\n                new_fitness = func(new_position)\n                self.budget -= 1\n                if self.budget <= 0:\n                    break\n\n                # Update personal best\n                if new_fitness < pbest_fitness[i]:\n                    \n                    if method == 'pso':\n                        self.success_memory.append(1)\n                    else:\n                        self.success_memory.append(0)\n                    \n                    pbest_fitness[i] = new_fitness\n                    pbest_positions[i] = new_position.copy()\n\n                    # Update global best\n                    if new_fitness < gbest_fitness:\n                        gbest_fitness = new_fitness\n                        gbest_position = new_position.copy()\n                        self.f_opt = gbest_fitness\n                        self.x_opt = gbest_position\n                else:\n                     if method == 'pso':\n                          self.success_memory.append(0)\n                     else:\n                          self.success_memory.append(1)\n\n                # Update population\n                population[i] = new_position.copy()\n                fitness[i] = new_fitness\n            \n            #Adapt PSO weight\n            if len(self.success_memory) > self.success_window:\n                self.success_memory = self.success_memory[-self.success_window:] #Keep memory within window\n                pso_successes = sum(self.success_memory)\n                self.pso_success_rate = pso_successes/len(self.success_memory)\n                self.pso_weight += self.adaptive_rate * (self.pso_success_rate - 0.5)\n                self.pso_weight = np.clip(self.pso_weight, self.min_pso_weight, self.max_pso_weight)\n\n            # Restart mechanism based on population diversity\n            if np.std(fitness) < 1e-6:  # If population has converged\n                population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                fitness = np.array([func(x) for x in population])\n                self.budget -= self.pop_size\n                if self.budget <= 0:\n                     break\n                pbest_positions = population.copy()\n                pbest_fitness = fitness.copy()\n\n                gbest_index = np.argmin(fitness)\n                gbest_position = population[gbest_index].copy()\n                gbest_fitness = fitness[gbest_index]\n                self.f_opt = gbest_fitness\n                self.x_opt = gbest_position\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "The algorithm AdaptiveHybridPSO_DE scored 0.283 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["6e355f4a-5357-47e2-9b94-4bf1197cc891"], "operator": null, "metadata": {"aucs": [0.14854238921619745, 0.24282228030600195, 0.36581814137509383, 0.23996084170707987, 0.22502386459074264, 0.2125402833860186, 0.2528522524966158, 0.28395062309362806, 0.32696778562289186, 0.1915141445020313, 0.24218488272458483, 0.9949293977268171, 0.2561650308443133, 0.2578434613904055, 0]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "626b08eb-33ec-46cc-89c8-30f9dce3a519", "fitness": "-inf", "name": "AdaptivePopulationSearch", "description": "Adaptive population search with improved offspring generation using a blend of differential evolution and orthogonal learning, enhanced with dynamic parameter adjustment.", "code": "import numpy as np\n\nclass AdaptivePopulationSearch:\n    def __init__(self, budget=10000, dim=10, pop_size=20, mutation_rate=0.1, orthogonal_learning_rate = 0.1, de_scaling_factor=0.5, crossover_rate=0.7):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.mutation_rate = mutation_rate\n        self.orthogonal_learning_rate = orthogonal_learning_rate\n        self.de_scaling_factor = de_scaling_factor\n        self.crossover_rate = crossover_rate\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        \n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n        \n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index]\n        \n        generation = 0\n        while self.budget > 0:\n            generation += 1\n            \n            # Selection (Tournament Selection)\n            indices = np.random.choice(self.pop_size, size=self.pop_size // 2 * 2, replace=False)\n            tournament_fitness = fitness[indices].reshape(-1, 2)\n            tournament_winners = np.argmin(tournament_fitness, axis=1)\n            parents = population[indices.reshape(-1, 2)[np.arange(self.pop_size // 2), tournament_winners]]\n\n            # Offspring Generation (Blend of DE and Orthogonal Learning)\n            offspring = np.zeros((self.pop_size // 2, self.dim))  # Initialize offspring array\n\n            for i in range(self.pop_size // 2):\n                # Differential Evolution\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = population[idxs]\n                \n                # Crossover\n                trial_vector = parents[i] + self.de_scaling_factor * (x_r1 - x_r2)\n                crossover_mask = np.random.rand(self.dim) < self.crossover_rate\n                trial_vector[crossover_mask] = parents[i][crossover_mask]\n\n                # Orthogonal Learning\n                if np.random.rand() < self.orthogonal_learning_rate:\n                    dim_index = np.random.randint(0, self.dim)\n                    orthogonal_point = self.x_opt.copy()\n                    orthogonal_point[dim_index] = np.random.uniform(func.bounds.lb, func.bounds.ub)\n                    \n                    # Blend DE and Orthogonal Learning\n                    offspring[i] = 0.5 * (trial_vector + orthogonal_point)\n                else:\n                     offspring[i] = trial_vector\n                \n                # Clip to bounds\n                offspring[i] = np.clip(offspring[i], func.bounds.lb, func.bounds.ub)\n            \n            # Mutation (Uniform Mutation)\n            mutation_mask = np.random.rand(self.pop_size // 2, self.dim) < self.mutation_rate\n            offspring[mutation_mask] = np.random.uniform(func.bounds.lb, func.bounds.ub, size=np.sum(mutation_mask))\n\n            # Evaluate offspring\n            offspring_fitness = np.array([func(x) for x in offspring])\n            self.budget -= self.pop_size // 2\n            \n            # Replace worst individuals in population\n            worst_indices = np.argsort(fitness)[-self.pop_size // 2:]\n            population[worst_indices] = offspring\n            fitness[worst_indices] = offspring_fitness\n            \n            # Update best solution\n            best_index = np.argmin(fitness)\n            if fitness[best_index] < self.f_opt:\n                self.f_opt = fitness[best_index]\n                self.x_opt = population[best_index]\n\n            # Adaptive Parameter Adjustment (example: mutation rate)\n            if generation % 10 == 0:  # Adjust every 10 generations\n                if np.std(fitness) < 1e-3: # if fitness is converging\n                    self.mutation_rate *= 1.1 # Increase mutation\n                else:\n                    self.mutation_rate *= 0.9 # Decrease mutation\n                self.mutation_rate = np.clip(self.mutation_rate, 0.01, 0.5)  # Keep mutation rate within bounds\n        \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "In the code, line 53, in __call__, the following error occurred:\nValueError: setting an array element with a sequence.\nOn line: orthogonal_point[dim_index] = np.random.uniform(func.bounds.lb, func.bounds.ub)", "error": "In the code, line 53, in __call__, the following error occurred:\nValueError: setting an array element with a sequence.\nOn line: orthogonal_point[dim_index] = np.random.uniform(func.bounds.lb, func.bounds.ub)", "parent_ids": ["9f9c16ea-af73-46de-bf5b-a8f732df56c8"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "791d324e-f228-4e4d-8980-8a92d538482d", "fitness": 0.5360686853780046, "name": "AdaptiveDE", "description": "An Adaptive Differential Evolution with orthogonal crossover and a dynamically updated archive, focusing on exploration in early stages and exploitation later by adjusting F and CR.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.F = 0.5  # Initial value for differential weight\n        self.CR = 0.7 # Initial value for crossover rate\n        self.generation = 0\n\n    def orthogonal_crossover(self, x, mutant):\n        \"\"\"Performs orthogonal crossover.\"\"\"\n        basis_vectors = np.random.randn(self.dim, self.dim)\n        Q, _ = np.linalg.qr(basis_vectors)\n        new_vector = np.zeros_like(x)\n        for i in range(self.dim):\n            if np.random.rand() < 0.5:\n                alpha = np.dot(mutant - x, Q[:, i])\n                new_vector += alpha * Q[:, i]\n        return x + new_vector\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        # Initialize archive\n        archive = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.archive_size, self.dim))\n        archive_fitness = np.full(self.archive_size, np.inf)\n\n        # Main loop\n        while self.budget > 0:\n            self.generation += 1\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x1, x2, x3 = population[idxs]\n\n                # Add a random vector from archive with decreasing probability\n                archive_prob = 0.2 * np.exp(-self.generation / (self.budget/10)) #Decreasing Archive Probability\n                if np.random.rand() < archive_prob: \n                    idx_archive = np.random.choice(self.archive_size)\n                    mutant = x1 + self.F * (x2 - x3) + self.F * (archive[idx_archive] - x1)  # Mutation with archive\n\n                else:\n                    mutant = x1 + self.F * (x2 - x3) # Standard DE mutation\n\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub) # Clip to boundaries\n\n                # Crossover (Orthogonal Crossover)\n                trial_vector = self.orthogonal_crossover(population[i], mutant)\n                trial_vector = np.clip(trial_vector, func.bounds.lb, func.bounds.ub)\n\n                # Evaluation\n                f_trial = func(trial_vector)\n                self.budget -= 1\n\n                # Selection\n                if f_trial < fitness[i]:\n                    # Update archive (only if better than worst AND also better than current individual)\n                    if f_trial < np.max(archive_fitness) and f_trial < fitness[i]:\n                        worst_archive_idx = np.argmax(archive_fitness)\n                        archive[worst_archive_idx] = trial_vector\n                        archive_fitness[worst_archive_idx] = f_trial\n                    \n                    population[i] = trial_vector\n                    fitness[i] = f_trial\n\n                    # Self-adapt F and CR (adaptive range based on generation)\n                    F_range = [0.1, 0.9] # Exploration\n                    CR_range = [0.2, 0.9] # Exploration\n                    self.F = np.clip(np.random.normal(np.mean(F_range), 0.1), F_range[0], F_range[1])\n                    self.CR = np.clip(np.random.normal(np.mean(CR_range), 0.1), CR_range[0], CR_range[1])\n\n                # Update best\n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = trial_vector\n\n                if self.budget <= 0:\n                    break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "The algorithm AdaptiveDE scored 0.536 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["47de978c-c17f-4e30-afce-97497a2e0e34"], "operator": null, "metadata": {"aucs": [0.20095077083828627, 0.33443479548106236, 0.5517344869122605, 0.8161496709143781, 0.6031164103767008, 0.7280007180000738, 0.29550277501821176, 0.5608234843832081, 0.6248776468017673, 0.21560042146891023, 0.7268321570933196, 0.977820862226578, 0.3043281385364679, 0.5418052264128883, 0.8038121988811301, 0.5196298581809138, 0.4070527744393878, 0.7933183856980193, 0.21630033585124897, 0.49928259004528086]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "219b7bdd-340f-44df-8523-f75c555a4558", "fitness": "-inf", "name": "AdaptiveHybridPSO_DE", "description": "Adaptive hybrid algorithm that dynamically adjusts PSO/DE balance based on success, incorporates orthogonal learning, and utilizes a simplified DE update with a Cauchy mutation for increased exploration.", "code": "import numpy as np\n\nclass AdaptiveHybridPSO_DE:\n    def __init__(self, budget=10000, dim=10, pop_size=20, initial_pso_weight=0.7, de_cross_rate=0.7, ortho_groups = 5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.pso_weight = initial_pso_weight\n        self.de_cross_rate = de_cross_rate\n        self.pso_success_rate = 0.5\n        self.de_success_rate = 0.5\n        self.success_memory = [] #Memory of previous success of pso vs de\n        self.success_window = 10 #Size of memory window\n        self.adaptive_rate = 0.1 #Rate to adapt pso_weight\n        self.min_pso_weight = 0.2\n        self.max_pso_weight = 0.8\n        self.ortho_groups = ortho_groups\n\n\n    def orthogonal_learning(self, population, fitness, num_groups):\n        \"\"\"\n        Performs orthogonal learning to improve population diversity and find better solutions.\n        \"\"\"\n        dim = population.shape[1]\n        group_size = dim // num_groups\n        for i in range(num_groups):\n            start = i * group_size\n            end = (i + 1) * group_size if i < num_groups - 1 else dim\n            group_indices = np.arange(start, end)\n\n            # Select a random individual as the base\n            base_index = np.argmin(fitness)  # Use best individual as the base\n            base_solution = population[base_index, group_indices]\n\n            # Generate orthogonal array (simplified - random orthogonal directions)\n            orthogonal_directions = np.random.randn(len(group_indices), len(group_indices))\n            q, r = np.linalg.qr(orthogonal_directions) #Orthogonalize\n            num_samples = min(self.pop_size, len(group_indices))\n\n            # Generate new solutions based on orthogonal array\n            for j in range(num_samples):\n                new_solution = base_solution + 0.1 * q[j]  # Scaling factor for exploration\n                population[j, group_indices] = np.clip(new_solution, -5, 5) #Keep in bounds\n\n        return population\n\n    def cauchy_mutation(self, x, scale=0.1):\n        \"\"\"Applies Cauchy mutation to a vector x.\"\"\"\n        mutation = scale * np.random.standard_cauchy(size=x.shape)\n        return x + mutation\n\n    def __call__(self, func):\n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        # Initialize personal best positions and values\n        pbest_positions = population.copy()\n        pbest_fitness = fitness.copy()\n\n        # Initialize global best position and value\n        gbest_index = np.argmin(fitness)\n        gbest_position = population[gbest_index].copy()\n        gbest_fitness = fitness[gbest_index]\n\n        self.f_opt = gbest_fitness\n        self.x_opt = gbest_position\n\n        # Main optimization loop\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Decide whether to use PSO or DE\n                if np.random.rand() < self.pso_weight:\n                    # PSO update\n                    velocity = np.random.rand(self.dim) * (pbest_positions[i] - population[i]) + \\\n                               np.random.rand(self.dim) * (gbest_position - population[i])\n                    new_position = population[i] + velocity\n                    new_position = np.clip(new_position, func.bounds.lb, func.bounds.ub)\n                    method = 'pso'\n\n                else:\n                    # Simplified DE update with Cauchy mutation\n                    indices = np.random.choice(self.pop_size, 3, replace=False)\n                    x_r1, x_r2, x_r3 = population[indices]\n                    diff_vector = x_r2 - x_r3\n                    new_position = population[i] + np.random.rand() * diff_vector\n                    new_position = self.cauchy_mutation(new_position, scale=0.01) # Cauchy mutation\n                    new_position = np.clip(new_position, func.bounds.lb, func.bounds.ub)\n                    method = 'de'\n\n                new_fitness = func(new_position)\n                self.budget -= 1\n                if self.budget <= 0:\n                    break\n\n                # Update personal best\n                if new_fitness < pbest_fitness[i]:\n                    \n                    if method == 'pso':\n                        self.success_memory.append(1)\n                    else:\n                        self.success_memory.append(0)\n                    \n                    pbest_fitness[i] = new_fitness\n                    pbest_positions[i] = new_position.copy()\n\n                    # Update global best\n                    if new_fitness < gbest_fitness:\n                        gbest_fitness = new_fitness\n                        gbest_position = new_position.copy()\n                        self.f_opt = gbest_fitness\n                        self.x_opt = gbest_position\n                else:\n                     if method == 'pso':\n                          self.success_memory.append(0)\n                     else:\n                          self.success_memory.append(1)\n\n                # Update population\n                population[i] = new_position.copy()\n                fitness[i] = new_fitness\n            \n            #Adapt PSO weight\n            if len(self.success_memory) > self.success_window:\n                self.success_memory = self.success_memory[-self.success_window:] #Keep memory within window\n                pso_successes = sum(self.success_memory)\n                self.pso_success_rate = pso_successes/len(self.success_memory)\n                self.pso_weight += self.adaptive_rate * (self.pso_success_rate - 0.5)\n                self.pso_weight = np.clip(self.pso_weight, self.min_pso_weight, self.max_pso_weight)\n\n            # Restart mechanism based on population diversity\n            if np.std(fitness) < 1e-6:  # If population has converged\n                population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                fitness = np.array([func(x) for x in population])\n                self.budget -= self.pop_size\n                if self.budget <= 0:\n                     break\n                pbest_positions = population.copy()\n                pbest_fitness = fitness.copy()\n\n                gbest_index = np.argmin(fitness)\n                gbest_position = population[gbest_index].copy()\n                gbest_fitness = fitness[gbest_index]\n                self.f_opt = gbest_fitness\n                self.x_opt = gbest_position\n\n            population = self.orthogonal_learning(population, fitness, self.ortho_groups)\n            fitness = np.array([func(x) for x in population])\n            self.budget -= self.pop_size #Account for orthogonal learning calls\n            if self.budget <= 0:\n                break\n            pbest_positions = population.copy()\n            pbest_fitness = fitness.copy()\n            gbest_index = np.argmin(fitness)\n            gbest_position = population[gbest_index].copy()\n            gbest_fitness = fitness[gbest_index]\n            self.f_opt = gbest_fitness\n            self.x_opt = gbest_position\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "Evaluation timed out after 300 seconds.", "error": "Evaluation timed out after 300 seconds.", "parent_ids": ["9b0db343-944c-47a7-8ce3-4a61c2b0ec46"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "03a323b0-78b9-4339-bd20-8be0fbfb67fd", "fitness": 0.4031334094291285, "name": "CMAES", "description": "Adaptive CMA-ES with simplified covariance update and dynamic step size adjustment based on success rate, using fewer parameters.", "code": "import numpy as np\n\nclass CMAES:\n    def __init__(self, budget=10000, dim=10, popsize=None, cs=0.3, cc=0.3, c_sigma=0.2, restarts=3, adapt_popsize=True):\n        self.budget = budget\n        self.dim = dim\n        self.restarts = restarts\n        self.adapt_popsize = adapt_popsize\n        \n        if popsize is None:\n            self.popsize = 4 + int(3 * np.log(self.dim))\n        else:\n            self.popsize = popsize\n\n        self.mu = self.popsize // 2  # Number of parents/elite solutions\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        \n        self.cs = cs  # Learning rate for step size\n        self.cc = cc  # Learning rate for mean\n        self.c_sigma = c_sigma  # Learning rate for step size adaptation\n        \n        self.C = None # Covariance matrix\n        self.m = None # Mean\n        self.sigma = None # Step size\n\n        self.p_sigma = None  # Evolution path for step size\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.success_rate = 0.5\n\n    def initialize(self):\n        self.m = np.random.uniform(-5.0, 5.0, self.dim) # Initialize mean within bounds\n        self.sigma = 0.5  # Initial step size, a reasonable value\n        self.C = np.eye(self.dim) # Initialize covariance matrix to identity\n        self.p_sigma = np.zeros(self.dim)\n\n    def __call__(self, func):\n        evals = 0\n        for _ in range(self.restarts):\n            self.initialize() # Restart\n\n            while evals < self.budget:\n                # 1. Sample population\n                z = np.random.normal(0, 1, size=(self.popsize, self.dim))\n                x = self.m + self.sigma * np.sqrt(np.diag(self.C)) * z\n                \n                #Clip to bounds\n                lb = func.bounds.lb\n                ub = func.bounds.ub\n                x = np.clip(x, lb, ub)\n                \n                f = np.array([func(xi) for xi in x])\n                evals += self.popsize\n\n                if evals > self.budget:\n                    break # Ensure budget is not exceeded\n\n                # 2. Selection and Recombination\n                idx = np.argsort(f)\n                x_elite = x[idx[:self.mu]]\n                z_elite = z[idx[:self.mu]]\n                f_elite = f[idx[:self.mu]]\n\n                # Update optimal solution\n                if f_elite[0] < self.f_opt:\n                    self.f_opt = f_elite[0]\n                    self.x_opt = x_elite[0]\n\n                # 3. Update Mean\n                m_old = self.m.copy()\n                self.m = np.sum(self.weights[:, None] * x_elite, axis=0)\n\n                # 4. Update Evolution Path\n                y_mean = np.sum(self.weights[:, None] * z_elite, axis=0)\n                self.p_sigma = (1 - self.cs) * self.p_sigma + np.sqrt(self.cs * (2 - self.cs)) * y_mean\n\n                # 5. Update Covariance Matrix (Simplified)\n                self.C = (1 - self.cc) * self.C + self.cc * (self.p_sigma[:, None] @ self.p_sigma[None, :])\n\n                # Dynamic step size adaptation based on success rate\n                success = (f < np.percentile(f, 25)).sum() / self.popsize\n                self.success_rate = 0.9 * self.success_rate + 0.1 * success\n\n                if self.success_rate > 0.3:\n                    self.sigma *= np.exp(self.c_sigma * self.success_rate)\n                else:\n                    self.sigma *= np.exp(-self.c_sigma * (1 - self.success_rate))\n                \n                self.sigma = max(self.sigma, 1e-10)\n\n                if self.adapt_popsize:\n                    if evals > self.budget // 2:\n                        self.popsize = max(4, self.popsize // 2)\n                        self.mu = self.popsize // 2\n                        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n                        self.weights = self.weights / np.sum(self.weights)\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "The algorithm CMAES scored 0.403 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["7c8db435-46f5-45f7-98ec-7b49ab5ba98b"], "operator": null, "metadata": {"aucs": [0.1335543223435085, 0.17361471922003058, 0.3716018937672477, 0.977202734623098, 0.2668883369015409, 0.24639161224366202, 0.2608094251754608, 0.5739340074141217, 0.1921800805523728, 0.1743636102524193, 0.37344889146439086, 0.9946766604476226, 0.2481561244442284, 0.5794633132183651, 0.587600718790644, 0.3356776951338516, 0.3127340853316306, 0.6099527064631873, 0.17190570543496253, 0.47851154536022433]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "af0cc63e-cb2a-4adc-a348-528241ad59a2", "fitness": 0.37745997959954153, "name": "AdaptiveDE_DynPop", "description": "An adaptive DE with orthogonal crossover and dynamic population size, simplified F/CR adaptation, and a probability-based archive update for enhanced exploration and exploitation.", "code": "import numpy as np\n\nclass AdaptiveDE_DynPop:\n    def __init__(self, budget=10000, dim=10, initial_pop_size=50, archive_size=10, success_rate_threshold=0.2):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = initial_pop_size\n        self.initial_pop_size = initial_pop_size\n        self.archive_size = archive_size\n        self.F = 0.5\n        self.CR = 0.7\n        self.success_rate_threshold = success_rate_threshold\n        self.success_count = 0\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.archive = None\n        self.archive_fitness = None\n\n\n    def __call__(self, func):\n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index]\n\n\n        # Initialize archive\n        self.archive = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.archive_size, self.dim))\n        self.archive_fitness = np.full(self.archive_size, np.inf)\n        archive_idx = np.random.choice(self.archive_size, min(self.archive_size, self.pop_size), replace=False)\n        self.archive[archive_idx] = population[:len(archive_idx)]\n        self.archive_fitness[archive_idx] = fitness[:len(archive_idx)]\n\n\n\n        while self.budget > 0:\n            self.success_count = 0\n\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x1, x2, x3 = population[idxs]\n\n                # Add a random vector from archive with probability\n                if np.random.rand() < 0.1:\n                    idx_archive = np.random.choice(self.archive_size)\n                    mutant = x1 + self.F * (x2 - x3) + self.F * (self.archive[idx_archive] - x1)\n                else:\n                    mutant = x1 + self.F * (x2 - x3)\n\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Orthogonal Crossover - simplified to one random direction\n                direction = np.random.randn(self.dim)\n                direction /= np.linalg.norm(direction) #normalize\n                trial_vector = population[i] + self.CR * (mutant - population[i]) + 0.01 * direction # Simplified orthogonal crossover\n                trial_vector = np.clip(trial_vector, func.bounds.lb, func.bounds.ub)\n\n\n                # Evaluation\n                f_trial = func(trial_vector)\n                self.budget -= 1\n\n                # Selection\n                if f_trial < fitness[i]:\n                    # Probabilistic Archive Update\n                    if np.random.rand() < 0.2:  # 20% chance to replace a random archive member\n                        archive_idx = np.random.randint(self.archive_size)\n                        self.archive[archive_idx] = trial_vector\n                        self.archive_fitness[archive_idx] = f_trial\n\n                    population[i] = trial_vector\n                    fitness[i] = f_trial\n                    self.success_count += 1\n\n                    # Simplified F/CR adaptation\n                    self.F = np.clip(np.random.normal(0.5, 0.05), 0.1, 1.0) # Reduced std\n                    self.CR = np.clip(np.random.normal(0.7, 0.05), 0.1, 1.0) # Reduced std\n\n                # Update best\n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = trial_vector\n\n                if self.budget <= 0:\n                    break\n            \n            # Adjust population size dynamically - simplified condition\n            success_rate = self.success_count / self.pop_size\n            if success_rate > self.success_rate_threshold and self.pop_size < 2*self.initial_pop_size:\n                self.pop_size = min(int(self.pop_size * 1.1), 2*self.initial_pop_size)\n                new_individuals = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size - len(population), self.dim))\n                new_fitness = np.array([func(x) for x in new_individuals])\n                self.budget -= len(new_individuals)\n                population = np.vstack((population, new_individuals))\n                fitness = np.concatenate((fitness, new_fitness))\n\n            elif success_rate < (self.success_rate_threshold / 2) and self.pop_size > self.initial_pop_size//2:\n                 self.pop_size = max(int(self.pop_size * 0.9), self.initial_pop_size//2)\n                 population = population[:self.pop_size]\n                 fitness = fitness[:self.pop_size]\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "The algorithm AdaptiveDE_DynPop scored 0.377 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["77501324-766b-45aa-8166-ede5ac3d4390"], "operator": null, "metadata": {"aucs": [0.14223043911998612, 0.6021238323990765, 0.3767260741175583, 0.24636435249324107, 0.25399124363060943, 0.4763343883431671, 0.26252011777156137, 0.37860998183813477, 0.2750476542102791, 0.23831356876935195, 0.3254820092003301, 0.9891537249263191, 0.30470736270385557, 0.27325660469729096, 0.605551730863163, 0.3083477193288622, 0.3445544608529515, 0.45497556643515735, 0.22223135420428886, 0.46867740608564556]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "6798df95-1e8a-4380-9e94-dd0480006ad5", "fitness": 0.0, "name": "AdaptiveHybridPSO_DE", "description": "Dynamically adjusts PSO and DE balance using success rates and a novel local search operator based on gradient estimation for improved exploitation.", "code": "import numpy as np\n\nclass AdaptiveHybridPSO_DE:\n    def __init__(self, budget=10000, dim=10, pop_size=20, initial_pso_weight=0.7, de_cross_rate=0.7, local_search_prob=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.pso_weight = initial_pso_weight\n        self.de_cross_rate = de_cross_rate\n        self.pso_success_rate = 0.5\n        self.de_success_rate = 0.5\n        self.success_memory = [] #Memory of previous success of pso vs de\n        self.success_window = 10 #Size of memory window\n        self.adaptive_rate = 0.1 #Rate to adapt pso_weight\n        self.min_pso_weight = 0.2\n        self.max_pso_weight = 0.8\n        self.local_search_prob = local_search_prob #Probability of performing local search\n\n    def local_search(self, func, position, current_fitness, step_size=0.1):\n        \"\"\"Performs a simple gradient-based local search.\"\"\"\n        best_position = position.copy()\n        best_fitness = current_fitness\n        for i in range(self.dim):\n            # Estimate gradient along each dimension\n            delta = np.zeros(self.dim)\n            delta[i] = step_size\n            fitness_plus = func(position + delta)\n            self.budget -=1\n            if self.budget <= 0:\n                return best_fitness, best_position\n\n            fitness_minus = func(position - delta)\n            self.budget -=1\n            if self.budget <= 0:\n                return best_fitness, best_position\n\n            gradient = (fitness_plus - fitness_minus) / (2 * step_size)\n\n            # Move in the direction of the negative gradient\n            new_position = position - 0.5 * step_size * gradient\n            new_position = np.clip(new_position, func.bounds.lb, func.bounds.ub)\n            new_fitness = func(new_position)\n            self.budget -= 1\n            if self.budget <= 0:\n                return best_fitness, best_position\n\n            if new_fitness < best_fitness:\n                best_fitness = new_fitness\n                best_position = new_position.copy()\n\n        return best_fitness, best_position\n\n    def __call__(self, func):\n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        # Initialize personal best positions and values\n        pbest_positions = population.copy()\n        pbest_fitness = fitness.copy()\n\n        # Initialize global best position and value\n        gbest_index = np.argmin(fitness)\n        gbest_position = population[gbest_index].copy()\n        gbest_fitness = fitness[gbest_index]\n\n        self.f_opt = gbest_fitness\n        self.x_opt = gbest_position\n\n        # Main optimization loop\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Decide whether to use PSO or DE\n                if np.random.rand() < self.pso_weight:\n                    # PSO update\n                    velocity = np.random.rand(self.dim) * (pbest_positions[i] - population[i]) + \\\n                               np.random.rand(self.dim) * (gbest_position - population[i])\n                    new_position = population[i] + velocity\n                    new_position = np.clip(new_position, func.bounds.lb, func.bounds.ub)\n                    method = 'pso'\n\n                else:\n                    # Simplified DE update\n                    indices = np.random.choice(self.pop_size, 3, replace=False)\n                    x_r1, x_r2, x_r3 = population[indices]\n                    diff_vector = x_r2 - x_r3\n                    new_position = population[i] + np.random.rand() * diff_vector\n                    new_position = np.clip(new_position, func.bounds.lb, func.bounds.ub)\n                    method = 'de'\n\n                new_fitness = func(new_position)\n                self.budget -= 1\n                if self.budget <= 0:\n                    break\n                \n                # Perform local search with a certain probability\n                if np.random.rand() < self.local_search_prob:\n                    new_fitness, new_position = self.local_search(func, new_position, new_fitness)\n\n                # Update personal best\n                if new_fitness < pbest_fitness[i]:\n                    \n                    if method == 'pso':\n                        self.success_memory.append(1)\n                    else:\n                        self.success_memory.append(0)\n                    \n                    pbest_fitness[i] = new_fitness\n                    pbest_positions[i] = new_position.copy()\n\n                    # Update global best\n                    if new_fitness < gbest_fitness:\n                        gbest_fitness = new_fitness\n                        gbest_position = new_position.copy()\n                        self.f_opt = gbest_fitness\n                        self.x_opt = gbest_position\n                else:\n                     if method == 'pso':\n                          self.success_memory.append(0)\n                     else:\n                          self.success_memory.append(1)\n\n                # Update population\n                population[i] = new_position.copy()\n                fitness[i] = new_fitness\n            \n            #Adapt PSO weight\n            if len(self.success_memory) > self.success_window:\n                self.success_memory = self.success_memory[-self.success_window:] #Keep memory within window\n                pso_successes = sum(self.success_memory)\n                self.pso_success_rate = pso_successes/len(self.success_memory)\n                self.pso_weight += self.adaptive_rate * (self.pso_success_rate - 0.5)\n                self.pso_weight = np.clip(self.pso_weight, self.min_pso_weight, self.max_pso_weight)\n\n            # Restart mechanism based on population diversity\n            if np.std(fitness) < 1e-6:  # If population has converged\n                population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                fitness = np.array([func(x) for x in population])\n                self.budget -= self.pop_size\n                if self.budget <= 0:\n                     break\n                pbest_positions = population.copy()\n                pbest_fitness = fitness.copy()\n\n                gbest_index = np.argmin(fitness)\n                gbest_position = population[gbest_index].copy()\n                gbest_fitness = fitness[gbest_index]\n                self.f_opt = gbest_fitness\n                self.x_opt = gbest_position\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "The algorithm AdaptiveHybridPSO_DE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["9b0db343-944c-47a7-8ce3-4a61c2b0ec46"], "operator": null, "metadata": {"aucs": [0]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "27d82145-7d16-4fe3-95f4-522bb39019a0", "fitness": 0.198654121744026, "name": "AdaptiveHybridPSO_Mutation", "description": "A hybrid algorithm combining a simplified PSO with a mutation-based search, adaptively adjusting exploration/exploitation balance based on the success history of each component and incorporating a local search refinement step.", "code": "import numpy as np\n\nclass AdaptiveHybridPSO_Mutation:\n    def __init__(self, budget=10000, dim=10, pop_size=20, initial_pso_weight=0.7, mutation_rate=0.1, local_search_probability = 0.05):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.pso_weight = initial_pso_weight\n        self.mutation_rate = mutation_rate\n        self.pso_success_rate = 0.5\n        self.mutation_success_rate = 0.5\n        self.success_memory = []\n        self.success_window = 10\n        self.adaptive_rate = 0.1\n        self.min_pso_weight = 0.2\n        self.max_pso_weight = 0.8\n        self.local_search_probability = local_search_probability\n\n    def local_search(self, func, x, radius=0.1, num_iterations=5):\n        \"\"\"Performs a local search around the given solution.\"\"\"\n        best_x = x.copy()\n        best_fitness = func(x)\n        self.budget -= 1\n        if self.budget <= 0:\n            return best_fitness, best_x\n\n        for _ in range(num_iterations):\n            # Generate a random neighbor within the radius\n            neighbor = x + np.random.uniform(-radius, radius, size=self.dim)\n            neighbor = np.clip(neighbor, func.bounds.lb, func.bounds.ub)\n            \n            neighbor_fitness = func(neighbor)\n            self.budget -= 1\n            if self.budget <= 0:\n                return best_fitness, best_x\n\n            if neighbor_fitness < best_fitness:\n                best_fitness = neighbor_fitness\n                best_x = neighbor.copy()\n\n        return best_fitness, best_x\n\n\n    def __call__(self, func):\n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        # Initialize personal best positions and values\n        pbest_positions = population.copy()\n        pbest_fitness = fitness.copy()\n\n        # Initialize global best position and value\n        gbest_index = np.argmin(fitness)\n        gbest_position = population[gbest_index].copy()\n        gbest_fitness = fitness[gbest_index]\n        self.f_opt = gbest_fitness\n        self.x_opt = gbest_position\n\n\n        # Main optimization loop\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Decide whether to use PSO or Mutation\n                if np.random.rand() < self.pso_weight:\n                    # Simplified PSO update\n                    velocity = np.random.rand(self.dim) * (pbest_positions[i] - population[i]) + \\\n                               np.random.rand(self.dim) * (gbest_position - population[i])\n                    new_position = population[i] + velocity\n                    new_position = np.clip(new_position, func.bounds.lb, func.bounds.ub)\n                    method = 'pso'\n\n                else:\n                    # Mutation based search\n                    new_position = population[i] + self.mutation_rate * np.random.normal(0, 1, size=self.dim)\n                    new_position = np.clip(new_position, func.bounds.lb, func.bounds.ub)\n                    method = 'mutation'\n\n                new_fitness = func(new_position)\n                self.budget -= 1\n                if self.budget <= 0:\n                    break\n\n                # Local Search Refinement\n                if np.random.rand() < self.local_search_probability:\n                    new_fitness, new_position = self.local_search(func, new_position)\n                    \n                # Update personal best\n                if new_fitness < pbest_fitness[i]:\n                    if method == 'pso':\n                        self.success_memory.append(1)\n                    else:\n                        self.success_memory.append(0)\n                    pbest_fitness[i] = new_fitness\n                    pbest_positions[i] = new_position.copy()\n\n                    # Update global best\n                    if new_fitness < gbest_fitness:\n                        gbest_fitness = new_fitness\n                        gbest_position = new_position.copy()\n                        self.f_opt = gbest_fitness\n                        self.x_opt = gbest_position\n\n                else:\n                    if method == 'pso':\n                        self.success_memory.append(0)\n                    else:\n                        self.success_memory.append(1)\n\n                # Update population\n                population[i] = new_position.copy()\n                fitness[i] = new_fitness\n\n            # Adapt PSO weight\n            if len(self.success_memory) > self.success_window:\n                self.success_memory = self.success_memory[-self.success_window:]\n                pso_successes = sum(self.success_memory)\n                self.pso_success_rate = pso_successes / len(self.success_memory)\n                self.pso_weight += self.adaptive_rate * (self.pso_success_rate - 0.5)\n                self.pso_weight = np.clip(self.pso_weight, self.min_pso_weight, self.max_pso_weight)\n\n            # Restart mechanism based on population diversity\n            if np.std(fitness) < 1e-6:  # If population has converged\n                population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                fitness = np.array([func(x) for x in population])\n                self.budget -= self.pop_size\n                if self.budget <= 0:\n                    break\n\n                pbest_positions = population.copy()\n                pbest_fitness = fitness.copy()\n\n                gbest_index = np.argmin(fitness)\n                gbest_position = population[gbest_index].copy()\n                gbest_fitness = fitness[gbest_index]\n                self.f_opt = gbest_fitness\n                self.x_opt = gbest_position\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "The algorithm AdaptiveHybridPSO_Mutation scored 0.199 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["9b0db343-944c-47a7-8ce3-4a61c2b0ec46"], "operator": null, "metadata": {"aucs": [0.12635571316337013, 0.2767675282424973, 0.39149324557023657, 0]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "9baaf3e2-5754-488d-9937-987ce7598085", "fitness": 0.5452421568979569, "name": "AdaptiveDEToroidalLocal", "description": "A self-adaptive Differential Evolution with toroidal search space handling, dynamically adjusted population size based on stagnation detection, and a local search operator triggered by significant improvement.", "code": "import numpy as np\n\nclass AdaptiveDEToroidalLocal:\n    def __init__(self, budget=10000, dim=10, initial_pop_size=50, archive_size=10, stagnation_threshold=10, local_search_prob=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = initial_pop_size\n        self.initial_pop_size = initial_pop_size\n        self.archive_size = archive_size\n        self.F = 0.5\n        self.CR = 0.7\n        self.stagnation_threshold = stagnation_threshold\n        self.stagnation_counter = 0\n        self.best_fitness_history = []\n        self.local_search_prob = local_search_prob\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        # Initialize population\n        population = np.random.uniform(lb, ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        # Initialize archive\n        archive = np.random.uniform(lb, ub, size=(self.archive_size, self.dim))\n        archive_fitness = np.full(self.archive_size, np.inf)\n\n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)]\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x1, x2, x3 = population[idxs]\n\n                mutant = x1 + self.F * (x2 - x3)\n\n                # Toroidal Boundary Handling\n                mutant = np.mod(mutant - lb, ub - lb) + lb\n\n                # Crossover\n                trial_vector = np.where(np.random.rand(self.dim) < self.CR, mutant, population[i])\n\n\n                # Evaluation\n                f_trial = func(trial_vector)\n                self.budget -= 1\n\n                # Selection\n                if f_trial < fitness[i]:\n                    # Update archive\n                    if f_trial < np.max(archive_fitness):\n                        worst_archive_idx = np.argmax(archive_fitness)\n                        archive[worst_archive_idx] = trial_vector\n                        archive_fitness[worst_archive_idx] = f_trial\n                    \n                    population[i] = trial_vector\n                    fitness[i] = f_trial\n\n                    # Self-adapt F and CR (simplified)\n                    self.F = np.clip(np.random.normal(0.5, 0.1), 0.1, 1.0)\n                    self.CR = np.clip(np.random.normal(0.7, 0.1), 0.1, 1.0)\n\n                # Update best\n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = trial_vector\n                    self.stagnation_counter = 0  # Reset stagnation counter\n\n                    # Trigger local search with probability\n                    if np.random.rand() < self.local_search_prob:\n                        self.x_opt, self.f_opt = self.local_search(func, self.x_opt, self.f_opt)\n\n                if self.budget <= 0:\n                    break\n\n            # Stagnation Check\n            self.best_fitness_history.append(self.f_opt)\n            if len(self.best_fitness_history) > self.stagnation_threshold:\n                if np.abs(self.best_fitness_history[-1] - np.mean(self.best_fitness_history[-self.stagnation_threshold:])) < 1e-6:\n                    self.stagnation_counter += 1\n                else:\n                    self.stagnation_counter = 0\n                self.best_fitness_history.pop(0)\n\n            # Population Adjustment based on Stagnation\n            if self.stagnation_counter > self.stagnation_threshold:\n                self.pop_size = int(self.initial_pop_size * 0.8)  # Reduce population\n                self.pop_size = max(10, self.pop_size) # Keep a minimum population size\n                population = population[:self.pop_size] #Truncate the population.\n                fitness = fitness[:self.pop_size]\n\n                new_individuals = np.random.uniform(lb, ub, size=(self.initial_pop_size - self.pop_size, self.dim))\n                new_fitness = np.array([func(x) for x in new_individuals])\n                self.budget -= len(new_individuals)\n\n                population = np.vstack((population, new_individuals))\n                fitness = np.concatenate((fitness, new_fitness))\n                self.pop_size = self.initial_pop_size # Set population back to the initial size\n                self.stagnation_counter = 0 #Reset the stagnation counter\n\n                if self.budget <= 0:\n                    break\n\n\n        return self.f_opt, self.x_opt\n\n    def local_search(self, func, x_opt, f_opt, radius=0.1, num_steps=5):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        for _ in range(num_steps):\n            # Generate a random direction\n            direction = np.random.randn(self.dim)\n            direction /= np.linalg.norm(direction)\n\n            # Generate a random step size\n            step_size = np.random.uniform(-radius, radius)\n\n            # Create a new candidate solution\n            x_new = x_opt + step_size * direction\n            x_new = np.clip(x_new, lb, ub)\n\n            # Evaluate the new solution\n            f_new = func(x_new)\n            self.budget -= 1\n\n            # Update the best solution if necessary\n            if f_new < f_opt:\n                f_opt = f_new\n                x_opt = x_new\n\n            if self.budget <= 0:\n                break\n\n        return x_opt, f_opt", "configspace": "", "generation": 2, "feedback": "The algorithm AdaptiveDEToroidalLocal scored 0.545 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["77501324-766b-45aa-8166-ede5ac3d4390"], "operator": null, "metadata": {"aucs": [0.20938760447812477, 0.4344858008853274, 0.5919100102961536, 0.7730180211067704, 0.6472172650955064, 0.753488575721921, 0.48508520446706704, 0.5766829501316484, 0.6791233032461843, 0.3598804373374599, 0.8074247604653995, 0.9918519110424291, 0.32383435229740354, 0]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "87287949-cff3-422a-abd4-f30d30b296a0", "fitness": 0.29230384366021694, "name": "AdaptiveHybridPSO_DE", "description": "Dynamically adjusts PSO and DE based on the recent improvement they achieve, using orthogonal crossover for DE and an aging mechanism to promote exploration.", "code": "import numpy as np\n\nclass AdaptiveHybridPSO_DE:\n    def __init__(self, budget=10000, dim=10, pop_size=20, initial_pso_weight=0.7, de_cross_rate=0.7, age_limit=50):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.pso_weight = initial_pso_weight\n        self.de_cross_rate = de_cross_rate\n        self.pso_success_rate = 0.5\n        self.de_success_rate = 0.5\n        self.success_memory = []\n        self.success_window = 10\n        self.adaptive_rate = 0.1\n        self.min_pso_weight = 0.2\n        self.max_pso_weight = 0.8\n        self.age_limit = age_limit #Maximum age before a particle is reset\n        self.ages = np.zeros(pop_size)  # Track ages of particles\n\n    def orthogonal_crossover(self, x, x_r1, x_r2):\n        \"\"\"Performs orthogonal crossover for DE.\"\"\"\n        dim = len(x)\n        new_x = x.copy()\n        num_directions = min(dim, 5) #Ensure that directions is not larger than dim.\n        directions = np.random.choice(dim, num_directions, replace=False)\n        for j in directions:\n            new_x[j] = x_r1[j] + np.random.rand() * (x_r2[j] - x_r1[j])\n        return new_x\n\n    def __call__(self, func):\n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n        if self.budget <= 0:\n            self.f_opt = np.min(fitness)\n            self.x_opt = population[np.argmin(fitness)].copy()\n            return self.f_opt, self.x_opt\n\n        # Initialize personal best positions and values\n        pbest_positions = population.copy()\n        pbest_fitness = fitness.copy()\n\n        # Initialize global best position and value\n        gbest_index = np.argmin(fitness)\n        gbest_position = population[gbest_index].copy()\n        gbest_fitness = fitness[gbest_index]\n\n        self.f_opt = gbest_fitness\n        self.x_opt = gbest_position\n\n        # Main optimization loop\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Decide whether to use PSO or DE\n                if np.random.rand() < self.pso_weight:\n                    # PSO update\n                    velocity = np.random.rand(self.dim) * (pbest_positions[i] - population[i]) + \\\n                               np.random.rand(self.dim) * (gbest_position - population[i])\n                    new_position = population[i] + velocity\n                    new_position = np.clip(new_position, func.bounds.lb, func.bounds.ub)\n                    method = 'pso'\n\n                else:\n                    # DE update with orthogonal crossover\n                    indices = np.random.choice(self.pop_size, 3, replace=False)\n                    x_r1, x_r2, x_r3 = population[indices]\n                    new_position = self.orthogonal_crossover(population[i], x_r1, x_r2)\n\n                    new_position = np.clip(new_position, func.bounds.lb, func.bounds.ub)\n                    method = 'de'\n\n                new_fitness = func(new_position)\n                self.budget -= 1\n                if self.budget <= 0:\n                    break\n\n                # Update personal best\n                if new_fitness < pbest_fitness[i]:\n                    if method == 'pso':\n                        self.success_memory.append(1)\n                    else:\n                        self.success_memory.append(0)\n\n                    pbest_fitness[i] = new_fitness\n                    pbest_positions[i] = new_position.copy()\n                    self.ages[i] = 0 #Reset age if improved\n\n                    # Update global best\n                    if new_fitness < gbest_fitness:\n                        gbest_fitness = new_fitness\n                        gbest_position = new_position.copy()\n                        self.f_opt = gbest_fitness\n                        self.x_opt = gbest_position\n                else:\n                    if method == 'pso':\n                        self.success_memory.append(0)\n                    else:\n                        self.success_memory.append(1)\n                    self.ages[i] +=1  # Increase age if no improvement\n                # Update population\n                population[i] = new_position.copy()\n                fitness[i] = new_fitness\n            \n            #Adapt PSO weight\n            if len(self.success_memory) > self.success_window:\n                self.success_memory = self.success_memory[-self.success_window:] #Keep memory within window\n                pso_successes = sum(self.success_memory)\n                self.pso_success_rate = pso_successes/len(self.success_memory)\n                self.pso_weight += self.adaptive_rate * (self.pso_success_rate - 0.5)\n                self.pso_weight = np.clip(self.pso_weight, self.min_pso_weight, self.max_pso_weight)\n            \n            # Aging mechanism: Reset particles that have not improved for a while\n            for i in range(self.pop_size):\n                if self.ages[i] > self.age_limit:\n                    population[i] = np.random.uniform(func.bounds.lb, func.bounds.ub, self.dim)\n                    fitness[i] = func(population[i])\n                    self.budget -= 1\n                    if self.budget <= 0:\n                        break\n                    pbest_positions[i] = population[i].copy()\n                    pbest_fitness[i] = fitness[i]\n                    self.ages[i] = 0\n\n                    if fitness[i] < gbest_fitness:\n                        gbest_fitness = fitness[i]\n                        gbest_position = population[i].copy()\n                        self.f_opt = gbest_fitness\n                        self.x_opt = gbest_position\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "The algorithm AdaptiveHybridPSO_DE scored 0.292 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["9b0db343-944c-47a7-8ce3-4a61c2b0ec46"], "operator": null, "metadata": {"aucs": [0.12139033890809026, 0.1803984120938883, 0.34182376672843395, 0.21213807669885232, 0.22474251432217074, 0.25534020432326066, 0.30916609392968186, 0.27872864134571296, 0.3109377383029349, 0.17503697337095514, 0.17142540397875272, 0.9976908634570653, 0.240467604493097, 0.2251608097305451, 0.2385836452052028, 0.2921860227783718, 0.24086217684911027, 0.36351850609389835, 0.17410157483235, 0.49237750576196493]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "6dfa9aeb-28db-4b5f-b5f2-b9445c63f4ab", "fitness": 0.4083834876702076, "name": "AdaptiveDE_DynPop_Simplified", "description": "An Adaptive Differential Evolution algorithm with a simplified orthogonal crossover, dynamic population size adjustment based on stagnation detection, and a memory-based mutation strategy for enhanced exploration and exploitation.", "code": "import numpy as np\n\nclass AdaptiveDE_DynPop_Simplified:\n    def __init__(self, budget=10000, dim=10, initial_pop_size=40, archive_size=15, stagnation_threshold=50):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = initial_pop_size\n        self.initial_pop_size = initial_pop_size\n        self.archive_size = archive_size\n        self.F = 0.5\n        self.CR = 0.7\n        self.stagnation_threshold = stagnation_threshold\n        self.stagnation_counter = 0\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.memory_F = []\n        self.memory_CR = []\n\n    def __call__(self, func):\n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index]\n\n        # Initialize archive\n        archive = np.copy(population)  # Initialize archive with diverse individuals\n        archive_fitness = np.copy(fitness)\n\n\n        while self.budget > 0:\n            best_fitness_before = self.f_opt\n\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x1, x2, x3 = population[idxs]\n\n                # Memory-based F and CR\n                if self.memory_F and self.memory_CR and np.random.rand() < 0.2:\n                    self.F = np.random.choice(self.memory_F)\n                    self.CR = np.random.choice(self.memory_CR)\n                else:\n                    self.F = np.clip(np.random.normal(0.5, 0.1), 0.1, 1.0)\n                    self.CR = np.clip(np.random.normal(0.7, 0.1), 0.1, 1.0)\n\n                # Archive-enhanced mutation\n                if np.random.rand() < 0.1:\n                    idx_archive = np.random.choice(self.archive_size)\n                    mutant = x1 + self.F * (x2 - x3) + self.F * (archive[idx_archive] - x1) # Archive-enhanced mutation\n\n                else:\n                    mutant = x1 + self.F * (x2 - x3)  # Standard DE mutation\n\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n\n                # Simplified Orthogonal Crossover (only one direction)\n                direction = np.random.randn(self.dim)\n                direction /= np.linalg.norm(direction)  # Normalize the direction\n                trial_vector = population[i] + self.CR * (mutant - population[i]) + 0.05 * direction #Simplified Crossover\n\n\n                trial_vector = np.clip(trial_vector, func.bounds.lb, func.bounds.ub)\n\n                # Evaluation\n                f_trial = func(trial_vector)\n                self.budget -= 1\n\n                # Selection\n                if f_trial < fitness[i]:\n                    # Update archive (replace worst)\n                    max_archive_index = np.argmax(archive_fitness)\n                    if f_trial < archive_fitness[max_archive_index]:\n                        archive[max_archive_index] = trial_vector\n                        archive_fitness[max_archive_index] = f_trial\n\n                    # Update population\n                    population[i] = trial_vector\n                    fitness[i] = f_trial\n\n                    # Store successful F and CR\n                    self.memory_F.append(self.F)\n                    self.memory_CR.append(self.CR)\n                    if len(self.memory_F) > 20:\n                        self.memory_F.pop(0)\n                        self.memory_CR.pop(0)\n\n                # Update best solution\n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = trial_vector\n\n                if self.budget <= 0:\n                    break\n\n            # Dynamic population size adjustment based on stagnation\n            if self.f_opt < best_fitness_before:\n                self.stagnation_counter = 0\n            else:\n                self.stagnation_counter += 1\n\n            if self.stagnation_counter > self.stagnation_threshold:\n                if self.pop_size > self.initial_pop_size // 2:\n                    self.pop_size = max(self.initial_pop_size // 2, int(self.pop_size * 0.8))\n                    population = population[:self.pop_size]\n                    fitness = fitness[:self.pop_size]\n                self.stagnation_counter = 0\n            elif self.pop_size < 2 * self.initial_pop_size and np.random.rand() < 0.1:\n                self.pop_size = min(2 * self.initial_pop_size, int(self.pop_size * 1.1))\n                new_individuals = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size - len(population), self.dim))\n                new_fitness = np.array([func(x) for x in new_individuals])\n                self.budget -= len(new_individuals)\n                population = np.vstack((population, new_individuals))\n                fitness = np.concatenate((fitness, new_fitness))\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "The algorithm AdaptiveDE_DynPop_Simplified scored 0.408 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["77501324-766b-45aa-8166-ede5ac3d4390"], "operator": null, "metadata": {"aucs": [0.19249903843102723, 0.2562567301235886, 0.4514715669425251, 0.5440926374780204, 0.2627537972790397, 0.3789566052027289, 0.2794391372370727, 0.41391144806437874, 0.3998337305991182, 0.2018636162570776, 0.5698406337681405, 0.9989031146877184, 0.2506700171903322, 0.39487951536327315, 0.6917807523712959, 0.3230086161668818, 0.3345941878332914, 0.5063597614478093, 0.23772585329156715, 0.4788289936692637]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "b57c8e58-1f6d-4338-9fba-dd27a7870d98", "fitness": 0.409478995886842, "name": "AdaptiveDEToroidalNeighborhood", "description": "Adaptive Differential Evolution with toroidal search space handling, success-history based parameter adaptation, and a neighborhood-based mutation operator for improved exploration.", "code": "import numpy as np\n\nclass AdaptiveDEToroidalNeighborhood:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=10, memory_size=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.memory_size = memory_size\n        self.F_memory = np.full(self.memory_size, 0.5)\n        self.CR_memory = np.full(self.memory_size, 0.7)\n        self.F = 0.5\n        self.CR = 0.7\n        self.memory_index = 0\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def toroidal_distance(self, x, y, lb, ub):\n        \"\"\"Calculates the toroidal distance between two points.\"\"\"\n        delta = np.abs(x - y)\n        delta = np.minimum(delta, ub - lb - delta)\n        return delta\n\n    def toroidal_mutation(self, population, i, F, lb, ub):\n        \"\"\"Performs toroidal mutation.\"\"\"\n        idxs = np.random.choice(len(population), 3, replace=False)\n        x1, x2, x3 = population[idxs]\n        \n        # Vectorized toroidal difference\n        diff1 = x2 - x3\n        diff1 = np.where(diff1 > 0, diff1 - (ub - lb) * np.floor(diff1 / (ub - lb)), diff1 + (ub - lb) * np.floor(-diff1 / (ub - lb)))\n\n        mutant = x1 + F * diff1\n        mutant = np.clip(mutant, lb, ub)\n        return mutant\n\n    def neighborhood_mutation(self, population, i, F, lb, ub, neighborhood_size=5):\n        \"\"\"Performs mutation based on the neighborhood of the individual.\"\"\"\n        \n        #Find neighborhood\n        distances = np.linalg.norm(population - population[i], axis=1) #Euclidean distance to other members\n        neighborhood_indices = np.argsort(distances)[:neighborhood_size] #Get closest members\n        \n        #Select 2 random members within the neighborhood\n        selected_indices = np.random.choice(neighborhood_indices, size=2, replace=False)\n        \n        x1 = population[i]\n        x2 = population[selected_indices[0]]\n        x3 = population[selected_indices[1]]\n\n        #Toroidal difference\n        diff = x2 - x3\n        diff = np.where(diff > 0, diff - (ub - lb) * np.floor(diff / (ub - lb)), diff + (ub - lb) * np.floor(-diff / (ub - lb)))\n\n\n        mutant = x1 + F * diff\n        mutant = np.clip(mutant, lb, ub)\n        \n        return mutant\n    \n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        archive = np.random.uniform(lb, ub, size=(self.archive_size, self.dim))\n        archive_fitness = np.full(self.archive_size, np.inf)\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation\n                mutant = self.neighborhood_mutation(population, i, self.F, lb, ub)\n\n                # Crossover\n                trial_vector = np.copy(population[i])\n                crossover_points = np.random.rand(self.dim) < self.CR\n                trial_vector[crossover_points] = mutant[crossover_points]\n                trial_vector = np.clip(trial_vector, lb, ub)\n\n                # Evaluation\n                f_trial = func(trial_vector)\n                self.budget -= 1\n\n                # Selection\n                if f_trial < fitness[i]:\n                    # Update archive\n                    if f_trial < np.max(archive_fitness):\n                        worst_archive_idx = np.argmax(archive_fitness)\n                        archive[worst_archive_idx] = trial_vector\n                        archive_fitness[worst_archive_idx] = f_trial\n\n                    # Update population\n                    population[i] = trial_vector\n                    fitness[i] = f_trial\n\n                    # Update success memory\n                    self.F_memory[self.memory_index] = self.F\n                    self.CR_memory[self.memory_index] = self.CR\n                    self.memory_index = (self.memory_index + 1) % self.memory_size\n\n                    # Adapt F and CR based on success history\n                    self.F = np.mean(self.F_memory)\n                    self.CR = np.mean(self.CR_memory)\n\n                # Update best solution\n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = trial_vector\n\n                if self.budget <= 0:\n                    break\n\n            # Adapt F and CR (exploration-exploitation balance)\n            self.F = np.clip(np.random.normal(self.F, 0.1), 0.1, 1.0)\n            self.CR = np.clip(np.random.normal(self.CR, 0.1), 0.1, 1.0)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "The algorithm AdaptiveDEToroidalNeighborhood scored 0.409 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["77501324-766b-45aa-8166-ede5ac3d4390"], "operator": null, "metadata": {"aucs": [0.15971303613640597, 0.26721673118374023, 0.35549578155165673, 0.6566359513832574, 0.22703264146070412, 0.5647879188079219, 0.26970033041543384, 0.41187580439050875, 0.5026007457700807, 0.18709048397986194, 0.3333004476489011, 0.9963096238903605, 0.29176291638815344, 0.29220832619099046, 0.6795556864986616, 0.3471327105508373, 0.34005306038649774, 0.6217838334416717, 0.18216908383602126, 0.503154803825172]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "ff236f0f-b07e-423e-8b90-a4581f99dad9", "fitness": "-inf", "name": "AdaptiveDE_Cauchy", "description": "A Differential Evolution strategy with a dynamically adjusted population size based on fitness variance and a Cauchy mutation operator for enhanced exploration.", "code": "import numpy as np\n\nclass AdaptiveDE_Cauchy:\n    def __init__(self, budget=10000, dim=10, initial_pop_size=50, min_pop_size=10, max_pop_size=100):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = initial_pop_size\n        self.min_pop_size = min_pop_size\n        self.max_pop_size = max_pop_size\n        self.F = 0.5\n        self.CR = 0.7\n        self.generation = 0\n\n    def cauchy_mutation(self, x1, x2, x3):\n        \"\"\"Performs Cauchy mutation.\"\"\"\n        scale = 0.1  # Scale parameter for Cauchy distribution\n        return x1 + scale * np.random.standard_cauchy(size=self.dim) * (x2 - x3)\n\n    def adjust_population_size(self, fitness):\n        \"\"\"Adjusts population size based on fitness variance.\"\"\"\n        fitness_variance = np.var(fitness)\n        if fitness_variance > 1e-3:  # High variance, increase population\n            self.pop_size = min(self.pop_size + 5, self.max_pop_size)\n        else:  # Low variance, decrease population\n            self.pop_size = max(self.pop_size - 5, self.min_pop_size)\n        self.pop_size = int(self.pop_size) # ensure integer\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        # Main loop\n        while self.budget > 0:\n            self.generation += 1\n\n            # Adjust population size\n            self.adjust_population_size(fitness)\n\n            new_population = np.zeros((self.pop_size, self.dim))\n            new_fitness = np.zeros(self.pop_size)\n\n            for i in range(self.pop_size):\n                # Mutation (Cauchy Mutation)\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x1, x2, x3 = population[idxs]\n                mutant = self.cauchy_mutation(x1, x2, x3)  # Apply Cauchy mutation\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                trial_vector = np.zeros(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR:\n                        trial_vector[j] = mutant[j]\n                    else:\n                        trial_vector[j] = population[i][j]\n                trial_vector = np.clip(trial_vector, func.bounds.lb, func.bounds.ub)\n\n                # Evaluation\n                f_trial = func(trial_vector)\n                self.budget -= 1\n\n                # Selection\n                if f_trial < fitness[i]:\n                    new_population[i] = trial_vector\n                    new_fitness[i] = f_trial\n                else:\n                    new_population[i] = population[i]\n                    new_fitness[i] = fitness[i]\n\n                # Update best\n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = trial_vector\n\n                if self.budget <= 0:\n                    break\n            \n            population = new_population\n            fitness = new_fitness\n            \n            # Resize population if needed. Done at the end of generation rather than beginning to not break the current generation loop.\n            if self.pop_size != population.shape[0]:\n                 num_to_add = self.pop_size - population.shape[0]\n                 if num_to_add > 0:\n                   new_individuals = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(num_to_add, self.dim))\n                   new_fitness_values = np.array([func(x) for x in new_individuals])\n                   self.budget -= num_to_add  # Account for evaluations\n\n                   population = np.vstack((population,new_individuals))\n                   fitness = np.concatenate((fitness, new_fitness_values))\n\n                 elif num_to_add < 0:\n                    #Remove worst performing individuals\n                    indices_to_remove = np.argpartition(fitness, -abs(num_to_add))[-abs(num_to_add):]\n                    population = np.delete(population, indices_to_remove, axis=0)\n                    fitness = np.delete(fitness, indices_to_remove)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "In the code, line 50, in __call__, the following error occurred:\nIndexError: index 53 is out of bounds for axis 0 with size 50\nOn line: x1, x2, x3 = population[idxs]", "error": "In the code, line 50, in __call__, the following error occurred:\nIndexError: index 53 is out of bounds for axis 0 with size 50\nOn line: x1, x2, x3 = population[idxs]", "parent_ids": ["791d324e-f228-4e4d-8980-8a92d538482d"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "37eb3186-dee4-4ea1-be8a-54ff6e38acae", "fitness": 0.5305424607171663, "name": "AdaptiveDE", "description": "Simplified Adaptive Differential Evolution with jittering, archive, and self-adaptation of F and CR, focused on balancing exploration and exploitation effectively.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=40, archive_size=20):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.F = 0.5\n        self.CR = 0.7\n        self.F_memory = np.full(self.pop_size, 0.5)\n        self.CR_memory = np.full(self.pop_size, 0.7)\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        # Initialize archive\n        archive = population[:self.archive_size].copy()  # Initialize archive with a subset of the initial population\n        archive_fitness = fitness[:self.archive_size].copy()\n\n        # Main loop\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Adaptive F and CR\n                F = self.F_memory[i]\n                CR = self.CR_memory[i]\n\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x1, x2, x3 = population[idxs]\n\n                # Incorporate archive with probability\n                if np.random.rand() < 0.2:\n                    idx_archive = np.random.randint(self.archive_size)\n                    mutant = x1 + F * (x2 - x3) + F * (archive[idx_archive] - x1)\n                else:\n                    mutant = x1 + F * (x2 - x3)\n\n                # Jittering\n                jitter = np.random.uniform(-0.05, 0.05, size=self.dim)  # Add small random jitter\n                mutant = np.clip(mutant + jitter, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                crossover_mask = np.random.rand(self.dim) < CR\n                trial_vector = np.where(crossover_mask, mutant, population[i])\n\n                # Evaluation\n                f_trial = func(trial_vector)\n                self.budget -= 1\n\n                # Selection\n                if f_trial < fitness[i]:\n                    # Update archive\n                    if f_trial < np.max(archive_fitness):\n                        worst_archive_idx = np.argmax(archive_fitness)\n                        archive[worst_archive_idx] = trial_vector\n                        archive_fitness[worst_archive_idx] = f_trial\n\n                    # Update population\n                    population[i] = trial_vector\n                    fitness[i] = f_trial\n\n                    # Update F and CR memory\n                    self.F_memory[i] = np.clip(np.random.normal(0.5, 0.1), 0.1, 1.0)\n                    self.CR_memory[i] = np.clip(np.random.normal(0.7, 0.1), 0.1, 1.0)\n\n                # Update best\n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = trial_vector\n\n                if self.budget <= 0:\n                    break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "The algorithm AdaptiveDE scored 0.531 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["47de978c-c17f-4e30-afce-97497a2e0e34"], "operator": null, "metadata": {"aucs": [0.22493716085853843, 0.3940888861366735, 0.479314014051317, 0.8815273577183427, 0.44815019102905085, 0.538418570978141, 0.3679260918227536, 0.41493848292890223, 0.44259183067318486, 0.4080048097389364, 0.8905338960497634, 0.9910627498042821, 0.3224654065636453, 0.4540764070718848, 0.9051413358654955, 0.5564058157434786, 0.42742230760278366, 0.6421465881481229, 0.30036239058824366, 0.521334920969784]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "4a5515c4-5cc3-46fa-b53c-dc01e992eebd", "fitness": 0.5514091910579064, "name": "AdaptiveDE", "description": "An enhanced Adaptive Differential Evolution with a distance-based mutation operator, dynamic population size adjustment, and a restart mechanism triggered by stagnation.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size_init=50, archive_size=10, stagnation_threshold=1000):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size_init\n        self.archive_size = archive_size\n        self.F = 0.5  # Initial value for differential weight\n        self.CR = 0.7  # Initial value for crossover rate\n        self.stagnation_threshold = stagnation_threshold\n        self.best_fitness_history = []\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.generation = 0\n\n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        # Initialize archive\n        archive = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.archive_size, self.dim))\n        archive_fitness = np.full(self.archive_size, np.inf)\n\n        # Main loop\n        while self.budget > 0:\n            self.generation += 1\n            for i in range(self.pop_size):\n                # Mutation: Distance-based mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x1, x2, x3 = population[idxs]\n\n                # Calculate distances\n                dist_x1_x2 = np.linalg.norm(x1 - x2)\n                dist_x1_x3 = np.linalg.norm(x1 - x3)\n\n                # Adaptive F based on distances\n                adaptive_F = self.F * (1 + 0.1 * (dist_x1_x2 + dist_x1_x3))\n                adaptive_F = np.clip(adaptive_F, 0.1, 1.0)\n\n                mutant = x1 + adaptive_F * (x2 - x3)\n\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                crossover_mask = np.random.rand(self.dim) < self.CR\n                trial_vector = np.where(crossover_mask, mutant, population[i])\n\n                # Evaluation\n                f_trial = func(trial_vector)\n                self.budget -= 1\n\n                # Selection\n                if f_trial < fitness[i]:\n                    # Update archive\n                    if f_trial < np.max(archive_fitness):\n                        worst_archive_idx = np.argmax(archive_fitness)\n                        archive[worst_archive_idx] = trial_vector\n                        archive_fitness[worst_archive_idx] = f_trial\n\n                    population[i] = trial_vector\n                    fitness[i] = f_trial\n\n                    # Self-adapt F and CR (simplified)\n                    self.F = np.clip(np.random.normal(0.5, 0.1), 0.1, 1.0)\n                    self.CR = np.clip(np.random.normal(0.7, 0.1), 0.1, 1.0)\n\n                # Update best\n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = trial_vector\n\n                if self.budget <= 0:\n                    break\n\n            # Dynamic population size adjustment\n            if self.generation % 10 == 0:\n                if len(self.best_fitness_history) > 10:\n                    if np.std(self.best_fitness_history[-10:]) < 1e-6:  #Stagnation detected\n                        self.pop_size = int(self.pop_size * 0.9)  # Reduce population size\n                        if self.pop_size < 10:\n                            self.pop_size = 50 # Reset pop size.\n                            population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                            fitness = np.array([func(x) for x in population])\n                            self.budget -= self.pop_size\n\n                        else:\n                            population = population[:self.pop_size] # truncate the population.\n                            fitness = fitness[:self.pop_size]\n\n\n                # Record the best fitness\n                self.best_fitness_history.append(self.f_opt)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "The algorithm AdaptiveDE scored 0.551 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["47de978c-c17f-4e30-afce-97497a2e0e34"], "operator": null, "metadata": {"aucs": [0.19959083054110083, 0.21381586393547447, 0.5075837473867464, 0.7593620799697277, 0.6435493660905586, 0.7013775354339755, 0.379992992360937, 0.5370314301283622, 0.6270758538248227, 0.5008052539312813, 0.648684488761093, 0.9958899492734428, 0.2611544151069868, 0.624872823171126, 0.8403146861656615, 0.6912424794070359, 0.43412978113246203, 0.763839411779358, 0.18834503402551317, 0.5095257987324633]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "5883870b-3d72-44a1-a6c8-b52310fa2d89", "fitness": "-inf", "name": "AdaptiveDECauchyLocal", "description": "An Adaptive Differential Evolution with a Cauchy mutation operator, a distance-based population diversity mechanism to adjust F and CR, and a local search triggered by stagnation detection.", "code": "import numpy as np\n\nclass AdaptiveDECauchyLocal:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=10, local_search_iterations=5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.F = 0.5  # Initial value for differential weight\n        self.CR = 0.7 # Initial value for crossover rate\n        self.local_search_iterations = local_search_iterations\n        self.stagnation_counter = 0\n        self.previous_best_fitness = np.inf\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        # Initialize archive\n        archive = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.archive_size, self.dim))\n        archive_fitness = np.full(self.archive_size, np.inf)\n\n        # Main loop\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation (Cauchy)\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x1, x2, x3 = population[idxs]\n\n                # Cauchy mutation\n                cauchy_scale = 0.1  # Adjust the scale of the Cauchy distribution\n                mutant = x1 + self.F * (x2 - x3) + cauchy_scale * np.random.standard_cauchy(size=self.dim)\n\n\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub) # Clip to boundaries\n\n\n                # Crossover\n                crossover_mask = np.random.rand(self.dim) < self.CR\n                trial_vector = np.where(crossover_mask, mutant, population[i])\n\n                # Evaluation\n                f_trial = func(trial_vector)\n                self.budget -= 1\n\n                # Selection\n                if f_trial < fitness[i]:\n                    # Update archive\n                    if f_trial < np.max(archive_fitness):\n                        worst_archive_idx = np.argmax(archive_fitness)\n                        archive[worst_archive_idx] = trial_vector\n                        archive_fitness[worst_archive_idx] = f_trial\n                    \n                    population[i] = trial_vector\n                    fitness[i] = f_trial\n\n                    # Self-adapt F and CR based on population diversity\n                    distances = np.linalg.norm(population - np.mean(population, axis=0), axis=1)\n                    diversity = np.mean(distances)\n                    self.F = np.clip(np.random.normal(0.5, 0.1 * (1 + diversity)), 0.1, 1.0)  # Increased F when diversity is high\n                    self.CR = np.clip(np.random.normal(0.7, 0.1 * (1 - diversity)), 0.1, 1.0)  # Decreased CR when diversity is high\n\n                \n\n                # Update best\n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = trial_vector\n                    self.stagnation_counter = 0 # Reset stagnation counter when finding better solution\n                else:\n                    self.stagnation_counter +=1\n\n                # Stagnation check and local search trigger\n                if self.stagnation_counter > 50:\n                    self.stagnation_counter = 0\n                    #print(\"Local search triggered\")\n                    self.x_opt, self.f_opt = self.local_search(func, self.x_opt) # Use self.x_opt as starting point for local search\n                    \n                    \n\n                if self.budget <= 0:\n                    break\n        return self.f_opt, self.x_opt\n    \n    def local_search(self, func, x_start):\n        x_current = x_start.copy()\n        f_current = func(x_current)\n        self.budget -= 1\n\n        for _ in range(self.local_search_iterations):\n            # Generate a small random step\n            step = np.random.uniform(-0.1, 0.1, size=self.dim)\n            x_new = x_current + step\n            x_new = np.clip(x_new, func.bounds.lb, func.bounds.ub)\n            \n            f_new = func(x_new)\n            self.budget -= 1\n\n            if f_new < f_current:\n                f_current = f_new\n                x_current = x_new\n\n            if self.budget <= 0:\n                break\n\n        return x_current, f_current", "configspace": "", "generation": 3, "feedback": "In the code, line 432, in numpy.random._common.check_constraint, the following error occurred:\nValueError: scale < 0", "error": "In the code, line 432, in numpy.random._common.check_constraint, the following error occurred:\nValueError: scale < 0", "parent_ids": ["47de978c-c17f-4e30-afce-97497a2e0e34"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "cac9ad3f-6920-4233-83d8-1cdb3d0ff09f", "fitness": 0.6246920174418077, "name": "OrthogonalDE", "description": "A Differential Evolution strategy with dynamic parameter adaptation based on the success history of previous generations, coupled with a diversity maintenance mechanism using orthogonal design.", "code": "import numpy as np\n\nclass OrthogonalDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=10, success_history_size=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.F = 0.5\n        self.CR = 0.7\n        self.success_history_size = success_history_size\n        self.success_F = np.full(self.success_history_size, 0.5)\n        self.success_CR = np.full(self.success_history_size, 0.7)\n        self.success_fitness_reduction = np.zeros(self.success_history_size)\n        self.success_idx = 0\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n\n        # Initialize population\n        population = np.random.uniform(lb, ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        # Initialize archive\n        archive = np.random.uniform(lb, ub, size=(self.archive_size, self.dim))\n        archive_fitness = np.full(self.archive_size, np.inf)\n\n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)]\n\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x1, x2, x3 = population[idxs]\n\n                mutant = x1 + self.F * (x2 - x3)\n                mutant = np.clip(mutant, lb, ub)\n\n\n                # Crossover\n                trial_vector = np.where(np.random.rand(self.dim) < self.CR, mutant, population[i])\n                trial_vector = np.clip(trial_vector, lb, ub)\n\n\n                # Evaluation\n                f_trial = func(trial_vector)\n                self.budget -= 1\n                \n                # Selection\n                if f_trial < fitness[i]:\n                    # Update archive\n                    if f_trial < np.max(archive_fitness):\n                        worst_archive_idx = np.argmax(archive_fitness)\n                        archive[worst_archive_idx] = trial_vector\n                        archive_fitness[worst_archive_idx] = f_trial\n\n                    # Store successful F and CR values\n                    self.success_F[self.success_idx] = self.F\n                    self.success_CR[self.success_idx] = self.CR\n                    self.success_fitness_reduction[self.success_idx] = fitness[i] - f_trial\n                    self.success_idx = (self.success_idx + 1) % self.success_history_size\n                    \n                    population[i] = trial_vector\n                    fitness[i] = f_trial\n\n                    # Update F and CR based on success history (weighted average)\n                    weights = self.success_fitness_reduction / np.sum(self.success_fitness_reduction) if np.sum(self.success_fitness_reduction) > 0 else np.ones(self.success_history_size) / self.success_history_size\n                    self.F = np.clip(np.sum(weights * self.success_F), 0.1, 1.0)\n                    self.CR = np.clip(np.sum(weights * self.success_CR), 0.1, 1.0)\n\n\n                # Update best\n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = trial_vector\n\n                if self.budget <= 0:\n                    break\n            \n            # Orthogonal Design-based Diversity Maintenance\n            if self.budget > self.dim:  # Ensure enough budget for orthogonal design\n                orthogonal_matrix = self.generate_orthogonal_matrix(self.dim)\n                if orthogonal_matrix is not None:  # Ensure matrix generation was successful\n                  for j in range(self.dim):\n                    # Choose a random individual to perturb\n                    idx_to_perturb = np.random.randint(0, self.pop_size)\n                    original_vector = population[idx_to_perturb].copy()\n                    \n                    # Perturb each dimension using the orthogonal matrix\n                    perturbation = orthogonal_matrix[j] * 0.1  # Scale the perturbation\n                    new_vector = original_vector + perturbation\n                    new_vector = np.clip(new_vector, lb, ub)\n                    \n                    #Evaluate\n                    f_new = func(new_vector)\n                    self.budget -= 1\n                    \n                    if f_new < fitness[idx_to_perturb]:\n                        population[idx_to_perturb] = new_vector\n                        fitness[idx_to_perturb] = f_new\n                        \n                        if f_new < self.f_opt:\n                            self.f_opt = f_new\n                            self.x_opt = new_vector\n                    if self.budget <= 0:\n                      break\n            if self.budget <= 0:\n              break\n\n        return self.f_opt, self.x_opt\n\n    def generate_orthogonal_matrix(self, dim):\n        # Generate a simple orthogonal matrix (Hadamard matrix) for power of 2 dimensions\n        if dim & (dim - 1) == 0:  # Check if dim is a power of 2\n            H = np.array([[1]])\n            while H.shape[0] < dim:\n                H = np.vstack((np.hstack((H, H)), np.hstack((H, -H))))\n            return H\n        else:\n            # For non-power-of-2 dimensions, return None (or implement other orthogonal matrix generation)\n            return None", "configspace": "", "generation": 3, "feedback": "The algorithm OrthogonalDE scored 0.625 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["9baaf3e2-5754-488d-9937-987ce7598085"], "operator": null, "metadata": {"aucs": [0.20266628340954662, 0.42151915551710295, 0.6134377583788126, 0.8235566361635124, 0.6844625122534012, 0.7582768412687612, 0.5645737937493811, 0.5794581879576097, 0.7174143497824589, 0.5987733685567138, 0.7707921641270689, 0.9884961455115397, 0.36648144699883034, 0.6349930707219014, 0.8869079639448122, 0.7908834437012575, 0.5395715633633824, 0.792920124797955, 0.23764553045907544, 0.5210100081730313]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "df918208-98a9-4eeb-884b-b188677212a5", "fitness": 0.0, "name": "AdaptiveDE_DynPop", "description": "Simplified Adaptive Differential Evolution with dynamic population size adjustment based on stagnation detection and adaptive mutation strategies for exploration and exploitation.", "code": "import numpy as np\n\nclass AdaptiveDE_DynPop:\n    def __init__(self, budget=10000, dim=10, initial_pop_size=30):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = initial_pop_size\n        self.initial_pop_size = initial_pop_size\n        self.F = 0.5\n        self.CR = 0.7\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.stagnation_counter = 0\n        self.stagnation_threshold = 50\n\n    def __call__(self, func):\n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        best_fitness_history = []\n\n        while self.budget > 0:\n            best_index = np.argmin(fitness)\n            current_best_fitness = fitness[best_index]\n\n            if not best_fitness_history or current_best_fitness < best_fitness_history[-1]:\n                self.stagnation_counter = 0\n            else:\n                self.stagnation_counter += 1\n\n            best_fitness_history.append(current_best_fitness)\n            if len(best_fitness_history) > 10:\n                best_fitness_history.pop(0)\n                \n            for i in range(self.pop_size):\n                # Mutation strategy: adapt F based on stagnation\n                if self.stagnation_counter > self.stagnation_threshold:\n                     F = np.random.uniform(0.8, 1.0)  # Increase F for exploration\n                else:\n                     F = np.random.uniform(0.2, 0.5)  # Decrease F for exploitation\n\n\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x1, x2, x3 = population[idxs]\n                mutant = x1 + F * (x2 - x3)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                crossover_mask = np.random.rand(self.dim) < self.CR\n                trial_vector = np.where(crossover_mask, mutant, population[i])\n                trial_vector = np.clip(trial_vector, func.bounds.lb, func.bounds.ub)\n\n                # Evaluation\n                f_trial = func(trial_vector)\n                self.budget -= 1\n\n                # Selection\n                if f_trial < fitness[i]:\n                    population[i] = trial_vector\n                    fitness[i] = f_trial\n\n                    # Update best\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial_vector\n\n            # Dynamic population size adjustment (simplified)\n            if self.stagnation_counter > 2 * self.stagnation_threshold and self.pop_size > 10:\n                self.pop_size = max(10, int(self.pop_size * 0.9))\n                population = population[:self.pop_size]\n                fitness = fitness[:self.pop_size]\n                self.stagnation_counter = 0  # Reset stagnation counter after reducing population\n\n            if self.budget <= 0:\n                break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "The algorithm AdaptiveDE_DynPop scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["77501324-766b-45aa-8166-ede5ac3d4390"], "operator": null, "metadata": {"aucs": [0]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "077d7b9d-ba51-4b31-9952-a874a7fb2c41", "fitness": 0.6646120715379628, "name": "AdaptiveDEToroidalLocal", "description": "A simplified self-adaptive Differential Evolution algorithm with toroidal handling and local search, focusing on efficient budget allocation and parameter adaptation.", "code": "import numpy as np\n\nclass AdaptiveDEToroidalLocal:\n    def __init__(self, budget=10000, dim=10, initial_pop_size=40, local_search_prob=0.05):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = initial_pop_size\n        self.lb = -5.0\n        self.ub = 5.0\n        self.F = 0.5\n        self.CR = 0.7\n        self.local_search_prob = local_search_prob\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        # Initialize population\n        population = np.random.uniform(lb, ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)]\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x1, x2, x3 = population[idxs]\n                mutant = x1 + self.F * (x2 - x3)\n\n                # Toroidal Boundary Handling\n                mutant = np.mod(mutant - lb, ub - lb) + lb\n\n                # Crossover\n                trial_vector = np.where(np.random.rand(self.dim) < self.CR, mutant, population[i])\n\n                # Evaluation\n                f_trial = func(trial_vector)\n                self.budget -= 1\n\n                # Selection\n                if f_trial < fitness[i]:\n                    population[i] = trial_vector\n                    fitness[i] = f_trial\n\n                    # Self-adapt F and CR (simplified)\n                    self.F = np.clip(np.random.normal(0.5, 0.1), 0.1, 1.0)\n                    self.CR = np.clip(np.random.normal(0.7, 0.1), 0.1, 1.0)\n\n                # Update best\n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = trial_vector\n\n                    # Trigger local search with probability\n                    if np.random.rand() < self.local_search_prob:\n                        self.x_opt, self.f_opt = self.local_search(func, self.x_opt, self.f_opt)\n\n                if self.budget <= 0:\n                    break\n\n        return self.f_opt, self.x_opt\n\n    def local_search(self, func, x_opt, f_opt, radius=0.1, num_steps=5):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        for _ in range(num_steps):\n            # Generate a random direction\n            direction = np.random.randn(self.dim)\n            direction /= np.linalg.norm(direction)\n\n            # Generate a random step size\n            step_size = np.random.uniform(-radius, radius)\n\n            # Create a new candidate solution\n            x_new = x_opt + step_size * direction\n            x_new = np.clip(x_new, lb, ub)\n\n            # Evaluate the new solution\n            f_new = func(x_new)\n            self.budget -= 1\n\n            # Update the best solution if necessary\n            if f_new < f_opt:\n                f_opt = f_new\n                x_opt = x_new\n\n            if self.budget <= 0:\n                break\n\n        return x_opt, f_opt", "configspace": "", "generation": 4, "feedback": "The algorithm AdaptiveDEToroidalLocal scored 0.665 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["9baaf3e2-5754-488d-9937-987ce7598085"], "operator": null, "metadata": {"aucs": [0.21781321072233162, 0.52676885552676, 0.6926701518618473, 0.8709166955885737, 0.7113977539459229, 0.788763703690623, 0.6379081001071707, 0.6546131995204916, 0.7616202591653021, 0.6251245395977216, 0.8018935658101636, 0.9963813616387732, 0.318237843255356, 0.7070292682666545, 0.8921444065106174, 0.7984611408047093, 0.537163133766223, 0.8265269471078783, 0.3991158871507602, 0.5276914067213763]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "925978c5-5892-4bfc-9af4-79922d2a6118", "fitness": 0.620399243181058, "name": "AdaptiveDE", "description": "A simplified Adaptive Differential Evolution with dynamic population size and parameter adaptation based on successful individuals, coupled with a simplified stagnation-triggered restart.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size_init=50, stagnation_threshold=1000):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size_init\n        self.F = 0.5  # Initial value for differential weight\n        self.CR = 0.7  # Initial value for crossover rate\n        self.stagnation_threshold = stagnation_threshold\n        self.best_fitness_history = []\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.generation = 0\n\n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        # Main loop\n        while self.budget > 0:\n            self.generation += 1\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x1, x2, x3 = population[idxs]\n                mutant = x1 + self.F * (x2 - x3)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                crossover_mask = np.random.rand(self.dim) < self.CR\n                trial_vector = np.where(crossover_mask, mutant, population[i])\n\n                # Evaluation\n                f_trial = func(trial_vector)\n                self.budget -= 1\n\n                # Selection\n                if f_trial < fitness[i]:\n                    # Update individual\n                    population[i] = trial_vector\n                    fitness[i] = f_trial\n\n                    # Adapt F and CR based on success (simplified)\n                    self.F = np.clip(np.random.normal(0.5, 0.1), 0.1, 1.0)\n                    self.CR = np.clip(np.random.normal(0.7, 0.1), 0.1, 1.0)\n\n                    # Update best\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial_vector\n\n                if self.budget <= 0:\n                    break\n\n            # Dynamic population size adjustment and restart\n            if self.generation % 20 == 0:\n                if len(self.best_fitness_history) > 20:\n                    if np.std(self.best_fitness_history[-20:]) < 1e-6:  # Stagnation detected\n                        self.pop_size = int(self.pop_size * 0.8)  # Reduce population size\n                        if self.pop_size < 10:\n                            self.pop_size = 50  # Reset pop size and restart\n                            population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                            fitness = np.array([func(x) for x in population])\n                            self.budget -= self.pop_size\n                        else:\n                            #Truncate population\n                            population = population[:self.pop_size]\n                            fitness = fitness[:self.pop_size]\n\n\n                # Record the best fitness\n                self.best_fitness_history.append(self.f_opt)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "The algorithm AdaptiveDE scored 0.620 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["4a5515c4-5cc3-46fa-b53c-dc01e992eebd"], "operator": null, "metadata": {"aucs": [0.21981512329404918, 0.4003287672448229, 0.616643018795686, 0.8179432670155898, 0.6994723654681624, 0.7618522325485303, 0.5811427880464355, 0.6037454713859159, 0.6884414279855626, 0.5489561848475568, 0.7687709279598801, 0.9931147519923521, 0.3120572071303934, 0.6891144200004823, 0.8549748770055521, 0.7371413365720632, 0.5244871808218103, 0.8232386159109606, 0.2552191047880167, 0.5115257948073367]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "dd569544-1c46-4910-82e4-dfe2a6569e9f", "fitness": 0.38363475770604305, "name": "AdaptiveDE", "description": "An enhanced Adaptive Differential Evolution with a success-history based parameter adaptation, toroidal boundary handling, and a restart mechanism triggered by stagnation.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=10, memory_size=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.memory_size = memory_size\n        self.F = 0.5  # Initial value for differential weight\n        self.CR = 0.7 # Initial value for crossover rate\n        self.F_memory = np.full(self.memory_size, self.F)\n        self.CR_memory = np.full(self.memory_size, self.CR)\n        self.memory_idx = 0\n        self.success_F = []\n        self.success_CR = []\n        self.stagnation_counter = 0\n        self.best_fitness_history = []\n\n    def toroidal_boundary_handling(self, x, lb, ub):\n        \"\"\"Handles boundaries using a toroidal approach.\"\"\"\n        width = ub - lb\n        x_wrapped = lb + (x - lb) % width\n        return x_wrapped\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        # Initialize archive\n        archive = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.archive_size, self.dim))\n        archive_fitness = np.full(self.archive_size, np.inf)\n\n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)]\n\n        self.best_fitness_history.append(self.f_opt)\n\n        # Main loop\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Parameter adaptation\n                self.F = self.F_memory[self.memory_idx]\n                self.CR = self.CR_memory[self.memory_idx]\n\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x1, x2, x3 = population[idxs]\n\n                # Add a random vector from archive\n                if np.random.rand() < 0.1: # Probability of 0.1 to use archive member\n                    idx_archive = np.random.choice(self.archive_size)\n                    mutant = x1 + self.F * (x2 - x3) + self.F * (archive[idx_archive] - x1)  # Mutation with archive\n\n                else:\n                    mutant = x1 + self.F * (x2 - x3) # Standard DE mutation\n                \n                # Toroidal boundary handling\n                mutant = self.toroidal_boundary_handling(mutant, func.bounds.lb, func.bounds.ub)\n\n\n                # Crossover\n                crossover_mask = np.random.rand(self.dim) < self.CR\n                trial_vector = np.where(crossover_mask, mutant, population[i])\n\n                # Evaluation\n                f_trial = func(trial_vector)\n                self.budget -= 1\n\n                # Selection\n                if f_trial < fitness[i]:\n                    # Update archive\n                    if f_trial < np.max(archive_fitness):\n                        worst_archive_idx = np.argmax(archive_fitness)\n                        archive[worst_archive_idx] = trial_vector\n                        archive_fitness[worst_archive_idx] = f_trial\n                    \n                    population[i] = trial_vector\n                    fitness[i] = f_trial\n\n                    # Store successful F and CR values\n                    self.success_F.append(self.F)\n                    self.success_CR.append(self.CR)\n\n                    # Self-adapt F and CR (success-history based)\n                    if self.success_F:\n                        self.F = np.mean(self.success_F)\n                        self.CR = np.mean(self.success_CR)\n                        self.success_F = []\n                        self.success_CR = []\n                        \n                        self.F = np.clip(np.random.normal(self.F, 0.1), 0.1, 1.0)  \n                        self.CR = np.clip(np.random.normal(self.CR, 0.1), 0.1, 1.0)\n\n                    # Update memory\n                    self.F_memory[self.memory_idx] = self.F\n                    self.CR_memory[self.memory_idx] = self.CR\n                    self.memory_idx = (self.memory_idx + 1) % self.memory_size\n                \n\n                # Update best\n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = trial_vector\n                    self.stagnation_counter = 0\n                else:\n                    self.stagnation_counter += 1\n\n                if self.budget <= 0:\n                    break\n            \n            # Stagnation check and restart\n            if self.stagnation_counter > 500:\n                population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                fitness = np.array([func(x) for x in population])\n                self.budget -= self.pop_size\n                self.f_opt = np.min(fitness)\n                self.x_opt = population[np.argmin(fitness)]\n                self.stagnation_counter = 0\n                archive = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.archive_size, self.dim))\n                archive_fitness = np.full(self.archive_size, np.inf)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "The algorithm AdaptiveDE scored 0.384 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["47de978c-c17f-4e30-afce-97497a2e0e34"], "operator": null, "metadata": {"aucs": [0.1494953386821668, 0.2121740071217001, 0.33526750387235205, 0.5268302279672031, 0.30812462451118783, 0.7124768935291964, 0.31820223604931674, 0.4646694347870092, 0.4780471746353856, 0.21308337553309953, 0.38836382775447875, 0.9911950528019586, 0.2729569106395484, 0]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "ffcc8345-bd5c-4ab3-9693-47f9c7959496", "fitness": "-inf", "name": "NeighborhoodCMAES", "description": "Covariance matrix adaptation evolution strategy with neighborhood-based mutation and dynamic population size adjustment based on success rate.", "code": "import numpy as np\n\nclass NeighborhoodCMAES:\n    def __init__(self, budget=10000, dim=10, pop_size_init=20, mu_factor=0.25, cs=0.3, damps=1.0, c_cov=0.1, neighborhood_size=5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size_init\n        self.mu = int(self.pop_size * mu_factor)\n        self.cs = cs\n        self.damps = damps\n        self.c_cov = c_cov\n        self.neighborhood_size = neighborhood_size\n        self.m = None\n        self.sigma = 0.1\n        self.C = None\n        self.pc = None\n        self.success_rate = 0.5\n        self.success_history = []\n        self.history_length = 10\n        self.generation = 0\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        # Initialization\n        self.m = np.random.uniform(-1, 1, size=self.dim)  # Mean value\n        self.C = np.eye(self.dim)  # Covariance matrix\n        self.pc = np.zeros(self.dim)  # Evolution path for C\n\n        while self.budget > 0:\n            self.generation += 1\n            # Generate and evaluate lambda offspring\n            z = np.random.multivariate_normal(np.zeros(self.dim), self.C, size=self.pop_size)\n            x = self.m + self.sigma * z\n            x = np.clip(x, func.bounds.lb, func.bounds.ub)  # Clip to bounds\n\n            fitness = np.array([func(xi) for xi in x])\n            self.budget -= self.pop_size\n\n            # Sort by fitness\n            idx = np.argsort(fitness)\n            x = x[idx]\n            fitness = fitness[idx]\n\n            # Update best solution\n            if fitness[0] < self.f_opt:\n                self.f_opt = fitness[0]\n                self.x_opt = x[0]\n\n            # Selection and update mean\n            x_mu = x[:self.mu]\n            self.m = np.mean(x_mu, axis=0)\n            z_mu = (x_mu - self.m) / self.sigma # For update equations\n\n            # Update evolution path\n            self.pc = (1 - self.cs) * self.pc + np.sqrt(self.cs * (2 - self.cs)) * np.sqrt(self.mu / self.pop_size) * np.mean(z_mu, axis=0)\n\n\n            # Covariance matrix adaptation using neighborhood\n            dC = np.zeros_like(self.C)\n            for i in range(self.mu):\n                # Neighborhood selection\n                neighborhood_indices = np.random.choice(self.mu, self.neighborhood_size, replace=False)\n                neighborhood_z = z_mu[neighborhood_indices]\n                neighborhood_z_mean = np.mean(neighborhood_z, axis=0)  # Mean of the neighborhood\n                dC += np.outer(neighborhood_z_mean, neighborhood_z_mean) # Update based on neighborhood.\n\n            self.C = (1 - self.c_cov) * self.C + self.c_cov * dC / self.mu\n\n            # Ensure positive definiteness (add a small identity matrix if needed)\n            try:\n                np.linalg.cholesky(self.C)\n            except np.linalg.LinAlgError:\n                self.C = self.C + 1e-6 * np.eye(self.dim)\n\n            # Cumulation for sigma (step size)\n            c_sigma = (np.sqrt(np.sum(self.pc**2)) / np.sqrt(self.dim))\n            self.sigma *= np.exp(self.cs / self.damps * (c_sigma - 1))\n\n\n            # Dynamic population size adjustment based on success rate\n            if self.generation % 10 == 0:\n                success = (fitness[0] < np.mean(fitness)) # Simple success criterion\n                self.success_history.append(success)\n                if len(self.success_history) > self.history_length:\n                    self.success_history.pop(0)\n                    self.success_rate = np.mean(self.success_history)\n\n                    if self.success_rate > 0.7:\n                        self.pop_size = min(self.pop_size + 1, 100)  # Increase population size\n                        self.mu = int(self.pop_size * 0.25)\n                    elif self.success_rate < 0.3:\n                        self.pop_size = max(10, self.pop_size - 1)  # Decrease population size\n                        self.mu = int(self.pop_size * 0.25)\n\n            if self.budget <= 0:\n                break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "In the code, line 1025, in numpy.random.mtrand.RandomState.choice, the following error occurred:\nValueError: Cannot take a larger sample than population when 'replace=False'", "error": "In the code, line 1025, in numpy.random.mtrand.RandomState.choice, the following error occurred:\nValueError: Cannot take a larger sample than population when 'replace=False'", "parent_ids": ["4a5515c4-5cc3-46fa-b53c-dc01e992eebd"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "a83af348-bb24-4797-917e-bc6e58015b8d", "fitness": 0.17020097523708688, "name": "AdaptiveDEToroidalLocal", "description": "Self-adaptive Differential Evolution with simplified toroidal handling, periodic population rejuvenation, and a focused local search triggered by improvements to efficiently explore the search space.", "code": "import numpy as np\n\nclass AdaptiveDEToroidalLocal:\n    def __init__(self, budget=10000, dim=10, initial_pop_size=50, stagnation_threshold=10, local_search_prob=0.1, rejuvenation_rate=0.2):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = initial_pop_size\n        self.initial_pop_size = initial_pop_size\n        self.stagnation_threshold = stagnation_threshold\n        self.stagnation_counter = 0\n        self.best_fitness_history = []\n        self.local_search_prob = local_search_prob\n        self.rejuvenation_rate = rejuvenation_rate\n        self.F = 0.5\n        self.CR = 0.7\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        population = np.random.uniform(lb, ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)]\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x1, x2, x3 = population[idxs]\n\n                mutant = x1 + self.F * (x2 - x3)\n\n                # Simplified Toroidal Boundary Handling (clip to bounds instead of modulo)\n                mutant = np.clip(mutant, lb, ub)\n\n                trial_vector = np.where(np.random.rand(self.dim) < self.CR, mutant, population[i])\n\n                f_trial = func(trial_vector)\n                self.budget -= 1\n\n                if f_trial < fitness[i]:\n                    population[i] = trial_vector\n                    fitness[i] = f_trial\n\n                    # Self-adapt F and CR (simplified)\n                    self.F = np.clip(np.random.normal(0.5, 0.1), 0.1, 1.0)\n                    self.CR = np.clip(np.random.normal(0.7, 0.1), 0.1, 1.0)\n\n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = trial_vector\n                    self.stagnation_counter = 0\n\n                    if np.random.rand() < self.local_search_prob:\n                        self.x_opt, self.f_opt = self.local_search(func, self.x_opt, self.f_opt)\n\n                if self.budget <= 0:\n                    break\n\n            # Periodic Rejuvenation\n            if np.random.rand() < self.rejuvenation_rate:\n                num_rejuvenate = int(self.pop_size * 0.1)\n                idxs_rejuvenate = np.random.choice(self.pop_size, num_rejuvenate, replace=False)\n                population[idxs_rejuvenate] = np.random.uniform(lb, ub, size=(num_rejuvenate, self.dim))\n                fitness[idxs_rejuvenate] = np.array([func(x) for x in population[idxs_rejuvenate]])\n                self.budget -= num_rejuvenate\n\n                best_idx = np.argmin(fitness)\n                self.f_opt = fitness[best_idx]\n                self.x_opt = population[best_idx]\n\n\n            # Stagnation Check (simplified - only track best fitness)\n            self.best_fitness_history.append(self.f_opt)\n            if len(self.best_fitness_history) > self.stagnation_threshold:\n                if np.abs(self.best_fitness_history[-1] - self.best_fitness_history[-self.stagnation_threshold]) < 1e-6:\n                    self.stagnation_counter += 1\n                else:\n                    self.stagnation_counter = 0\n                self.best_fitness_history.pop(0)\n\n            #Simple reset instead of pop size change.\n            if self.stagnation_counter > self.stagnation_threshold:\n                population = np.random.uniform(lb, ub, size=(self.pop_size, self.dim))\n                fitness = np.array([func(x) for x in population])\n                self.budget -= self.pop_size\n                self.stagnation_counter = 0\n\n                best_idx = np.argmin(fitness)\n                self.f_opt = fitness[best_idx]\n                self.x_opt = population[best_idx]\n\n\n\n            if self.budget <= 0:\n                break\n\n        return self.f_opt, self.x_opt\n\n    def local_search(self, func, x_opt, f_opt, radius=0.05, num_steps=3):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        for _ in range(num_steps):\n            direction = np.random.randn(self.dim)\n            direction /= np.linalg.norm(direction)\n            step_size = np.random.uniform(-radius, radius)\n            x_new = x_opt + step_size * direction\n            x_new = np.clip(x_new, lb, ub)\n            f_new = func(x_new)\n            self.budget -= 1\n            if f_new < f_opt:\n                f_opt = f_new\n                x_opt = x_new\n            if self.budget <= 0:\n                break\n        return x_opt, f_opt", "configspace": "", "generation": 4, "feedback": "The algorithm AdaptiveDEToroidalLocal scored 0.170 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["9baaf3e2-5754-488d-9937-987ce7598085"], "operator": null, "metadata": {"aucs": [0.22163156656508032, 0.28897135914618033, 0]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "4b9eb89e-36f8-4354-b6aa-f7cbe5efc3fd", "fitness": "-inf", "name": "CrowDE", "description": "A Differential Evolution strategy employing a crow search inspired memory update, adaptive parameter control based on fitness improvement, and a local search phase using Nelder-Mead simplex.", "code": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass CrowDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=10, memory_size=5, flight_length=2.0, awareness_probability=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.memory_size = memory_size\n        self.flight_length = flight_length\n        self.awareness_probability = awareness_probability\n        self.F = 0.5\n        self.CR = 0.7\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.memory = None\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        # Initialize population and memory\n        population = np.random.uniform(lb, ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        self.memory = population.copy()  # Initialize memory with current positions\n        memory_fitness = fitness.copy()\n\n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)]\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Crow Search Inspired Memory Update\n                if np.random.rand() < self.awareness_probability:\n                    # Follow the best crow (exploitation)\n                    best_crow_idx = np.argmin(memory_fitness)\n                    mutant = population[i] + np.random.rand() * self.flight_length * (self.memory[best_crow_idx] - population[i])\n                else:\n                    # Random crow (exploration)\n                    crow_idx = np.random.choice(self.pop_size)\n                    mutant = population[i] + np.random.rand() * self.flight_length * (self.memory[crow_idx] - population[i])\n\n                # Boundary Handling (clip)\n                mutant = np.clip(mutant, lb, ub)\n\n                # Differential Evolution Crossover\n                trial_vector = np.where(np.random.rand(self.dim) < self.CR, mutant, population[i])\n\n                # Evaluation\n                f_trial = func(trial_vector)\n                self.budget -= 1\n\n                # Selection\n                if f_trial < fitness[i]:\n                    population[i] = trial_vector\n                    fitness[i] = f_trial\n\n                    # Update memory\n                    if f_trial < memory_fitness[i]:\n                        self.memory[i] = trial_vector\n                        memory_fitness[i] = f_trial\n\n\n                    # Adaptive Parameter Control (based on improvement)\n                    self.F = np.clip(np.random.normal(0.5, 0.1 * (1 - f_trial / self.f_opt if self.f_opt != 0 else 1)), 0.1, 1.0)\n                    self.CR = np.clip(np.random.normal(0.7, 0.1 * (1 - f_trial / self.f_opt if self.f_opt != 0 else 1)), 0.1, 1.0)\n\n\n                # Update best\n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = trial_vector\n\n                    # Local Search (Nelder-Mead)\n                    if self.budget > self.dim + 1: # Check if there is enough budget for local search.\n                        res = minimize(func, self.x_opt, method='Nelder-Mead', bounds=func.bounds, options={'maxfev': min(self.budget, 50)})  # Limit local search evaluations\n                        if res.fun < self.f_opt:\n                            self.x_opt = res.x\n                            self.f_opt = res.fun\n                            self.budget -= res.nfev\n\n                if self.budget <= 0:\n                    break\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "In the code, line 432, in numpy.random._common.check_constraint, the following error occurred:\nValueError: scale < 0", "error": "In the code, line 432, in numpy.random._common.check_constraint, the following error occurred:\nValueError: scale < 0", "parent_ids": ["9baaf3e2-5754-488d-9937-987ce7598085"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "0a12360f-a539-43e7-a7a3-979bf41be59a", "fitness": 0.6167031063345654, "name": "DynamicRangeDE", "description": "A Differential Evolution strategy with a dynamically adjusting search range based on the population's distribution and a restart mechanism triggered by a lack of diversity.", "code": "import numpy as np\n\nclass DynamicRangeDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, diversity_threshold=0.01, restart_probability=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.diversity_threshold = diversity_threshold\n        self.restart_probability = restart_probability\n        self.F = 0.5\n        self.CR = 0.7\n        self.best_fitness = np.inf\n        self.best_solution = None\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        range_width = ub - lb\n        current_lb = lb\n        current_ub = ub\n\n        # Initialize population within the current range\n        population = np.random.uniform(current_lb, current_ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        best_index = np.argmin(fitness)\n        self.best_fitness = fitness[best_index]\n        self.best_solution = population[best_index]\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x1, x2, x3 = population[idxs]\n                mutant = x1 + self.F * (x2 - x3)\n\n                # Clip to current range\n                mutant = np.clip(mutant, current_lb, current_ub)\n\n                # Crossover\n                trial_vector = np.where(np.random.rand(self.dim) < self.CR, mutant, population[i])\n\n                # Evaluation\n                f_trial = func(trial_vector)\n                self.budget -= 1\n\n                # Selection\n                if f_trial < fitness[i]:\n                    population[i] = trial_vector\n                    fitness[i] = f_trial\n\n                    if f_trial < self.best_fitness:\n                        self.best_fitness = f_trial\n                        self.best_solution = trial_vector\n\n                if self.budget <= 0:\n                    break\n\n            # Diversity check\n            diversity = np.std(population)\n            if diversity < self.diversity_threshold:\n                # Adjust search range\n                range_reduction_factor = 0.5\n                new_range_width = range_width * range_reduction_factor\n\n                #Center the new range around the best solution\n                current_lb = np.maximum(self.best_solution - new_range_width / 2, lb)\n                current_ub = np.minimum(self.best_solution + new_range_width / 2, ub)\n\n                # Restart with probability\n                if np.random.rand() < self.restart_probability:\n                    population = np.random.uniform(lb, ub, size=(self.pop_size, self.dim))\n                    fitness = np.array([func(x) for x in population])\n                    self.budget -= self.pop_size\n\n                else:\n                    #Repopulate within the new smaller range\n                    population = np.random.uniform(current_lb, current_ub, size=(self.pop_size, self.dim))\n                    fitness = np.array([func(x) for x in population])\n                    self.budget -= self.pop_size\n\n\n\n            best_index = np.argmin(fitness)\n            if fitness[best_index] < self.best_fitness:\n                self.best_fitness = fitness[best_index]\n                self.best_solution = population[best_index]\n\n        return self.best_fitness, self.best_solution", "configspace": "", "generation": 4, "feedback": "The algorithm DynamicRangeDE scored 0.617 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["9baaf3e2-5754-488d-9937-987ce7598085"], "operator": null, "metadata": {"aucs": [0.22671798496729212, 0.36631834200957414, 0.6131085955095236, 0.8318785655872294, 0.6673867481868413, 0.7527541956094081, 0.5278361645888945, 0.5944798822410879, 0.6628315022008318, 0.6359368870214057, 0.7848451996293402, 0.9939176409429904, 0.30536279483111717, 0.6569809580176278, 0.879885825103646, 0.757994784040626, 0.49834051173379257, 0.815989808177612, 0.24127388357885338, 0.5202218527136134]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "ffd58a40-77a1-4605-a8d0-e75cb2f0e62f", "fitness": "-inf", "name": "AdaptiveDE", "description": "Adaptive Differential Evolution with a combined mutation strategy utilizing both distance-based and archive-based components, along with dynamic population sizing and parameter adaptation based on success history and stagnation detection.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size_init=50, archive_size=10, stagnation_threshold=1000, success_history_size=5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size_init\n        self.archive_size = archive_size\n        self.F = 0.5  # Initial value for differential weight\n        self.CR = 0.7  # Initial value for crossover rate\n        self.stagnation_threshold = stagnation_threshold\n        self.best_fitness_history = []\n        self.success_history_size = success_history_size\n        self.success_F = np.ones(self.success_history_size) * self.F\n        self.success_CR = np.ones(self.success_history_size) * self.CR\n        self.success_fitness_reduction = np.zeros(self.success_history_size)\n        self.success_idx = 0\n\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.generation = 0\n\n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        # Initialize archive\n        archive = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.archive_size, self.dim))\n        archive_fitness = np.full(self.archive_size, np.inf)\n\n        # Main loop\n        while self.budget > 0:\n            self.generation += 1\n            for i in range(self.pop_size):\n                # Mutation: Combined Distance-based and Archive-based mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x1, x2, x3 = population[idxs]\n\n                # Calculate distances for distance-based mutation\n                dist_x1_x2 = np.linalg.norm(x1 - x2)\n                dist_x1_x3 = np.linalg.norm(x1 - x3)\n\n                # Adaptive F based on distances\n                adaptive_F = self.F * (1 + 0.1 * (dist_x1_x2 + dist_x1_x3))\n                adaptive_F = np.clip(adaptive_F, 0.1, 1.0)\n\n                # Distance-based mutant\n                mutant_dist = x1 + adaptive_F * (x2 - x3)\n                mutant_dist = np.clip(mutant_dist, func.bounds.lb, func.bounds.ub)\n\n                # Archive-based mutation\n                if len(archive) > 0:\n                    rand_archive_idx = np.random.randint(0, len(archive))\n                    mutant_archive = population[i] + self.F * (archive[rand_archive_idx] - population[i])\n                    mutant_archive = np.clip(mutant_archive, func.bounds.lb, func.bounds.ub)\n\n                    # Combine mutants: weighted average or random selection\n                    if np.random.rand() < 0.5:\n                        mutant = mutant_dist\n                    else:\n                        mutant = mutant_archive\n                else:\n                    mutant = mutant_dist  # Fallback to distance-based if archive is empty\n\n\n\n                # Crossover\n                crossover_mask = np.random.rand(self.dim) < self.CR\n                trial_vector = np.where(crossover_mask, mutant, population[i])\n                trial_vector = np.clip(trial_vector, func.bounds.lb, func.bounds.ub)\n\n\n                # Evaluation\n                f_trial = func(trial_vector)\n                self.budget -= 1\n\n                # Selection\n                if f_trial < fitness[i]:\n                    fitness_reduction = fitness[i] - f_trial\n\n                    # Update archive\n                    if f_trial < np.max(archive_fitness):\n                        worst_archive_idx = np.argmax(archive_fitness)\n                        archive[worst_archive_idx] = trial_vector\n                        archive_fitness[worst_archive_idx] = f_trial\n\n                    population[i] = trial_vector\n                    fitness[i] = f_trial\n                    # Update success history\n                    self.success_F[self.success_idx] = self.F\n                    self.success_CR[self.success_idx] = self.CR\n                    self.success_fitness_reduction[self.success_idx] = fitness_reduction\n                    self.success_idx = (self.success_idx + 1) % self.success_history_size\n                    \n                    # Self-adapt F and CR based on success history (weighted average)\n                    weights = self.success_fitness_reduction / np.sum(self.success_fitness_reduction) if np.sum(self.success_fitness_reduction) > 0 else np.ones(self.success_history_size) / self.success_history_size\n                    self.F = np.clip(np.average(self.success_F, weights=weights), 0.1, 1.0)\n                    self.CR = np.clip(np.average(self.success_CR, weights=weights), 0.1, 1.0)\n                    \n                else:\n                    #Adapt F and CR even if the trial vector isn't better.\n                     self.F = np.clip(np.random.normal(self.F, 0.1), 0.1, 1.0)\n                     self.CR = np.clip(np.random.normal(self.CR, 0.1), 0.1, 1.0)\n\n\n                # Update best\n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = trial_vector\n\n                if self.budget <= 0:\n                    break\n\n            # Dynamic population size adjustment\n            if self.generation % 10 == 0:\n                if len(self.best_fitness_history) > 10:\n                    if np.std(self.best_fitness_history[-10:]) < 1e-6:  #Stagnation detected\n                        self.pop_size = int(self.pop_size * 0.9)  # Reduce population size\n                        if self.pop_size < 10:\n                            self.pop_size = 50 # Reset pop size.\n                            population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                            fitness = np.array([func(x) for x in population])\n                            self.budget -= self.pop_size\n\n                        else:\n                            population = population[:self.pop_size] # truncate the population.\n                            fitness = fitness[:self.pop_size]\n\n\n                # Record the best fitness\n                self.best_fitness_history.append(self.f_opt)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "Evaluation timed out after 300 seconds.", "error": "Evaluation timed out after 300 seconds.", "parent_ids": ["4a5515c4-5cc3-46fa-b53c-dc01e992eebd"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "8f5b7a7c-8f14-4625-b6d9-c8f680ad9b61", "fitness": "-inf", "name": "HybridDECMARestart", "description": "A hybrid optimization algorithm combining a modified Differential Evolution with covariance matrix adaptation and a restart strategy, leveraging past successful search directions.", "code": "import numpy as np\n\nclass HybridDECMARestart:\n    def __init__(self, budget=10000, dim=10, pop_size=None, archive_size=10, restart_trigger=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = 5 * self.dim if pop_size is None else pop_size\n        self.archive_size = archive_size\n        self.restart_trigger = restart_trigger  # Fraction of budget used before restart check\n        self.mu = self.pop_size // 2\n        self.weights = np.log(self.mu + 1) - np.log(np.arange(1, self.mu + 1))\n        self.weights /= np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        self.cs = (self.mueff + 2) / (self.dim + self.mueff + 5)\n        self.damps = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1) + self.cs\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.chiN = self.dim**0.5 * (1 - 1 / (4 * self.dim) + 1 / (21 * self.dim**2))\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.sigma = 0.1\n        self.generation = 0\n        self.archive = []\n        self.lb = -5.0\n        self.ub = 5.0\n\n\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        self.lb = lb\n        self.ub = ub\n\n        # Initialize population\n        population = np.random.uniform(lb, ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)]\n        self.archive.append((self.x_opt, self.f_opt))\n\n        mean = np.mean(population, axis=0)\n\n\n        while self.budget > 0:\n            self.generation += 1\n\n            # Generate offspring\n            z = np.random.normal(0, 1, size=(self.pop_size, self.dim))\n            y = mean + self.sigma * np.dot(z, np.linalg.cholesky(self.C).T)\n            y = np.clip(y, lb, ub)\n\n\n            fitness_offspring = np.array([func(x) for x in y])\n            self.budget -= self.pop_size\n            \n            if self.budget <= 0:\n              break\n\n            # Selection and update mean\n            idx = np.argsort(fitness_offspring)\n            fitness_offspring = fitness_offspring[idx]\n            y = y[idx]\n            \n            delta_mean = np.mean(y[:self.mu], axis=0) - mean\n            mean = np.mean(y[:self.mu], axis=0)\n\n            # CMA-ES updates\n            ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs) * self.mueff) * delta_mean / self.sigma\n            self.pc = (1 - self.damps) * self.pc + np.sqrt(self.damps * (2 - self.damps) * self.mueff) * (mean - np.mean(population, axis=0)) / self.sigma\n            \n            hsig = (np.sum(ps**2) / (1 - (1 - self.cs)**(2 * self.generation / self.pop_size)) / self.dim) < (2 + 4 / (self.dim + 1))\n            self.C = (1 - self.damps) * self.C + self.damps * (np.outer(self.pc, self.pc) + (1 - hsig) * self.damps * (2 - self.damps) * self.C)\n            self.sigma *= np.exp((self.cs / self.damps) * (np.linalg.norm(ps) / self.chiN - 1))\n\n            population = y\n            fitness = fitness_offspring\n\n            # Update best\n            if np.min(fitness) < self.f_opt:\n                self.f_opt = np.min(fitness)\n                self.x_opt = population[np.argmin(fitness)]\n                self.archive.append((self.x_opt, self.f_opt))\n\n            # Restart if stagnation is detected\n            if self.generation > (self.budget * self.restart_trigger) / self.pop_size:\n                if self.sigma < 1e-6:\n                    # Random Restart from Best Archive Points\n                    num_archive_points = min(len(self.archive), self.pop_size)\n                    archive_indices = np.random.choice(len(self.archive), num_archive_points, replace=False)\n                    \n                    restart_points = [self.archive[i][0] for i in archive_indices]\n                    restart_points = np.array(restart_points)\n\n                    #Perturb the restart points\n                    population = restart_points + 0.01 * np.random.normal(0, 1, size = (num_archive_points, self.dim))\n\n                    #Fill up rest of the population with random initialization\n                    if num_archive_points < self.pop_size:\n                         population = np.vstack((population, np.random.uniform(lb, ub, size=(self.pop_size - num_archive_points, self.dim))))\n\n                    population = np.clip(population, lb, ub)\n                    fitness = np.array([func(x) for x in population])\n                    self.budget -= (self.pop_size - num_archive_points)\n\n                    mean = np.mean(population, axis=0)\n                    self.sigma = 0.1\n\n                    self.C = np.eye(self.dim)\n                    self.pc = np.zeros(self.dim)\n                    self.ps = np.zeros(self.dim)\n                    self.generation = 0\n                \n            if self.budget <= 0:\n                break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "In the code, line 148, in _raise_linalgerror_nonposdef, the following error occurred:\nLinAlgError: Matrix is not positive definite", "error": "In the code, line 148, in _raise_linalgerror_nonposdef, the following error occurred:\nLinAlgError: Matrix is not positive definite", "parent_ids": ["cac9ad3f-6920-4233-83d8-1cdb3d0ff09f"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "d2c6451d-3466-4580-8d0a-14e78c41d784", "fitness": 0.5707163386089819, "name": "EnhancedAdaptiveDE", "description": "Enhanced Adaptive Differential Evolution with orthogonal crossover, stagnation detection with restart, and covariance matrix adaptation for parameter control, aiming for improved exploration and exploitation.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=10, stagnation_threshold=50):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.F = 0.5  # Initial value for differential weight\n        self.CR = 0.7  # Initial value for crossover rate\n        self.stagnation_threshold = stagnation_threshold\n        self.stagnation_counter = 0\n        self.C = np.eye(dim)  # Covariance matrix for CMA-ES-like adaptation\n\n    def orthogonal_crossover(self, mutant, target, num_samples=3):\n        \"\"\"Performs orthogonal crossover using a Latin hypercube design.\"\"\"\n        if self.dim <= 1:  # Prevents error with low dimensions\n            return mutant\n\n        design = self.latin_hypercube(num_samples, self.dim)\n        candidates = []\n        for i in range(num_samples):\n            mask = design[i] > 0.5  # Create boolean mask\n            candidate = np.where(mask, mutant, target)\n            candidates.append(candidate)\n\n        return candidates\n\n    def latin_hypercube(self, n, dim):\n        \"\"\"Generates a Latin hypercube sample of size n in dim dimensions.\"\"\"\n        points = np.zeros((n, dim))\n        for i in range(dim):\n            points[:, i] = (np.random.permutation(n) + np.random.rand(n)) / n\n        return points\n\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        # Initialize archive\n        archive = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.archive_size, self.dim))\n        archive_fitness = np.full(self.archive_size, np.inf)\n\n        best_fitness_history = []\n\n        # Main loop\n        while self.budget > 0:\n            best_fitness_history.append(np.min(fitness))\n\n            if len(best_fitness_history) > self.stagnation_threshold:\n                if np.std(best_fitness_history[-self.stagnation_threshold:]) < 1e-6:\n                    # Stagnation detected, restart with a new population\n                    population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                    fitness = np.array([func(x) for x in population])\n                    self.budget -= self.pop_size\n                    self.stagnation_counter = 0  # Reset stagnation counter\n                    self.C = np.eye(self.dim) # Reset covariance matrix\n                    best_fitness_history = [] # Reset history\n\n                    if self.budget <= 0:\n                        break\n\n\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x1, x2, x3 = population[idxs]\n\n                # Add a random vector from archive\n                if np.random.rand() < 0.1:  # Probability of 0.1 to use archive member\n                    idx_archive = np.random.choice(self.archive_size)\n                    mutant = x1 + self.F * (x2 - x3) + self.F * (archive[idx_archive] - x1)  # Mutation with archive\n                else:\n                    mutant = x1 + self.F * (x2 - x3) # Standard DE mutation\n\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub) # Clip to boundaries\n\n                # Orthogonal Crossover\n                candidates = self.orthogonal_crossover(mutant, population[i])\n                trial_vector = candidates[np.random.randint(len(candidates))] # Choose one candidate randomly.\n\n                # Evaluation\n                f_trial = func(trial_vector)\n                self.budget -= 1\n\n                # Selection\n                if f_trial < fitness[i]:\n                    # Update archive\n                    if f_trial < np.max(archive_fitness):\n                        worst_archive_idx = np.argmax(archive_fitness)\n                        archive[worst_archive_idx] = trial_vector\n                        archive_fitness[worst_archive_idx] = f_trial\n                    \n                    population[i] = trial_vector\n                    fitness[i] = f_trial\n\n                    # Self-adapt F and CR (simplified) using CMA-ES like adaptation\n                    self.F = np.clip(np.random.normal(0.5, 0.1), 0.1, 1.0)\n                    self.CR = np.clip(np.random.normal(0.7, 0.1), 0.1, 1.0)\n                    \n                    # Simplified Covariance Matrix Adaptation (CMA)\n                    d = trial_vector - population[i]  # Difference between new and old\n                    self.C = (1 - 0.1) * self.C + 0.1 * np.outer(d, d) # Update covariance matrix\n\n                # Update best\n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = trial_vector\n                    self.stagnation_counter = 0\n                else:\n                    self.stagnation_counter += 1\n\n                if self.budget <= 0:\n                    break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "The algorithm EnhancedAdaptiveDE scored 0.571 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["47de978c-c17f-4e30-afce-97497a2e0e34"], "operator": null, "metadata": {"aucs": [0.1946565175542666, 0.28148845565853187, 0.5373685958985872, 0.7861492179278281, 0.6422093584488037, 0.7149329231322268, 0.5111602938395938, 0.5276712293189116, 0.6273352214505734, 0.519572469035318, 0.7136076336321051, 0.9946025550077815, 0.3276303807169454, 0.5935829501244723, 0.7977373880777647, 0.7244811603575787, 0.42827722176014793, 0.7857650999359376, 0.20787414138966642, 0.4982239589125955]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "de461fd5-ae0f-4e57-a825-9525848e2f7b", "fitness": 0.3961981534586497, "name": "SelfOrganizingDE", "description": "A Differential Evolution variant with a self-organizing population structure based on fitness and dynamic landscape estimation.", "code": "import numpy as np\n\nclass SelfOrganizingDE:\n    def __init__(self, budget=10000, dim=10, initial_pop_size=100, min_pop_size=10, max_pop_size=200, learning_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.initial_pop_size = initial_pop_size\n        self.min_pop_size = min_pop_size\n        self.max_pop_size = max_pop_size\n        self.learning_rate = learning_rate  # For population size adjustment\n        self.population = None\n        self.fitness = None\n        self.F = 0.5\n        self.CR = 0.7\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.lb = -5.0\n        self.ub = 5.0\n\n\n    def __call__(self, func):\n        self.lb = func.bounds.lb\n        self.ub = func.bounds.ub\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.initial_pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.initial_pop_size\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n\n        while self.budget > 0:\n            # Sort population by fitness\n            sorted_indices = np.argsort(self.fitness)\n            self.population = self.population[sorted_indices]\n            self.fitness = self.fitness[sorted_indices]\n\n            # Dynamic population size adjustment\n            best_fitness = self.fitness[0]\n            worst_fitness = self.fitness[-1]\n\n            # Estimate landscape ruggedness. Higher value mean more rugged\n            ruggedness = abs(best_fitness - worst_fitness)\n\n            # Adjust population size based on ruggedness. Increase if rugged, decrease if smooth.\n            pop_size_change = int(self.learning_rate * (ruggedness - 0.5) * (self.max_pop_size - self.min_pop_size)) #Assuming fitness is between 0 and 1\n            new_pop_size = np.clip(len(self.population) + pop_size_change, self.min_pop_size, self.max_pop_size)\n\n            # Add or remove individuals\n            if new_pop_size > len(self.population):\n                num_to_add = new_pop_size - len(self.population)\n                new_individuals = np.random.uniform(self.lb, self.ub, size=(num_to_add, self.dim))\n                new_fitness = np.array([func(x) for x in new_individuals])\n                self.budget -= num_to_add\n                self.population = np.vstack((self.population, new_individuals))\n                self.fitness = np.concatenate((self.fitness, new_fitness))\n            elif new_pop_size < len(self.population):\n                num_to_remove = len(self.population) - new_pop_size\n                self.population = self.population[:-num_to_remove]\n                self.fitness = self.fitness[:-num_to_remove]\n            \n            current_pop_size = len(self.population)\n            for i in range(current_pop_size):\n                # Mutation\n                idxs = np.random.choice(current_pop_size, 3, replace=False)\n                x1, x2, x3 = self.population[idxs]\n                mutant = x1 + self.F * (x2 - x3)\n                mutant = np.clip(mutant, self.lb, self.ub)\n\n                # Crossover\n                trial_vector = np.where(np.random.rand(self.dim) < self.CR, mutant, self.population[i])\n                trial_vector = np.clip(trial_vector, self.lb, self.ub)\n\n                # Evaluation\n                f_trial = func(trial_vector)\n                self.budget -= 1\n\n                # Selection\n                if f_trial < self.fitness[i]:\n                    self.population[i] = trial_vector\n                    self.fitness[i] = f_trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial_vector\n                if self.budget <= 0:\n                    break\n\n            if self.budget <= 0:\n                break\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "The algorithm SelfOrganizingDE scored 0.396 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["cac9ad3f-6920-4233-83d8-1cdb3d0ff09f"], "operator": null, "metadata": {"aucs": [0.15936612482119983, 0.2233875670111306, 0.39985100244932215, 0.40096396443098414, 0.3030949794539921, 0.4347105913318202, 0.3730079594040663, 0.34497789398142886, 0.29610293051834646, 0.21431100610522213, 0.33392272738589623, 0.9887941641858998, 0.2488105791702564, 0.32351416343641626, 0.7462787231359611, 0.563219752075328, 0.322531694967375, 0.5535795319777694, 0.18698851378793768, 0.5065491995426417]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "9d74f643-a6a0-4948-ac70-72ab41bb1049", "fitness": 0.6819631406962364, "name": "MirroredDESearch", "description": "A Differential Evolution variant that uses a mirrored sampling strategy for diversity and a decaying local search radius to refine promising solutions.", "code": "import numpy as np\n\nclass MirroredDESearch:\n    def __init__(self, budget=10000, dim=10, pop_size=40, local_search_prob=0.1, initial_radius=0.5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lb = -5.0\n        self.ub = 5.0\n        self.F = 0.5\n        self.CR = 0.7\n        self.local_search_prob = local_search_prob\n        self.initial_radius = initial_radius\n        self.radius = initial_radius  # Decaying radius for local search\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n\n        # Initialize population\n        population = np.random.uniform(lb, ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)]\n\n        generation = 0\n        while self.budget > 0:\n            generation += 1\n            # Adjust radius over time\n            self.radius = self.initial_radius * (1 - generation / (self.budget/self.pop_size + 1)) \n            \n            for i in range(self.pop_size):\n                # Mutation with mirrored sampling\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x1, x2, x3 = population[idxs]\n                mutant = x1 + self.F * (x2 - x3)\n\n                # Mirrored Boundary Handling: reflect out-of-bounds values\n                mutant = np.where(mutant < lb, 2*lb - mutant, mutant)\n                mutant = np.where(mutant > ub, 2*ub - mutant, mutant)\n                \n\n                # Crossover\n                trial_vector = np.where(np.random.rand(self.dim) < self.CR, mutant, population[i])\n\n                # Evaluation\n                f_trial = func(trial_vector)\n                self.budget -= 1\n\n                # Selection\n                if f_trial < fitness[i]:\n                    population[i] = trial_vector\n                    fitness[i] = f_trial\n\n                    # Self-adapt F and CR (simplified)\n                    self.F = np.clip(np.random.normal(0.5, 0.1), 0.1, 1.0)\n                    self.CR = np.clip(np.random.normal(0.7, 0.1), 0.1, 1.0)\n\n                # Update best\n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = trial_vector\n\n                    # Trigger local search with probability\n                    if np.random.rand() < self.local_search_prob:\n                        self.x_opt, self.f_opt = self.local_search(func, self.x_opt, self.f_opt, radius=self.radius)\n\n                if self.budget <= 0:\n                    break\n\n        return self.f_opt, self.x_opt\n\n    def local_search(self, func, x_opt, f_opt, radius, num_steps=5):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        for _ in range(num_steps):\n            # Generate a random direction\n            direction = np.random.randn(self.dim)\n            direction /= np.linalg.norm(direction)\n\n            # Generate a random step size\n            step_size = np.random.uniform(-radius, radius)\n\n            # Create a new candidate solution\n            x_new = x_opt + step_size * direction\n            x_new = np.clip(x_new, lb, ub)\n\n            # Evaluate the new solution\n            f_new = func(x_new)\n            self.budget -= 1\n\n            # Update the best solution if necessary\n            if f_new < f_opt:\n                f_opt = f_new\n                x_opt = x_new\n\n            if self.budget <= 0:\n                break\n\n        return x_opt, f_opt", "configspace": "", "generation": 5, "feedback": "The algorithm MirroredDESearch scored 0.682 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["077d7b9d-ba51-4b31-9952-a874a7fb2c41"], "operator": null, "metadata": {"aucs": [0.2214413715697846, 0.5003077908070128, 0.6669471957216389, 0.8877951930303265, 0.7360661551003999, 0.7952430484402486, 0.6683146152615435, 0.6712679513454833, 0.7671842335502432, 0.6256622290963122, 0.8433638431959991, 0.9946097821786294, 0.33669269186359396, 0.6844770182458504, 0.88237075937037, 0.8118552516184493, 0.5729091197305342, 0.8584883300724686, 0.49535753339141797, 0.6189087003344205]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "e55ddf4c-8613-4d6d-9a7d-189a0e2baabd", "fitness": 0.10456169362994024, "name": "BudgetAwareCMAES", "description": "Covariance matrix adaptation evolution strategy (CMA-ES) with a simplified parameter adaptation rule and budget-aware population sizing.", "code": "import numpy as np\n\nclass BudgetAwareCMAES:\n    def __init__(self, budget=10000, dim=10, initial_sigma=0.2, pop_size_init=None):\n        self.budget = budget\n        self.dim = dim\n        self.sigma = initial_sigma  # Step size\n        self.mean = None\n        self.C = None  # Covariance matrix\n        self.pc = None  # Evolution path for C\n        self.ps = None  # Evolution path for sigma\n        self.chiN = None\n        self.pop_size_init = pop_size_init if pop_size_init is not None else 4 + int(3 * np.log(self.dim))\n        self.pop_size = self.pop_size_init\n        self.c_sigma = None\n        self.d_sigma = None\n        self.c_c = None\n        self.c_1 = None\n        self.c_mu = None\n        self.mu = None\n        self.weights = None\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def initialize(self):\n        self.mean = np.random.uniform(-5.0, 5.0, size=self.dim)  # Initialize mean within bounds\n        self.C = np.eye(self.dim)  # Initialize covariance matrix\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n\n        self.mu = self.pop_size // 2 #Number of parents/selected individuals\n        self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n\n        self.c_sigma = (self.mu / self.dim) / 4\n        self.d_sigma = 1 + 2 * max(0, np.sqrt((self.mu - 1)/(self.dim + 1)) - 1) + self.c_sigma\n\n        self.c_c = (4 + self.mu/self.dim) / (self.dim + 4 + 2*self.mu/self.dim)\n\n        self.c_1 = 2 / ((self.dim + 1.3)**2 + self.mu)\n        self.c_mu = min(1 - self.c_1, 2 * (self.mu - 2 + 1/self.mu) / ((self.dim + 2)**2 + self.mu))\n\n        self.chiN = self.dim**0.5 * (1 - (1/(4*self.dim)) + 1/(21*self.dim**2))\n\n    def __call__(self, func):\n        self.initialize()\n        generation = 0\n        while self.budget > 0:\n            generation += 1\n            # Generate population\n            z = np.random.normal(0, 1, size=(self.pop_size, self.dim))\n            population = self.mean + self.sigma * np.dot(z, np.linalg.cholesky(self.C).T)\n            population = np.clip(population, func.bounds.lb, func.bounds.ub)  # Clip to bounds\n\n            # Evaluate population\n            fitness = np.array([func(x) for x in population])\n            self.budget -= self.pop_size\n\n            # Sort by fitness\n            idx_sorted = np.argsort(fitness)\n            fitness = fitness[idx_sorted]\n            population = population[idx_sorted]\n\n            #Update best solution found so far\n            if fitness[0] < self.f_opt:\n                self.f_opt = fitness[0]\n                self.x_opt = population[0]\n\n            # Selection and recombination\n            delta = population[:self.mu] - self.mean\n            weighted_delta = self.weights.reshape(-1, 1) * delta\n            mean_change = np.sum(weighted_delta, axis=0)\n\n            # Update evolution paths\n            self.ps = (1 - self.c_sigma) * self.ps + np.sqrt(self.c_sigma * (2 - self.c_sigma) * self.mu) * (np.linalg.solve(np.sqrt(self.C), mean_change.T))\n\n            norm_ps = np.linalg.norm(self.ps)\n            factor = self.chiN * np.sqrt(1 - (1 - self.c_sigma)**(2*generation))\n            if (norm_ps / factor) < (1.0 + 0.2):\n                self.pc = (1 - self.c_c) * self.pc + np.sqrt(self.c_c * (2 - self.c_c) * self.mu) * mean_change\n            else:\n                self.pc = (1 - self.c_c) * self.pc\n\n\n            # Update covariance matrix\n            self.C = (1 - self.c_1 - self.c_mu) * self.C + self.c_1 * np.outer(self.pc, self.pc)\n            self.C += self.c_mu * np.sum(self.weights.reshape(-1, 1, 1) * np.array([np.outer(delta[i], delta[i]) for i in range(self.mu)]), axis=0)\n\n            # Update step size\n            self.sigma *= np.exp((self.c_sigma / self.d_sigma) * (norm_ps/self.chiN - 1))\n            self.sigma = np.clip(self.sigma, 1e-10, 5)\n\n            # Update mean\n            self.mean += mean_change\n\n            #Budget aware population sizing\n            if self.budget / (self.pop_size * 2) < 1:\n                 self.pop_size = max(4, int(self.budget / 2))\n            else:\n                self.pop_size = self.pop_size_init\n            if self.pop_size < 4:\n                 break\n\n            if self.budget <= 0:\n                break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "The algorithm BudgetAwareCMAES scored 0.105 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["925978c5-5892-4bfc-9af4-79922d2a6118"], "operator": null, "metadata": {"aucs": [0.024258242027540455, 0.07653165181582122, 0.19127614400340787, 0.09881328811427992, 0.10874440398075003, 0.14305466978351256, 0.038441136203340776, 0.10237558989363182, 0.13545426716086473, 0.0787013454147878, 0.10488470662286742, 0.15140352937371715, 0.15756730742994496, 0.11865138063782565, 0.08553678328560144, 0.1323273452650776, 0.030886244345180836, 0.13399320118731195, 0.08049486244972248, 0.09783777360361778]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "5bb66d26-ed91-4c01-a9ab-40c1674c846d", "fitness": 0.6044084593499118, "name": "AdaptiveDE", "description": "Self-adaptive Differential Evolution with toroidal boundary handling, population diversity maintenance, and a simplified aging mechanism.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size_init=50, stagnation_threshold=1000, archive_size=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size_init\n        self.F = 0.5  # Initial value for differential weight\n        self.CR = 0.7  # Initial value for crossover rate\n        self.stagnation_threshold = stagnation_threshold\n        self.archive_size = archive_size\n        self.archive = []\n        self.archive_fitness = []\n        self.best_fitness_history = []\n\n    def toroidal_boundary_handling(self, x, lb, ub):\n        \"\"\"Handles boundary violations using toroidal wrapping.\"\"\"\n        width = ub - lb\n        x_wrapped = lb + (x - lb) % width\n        return x_wrapped\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.generation = 0\n\n        # Initialize population\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        population = np.random.uniform(lb, ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        # Main loop\n        while self.budget > 0:\n            self.generation += 1\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x1, x2, x3 = population[idxs]\n                mutant = x1 + self.F * (x2 - x3)\n                mutant = self.toroidal_boundary_handling(mutant, lb, ub) # Toroidal boundary handling\n\n                # Crossover\n                crossover_mask = np.random.rand(self.dim) < self.CR\n                trial_vector = np.where(crossover_mask, mutant, population[i])\n\n                # Evaluation\n                f_trial = func(trial_vector)\n                self.budget -= 1\n\n                # Selection\n                if f_trial < fitness[i]:\n                    # Update individual\n                    population[i] = trial_vector\n                    fitness[i] = f_trial\n\n                    # Adapt F and CR based on success (simplified)\n                    self.F = np.clip(np.random.normal(0.5, 0.1), 0.1, 1.0)\n                    self.CR = np.clip(np.random.normal(0.7, 0.1), 0.1, 1.0)\n\n                    # Update best\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial_vector\n\n                    # Update Archive\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(trial_vector)\n                        self.archive_fitness.append(f_trial)\n                    else:\n                        # Replace the worst member in the archive\n                        max_archive_index = np.argmax(self.archive_fitness)\n                        if f_trial < self.archive_fitness[max_archive_index]:\n                            self.archive[max_archive_index] = trial_vector\n                            self.archive_fitness[max_archive_index] = f_trial\n                else:\n                    # If the trial vector is not better, consider adding the original population member\n                    # to the archive to preserve diversity\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(population[i])\n                        self.archive_fitness.append(fitness[i])\n                    else:\n                        # Replace the worst member in the archive\n                        max_archive_index = np.argmax(self.archive_fitness)\n                        if fitness[i] < self.archive_fitness[max_archive_index]:\n                            self.archive[max_archive_index] = population[i]\n                            self.archive_fitness[max_archive_index] = fitness[i]\n\n\n                if self.budget <= 0:\n                    break\n\n            # Simplified Aging mechanism: Reduce population if there's no improvement\n            if self.generation % 20 == 0:\n                 if len(self.best_fitness_history) > 20:\n                     if np.std(self.best_fitness_history[-20:]) < 1e-6:\n                         self.pop_size = int(self.pop_size * 0.9)\n                         if self.pop_size < 10:\n                             self.pop_size = 20 # Minimal pop size\n                             population = np.random.uniform(lb, ub, size=(self.pop_size, self.dim))\n                             fitness = np.array([func(x) for x in population])\n                             self.budget -= self.pop_size\n                         else:\n                             population = population[:self.pop_size]\n                             fitness = fitness[:self.pop_size]\n\n            self.best_fitness_history.append(self.f_opt)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "The algorithm AdaptiveDE scored 0.604 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["925978c5-5892-4bfc-9af4-79922d2a6118"], "operator": null, "metadata": {"aucs": [0.21186858276790121, 0.39959692942084435, 0.5864281758336418, 0.8423285606970283, 0.6830910168036686, 0.7463319715846108, 0.5419337143894283, 0.5738065658110297, 0.6789707638970319, 0.408978239740028, 0.8246568349927592, 0.9919180208329671, 0.32924226081520724, 0.5919203914945697, 0.8667887180820022, 0.7435888738184963, 0.5341742611766853, 0.787922029914481, 0.20695527925095436, 0.5376679956748995]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "3ea7d5c0-e1c8-40d8-8030-e2b68b030087", "fitness": 0.3975265272831285, "name": "EnhancedAdaptiveDE", "description": "Enhanced Adaptive Differential Evolution with orthogonal crossover, archive-based learning, and dynamic parameter control for improved exploration and exploitation.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.F = 0.5  # Initial value for differential weight\n        self.CR = 0.7 # Initial value for crossover rate\n        self.p_archive = 0.1  # Probability of using archive member\n        self.F_memory = np.full(self.pop_size, 0.5)\n        self.CR_memory = np.full(self.pop_size, 0.7)\n        self.sf = np.full(self.pop_size, 0.0)\n        self.scr = np.full(self.pop_size, 0.0)\n        self.memory_size = 10\n        self.sf_memory = np.full(self.memory_size, 0.5)\n        self.scr_memory = np.full(self.memory_size, 0.7)\n        self.memory_idx = 0\n\n    def orthogonal_crossover(self, mutant, target_vector):\n        # Simple orthogonal design-based crossover\n        n = self.dim\n        levels = 2  # Levels for each factor (dimension)\n        design_matrix = np.array([[0, 0], [0, 1], [1, 0], [1, 1]]) # Example L4 orthogonal array\n        trial_vector = np.copy(target_vector)\n        \n        # Select a random row from the design matrix\n        design_row_idx = np.random.randint(0, len(design_matrix))\n        design_row = design_matrix[design_row_idx]\n\n        # Randomly select two dimensions to vary based on orthogonal design\n        dims_to_vary = np.random.choice(n, 2, replace=False)\n        \n        # Apply orthogonal crossover for the selected dimensions\n        trial_vector[dims_to_vary[0]] = mutant[dims_to_vary[0]] if design_row[0] == 1 else target_vector[dims_to_vary[0]]\n        trial_vector[dims_to_vary[1]] = mutant[dims_to_vary[1]] if design_row[1] == 1 else target_vector[dims_to_vary[1]]\n\n        return trial_vector\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        # Initialize archive\n        archive = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.archive_size, self.dim))\n        archive_fitness = np.full(self.archive_size, np.inf)\n\n        # Main loop\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x1, x2, x3 = population[idxs]\n\n                # Add a random vector from archive\n                if np.random.rand() < self.p_archive: # Probability of using archive member\n                    idx_archive = np.random.choice(self.archive_size)\n                    mutant = x1 + self.F_memory[i] * (x2 - x3) + self.F_memory[i] * (archive[idx_archive] - x1)  # Mutation with archive\n\n                else:\n                    mutant = x1 + self.F_memory[i] * (x2 - x3) # Standard DE mutation\n\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub) # Clip to boundaries\n\n\n                # Crossover (Orthogonal Crossover)\n                trial_vector = self.orthogonal_crossover(mutant, population[i])\n\n                # Evaluation\n                f_trial = func(trial_vector)\n                self.budget -= 1\n\n                # Selection\n                if f_trial < fitness[i]:\n                    # Update archive\n                    if f_trial < np.max(archive_fitness):\n                        worst_archive_idx = np.argmax(archive_fitness)\n                        archive[worst_archive_idx] = trial_vector\n                        archive_fitness[worst_archive_idx] = f_trial\n                    \n                    population[i] = trial_vector\n                    fitness[i] = f_trial\n                    self.sf[i] = self.F_memory[i]\n                    self.scr[i] = self.CR_memory[i]\n                else:\n                    self.sf[i] = 0.0\n                    self.scr[i] = 0.0\n                \n\n                # Update best\n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = trial_vector\n\n                if self.budget <= 0:\n                    break\n            \n            #Adapt F and CR\n            successful_F = self.sf[self.sf > 0]\n            successful_CR = self.scr[self.scr > 0]\n\n            if len(successful_F) > 0:\n                self.sf_memory[self.memory_idx] = np.mean(successful_F)\n            if len(successful_CR) > 0:\n                self.scr_memory[self.memory_idx] = np.mean(successful_CR)\n            \n            self.memory_idx = (self.memory_idx + 1) % self.memory_size\n            \n            for i in range(self.pop_size):\n                self.F_memory[i] = np.clip(np.random.normal(self.sf_memory.mean(), 0.1), 0.1, 1.0)\n                self.CR_memory[i] = np.clip(np.random.normal(self.scr_memory.mean(), 0.1), 0.1, 1.0)\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "The algorithm EnhancedAdaptiveDE scored 0.398 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["47de978c-c17f-4e30-afce-97497a2e0e34"], "operator": null, "metadata": {"aucs": [0.14694267801830596, 0.22909030060328173, 0.411021609248464, 0.3395986291627717, 0.3251406495991571, 0.5210621868567378, 0.32562761236833027, 0.38810742171946466, 0.3585637052925328, 0.2605670267838386, 0.2796733416583894, 0.9996701312129258, 0.23210921332800227, 0.30031401589692786, 0.744009566299322, 0.5467199638393787, 0.32162250511093027, 0.5245849167589384, 0.19679841290847166, 0.49930665899639937]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "55032471-a6c4-4515-8c00-5eae283f772a", "fitness": 0.19513887535030058, "name": "AdaptiveDEToroidalRestart", "description": "Self-adaptive Differential Evolution with simplified parameter control, toroidal boundary handling, and a probabilistic restart mechanism to escape local optima, focusing on exploration and exploitation balance.", "code": "import numpy as np\n\nclass AdaptiveDEToroidalRestart:\n    def __init__(self, budget=10000, dim=10, initial_pop_size=40, restart_prob=0.05):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = initial_pop_size\n        self.lb = -5.0\n        self.ub = 5.0\n        self.F = 0.5\n        self.CR = 0.7\n        self.restart_prob = restart_prob\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.stagnation_counter = 0\n        self.max_stagnation = 50\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        # Initialize population\n        population = np.random.uniform(lb, ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)]\n        best_fitness_history = [self.f_opt]\n\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x1, x2, x3 = population[idxs]\n                mutant = x1 + self.F * (x2 - x3)\n\n                # Toroidal Boundary Handling\n                mutant = np.mod(mutant - lb, ub - lb) + lb\n\n                # Crossover\n                trial_vector = np.where(np.random.rand(self.dim) < self.CR, mutant, population[i])\n\n                # Evaluation\n                f_trial = func(trial_vector)\n                self.budget -= 1\n\n                # Selection\n                if f_trial < fitness[i]:\n                    population[i] = trial_vector\n                    fitness[i] = f_trial\n\n                    # Simplified Self-adaptation\n                    self.F = np.clip(np.random.normal(self.F, 0.1), 0.1, 1.0)\n                    self.CR = np.clip(np.random.normal(self.CR, 0.1), 0.1, 1.0)\n\n                # Update best\n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = trial_vector\n                    self.stagnation_counter = 0\n                else:\n                    self.stagnation_counter += 1\n\n                if self.budget <= 0:\n                    break\n\n            # Stagnation Check and Restart\n            if self.stagnation_counter > self.max_stagnation or (np.random.rand() < self.restart_prob):\n                population = np.random.uniform(lb, ub, size=(self.pop_size, self.dim))\n                fitness = np.array([func(x) for x in population])\n                self.budget -= self.pop_size\n                self.f_opt = np.min(fitness)\n                self.x_opt = population[np.argmin(fitness)]\n                self.stagnation_counter = 0\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "The algorithm AdaptiveDEToroidalRestart scored 0.195 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["077d7b9d-ba51-4b31-9952-a874a7fb2c41"], "operator": null, "metadata": {"aucs": [0.13139636503861496, 0.17573464133229644, 0.2805434837045395, 0.24454906858839887, 0.20591762983429762, 0.2546353215093522, 0.24393041542685, 0.21954295271835567, 0]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "f225fd99-47e3-472c-87dd-37efb2483123", "fitness": 0.669092941426343, "name": "StochasticRankingDE", "description": "A Differential Evolution strategy employing a stochastic ranking selection and covariance matrix adaptation to guide the search.", "code": "import numpy as np\n\nclass StochasticRankingDE:\n    def __init__(self, budget=10000, dim=10, pop_size=40, p_f=0.45, initial_F=0.5, initial_CR=0.7, local_search_prob=0.1, initial_radius=0.5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lb = -5.0\n        self.ub = 5.0\n        self.F = initial_F\n        self.CR = initial_CR\n        self.local_search_prob = local_search_prob\n        self.initial_radius = initial_radius\n        self.radius = initial_radius  # Decaying radius for local search\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.p_f = p_f # Probability of constraint violation comparison\n\n        self.archive = []  # Archive for covariance matrix adaptation\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n\n        # Initialize population\n        population = np.random.uniform(lb, ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)]\n\n        generation = 0\n        while self.budget > 0:\n            generation += 1\n            # Adjust radius over time\n            self.radius = self.initial_radius * (1 - generation / (self.budget/self.pop_size + 1))\n\n            # CMA-ES like adaptation (simplified)\n            if len(self.archive) > self.dim + 1:\n                self.archive = self.archive[-self.dim-1:] #keep last ones\n                try:\n                    covariance_matrix = np.cov(np.array(self.archive).T)\n                except:\n                    covariance_matrix = np.eye(self.dim) * 0.01  # small identity if archive is degenerate\n\n                # Ensure positive definiteness (add small amount to diagonal)\n                covariance_matrix += np.eye(self.dim) * 1e-6\n            else:\n                covariance_matrix = np.eye(self.dim) * 0.01 # start small\n\n            for i in range(self.pop_size):\n                # Mutation with mirrored sampling\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x1, x2, x3 = population[idxs]\n                mutant = x1 + self.F * (x2 - x3)\n\n                # Mirrored Boundary Handling: reflect out-of-bounds values\n                mutant = np.where(mutant < lb, 2*lb - mutant, mutant)\n                mutant = np.where(mutant > ub, 2*ub - mutant, mutant)\n\n                # Crossover\n                trial_vector = np.where(np.random.rand(self.dim) < self.CR, mutant, population[i])\n\n                # Evaluation\n                f_trial = func(trial_vector)\n                self.budget -= 1\n\n                # Stochastic Ranking Selection\n                if np.random.rand() < self.p_f or (fitness[i] < 0 and f_trial < 0):  # Assuming smaller value is better\n                    if f_trial < fitness[i]:\n                        population[i] = trial_vector\n                        fitness[i] = f_trial\n\n                        # Add to archive if better\n                        self.archive.append(trial_vector)\n                else:\n                    if f_trial < fitness[i]:\n                        population[i] = trial_vector\n                        fitness[i] = f_trial\n                        # Add to archive if better\n                        self.archive.append(trial_vector)\n\n\n                # Update best\n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = trial_vector\n                    # Trigger local search with probability\n                    if np.random.rand() < self.local_search_prob:\n                        self.x_opt, self.f_opt = self.local_search(func, self.x_opt, self.f_opt, radius=self.radius, covariance_matrix = covariance_matrix)\n\n                if self.budget <= 0:\n                    break\n\n        return self.f_opt, self.x_opt\n\n    def local_search(self, func, x_opt, f_opt, radius, covariance_matrix, num_steps=5):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        for _ in range(num_steps):\n            # Generate a random direction using covariance matrix\n            try:\n                direction = np.random.multivariate_normal(np.zeros(self.dim), covariance_matrix)\n            except:\n                direction = np.random.randn(self.dim)\n\n            direction /= np.linalg.norm(direction)\n\n            # Generate a random step size\n            step_size = np.random.uniform(-radius, radius)\n\n            # Create a new candidate solution\n            x_new = x_opt + step_size * direction\n            x_new = np.clip(x_new, lb, ub)\n\n            # Evaluate the new solution\n            f_new = func(x_new)\n            self.budget -= 1\n\n            # Update the best solution if necessary\n            if f_new < f_opt:\n                f_opt = f_new\n                x_opt = x_new\n\n            if self.budget <= 0:\n                break\n\n        return x_opt, f_opt", "configspace": "", "generation": 6, "feedback": "The algorithm StochasticRankingDE scored 0.669 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["9d74f643-a6a0-4948-ac70-72ab41bb1049"], "operator": null, "metadata": {"aucs": [0.2510315908949583, 0.5050759230177296, 0.6751809551952861, 0.872806722988654, 0.7755785878107238, 0.7945022284885964, 0.6428742426322066, 0.6467130176199691, 0.7675396992782417, 0.6353344908007132, 0.8293777641487758, 0.9922959180387539, 0.409940256715954, 0.7099048886035366, 0.9073788961411922, 0.7929295031790913, 0.5810312860637541, 0.8509753498222472, 0.221548670125149, 0.5198388369613292]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "7125b985-82ff-4ee6-bb3b-4cd75d4c3fa8", "fitness": "-inf", "name": "AdaptiveDELevyRestart", "description": "Adaptive Differential Evolution with a dynamically adjusted population size based on success rate, enhanced exploration using Lvy flights, and periodic restarts for escaping local optima.", "code": "import numpy as np\n\nclass AdaptiveDELevyRestart:\n    def __init__(self, budget=10000, dim=10, initial_pop_size=50, archive_size=10, restart_interval=2000):\n        self.budget = budget\n        self.dim = dim\n        self.initial_pop_size = initial_pop_size\n        self.pop_size = initial_pop_size  # Start with initial size\n        self.archive_size = archive_size\n        self.F = 0.5\n        self.CR = 0.7\n        self.restart_interval = restart_interval\n        self.evals_since_restart = 0\n\n    def levy_flight(self, size, beta=1.5):\n        sigma = (np.math.gamma(1 + beta) * np.sin(np.pi * beta / 2) / (np.math.gamma((1 + beta) / 2) * beta * (2 ** ((beta - 1) / 2)))) ** (1 / beta)\n        u = np.random.normal(0, sigma, size=size)\n        v = np.random.normal(0, 1, size=size)\n        step = u / abs(v) ** (1 / beta)\n        return step\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n        self.evals_since_restart += self.pop_size\n\n        # Initialize archive\n        archive = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.archive_size, self.dim))\n        archive_fitness = np.full(self.archive_size, np.inf)\n\n        success_count = 0  # Track successful generations\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x1, x2, x3 = population[idxs]\n\n                # Levy flight for enhanced exploration\n                levy_steps = self.levy_flight(self.dim)\n                mutant = x1 + self.F * (x2 - x3) + 0.01 * levy_steps * (func.bounds.ub - func.bounds.lb) # Scale levy flights\n\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                crossover_mask = np.random.rand(self.dim) < self.CR\n                trial_vector = np.where(crossover_mask, mutant, population[i])\n\n                # Evaluation\n                f_trial = func(trial_vector)\n                self.budget -= 1\n                self.evals_since_restart += 1\n\n                # Selection\n                if f_trial < fitness[i]:\n                    # Update archive\n                    if f_trial < np.max(archive_fitness):\n                        worst_archive_idx = np.argmax(archive_fitness)\n                        archive[worst_archive_idx] = trial_vector\n                        archive_fitness[worst_archive_idx] = f_trial\n\n                    population[i] = trial_vector\n                    fitness[i] = f_trial\n                    success_count += 1\n\n                    # Self-adapt F and CR (simplified)\n                    self.F = np.clip(np.random.normal(0.5, 0.1), 0.1, 1.0)\n                    self.CR = np.clip(np.random.normal(0.7, 0.1), 0.1, 1.0)\n\n                # Update best\n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = trial_vector\n\n                if self.budget <= 0:\n                    break\n\n            # Adjust population size based on success\n            success_rate = success_count / self.pop_size\n            if success_rate > 0.4 and self.pop_size < 100: # Dynamic population sizing\n                self.pop_size = min(self.pop_size + 5, 100)\n                population = np.vstack((population, np.random.uniform(func.bounds.lb, func.bounds.ub, size=(5, self.dim))))\n                new_fitness = np.array([func(x) for x in population[-5:]])\n                fitness = np.concatenate((fitness, new_fitness))\n                self.budget -= 5\n                self.evals_since_restart += 5\n\n            elif success_rate < 0.1 and self.pop_size > 20:\n                self.pop_size = max(self.pop_size - 5, 20)\n                population = population[:self.pop_size]\n                fitness = fitness[:self.pop_size]\n\n            success_count = 0 # Reset\n\n            # Restart mechanism\n            if self.evals_since_restart > self.restart_interval:\n                population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                fitness = np.array([func(x) for x in population])\n                self.budget -= self.pop_size\n                self.evals_since_restart = self.pop_size # Reset counter after initialization\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "In the code, line 805, in __getattr__, the following error occurred:\nAttributeError: module 'numpy' has no attribute 'math'. Did you mean: 'emath'?", "error": "In the code, line 805, in __getattr__, the following error occurred:\nAttributeError: module 'numpy' has no attribute 'math'. Did you mean: 'emath'?", "parent_ids": ["47de978c-c17f-4e30-afce-97497a2e0e34"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "0a8d8882-2a42-45d2-94bb-433021a249d0", "fitness": 0.554535663057626, "name": "SuccessHistoryAdaptiveDE", "description": "A Differential Evolution strategy with a population-weighted mutation and adaptive parameter control using success history, enhanced by a restart mechanism for stagnation avoidance.", "code": "import numpy as np\n\nclass SuccessHistoryAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=10, memory_size=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.memory_size = memory_size\n        self.F = 0.5  # Initial value for differential weight\n        self.CR = 0.7 # Initial value for crossover rate\n        self.F_memory = np.full(self.memory_size, self.F)\n        self.CR_memory = np.full(self.memory_size, self.CR)\n        self.fitness_memory = np.zeros(self.memory_size)\n        self.memory_idx = 0\n        self.stagnation_counter = 0\n        self.stagnation_threshold = 50  # Number of iterations without improvement before restart\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        # Initialize archive\n        archive = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.archive_size, self.dim))\n        archive_fitness = np.full(self.archive_size, np.inf)\n\n        # Main loop\n        while self.budget > 0:\n            best_fitness = np.min(fitness)\n            if best_fitness >= self.f_opt:\n                self.stagnation_counter += 1\n            else:\n                self.stagnation_counter = 0\n                self.f_opt = best_fitness\n                self.x_opt = population[np.argmin(fitness)]\n\n            if self.stagnation_counter > self.stagnation_threshold:\n                # Restart mechanism\n                population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                fitness = np.array([func(x) for x in population])\n                self.budget -= self.pop_size\n                self.stagnation_counter = 0\n                archive = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.archive_size, self.dim))\n                archive_fitness = np.full(self.archive_size, np.inf)\n\n            weights = np.exp(-fitness / np.mean(fitness))\n            weights /= np.sum(weights)\n            \n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False, p=weights)  # Population weighted sampling\n                x1, x2, x3 = population[idxs]\n\n                # Add a random vector from archive\n                if np.random.rand() < 0.1: # Probability of 0.1 to use archive member\n                    idx_archive = np.random.choice(self.archive_size)\n                    mutant = x1 + self.F * (x2 - x3) + self.F * (archive[idx_archive] - x1)  # Mutation with archive\n\n                else:\n                    mutant = x1 + self.F * (x2 - x3) # Standard DE mutation\n\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub) # Clip to boundaries\n\n                # Crossover\n                crossover_mask = np.random.rand(self.dim) < self.CR\n                trial_vector = np.where(crossover_mask, mutant, population[i])\n\n                # Evaluation\n                f_trial = func(trial_vector)\n                self.budget -= 1\n\n                # Selection\n                if f_trial < fitness[i]:\n                    # Update archive\n                    if f_trial < np.max(archive_fitness):\n                        worst_archive_idx = np.argmax(archive_fitness)\n                        archive[worst_archive_idx] = trial_vector\n                        archive_fitness[worst_archive_idx] = f_trial\n\n                    # Update memory\n                    self.F_memory[self.memory_idx] = self.F\n                    self.CR_memory[self.memory_idx] = self.CR\n                    self.fitness_memory[self.memory_idx] = fitness[i] - f_trial # Fitness improvement\n                    self.memory_idx = (self.memory_idx + 1) % self.memory_size\n                    \n                    population[i] = trial_vector\n                    fitness[i] = f_trial\n\n                    # Self-adapt F and CR using success history\n                    positive_indices = self.fitness_memory > 0\n                    if np.any(positive_indices):\n                        F_success = self.F_memory[positive_indices]\n                        CR_success = self.CR_memory[positive_indices]\n                        self.F = np.clip(np.random.choice(F_success), 0.1, 1.0) if len(F_success) > 0 else np.clip(np.random.normal(0.5, 0.1), 0.1, 1.0)\n                        self.CR = np.clip(np.random.choice(CR_success), 0.1, 1.0) if len(CR_success) > 0 else np.clip(np.random.normal(0.7, 0.1), 0.1, 1.0)\n\n\n                # Update best\n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = trial_vector\n\n                if self.budget <= 0:\n                    break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "The algorithm SuccessHistoryAdaptiveDE scored 0.555 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["47de978c-c17f-4e30-afce-97497a2e0e34"], "operator": null, "metadata": {"aucs": [0.19335867567583953, 0.369468491873289, 0.4939833683701127, 0.8868880486396993, 0.5398573403047415, 0.6597468660246806, 0.3582158877231738, 0.48790380158509694, 0.554004833889767, 0.3190437784673713, 0.8885322314594454, 0.9908143423159715, 0.3151472259954965, 0.54582453758432, 0.8824484239522349, 0.6459192945756607, 0.4168126554363384, 0.8255756365038504, 0.21318585879410046, 0.5039819619813304]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "35a10794-13f6-4d75-a69c-fdcf0965f7bf", "fitness": "-inf", "name": "EnhancedOrthogonalDE", "description": "Enhanced Orthogonal DE with adaptive population sizing, dynamic orthogonal array scaling, and a restart mechanism based on stagnation detection to improve exploration and exploitation.", "code": "import numpy as np\n\nclass EnhancedOrthogonalDE:\n    def __init__(self, budget=10000, dim=10, pop_size_initial=50, archive_size=10, success_history_size=10, stagnation_threshold=1000):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size_initial = pop_size_initial\n        self.pop_size = pop_size_initial\n        self.archive_size = archive_size\n        self.F = 0.5\n        self.CR = 0.7\n        self.success_history_size = success_history_size\n        self.success_F = np.full(self.success_history_size, 0.5)\n        self.success_CR = np.full(self.success_history_size, 0.7)\n        self.success_fitness_reduction = np.zeros(self.success_history_size)\n        self.success_idx = 0\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.stagnation_threshold = stagnation_threshold\n        self.stagnation_counter = 0\n        self.last_improvement = 0\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n\n        # Initialize population\n        population = np.random.uniform(lb, ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        # Initialize archive\n        archive = np.random.uniform(lb, ub, size=(self.archive_size, self.dim))\n        archive_fitness = np.full(self.archive_size, np.inf)\n\n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)]\n        self.last_improvement = 0\n\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x1, x2, x3 = population[idxs]\n\n                mutant = x1 + self.F * (x2 - x3)\n                mutant = np.clip(mutant, lb, ub)\n\n\n                # Crossover\n                trial_vector = np.where(np.random.rand(self.dim) < self.CR, mutant, population[i])\n                trial_vector = np.clip(trial_vector, lb, ub)\n\n\n                # Evaluation\n                f_trial = func(trial_vector)\n                self.budget -= 1\n                \n                # Selection\n                if f_trial < fitness[i]:\n                    # Update archive\n                    if f_trial < np.max(archive_fitness):\n                        worst_archive_idx = np.argmax(archive_fitness)\n                        archive[worst_archive_idx] = trial_vector\n                        archive_fitness[worst_archive_idx] = f_trial\n\n                    # Store successful F and CR values\n                    self.success_F[self.success_idx] = self.F\n                    self.success_CR[self.success_idx] = self.CR\n                    self.success_fitness_reduction[self.success_idx] = fitness[i] - f_trial\n                    self.success_idx = (self.success_idx + 1) % self.success_history_size\n                    \n                    population[i] = trial_vector\n                    fitness[i] = f_trial\n\n                    # Update F and CR based on success history (weighted average)\n                    weights = self.success_fitness_reduction / (np.sum(self.success_fitness_reduction) + 1e-9)\n                    self.F = np.clip(np.sum(weights * self.success_F), 0.1, 1.0)\n                    self.CR = np.clip(np.sum(weights * self.success_CR), 0.1, 1.0)\n\n\n                # Update best\n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = trial_vector\n                    self.stagnation_counter = 0\n                else:\n                    self.stagnation_counter +=1\n                    \n                if self.budget <= 0:\n                    break\n            \n            # Orthogonal Design-based Diversity Maintenance\n            if self.budget > self.dim:  # Ensure enough budget for orthogonal design\n                orthogonal_matrix = self.generate_orthogonal_matrix(self.dim)\n                if orthogonal_matrix is not None:  # Ensure matrix generation was successful\n                  # Dynamically scale the perturbation\n                  scale = 0.1 * np.exp(-self.stagnation_counter / self.stagnation_threshold)\n                  for j in range(self.dim):\n                    # Choose a random individual to perturb\n                    idx_to_perturb = np.random.randint(0, self.pop_size)\n                    original_vector = population[idx_to_perturb].copy()\n                    \n                    # Perturb each dimension using the orthogonal matrix\n                    perturbation = orthogonal_matrix[j] * scale  # Scale the perturbation\n                    new_vector = original_vector + perturbation\n                    new_vector = np.clip(new_vector, lb, ub)\n                    \n                    #Evaluate\n                    f_new = func(new_vector)\n                    self.budget -= 1\n                    \n                    if f_new < fitness[idx_to_perturb]:\n                        population[idx_to_perturb] = new_vector\n                        fitness[idx_to_perturb] = f_new\n                        \n                        if f_new < self.f_opt:\n                            self.f_opt = f_new\n                            self.x_opt = new_vector\n                            self.stagnation_counter = 0 # Reset stagnation counter\n                    if self.budget <= 0:\n                      break\n            if self.budget <= 0:\n              break\n            \n            # Stagnation Check and Restart\n            if self.stagnation_counter > self.stagnation_threshold:\n                # Restart the population with a smaller population size\n                self.pop_size = max(10, self.pop_size // 2)\n                population = np.random.uniform(lb, ub, size=(self.pop_size, self.dim))\n                fitness = np.array([func(x) for x in population])\n                self.budget -= self.pop_size\n                self.f_opt = np.min(fitness)\n                self.x_opt = population[np.argmin(fitness)]\n                self.stagnation_counter = 0  # Reset stagnation counter\n                print(\"Restarting with population size:\", self.pop_size) #debugging\n                if self.budget <=0:\n                    break\n\n\n        return self.f_opt, self.x_opt\n\n    def generate_orthogonal_matrix(self, dim):\n        # Generate a simple orthogonal matrix (Hadamard matrix) for power of 2 dimensions\n        if dim & (dim - 1) == 0:  # Check if dim is a power of 2\n            H = np.array([[1]])\n            while H.shape[0] < dim:\n                H = np.vstack((np.hstack((H, H)), np.hstack((H, -H))))\n            return H\n        else:\n            # For non-power-of-2 dimensions, return None (or implement other orthogonal matrix generation)\n            return None", "configspace": "", "generation": 6, "feedback": "Evaluation timed out after 300 seconds.", "error": "Evaluation timed out after 300 seconds.", "parent_ids": ["cac9ad3f-6920-4233-83d8-1cdb3d0ff09f"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "55ec74f3-57db-4486-b2db-cc1374ba9e51", "fitness": "-inf", "name": "EnhancedOrthogonalDE", "description": "Enhanced Differential Evolution with orthogonal design-based diversity and a local search refinement step using a Gaussian process surrogate model for efficient exploration around promising solutions.", "code": "import numpy as np\nfrom sklearn.gaussian_process import GaussianProcessRegressor\nfrom sklearn.gaussian_process.kernels import RBF, ConstantKernel, WhiteKernel\nfrom scipy.stats import norm\n\n\nclass EnhancedOrthogonalDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=10, success_history_size=10, exploration_probability=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.F = 0.5\n        self.CR = 0.7\n        self.success_history_size = success_history_size\n        self.success_F = np.full(self.success_history_size, 0.5)\n        self.success_CR = np.full(self.success_history_size, 0.7)\n        self.success_fitness_reduction = np.zeros(self.success_history_size)\n        self.success_idx = 0\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.exploration_probability = exploration_probability\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n\n        # Initialize population\n        population = np.random.uniform(lb, ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        # Initialize archive\n        archive = np.random.uniform(lb, ub, size=(self.archive_size, self.dim))\n        archive_fitness = np.full(self.archive_size, np.inf)\n\n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)]\n\n        # Initialize Gaussian Process\n        kernel = ConstantKernel(constant_value=1.0, constant_value_bounds=(1e-3, 1e3)) * RBF(length_scale=1.0, length_scale_bounds=(1e-2, 1e2)) + WhiteKernel(noise_level=1e-3, noise_level_bounds=(1e-5, 1e1))\n        self.gaussian_process = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=5)\n\n        X_train = population.copy()\n        y_train = fitness.copy()\n        self.gaussian_process.fit(X_train, y_train)\n\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x1, x2, x3 = population[idxs]\n\n                mutant = x1 + self.F * (x2 - x3)\n                mutant = np.clip(mutant, lb, ub)\n\n                # Crossover\n                trial_vector = np.where(np.random.rand(self.dim) < self.CR, mutant, population[i])\n                trial_vector = np.clip(trial_vector, lb, ub)\n\n                # Evaluation\n                f_trial = func(trial_vector)\n                self.budget -= 1\n\n                # Selection\n                if f_trial < fitness[i]:\n                    # Update archive\n                    if f_trial < np.max(archive_fitness):\n                        worst_archive_idx = np.argmax(archive_fitness)\n                        archive[worst_archive_idx] = trial_vector\n                        archive_fitness[worst_archive_idx] = f_trial\n\n                    # Store successful F and CR values\n                    self.success_F[self.success_idx] = self.F\n                    self.success_CR[self.success_idx] = self.CR\n                    self.success_fitness_reduction[self.success_idx] = fitness[i] - f_trial\n                    self.success_idx = (self.success_idx + 1) % self.success_history_size\n                    \n                    population[i] = trial_vector\n                    fitness[i] = f_trial\n                    \n                    # Update F and CR based on success history (weighted average)\n                    weights = self.success_fitness_reduction / np.sum(self.success_fitness_reduction) if np.sum(self.success_fitness_reduction) > 0 else np.ones(self.success_history_size) / self.success_history_size\n                    self.F = np.clip(np.sum(weights * self.success_F), 0.1, 1.0)\n                    self.CR = np.clip(np.sum(weights * self.success_CR), 0.1, 1.0)\n\n                    # Update Gaussian Process training data\n                    X_train = np.vstack((X_train, trial_vector))\n                    y_train = np.append(y_train, f_trial)\n                    self.gaussian_process.fit(X_train, y_train)\n\n\n                # Update best\n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = trial_vector\n\n                if self.budget <= 0:\n                    break\n            \n            # Orthogonal Design-based Diversity Maintenance\n            if self.budget > self.dim and np.random.rand() < self.exploration_probability:  # Ensure enough budget for orthogonal design\n                orthogonal_matrix = self.generate_orthogonal_matrix(self.dim)\n                if orthogonal_matrix is not None:  # Ensure matrix generation was successful\n                    for j in range(self.dim):\n                        # Choose a random individual to perturb\n                        idx_to_perturb = np.random.randint(0, self.pop_size)\n                        original_vector = population[idx_to_perturb].copy()\n                        \n                        # Perturb each dimension using the orthogonal matrix\n                        perturbation = orthogonal_matrix[j] * 0.1  # Scale the perturbation\n                        new_vector = original_vector + perturbation\n                        new_vector = np.clip(new_vector, lb, ub)\n                        \n                        #Evaluate\n                        f_new = func(new_vector)\n                        self.budget -= 1\n                        \n                        if f_new < fitness[idx_to_perturb]:\n                            population[idx_to_perturb] = new_vector\n                            fitness[idx_to_perturb] = f_new\n                            \n                            if f_new < self.f_opt:\n                                self.f_opt = f_new\n                                self.x_opt = new_vector\n\n                            # Update Gaussian Process training data\n                            X_train = np.vstack((X_train, new_vector))\n                            y_train = np.append(y_train, f_new)\n                            self.gaussian_process.fit(X_train, y_train)\n\n                        if self.budget <= 0:\n                            break\n            if self.budget <= 0:\n                break\n\n            # Gaussian Process-based Local Search\n            if self.budget > self.dim:\n                # Find the most promising individual\n                best_idx = np.argmin(fitness)\n                best_individual = population[best_idx]\n\n                # Sample new points from the GP around the best individual\n                num_local_samples = min(self.dim, self.budget)  # Adjust the number of samples based on the remaining budget\n                sampled_points = np.random.normal(loc=best_individual, scale=0.05, size=(num_local_samples, self.dim))\n                sampled_points = np.clip(sampled_points, lb, ub)\n\n\n                for k in range(num_local_samples):\n                    local_point = sampled_points[k]\n                    f_local = func(local_point)\n                    self.budget -= 1\n\n                    if f_local < fitness[best_idx]:\n                        population[best_idx] = local_point\n                        fitness[best_idx] = f_local\n                        if f_local < self.f_opt:\n                            self.f_opt = f_local\n                            self.x_opt = local_point\n\n                        # Update Gaussian Process training data\n                        X_train = np.vstack((X_train, local_point))\n                        y_train = np.append(y_train, f_local)\n                        self.gaussian_process.fit(X_train, y_train)\n\n                    if self.budget <= 0:\n                        break\n\n            if self.budget <= 0:\n                break\n\n        return self.f_opt, self.x_opt\n\n    def generate_orthogonal_matrix(self, dim):\n        # Generate a simple orthogonal matrix (Hadamard matrix) for power of 2 dimensions\n        if dim & (dim - 1) == 0:  # Check if dim is a power of 2\n            H = np.array([[1]])\n            while H.shape[0] < dim:\n                H = np.vstack((np.hstack((H, H)), np.hstack((H, -H))))\n            return H\n        else:\n            # For non-power-of-2 dimensions, return None (or implement other orthogonal matrix generation)\n            return None", "configspace": "", "generation": 6, "feedback": "In the code, line 41, in __call__, the following error occurred:\nNameError: name 'ConstantKernel' is not defined\nOn line: kernel = ConstantKernel(constant_value=1.0, constant_value_bounds=(1e-3, 1e3)) * RBF(length_scale=1.0, length_scale_bounds=(1e-2, 1e2)) + WhiteKernel(noise_level=1e-3, noise_level_bounds=(1e-5, 1e1))", "error": "In the code, line 41, in __call__, the following error occurred:\nNameError: name 'ConstantKernel' is not defined\nOn line: kernel = ConstantKernel(constant_value=1.0, constant_value_bounds=(1e-3, 1e3)) * RBF(length_scale=1.0, length_scale_bounds=(1e-2, 1e2)) + WhiteKernel(noise_level=1e-3, noise_level_bounds=(1e-5, 1e1))", "parent_ids": ["cac9ad3f-6920-4233-83d8-1cdb3d0ff09f"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "4235f33e-d2e2-4d2c-b49e-c938be7fb82f", "fitness": 0.48894451758234875, "name": "AdaptiveDEToroidalLocalAdv", "description": "Self-adaptive Differential Evolution with a dynamically adjusted population size and a more focused local search based on gradient estimation.", "code": "import numpy as np\n\nclass AdaptiveDEToroidalLocalAdv:\n    def __init__(self, budget=10000, dim=10, initial_pop_size=40, local_search_prob=0.1, pop_decay_rate=0.99, gradient_samples=5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = initial_pop_size\n        self.lb = -5.0\n        self.ub = 5.0\n        self.F = 0.5\n        self.CR = 0.7\n        self.local_search_prob = local_search_prob\n        self.pop_decay_rate = pop_decay_rate\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.gradient_samples = gradient_samples\n\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        # Initialize population\n        population = np.random.uniform(lb, ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)]\n\n        generation = 0\n        while self.budget > 0:\n            generation += 1\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x1, x2, x3 = population[idxs]\n                mutant = x1 + self.F * (x2 - x3)\n\n                # Toroidal Boundary Handling\n                mutant = np.mod(mutant - lb, ub - lb) + lb\n\n                # Crossover\n                trial_vector = np.where(np.random.rand(self.dim) < self.CR, mutant, population[i])\n\n                # Evaluation\n                f_trial = func(trial_vector)\n                self.budget -= 1\n\n                # Selection\n                if f_trial < fitness[i]:\n                    population[i] = trial_vector\n                    fitness[i] = f_trial\n\n                    # Self-adapt F and CR (simplified)\n                    self.F = np.clip(np.random.normal(0.5, 0.1), 0.1, 1.0)\n                    self.CR = np.clip(np.random.normal(0.7, 0.1), 0.1, 1.0)\n\n                # Update best\n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = trial_vector\n\n                    # Trigger local search with probability\n                    if np.random.rand() < self.local_search_prob:\n                        self.x_opt, self.f_opt = self.local_search(func, self.x_opt, self.f_opt)\n\n                if self.budget <= 0:\n                    break\n            \n            # Dynamic population size adjustment\n            self.pop_size = max(int(self.pop_size * self.pop_decay_rate), 10)  # Ensure a minimum population size\n            if self.pop_size < population.shape[0]:\n                # Reduce population by removing worst individuals\n                worst_indices = np.argsort(fitness)[- (population.shape[0] - self.pop_size):]\n                population = np.delete(population, worst_indices, axis=0)\n                fitness = np.delete(fitness, worst_indices)\n            elif self.pop_size > population.shape[0]:\n                #Increase population with random individuals\n                num_new = self.pop_size - population.shape[0]\n                new_population = np.random.uniform(lb, ub, size=(num_new, self.dim))\n                new_fitness = np.array([func(x) for x in new_population])\n                self.budget -= num_new\n                population = np.concatenate((population, new_population))\n                fitness = np.concatenate((fitness, new_fitness))\n\n\n\n        return self.f_opt, self.x_opt\n\n    def local_search(self, func, x_opt, f_opt, radius=0.1):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        \n        # Estimate gradient using finite differences\n        gradient = np.zeros(self.dim)\n        for _ in range(self.gradient_samples):\n            direction = np.random.randn(self.dim)\n            direction /= np.linalg.norm(direction)\n            \n            x_plus = np.clip(x_opt + radius * direction, lb, ub)\n            x_minus = np.clip(x_opt - radius * direction, lb, ub)\n            \n            f_plus = func(x_plus)\n            f_minus = func(x_minus)\n            self.budget -= 2\n            \n            gradient += (f_plus - f_minus) * direction / (2 * radius)\n            if self.budget <=0:\n                break\n\n        if self.budget <=0:\n            return x_opt, f_opt\n        \n        gradient /= self.gradient_samples\n        \n        # Move in the opposite direction of the gradient\n        x_new = x_opt - 0.1 * radius * gradient  # Smaller step size\n        x_new = np.clip(x_new, lb, ub)\n        \n        # Evaluate the new solution\n        f_new = func(x_new)\n        self.budget -= 1\n        \n        # Update the best solution if necessary\n        if f_new < f_opt:\n            f_opt = f_new\n            x_opt = x_new\n\n        return x_opt, f_opt", "configspace": "", "generation": 6, "feedback": "The algorithm AdaptiveDEToroidalLocalAdv scored 0.489 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["077d7b9d-ba51-4b31-9952-a874a7fb2c41"], "operator": null, "metadata": {"aucs": [0.247999638285053, 0.8431862758829904, 0.42783378009077777, 0.9257028936529227, 0]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
