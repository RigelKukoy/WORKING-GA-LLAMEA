{"id": "b480012c-972b-40b2-9156-c4552cb71bb5", "fitness": "-inf", "name": "HybridPSO_DE", "description": "A population-based algorithm that combines aspects of particle swarm optimization and differential evolution, using a modified velocity update and a self-adaptive mutation strategy.", "code": "import numpy as np\n\nclass HybridPSO_DE:\n    def __init__(self, budget=10000, dim=10, pop_size=20, w=0.7, c1=1.5, c2=1.5, cr=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.w = w  # Inertia weight\n        self.c1 = c1  # Cognitive coefficient\n        self.c2 = c2  # Social coefficient\n        self.cr = cr  # Crossover rate\n\n    def __call__(self, func):\n        # Initialization\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size # deduct initial evaluations\n\n        personal_best_positions = population.copy()\n        personal_best_fitness = fitness.copy()\n        global_best_index = np.argmin(fitness)\n        global_best_position = population[global_best_index].copy()\n        global_best_fitness = fitness[global_best_index]\n        \n        velocities = np.random.uniform(-1, 1, size=(self.pop_size, self.dim)) # Initialize velocities\n\n        # Optimization loop\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Velocity update (PSO component)\n                velocities[i] = self.w * velocities[i] + \\\n                                self.c1 * np.random.rand(self.dim) * (personal_best_positions[i] - population[i]) + \\\n                                self.c2 * np.random.rand(self.dim) * (global_best_position - population[i])\n                \n                # Apply velocity limits (optional)\n                # velocities[i] = np.clip(velocities[i], -1, 1)\n\n                # Generate trial vector (DE component)\n                # Select three distinct individuals\n                indices = list(range(self.pop_size))\n                indices.remove(i)\n                r1, r2, r3 = np.random.choice(indices, 3, replace=False)\n                \n                # Self-adaptive mutation factor\n                F = np.random.normal(0.5, 0.3)\n                F = np.clip(F, 0.0, 1.0)\n\n                trial_vector = population[r1] + F * (population[r2] - population[r3])\n\n                # Crossover\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() > self.cr and j != j_rand:\n                        trial_vector[j] = population[i, j]\n\n                # Clip the solution to the bounds of the optimization problem\n                trial_vector = np.clip(trial_vector, func.bounds.lb, func.bounds.ub)\n\n                # Evaluation\n                f_trial = func(trial_vector)\n                self.budget -= 1\n\n                # Selection\n                if f_trial < fitness[i]:\n                    population[i] = trial_vector\n                    fitness[i] = f_trial\n                    \n                    # Update personal best\n                    if f_trial < personal_best_fitness[i]:\n                        personal_best_fitness[i] = f_trial\n                        personal_best_positions[i] = trial_vector.copy()\n                        \n                        # Update global best\n                        if f_trial < global_best_fitness:\n                            global_best_fitness = f_trial\n                            global_best_position = trial_vector.copy()\n                            \n                # Stop if budget is zero\n                if self.budget <= 0:\n                    break\n\n\n        return global_best_fitness, global_best_position", "configspace": "", "generation": 0, "feedback": "Evaluation timed out after 300 seconds.", "error": "Evaluation timed out after 300 seconds.", "parent_ids": [], "operator": "init", "metadata": {"llm_response": "# Description: A population-based algorithm that combines aspects of particle swarm optimization and differential evolution, using a modified velocity update and a self-adaptive mutation strategy.\n# Code:\n```python\nimport numpy as np\n\nclass HybridPSO_DE:\n    def __init__(self, budget=10000, dim=10, pop_size=20, w=0.7, c1=1.5, c2=1.5, cr=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.w = w  # Inertia weight\n        self.c1 = c1  # Cognitive coefficient\n        self.c2 = c2  # Social coefficient\n        self.cr = cr  # Crossover rate\n\n    def __call__(self, func):\n        # Initialization\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size # deduct initial evaluations\n\n        personal_best_positions = population.copy()\n        personal_best_fitness = fitness.copy()\n        global_best_index = np.argmin(fitness)\n        global_best_position = population[global_best_index].copy()\n        global_best_fitness = fitness[global_best_index]\n        \n        velocities = np.random.uniform(-1, 1, size=(self.pop_size, self.dim)) # Initialize velocities\n\n        # Optimization loop\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Velocity update (PSO component)\n                velocities[i] = self.w * velocities[i] + \\\n                                self.c1 * np.random.rand(self.dim) * (personal_best_positions[i] - population[i]) + \\\n                                self.c2 * np.random.rand(self.dim) * (global_best_position - population[i])\n                \n                # Apply velocity limits (optional)\n                # velocities[i] = np.clip(velocities[i], -1, 1)\n\n                # Generate trial vector (DE component)\n                # Select three distinct individuals\n                indices = list(range(self.pop_size))\n                indices.remove(i)\n                r1, r2, r3 = np.random.choice(indices, 3, replace=False)\n                \n                # Self-adaptive mutation factor\n                F = np.random.normal(0.5, 0.3)\n                F = np.clip(F, 0.0, 1.0)\n\n                trial_vector = population[r1] + F * (population[r2] - population[r3])\n\n                # Crossover\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() > self.cr and j != j_rand:\n                        trial_vector[j] = population[i, j]\n\n                # Clip the solution to the bounds of the optimization problem\n                trial_vector = np.clip(trial_vector, func.bounds.lb, func.bounds.ub)\n\n                # Evaluation\n                f_trial = func(trial_vector)\n                self.budget -= 1\n\n                # Selection\n                if f_trial < fitness[i]:\n                    population[i] = trial_vector\n                    fitness[i] = f_trial\n                    \n                    # Update personal best\n                    if f_trial < personal_best_fitness[i]:\n                        personal_best_fitness[i] = f_trial\n                        personal_best_positions[i] = trial_vector.copy()\n                        \n                        # Update global best\n                        if f_trial < global_best_fitness:\n                            global_best_fitness = f_trial\n                            global_best_position = trial_vector.copy()\n                            \n                # Stop if budget is zero\n                if self.budget <= 0:\n                    break\n\n\n        return global_best_fitness, global_best_position\n```", "generation": 0, "operator": "init"}, "task_prompt": ""}
{"id": "dd0f1f2e-9fde-491b-90f4-59ed0fe87d71", "fitness": 0.0, "name": "SelfAdjustingDE", "description": "A self-adjusting differential evolution strategy with probabilistic parameter adaptation and a local search component.", "code": "import numpy as np\n\nclass SelfAdjustingDE:\n    def __init__(self, budget=10000, dim=10, pop_size=None, F=0.5, Cr=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size if pop_size is not None else 10*dim\n        self.F = F\n        self.Cr = Cr\n        self.lb = -5.0\n        self.ub = 5.0\n        self.local_search_prob = 0.1\n        self.F_adapt_prob = 0.1\n        self.Cr_adapt_prob = 0.1\n        self.ls_radius = 0.1\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.pop = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.nevals = self.pop_size  # Increment evaluation count\n\n        while self.nevals < self.budget:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = self.pop[idxs]\n                \n                #Probabilistic adaptation of F\n                if np.random.rand() < self.F_adapt_prob:\n                   F = np.random.normal(loc=0.5, scale=0.3)\n                   F = np.clip(F, 0.0, 1.0)\n                else:\n                   F = self.F \n\n                x_mutated = self.pop[i] + F * (x_r2 - x_r3)\n                x_mutated = np.clip(x_mutated, self.lb, self.ub)\n\n                # Crossover\n                x_trial = np.copy(self.pop[i])\n                \n                #Probabilistic adaptation of Cr\n                if np.random.rand() < self.Cr_adapt_prob:\n                    Cr = np.random.normal(loc=0.9, scale=0.1)\n                    Cr = np.clip(Cr, 0.0, 1.0)\n                else:\n                    Cr = self.Cr\n\n                crossover_points = np.random.rand(self.dim) < Cr\n                if not np.any(crossover_points):\n                    crossover_points[np.random.randint(0, self.dim)] = True\n                x_trial[crossover_points] = x_mutated[crossover_points]\n                \n                # Evaluation\n                f_trial = func(x_trial)\n                self.nevals += 1\n\n                # Selection\n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.pop[i] = x_trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = x_trial\n\n                # Local Search (Probabilistic)\n                if np.random.rand() < self.local_search_prob and self.nevals < self.budget:\n                    x_ls = np.random.uniform(low=np.maximum(self.pop[i] - self.ls_radius, self.lb),\n                                            high=np.minimum(self.pop[i] + self.ls_radius, self.ub))\n                    f_ls = func(x_ls)\n                    self.nevals += 1\n\n                    if f_ls < self.fitness[i]:\n                        self.fitness[i] = f_ls\n                        self.pop[i] = x_ls\n                        if f_ls < self.f_opt:\n                            self.f_opt = f_ls\n                            self.x_opt = x_ls\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm SelfAdjustingDE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": "init", "metadata": {"llm_response": "# Description: A self-adjusting differential evolution strategy with probabilistic parameter adaptation and a local search component.\n# Code:\n```python\nimport numpy as np\n\nclass SelfAdjustingDE:\n    def __init__(self, budget=10000, dim=10, pop_size=None, F=0.5, Cr=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size if pop_size is not None else 10*dim\n        self.F = F\n        self.Cr = Cr\n        self.lb = -5.0\n        self.ub = 5.0\n        self.local_search_prob = 0.1\n        self.F_adapt_prob = 0.1\n        self.Cr_adapt_prob = 0.1\n        self.ls_radius = 0.1\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.pop = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.nevals = self.pop_size  # Increment evaluation count\n\n        while self.nevals < self.budget:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = self.pop[idxs]\n                \n                #Probabilistic adaptation of F\n                if np.random.rand() < self.F_adapt_prob:\n                   F = np.random.normal(loc=0.5, scale=0.3)\n                   F = np.clip(F, 0.0, 1.0)\n                else:\n                   F = self.F \n\n                x_mutated = self.pop[i] + F * (x_r2 - x_r3)\n                x_mutated = np.clip(x_mutated, self.lb, self.ub)\n\n                # Crossover\n                x_trial = np.copy(self.pop[i])\n                \n                #Probabilistic adaptation of Cr\n                if np.random.rand() < self.Cr_adapt_prob:\n                    Cr = np.random.normal(loc=0.9, scale=0.1)\n                    Cr = np.clip(Cr, 0.0, 1.0)\n                else:\n                    Cr = self.Cr\n\n                crossover_points = np.random.rand(self.dim) < Cr\n                if not np.any(crossover_points):\n                    crossover_points[np.random.randint(0, self.dim)] = True\n                x_trial[crossover_points] = x_mutated[crossover_points]\n                \n                # Evaluation\n                f_trial = func(x_trial)\n                self.nevals += 1\n\n                # Selection\n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.pop[i] = x_trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = x_trial\n\n                # Local Search (Probabilistic)\n                if np.random.rand() < self.local_search_prob and self.nevals < self.budget:\n                    x_ls = np.random.uniform(low=np.maximum(self.pop[i] - self.ls_radius, self.lb),\n                                            high=np.minimum(self.pop[i] + self.ls_radius, self.ub))\n                    f_ls = func(x_ls)\n                    self.nevals += 1\n\n                    if f_ls < self.fitness[i]:\n                        self.fitness[i] = f_ls\n                        self.pop[i] = x_ls\n                        if f_ls < self.f_opt:\n                            self.f_opt = f_ls\n                            self.x_opt = x_ls\n\n        return self.f_opt, self.x_opt\n```", "generation": 0, "operator": "init", "aucs": [0]}, "task_prompt": ""}
{"id": "32580f0d-1727-46bf-9f2d-566363163918", "fitness": "-inf", "name": "AdaptiveHybridOptimization", "description": "A population-based algorithm that combines aspects of particle swarm optimization and differential evolution, using a dynamic selection of update strategies based on their recent success.", "code": "import numpy as np\n\nclass AdaptiveHybridOptimization:\n    def __init__(self, budget=10000, dim=10, pop_size=20, pso_weight=0.7, de_weight=0.7):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.pso_weight = pso_weight\n        self.de_weight = de_weight\n        self.lb = -5.0\n        self.ub = 5.0\n        self.success_rate_pso = 0.5\n        self.success_rate_de = 0.5\n        self.learning_rate = 0.1\n\n    def initialize_population(self):\n        return np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        fitness = np.array([func(x) for x in population])\n        return fitness\n\n    def pso_update(self, population, velocities, personal_best_positions, global_best_position):\n        inertia_weight = 0.7\n        cognitive_coeff = 1.4\n        social_coeff = 1.4\n\n        r1 = np.random.rand(self.pop_size, self.dim)\n        r2 = np.random.rand(self.pop_size, self.dim)\n\n        velocities = (inertia_weight * velocities +\n                      cognitive_coeff * r1 * (personal_best_positions - population) +\n                      social_coeff * r2 * (global_best_position - population))\n        \n        population = population + velocities\n        population = np.clip(population, self.lb, self.ub)  # Keep within bounds\n        return population, velocities\n\n    def de_update(self, population, F=0.8, CR=0.9):\n        new_population = np.copy(population)\n        for i in range(self.pop_size):\n            idxs = [idx for idx in range(self.pop_size) if idx != i]\n            a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n            \n            for j in range(self.dim):\n                if np.random.rand() < CR or j == np.random.randint(self.dim):\n                    new_population[i, j] = a[j] + F * (b[j] - c[j])\n            \n            new_population[i] = np.clip(new_population[i], self.lb, self.ub)\n        return new_population\n\n    def __call__(self, func):\n        population = self.initialize_population()\n        velocities = np.zeros((self.pop_size, self.dim))\n        fitness = self.evaluate_population(func, population)\n        \n        personal_best_fitness = np.copy(fitness)\n        personal_best_positions = np.copy(population)\n        \n        global_best_index = np.argmin(fitness)\n        global_best_fitness = fitness[global_best_index]\n        global_best_position = population[global_best_index]\n        \n        eval_count = self.pop_size\n\n        pso_successes = 0\n        de_successes = 0\n\n        while eval_count < self.budget:\n\n            # PSO Update\n            if np.random.rand() < self.pso_weight:\n                old_population = np.copy(population)\n                population, velocities = self.pso_update(population, velocities, personal_best_positions, global_best_position)\n                new_fitness = self.evaluate_population(func, population)\n                eval_count += self.pop_size\n\n                for i in range(self.pop_size):\n                    if new_fitness[i] < personal_best_fitness[i]:\n                        personal_best_fitness[i] = new_fitness[i]\n                        personal_best_positions[i] = population[i]\n\n                        if new_fitness[i] < global_best_fitness:\n                            global_best_fitness = new_fitness[i]\n                            global_best_position = population[i]\n                            pso_successes += 1\n\n\n            # DE Update\n            else:\n                old_population = np.copy(population)\n                population = self.de_update(population)\n                new_fitness = self.evaluate_population(func, population)\n                eval_count += self.pop_size\n\n                for i in range(self.pop_size):\n                    if new_fitness[i] < personal_best_fitness[i]:\n                        personal_best_fitness[i] = new_fitness[i]\n                        personal_best_positions[i] = population[i]\n                        \n                        if new_fitness[i] < global_best_fitness:\n                            global_best_fitness = new_fitness[i]\n                            global_best_position = population[i]\n                            de_successes += 1\n\n            # Adjust weights based on success\n            total_updates = eval_count // self.pop_size\n            if total_updates > 0:\n                self.success_rate_pso = pso_successes / total_updates\n                self.success_rate_de = de_successes / total_updates\n\n            if self.success_rate_pso + self.success_rate_de > 0: # avoid division by zero\n                self.pso_weight = (1-self.learning_rate)*self.pso_weight + self.learning_rate * (self.success_rate_pso/(self.success_rate_pso+self.success_rate_de))\n\n            self.pso_weight = np.clip(self.pso_weight, 0.1, 0.9)\n\n\n\n        return global_best_fitness, global_best_position", "configspace": "", "generation": 0, "feedback": "Evaluation timed out after 300 seconds.", "error": "Evaluation timed out after 300 seconds.", "parent_ids": [], "operator": "init", "metadata": {"llm_response": "# Description: A population-based algorithm that combines aspects of particle swarm optimization and differential evolution, using a dynamic selection of update strategies based on their recent success.\n# Code: \n```python\nimport numpy as np\n\nclass AdaptiveHybridOptimization:\n    def __init__(self, budget=10000, dim=10, pop_size=20, pso_weight=0.7, de_weight=0.7):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.pso_weight = pso_weight\n        self.de_weight = de_weight\n        self.lb = -5.0\n        self.ub = 5.0\n        self.success_rate_pso = 0.5\n        self.success_rate_de = 0.5\n        self.learning_rate = 0.1\n\n    def initialize_population(self):\n        return np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        fitness = np.array([func(x) for x in population])\n        return fitness\n\n    def pso_update(self, population, velocities, personal_best_positions, global_best_position):\n        inertia_weight = 0.7\n        cognitive_coeff = 1.4\n        social_coeff = 1.4\n\n        r1 = np.random.rand(self.pop_size, self.dim)\n        r2 = np.random.rand(self.pop_size, self.dim)\n\n        velocities = (inertia_weight * velocities +\n                      cognitive_coeff * r1 * (personal_best_positions - population) +\n                      social_coeff * r2 * (global_best_position - population))\n        \n        population = population + velocities\n        population = np.clip(population, self.lb, self.ub)  # Keep within bounds\n        return population, velocities\n\n    def de_update(self, population, F=0.8, CR=0.9):\n        new_population = np.copy(population)\n        for i in range(self.pop_size):\n            idxs = [idx for idx in range(self.pop_size) if idx != i]\n            a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n            \n            for j in range(self.dim):\n                if np.random.rand() < CR or j == np.random.randint(self.dim):\n                    new_population[i, j] = a[j] + F * (b[j] - c[j])\n            \n            new_population[i] = np.clip(new_population[i], self.lb, self.ub)\n        return new_population\n\n    def __call__(self, func):\n        population = self.initialize_population()\n        velocities = np.zeros((self.pop_size, self.dim))\n        fitness = self.evaluate_population(func, population)\n        \n        personal_best_fitness = np.copy(fitness)\n        personal_best_positions = np.copy(population)\n        \n        global_best_index = np.argmin(fitness)\n        global_best_fitness = fitness[global_best_index]\n        global_best_position = population[global_best_index]\n        \n        eval_count = self.pop_size\n\n        pso_successes = 0\n        de_successes = 0\n\n        while eval_count < self.budget:\n\n            # PSO Update\n            if np.random.rand() < self.pso_weight:\n                old_population = np.copy(population)\n                population, velocities = self.pso_update(population, velocities, personal_best_positions, global_best_position)\n                new_fitness = self.evaluate_population(func, population)\n                eval_count += self.pop_size\n\n                for i in range(self.pop_size):\n                    if new_fitness[i] < personal_best_fitness[i]:\n                        personal_best_fitness[i] = new_fitness[i]\n                        personal_best_positions[i] = population[i]\n\n                        if new_fitness[i] < global_best_fitness:\n                            global_best_fitness = new_fitness[i]\n                            global_best_position = population[i]\n                            pso_successes += 1\n\n\n            # DE Update\n            else:\n                old_population = np.copy(population)\n                population = self.de_update(population)\n                new_fitness = self.evaluate_population(func, population)\n                eval_count += self.pop_size\n\n                for i in range(self.pop_size):\n                    if new_fitness[i] < personal_best_fitness[i]:\n                        personal_best_fitness[i] = new_fitness[i]\n                        personal_best_positions[i] = population[i]\n                        \n                        if new_fitness[i] < global_best_fitness:\n                            global_best_fitness = new_fitness[i]\n                            global_best_position = population[i]\n                            de_successes += 1\n\n            # Adjust weights based on success\n            total_updates = eval_count // self.pop_size\n            if total_updates > 0:\n                self.success_rate_pso = pso_successes / total_updates\n                self.success_rate_de = de_successes / total_updates\n\n            if self.success_rate_pso + self.success_rate_de > 0: # avoid division by zero\n                self.pso_weight = (1-self.learning_rate)*self.pso_weight + self.learning_rate * (self.success_rate_pso/(self.success_rate_pso+self.success_rate_de))\n\n            self.pso_weight = np.clip(self.pso_weight, 0.1, 0.9)\n\n\n\n        return global_best_fitness, global_best_position\n```", "generation": 0, "operator": "init"}, "task_prompt": ""}
{"id": "610d9c62-544d-4b4e-a2da-59db9a090910", "fitness": "-inf", "name": "HybridDECMASearch", "description": "A population-based algorithm that uses a combination of differential evolution and covariance matrix adaptation evolution strategy (CMA-ES) to balance exploration and exploitation.", "code": "import numpy as np\n\nclass HybridDECMASearch:\n    def __init__(self, budget=10000, dim=10, pop_size=20, de_mutation_factor=0.5, de_crossover_rate=0.7, cma_sigma=0.5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.de_mutation_factor = de_mutation_factor\n        self.de_crossover_rate = de_crossover_rate\n        self.cma_sigma = cma_sigma\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.eval_count = 0  # Track function evaluations\n\n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.eval_count += self.pop_size\n\n        # CMA-ES parameters\n        mean = np.mean(population, axis=0)\n        covariance = np.eye(self.dim)\n        pc = np.zeros(self.dim)\n        ps = np.zeros(self.dim)\n        damps = 1 + (self.dim / 2)\n        cs = 0.3\n        ccov = 2 / (self.dim**0.5)\n        mu = self.pop_size // 2\n        weights = np.log(mu + 1/2) - np.log(np.arange(1, mu + 1))\n        weights = weights / np.sum(weights)\n        B = np.eye(self.dim) # Eigenvectors of C\n        D = np.ones(self.dim) # Eigenvalues of C\n\n        while self.eval_count < self.budget:\n            # Differential Evolution\n            for i in range(self.pop_size):\n                if self.eval_count >= self.budget:\n                    break\n                # Select three random individuals (excluding the current one)\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                if i in idxs:\n                    idxs = np.random.choice(self.pop_size, 3, replace=False)\n                    while i in idxs:\n                        idxs = np.random.choice(self.pop_size, 3, replace=False)\n\n                x1, x2, x3 = population[idxs]\n\n                # Mutation and crossover\n                v = x1 + self.de_mutation_factor * (x2 - x3)\n                trial_vector = np.copy(population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < self.de_crossover_rate or j == j_rand:\n                        trial_vector[j] = v[j]\n\n                # Clip to bounds\n                trial_vector = np.clip(trial_vector, func.bounds.lb, func.bounds.ub)\n\n                # Evaluate trial vector\n                f = func(trial_vector)\n                self.eval_count += 1\n\n                # Update population if the trial vector is better\n                if f < fitness[i]:\n                    fitness[i] = f\n                    population[i] = trial_vector\n\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = trial_vector.copy() # important copy!\n\n            # CMA-ES\n            if self.eval_count < self.budget:\n                z = np.random.normal(0, 1, size=(self.pop_size, self.dim))\n                y = B @ (D[:,None] * z.T) # y_i ~ N(0,C)\n                x_cma = mean + self.cma_sigma * y.T\n                x_cma = np.clip(x_cma, func.bounds.lb, func.bounds.ub)\n\n                fitness_cma = np.array([func(x) for x in x_cma])\n                self.eval_count += len(fitness_cma)\n\n                idx_sort = np.argsort(fitness_cma)\n                x_cma = x_cma[idx_sort]\n                fitness_cma = fitness_cma[idx_sort]\n\n                delta_mean = np.sum(weights[:,None] * x_cma[:mu], axis=0) - mean\n                mean = mean + delta_mean\n\n                ps = (1-cs)*ps + np.sqrt(cs*(2-cs))*delta_mean/(self.cma_sigma)\n                pc = (1-ccov)*pc + np.sqrt(ccov*(2-ccov)) * B @ (D[:,None] * np.mean(z[:mu], axis=0)[:,None]).flatten()\n                C = (1-ccov) * covariance + ccov*pc[:,None] @ pc[None,:] + ccov * (1/mu) * np.sum([weights[i] * (x_cma[i] - mean)[:,None] @ (x_cma[i] - mean)[None,:] for i in range(mu)], axis=0)\n\n                # eigendecomposition\n                try:\n                    covariance = np.linalg.cholesky(C)\n                    B = np.linalg.qr(np.random.randn(self.dim, self.dim))[0] #random orthogonal matrix to re-estimate basis\n                except np.linalg.LinAlgError:\n                    covariance = np.eye(self.dim)\n\n                # update B and D\n                try:\n                    D, B = np.linalg.eig(C)\n                    D = np.sqrt(D)\n                except np.linalg.LinAlgError:\n                    D = np.ones(self.dim)\n                    B = np.eye(self.dim)\n\n                for i in range(len(fitness_cma)):\n                     if self.eval_count >= self.budget:\n                         break\n                     if fitness_cma[i] < self.f_opt:\n                         self.f_opt = fitness_cma[i]\n                         self.x_opt = x_cma[i].copy()\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "Evaluation timed out after 300 seconds.", "error": "Evaluation timed out after 300 seconds.", "parent_ids": [], "operator": "init", "metadata": {"llm_response": "# Description: A population-based algorithm that uses a combination of differential evolution and covariance matrix adaptation evolution strategy (CMA-ES) to balance exploration and exploitation.\n\n# Code:\n```python\nimport numpy as np\n\nclass HybridDECMASearch:\n    def __init__(self, budget=10000, dim=10, pop_size=20, de_mutation_factor=0.5, de_crossover_rate=0.7, cma_sigma=0.5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.de_mutation_factor = de_mutation_factor\n        self.de_crossover_rate = de_crossover_rate\n        self.cma_sigma = cma_sigma\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.eval_count = 0  # Track function evaluations\n\n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.eval_count += self.pop_size\n\n        # CMA-ES parameters\n        mean = np.mean(population, axis=0)\n        covariance = np.eye(self.dim)\n        pc = np.zeros(self.dim)\n        ps = np.zeros(self.dim)\n        damps = 1 + (self.dim / 2)\n        cs = 0.3\n        ccov = 2 / (self.dim**0.5)\n        mu = self.pop_size // 2\n        weights = np.log(mu + 1/2) - np.log(np.arange(1, mu + 1))\n        weights = weights / np.sum(weights)\n        B = np.eye(self.dim) # Eigenvectors of C\n        D = np.ones(self.dim) # Eigenvalues of C\n\n        while self.eval_count < self.budget:\n            # Differential Evolution\n            for i in range(self.pop_size):\n                if self.eval_count >= self.budget:\n                    break\n                # Select three random individuals (excluding the current one)\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                if i in idxs:\n                    idxs = np.random.choice(self.pop_size, 3, replace=False)\n                    while i in idxs:\n                        idxs = np.random.choice(self.pop_size, 3, replace=False)\n\n                x1, x2, x3 = population[idxs]\n\n                # Mutation and crossover\n                v = x1 + self.de_mutation_factor * (x2 - x3)\n                trial_vector = np.copy(population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < self.de_crossover_rate or j == j_rand:\n                        trial_vector[j] = v[j]\n\n                # Clip to bounds\n                trial_vector = np.clip(trial_vector, func.bounds.lb, func.bounds.ub)\n\n                # Evaluate trial vector\n                f = func(trial_vector)\n                self.eval_count += 1\n\n                # Update population if the trial vector is better\n                if f < fitness[i]:\n                    fitness[i] = f\n                    population[i] = trial_vector\n\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = trial_vector.copy() # important copy!\n\n            # CMA-ES\n            if self.eval_count < self.budget:\n                z = np.random.normal(0, 1, size=(self.pop_size, self.dim))\n                y = B @ (D[:,None] * z.T) # y_i ~ N(0,C)\n                x_cma = mean + self.cma_sigma * y.T\n                x_cma = np.clip(x_cma, func.bounds.lb, func.bounds.ub)\n\n                fitness_cma = np.array([func(x) for x in x_cma])\n                self.eval_count += len(fitness_cma)\n\n                idx_sort = np.argsort(fitness_cma)\n                x_cma = x_cma[idx_sort]\n                fitness_cma = fitness_cma[idx_sort]\n\n                delta_mean = np.sum(weights[:,None] * x_cma[:mu], axis=0) - mean\n                mean = mean + delta_mean\n\n                ps = (1-cs)*ps + np.sqrt(cs*(2-cs))*delta_mean/(self.cma_sigma)\n                pc = (1-ccov)*pc + np.sqrt(ccov*(2-ccov)) * B @ (D[:,None] * np.mean(z[:mu], axis=0)[:,None]).flatten()\n                C = (1-ccov) * covariance + ccov*pc[:,None] @ pc[None,:] + ccov * (1/mu) * np.sum([weights[i] * (x_cma[i] - mean)[:,None] @ (x_cma[i] - mean)[None,:] for i in range(mu)], axis=0)\n\n                # eigendecomposition\n                try:\n                    covariance = np.linalg.cholesky(C)\n                    B = np.linalg.qr(np.random.randn(self.dim, self.dim))[0] #random orthogonal matrix to re-estimate basis\n                except np.linalg.LinAlgError:\n                    covariance = np.eye(self.dim)\n\n                # update B and D\n                try:\n                    D, B = np.linalg.eig(C)\n                    D = np.sqrt(D)\n                except np.linalg.LinAlgError:\n                    D = np.ones(self.dim)\n                    B = np.eye(self.dim)\n\n                for i in range(len(fitness_cma)):\n                     if self.eval_count >= self.budget:\n                         break\n                     if fitness_cma[i] < self.f_opt:\n                         self.f_opt = fitness_cma[i]\n                         self.x_opt = x_cma[i].copy()\n\n        return self.f_opt, self.x_opt\n```", "generation": 0, "operator": "init"}, "task_prompt": ""}
{"id": "cede0658-db00-474c-bd9d-4e797090ea92", "fitness": "-inf", "name": "AdaptiveCoordinateDescent", "description": "Adaptive Coordinate Descent with shrinking step size based on function evaluations.", "code": "import numpy as np\n\nclass AdaptiveCoordinateDescent:\n    def __init__(self, budget=10000, dim=10, initial_step_size=1.0):\n        self.budget = budget\n        self.dim = dim\n        self.initial_step_size = initial_step_size\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = np.random.uniform(func.bounds.lb, func.bounds.ub)  # Initialize randomly\n        f_current = func(self.x_opt)\n        self.f_opt = f_current\n\n        step_size = self.initial_step_size\n        eval_count = 1  # Account for initial evaluation\n\n        while eval_count < self.budget:\n            for i in range(self.dim):\n                if eval_count >= self.budget:\n                    break\n                \n                # Try positive direction\n                x_plus = self.x_opt.copy()\n                x_plus[i] += step_size\n                x_plus[i] = np.clip(x_plus[i], func.bounds.lb[i], func.bounds.ub[i])  # Clip to bounds\n                \n                f_plus = func(x_plus)\n                eval_count += 1\n                \n                if f_plus < self.f_current:\n                    self.x_opt = x_plus\n                    self.f_current = f_plus\n                    self.f_opt = f_plus\n                    continue  # Move to next dimension if improvement found\n\n                if eval_count >= self.budget:\n                    break\n\n                # Try negative direction\n                x_minus = self.x_opt.copy()\n                x_minus[i] -= step_size\n                x_minus[i] = np.clip(x_minus[i], func.bounds.lb[i], func.bounds.ub[i])  # Clip to bounds\n\n                f_minus = func(x_minus)\n                eval_count += 1\n                \n                if f_minus < self.f_current:\n                    self.x_opt = x_minus\n                    self.f_current = f_minus\n                    self.f_opt = f_minus\n                else:\n                    # No improvement, reduce step size\n                    step_size *= 0.9  # Reduce step size if no improvement\n\n                if step_size < 1e-6:\n                    step_size = self.initial_step_size  # Reset to initial if too small\n            \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "In the code, line 31, in __call__, the following error occurred:\nAttributeError: 'AdaptiveCoordinateDescent' object has no attribute 'f_current'\nOn line: if f_plus < self.f_current:", "error": "In the code, line 31, in __call__, the following error occurred:\nAttributeError: 'AdaptiveCoordinateDescent' object has no attribute 'f_current'\nOn line: if f_plus < self.f_current:", "parent_ids": [], "operator": "random_new", "metadata": {"llm_response": "# Description: Adaptive Coordinate Descent with shrinking step size based on function evaluations.\n# Code: \n```python\nimport numpy as np\n\nclass AdaptiveCoordinateDescent:\n    def __init__(self, budget=10000, dim=10, initial_step_size=1.0):\n        self.budget = budget\n        self.dim = dim\n        self.initial_step_size = initial_step_size\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = np.random.uniform(func.bounds.lb, func.bounds.ub)  # Initialize randomly\n        f_current = func(self.x_opt)\n        self.f_opt = f_current\n\n        step_size = self.initial_step_size\n        eval_count = 1  # Account for initial evaluation\n\n        while eval_count < self.budget:\n            for i in range(self.dim):\n                if eval_count >= self.budget:\n                    break\n                \n                # Try positive direction\n                x_plus = self.x_opt.copy()\n                x_plus[i] += step_size\n                x_plus[i] = np.clip(x_plus[i], func.bounds.lb[i], func.bounds.ub[i])  # Clip to bounds\n                \n                f_plus = func(x_plus)\n                eval_count += 1\n                \n                if f_plus < self.f_current:\n                    self.x_opt = x_plus\n                    self.f_current = f_plus\n                    self.f_opt = f_plus\n                    continue  # Move to next dimension if improvement found\n\n                if eval_count >= self.budget:\n                    break\n\n                # Try negative direction\n                x_minus = self.x_opt.copy()\n                x_minus[i] -= step_size\n                x_minus[i] = np.clip(x_minus[i], func.bounds.lb[i], func.bounds.ub[i])  # Clip to bounds\n\n                f_minus = func(x_minus)\n                eval_count += 1\n                \n                if f_minus < self.f_current:\n                    self.x_opt = x_minus\n                    self.f_current = f_minus\n                    self.f_opt = f_minus\n                else:\n                    # No improvement, reduce step size\n                    step_size *= 0.9  # Reduce step size if no improvement\n\n                if step_size < 1e-6:\n                    step_size = self.initial_step_size  # Reset to initial if too small\n            \n        return self.f_opt, self.x_opt\n```", "operator": "random_new", "theta_sampled": 0.345890397846714, "generation": 1}, "task_prompt": ""}
{"id": "e9252716-c8bb-41ad-93b0-8bac9015a54b", "fitness": 0.0, "name": "RankBasedDE", "description": "Self-adjusting Differential Evolution with rank-based mutation and exploration intensification.", "code": "import numpy as np\n\nclass RankBasedDE:\n    def __init__(self, budget=10000, dim=10, pop_size=None, F=0.5, Cr=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size if pop_size is not None else 10*dim\n        self.F = F\n        self.Cr = Cr\n        self.lb = -5.0\n        self.ub = 5.0\n        self.local_search_prob = 0.1\n        self.ls_radius = 0.1\n        self.exploration_prob = 0.1\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.pop = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.nevals = self.pop_size\n\n        while self.nevals < self.budget:\n            # Sort population by fitness\n            ranked_indices = np.argsort(self.fitness)\n            ranked_pop = self.pop[ranked_indices]\n\n            for i in range(self.pop_size):\n                # Mutation: Rank-based selection of individuals for mutation\n                pbest_index = ranked_indices[0] # Index of the best individual\n                idxs = np.random.choice(self.pop_size, 2, replace=False)\n                x_r1, x_r2 = ranked_pop[idxs]\n                \n                # Create mutant vector\n                x_mutated = self.pop[i] + self.F * (x_r1 - x_r2)\n\n                # Exploration: Add a random component to the mutation to improve exploration\n                if np.random.rand() < self.exploration_prob:\n                  x_mutated = x_mutated + np.random.uniform(-self.ls_radius, self.ls_radius, size=self.dim)\n\n                x_mutated = np.clip(x_mutated, self.lb, self.ub)\n\n                # Crossover\n                x_trial = np.copy(self.pop[i])\n                crossover_points = np.random.rand(self.dim) < self.Cr\n                if not np.any(crossover_points):\n                    crossover_points[np.random.randint(0, self.dim)] = True\n                x_trial[crossover_points] = x_mutated[crossover_points]\n                \n                # Evaluation\n                f_trial = func(x_trial)\n                self.nevals += 1\n\n                # Selection\n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.pop[i] = x_trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = x_trial\n\n                # Local Search (Probabilistic)\n                if np.random.rand() < self.local_search_prob and self.nevals < self.budget:\n                    x_ls = np.random.uniform(low=np.maximum(self.pop[i] - self.ls_radius, self.lb),\n                                            high=np.minimum(self.pop[i] + self.ls_radius, self.ub))\n                    f_ls = func(x_ls)\n                    self.nevals += 1\n\n                    if f_ls < self.fitness[i]:\n                        self.fitness[i] = f_ls\n                        self.pop[i] = x_ls\n                        if f_ls < self.f_opt:\n                            self.f_opt = f_ls\n                            self.x_opt = x_ls\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "The algorithm RankBasedDE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["dd0f1f2e-9fde-491b-90f4-59ed0fe87d71", "dd0f1f2e-9fde-491b-90f4-59ed0fe87d71"], "operator": "crossover", "metadata": {"llm_response": "# Description: Self-adjusting Differential Evolution with rank-based mutation and exploration intensification.\n# Code: \n```python\nimport numpy as np\n\nclass RankBasedDE:\n    def __init__(self, budget=10000, dim=10, pop_size=None, F=0.5, Cr=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size if pop_size is not None else 10*dim\n        self.F = F\n        self.Cr = Cr\n        self.lb = -5.0\n        self.ub = 5.0\n        self.local_search_prob = 0.1\n        self.ls_radius = 0.1\n        self.exploration_prob = 0.1\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.pop = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.nevals = self.pop_size\n\n        while self.nevals < self.budget:\n            # Sort population by fitness\n            ranked_indices = np.argsort(self.fitness)\n            ranked_pop = self.pop[ranked_indices]\n\n            for i in range(self.pop_size):\n                # Mutation: Rank-based selection of individuals for mutation\n                pbest_index = ranked_indices[0] # Index of the best individual\n                idxs = np.random.choice(self.pop_size, 2, replace=False)\n                x_r1, x_r2 = ranked_pop[idxs]\n                \n                # Create mutant vector\n                x_mutated = self.pop[i] + self.F * (x_r1 - x_r2)\n\n                # Exploration: Add a random component to the mutation to improve exploration\n                if np.random.rand() < self.exploration_prob:\n                  x_mutated = x_mutated + np.random.uniform(-self.ls_radius, self.ls_radius, size=self.dim)\n\n                x_mutated = np.clip(x_mutated, self.lb, self.ub)\n\n                # Crossover\n                x_trial = np.copy(self.pop[i])\n                crossover_points = np.random.rand(self.dim) < self.Cr\n                if not np.any(crossover_points):\n                    crossover_points[np.random.randint(0, self.dim)] = True\n                x_trial[crossover_points] = x_mutated[crossover_points]\n                \n                # Evaluation\n                f_trial = func(x_trial)\n                self.nevals += 1\n\n                # Selection\n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.pop[i] = x_trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = x_trial\n\n                # Local Search (Probabilistic)\n                if np.random.rand() < self.local_search_prob and self.nevals < self.budget:\n                    x_ls = np.random.uniform(low=np.maximum(self.pop[i] - self.ls_radius, self.lb),\n                                            high=np.minimum(self.pop[i] + self.ls_radius, self.ub))\n                    f_ls = func(x_ls)\n                    self.nevals += 1\n\n                    if f_ls < self.fitness[i]:\n                        self.fitness[i] = f_ls\n                        self.pop[i] = x_ls\n                        if f_ls < self.f_opt:\n                            self.f_opt = f_ls\n                            self.x_opt = x_ls\n\n        return self.f_opt, self.x_opt\n```", "operator": "crossover", "theta_sampled": 0.10242481151585839, "generation": 1, "aucs": [0]}, "task_prompt": ""}
{"id": "1460e948-3e5f-4e35-abf6-cee842f44f89", "fitness": 0.41441230704879206, "name": "AdaptiveSelfAdjustingDE", "description": "Self-Adjusting DE with adaptive F and Cr parameters based on the success of previous generations.", "code": "import numpy as np\n\nclass AdaptiveSelfAdjustingDE:\n    def __init__(self, budget=10000, dim=10, pop_size=None, F=0.5, Cr=0.9, F_adapt_rate=0.1, Cr_adapt_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size if pop_size is not None else 10*dim\n        self.F = F\n        self.Cr = Cr\n        self.lb = -5.0\n        self.ub = 5.0\n        self.F_adapt_rate = F_adapt_rate\n        self.Cr_adapt_rate = Cr_adapt_rate\n        self.archive_F = []\n        self.archive_Cr = []\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.pop = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.nevals = self.pop_size\n\n        while self.nevals < self.budget:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = self.pop[idxs]\n                x_mutated = self.pop[i] + self.F * (x_r2 - x_r3)\n                x_mutated = np.clip(x_mutated, self.lb, self.ub)\n\n                # Crossover\n                x_trial = np.copy(self.pop[i])\n                crossover_points = np.random.rand(self.dim) < self.Cr\n                if not np.any(crossover_points):\n                    crossover_points[np.random.randint(0, self.dim)] = True\n                x_trial[crossover_points] = x_mutated[crossover_points]\n\n                # Evaluation\n                f_trial = func(x_trial)\n                self.nevals += 1\n\n                # Selection\n                if f_trial < self.fitness[i]:\n                    self.archive_F.append(self.F)\n                    self.archive_Cr.append(self.Cr)\n\n                    self.fitness[i] = f_trial\n                    self.pop[i] = x_trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = x_trial\n            \n            #Adapt F and Cr\n            if len(self.archive_F) > 0:\n                self.F = 0.9 * self.F + 0.1 * np.mean(self.archive_F)\n                self.Cr = 0.9 * self.Cr + 0.1 * np.mean(self.archive_Cr)\n                self.archive_F = []\n                self.archive_Cr = []\n            self.F = np.clip(self.F, 0.0, 1.0)\n            self.Cr = np.clip(self.Cr, 0.0, 1.0)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "The algorithm AdaptiveSelfAdjustingDE scored 0.414 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["dd0f1f2e-9fde-491b-90f4-59ed0fe87d71", "dd0f1f2e-9fde-491b-90f4-59ed0fe87d71"], "operator": "crossover", "metadata": {"llm_response": "# Description: Self-Adjusting DE with adaptive F and Cr parameters based on the success of previous generations.\n# Code: \n```python\nimport numpy as np\n\nclass AdaptiveSelfAdjustingDE:\n    def __init__(self, budget=10000, dim=10, pop_size=None, F=0.5, Cr=0.9, F_adapt_rate=0.1, Cr_adapt_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size if pop_size is not None else 10*dim\n        self.F = F\n        self.Cr = Cr\n        self.lb = -5.0\n        self.ub = 5.0\n        self.F_adapt_rate = F_adapt_rate\n        self.Cr_adapt_rate = Cr_adapt_rate\n        self.archive_F = []\n        self.archive_Cr = []\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.pop = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.nevals = self.pop_size\n\n        while self.nevals < self.budget:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = self.pop[idxs]\n                x_mutated = self.pop[i] + self.F * (x_r2 - x_r3)\n                x_mutated = np.clip(x_mutated, self.lb, self.ub)\n\n                # Crossover\n                x_trial = np.copy(self.pop[i])\n                crossover_points = np.random.rand(self.dim) < self.Cr\n                if not np.any(crossover_points):\n                    crossover_points[np.random.randint(0, self.dim)] = True\n                x_trial[crossover_points] = x_mutated[crossover_points]\n\n                # Evaluation\n                f_trial = func(x_trial)\n                self.nevals += 1\n\n                # Selection\n                if f_trial < self.fitness[i]:\n                    self.archive_F.append(self.F)\n                    self.archive_Cr.append(self.Cr)\n\n                    self.fitness[i] = f_trial\n                    self.pop[i] = x_trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = x_trial\n            \n            #Adapt F and Cr\n            if len(self.archive_F) > 0:\n                self.F = 0.9 * self.F + 0.1 * np.mean(self.archive_F)\n                self.Cr = 0.9 * self.Cr + 0.1 * np.mean(self.archive_Cr)\n                self.archive_F = []\n                self.archive_Cr = []\n            self.F = np.clip(self.F, 0.0, 1.0)\n            self.Cr = np.clip(self.Cr, 0.0, 1.0)\n\n        return self.f_opt, self.x_opt\n```", "operator": "crossover", "theta_sampled": -0.011783538495884417, "generation": 1, "aucs": [0.15488883024635614, 0.2787936171559532, 0.38723296429156506, 0.49008758081460935, 0.290406359867317, 0.45279586167860364, 0.30128504199709516, 0.3507654456474586, 0.33653335447976396, 0.1897164443532311, 0.6272461541117125, 0.9979982375783446, 0.38271477235509277, 0.3108568957189335, 0.7237731668660234, 0.44372694078194175, 0.3460826238692021, 0.5535459030833371, 0.18418623615553165, 0.4856097099237692]}, "task_prompt": ""}
{"id": "4429293f-355e-479c-86cf-30c75aa07c98", "fitness": "-inf", "name": "SelfAdjustingDE_Archive", "description": "Self-Adjusting DE with archive and rank-based F/Cr adaptation.", "code": "import numpy as np\n\nclass SelfAdjustingDE_Archive:\n    def __init__(self, budget=10000, dim=10, pop_size=None, F=0.5, Cr=0.9, archive_size=5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size if pop_size is not None else 10 * dim\n        self.F = F\n        self.Cr = Cr\n        self.lb = -5.0\n        self.ub = 5.0\n        self.local_search_prob = 0.1\n        self.F_adapt_prob = 0.1\n        self.Cr_adapt_prob = 0.1\n        self.ls_radius = 0.1\n        self.archive_size = archive_size\n        self.archive = []\n        self.archive_fitness = []\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.pop = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.nevals = self.pop_size  # Increment evaluation count\n\n        while self.nevals < self.budget:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = self.pop[idxs]\n\n                # Rank-based adaptation of F\n                rank = np.argsort(self.fitness)\n                best_idx = rank[0]\n                worst_idx = rank[-1]\n                F = self.F + 0.1 * np.random.randn()  # Add some noise\n                F = np.clip(F, 0.0, 1.0)\n                    \n\n                x_mutated = self.pop[i] + F * (x_r2 - x_r3)\n                x_mutated = np.clip(x_mutated, self.lb, self.ub)\n\n                # Crossover\n                x_trial = np.copy(self.pop[i])\n\n                # Rank-based adaptation of Cr\n                Cr = self.Cr + 0.05 * np.random.randn()\n                Cr = np.clip(Cr, 0.0, 1.0)\n                \n\n                crossover_points = np.random.rand(self.dim) < Cr\n                if not np.any(crossover_points):\n                    crossover_points[np.random.randint(0, self.dim)] = True\n                x_trial[crossover_points] = x_mutated[crossover_points]\n\n                # Evaluation\n                f_trial = func(x_trial)\n                self.nevals += 1\n\n                # Selection\n                if f_trial < self.fitness[i]:\n                    # Update archive\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(self.pop[i].copy())\n                        self.archive_fitness.append(self.fitness[i])\n                    else:\n                        max_archive_fitness_idx = np.argmax(self.archive_fitness)\n                        if self.fitness[i] < self.archive_fitness[max_archive_fitness_idx]:\n                            self.archive[max_archive_fitness_idx] = self.pop[i].copy()\n                            self.archive_fitness[max_archive_fitness_idx] = self.fitness[i]\n\n                    self.fitness[i] = f_trial\n                    self.pop[i] = x_trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = x_trial\n\n                # Local Search (Probabilistic)\n                if np.random.rand() < self.local_search_prob and self.nevals < self.budget:\n                    x_ls = np.random.uniform(low=np.maximum(self.pop[i] - self.ls_radius, self.lb),\n                                            high=np.minimum(self.pop[i] + self.ls_radius, self.ub))\n                    f_ls = func(x_ls)\n                    self.nevals += 1\n\n                    if f_ls < self.fitness[i]:\n                        \n                        if len(self.archive) < self.archive_size:\n                            self.archive.append(self.pop[i].copy())\n                            self.archive_fitness.append(self.fitness[i])\n                        else:\n                            max_archive_fitness_idx = np.argmax(self.archive_fitness)\n                            if self.fitness[i] < self.archive_fitness[max_archive_fitness_idx]:\n                                self.archive[max_archive_fitness_idx] = self.pop[i].copy()\n                                self.archive_fitness[max_archive_fitness_idx] = self.fitness[i]\n                        \n                        self.fitness[i] = f_ls\n                        self.pop[i] = x_ls\n                        if f_ls < self.f_opt:\n                            self.f_opt = f_ls\n                            self.x_opt = x_ls\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "Evaluation timed out after 300 seconds.", "error": "Evaluation timed out after 300 seconds.", "parent_ids": ["dd0f1f2e-9fde-491b-90f4-59ed0fe87d71", "dd0f1f2e-9fde-491b-90f4-59ed0fe87d71"], "operator": "crossover", "metadata": {"llm_response": "# Description: Self-Adjusting DE with archive and rank-based F/Cr adaptation.\n# Code: \n```python\nimport numpy as np\n\nclass SelfAdjustingDE_Archive:\n    def __init__(self, budget=10000, dim=10, pop_size=None, F=0.5, Cr=0.9, archive_size=5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size if pop_size is not None else 10 * dim\n        self.F = F\n        self.Cr = Cr\n        self.lb = -5.0\n        self.ub = 5.0\n        self.local_search_prob = 0.1\n        self.F_adapt_prob = 0.1\n        self.Cr_adapt_prob = 0.1\n        self.ls_radius = 0.1\n        self.archive_size = archive_size\n        self.archive = []\n        self.archive_fitness = []\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.pop = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.nevals = self.pop_size  # Increment evaluation count\n\n        while self.nevals < self.budget:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = self.pop[idxs]\n\n                # Rank-based adaptation of F\n                rank = np.argsort(self.fitness)\n                best_idx = rank[0]\n                worst_idx = rank[-1]\n                F = self.F + 0.1 * np.random.randn()  # Add some noise\n                F = np.clip(F, 0.0, 1.0)\n                    \n\n                x_mutated = self.pop[i] + F * (x_r2 - x_r3)\n                x_mutated = np.clip(x_mutated, self.lb, self.ub)\n\n                # Crossover\n                x_trial = np.copy(self.pop[i])\n\n                # Rank-based adaptation of Cr\n                Cr = self.Cr + 0.05 * np.random.randn()\n                Cr = np.clip(Cr, 0.0, 1.0)\n                \n\n                crossover_points = np.random.rand(self.dim) < Cr\n                if not np.any(crossover_points):\n                    crossover_points[np.random.randint(0, self.dim)] = True\n                x_trial[crossover_points] = x_mutated[crossover_points]\n\n                # Evaluation\n                f_trial = func(x_trial)\n                self.nevals += 1\n\n                # Selection\n                if f_trial < self.fitness[i]:\n                    # Update archive\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(self.pop[i].copy())\n                        self.archive_fitness.append(self.fitness[i])\n                    else:\n                        max_archive_fitness_idx = np.argmax(self.archive_fitness)\n                        if self.fitness[i] < self.archive_fitness[max_archive_fitness_idx]:\n                            self.archive[max_archive_fitness_idx] = self.pop[i].copy()\n                            self.archive_fitness[max_archive_fitness_idx] = self.fitness[i]\n\n                    self.fitness[i] = f_trial\n                    self.pop[i] = x_trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = x_trial\n\n                # Local Search (Probabilistic)\n                if np.random.rand() < self.local_search_prob and self.nevals < self.budget:\n                    x_ls = np.random.uniform(low=np.maximum(self.pop[i] - self.ls_radius, self.lb),\n                                            high=np.minimum(self.pop[i] + self.ls_radius, self.ub))\n                    f_ls = func(x_ls)\n                    self.nevals += 1\n\n                    if f_ls < self.fitness[i]:\n                        \n                        if len(self.archive) < self.archive_size:\n                            self.archive.append(self.pop[i].copy())\n                            self.archive_fitness.append(self.fitness[i])\n                        else:\n                            max_archive_fitness_idx = np.argmax(self.archive_fitness)\n                            if self.fitness[i] < self.archive_fitness[max_archive_fitness_idx]:\n                                self.archive[max_archive_fitness_idx] = self.pop[i].copy()\n                                self.archive_fitness[max_archive_fitness_idx] = self.fitness[i]\n                        \n                        self.fitness[i] = f_ls\n                        self.pop[i] = x_ls\n                        if f_ls < self.f_opt:\n                            self.f_opt = f_ls\n                            self.x_opt = x_ls\n\n        return self.f_opt, self.x_opt\n```", "operator": "crossover", "theta_sampled": 0.2303241181967514, "generation": 1}, "task_prompt": ""}
{"id": "4b1b3244-c858-4729-9a9c-a34c74faa55a", "fitness": 0.37592801433903833, "name": "AdaptiveDESuccess", "description": "Self-adjusting Differential Evolution with adaptive mutation factor F based on success.", "code": "import numpy as np\n\nclass AdaptiveDESuccess:\n    def __init__(self, budget=10000, dim=10, pop_size=None, Cr=0.9, F_init=0.5, F_adapt_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size if pop_size is not None else 10*dim\n        self.Cr = Cr\n        self.F = np.full(self.pop_size, F_init)  # Individual F for each population member\n        self.F_init = F_init\n        self.F_adapt_rate = F_adapt_rate\n        self.lb = -5.0\n        self.ub = 5.0\n        self.nevals = 0\n        self.success_F = []\n        self.success_count = 0\n        self.x_opt = None\n        self.f_opt = np.inf\n        self.pop = None\n        self.fitness = None\n\n    def __call__(self, func):\n        self.pop = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.nevals = self.pop_size\n\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.pop[np.argmin(self.fitness)]\n\n        while self.nevals < self.budget:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = self.pop[idxs]\n                \n                x_mutated = self.pop[i] + self.F[i] * (x_r2 - x_r3)\n                x_mutated = np.clip(x_mutated, self.lb, self.ub)\n\n                # Crossover\n                x_trial = np.copy(self.pop[i])\n                crossover_points = np.random.rand(self.dim) < self.Cr\n                if not np.any(crossover_points):\n                    crossover_points[np.random.randint(0, self.dim)] = True\n                x_trial[crossover_points] = x_mutated[crossover_points]\n                \n                # Evaluation\n                f_trial = func(x_trial)\n                self.nevals += 1\n\n                # Selection and F adaptation\n                if f_trial < self.fitness[i]:\n                    self.success_count += 1\n                    self.fitness[i] = f_trial\n                    self.pop[i] = x_trial\n                    self.success_F.append(self.F[i])\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = x_trial\n                    \n                    # Adapt F based on success\n                    self.F[i] = np.random.normal(loc=self.F_init, scale=0.3)\n                    self.F[i] = np.clip(self.F[i], 0.0, 1.0)\n                else:\n                    #If unsuccessful, decrease F\n                    self.F[i] *= (1 - self.F_adapt_rate)\n                    self.F[i] = np.clip(self.F[i], 0.0, 1.0)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "The algorithm AdaptiveDESuccess scored 0.376 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["dd0f1f2e-9fde-491b-90f4-59ed0fe87d71", "dd0f1f2e-9fde-491b-90f4-59ed0fe87d71"], "operator": "crossover", "metadata": {"llm_response": "# Description: Self-adjusting Differential Evolution with adaptive mutation factor F based on success.\n# Code: \n```python\nimport numpy as np\n\nclass AdaptiveDESuccess:\n    def __init__(self, budget=10000, dim=10, pop_size=None, Cr=0.9, F_init=0.5, F_adapt_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size if pop_size is not None else 10*dim\n        self.Cr = Cr\n        self.F = np.full(self.pop_size, F_init)  # Individual F for each population member\n        self.F_init = F_init\n        self.F_adapt_rate = F_adapt_rate\n        self.lb = -5.0\n        self.ub = 5.0\n        self.nevals = 0\n        self.success_F = []\n        self.success_count = 0\n        self.x_opt = None\n        self.f_opt = np.inf\n        self.pop = None\n        self.fitness = None\n\n    def __call__(self, func):\n        self.pop = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.nevals = self.pop_size\n\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.pop[np.argmin(self.fitness)]\n\n        while self.nevals < self.budget:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = self.pop[idxs]\n                \n                x_mutated = self.pop[i] + self.F[i] * (x_r2 - x_r3)\n                x_mutated = np.clip(x_mutated, self.lb, self.ub)\n\n                # Crossover\n                x_trial = np.copy(self.pop[i])\n                crossover_points = np.random.rand(self.dim) < self.Cr\n                if not np.any(crossover_points):\n                    crossover_points[np.random.randint(0, self.dim)] = True\n                x_trial[crossover_points] = x_mutated[crossover_points]\n                \n                # Evaluation\n                f_trial = func(x_trial)\n                self.nevals += 1\n\n                # Selection and F adaptation\n                if f_trial < self.fitness[i]:\n                    self.success_count += 1\n                    self.fitness[i] = f_trial\n                    self.pop[i] = x_trial\n                    self.success_F.append(self.F[i])\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = x_trial\n                    \n                    # Adapt F based on success\n                    self.F[i] = np.random.normal(loc=self.F_init, scale=0.3)\n                    self.F[i] = np.clip(self.F[i], 0.0, 1.0)\n                else:\n                    #If unsuccessful, decrease F\n                    self.F[i] *= (1 - self.F_adapt_rate)\n                    self.F[i] = np.clip(self.F[i], 0.0, 1.0)\n\n        return self.f_opt, self.x_opt\n```", "operator": "crossover", "theta_sampled": 0.02481612266841858, "generation": 1, "aucs": [0.1377108297880919, 0.27249539354859664, 0.3545231327876628, 0.3854115145226338, 0.2794750584797202, 0.404281951701973, 0.30553179437805045, 0.31466686687018475, 0.3141680220868083, 0.17969388638749384, 0.3641992142512994, 0.9994941941548432, 0.3670043268898384, 0.28462862637972, 0.7206943321746717, 0.37141978630227446, 0.32898026739983466, 0.45024229951305805, 0.19618012646823313, 0.48775866269577817]}, "task_prompt": ""}
{"id": "9ab22f69-1bbe-4036-b670-38eb73b8c4c8", "fitness": "-inf", "name": "AdaptivePSO_DE", "description": "A population-based algorithm that combines elements of Particle Swarm Optimization (PSO) and Differential Evolution (DE) with adaptive parameter control based on the success of previous steps.", "code": "import numpy as np\n\nclass AdaptivePSO_DE:\n    def __init__(self, budget=10000, dim=10, pop_size=20, w_init=0.9, c1_init=2.0, c2_init=2.0, cr_init=0.5, f_init=0.8):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.w = w_init\n        self.c1 = c1_init\n        self.c2 = c2_init\n        self.cr = cr_init\n        self.f = f_init\n        self.pop = None\n        self.fitness = None\n        self.pbest_pos = None\n        self.pbest_fitness = None\n        self.gbest_pos = None\n        self.gbest_fitness = np.inf\n        self.eval_count = 0\n        self.success_history = []\n\n    def initialize_population(self, func):\n        self.pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.eval_count += self.pop_size\n        self.pbest_pos = self.pop.copy()\n        self.pbest_fitness = self.fitness.copy()\n        self.gbest_pos = self.pop[np.argmin(self.fitness)].copy()\n        self.gbest_fitness = np.min(self.fitness)\n\n    def update_parameters(self):\n        # Adaptive parameter control based on recent success\n        if len(self.success_history) > 5:\n            success_rate = np.mean(self.success_history[-5:])\n            self.w = max(0.4, min(0.9, self.w + 0.1 * (success_rate - 0.5)))\n            self.c1 = max(1.0, min(2.5, self.c1 - 0.05 * (success_rate - 0.5)))\n            self.c2 = max(1.0, min(2.5, self.c2 - 0.05 * (success_rate - 0.5)))\n            self.cr = max(0.1, min(0.9, self.cr + 0.1 * (success_rate - 0.5)))\n            self.f = max(0.1, min(1.0, self.f + 0.1 * (success_rate - 0.5)))\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.eval_count < self.budget:\n            self.update_parameters()\n\n            for i in range(self.pop_size):\n                # PSO component\n                v = self.w * (self.pop[i] - self.pop[i]) + self.c1 * np.random.rand(self.dim) * (self.pbest_pos[i] - self.pop[i]) + self.c2 * np.random.rand(self.dim) * (self.gbest_pos - self.pop[i])\n                new_pos_pso = self.pop[i] + v\n                new_pos_pso = np.clip(new_pos_pso, func.bounds.lb, func.bounds.ub)\n                \n                # DE component\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x1, x2, x3 = self.pop[idxs[0]], self.pop[idxs[1]], self.pop[idxs[2]]\n                diff = x2 - x3\n                mutated = x1 + self.f * diff\n                mutated = np.clip(mutated, func.bounds.lb, func.bounds.ub)\n\n                j_rand = np.random.randint(self.dim)\n                new_pos_de = self.pop[i].copy()\n                for j in range(self.dim):\n                    if np.random.rand() < self.cr or j == j_rand:\n                        new_pos_de[j] = mutated[j]\n\n                # Evaluation\n                f_pso = func(new_pos_pso)\n                f_de = func(new_pos_de)\n\n                self.eval_count += 2\n\n                # Selection\n                if f_pso < f_de:\n                    new_pos = new_pos_pso\n                    f_new = f_pso\n                else:\n                    new_pos = new_pos_de\n                    f_new = f_de\n                    \n                if f_new < self.fitness[i]:\n                    self.pop[i] = new_pos\n                    self.fitness[i] = f_new\n\n                    if f_new < self.pbest_fitness[i]:\n                        self.pbest_fitness[i] = f_new\n                        self.pbest_pos[i] = new_pos\n                        \n                        if f_new < self.gbest_fitness:\n                            self.gbest_fitness = f_new\n                            self.gbest_pos = new_pos\n                            self.success_history.append(1)  # Record success\n                            \n                    else:\n                         self.success_history.append(0) # Record failure\n                else:\n                    self.success_history.append(0) # Record failure\n\n                if self.eval_count >= self.budget:\n                   break # Ensure budget is not exceeded\n                \n        return self.gbest_fitness, self.gbest_pos", "configspace": "", "generation": 1, "feedback": "Evaluation timed out after 300 seconds.", "error": "Evaluation timed out after 300 seconds.", "parent_ids": [], "operator": "random_new", "metadata": {"llm_response": "# Description: A population-based algorithm that combines elements of Particle Swarm Optimization (PSO) and Differential Evolution (DE) with adaptive parameter control based on the success of previous steps.\n# Code: \n```python\nimport numpy as np\n\nclass AdaptivePSO_DE:\n    def __init__(self, budget=10000, dim=10, pop_size=20, w_init=0.9, c1_init=2.0, c2_init=2.0, cr_init=0.5, f_init=0.8):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.w = w_init\n        self.c1 = c1_init\n        self.c2 = c2_init\n        self.cr = cr_init\n        self.f = f_init\n        self.pop = None\n        self.fitness = None\n        self.pbest_pos = None\n        self.pbest_fitness = None\n        self.gbest_pos = None\n        self.gbest_fitness = np.inf\n        self.eval_count = 0\n        self.success_history = []\n\n    def initialize_population(self, func):\n        self.pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.eval_count += self.pop_size\n        self.pbest_pos = self.pop.copy()\n        self.pbest_fitness = self.fitness.copy()\n        self.gbest_pos = self.pop[np.argmin(self.fitness)].copy()\n        self.gbest_fitness = np.min(self.fitness)\n\n    def update_parameters(self):\n        # Adaptive parameter control based on recent success\n        if len(self.success_history) > 5:\n            success_rate = np.mean(self.success_history[-5:])\n            self.w = max(0.4, min(0.9, self.w + 0.1 * (success_rate - 0.5)))\n            self.c1 = max(1.0, min(2.5, self.c1 - 0.05 * (success_rate - 0.5)))\n            self.c2 = max(1.0, min(2.5, self.c2 - 0.05 * (success_rate - 0.5)))\n            self.cr = max(0.1, min(0.9, self.cr + 0.1 * (success_rate - 0.5)))\n            self.f = max(0.1, min(1.0, self.f + 0.1 * (success_rate - 0.5)))\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.eval_count < self.budget:\n            self.update_parameters()\n\n            for i in range(self.pop_size):\n                # PSO component\n                v = self.w * (self.pop[i] - self.pop[i]) + self.c1 * np.random.rand(self.dim) * (self.pbest_pos[i] - self.pop[i]) + self.c2 * np.random.rand(self.dim) * (self.gbest_pos - self.pop[i])\n                new_pos_pso = self.pop[i] + v\n                new_pos_pso = np.clip(new_pos_pso, func.bounds.lb, func.bounds.ub)\n                \n                # DE component\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x1, x2, x3 = self.pop[idxs[0]], self.pop[idxs[1]], self.pop[idxs[2]]\n                diff = x2 - x3\n                mutated = x1 + self.f * diff\n                mutated = np.clip(mutated, func.bounds.lb, func.bounds.ub)\n\n                j_rand = np.random.randint(self.dim)\n                new_pos_de = self.pop[i].copy()\n                for j in range(self.dim):\n                    if np.random.rand() < self.cr or j == j_rand:\n                        new_pos_de[j] = mutated[j]\n\n                # Evaluation\n                f_pso = func(new_pos_pso)\n                f_de = func(new_pos_de)\n\n                self.eval_count += 2\n\n                # Selection\n                if f_pso < f_de:\n                    new_pos = new_pos_pso\n                    f_new = f_pso\n                else:\n                    new_pos = new_pos_de\n                    f_new = f_de\n                    \n                if f_new < self.fitness[i]:\n                    self.pop[i] = new_pos\n                    self.fitness[i] = f_new\n\n                    if f_new < self.pbest_fitness[i]:\n                        self.pbest_fitness[i] = f_new\n                        self.pbest_pos[i] = new_pos\n                        \n                        if f_new < self.gbest_fitness:\n                            self.gbest_fitness = f_new\n                            self.gbest_pos = new_pos\n                            self.success_history.append(1)  # Record success\n                            \n                    else:\n                         self.success_history.append(0) # Record failure\n                else:\n                    self.success_history.append(0) # Record failure\n\n                if self.eval_count >= self.budget:\n                   break # Ensure budget is not exceeded\n                \n        return self.gbest_fitness, self.gbest_pos\n```", "operator": "random_new", "theta_sampled": 0.18305069491046252, "generation": 1}, "task_prompt": ""}
{"id": "bb54b16a-0e18-4249-ae8d-be44e91068c8", "fitness": 0.4184725907234939, "name": "AdaptiveDE", "description": "Self-Adjusting DE with adaptive F and Cr parameters based on success rate.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=None, F_init=0.5, Cr_init=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size if pop_size is not None else 10 * dim\n        self.F = F_init\n        self.Cr = Cr_init\n        self.lb = -5.0\n        self.ub = 5.0\n        self.memory_size = 10  # Size of memory for successful F and Cr values\n        self.F_memory = np.full(self.memory_size, F_init)\n        self.Cr_memory = np.full(self.memory_size, Cr_init)\n        self.F_idx = 0\n        self.Cr_idx = 0\n        self.success_F = []\n        self.success_Cr = []\n        self.p_selection = 0.1\n        self.epsilon = 1e-6\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.pop = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.nevals = self.pop_size\n\n        while self.nevals < self.budget:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = self.pop[idxs]\n\n                x_mutated = self.pop[i] + self.F * (x_r2 - x_r3)\n                x_mutated = np.clip(x_mutated, self.lb, self.ub)\n\n                # Crossover\n                x_trial = np.copy(self.pop[i])\n                crossover_points = np.random.rand(self.dim) < self.Cr\n                if not np.any(crossover_points):\n                    crossover_points[np.random.randint(0, self.dim)] = True\n                x_trial[crossover_points] = x_mutated[crossover_points]\n\n                # Evaluation\n                f_trial = func(x_trial)\n                self.nevals += 1\n\n                # Selection\n                if f_trial < self.fitness[i]:\n                    # Store successful F and Cr\n                    self.success_F.append(self.F)\n                    self.success_Cr.append(self.Cr)\n                    \n                    self.fitness[i] = f_trial\n                    self.pop[i] = x_trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = x_trial\n\n            # Adapt F and Cr (after each generation)\n            self.adapt_parameters()\n\n        return self.f_opt, self.x_opt\n\n    def adapt_parameters(self):\n        if self.success_F:\n            # Update F using a weighted average of successful values\n            self.F = np.mean(self.success_F)\n            self.F = np.clip(self.F, 0.0, 1.0)\n            self.success_F = []  # Reset successful F values for the next generation\n\n        if self.success_Cr:\n            # Update Cr using a weighted average of successful values\n            self.Cr = np.mean(self.success_Cr)\n            self.Cr = np.clip(self.Cr, 0.0, 1.0)\n            self.success_Cr = []  # Reset successful Cr values for the next generation", "configspace": "", "generation": 1, "feedback": "The algorithm AdaptiveDE scored 0.418 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["dd0f1f2e-9fde-491b-90f4-59ed0fe87d71", "dd0f1f2e-9fde-491b-90f4-59ed0fe87d71"], "operator": "crossover", "metadata": {"llm_response": "# Description: Self-Adjusting DE with adaptive F and Cr parameters based on success rate.\n# Code: \n```python\nimport numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=None, F_init=0.5, Cr_init=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size if pop_size is not None else 10 * dim\n        self.F = F_init\n        self.Cr = Cr_init\n        self.lb = -5.0\n        self.ub = 5.0\n        self.memory_size = 10  # Size of memory for successful F and Cr values\n        self.F_memory = np.full(self.memory_size, F_init)\n        self.Cr_memory = np.full(self.memory_size, Cr_init)\n        self.F_idx = 0\n        self.Cr_idx = 0\n        self.success_F = []\n        self.success_Cr = []\n        self.p_selection = 0.1\n        self.epsilon = 1e-6\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.pop = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.nevals = self.pop_size\n\n        while self.nevals < self.budget:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = self.pop[idxs]\n\n                x_mutated = self.pop[i] + self.F * (x_r2 - x_r3)\n                x_mutated = np.clip(x_mutated, self.lb, self.ub)\n\n                # Crossover\n                x_trial = np.copy(self.pop[i])\n                crossover_points = np.random.rand(self.dim) < self.Cr\n                if not np.any(crossover_points):\n                    crossover_points[np.random.randint(0, self.dim)] = True\n                x_trial[crossover_points] = x_mutated[crossover_points]\n\n                # Evaluation\n                f_trial = func(x_trial)\n                self.nevals += 1\n\n                # Selection\n                if f_trial < self.fitness[i]:\n                    # Store successful F and Cr\n                    self.success_F.append(self.F)\n                    self.success_Cr.append(self.Cr)\n                    \n                    self.fitness[i] = f_trial\n                    self.pop[i] = x_trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = x_trial\n\n            # Adapt F and Cr (after each generation)\n            self.adapt_parameters()\n\n        return self.f_opt, self.x_opt\n\n    def adapt_parameters(self):\n        if self.success_F:\n            # Update F using a weighted average of successful values\n            self.F = np.mean(self.success_F)\n            self.F = np.clip(self.F, 0.0, 1.0)\n            self.success_F = []  # Reset successful F values for the next generation\n\n        if self.success_Cr:\n            # Update Cr using a weighted average of successful values\n            self.Cr = np.mean(self.success_Cr)\n            self.Cr = np.clip(self.Cr, 0.0, 1.0)\n            self.success_Cr = []  # Reset successful Cr values for the next generation\n```", "operator": "crossover", "theta_sampled": 0.050118154556698186, "generation": 1, "aucs": [0.16113505533829608, 0.3067980106044559, 0.39850041967755, 0.5955936628720646, 0.3202531433737378, 0.43029386648185985, 0.292687623272221, 0.3436464686441172, 0.33969109956471644, 0.19110945781313693, 0.5913797556127797, 0.9968720142213554, 0.3956114216010189, 0.29999368963849127, 0.7547029010314819, 0.4283143725748916, 0.32190943953111806, 0.5358517866415416, 0.1787786907051725, 0.48632893526987186]}, "task_prompt": ""}
{"id": "f4bfc92c-0bec-4495-a9f6-7b03c011eab4", "fitness": "-inf", "name": "AdaptiveCauchyDE", "description": "An adaptive Differential Evolution algorithm that adjusts its parameters based on the success rate of generating better solutions, incorporating a Cauchy mutation operator for broader exploration.", "code": "import numpy as np\n\nclass AdaptiveCauchyDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, CR=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F  # Mutation factor\n        self.CR = CR # Crossover rate\n        self.population = None\n        self.fitness = None\n        self.lb = None\n        self.ub = None\n        self.success_history = []\n        self.F_history = []\n        self.CR_history = []\n\n    def initialize_population(self, func):\n        self.lb = func.bounds.lb\n        self.ub = func.bounds.ub\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.eval_count = self.pop_size #Initial evaluation count.\n\n    def mutate(self, pop, F):\n        idx = np.random.choice(self.pop_size, size=(self.pop_size, 3), replace=False)\n        a, b, c = pop[idx[:, 0]], pop[idx[:, 1]], pop[idx[:, 2]]\n        cauchy_noise = F * np.random.standard_cauchy(size=(self.pop_size, self.dim))\n        mutated = a + cauchy_noise*(b - c)\n        return np.clip(mutated, self.lb, self.ub)\n\n\n    def crossover(self, pop, mutated, CR):\n        crossover_mask = np.random.rand(self.pop_size, self.dim) < CR\n        trial = np.copy(pop)\n        trial[crossover_mask] = mutated[crossover_mask]\n        return trial\n\n    def selection(self, func, trial):\n        trial_fitness = np.array([func(x) for x in trial])\n        self.eval_count += self.pop_size\n        improved = trial_fitness < self.fitness\n        self.success_history.append(np.sum(improved) / self.pop_size)\n        self.population[improved] = trial[improved]\n        self.fitness[improved] = trial_fitness[improved]\n        return improved\n\n    def adjust_parameters(self):\n          #Adaptive F: Increase exploration after stagnation\n        if len(self.success_history) > 10: #After some iterations\n            recent_success = np.mean(self.success_history[-10:])\n            if recent_success < 0.1: #Stagnation\n                self.F = min(1.0, self.F * 1.2)  #Increase mutation factor\n            else:\n                self.F = max(0.1, self.F * 0.9)  # Reduce mutation factor\n\n        #Adaptive CR: Adjust crossover based on past success.\n        if len(self.success_history) > 10:\n            successful_CRs = [self.CR_history[i] for i in range(len(self.success_history)) if self.success_history[i] > 0.2]\n            if successful_CRs:\n                self.CR = np.mean(successful_CRs)\n            else:\n                self.CR = 0.9 #Default value if no successful CR.\n        \n        self.F_history.append(self.F)\n        self.CR_history.append(self.CR)\n\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        self.eval_count = self.pop_size\n        \n        while self.eval_count < self.budget:\n            mutated = self.mutate(self.population, self.F)\n            trial = self.crossover(self.population, mutated, self.CR)\n            improved = self.selection(func, trial)\n            self.adjust_parameters()\n\n        best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_index]\n        self.x_opt = self.population[best_index]\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "In the code, line 1025, in numpy.random.mtrand.RandomState.choice, the following error occurred:\nValueError: Cannot take a larger sample than population when 'replace=False'", "error": "In the code, line 1025, in numpy.random.mtrand.RandomState.choice, the following error occurred:\nValueError: Cannot take a larger sample than population when 'replace=False'", "parent_ids": [], "operator": "random_new", "metadata": {"llm_response": "# Description: An adaptive Differential Evolution algorithm that adjusts its parameters based on the success rate of generating better solutions, incorporating a Cauchy mutation operator for broader exploration.\n# Code: \n```python\nimport numpy as np\n\nclass AdaptiveCauchyDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, CR=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F  # Mutation factor\n        self.CR = CR # Crossover rate\n        self.population = None\n        self.fitness = None\n        self.lb = None\n        self.ub = None\n        self.success_history = []\n        self.F_history = []\n        self.CR_history = []\n\n    def initialize_population(self, func):\n        self.lb = func.bounds.lb\n        self.ub = func.bounds.ub\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.eval_count = self.pop_size #Initial evaluation count.\n\n    def mutate(self, pop, F):\n        idx = np.random.choice(self.pop_size, size=(self.pop_size, 3), replace=False)\n        a, b, c = pop[idx[:, 0]], pop[idx[:, 1]], pop[idx[:, 2]]\n        cauchy_noise = F * np.random.standard_cauchy(size=(self.pop_size, self.dim))\n        mutated = a + cauchy_noise*(b - c)\n        return np.clip(mutated, self.lb, self.ub)\n\n\n    def crossover(self, pop, mutated, CR):\n        crossover_mask = np.random.rand(self.pop_size, self.dim) < CR\n        trial = np.copy(pop)\n        trial[crossover_mask] = mutated[crossover_mask]\n        return trial\n\n    def selection(self, func, trial):\n        trial_fitness = np.array([func(x) for x in trial])\n        self.eval_count += self.pop_size\n        improved = trial_fitness < self.fitness\n        self.success_history.append(np.sum(improved) / self.pop_size)\n        self.population[improved] = trial[improved]\n        self.fitness[improved] = trial_fitness[improved]\n        return improved\n\n    def adjust_parameters(self):\n          #Adaptive F: Increase exploration after stagnation\n        if len(self.success_history) > 10: #After some iterations\n            recent_success = np.mean(self.success_history[-10:])\n            if recent_success < 0.1: #Stagnation\n                self.F = min(1.0, self.F * 1.2)  #Increase mutation factor\n            else:\n                self.F = max(0.1, self.F * 0.9)  # Reduce mutation factor\n\n        #Adaptive CR: Adjust crossover based on past success.\n        if len(self.success_history) > 10:\n            successful_CRs = [self.CR_history[i] for i in range(len(self.success_history)) if self.success_history[i] > 0.2]\n            if successful_CRs:\n                self.CR = np.mean(successful_CRs)\n            else:\n                self.CR = 0.9 #Default value if no successful CR.\n        \n        self.F_history.append(self.F)\n        self.CR_history.append(self.CR)\n\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        self.eval_count = self.pop_size\n        \n        while self.eval_count < self.budget:\n            mutated = self.mutate(self.population, self.F)\n            trial = self.crossover(self.population, mutated, self.CR)\n            improved = self.selection(func, trial)\n            self.adjust_parameters()\n\n        best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_index]\n        self.x_opt = self.population[best_index]\n        return self.f_opt, self.x_opt\n```", "operator": "random_new", "theta_sampled": 0.3443210543788024, "generation": 1}, "task_prompt": ""}
{"id": "993188b5-7f1b-4df5-b4d9-f6bb1ade6759", "fitness": 0.42722098412508613, "name": "AdaptiveDESmoothed", "description": "Adaptive Differential Evolution with smoothing of F and Cr parameters based on successful values.", "code": "import numpy as np\n\nclass AdaptiveDESmoothed:\n    def __init__(self, budget=10000, dim=10, pop_size=None, F_init=0.5, Cr_init=0.9, smoothing_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size if pop_size is not None else 10 * dim\n        self.F = F_init\n        self.Cr = Cr_init\n        self.lb = -5.0\n        self.ub = 5.0\n        self.smoothing_rate = smoothing_rate\n        self.success_F = []\n        self.success_Cr = []\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.pop = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.nevals = self.pop_size\n\n        while self.nevals < self.budget:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = self.pop[idxs]\n\n                x_mutated = self.pop[i] + self.F * (x_r2 - x_r3)\n                x_mutated = np.clip(x_mutated, self.lb, self.ub)\n\n                # Crossover\n                x_trial = np.copy(self.pop[i])\n                crossover_points = np.random.rand(self.dim) < self.Cr\n                if not np.any(crossover_points):\n                    crossover_points[np.random.randint(0, self.dim)] = True\n                x_trial[crossover_points] = x_mutated[crossover_points]\n\n                # Evaluation\n                f_trial = func(x_trial)\n                self.nevals += 1\n\n                # Selection\n                if f_trial < self.fitness[i]:\n                    # Store successful F and Cr\n                    self.success_F.append(self.F)\n                    self.success_Cr.append(self.Cr)\n                    \n                    self.fitness[i] = f_trial\n                    self.pop[i] = x_trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = x_trial\n\n            # Adapt F and Cr (after each generation)\n            self.adapt_parameters()\n\n        return self.f_opt, self.x_opt\n\n    def adapt_parameters(self):\n        if self.success_F:\n            # Update F using a weighted average of successful values with smoothing\n            mean_F = np.mean(self.success_F)\n            self.F = (1 - self.smoothing_rate) * self.F + self.smoothing_rate * mean_F\n            self.F = np.clip(self.F, 0.0, 1.0)\n            self.success_F = []  # Reset successful F values for the next generation\n\n        if self.success_Cr:\n            # Update Cr using a weighted average of successful values with smoothing\n            mean_Cr = np.mean(self.success_Cr)\n            self.Cr = (1 - self.smoothing_rate) * self.Cr + self.smoothing_rate * mean_Cr\n            self.Cr = np.clip(self.Cr, 0.0, 1.0)\n            self.success_Cr = []  # Reset successful Cr values for the next generation", "configspace": "", "generation": 2, "feedback": "The algorithm AdaptiveDESmoothed scored 0.427 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["bb54b16a-0e18-4249-ae8d-be44e91068c8", "1460e948-3e5f-4e35-abf6-cee842f44f89"], "operator": "crossover", "metadata": {"llm_response": "# Description: Adaptive Differential Evolution with smoothing of F and Cr parameters based on successful values.\n# Code: \n```python\nimport numpy as np\n\nclass AdaptiveDESmoothed:\n    def __init__(self, budget=10000, dim=10, pop_size=None, F_init=0.5, Cr_init=0.9, smoothing_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size if pop_size is not None else 10 * dim\n        self.F = F_init\n        self.Cr = Cr_init\n        self.lb = -5.0\n        self.ub = 5.0\n        self.smoothing_rate = smoothing_rate\n        self.success_F = []\n        self.success_Cr = []\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.pop = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.nevals = self.pop_size\n\n        while self.nevals < self.budget:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = self.pop[idxs]\n\n                x_mutated = self.pop[i] + self.F * (x_r2 - x_r3)\n                x_mutated = np.clip(x_mutated, self.lb, self.ub)\n\n                # Crossover\n                x_trial = np.copy(self.pop[i])\n                crossover_points = np.random.rand(self.dim) < self.Cr\n                if not np.any(crossover_points):\n                    crossover_points[np.random.randint(0, self.dim)] = True\n                x_trial[crossover_points] = x_mutated[crossover_points]\n\n                # Evaluation\n                f_trial = func(x_trial)\n                self.nevals += 1\n\n                # Selection\n                if f_trial < self.fitness[i]:\n                    # Store successful F and Cr\n                    self.success_F.append(self.F)\n                    self.success_Cr.append(self.Cr)\n                    \n                    self.fitness[i] = f_trial\n                    self.pop[i] = x_trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = x_trial\n\n            # Adapt F and Cr (after each generation)\n            self.adapt_parameters()\n\n        return self.f_opt, self.x_opt\n\n    def adapt_parameters(self):\n        if self.success_F:\n            # Update F using a weighted average of successful values with smoothing\n            mean_F = np.mean(self.success_F)\n            self.F = (1 - self.smoothing_rate) * self.F + self.smoothing_rate * mean_F\n            self.F = np.clip(self.F, 0.0, 1.0)\n            self.success_F = []  # Reset successful F values for the next generation\n\n        if self.success_Cr:\n            # Update Cr using a weighted average of successful values with smoothing\n            mean_Cr = np.mean(self.success_Cr)\n            self.Cr = (1 - self.smoothing_rate) * self.Cr + self.smoothing_rate * mean_Cr\n            self.Cr = np.clip(self.Cr, 0.0, 1.0)\n            self.success_Cr = []  # Reset successful Cr values for the next generation\n```", "operator": "crossover", "theta_sampled": 0.3592787315807938, "generation": 2, "aucs": [0.1622218489438756, 0.32435572110991806, 0.387495827033845, 0.562389807866945, 0.31894720122380926, 0.4592877325332386, 0.29872642727703713, 0.35388757168436014, 0.33943955332031106, 0.20984632163411132, 0.6285435605483289, 0.9894002915546414, 0.36952120927598064, 0.31205274295173335, 0.82316514175989, 0.4393417495559476, 0.32062255592817446, 0.5692823620568275, 0.1829114551968859, 0.4929806010458614]}, "task_prompt": ""}
{"id": "79cce7be-a121-4a1c-815e-61b8946185f0", "fitness": 0.4308117715718692, "name": "AdaptiveDEMovingAverage", "description": "Adapts F and Cr based on a moving average of successful values, providing smoother adaptation than immediate averaging.", "code": "import numpy as np\n\nclass AdaptiveDEMovingAverage:\n    def __init__(self, budget=10000, dim=10, pop_size=None, F_init=0.5, Cr_init=0.9, memory_size=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size if pop_size is not None else 10 * dim\n        self.F = F_init\n        self.Cr = Cr_init\n        self.lb = -5.0\n        self.ub = 5.0\n        self.memory_size = memory_size\n        self.F_memory = np.full(self.memory_size, F_init)\n        self.Cr_memory = np.full(self.memory_size, Cr_init)\n        self.F_idx = 0\n        self.Cr_idx = 0\n        self.success_F = []\n        self.success_Cr = []\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.pop = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.nevals = self.pop_size\n\n        while self.nevals < self.budget:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = self.pop[idxs]\n\n                x_mutated = self.pop[i] + self.F * (x_r2 - x_r3)\n                x_mutated = np.clip(x_mutated, self.lb, self.ub)\n\n                # Crossover\n                x_trial = np.copy(self.pop[i])\n                crossover_points = np.random.rand(self.dim) < self.Cr\n                if not np.any(crossover_points):\n                    crossover_points[np.random.randint(0, self.dim)] = True\n                x_trial[crossover_points] = x_mutated[crossover_points]\n\n                # Evaluation\n                f_trial = func(x_trial)\n                self.nevals += 1\n\n                # Selection\n                if f_trial < self.fitness[i]:\n                    # Store successful F and Cr\n                    self.success_F.append(self.F)\n                    self.success_Cr.append(self.Cr)\n                    \n                    self.fitness[i] = f_trial\n                    self.pop[i] = x_trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = x_trial\n\n            # Adapt F and Cr (after each generation)\n            self.adapt_parameters()\n\n        return self.f_opt, self.x_opt\n\n    def adapt_parameters(self):\n        if self.success_F:\n            # Update F using a moving average of successful values\n            self.F_memory[self.F_idx] = np.mean(self.success_F)\n            self.F_idx = (self.F_idx + 1) % self.memory_size\n            self.F = np.mean(self.F_memory)\n            self.F = np.clip(self.F, 0.0, 1.0)\n            self.success_F = []  # Reset successful F values for the next generation\n\n        if self.success_Cr:\n            # Update Cr using a moving average of successful values\n            self.Cr_memory[self.Cr_idx] = np.mean(self.success_Cr)\n            self.Cr_idx = (self.Cr_idx + 1) % self.memory_size\n            self.Cr = np.mean(self.Cr_memory)\n            self.Cr = np.clip(self.Cr, 0.0, 1.0)\n            self.success_Cr = []  # Reset successful Cr values for the next generation", "configspace": "", "generation": 2, "feedback": "The algorithm AdaptiveDEMovingAverage scored 0.431 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["bb54b16a-0e18-4249-ae8d-be44e91068c8", "1460e948-3e5f-4e35-abf6-cee842f44f89"], "operator": "crossover", "metadata": {"llm_response": "# Description: Adapts F and Cr based on a moving average of successful values, providing smoother adaptation than immediate averaging.\n# Code:\n```python\nimport numpy as np\n\nclass AdaptiveDEMovingAverage:\n    def __init__(self, budget=10000, dim=10, pop_size=None, F_init=0.5, Cr_init=0.9, memory_size=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size if pop_size is not None else 10 * dim\n        self.F = F_init\n        self.Cr = Cr_init\n        self.lb = -5.0\n        self.ub = 5.0\n        self.memory_size = memory_size\n        self.F_memory = np.full(self.memory_size, F_init)\n        self.Cr_memory = np.full(self.memory_size, Cr_init)\n        self.F_idx = 0\n        self.Cr_idx = 0\n        self.success_F = []\n        self.success_Cr = []\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.pop = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.nevals = self.pop_size\n\n        while self.nevals < self.budget:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = self.pop[idxs]\n\n                x_mutated = self.pop[i] + self.F * (x_r2 - x_r3)\n                x_mutated = np.clip(x_mutated, self.lb, self.ub)\n\n                # Crossover\n                x_trial = np.copy(self.pop[i])\n                crossover_points = np.random.rand(self.dim) < self.Cr\n                if not np.any(crossover_points):\n                    crossover_points[np.random.randint(0, self.dim)] = True\n                x_trial[crossover_points] = x_mutated[crossover_points]\n\n                # Evaluation\n                f_trial = func(x_trial)\n                self.nevals += 1\n\n                # Selection\n                if f_trial < self.fitness[i]:\n                    # Store successful F and Cr\n                    self.success_F.append(self.F)\n                    self.success_Cr.append(self.Cr)\n                    \n                    self.fitness[i] = f_trial\n                    self.pop[i] = x_trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = x_trial\n\n            # Adapt F and Cr (after each generation)\n            self.adapt_parameters()\n\n        return self.f_opt, self.x_opt\n\n    def adapt_parameters(self):\n        if self.success_F:\n            # Update F using a moving average of successful values\n            self.F_memory[self.F_idx] = np.mean(self.success_F)\n            self.F_idx = (self.F_idx + 1) % self.memory_size\n            self.F = np.mean(self.F_memory)\n            self.F = np.clip(self.F, 0.0, 1.0)\n            self.success_F = []  # Reset successful F values for the next generation\n\n        if self.success_Cr:\n            # Update Cr using a moving average of successful values\n            self.Cr_memory[self.Cr_idx] = np.mean(self.success_Cr)\n            self.Cr_idx = (self.Cr_idx + 1) % self.memory_size\n            self.Cr = np.mean(self.Cr_memory)\n            self.Cr = np.clip(self.Cr, 0.0, 1.0)\n            self.success_Cr = []  # Reset successful Cr values for the next generation\n```", "operator": "crossover", "theta_sampled": 0.3053093222112962, "generation": 2, "aucs": [0.16057928425795165, 0.306924466361707, 0.40693477239802434, 0.6439465804735778, 0.32589879076416206, 0.4703496341644504, 0.3030782693450471, 0.36566971662535075, 0.3270151772050929, 0.18523428910539974, 0.6502717614381586, 0.9851040830388014, 0.4158556655812634, 0.30650146020868674, 0.7300814538955087, 0.4591580137929089, 0.3283189229889477, 0.5589516293648271, 0.20213736021890827, 0.4842241002086083]}, "task_prompt": ""}
{"id": "f112be33-766e-4f90-8ada-6c8e7e948a0a", "fitness": 0.35955840879941003, "name": "AdaptiveDEPerturbation", "description": "Adaptively adjusts F and Cr based on success, and perturbs F values for unsuccessful individuals to encourage exploration.", "code": "import numpy as np\n\nclass AdaptiveDEPerturbation:\n    def __init__(self, budget=10000, dim=10, pop_size=None, F_init=0.5, Cr_init=0.9, F_adapt_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size if pop_size is not None else 10 * dim\n        self.F = np.full(self.pop_size, F_init)\n        self.Cr = Cr_init\n        self.lb = -5.0\n        self.ub = 5.0\n        self.memory_size = 10\n        self.F_memory = np.full(self.memory_size, F_init)\n        self.Cr_memory = np.full(self.memory_size, Cr_init)\n        self.F_idx = 0\n        self.Cr_idx = 0\n        self.success_F = []\n        self.success_Cr = []\n        self.p_selection = 0.1\n        self.epsilon = 1e-6\n        self.F_adapt_rate = F_adapt_rate\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.pop = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.nevals = self.pop_size\n\n        while self.nevals < self.budget:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = self.pop[idxs]\n\n                x_mutated = self.pop[i] + self.F[i] * (x_r2 - x_r3)\n                x_mutated = np.clip(x_mutated, self.lb, self.ub)\n\n                # Crossover\n                x_trial = np.copy(self.pop[i])\n                crossover_points = np.random.rand(self.dim) < self.Cr\n                if not np.any(crossover_points):\n                    crossover_points[np.random.randint(0, self.dim)] = True\n                x_trial[crossover_points] = x_mutated[crossover_points]\n\n                # Evaluation\n                f_trial = func(x_trial)\n                self.nevals += 1\n\n                # Selection\n                if f_trial < self.fitness[i]:\n                    # Store successful F and Cr\n                    self.success_F.append(self.F[i])\n                    self.success_Cr.append(self.Cr)\n                    \n                    self.fitness[i] = f_trial\n                    self.pop[i] = x_trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = x_trial\n                else:\n                     #If unsuccessful, perturb F\n                    self.F[i] *= (1 - self.F_adapt_rate)\n                    self.F[i] = np.clip(self.F[i], 0.0, 1.0)\n\n\n            # Adapt F and Cr (after each generation)\n            self.adapt_parameters()\n\n        return self.f_opt, self.x_opt\n\n    def adapt_parameters(self):\n        if self.success_F:\n            # Update F using a weighted average of successful values\n            mean_F = np.mean(self.success_F)\n            for i in range(self.pop_size):\n                if self.F[i] > mean_F:  # Adjust individuals towards the mean\n                    self.F[i] = 0.8*self.F[i] + 0.2*mean_F\n            \n            self.F = np.clip(self.F, 0.0, 1.0)\n            self.success_F = []  # Reset successful F values for the next generation\n\n        if self.success_Cr:\n            # Update Cr using a weighted average of successful values\n            self.Cr = np.mean(self.success_Cr)\n            self.Cr = np.clip(self.Cr, 0.0, 1.0)\n            self.success_Cr = []  # Reset successful Cr values for the next generation", "configspace": "", "generation": 2, "feedback": "The algorithm AdaptiveDEPerturbation scored 0.360 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["bb54b16a-0e18-4249-ae8d-be44e91068c8", "4b1b3244-c858-4729-9a9c-a34c74faa55a"], "operator": "crossover", "metadata": {"llm_response": "# Description: Adaptively adjusts F and Cr based on success, and perturbs F values for unsuccessful individuals to encourage exploration.\n# Code: \n```python\nimport numpy as np\n\nclass AdaptiveDEPerturbation:\n    def __init__(self, budget=10000, dim=10, pop_size=None, F_init=0.5, Cr_init=0.9, F_adapt_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size if pop_size is not None else 10 * dim\n        self.F = np.full(self.pop_size, F_init)\n        self.Cr = Cr_init\n        self.lb = -5.0\n        self.ub = 5.0\n        self.memory_size = 10\n        self.F_memory = np.full(self.memory_size, F_init)\n        self.Cr_memory = np.full(self.memory_size, Cr_init)\n        self.F_idx = 0\n        self.Cr_idx = 0\n        self.success_F = []\n        self.success_Cr = []\n        self.p_selection = 0.1\n        self.epsilon = 1e-6\n        self.F_adapt_rate = F_adapt_rate\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.pop = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.nevals = self.pop_size\n\n        while self.nevals < self.budget:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = self.pop[idxs]\n\n                x_mutated = self.pop[i] + self.F[i] * (x_r2 - x_r3)\n                x_mutated = np.clip(x_mutated, self.lb, self.ub)\n\n                # Crossover\n                x_trial = np.copy(self.pop[i])\n                crossover_points = np.random.rand(self.dim) < self.Cr\n                if not np.any(crossover_points):\n                    crossover_points[np.random.randint(0, self.dim)] = True\n                x_trial[crossover_points] = x_mutated[crossover_points]\n\n                # Evaluation\n                f_trial = func(x_trial)\n                self.nevals += 1\n\n                # Selection\n                if f_trial < self.fitness[i]:\n                    # Store successful F and Cr\n                    self.success_F.append(self.F[i])\n                    self.success_Cr.append(self.Cr)\n                    \n                    self.fitness[i] = f_trial\n                    self.pop[i] = x_trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = x_trial\n                else:\n                     #If unsuccessful, perturb F\n                    self.F[i] *= (1 - self.F_adapt_rate)\n                    self.F[i] = np.clip(self.F[i], 0.0, 1.0)\n\n\n            # Adapt F and Cr (after each generation)\n            self.adapt_parameters()\n\n        return self.f_opt, self.x_opt\n\n    def adapt_parameters(self):\n        if self.success_F:\n            # Update F using a weighted average of successful values\n            mean_F = np.mean(self.success_F)\n            for i in range(self.pop_size):\n                if self.F[i] > mean_F:  # Adjust individuals towards the mean\n                    self.F[i] = 0.8*self.F[i] + 0.2*mean_F\n            \n            self.F = np.clip(self.F, 0.0, 1.0)\n            self.success_F = []  # Reset successful F values for the next generation\n\n        if self.success_Cr:\n            # Update Cr using a weighted average of successful values\n            self.Cr = np.mean(self.success_Cr)\n            self.Cr = np.clip(self.Cr, 0.0, 1.0)\n            self.success_Cr = []  # Reset successful Cr values for the next generation\n```", "operator": "crossover", "theta_sampled": 0.366126126382589, "generation": 2, "aucs": [0.1549844021246528, 0.2755200975761596, 0.3650528725965134, 0.41674005822928595, 0.23141323441661688, 0.3499546059509887, 0.2961985684945794, 0.33330403753231486, 0.26591735913569314, 0.203278912067125, 0.308705157306179, 0.9953877924149027, 0.3284108618049195, 0.3014565857195125, 0.6583312489463184, 0.31014440249735686, 0.29546805499113227, 0.4352093192735923, 0.1919785538328932, 0.4737120510774636]}, "task_prompt": ""}
{"id": "b3d902a5-e4f8-4f92-81fe-5f30bec513b6", "fitness": 0.42772326826886414, "name": "AdaptiveDESmoothed", "description": "Adaptive Differential Evolution with smoothed parameter adaptation using a weighted average of past successful values.", "code": "import numpy as np\n\nclass AdaptiveDESmoothed:\n    def __init__(self, budget=10000, dim=10, pop_size=None, F_init=0.5, Cr_init=0.9, adapt_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size if pop_size is not None else 10 * dim\n        self.F = F_init\n        self.Cr = Cr_init\n        self.lb = -5.0\n        self.ub = 5.0\n        self.adapt_rate = adapt_rate\n        self.success_F = []\n        self.success_Cr = []\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.pop = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.nevals = self.pop_size\n\n        while self.nevals < self.budget:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = self.pop[idxs]\n\n                x_mutated = self.pop[i] + self.F * (x_r2 - x_r3)\n                x_mutated = np.clip(x_mutated, self.lb, self.ub)\n\n                # Crossover\n                x_trial = np.copy(self.pop[i])\n                crossover_points = np.random.rand(self.dim) < self.Cr\n                if not np.any(crossover_points):\n                    crossover_points[np.random.randint(0, self.dim)] = True\n                x_trial[crossover_points] = x_mutated[crossover_points]\n\n                # Evaluation\n                f_trial = func(x_trial)\n                self.nevals += 1\n\n                # Selection\n                if f_trial < self.fitness[i]:\n                    # Store successful F and Cr\n                    self.success_F.append(self.F)\n                    self.success_Cr.append(self.Cr)\n                    \n                    self.fitness[i] = f_trial\n                    self.pop[i] = x_trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = x_trial\n\n            # Adapt F and Cr (after each generation)\n            self.adapt_parameters()\n\n        return self.f_opt, self.x_opt\n\n    def adapt_parameters(self):\n        if self.success_F:\n            self.F = (1 - self.adapt_rate) * self.F + self.adapt_rate * np.mean(self.success_F)\n            self.F = np.clip(self.F, 0.0, 1.0)\n            self.success_F = []\n\n        if self.success_Cr:\n            self.Cr = (1 - self.adapt_rate) * self.Cr + self.adapt_rate * np.mean(self.success_Cr)\n            self.Cr = np.clip(self.Cr, 0.0, 1.0)\n            self.success_Cr = []", "configspace": "", "generation": 2, "feedback": "The algorithm AdaptiveDESmoothed scored 0.428 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["bb54b16a-0e18-4249-ae8d-be44e91068c8", "1460e948-3e5f-4e35-abf6-cee842f44f89"], "operator": "crossover", "metadata": {"llm_response": "# Description: Adaptive Differential Evolution with smoothed parameter adaptation using a weighted average of past successful values.\n# Code: \n```python\nimport numpy as np\n\nclass AdaptiveDESmoothed:\n    def __init__(self, budget=10000, dim=10, pop_size=None, F_init=0.5, Cr_init=0.9, adapt_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size if pop_size is not None else 10 * dim\n        self.F = F_init\n        self.Cr = Cr_init\n        self.lb = -5.0\n        self.ub = 5.0\n        self.adapt_rate = adapt_rate\n        self.success_F = []\n        self.success_Cr = []\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.pop = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.nevals = self.pop_size\n\n        while self.nevals < self.budget:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = self.pop[idxs]\n\n                x_mutated = self.pop[i] + self.F * (x_r2 - x_r3)\n                x_mutated = np.clip(x_mutated, self.lb, self.ub)\n\n                # Crossover\n                x_trial = np.copy(self.pop[i])\n                crossover_points = np.random.rand(self.dim) < self.Cr\n                if not np.any(crossover_points):\n                    crossover_points[np.random.randint(0, self.dim)] = True\n                x_trial[crossover_points] = x_mutated[crossover_points]\n\n                # Evaluation\n                f_trial = func(x_trial)\n                self.nevals += 1\n\n                # Selection\n                if f_trial < self.fitness[i]:\n                    # Store successful F and Cr\n                    self.success_F.append(self.F)\n                    self.success_Cr.append(self.Cr)\n                    \n                    self.fitness[i] = f_trial\n                    self.pop[i] = x_trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = x_trial\n\n            # Adapt F and Cr (after each generation)\n            self.adapt_parameters()\n\n        return self.f_opt, self.x_opt\n\n    def adapt_parameters(self):\n        if self.success_F:\n            self.F = (1 - self.adapt_rate) * self.F + self.adapt_rate * np.mean(self.success_F)\n            self.F = np.clip(self.F, 0.0, 1.0)\n            self.success_F = []\n\n        if self.success_Cr:\n            self.Cr = (1 - self.adapt_rate) * self.Cr + self.adapt_rate * np.mean(self.success_Cr)\n            self.Cr = np.clip(self.Cr, 0.0, 1.0)\n            self.success_Cr = []\n```", "operator": "crossover", "theta_sampled": -0.006461816364961459, "generation": 2, "aucs": [0.1562116569366524, 0.3215464755757079, 0.3899159547074179, 0.6096550725181598, 0.34695363478726204, 0.4775800869164396, 0.30497007460876924, 0.3406757619239186, 0.3343224686639893, 0.19406729035204984, 0.6325899583425358, 0.9992882641751246, 0.389950872799785, 0.30078255735557635, 0.7437153887147032, 0.4242010286694172, 0.34252217652551553, 0.5614949563393244, 0.19477147510362725, 0.48925021036130545]}, "task_prompt": ""}
{"id": "be825261-44f9-49a7-9a68-ab6e31be97d2", "fitness": 0.37796351017778457, "name": "AdaptiveDEIndividualF", "description": "Adaptive Differential Evolution with individual F adaptation based on success/failure.", "code": "import numpy as np\n\nclass AdaptiveDEIndividualF:\n    def __init__(self, budget=10000, dim=10, pop_size=None, Cr_init=0.9, F_init=0.5, F_adapt_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size if pop_size is not None else 10 * dim\n        self.Cr = Cr_init\n        self.F = np.full(self.pop_size, F_init)  # Individual F for each population member\n        self.F_init = F_init\n        self.F_adapt_rate = F_adapt_rate\n        self.lb = -5.0\n        self.ub = 5.0\n        self.nevals = 0\n        self.x_opt = None\n        self.f_opt = np.inf\n        self.pop = None\n        self.fitness = None\n\n    def __call__(self, func):\n        self.pop = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.nevals = self.pop_size\n\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.pop[np.argmin(self.fitness)]\n\n        while self.nevals < self.budget:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = self.pop[idxs]\n                \n                x_mutated = self.pop[i] + self.F[i] * (x_r2 - x_r3)\n                x_mutated = np.clip(x_mutated, self.lb, self.ub)\n\n                # Crossover\n                x_trial = np.copy(self.pop[i])\n                crossover_points = np.random.rand(self.dim) < self.Cr\n                if not np.any(crossover_points):\n                    crossover_points[np.random.randint(0, self.dim)] = True\n                x_trial[crossover_points] = x_mutated[crossover_points]\n                \n                # Evaluation\n                f_trial = func(x_trial)\n                self.nevals += 1\n\n                # Selection and F adaptation\n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.pop[i] = x_trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = x_trial\n                    \n                    # Adapt F based on success\n                    self.F[i] = np.random.normal(loc=self.F_init, scale=0.3)\n                    self.F[i] = np.clip(self.F[i], 0.0, 1.0)\n                else:\n                    #If unsuccessful, decrease F\n                    self.F[i] *= (1 - self.F_adapt_rate)\n                    self.F[i] = np.clip(self.F[i], 0.0, 1.0)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "The algorithm AdaptiveDEIndividualF scored 0.378 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["bb54b16a-0e18-4249-ae8d-be44e91068c8", "4b1b3244-c858-4729-9a9c-a34c74faa55a"], "operator": "crossover", "metadata": {"llm_response": "# Description: Adaptive Differential Evolution with individual F adaptation based on success/failure.\n# Code: \n```python\nimport numpy as np\n\nclass AdaptiveDEIndividualF:\n    def __init__(self, budget=10000, dim=10, pop_size=None, Cr_init=0.9, F_init=0.5, F_adapt_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size if pop_size is not None else 10 * dim\n        self.Cr = Cr_init\n        self.F = np.full(self.pop_size, F_init)  # Individual F for each population member\n        self.F_init = F_init\n        self.F_adapt_rate = F_adapt_rate\n        self.lb = -5.0\n        self.ub = 5.0\n        self.nevals = 0\n        self.x_opt = None\n        self.f_opt = np.inf\n        self.pop = None\n        self.fitness = None\n\n    def __call__(self, func):\n        self.pop = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.nevals = self.pop_size\n\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.pop[np.argmin(self.fitness)]\n\n        while self.nevals < self.budget:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = self.pop[idxs]\n                \n                x_mutated = self.pop[i] + self.F[i] * (x_r2 - x_r3)\n                x_mutated = np.clip(x_mutated, self.lb, self.ub)\n\n                # Crossover\n                x_trial = np.copy(self.pop[i])\n                crossover_points = np.random.rand(self.dim) < self.Cr\n                if not np.any(crossover_points):\n                    crossover_points[np.random.randint(0, self.dim)] = True\n                x_trial[crossover_points] = x_mutated[crossover_points]\n                \n                # Evaluation\n                f_trial = func(x_trial)\n                self.nevals += 1\n\n                # Selection and F adaptation\n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.pop[i] = x_trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = x_trial\n                    \n                    # Adapt F based on success\n                    self.F[i] = np.random.normal(loc=self.F_init, scale=0.3)\n                    self.F[i] = np.clip(self.F[i], 0.0, 1.0)\n                else:\n                    #If unsuccessful, decrease F\n                    self.F[i] *= (1 - self.F_adapt_rate)\n                    self.F[i] = np.clip(self.F[i], 0.0, 1.0)\n\n        return self.f_opt, self.x_opt\n```", "operator": "crossover", "theta_sampled": 0.05600918115820471, "generation": 2, "aucs": [0.15891283423709024, 0.28961891112875804, 0.35420661380900664, 0.4174118845877518, 0.2732116891430554, 0.3888960450173852, 0.3081737657262109, 0.30022724733857165, 0.28230755306489796, 0.23990444279897105, 0.4007521849744944, 0.9969095201135113, 0.33273494508877033, 0.29681687616953745, 0.6828894411520383, 0.3752109962850969, 0.31425793468247987, 0.47168403510441226, 0.19199336380486043, 0.4831499193287907]}, "task_prompt": ""}
{"id": "e3620301-d864-4878-8557-8a08ce0103e7", "fitness": "-inf", "name": "AdaptiveDEArchive", "description": "Adaptive Differential Evolution with Archive, which maintains an archive of inferior solutions to enhance exploration and adaptively adjusts F and CR based on success.", "code": "import numpy as np\n\nclass AdaptiveDEArchive:\n    def __init__(self, budget=10000, dim=10, pop_size=None, archive_size=None, Cr_init=0.5, F_init=0.5, F_adapt_rate=0.1, Cr_adapt_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size if pop_size is not None else 10 * dim\n        self.archive_size = archive_size if archive_size is not None else self.pop_size\n        self.Cr = np.full(self.pop_size, Cr_init)\n        self.F = np.full(self.pop_size, F_init)\n        self.F_init = F_init\n        self.Cr_init = Cr_init\n        self.F_adapt_rate = F_adapt_rate\n        self.Cr_adapt_rate = Cr_adapt_rate\n        self.lb = -5.0\n        self.ub = 5.0\n        self.nevals = 0\n        self.archive = []\n        self.archive_fitness = []\n        self.x_opt = None\n        self.f_opt = np.inf\n        self.pop = None\n        self.fitness = None\n\n    def __call__(self, func):\n        self.pop = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.nevals = self.pop_size\n\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.pop[np.argmin(self.fitness)]\n\n        while self.nevals < self.budget:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 2, replace=False)\n                x_r1, x_r2 = self.pop[idxs]\n\n                if len(self.archive) > 0 and np.random.rand() < 0.1:  # Use archive occasionally\n                    idx_archive = np.random.randint(0, len(self.archive))\n                    x_r3 = self.archive[idx_archive]\n                else:\n                    idx_pop = np.random.choice(self.pop_size, 1, replace=False)[0]\n                    x_r3 = self.pop[idx_pop]\n\n                x_mutated = self.pop[i] + self.F[i] * (x_r1 - x_r2)\n                x_mutated = np.clip(x_mutated, self.lb, self.ub)\n\n                # Crossover\n                x_trial = np.copy(self.pop[i])\n                crossover_points = np.random.rand(self.dim) < self.Cr[i]\n                if not np.any(crossover_points):\n                    crossover_points[np.random.randint(0, self.dim)] = True\n                x_trial[crossover_points] = x_mutated[crossover_points]\n\n                # Evaluation\n                f_trial = func(x_trial)\n                self.nevals += 1\n\n                # Selection and Adaptation\n                if f_trial < self.fitness[i]:\n                    # Success\n                    delta_f = np.abs(f_trial - self.fitness[i])\n                    \n                    #Adapt parameters\n                    self.F[i] = np.random.normal(loc=self.F_init, scale=0.1)\n                    self.F[i] = np.clip(self.F[i], 0.0, 1.0)\n\n                    self.Cr[i] = np.random.normal(loc=self.Cr_init, scale=0.1)\n                    self.Cr[i] = np.clip(self.Cr[i], 0.0, 1.0)\n\n                    # Archive\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(self.pop[i].copy())\n                    else:\n                        j = np.random.randint(0, self.archive_size)\n                        self.archive[j] = self.pop[i].copy()\n                    \n                    self.fitness[i] = f_trial\n                    self.pop[i] = x_trial\n                    \n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = x_trial\n                \n                else:\n                    #Failure\n                    self.F[i] *= (1 - self.F_adapt_rate)\n                    self.F[i] = np.clip(self.F[i], 0.0, 1.0)\n                    self.Cr[i] *= (1 - self.Cr_adapt_rate)\n                    self.Cr[i] = np.clip(self.Cr[i], 0.0, 1.0)\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "Evaluation timed out after 300 seconds.", "error": "Evaluation timed out after 300 seconds.", "parent_ids": ["4b1b3244-c858-4729-9a9c-a34c74faa55a"], "operator": "simplify", "metadata": {"llm_response": "# Description: Adaptive Differential Evolution with Archive, which maintains an archive of inferior solutions to enhance exploration and adaptively adjusts F and CR based on success.\n# Code: \n```python\nimport numpy as np\n\nclass AdaptiveDEArchive:\n    def __init__(self, budget=10000, dim=10, pop_size=None, archive_size=None, Cr_init=0.5, F_init=0.5, F_adapt_rate=0.1, Cr_adapt_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size if pop_size is not None else 10 * dim\n        self.archive_size = archive_size if archive_size is not None else self.pop_size\n        self.Cr = np.full(self.pop_size, Cr_init)\n        self.F = np.full(self.pop_size, F_init)\n        self.F_init = F_init\n        self.Cr_init = Cr_init\n        self.F_adapt_rate = F_adapt_rate\n        self.Cr_adapt_rate = Cr_adapt_rate\n        self.lb = -5.0\n        self.ub = 5.0\n        self.nevals = 0\n        self.archive = []\n        self.archive_fitness = []\n        self.x_opt = None\n        self.f_opt = np.inf\n        self.pop = None\n        self.fitness = None\n\n    def __call__(self, func):\n        self.pop = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.nevals = self.pop_size\n\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.pop[np.argmin(self.fitness)]\n\n        while self.nevals < self.budget:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 2, replace=False)\n                x_r1, x_r2 = self.pop[idxs]\n\n                if len(self.archive) > 0 and np.random.rand() < 0.1:  # Use archive occasionally\n                    idx_archive = np.random.randint(0, len(self.archive))\n                    x_r3 = self.archive[idx_archive]\n                else:\n                    idx_pop = np.random.choice(self.pop_size, 1, replace=False)[0]\n                    x_r3 = self.pop[idx_pop]\n\n                x_mutated = self.pop[i] + self.F[i] * (x_r1 - x_r2)\n                x_mutated = np.clip(x_mutated, self.lb, self.ub)\n\n                # Crossover\n                x_trial = np.copy(self.pop[i])\n                crossover_points = np.random.rand(self.dim) < self.Cr[i]\n                if not np.any(crossover_points):\n                    crossover_points[np.random.randint(0, self.dim)] = True\n                x_trial[crossover_points] = x_mutated[crossover_points]\n\n                # Evaluation\n                f_trial = func(x_trial)\n                self.nevals += 1\n\n                # Selection and Adaptation\n                if f_trial < self.fitness[i]:\n                    # Success\n                    delta_f = np.abs(f_trial - self.fitness[i])\n                    \n                    #Adapt parameters\n                    self.F[i] = np.random.normal(loc=self.F_init, scale=0.1)\n                    self.F[i] = np.clip(self.F[i], 0.0, 1.0)\n\n                    self.Cr[i] = np.random.normal(loc=self.Cr_init, scale=0.1)\n                    self.Cr[i] = np.clip(self.Cr[i], 0.0, 1.0)\n\n                    # Archive\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(self.pop[i].copy())\n                    else:\n                        j = np.random.randint(0, self.archive_size)\n                        self.archive[j] = self.pop[i].copy()\n                    \n                    self.fitness[i] = f_trial\n                    self.pop[i] = x_trial\n                    \n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = x_trial\n                \n                else:\n                    #Failure\n                    self.F[i] *= (1 - self.F_adapt_rate)\n                    self.F[i] = np.clip(self.F[i], 0.0, 1.0)\n                    self.Cr[i] *= (1 - self.Cr_adapt_rate)\n                    self.Cr[i] = np.clip(self.Cr[i], 0.0, 1.0)\n        return self.f_opt, self.x_opt\n```", "operator": "simplify", "theta_sampled": 0.20542972179417088, "generation": 2}, "task_prompt": ""}
{"id": "e1cd5bd9-27d9-4945-a9d8-cf62045193e8", "fitness": 0.423378414143175, "name": "AdaptiveDE", "description": "Adaptive Differential Evolution with scaled mutation factor and self-adjusting crossover probability based on successful trials.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=None, F=0.5, Cr=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size if pop_size is not None else 10*dim\n        self.F = F\n        self.Cr = Cr\n        self.lb = -5.0\n        self.ub = 5.0\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.pop = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.nevals = self.pop_size\n\n        success_history_F = []\n        success_history_CR = []\n\n        while self.nevals < self.budget:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = self.pop[idxs]\n                \n                # Scale F based on dimensionality\n                F_scaled = self.F * (1 + 0.1 * (self.dim / 10)) \n\n                x_mutated = self.pop[i] + F_scaled * (x_r2 - x_r3)\n                x_mutated = np.clip(x_mutated, self.lb, self.ub)\n\n                # Crossover\n                x_trial = np.copy(self.pop[i])\n                \n                # Self-adjust Cr based on success history\n                if success_history_CR:\n                    Cr_current = np.clip(np.mean(success_history_CR), 0.0, 1.0)\n                else:\n                     Cr_current = self.Cr\n                \n                crossover_points = np.random.rand(self.dim) < Cr_current\n                if not np.any(crossover_points):\n                    crossover_points[np.random.randint(0, self.dim)] = True\n                x_trial[crossover_points] = x_mutated[crossover_points]\n\n                # Evaluation\n                f_trial = func(x_trial)\n                self.nevals += 1\n\n                # Selection\n                if f_trial < self.fitness[i]:\n                    success_history_F.append(F_scaled)\n                    success_history_CR.append(Cr_current)\n\n                    self.fitness[i] = f_trial\n                    self.pop[i] = x_trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = x_trial\n\n            # Keep the history short\n            if len(success_history_F) > self.pop_size:\n                success_history_F = success_history_F[-self.pop_size:]\n                success_history_CR = success_history_CR[-self.pop_size:]\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "The algorithm AdaptiveDE scored 0.423 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["1460e948-3e5f-4e35-abf6-cee842f44f89"], "operator": "simplify", "metadata": {"llm_response": "# Description: Adaptive Differential Evolution with scaled mutation factor and self-adjusting crossover probability based on successful trials.\n# Code: \n```python\nimport numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=None, F=0.5, Cr=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size if pop_size is not None else 10*dim\n        self.F = F\n        self.Cr = Cr\n        self.lb = -5.0\n        self.ub = 5.0\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.pop = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.nevals = self.pop_size\n\n        success_history_F = []\n        success_history_CR = []\n\n        while self.nevals < self.budget:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = self.pop[idxs]\n                \n                # Scale F based on dimensionality\n                F_scaled = self.F * (1 + 0.1 * (self.dim / 10)) \n\n                x_mutated = self.pop[i] + F_scaled * (x_r2 - x_r3)\n                x_mutated = np.clip(x_mutated, self.lb, self.ub)\n\n                # Crossover\n                x_trial = np.copy(self.pop[i])\n                \n                # Self-adjust Cr based on success history\n                if success_history_CR:\n                    Cr_current = np.clip(np.mean(success_history_CR), 0.0, 1.0)\n                else:\n                     Cr_current = self.Cr\n                \n                crossover_points = np.random.rand(self.dim) < Cr_current\n                if not np.any(crossover_points):\n                    crossover_points[np.random.randint(0, self.dim)] = True\n                x_trial[crossover_points] = x_mutated[crossover_points]\n\n                # Evaluation\n                f_trial = func(x_trial)\n                self.nevals += 1\n\n                # Selection\n                if f_trial < self.fitness[i]:\n                    success_history_F.append(F_scaled)\n                    success_history_CR.append(Cr_current)\n\n                    self.fitness[i] = f_trial\n                    self.pop[i] = x_trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = x_trial\n\n            # Keep the history short\n            if len(success_history_F) > self.pop_size:\n                success_history_F = success_history_F[-self.pop_size:]\n                success_history_CR = success_history_CR[-self.pop_size:]\n\n        return self.f_opt, self.x_opt\n```", "operator": "simplify", "theta_sampled": 0.17282779554766234, "generation": 2, "aucs": [0.1552113996396487, 0.28923278560417864, 0.38742750313518304, 0.6142865455834048, 0.3218716757349651, 0.4494395161090592, 0.30685195318511815, 0.36072831204254874, 0.33762316566397566, 0.20076418302297838, 0.6103290260154846, 0.9983923597145231, 0.39702188138822936, 0.312352094108325, 0.749524234007608, 0.4184205373429958, 0.33884791133415293, 0.5551612377553483, 0.17937147270904097, 0.4847104887667316]}, "task_prompt": ""}
{"id": "7080b9d9-57ea-40f5-bf3b-394474cd6a0b", "fitness": 0.28036232102632674, "name": "AdaptiveStepSizeES", "description": "A population-based algorithm that adapts its step size based on the success rate of previous steps and combines exploration and exploitation using a dynamic switching probability.", "code": "import numpy as np\n\nclass AdaptiveStepSizeES:\n    def __init__(self, budget=10000, dim=10, pop_size=20, initial_step_size=0.1, success_threshold=0.2):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.initial_step_size = initial_step_size\n        self.success_threshold = success_threshold\n        self.population = None\n        self.fitness = None\n        self.step_sizes = None\n        self.success_rates = None\n        self.switch_prob = 0.1  # Probability to switch to exploration\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        self.step_sizes = np.full(self.pop_size, self.initial_step_size)\n        self.success_rates = np.zeros(self.pop_size)\n        self.success_counts = np.zeros(self.pop_size)\n        self.trial_counts = np.zeros(self.pop_size)\n\n    def mutate(self, individual, step_size, func):\n        mutation = np.random.normal(0, step_size, size=self.dim)\n        mutated_individual = individual + mutation\n        mutated_individual = np.clip(mutated_individual, func.bounds.lb, func.bounds.ub)\n        return mutated_individual\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        self.initialize_population(func)\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                if self.budget <= 0:\n                    break\n                    \n                # Exploration or Exploitation\n                if np.random.rand() < self.switch_prob:  # Exploration: Random Reset\n                    trial_individual = np.random.uniform(func.bounds.lb, func.bounds.ub)\n                else:\n                    trial_individual = self.mutate(self.population[i], self.step_sizes[i], func)\n\n                trial_fitness = func(trial_individual)\n                self.budget -= 1\n\n                if trial_fitness < self.fitness[i]:\n                    self.fitness[i] = trial_fitness\n                    self.population[i] = trial_individual\n                    self.success_counts[i] += 1\n                    self.success_rates[i] = self.success_counts[i] / (self.trial_counts[i] + 1)\n                    \n                    # Increase step size if it's successful\n                    if self.success_rates[i] > self.success_threshold:\n                        self.step_sizes[i] *= 1.2  # Increase step size\n                else:\n                    # Decrease step size if it's not successful\n                    self.step_sizes[i] *= 0.8  # Decrease step size\n\n                self.trial_counts[i] += 1\n                \n                if trial_fitness < self.f_opt:\n                    self.f_opt = trial_fitness\n                    self.x_opt = trial_individual\n        \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "The algorithm AdaptiveStepSizeES scored 0.280 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": "random_new", "metadata": {"llm_response": "# Description: A population-based algorithm that adapts its step size based on the success rate of previous steps and combines exploration and exploitation using a dynamic switching probability.\n# Code: \n```python\nimport numpy as np\n\nclass AdaptiveStepSizeES:\n    def __init__(self, budget=10000, dim=10, pop_size=20, initial_step_size=0.1, success_threshold=0.2):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.initial_step_size = initial_step_size\n        self.success_threshold = success_threshold\n        self.population = None\n        self.fitness = None\n        self.step_sizes = None\n        self.success_rates = None\n        self.switch_prob = 0.1  # Probability to switch to exploration\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        self.step_sizes = np.full(self.pop_size, self.initial_step_size)\n        self.success_rates = np.zeros(self.pop_size)\n        self.success_counts = np.zeros(self.pop_size)\n        self.trial_counts = np.zeros(self.pop_size)\n\n    def mutate(self, individual, step_size, func):\n        mutation = np.random.normal(0, step_size, size=self.dim)\n        mutated_individual = individual + mutation\n        mutated_individual = np.clip(mutated_individual, func.bounds.lb, func.bounds.ub)\n        return mutated_individual\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        self.initialize_population(func)\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                if self.budget <= 0:\n                    break\n                    \n                # Exploration or Exploitation\n                if np.random.rand() < self.switch_prob:  # Exploration: Random Reset\n                    trial_individual = np.random.uniform(func.bounds.lb, func.bounds.ub)\n                else:\n                    trial_individual = self.mutate(self.population[i], self.step_sizes[i], func)\n\n                trial_fitness = func(trial_individual)\n                self.budget -= 1\n\n                if trial_fitness < self.fitness[i]:\n                    self.fitness[i] = trial_fitness\n                    self.population[i] = trial_individual\n                    self.success_counts[i] += 1\n                    self.success_rates[i] = self.success_counts[i] / (self.trial_counts[i] + 1)\n                    \n                    # Increase step size if it's successful\n                    if self.success_rates[i] > self.success_threshold:\n                        self.step_sizes[i] *= 1.2  # Increase step size\n                else:\n                    # Decrease step size if it's not successful\n                    self.step_sizes[i] *= 0.8  # Decrease step size\n\n                self.trial_counts[i] += 1\n                \n                if trial_fitness < self.f_opt:\n                    self.f_opt = trial_fitness\n                    self.x_opt = trial_individual\n        \n        return self.f_opt, self.x_opt\n```", "operator": "random_new", "theta_sampled": 0.07648547202580705, "generation": 2, "aucs": [0.1343218853764231, 0.1645680972116279, 0.2805032441797888, 0.20916416890279343, 0.20883362954182194, 0.1884623544615751, 0.2257753002575933, 0.19635728616853365, 0.19287700138800623, 0.14832423778580883, 0.24571164816095659, 0.9922876025115752, 0.2502273331842424, 0.19443660501386206, 0.5447669342004947, 0.27640480097477416, 0.22503204612947314, 0.3390973776114068, 0.1513819620781729, 0.438712905387605]}, "task_prompt": ""}
{"id": "f78ab9aa-4767-442e-9f28-76cc46c1053b", "fitness": 0.22223726233265104, "name": "AdaptiveSOMA", "description": "Implements a Self-Organizing Migrating Algorithm (SOMA) with adaptive step size control based on success rate.", "code": "import numpy as np\n\nclass AdaptiveSOMA:\n    def __init__(self, budget=10000, dim=10, pop_size=20, path_length=3.0, step_size=0.01, perturbation=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.path_length = path_length\n        self.step_size = step_size\n        self.perturbation = perturbation\n        self.lb = -5.0\n        self.ub = 5.0\n        self.success_rate = 0.5 # Initialize success rate\n        self.success_memory = []\n\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        eval_count = 0\n\n        # Initialize population\n        population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        eval_count += self.pop_size\n\n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)]\n\n        while eval_count < self.budget:\n            leader_index = np.argmin(fitness)\n            leader = population[leader_index]\n            \n            successes = 0\n            for i in range(self.pop_size):\n                if i == leader_index:\n                    continue\n\n                # Migration\n                original_fitness = fitness[i]\n                \n                for step in np.arange(self.step_size, self.path_length + self.step_size, self.step_size):\n                    \n                    new_position = population[i] + (leader - population[i]) * step\n                    \n                    # Perturbation\n                    perturbation_vector = np.random.uniform(-self.perturbation, self.perturbation, size=self.dim)\n                    new_position = new_position + perturbation_vector\n                    \n                    # Boundary handling (clamping)\n                    new_position = np.clip(new_position, self.lb, self.ub)\n                    \n                    new_fitness = func(new_position)\n                    eval_count += 1\n\n                    if new_fitness < fitness[i]:\n                        population[i] = new_position\n                        fitness[i] = new_fitness\n                        successes += 1\n                        \n                        if new_fitness < self.f_opt:\n                            self.f_opt = new_fitness\n                            self.x_opt = new_position\n                    \n                    if eval_count >= self.budget:\n                        break\n                if eval_count >= self.budget:\n                    break    \n            # Adaptive Step Size Control\n            current_success_rate = successes / (self.pop_size - 1) / (self.path_length/self.step_size) if (self.pop_size - 1) > 0 else 0.0\n\n            self.success_memory.append(current_success_rate)\n\n            if len(self.success_memory) > 10:  # Maintain a short history\n                self.success_memory.pop(0)\n            \n            avg_success_rate = np.mean(self.success_memory) if self.success_memory else current_success_rate\n            \n            if avg_success_rate > self.success_rate:\n                self.step_size *= 1.1  # Increase step size\n            elif avg_success_rate < self.success_rate:\n                self.step_size *= 0.9  # Decrease step size\n\n            self.step_size = np.clip(self.step_size, 0.0001, 1.0) #Prevent the stepsize from becoming too small or too large\n\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "The algorithm AdaptiveSOMA scored 0.222 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": "random_new", "metadata": {"llm_response": "# Description: Implements a Self-Organizing Migrating Algorithm (SOMA) with adaptive step size control based on success rate.\n# Code: \n```python\nimport numpy as np\n\nclass AdaptiveSOMA:\n    def __init__(self, budget=10000, dim=10, pop_size=20, path_length=3.0, step_size=0.01, perturbation=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.path_length = path_length\n        self.step_size = step_size\n        self.perturbation = perturbation\n        self.lb = -5.0\n        self.ub = 5.0\n        self.success_rate = 0.5 # Initialize success rate\n        self.success_memory = []\n\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        eval_count = 0\n\n        # Initialize population\n        population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        eval_count += self.pop_size\n\n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)]\n\n        while eval_count < self.budget:\n            leader_index = np.argmin(fitness)\n            leader = population[leader_index]\n            \n            successes = 0\n            for i in range(self.pop_size):\n                if i == leader_index:\n                    continue\n\n                # Migration\n                original_fitness = fitness[i]\n                \n                for step in np.arange(self.step_size, self.path_length + self.step_size, self.step_size):\n                    \n                    new_position = population[i] + (leader - population[i]) * step\n                    \n                    # Perturbation\n                    perturbation_vector = np.random.uniform(-self.perturbation, self.perturbation, size=self.dim)\n                    new_position = new_position + perturbation_vector\n                    \n                    # Boundary handling (clamping)\n                    new_position = np.clip(new_position, self.lb, self.ub)\n                    \n                    new_fitness = func(new_position)\n                    eval_count += 1\n\n                    if new_fitness < fitness[i]:\n                        population[i] = new_position\n                        fitness[i] = new_fitness\n                        successes += 1\n                        \n                        if new_fitness < self.f_opt:\n                            self.f_opt = new_fitness\n                            self.x_opt = new_position\n                    \n                    if eval_count >= self.budget:\n                        break\n                if eval_count >= self.budget:\n                    break    \n            # Adaptive Step Size Control\n            current_success_rate = successes / (self.pop_size - 1) / (self.path_length/self.step_size) if (self.pop_size - 1) > 0 else 0.0\n\n            self.success_memory.append(current_success_rate)\n\n            if len(self.success_memory) > 10:  # Maintain a short history\n                self.success_memory.pop(0)\n            \n            avg_success_rate = np.mean(self.success_memory) if self.success_memory else current_success_rate\n            \n            if avg_success_rate > self.success_rate:\n                self.step_size *= 1.1  # Increase step size\n            elif avg_success_rate < self.success_rate:\n                self.step_size *= 0.9  # Decrease step size\n\n            self.step_size = np.clip(self.step_size, 0.0001, 1.0) #Prevent the stepsize from becoming too small or too large\n\n\n\n        return self.f_opt, self.x_opt\n```", "operator": "random_new", "theta_sampled": 0.16232074330797286, "generation": 3, "aucs": [0.17444993277957732, 0.207288667736517, 0.2522047434704058, 0.17768439320306495, 0.19144094772665854, 0.18338454613233546, 0.1950592513743591, 0.19103794393174944, 0.23105425246395472, 0.12455948432144559, 0.14855862763561234, 0.20510882500896888, 0.22207190180432257, 0.12887150902764188, 0.5457556083701113, 0.2550468121842422, 0.22492238573888912, 0.1677681510439223, 0.18226144637918384, 0.43621581632005746]}, "task_prompt": ""}
{"id": "c40c73f1-509c-4981-a5f5-cd5e20805624", "fitness": "-inf", "name": "DECMASearch", "description": "Population-based algorithm that combines Differential Evolution with a covariance matrix adaptation strategy to guide the search.", "code": "import numpy as np\n\nclass DECMASearch:\n    def __init__(self, budget=10000, dim=10, pop_size=None):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size if pop_size is not None else 4 + int(3 * np.log(dim))\n        self.mu = self.pop_size // 2  # Number of individuals for recombination\n        self.C = np.eye(dim)  # Covariance matrix\n        self.pc = np.zeros(dim) # Evolution path for C\n        self.ps = np.zeros(dim) # Evolution path for step size\n        self.c_sigma = (self.mu / (self.dim + self.mu))**0.5\n        self.d_sigma = 1 + 2 * max(0, ((self.mu - 1) / (self.dim + 1)) - 1) + self.c_sigma\n        self.c_c = (4 + self.mu / self.dim)**-1\n        self.c_mu = self.mu / (np.sum(range(1, self.mu + 1)) * self.dim**2) #mu needs to be an integer\n\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.eval_count = 0\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        \n        # Initialize population\n        population = np.random.uniform(lb, ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.eval_count += self.pop_size\n\n        while self.eval_count < self.budget:\n            # Selection and recombination\n            sorted_indices = np.argsort(fitness)\n            parents = population[sorted_indices[:self.mu]]\n\n            offspring = np.zeros((self.pop_size, self.dim))\n            for k in range(self.pop_size):\n                # Differential Evolution crossover and mutation\n                idxs = np.random.choice(self.mu, 3, replace=False)\n                x_r1, x_r2, x_r3 = parents[idxs]\n                mutation = x_r1 + 0.5 * (x_r2 - x_r3)\n                j_rand = np.random.randint(self.dim)\n                offspring[k] = parents[0].copy()\n                for j in range(self.dim):\n                    if np.random.rand() < 0.9 or j == j_rand: #Crossover rate of 0.9\n                        offspring[k][j] = mutation[j]\n                \n                #Apply CMA based adaptation\n                z = np.random.multivariate_normal(np.zeros(self.dim), self.C)\n                offspring[k] = parents[0] + self.c_sigma * np.sqrt(np.diag(self.C)) * z\n\n                # Bound handling (mirroring)\n                for j in range(self.dim):\n                    if offspring[k][j] < lb:\n                        offspring[k][j] = 2 * lb - offspring[k][j]\n                    elif offspring[k][j] > ub:\n                        offspring[k][j] = 2 * ub - offspring[k][j]\n\n            # Evaluation\n            offspring_fitness = np.array([func(x) for x in offspring])\n            self.eval_count += self.pop_size\n            \n            # Update population\n            population = np.copy(offspring)\n            fitness = np.copy(offspring_fitness)\n\n            # Update best solution\n            best_idx = np.argmin(fitness)\n            if fitness[best_idx] < self.f_opt:\n                self.f_opt = fitness[best_idx]\n                self.x_opt = population[best_idx]\n                \n            #CMA-ES like update of C\n            y = (population[np.argsort(fitness)[:self.mu]] - np.mean(parents, axis=0)).T\n            self.ps = (1 - self.c_sigma) * self.ps + (self.c_sigma * (2 - self.c_sigma))**0.5 * np.mean(y, axis=1)\n            self.pc = (1 - self.c_c) * self.pc + (self.c_c * (2 - self.c_c))**0.5 * np.mean(y, axis=1)\n            self.C = (1 - self.c_mu) * self.C + self.c_mu * np.dot(y, y.T)\n            self.C = np.triu(self.C) + np.triu(self.C, k=1).T\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "In the code, line 53, in __call__, the following error occurred:\nValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\nOn line: if offspring[k][j] < lb:", "error": "In the code, line 53, in __call__, the following error occurred:\nValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\nOn line: if offspring[k][j] < lb:", "parent_ids": [], "operator": "random_new", "metadata": {"llm_response": "# Description: Population-based algorithm that combines Differential Evolution with a covariance matrix adaptation strategy to guide the search.\n# Code: \n```python\nimport numpy as np\n\nclass DECMASearch:\n    def __init__(self, budget=10000, dim=10, pop_size=None):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size if pop_size is not None else 4 + int(3 * np.log(dim))\n        self.mu = self.pop_size // 2  # Number of individuals for recombination\n        self.C = np.eye(dim)  # Covariance matrix\n        self.pc = np.zeros(dim) # Evolution path for C\n        self.ps = np.zeros(dim) # Evolution path for step size\n        self.c_sigma = (self.mu / (self.dim + self.mu))**0.5\n        self.d_sigma = 1 + 2 * max(0, ((self.mu - 1) / (self.dim + 1)) - 1) + self.c_sigma\n        self.c_c = (4 + self.mu / self.dim)**-1\n        self.c_mu = self.mu / (np.sum(range(1, self.mu + 1)) * self.dim**2) #mu needs to be an integer\n\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.eval_count = 0\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        \n        # Initialize population\n        population = np.random.uniform(lb, ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.eval_count += self.pop_size\n\n        while self.eval_count < self.budget:\n            # Selection and recombination\n            sorted_indices = np.argsort(fitness)\n            parents = population[sorted_indices[:self.mu]]\n\n            offspring = np.zeros((self.pop_size, self.dim))\n            for k in range(self.pop_size):\n                # Differential Evolution crossover and mutation\n                idxs = np.random.choice(self.mu, 3, replace=False)\n                x_r1, x_r2, x_r3 = parents[idxs]\n                mutation = x_r1 + 0.5 * (x_r2 - x_r3)\n                j_rand = np.random.randint(self.dim)\n                offspring[k] = parents[0].copy()\n                for j in range(self.dim):\n                    if np.random.rand() < 0.9 or j == j_rand: #Crossover rate of 0.9\n                        offspring[k][j] = mutation[j]\n                \n                #Apply CMA based adaptation\n                z = np.random.multivariate_normal(np.zeros(self.dim), self.C)\n                offspring[k] = parents[0] + self.c_sigma * np.sqrt(np.diag(self.C)) * z\n\n                # Bound handling (mirroring)\n                for j in range(self.dim):\n                    if offspring[k][j] < lb:\n                        offspring[k][j] = 2 * lb - offspring[k][j]\n                    elif offspring[k][j] > ub:\n                        offspring[k][j] = 2 * ub - offspring[k][j]\n\n            # Evaluation\n            offspring_fitness = np.array([func(x) for x in offspring])\n            self.eval_count += self.pop_size\n            \n            # Update population\n            population = np.copy(offspring)\n            fitness = np.copy(offspring_fitness)\n\n            # Update best solution\n            best_idx = np.argmin(fitness)\n            if fitness[best_idx] < self.f_opt:\n                self.f_opt = fitness[best_idx]\n                self.x_opt = population[best_idx]\n                \n            #CMA-ES like update of C\n            y = (population[np.argsort(fitness)[:self.mu]] - np.mean(parents, axis=0)).T\n            self.ps = (1 - self.c_sigma) * self.ps + (self.c_sigma * (2 - self.c_sigma))**0.5 * np.mean(y, axis=1)\n            self.pc = (1 - self.c_c) * self.pc + (self.c_c * (2 - self.c_c))**0.5 * np.mean(y, axis=1)\n            self.C = (1 - self.c_mu) * self.C + self.c_mu * np.dot(y, y.T)\n            self.C = np.triu(self.C) + np.triu(self.C, k=1).T\n\n        return self.f_opt, self.x_opt\n```", "operator": "random_new", "theta_sampled": 0.619088323695122, "generation": 3}, "task_prompt": ""}
{"id": "cef8adb2-3392-417d-b15e-4d29785d3f80", "fitness": 0.4254401099425336, "name": "AdaptiveDESmoothedMemory", "description": "Adaptive DE with a smoothed update rule for F and Cr, coupled with a memory of past successful values to provide a longer-term perspective.", "code": "import numpy as np\n\nclass AdaptiveDESmoothedMemory:\n    def __init__(self, budget=10000, dim=10, pop_size=None, F_init=0.5, Cr_init=0.9, adapt_rate=0.1, memory_size=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size if pop_size is not None else 10 * dim\n        self.F = F_init\n        self.Cr = Cr_init\n        self.lb = -5.0\n        self.ub = 5.0\n        self.adapt_rate = adapt_rate\n        self.memory_size = memory_size\n        self.F_memory = np.full(self.memory_size, F_init)\n        self.Cr_memory = np.full(self.memory_size, Cr_init)\n        self.F_idx = 0\n        self.Cr_idx = 0\n        self.success_F = []\n        self.success_Cr = []\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.pop = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.nevals = self.pop_size\n\n        while self.nevals < self.budget:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = self.pop[idxs]\n\n                x_mutated = self.pop[i] + self.F * (x_r2 - x_r3)\n                x_mutated = np.clip(x_mutated, self.lb, self.ub)\n\n                # Crossover\n                x_trial = np.copy(self.pop[i])\n                crossover_points = np.random.rand(self.dim) < self.Cr\n                if not np.any(crossover_points):\n                    crossover_points[np.random.randint(0, self.dim)] = True\n                x_trial[crossover_points] = x_mutated[crossover_points]\n\n                # Evaluation\n                f_trial = func(x_trial)\n                self.nevals += 1\n\n                # Selection\n                if f_trial < self.fitness[i]:\n                    # Store successful F and Cr\n                    self.success_F.append(self.F)\n                    self.success_Cr.append(self.Cr)\n                    \n                    self.fitness[i] = f_trial\n                    self.pop[i] = x_trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = x_trial\n\n            # Adapt F and Cr (after each generation)\n            self.adapt_parameters()\n\n        return self.f_opt, self.x_opt\n\n    def adapt_parameters(self):\n        if self.success_F:\n            mean_success_F = np.mean(self.success_F)\n            self.F_memory[self.F_idx] = mean_success_F\n            self.F_idx = (self.F_idx + 1) % self.memory_size\n            self.F = (1 - self.adapt_rate) * self.F + self.adapt_rate * np.mean(self.F_memory)\n            self.F = np.clip(self.F, 0.0, 1.0)\n            self.success_F = []\n\n        if self.success_Cr:\n            mean_success_Cr = np.mean(self.success_Cr)\n            self.Cr_memory[self.Cr_idx] = mean_success_Cr\n            self.Cr_idx = (self.Cr_idx + 1) % self.memory_size\n            self.Cr = (1 - self.adapt_rate) * self.Cr + self.adapt_rate * np.mean(self.Cr_memory)\n            self.Cr = np.clip(self.Cr, 0.0, 1.0)\n            self.success_Cr = []", "configspace": "", "generation": 3, "feedback": "The algorithm AdaptiveDESmoothedMemory scored 0.425 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["79cce7be-a121-4a1c-815e-61b8946185f0", "b3d902a5-e4f8-4f92-81fe-5f30bec513b6"], "operator": "crossover", "metadata": {"llm_response": "# Description: Adaptive DE with a smoothed update rule for F and Cr, coupled with a memory of past successful values to provide a longer-term perspective.\n# Code:\n```python\nimport numpy as np\n\nclass AdaptiveDESmoothedMemory:\n    def __init__(self, budget=10000, dim=10, pop_size=None, F_init=0.5, Cr_init=0.9, adapt_rate=0.1, memory_size=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size if pop_size is not None else 10 * dim\n        self.F = F_init\n        self.Cr = Cr_init\n        self.lb = -5.0\n        self.ub = 5.0\n        self.adapt_rate = adapt_rate\n        self.memory_size = memory_size\n        self.F_memory = np.full(self.memory_size, F_init)\n        self.Cr_memory = np.full(self.memory_size, Cr_init)\n        self.F_idx = 0\n        self.Cr_idx = 0\n        self.success_F = []\n        self.success_Cr = []\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.pop = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.nevals = self.pop_size\n\n        while self.nevals < self.budget:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = self.pop[idxs]\n\n                x_mutated = self.pop[i] + self.F * (x_r2 - x_r3)\n                x_mutated = np.clip(x_mutated, self.lb, self.ub)\n\n                # Crossover\n                x_trial = np.copy(self.pop[i])\n                crossover_points = np.random.rand(self.dim) < self.Cr\n                if not np.any(crossover_points):\n                    crossover_points[np.random.randint(0, self.dim)] = True\n                x_trial[crossover_points] = x_mutated[crossover_points]\n\n                # Evaluation\n                f_trial = func(x_trial)\n                self.nevals += 1\n\n                # Selection\n                if f_trial < self.fitness[i]:\n                    # Store successful F and Cr\n                    self.success_F.append(self.F)\n                    self.success_Cr.append(self.Cr)\n                    \n                    self.fitness[i] = f_trial\n                    self.pop[i] = x_trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = x_trial\n\n            # Adapt F and Cr (after each generation)\n            self.adapt_parameters()\n\n        return self.f_opt, self.x_opt\n\n    def adapt_parameters(self):\n        if self.success_F:\n            mean_success_F = np.mean(self.success_F)\n            self.F_memory[self.F_idx] = mean_success_F\n            self.F_idx = (self.F_idx + 1) % self.memory_size\n            self.F = (1 - self.adapt_rate) * self.F + self.adapt_rate * np.mean(self.F_memory)\n            self.F = np.clip(self.F, 0.0, 1.0)\n            self.success_F = []\n\n        if self.success_Cr:\n            mean_success_Cr = np.mean(self.success_Cr)\n            self.Cr_memory[self.Cr_idx] = mean_success_Cr\n            self.Cr_idx = (self.Cr_idx + 1) % self.memory_size\n            self.Cr = (1 - self.adapt_rate) * self.Cr + self.adapt_rate * np.mean(self.Cr_memory)\n            self.Cr = np.clip(self.Cr, 0.0, 1.0)\n            self.success_Cr = []\n```", "operator": "crossover", "theta_sampled": 0.03832801943137881, "generation": 3, "aucs": [0.15829432133867471, 0.294529351486039, 0.3934059929588489, 0.5942784847290871, 0.3085158524393241, 0.4342015308162578, 0.3011156739070512, 0.3524408990027813, 0.31864422938255466, 0.19088703551199626, 0.6717929490083064, 0.9960782673321508, 0.38721327461177224, 0.31494397264079443, 0.7914999615677917, 0.4283267351432306, 0.351045715187177, 0.5541481411690792, 0.17098121076034822, 0.49645859985740526]}, "task_prompt": ""}
{"id": "96cc787b-8025-468e-9946-15e655af3063", "fitness": 0.4216865994723437, "name": "AdaptiveDESmoothedMovingAverage", "description": "Combines moving average adaptation for F and Cr with smoothing to avoid abrupt changes, enhancing stability.", "code": "import numpy as np\n\nclass AdaptiveDESmoothedMovingAverage:\n    def __init__(self, budget=10000, dim=10, pop_size=None, F_init=0.5, Cr_init=0.9, memory_size=10, adapt_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size if pop_size is not None else 10 * dim\n        self.F = F_init\n        self.Cr = Cr_init\n        self.lb = -5.0\n        self.ub = 5.0\n        self.memory_size = memory_size\n        self.adapt_rate = adapt_rate\n        self.F_memory = np.full(self.memory_size, F_init)\n        self.Cr_memory = np.full(self.memory_size, Cr_init)\n        self.F_idx = 0\n        self.Cr_idx = 0\n        self.success_F = []\n        self.success_Cr = []\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.pop = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.nevals = self.pop_size\n\n        while self.nevals < self.budget:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = self.pop[idxs]\n\n                x_mutated = self.pop[i] + self.F * (x_r2 - x_r3)\n                x_mutated = np.clip(x_mutated, self.lb, self.ub)\n\n                # Crossover\n                x_trial = np.copy(self.pop[i])\n                crossover_points = np.random.rand(self.dim) < self.Cr\n                if not np.any(crossover_points):\n                    crossover_points[np.random.randint(0, self.dim)] = True\n                x_trial[crossover_points] = x_mutated[crossover_points]\n\n                # Evaluation\n                f_trial = func(x_trial)\n                self.nevals += 1\n\n                # Selection\n                if f_trial < self.fitness[i]:\n                    # Store successful F and Cr\n                    self.success_F.append(self.F)\n                    self.success_Cr.append(self.Cr)\n                    \n                    self.fitness[i] = f_trial\n                    self.pop[i] = x_trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = x_trial\n\n            # Adapt F and Cr (after each generation)\n            self.adapt_parameters()\n\n        return self.f_opt, self.x_opt\n\n    def adapt_parameters(self):\n        if self.success_F:\n            mean_success_F = np.mean(self.success_F)\n            self.F_memory[self.F_idx] = mean_success_F\n            self.F_idx = (self.F_idx + 1) % self.memory_size\n            \n            # Smoothing with adapt_rate\n            self.F = (1 - self.adapt_rate) * self.F + self.adapt_rate * np.mean(self.F_memory)\n            self.F = np.clip(self.F, 0.0, 1.0)\n            self.success_F = []  # Reset successful F values for the next generation\n\n        if self.success_Cr:\n            mean_success_Cr = np.mean(self.success_Cr)\n            self.Cr_memory[self.Cr_idx] = mean_success_Cr\n            self.Cr_idx = (self.Cr_idx + 1) % self.memory_size\n\n            # Smoothing with adapt_rate\n            self.Cr = (1 - self.adapt_rate) * self.Cr + self.adapt_rate * np.mean(self.Cr_memory)\n            self.Cr = np.clip(self.Cr, 0.0, 1.0)\n            self.success_Cr = []  # Reset successful Cr values for the next generation", "configspace": "", "generation": 3, "feedback": "The algorithm AdaptiveDESmoothedMovingAverage scored 0.422 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["79cce7be-a121-4a1c-815e-61b8946185f0", "b3d902a5-e4f8-4f92-81fe-5f30bec513b6"], "operator": "crossover", "metadata": {"llm_response": "# Description: Combines moving average adaptation for F and Cr with smoothing to avoid abrupt changes, enhancing stability.\n# Code:\n```python\nimport numpy as np\n\nclass AdaptiveDESmoothedMovingAverage:\n    def __init__(self, budget=10000, dim=10, pop_size=None, F_init=0.5, Cr_init=0.9, memory_size=10, adapt_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size if pop_size is not None else 10 * dim\n        self.F = F_init\n        self.Cr = Cr_init\n        self.lb = -5.0\n        self.ub = 5.0\n        self.memory_size = memory_size\n        self.adapt_rate = adapt_rate\n        self.F_memory = np.full(self.memory_size, F_init)\n        self.Cr_memory = np.full(self.memory_size, Cr_init)\n        self.F_idx = 0\n        self.Cr_idx = 0\n        self.success_F = []\n        self.success_Cr = []\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.pop = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.nevals = self.pop_size\n\n        while self.nevals < self.budget:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = self.pop[idxs]\n\n                x_mutated = self.pop[i] + self.F * (x_r2 - x_r3)\n                x_mutated = np.clip(x_mutated, self.lb, self.ub)\n\n                # Crossover\n                x_trial = np.copy(self.pop[i])\n                crossover_points = np.random.rand(self.dim) < self.Cr\n                if not np.any(crossover_points):\n                    crossover_points[np.random.randint(0, self.dim)] = True\n                x_trial[crossover_points] = x_mutated[crossover_points]\n\n                # Evaluation\n                f_trial = func(x_trial)\n                self.nevals += 1\n\n                # Selection\n                if f_trial < self.fitness[i]:\n                    # Store successful F and Cr\n                    self.success_F.append(self.F)\n                    self.success_Cr.append(self.Cr)\n                    \n                    self.fitness[i] = f_trial\n                    self.pop[i] = x_trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = x_trial\n\n            # Adapt F and Cr (after each generation)\n            self.adapt_parameters()\n\n        return self.f_opt, self.x_opt\n\n    def adapt_parameters(self):\n        if self.success_F:\n            mean_success_F = np.mean(self.success_F)\n            self.F_memory[self.F_idx] = mean_success_F\n            self.F_idx = (self.F_idx + 1) % self.memory_size\n            \n            # Smoothing with adapt_rate\n            self.F = (1 - self.adapt_rate) * self.F + self.adapt_rate * np.mean(self.F_memory)\n            self.F = np.clip(self.F, 0.0, 1.0)\n            self.success_F = []  # Reset successful F values for the next generation\n\n        if self.success_Cr:\n            mean_success_Cr = np.mean(self.success_Cr)\n            self.Cr_memory[self.Cr_idx] = mean_success_Cr\n            self.Cr_idx = (self.Cr_idx + 1) % self.memory_size\n\n            # Smoothing with adapt_rate\n            self.Cr = (1 - self.adapt_rate) * self.Cr + self.adapt_rate * np.mean(self.Cr_memory)\n            self.Cr = np.clip(self.Cr, 0.0, 1.0)\n            self.success_Cr = []  # Reset successful Cr values for the next generation\n```", "operator": "crossover", "theta_sampled": 0.11900246214547046, "generation": 3, "aucs": [0.15299094195514007, 0.3163409050304603, 0.3901645573030198, 0.5560289006788741, 0.29465498425407555, 0.42787192684130815, 0.29622538953357114, 0.36971441241255076, 0.3422292303429765, 0.2042934636857362, 0.6318693865370595, 0.9993375104158558, 0.38452452401197235, 0.32009450974352527, 0.7590744029839736, 0.46454551742024264, 0.32625466215860455, 0.5340077014041307, 0.18155581465804993, 0.4819532480757468]}, "task_prompt": ""}
{"id": "1cc7f585-d618-4e24-9c24-dc1aa7e337d3", "fitness": "-inf", "name": "ShrinkingPopulationDE", "description": "An adaptive Differential Evolution algorithm with a population size that shrinks over time, focusing on promising regions.", "code": "import numpy as np\n\nclass ShrinkingPopulationDE:\n    def __init__(self, budget=10000, dim=10, initial_population_size=None, F=0.5, CR=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.F = F\n        self.CR = CR\n        self.initial_population_size = initial_population_size if initial_population_size is not None else 2 * dim\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        \n        population_size = self.initial_population_size\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(population_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        eval_count = population_size\n\n        while eval_count < self.budget:\n            \n            # Shrink the population every so often\n            if eval_count > self.budget * 0.75 and population_size > self.dim + 1:\n              \n              sorted_indices = np.argsort(fitness)\n              population = population[sorted_indices[:population_size // 2]]\n              fitness = fitness[sorted_indices[:population_size // 2]]\n              population_size = population_size // 2\n              \n\n            for i in range(population_size):\n                if eval_count >= self.budget:\n                    break\n\n                # Mutation\n                idxs = [idx for idx in range(population_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = a + self.F * (b - c)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                f_trial = func(trial)\n                eval_count += 1\n\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    population[i] = trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n            \n        best_index = np.argmin(fitness)\n        if fitness[best_index] < self.f_opt:\n            self.f_opt = fitness[best_index]\n            self.x_opt = population[best_index]\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "In the code, line 1025, in numpy.random.mtrand.RandomState.choice, the following error occurred:\nValueError: Cannot take a larger sample than population when 'replace=False'", "error": "In the code, line 1025, in numpy.random.mtrand.RandomState.choice, the following error occurred:\nValueError: Cannot take a larger sample than population when 'replace=False'", "parent_ids": [], "operator": "random_new", "metadata": {"llm_response": "# Description: An adaptive Differential Evolution algorithm with a population size that shrinks over time, focusing on promising regions.\n# Code: \n```python\nimport numpy as np\n\nclass ShrinkingPopulationDE:\n    def __init__(self, budget=10000, dim=10, initial_population_size=None, F=0.5, CR=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.F = F\n        self.CR = CR\n        self.initial_population_size = initial_population_size if initial_population_size is not None else 2 * dim\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        \n        population_size = self.initial_population_size\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(population_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        eval_count = population_size\n\n        while eval_count < self.budget:\n            \n            # Shrink the population every so often\n            if eval_count > self.budget * 0.75 and population_size > self.dim + 1:\n              \n              sorted_indices = np.argsort(fitness)\n              population = population[sorted_indices[:population_size // 2]]\n              fitness = fitness[sorted_indices[:population_size // 2]]\n              population_size = population_size // 2\n              \n\n            for i in range(population_size):\n                if eval_count >= self.budget:\n                    break\n\n                # Mutation\n                idxs = [idx for idx in range(population_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = a + self.F * (b - c)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                f_trial = func(trial)\n                eval_count += 1\n\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    population[i] = trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n            \n        best_index = np.argmin(fitness)\n        if fitness[best_index] < self.f_opt:\n            self.f_opt = fitness[best_index]\n            self.x_opt = population[best_index]\n\n\n        return self.f_opt, self.x_opt\n```", "operator": "random_new", "theta_sampled": 0.10667532444694157, "generation": 3}, "task_prompt": ""}
{"id": "9689318f-70fd-40ba-9e40-c83f300a1a16", "fitness": 0.4172120872657725, "name": "AdaptiveDEJitter", "description": "Adaptive DE with smoothed parameter adaptation and jitter-based mutation.", "code": "import numpy as np\n\nclass AdaptiveDEJitter:\n    def __init__(self, budget=10000, dim=10, pop_size=None, F_init=0.5, Cr_init=0.9, adapt_rate=0.1, jitter_rate=0.01):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size if pop_size is not None else 10 * dim\n        self.F = F_init\n        self.Cr = Cr_init\n        self.lb = -5.0\n        self.ub = 5.0\n        self.adapt_rate = adapt_rate\n        self.jitter_rate = jitter_rate\n        self.success_F = []\n        self.success_Cr = []\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.pop = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.nevals = self.pop_size\n\n        while self.nevals < self.budget:\n            for i in range(self.pop_size):\n                # Mutation with jitter\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = self.pop[idxs]\n\n                jitter = np.random.uniform(-self.jitter_rate, self.jitter_rate, size=self.dim)\n                x_mutated = self.pop[i] + self.F * (x_r2 - x_r3) + jitter\n                x_mutated = np.clip(x_mutated, self.lb, self.ub)\n\n                # Crossover\n                x_trial = np.copy(self.pop[i])\n                crossover_points = np.random.rand(self.dim) < self.Cr\n                if not np.any(crossover_points):\n                    crossover_points[np.random.randint(0, self.dim)] = True\n                x_trial[crossover_points] = x_mutated[crossover_points]\n\n                # Evaluation\n                f_trial = func(x_trial)\n                self.nevals += 1\n\n                # Selection\n                if f_trial < self.fitness[i]:\n                    # Store successful F and Cr\n                    self.success_F.append(self.F)\n                    self.success_Cr.append(self.Cr)\n                    \n                    self.fitness[i] = f_trial\n                    self.pop[i] = x_trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = x_trial\n\n            # Adapt F and Cr (after each generation)\n            self.adapt_parameters()\n\n        return self.f_opt, self.x_opt\n\n    def adapt_parameters(self):\n        if self.success_F:\n            self.F = (1 - self.adapt_rate) * self.F + self.adapt_rate * np.mean(self.success_F)\n            self.F = np.clip(self.F, 0.0, 1.0)\n            self.success_F = []\n\n        if self.success_Cr:\n            self.Cr = (1 - self.adapt_rate) * self.Cr + self.adapt_rate * np.mean(self.success_Cr)\n            self.Cr = np.clip(self.Cr, 0.0, 1.0)\n            self.success_Cr = []", "configspace": "", "generation": 3, "feedback": "The algorithm AdaptiveDEJitter scored 0.417 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["79cce7be-a121-4a1c-815e-61b8946185f0", "b3d902a5-e4f8-4f92-81fe-5f30bec513b6"], "operator": "crossover", "metadata": {"llm_response": "# Description: Adaptive DE with smoothed parameter adaptation and jitter-based mutation.\n# Code: \n```python\nimport numpy as np\n\nclass AdaptiveDEJitter:\n    def __init__(self, budget=10000, dim=10, pop_size=None, F_init=0.5, Cr_init=0.9, adapt_rate=0.1, jitter_rate=0.01):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size if pop_size is not None else 10 * dim\n        self.F = F_init\n        self.Cr = Cr_init\n        self.lb = -5.0\n        self.ub = 5.0\n        self.adapt_rate = adapt_rate\n        self.jitter_rate = jitter_rate\n        self.success_F = []\n        self.success_Cr = []\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.pop = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.nevals = self.pop_size\n\n        while self.nevals < self.budget:\n            for i in range(self.pop_size):\n                # Mutation with jitter\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = self.pop[idxs]\n\n                jitter = np.random.uniform(-self.jitter_rate, self.jitter_rate, size=self.dim)\n                x_mutated = self.pop[i] + self.F * (x_r2 - x_r3) + jitter\n                x_mutated = np.clip(x_mutated, self.lb, self.ub)\n\n                # Crossover\n                x_trial = np.copy(self.pop[i])\n                crossover_points = np.random.rand(self.dim) < self.Cr\n                if not np.any(crossover_points):\n                    crossover_points[np.random.randint(0, self.dim)] = True\n                x_trial[crossover_points] = x_mutated[crossover_points]\n\n                # Evaluation\n                f_trial = func(x_trial)\n                self.nevals += 1\n\n                # Selection\n                if f_trial < self.fitness[i]:\n                    # Store successful F and Cr\n                    self.success_F.append(self.F)\n                    self.success_Cr.append(self.Cr)\n                    \n                    self.fitness[i] = f_trial\n                    self.pop[i] = x_trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = x_trial\n\n            # Adapt F and Cr (after each generation)\n            self.adapt_parameters()\n\n        return self.f_opt, self.x_opt\n\n    def adapt_parameters(self):\n        if self.success_F:\n            self.F = (1 - self.adapt_rate) * self.F + self.adapt_rate * np.mean(self.success_F)\n            self.F = np.clip(self.F, 0.0, 1.0)\n            self.success_F = []\n\n        if self.success_Cr:\n            self.Cr = (1 - self.adapt_rate) * self.Cr + self.adapt_rate * np.mean(self.success_Cr)\n            self.Cr = np.clip(self.Cr, 0.0, 1.0)\n            self.success_Cr = []\n```", "operator": "crossover", "theta_sampled": 0.010568620251967725, "generation": 3, "aucs": [0.14916221402267726, 0.3035284857301751, 0.3828372592732492, 0.5379031649285083, 0.31744646199844506, 0.45247443954728983, 0.3117779994822004, 0.35936809139542225, 0.31763179457470303, 0.19714140462396856, 0.6387184462736792, 0.9999124655509979, 0.3804974975926375, 0.29935964272882365, 0.725819118399565, 0.41737215323400956, 0.34012205439998566, 0.531165717956126, 0.1923217851294161, 0.48968154847357104]}, "task_prompt": ""}
{"id": "d7d9f5ba-770d-44f2-a065-8db68680812f", "fitness": 0.28262055229372357, "name": "AdaptiveDESmoothedVelocity", "description": "Adaptive Differential Evolution with smoothed parameter adaptation and velocity-based population initialization.", "code": "import numpy as np\n\nclass AdaptiveDESmoothedVelocity:\n    def __init__(self, budget=10000, dim=10, pop_size=None, F_init=0.5, Cr_init=0.9, adapt_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size if pop_size is not None else 10 * dim\n        self.F = F_init\n        self.Cr = Cr_init\n        self.lb = -5.0\n        self.ub = 5.0\n        self.adapt_rate = adapt_rate\n        self.success_F = []\n        self.success_Cr = []\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        \n        # Initialize population with velocities\n        self.pop = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.velocities = np.random.uniform(-1.0, 1.0, size=(self.pop_size, self.dim)) * 0.1  # Smaller initial velocities\n        \n        self.fitness = np.array([func(x) for x in self.pop])\n        self.nevals = self.pop_size\n\n        while self.nevals < self.budget:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = self.pop[idxs]\n\n                x_mutated = self.pop[i] + self.F * (x_r2 - x_r3)\n                x_mutated = np.clip(x_mutated, self.lb, self.ub)\n\n                # Crossover\n                x_trial = np.copy(self.pop[i])\n                crossover_points = np.random.rand(self.dim) < self.Cr\n                if not np.any(crossover_points):\n                    crossover_points[np.random.randint(0, self.dim)] = True\n                x_trial[crossover_points] = x_mutated[crossover_points]\n\n                # Evaluation\n                f_trial = func(x_trial)\n                self.nevals += 1\n\n                # Selection\n                if f_trial < self.fitness[i]:\n                    # Store successful F and Cr\n                    self.success_F.append(self.F)\n                    self.success_Cr.append(self.Cr)\n                    \n                    self.fitness[i] = f_trial\n                    self.pop[i] = x_trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = x_trial\n                \n                # Update velocity based on the trial\n                self.velocities[i] = 0.5 * self.velocities[i] + 0.5 * (x_trial - self.pop[i])\n                self.pop[i] = np.clip(self.pop[i] + self.velocities[i], self.lb, self.ub)\n\n            # Adapt F and Cr (after each generation)\n            self.adapt_parameters()\n\n        return self.f_opt, self.x_opt\n\n    def adapt_parameters(self):\n        if self.success_F:\n            self.F = (1 - self.adapt_rate) * self.F + self.adapt_rate * np.mean(self.success_F)\n            self.F = np.clip(self.F, 0.0, 1.0)\n            self.success_F = []\n\n        if self.success_Cr:\n            self.Cr = (1 - self.adapt_rate) * self.Cr + self.adapt_rate * np.mean(self.success_Cr)\n            self.Cr = np.clip(self.Cr, 0.0, 1.0)\n            self.success_Cr = []", "configspace": "", "generation": 3, "feedback": "The algorithm AdaptiveDESmoothedVelocity scored 0.283 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["79cce7be-a121-4a1c-815e-61b8946185f0", "b3d902a5-e4f8-4f92-81fe-5f30bec513b6"], "operator": "crossover", "metadata": {"llm_response": "# Description: Adaptive Differential Evolution with smoothed parameter adaptation and velocity-based population initialization.\n# Code: \n```python\nimport numpy as np\n\nclass AdaptiveDESmoothedVelocity:\n    def __init__(self, budget=10000, dim=10, pop_size=None, F_init=0.5, Cr_init=0.9, adapt_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size if pop_size is not None else 10 * dim\n        self.F = F_init\n        self.Cr = Cr_init\n        self.lb = -5.0\n        self.ub = 5.0\n        self.adapt_rate = adapt_rate\n        self.success_F = []\n        self.success_Cr = []\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        \n        # Initialize population with velocities\n        self.pop = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.velocities = np.random.uniform(-1.0, 1.0, size=(self.pop_size, self.dim)) * 0.1  # Smaller initial velocities\n        \n        self.fitness = np.array([func(x) for x in self.pop])\n        self.nevals = self.pop_size\n\n        while self.nevals < self.budget:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = self.pop[idxs]\n\n                x_mutated = self.pop[i] + self.F * (x_r2 - x_r3)\n                x_mutated = np.clip(x_mutated, self.lb, self.ub)\n\n                # Crossover\n                x_trial = np.copy(self.pop[i])\n                crossover_points = np.random.rand(self.dim) < self.Cr\n                if not np.any(crossover_points):\n                    crossover_points[np.random.randint(0, self.dim)] = True\n                x_trial[crossover_points] = x_mutated[crossover_points]\n\n                # Evaluation\n                f_trial = func(x_trial)\n                self.nevals += 1\n\n                # Selection\n                if f_trial < self.fitness[i]:\n                    # Store successful F and Cr\n                    self.success_F.append(self.F)\n                    self.success_Cr.append(self.Cr)\n                    \n                    self.fitness[i] = f_trial\n                    self.pop[i] = x_trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = x_trial\n                \n                # Update velocity based on the trial\n                self.velocities[i] = 0.5 * self.velocities[i] + 0.5 * (x_trial - self.pop[i])\n                self.pop[i] = np.clip(self.pop[i] + self.velocities[i], self.lb, self.ub)\n\n            # Adapt F and Cr (after each generation)\n            self.adapt_parameters()\n\n        return self.f_opt, self.x_opt\n\n    def adapt_parameters(self):\n        if self.success_F:\n            self.F = (1 - self.adapt_rate) * self.F + self.adapt_rate * np.mean(self.success_F)\n            self.F = np.clip(self.F, 0.0, 1.0)\n            self.success_F = []\n\n        if self.success_Cr:\n            self.Cr = (1 - self.adapt_rate) * self.Cr + self.adapt_rate * np.mean(self.success_Cr)\n            self.Cr = np.clip(self.Cr, 0.0, 1.0)\n            self.success_Cr = []\n```", "operator": "crossover", "theta_sampled": 0.08929444276734828, "generation": 3, "aucs": [0.1036211396683906, 0.1776474391432481, 0.2827630852839129, 0.1828994763846289, 0.2091790315594526, 0.22420940995803962, 0.23256192909938023, 0.22650738259279057, 0.203012880791134, 0.16809496276384672, 0.20472709150136847, 0.9935534277088867, 0.2402106337805009, 0.24224123109068252, 0.5844234181630532, 0.27423757328198595, 0.22280051218106223, 0.24638907280950195, 0.17146857540066707, 0.46186277271193854]}, "task_prompt": ""}
{"id": "50253252-7829-45ff-ac5d-f27c728e3aa7", "fitness": "-inf", "name": "AdaptiveDEPopSize", "description": "An adaptive Differential Evolution (DE) algorithm with a dynamic population size and a mutation strategy selection mechanism based on past performance.", "code": "import numpy as np\n\nclass AdaptiveDEPopSize:\n    def __init__(self, budget=10000, dim=10, pop_size_factor=5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size_factor = pop_size_factor\n        self.pop_size = int(self.pop_size_factor * self.dim)\n        self.CR = 0.5\n        self.F = 0.7\n        self.mutation_strategies = [\"rand1bin\", \"best1bin\", \"currenttobest1bin\"]\n        self.strategy_success = np.zeros(len(self.mutation_strategies))\n        self.strategy_counts = np.zeros(len(self.mutation_strategies))\n        self.archive_factor = 2.0 #archive size related to population size\n        self.archive = []\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.archive = []\n        \n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.n_evals = self.pop_size # Initial evaluation cost\n\n        while self.n_evals < self.budget:\n            # Adaptive Population Size\n            self.pop_size = int(self.pop_size_factor * self.dim * (1 - (self.n_evals / self.budget))) + self.dim # Reduce population size over time\n            self.pop_size = max(self.dim + 1, self.pop_size) #Ensure population size is above dim + 1 to enable DE\n            if population.shape[0] > self.pop_size:\n                 # Prune worst individuals if pop size decreased\n                indices_to_keep = np.argsort(fitness)[:self.pop_size]\n                population = population[indices_to_keep]\n                fitness = fitness[indices_to_keep]\n            elif population.shape[0] < self.pop_size:\n                # Add new random individuals\n                num_to_add = self.pop_size - population.shape[0]\n                new_individuals = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(num_to_add, self.dim))\n                population = np.concatenate((population, new_individuals))\n                new_fitness = np.array([func(x) for x in new_individuals])\n                fitness = np.concatenate((fitness, new_fitness))\n                self.n_evals += num_to_add\n\n\n            for i in range(population.shape[0]):\n                # Strategy Selection\n                strategy_index = np.argmax(self.strategy_success) # Choose strategy with best recent success\n                self.strategy_counts[strategy_index] += 1\n                strategy = self.mutation_strategies[strategy_index]\n\n                # Mutation\n                if strategy == \"rand1bin\":\n                    idxs = np.random.choice(population.shape[0], 3, replace=False)\n                    x1, x2, x3 = population[idxs]\n                    v = population[i] + self.F * (x1 - x2)\n                elif strategy == \"best1bin\":\n                    best_idx = np.argmin(fitness)\n                    best = population[best_idx]\n                    idxs = np.random.choice(population.shape[0], 2, replace=False)\n                    x1, x2 = population[idxs]\n                    v = population[i] + self.F * (best - population[i]) + self.F * (x1 - x2)\n                elif strategy == \"currenttobest1bin\":\n                    best_idx = np.argmin(fitness)\n                    best = population[best_idx]\n                    idxs = np.random.choice(population.shape[0], 1, replace=False)\n                    x1 = population[idxs[0]]\n\n                    v = population[i] + self.F * (best - population[i]) + self.F * (population[i] - x1)\n\n                # Crossover\n                jrand = np.random.randint(self.dim)\n                u = np.array([v[j] if np.random.rand() < self.CR or j == jrand else population[i, j] for j in range(self.dim)])\n\n                # Boundary Handling\n                u = np.clip(u, func.bounds.lb, func.bounds.ub)\n                \n                # Selection\n                f_u = func(u)\n                self.n_evals += 1\n                \n                if f_u < fitness[i]:\n                    #Update archive\n                    self.archive.append(population[i])\n                    population[i] = u\n                    fitness[i] = f_u\n                    self.strategy_success[strategy_index] += (fitness[i] - f_u)  # Reward successful strategies\n                else:\n                    self.archive.append(u)\n                \n\n                if f_u < self.f_opt:\n                    self.f_opt = f_u\n                    self.x_opt = u\n            \n            # Normalize strategy success\n            self.strategy_success = self.strategy_success / (self.strategy_counts + 1e-9) \n            self.strategy_counts[:] = 0 # reset strategy counts each generation\n\n\n            # Archive management\n            if len(self.archive) > self.pop_size * self.archive_factor:\n                self.archive = self.archive[-int(self.pop_size * self.archive_factor):]\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "Evaluation timed out after 300 seconds.", "error": "Evaluation timed out after 300 seconds.", "parent_ids": [], "operator": "random_new", "metadata": {"llm_response": "# Description: An adaptive Differential Evolution (DE) algorithm with a dynamic population size and a mutation strategy selection mechanism based on past performance.\n# Code: \n```python\nimport numpy as np\n\nclass AdaptiveDEPopSize:\n    def __init__(self, budget=10000, dim=10, pop_size_factor=5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size_factor = pop_size_factor\n        self.pop_size = int(self.pop_size_factor * self.dim)\n        self.CR = 0.5\n        self.F = 0.7\n        self.mutation_strategies = [\"rand1bin\", \"best1bin\", \"currenttobest1bin\"]\n        self.strategy_success = np.zeros(len(self.mutation_strategies))\n        self.strategy_counts = np.zeros(len(self.mutation_strategies))\n        self.archive_factor = 2.0 #archive size related to population size\n        self.archive = []\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.archive = []\n        \n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.n_evals = self.pop_size # Initial evaluation cost\n\n        while self.n_evals < self.budget:\n            # Adaptive Population Size\n            self.pop_size = int(self.pop_size_factor * self.dim * (1 - (self.n_evals / self.budget))) + self.dim # Reduce population size over time\n            self.pop_size = max(self.dim + 1, self.pop_size) #Ensure population size is above dim + 1 to enable DE\n            if population.shape[0] > self.pop_size:\n                 # Prune worst individuals if pop size decreased\n                indices_to_keep = np.argsort(fitness)[:self.pop_size]\n                population = population[indices_to_keep]\n                fitness = fitness[indices_to_keep]\n            elif population.shape[0] < self.pop_size:\n                # Add new random individuals\n                num_to_add = self.pop_size - population.shape[0]\n                new_individuals = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(num_to_add, self.dim))\n                population = np.concatenate((population, new_individuals))\n                new_fitness = np.array([func(x) for x in new_individuals])\n                fitness = np.concatenate((fitness, new_fitness))\n                self.n_evals += num_to_add\n\n\n            for i in range(population.shape[0]):\n                # Strategy Selection\n                strategy_index = np.argmax(self.strategy_success) # Choose strategy with best recent success\n                self.strategy_counts[strategy_index] += 1\n                strategy = self.mutation_strategies[strategy_index]\n\n                # Mutation\n                if strategy == \"rand1bin\":\n                    idxs = np.random.choice(population.shape[0], 3, replace=False)\n                    x1, x2, x3 = population[idxs]\n                    v = population[i] + self.F * (x1 - x2)\n                elif strategy == \"best1bin\":\n                    best_idx = np.argmin(fitness)\n                    best = population[best_idx]\n                    idxs = np.random.choice(population.shape[0], 2, replace=False)\n                    x1, x2 = population[idxs]\n                    v = population[i] + self.F * (best - population[i]) + self.F * (x1 - x2)\n                elif strategy == \"currenttobest1bin\":\n                    best_idx = np.argmin(fitness)\n                    best = population[best_idx]\n                    idxs = np.random.choice(population.shape[0], 1, replace=False)\n                    x1 = population[idxs[0]]\n\n                    v = population[i] + self.F * (best - population[i]) + self.F * (population[i] - x1)\n\n                # Crossover\n                jrand = np.random.randint(self.dim)\n                u = np.array([v[j] if np.random.rand() < self.CR or j == jrand else population[i, j] for j in range(self.dim)])\n\n                # Boundary Handling\n                u = np.clip(u, func.bounds.lb, func.bounds.ub)\n                \n                # Selection\n                f_u = func(u)\n                self.n_evals += 1\n                \n                if f_u < fitness[i]:\n                    #Update archive\n                    self.archive.append(population[i])\n                    population[i] = u\n                    fitness[i] = f_u\n                    self.strategy_success[strategy_index] += (fitness[i] - f_u)  # Reward successful strategies\n                else:\n                    self.archive.append(u)\n                \n\n                if f_u < self.f_opt:\n                    self.f_opt = f_u\n                    self.x_opt = u\n            \n            # Normalize strategy success\n            self.strategy_success = self.strategy_success / (self.strategy_counts + 1e-9) \n            self.strategy_counts[:] = 0 # reset strategy counts each generation\n\n\n            # Archive management\n            if len(self.archive) > self.pop_size * self.archive_factor:\n                self.archive = self.archive[-int(self.pop_size * self.archive_factor):]\n\n\n        return self.f_opt, self.x_opt\n```", "operator": "random_new", "theta_sampled": 0.12106145262606703, "generation": 3}, "task_prompt": ""}
{"id": "40bd89f0-32fe-4e6f-8887-5e032130b8e1", "fitness": 0.4521538703159219, "name": "AdaptiveDEWindow", "description": "An adaptive Differential Evolution strategy that adjusts its parameters based on the success of past parameter choices, using a sliding window to track performance. It favors exploitation when near-optimal solutions are found and exploration when diversity is low.", "code": "import numpy as np\n\nclass AdaptiveDEWindow:\n    def __init__(self, budget=10000, dim=10, pop_size=40, F=0.5, Cr=0.9, window_size=50):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F  # Initial scaling factor\n        self.Cr = Cr  # Initial crossover rate\n        self.window_size = window_size\n        self.F_history = []\n        self.Cr_history = []\n        self.success_history = []\n        self.archive_rate = 0.1\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in self.population])\n        self.n_evals = self.pop_size\n        \n        while self.n_evals < self.budget:\n            for i in range(self.pop_size):\n                # Parameter adaptation based on history\n                if len(self.F_history) > 0:\n                    recent_F = self.F_history[-self.window_size:]\n                    self.F = np.mean(recent_F) if len(recent_F) > 0 else 0.5\n                    self.F = np.clip(self.F, 0.1, 1.0)  # Keep F within bounds\n\n                    recent_Cr = self.Cr_history[-self.window_size:]\n                    self.Cr = np.mean(recent_Cr) if len(recent_Cr) > 0 else 0.9\n                    self.Cr = np.clip(self.Cr, 0.1, 1.0)  # Keep Cr within bounds\n                \n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_1, x_2, x_3 = self.population[idxs]\n                v = self.population[i] + self.F * (x_2 - x_3)\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n                \n                # Crossover\n                j_rand = np.random.randint(self.dim)\n                u = np.array([v[j] if (np.random.rand() < self.Cr or j == j_rand) else self.population[i, j] for j in range(self.dim)])\n\n                # Selection\n                f_u = func(u)\n                self.n_evals += 1\n                \n                if f_u < fitness[i]:\n                    self.population[i] = u\n                    fitness[i] = f_u\n                    self.F_history.append(self.F)\n                    self.Cr_history.append(self.Cr)\n                    self.success_history.append(1)  # Success\n                else:\n                    self.success_history.append(0) # Failure\n                    \n                if f_u < self.f_opt:\n                    self.f_opt = f_u\n                    self.x_opt = u\n\n                if self.n_evals >= self.budget:\n                    break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "The algorithm AdaptiveDEWindow scored 0.452 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": "random_new", "metadata": {"llm_response": "# Description: An adaptive Differential Evolution strategy that adjusts its parameters based on the success of past parameter choices, using a sliding window to track performance. It favors exploitation when near-optimal solutions are found and exploration when diversity is low.\n# Code: \n```python\nimport numpy as np\n\nclass AdaptiveDEWindow:\n    def __init__(self, budget=10000, dim=10, pop_size=40, F=0.5, Cr=0.9, window_size=50):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F  # Initial scaling factor\n        self.Cr = Cr  # Initial crossover rate\n        self.window_size = window_size\n        self.F_history = []\n        self.Cr_history = []\n        self.success_history = []\n        self.archive_rate = 0.1\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in self.population])\n        self.n_evals = self.pop_size\n        \n        while self.n_evals < self.budget:\n            for i in range(self.pop_size):\n                # Parameter adaptation based on history\n                if len(self.F_history) > 0:\n                    recent_F = self.F_history[-self.window_size:]\n                    self.F = np.mean(recent_F) if len(recent_F) > 0 else 0.5\n                    self.F = np.clip(self.F, 0.1, 1.0)  # Keep F within bounds\n\n                    recent_Cr = self.Cr_history[-self.window_size:]\n                    self.Cr = np.mean(recent_Cr) if len(recent_Cr) > 0 else 0.9\n                    self.Cr = np.clip(self.Cr, 0.1, 1.0)  # Keep Cr within bounds\n                \n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_1, x_2, x_3 = self.population[idxs]\n                v = self.population[i] + self.F * (x_2 - x_3)\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n                \n                # Crossover\n                j_rand = np.random.randint(self.dim)\n                u = np.array([v[j] if (np.random.rand() < self.Cr or j == j_rand) else self.population[i, j] for j in range(self.dim)])\n\n                # Selection\n                f_u = func(u)\n                self.n_evals += 1\n                \n                if f_u < fitness[i]:\n                    self.population[i] = u\n                    fitness[i] = f_u\n                    self.F_history.append(self.F)\n                    self.Cr_history.append(self.Cr)\n                    self.success_history.append(1)  # Success\n                else:\n                    self.success_history.append(0) # Failure\n                    \n                if f_u < self.f_opt:\n                    self.f_opt = f_u\n                    self.x_opt = u\n\n                if self.n_evals >= self.budget:\n                    break\n\n        return self.f_opt, self.x_opt\n```", "operator": "random_new", "theta_sampled": 0.28250897960261306, "generation": 4, "aucs": [0.16190736031509745, 0.34827434252245115, 0.4331111078695927, 0.6393469855755056, 0.34544949491158194, 0.5049383283963882, 0.30366680423300363, 0.37964898449953954, 0.35999408358357676, 0.19448068044375955, 0.656786102123797, 0.9952982233736851, 0.42486590453414663, 0.3431337026045246, 0.8474432157809728, 0.4859512114653468, 0.35663664444932197, 0.5959773262177548, 0.18245413107244401, 0.483712772345949]}, "task_prompt": ""}
{"id": "b9f91bd8-d6aa-452c-bd8e-2b205563708d", "fitness": 0.42411786224214126, "name": "AdaptiveDESmoothedMovingAverage", "description": "Adaptive DE with moving average and smoothed adaptation rate, prioritizing recent successful F/Cr values.", "code": "import numpy as np\n\nclass AdaptiveDESmoothedMovingAverage:\n    def __init__(self, budget=10000, dim=10, pop_size=None, F_init=0.5, Cr_init=0.9, memory_size=10, adapt_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size if pop_size is not None else 10 * dim\n        self.F = F_init\n        self.Cr = Cr_init\n        self.lb = -5.0\n        self.ub = 5.0\n        self.memory_size = memory_size\n        self.adapt_rate = adapt_rate\n        self.F_memory = np.full(self.memory_size, F_init)\n        self.Cr_memory = np.full(self.memory_size, Cr_init)\n        self.F_idx = 0\n        self.Cr_idx = 0\n        self.success_F = []\n        self.success_Cr = []\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.pop = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.nevals = self.pop_size\n\n        while self.nevals < self.budget:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = self.pop[idxs]\n\n                x_mutated = self.pop[i] + self.F * (x_r2 - x_r3)\n                x_mutated = np.clip(x_mutated, self.lb, self.ub)\n\n                # Crossover\n                x_trial = np.copy(self.pop[i])\n                crossover_points = np.random.rand(self.dim) < self.Cr\n                if not np.any(crossover_points):\n                    crossover_points[np.random.randint(0, self.dim)] = True\n                x_trial[crossover_points] = x_mutated[crossover_points]\n\n                # Evaluation\n                f_trial = func(x_trial)\n                self.nevals += 1\n\n                # Selection\n                if f_trial < self.fitness[i]:\n                    # Store successful F and Cr\n                    self.success_F.append(self.F)\n                    self.success_Cr.append(self.Cr)\n                    \n                    self.fitness[i] = f_trial\n                    self.pop[i] = x_trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = x_trial\n\n            # Adapt F and Cr (after each generation)\n            self.adapt_parameters()\n\n        return self.f_opt, self.x_opt\n\n    def adapt_parameters(self):\n        if self.success_F:\n            # Smoothed update of F memory\n            new_F = np.mean(self.success_F)\n            self.F_memory[self.F_idx] = (1 - self.adapt_rate) * self.F_memory[self.F_idx] + self.adapt_rate * new_F\n            self.F_idx = (self.F_idx + 1) % self.memory_size\n            self.F = np.mean(self.F_memory)\n            self.F = np.clip(self.F, 0.0, 1.0)\n            self.success_F = []  # Reset successful F values\n\n        if self.success_Cr:\n            # Smoothed update of Cr memory\n            new_Cr = np.mean(self.success_Cr)\n            self.Cr_memory[self.Cr_idx] = (1 - self.adapt_rate) * self.Cr_memory[self.Cr_idx] + self.adapt_rate * new_Cr\n            self.Cr_idx = (self.Cr_idx + 1) % self.memory_size\n            self.Cr = np.mean(self.Cr_memory)\n            self.Cr = np.clip(self.Cr, 0.0, 1.0)\n            self.success_Cr = []  # Reset successful Cr values", "configspace": "", "generation": 4, "feedback": "The algorithm AdaptiveDESmoothedMovingAverage scored 0.424 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["79cce7be-a121-4a1c-815e-61b8946185f0", "b3d902a5-e4f8-4f92-81fe-5f30bec513b6"], "operator": "crossover", "metadata": {"llm_response": "# Description: Adaptive DE with moving average and smoothed adaptation rate, prioritizing recent successful F/Cr values.\n# Code: \n```python\nimport numpy as np\n\nclass AdaptiveDESmoothedMovingAverage:\n    def __init__(self, budget=10000, dim=10, pop_size=None, F_init=0.5, Cr_init=0.9, memory_size=10, adapt_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size if pop_size is not None else 10 * dim\n        self.F = F_init\n        self.Cr = Cr_init\n        self.lb = -5.0\n        self.ub = 5.0\n        self.memory_size = memory_size\n        self.adapt_rate = adapt_rate\n        self.F_memory = np.full(self.memory_size, F_init)\n        self.Cr_memory = np.full(self.memory_size, Cr_init)\n        self.F_idx = 0\n        self.Cr_idx = 0\n        self.success_F = []\n        self.success_Cr = []\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.pop = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.nevals = self.pop_size\n\n        while self.nevals < self.budget:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = self.pop[idxs]\n\n                x_mutated = self.pop[i] + self.F * (x_r2 - x_r3)\n                x_mutated = np.clip(x_mutated, self.lb, self.ub)\n\n                # Crossover\n                x_trial = np.copy(self.pop[i])\n                crossover_points = np.random.rand(self.dim) < self.Cr\n                if not np.any(crossover_points):\n                    crossover_points[np.random.randint(0, self.dim)] = True\n                x_trial[crossover_points] = x_mutated[crossover_points]\n\n                # Evaluation\n                f_trial = func(x_trial)\n                self.nevals += 1\n\n                # Selection\n                if f_trial < self.fitness[i]:\n                    # Store successful F and Cr\n                    self.success_F.append(self.F)\n                    self.success_Cr.append(self.Cr)\n                    \n                    self.fitness[i] = f_trial\n                    self.pop[i] = x_trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = x_trial\n\n            # Adapt F and Cr (after each generation)\n            self.adapt_parameters()\n\n        return self.f_opt, self.x_opt\n\n    def adapt_parameters(self):\n        if self.success_F:\n            # Smoothed update of F memory\n            new_F = np.mean(self.success_F)\n            self.F_memory[self.F_idx] = (1 - self.adapt_rate) * self.F_memory[self.F_idx] + self.adapt_rate * new_F\n            self.F_idx = (self.F_idx + 1) % self.memory_size\n            self.F = np.mean(self.F_memory)\n            self.F = np.clip(self.F, 0.0, 1.0)\n            self.success_F = []  # Reset successful F values\n\n        if self.success_Cr:\n            # Smoothed update of Cr memory\n            new_Cr = np.mean(self.success_Cr)\n            self.Cr_memory[self.Cr_idx] = (1 - self.adapt_rate) * self.Cr_memory[self.Cr_idx] + self.adapt_rate * new_Cr\n            self.Cr_idx = (self.Cr_idx + 1) % self.memory_size\n            self.Cr = np.mean(self.Cr_memory)\n            self.Cr = np.clip(self.Cr, 0.0, 1.0)\n            self.success_Cr = []  # Reset successful Cr values\n```", "operator": "crossover", "theta_sampled": 0.05384875225965015, "generation": 4, "aucs": [0.16743486339780933, 0.3099659108715366, 0.3928893455429062, 0.5779500750707418, 0.3275906332752383, 0.43791038186893927, 0.30080673380196177, 0.35139657549972847, 0.3415939193321602, 0.19315732694117438, 0.5849485575617552, 0.9924837668883743, 0.3962631798281018, 0.30440556036607014, 0.7925329923419357, 0.4399491366447815, 0.3425087476457618, 0.5610136406440333, 0.17905935810699414, 0.48849653921282166]}, "task_prompt": ""}
{"id": "1d0b36d9-7389-4b21-82a2-8885bdb85699", "fitness": "-inf", "name": "APSODE", "description": "A population-based algorithm that combines aspects of Particle Swarm Optimization and Differential Evolution with adaptive parameter control based on success rate.", "code": "import numpy as np\n\nclass APSODE:\n    def __init__(self, budget=10000, dim=10, pop_size=20, w_init=0.9, w_end=0.4, c1=2.0, c2=2.0, F=0.5, CR=0.7, success_rate_memory=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.w_init = w_init\n        self.w_end = w_end\n        self.c1 = c1\n        self.c2 = c2\n        self.F = F\n        self.CR = CR\n        self.success_rate_memory = success_rate_memory\n        self.success_history_F = []\n        self.success_history_CR = []\n        self.archive = []\n\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        self.best_index = np.argmin(self.fitness)\n        self.global_best_position = self.population[self.best_index].copy()\n        self.global_best_fitness = self.fitness[self.best_index].copy()\n\n        for i in range(self.budget):\n            # Inertia weight adaptation\n            w = self.w_init - (self.w_init - self.w_end) * (i / self.budget)\n\n            # Parameter adaptation based on success history\n            if self.success_history_F:\n                self.F = np.mean(self.success_history_F)\n            if self.success_history_CR:\n                self.CR = np.mean(self.success_history_CR)\n\n            for j in range(self.pop_size):\n                # PSO update\n                velocity = w * (self.population[j] - self.population[j]) + \\\n                           self.c1 * np.random.rand(self.dim) * (self.global_best_position - self.population[j]) + \\\n                           self.c2 * np.random.rand(self.dim) * (self.population[np.random.randint(self.pop_size)] - self.population[j])\n\n                # DE mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != j]\n                a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n                mutant = a + self.F * (b - c)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                  cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, self.population[j])\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub) # Respect bounds\n\n\n                f_trial = func(trial)\n\n                if f_trial < self.fitness[j]:\n                    self.population[j] = trial\n                    self.fitness[j] = f_trial\n\n                    # Update success history for F and CR\n                    self.success_history_F.append(self.F)\n                    self.success_history_CR.append(self.CR)\n\n                    # Keep the memory of success parameters to a certain size\n                    if len(self.success_history_F) > self.success_rate_memory:\n                        self.success_history_F.pop(0)\n                        self.success_history_CR.pop(0)\n\n                    # Archive for potential parameter adaptation (not used)\n                    self.archive.append(self.population[j])\n\n                # Global best update\n                if f_trial < self.global_best_fitness:\n                   self.global_best_fitness = f_trial\n                   self.global_best_position = trial.copy()\n                \n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = trial.copy()\n            \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "Evaluation timed out after 300 seconds.", "error": "Evaluation timed out after 300 seconds.", "parent_ids": [], "operator": "random_new", "metadata": {"llm_response": "# Description: A population-based algorithm that combines aspects of Particle Swarm Optimization and Differential Evolution with adaptive parameter control based on success rate.\n\n# Code:\n```python\nimport numpy as np\n\nclass APSODE:\n    def __init__(self, budget=10000, dim=10, pop_size=20, w_init=0.9, w_end=0.4, c1=2.0, c2=2.0, F=0.5, CR=0.7, success_rate_memory=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.w_init = w_init\n        self.w_end = w_end\n        self.c1 = c1\n        self.c2 = c2\n        self.F = F\n        self.CR = CR\n        self.success_rate_memory = success_rate_memory\n        self.success_history_F = []\n        self.success_history_CR = []\n        self.archive = []\n\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        self.best_index = np.argmin(self.fitness)\n        self.global_best_position = self.population[self.best_index].copy()\n        self.global_best_fitness = self.fitness[self.best_index].copy()\n\n        for i in range(self.budget):\n            # Inertia weight adaptation\n            w = self.w_init - (self.w_init - self.w_end) * (i / self.budget)\n\n            # Parameter adaptation based on success history\n            if self.success_history_F:\n                self.F = np.mean(self.success_history_F)\n            if self.success_history_CR:\n                self.CR = np.mean(self.success_history_CR)\n\n            for j in range(self.pop_size):\n                # PSO update\n                velocity = w * (self.population[j] - self.population[j]) + \\\n                           self.c1 * np.random.rand(self.dim) * (self.global_best_position - self.population[j]) + \\\n                           self.c2 * np.random.rand(self.dim) * (self.population[np.random.randint(self.pop_size)] - self.population[j])\n\n                # DE mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != j]\n                a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n                mutant = a + self.F * (b - c)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                  cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, self.population[j])\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub) # Respect bounds\n\n\n                f_trial = func(trial)\n\n                if f_trial < self.fitness[j]:\n                    self.population[j] = trial\n                    self.fitness[j] = f_trial\n\n                    # Update success history for F and CR\n                    self.success_history_F.append(self.F)\n                    self.success_history_CR.append(self.CR)\n\n                    # Keep the memory of success parameters to a certain size\n                    if len(self.success_history_F) > self.success_rate_memory:\n                        self.success_history_F.pop(0)\n                        self.success_history_CR.pop(0)\n\n                    # Archive for potential parameter adaptation (not used)\n                    self.archive.append(self.population[j])\n\n                # Global best update\n                if f_trial < self.global_best_fitness:\n                   self.global_best_fitness = f_trial\n                   self.global_best_position = trial.copy()\n                \n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = trial.copy()\n            \n        return self.f_opt, self.x_opt\n```", "operator": "random_new", "theta_sampled": 0.01880924024039778, "generation": 4}, "task_prompt": ""}
{"id": "3cc1727b-93ea-4b6e-a051-d1d034765666", "fitness": "-inf", "name": "VelocityRestartPSO", "description": "Population-based algorithm using velocity updates and a probabilistic restart mechanism based on stagnation.", "code": "import numpy as np\n\nclass VelocityRestartPSO:\n    def __init__(self, budget=10000, dim=10, pop_size=20, inertia=0.7, cognitive_coeff=1.5, social_coeff=1.5, restart_prob=0.05):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.inertia = inertia\n        self.cognitive_coeff = cognitive_coeff\n        self.social_coeff = social_coeff\n        self.restart_prob = restart_prob\n\n    def __call__(self, func):\n        # Initialization\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        velocities = np.random.uniform(-1, 1, size=(self.pop_size, self.dim)) * 0.1  # Initialize with small velocities\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n        \n        personal_best_positions = population.copy()\n        personal_best_fitness = fitness.copy()\n        \n        global_best_index = np.argmin(fitness)\n        global_best_position = population[global_best_index].copy()\n        global_best_fitness = fitness[global_best_index]\n        \n        stagnation_counter = 0\n        \n        while self.budget > 0:\n            # Update velocities and positions\n            r1 = np.random.rand(self.pop_size, self.dim)\n            r2 = np.random.rand(self.pop_size, self.dim)\n            \n            velocities = (self.inertia * velocities +\n                          self.cognitive_coeff * r1 * (personal_best_positions - population) +\n                          self.social_coeff * r2 * (global_best_position - population))\n            \n            population += velocities\n            \n            # Boundary handling (clip to bounds)\n            population = np.clip(population, func.bounds.lb, func.bounds.ub)\n            \n            # Evaluate new positions\n            new_fitness = np.array([func(x) if self.budget > 0 else np.inf for x in population])\n            valid_indices = new_fitness != np.inf\n            num_valid = np.sum(valid_indices)\n\n            new_fitness_valid = new_fitness[valid_indices]\n            \n            self.budget -= num_valid\n            \n            if num_valid > 0:\n                fitness[valid_indices] = new_fitness_valid\n\n\n            # Update personal bests\n            improved = fitness < personal_best_fitness\n            personal_best_positions[improved] = population[improved].copy()\n            personal_best_fitness[improved] = fitness[improved].copy()\n            \n            # Update global best\n            current_global_best_index = np.argmin(fitness)\n            if fitness[current_global_best_index] < global_best_fitness:\n                global_best_fitness = fitness[current_global_best_index]\n                global_best_position = population[current_global_best_index].copy()\n                stagnation_counter = 0  # Reset stagnation counter\n            else:\n                stagnation_counter += 1\n            \n            # Probabilistic restart if stagnant\n            if stagnation_counter > 50:  # Adjust threshold as needed\n                if np.random.rand() < self.restart_prob:\n                    population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                    velocities = np.random.uniform(-1, 1, size=(self.pop_size, self.dim)) * 0.1 \n                    new_fitness = np.array([func(x) if self.budget > 0 else np.inf for x in population])\n                    valid_indices = new_fitness != np.inf\n                    num_valid = np.sum(valid_indices)\n\n                    new_fitness_valid = new_fitness[valid_indices]\n                    \n                    self.budget -= num_valid\n                    if num_valid > 0:\n                        fitness[valid_indices] = new_fitness_valid\n\n                    personal_best_positions = population.copy()\n                    personal_best_fitness = fitness.copy()\n                    global_best_index = np.argmin(fitness)\n                    global_best_position = population[global_best_index].copy()\n                    global_best_fitness = fitness[global_best_index]\n                    stagnation_counter = 0\n\n        return global_best_fitness, global_best_position", "configspace": "", "generation": 4, "feedback": "Evaluation timed out after 300 seconds.", "error": "Evaluation timed out after 300 seconds.", "parent_ids": [], "operator": "random_new", "metadata": {"llm_response": "# Description: Population-based algorithm using velocity updates and a probabilistic restart mechanism based on stagnation.\n# Code: \n```python\nimport numpy as np\n\nclass VelocityRestartPSO:\n    def __init__(self, budget=10000, dim=10, pop_size=20, inertia=0.7, cognitive_coeff=1.5, social_coeff=1.5, restart_prob=0.05):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.inertia = inertia\n        self.cognitive_coeff = cognitive_coeff\n        self.social_coeff = social_coeff\n        self.restart_prob = restart_prob\n\n    def __call__(self, func):\n        # Initialization\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        velocities = np.random.uniform(-1, 1, size=(self.pop_size, self.dim)) * 0.1  # Initialize with small velocities\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n        \n        personal_best_positions = population.copy()\n        personal_best_fitness = fitness.copy()\n        \n        global_best_index = np.argmin(fitness)\n        global_best_position = population[global_best_index].copy()\n        global_best_fitness = fitness[global_best_index]\n        \n        stagnation_counter = 0\n        \n        while self.budget > 0:\n            # Update velocities and positions\n            r1 = np.random.rand(self.pop_size, self.dim)\n            r2 = np.random.rand(self.pop_size, self.dim)\n            \n            velocities = (self.inertia * velocities +\n                          self.cognitive_coeff * r1 * (personal_best_positions - population) +\n                          self.social_coeff * r2 * (global_best_position - population))\n            \n            population += velocities\n            \n            # Boundary handling (clip to bounds)\n            population = np.clip(population, func.bounds.lb, func.bounds.ub)\n            \n            # Evaluate new positions\n            new_fitness = np.array([func(x) if self.budget > 0 else np.inf for x in population])\n            valid_indices = new_fitness != np.inf\n            num_valid = np.sum(valid_indices)\n\n            new_fitness_valid = new_fitness[valid_indices]\n            \n            self.budget -= num_valid\n            \n            if num_valid > 0:\n                fitness[valid_indices] = new_fitness_valid\n\n\n            # Update personal bests\n            improved = fitness < personal_best_fitness\n            personal_best_positions[improved] = population[improved].copy()\n            personal_best_fitness[improved] = fitness[improved].copy()\n            \n            # Update global best\n            current_global_best_index = np.argmin(fitness)\n            if fitness[current_global_best_index] < global_best_fitness:\n                global_best_fitness = fitness[current_global_best_index]\n                global_best_position = population[current_global_best_index].copy()\n                stagnation_counter = 0  # Reset stagnation counter\n            else:\n                stagnation_counter += 1\n            \n            # Probabilistic restart if stagnant\n            if stagnation_counter > 50:  # Adjust threshold as needed\n                if np.random.rand() < self.restart_prob:\n                    population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                    velocities = np.random.uniform(-1, 1, size=(self.pop_size, self.dim)) * 0.1 \n                    new_fitness = np.array([func(x) if self.budget > 0 else np.inf for x in population])\n                    valid_indices = new_fitness != np.inf\n                    num_valid = np.sum(valid_indices)\n\n                    new_fitness_valid = new_fitness[valid_indices]\n                    \n                    self.budget -= num_valid\n                    if num_valid > 0:\n                        fitness[valid_indices] = new_fitness_valid\n\n                    personal_best_positions = population.copy()\n                    personal_best_fitness = fitness.copy()\n                    global_best_index = np.argmin(fitness)\n                    global_best_position = population[global_best_index].copy()\n                    global_best_fitness = fitness[global_best_index]\n                    stagnation_counter = 0\n\n        return global_best_fitness, global_best_position\n```", "operator": "random_new", "theta_sampled": -8.951118955073667e-05, "generation": 4}, "task_prompt": ""}
{"id": "eaba586f-83a5-41a9-aa96-c3670f82a23d", "fitness": 0.4173584408977904, "name": "AdaptiveDEBiasedMovingAverage", "description": "Combines moving average parameter adaptation with a bias towards past successful parameter values, promoting exploitation of promising search directions.", "code": "import numpy as np\n\nclass AdaptiveDEBiasedMovingAverage:\n    def __init__(self, budget=10000, dim=10, pop_size=None, F_init=0.5, Cr_init=0.9, memory_size=10, adapt_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size if pop_size is not None else 10 * dim\n        self.F = F_init\n        self.Cr = Cr_init\n        self.lb = -5.0\n        self.ub = 5.0\n        self.memory_size = memory_size\n        self.adapt_rate = adapt_rate\n        self.F_memory = np.full(self.memory_size, F_init)\n        self.Cr_memory = np.full(self.memory_size, Cr_init)\n        self.F_idx = 0\n        self.Cr_idx = 0\n        self.success_F = []\n        self.success_Cr = []\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.pop = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.nevals = self.pop_size\n\n        while self.nevals < self.budget:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = self.pop[idxs]\n\n                x_mutated = self.pop[i] + self.F * (x_r2 - x_r3)\n                x_mutated = np.clip(x_mutated, self.lb, self.ub)\n\n                # Crossover\n                x_trial = np.copy(self.pop[i])\n                crossover_points = np.random.rand(self.dim) < self.Cr\n                if not np.any(crossover_points):\n                    crossover_points[np.random.randint(0, self.dim)] = True\n                x_trial[crossover_points] = x_mutated[crossover_points]\n\n                # Evaluation\n                f_trial = func(x_trial)\n                self.nevals += 1\n\n                # Selection\n                if f_trial < self.fitness[i]:\n                    # Store successful F and Cr\n                    self.success_F.append(self.F)\n                    self.success_Cr.append(self.Cr)\n                    \n                    self.fitness[i] = f_trial\n                    self.pop[i] = x_trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = x_trial\n\n            # Adapt F and Cr (after each generation)\n            self.adapt_parameters()\n\n        return self.f_opt, self.x_opt\n\n    def adapt_parameters(self):\n        if self.success_F:\n            # Biased Moving Average for F\n            mean_success_F = np.mean(self.success_F)\n            self.F_memory[self.F_idx] = (1 - self.adapt_rate) * self.F_memory[self.F_idx] + self.adapt_rate * mean_success_F # Apply smoothing\n            self.F_idx = (self.F_idx + 1) % self.memory_size\n            self.F = np.mean(self.F_memory)\n            self.F = np.clip(self.F, 0.0, 1.0)\n            self.success_F = []\n\n        if self.success_Cr:\n            # Biased Moving Average for Cr\n            mean_success_Cr = np.mean(self.success_Cr)\n            self.Cr_memory[self.Cr_idx] = (1 - self.adapt_rate) * self.Cr_memory[self.Cr_idx] + self.adapt_rate * mean_success_Cr # Apply smoothing\n            self.Cr_idx = (self.Cr_idx + 1) % self.memory_size\n            self.Cr = np.mean(self.Cr_memory)\n            self.Cr = np.clip(self.Cr, 0.0, 1.0)\n            self.success_Cr = []", "configspace": "", "generation": 4, "feedback": "The algorithm AdaptiveDEBiasedMovingAverage scored 0.417 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["79cce7be-a121-4a1c-815e-61b8946185f0", "b3d902a5-e4f8-4f92-81fe-5f30bec513b6"], "operator": "crossover", "metadata": {"llm_response": "# Description: Combines moving average parameter adaptation with a bias towards past successful parameter values, promoting exploitation of promising search directions.\n# Code: \n```python\nimport numpy as np\n\nclass AdaptiveDEBiasedMovingAverage:\n    def __init__(self, budget=10000, dim=10, pop_size=None, F_init=0.5, Cr_init=0.9, memory_size=10, adapt_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size if pop_size is not None else 10 * dim\n        self.F = F_init\n        self.Cr = Cr_init\n        self.lb = -5.0\n        self.ub = 5.0\n        self.memory_size = memory_size\n        self.adapt_rate = adapt_rate\n        self.F_memory = np.full(self.memory_size, F_init)\n        self.Cr_memory = np.full(self.memory_size, Cr_init)\n        self.F_idx = 0\n        self.Cr_idx = 0\n        self.success_F = []\n        self.success_Cr = []\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.pop = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.nevals = self.pop_size\n\n        while self.nevals < self.budget:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = self.pop[idxs]\n\n                x_mutated = self.pop[i] + self.F * (x_r2 - x_r3)\n                x_mutated = np.clip(x_mutated, self.lb, self.ub)\n\n                # Crossover\n                x_trial = np.copy(self.pop[i])\n                crossover_points = np.random.rand(self.dim) < self.Cr\n                if not np.any(crossover_points):\n                    crossover_points[np.random.randint(0, self.dim)] = True\n                x_trial[crossover_points] = x_mutated[crossover_points]\n\n                # Evaluation\n                f_trial = func(x_trial)\n                self.nevals += 1\n\n                # Selection\n                if f_trial < self.fitness[i]:\n                    # Store successful F and Cr\n                    self.success_F.append(self.F)\n                    self.success_Cr.append(self.Cr)\n                    \n                    self.fitness[i] = f_trial\n                    self.pop[i] = x_trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = x_trial\n\n            # Adapt F and Cr (after each generation)\n            self.adapt_parameters()\n\n        return self.f_opt, self.x_opt\n\n    def adapt_parameters(self):\n        if self.success_F:\n            # Biased Moving Average for F\n            mean_success_F = np.mean(self.success_F)\n            self.F_memory[self.F_idx] = (1 - self.adapt_rate) * self.F_memory[self.F_idx] + self.adapt_rate * mean_success_F # Apply smoothing\n            self.F_idx = (self.F_idx + 1) % self.memory_size\n            self.F = np.mean(self.F_memory)\n            self.F = np.clip(self.F, 0.0, 1.0)\n            self.success_F = []\n\n        if self.success_Cr:\n            # Biased Moving Average for Cr\n            mean_success_Cr = np.mean(self.success_Cr)\n            self.Cr_memory[self.Cr_idx] = (1 - self.adapt_rate) * self.Cr_memory[self.Cr_idx] + self.adapt_rate * mean_success_Cr # Apply smoothing\n            self.Cr_idx = (self.Cr_idx + 1) % self.memory_size\n            self.Cr = np.mean(self.Cr_memory)\n            self.Cr = np.clip(self.Cr, 0.0, 1.0)\n            self.success_Cr = []\n```", "operator": "crossover", "theta_sampled": 0.06270960501102375, "generation": 4, "aucs": [0.145769122564972, 0.30854777524547694, 0.3864249294347474, 0.6280100217188642, 0.32280117302404876, 0.435598105334132, 0.2890459816619547, 0.35548046350068363, 0.305194951226051, 0.19494870715716928, 0.5729671611461375, 0.99813768786316, 0.39605526042487593, 0.2978626304875355, 0.7543112560146292, 0.41828929532156556, 0.3383056307705058, 0.5188317536667844, 0.19175698527430052, 0.48882992611821363]}, "task_prompt": ""}
{"id": "7f40927b-6bef-4042-ad1e-d0b44ac53ab2", "fitness": "-inf", "name": "AdaptiveDEArchive", "description": "Adaptive DE with moving average and separate smoothing for F and Cr adaptation, with an archive to maintain diversity.", "code": "import numpy as np\n\nclass AdaptiveDEArchive:\n    def __init__(self, budget=10000, dim=10, pop_size=None, F_init=0.5, Cr_init=0.9, memory_size=10, archive_size=5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size if pop_size is not None else 10 * dim\n        self.F = F_init\n        self.Cr = Cr_init\n        self.lb = -5.0\n        self.ub = 5.0\n        self.memory_size = memory_size\n        self.archive_size = archive_size\n        self.F_memory = np.full(self.memory_size, F_init)\n        self.Cr_memory = np.full(self.memory_size, Cr_init)\n        self.F_idx = 0\n        self.Cr_idx = 0\n        self.success_F = []\n        self.success_Cr = []\n        self.archive = []\n        self.archive_fitness = []\n        self.smoothing_rate_F = 0.1  # Smoothing rate for F\n        self.smoothing_rate_Cr = 0.1 # Smoothing rate for Cr\n\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.pop = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.nevals = self.pop_size\n\n        while self.nevals < self.budget:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = self.pop[idxs]\n\n                x_mutated = self.pop[i] + self.F * (x_r2 - x_r3)\n                x_mutated = np.clip(x_mutated, self.lb, self.ub)\n\n                # Crossover\n                x_trial = np.copy(self.pop[i])\n                crossover_points = np.random.rand(self.dim) < self.Cr\n                if not np.any(crossover_points):\n                    crossover_points[np.random.randint(0, self.dim)] = True\n                x_trial[crossover_points] = x_mutated[crossover_points]\n\n                # Evaluation\n                f_trial = func(x_trial)\n                self.nevals += 1\n\n                # Selection\n                if f_trial < self.fitness[i]:\n                    # Store successful F and Cr\n                    self.success_F.append(self.F)\n                    self.success_Cr.append(self.Cr)\n                    \n                    self.fitness[i] = f_trial\n                    self.pop[i] = x_trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = x_trial\n                else:\n                    # Add to archive if not better\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(x_trial)\n                        self.archive_fitness.append(f_trial)\n                    else:\n                        # Replace worst in archive\n                        max_archive_fitness_idx = np.argmax(self.archive_fitness)\n                        if f_trial < self.archive_fitness[max_archive_fitness_idx]:\n                            self.archive[max_archive_fitness_idx] = x_trial\n                            self.archive_fitness[max_archive_fitness_idx] = f_trial\n\n            # Adapt F and Cr (after each generation)\n            self.adapt_parameters()\n\n        return self.f_opt, self.x_opt\n\n    def adapt_parameters(self):\n        # Adapt F with smoothing\n        if self.success_F:\n            mean_F = np.mean(self.success_F)\n            self.F_memory[self.F_idx] = mean_F\n            self.F_idx = (self.F_idx + 1) % self.memory_size\n            self.F = (1 - self.smoothing_rate_F) * self.F + self.smoothing_rate_F * np.mean(self.F_memory)\n            self.F = np.clip(self.F, 0.0, 1.0)\n            self.success_F = []\n\n        # Adapt Cr with smoothing\n        if self.success_Cr:\n            mean_Cr = np.mean(self.success_Cr)\n            self.Cr_memory[self.Cr_idx] = mean_Cr\n            self.Cr_idx = (self.Cr_idx + 1) % self.memory_size\n            self.Cr = (1 - self.smoothing_rate_Cr) * self.Cr + self.smoothing_rate_Cr * np.mean(self.Cr_memory)\n            self.Cr = np.clip(self.Cr, 0.0, 1.0)\n            self.success_Cr = []", "configspace": "", "generation": 4, "feedback": "Evaluation timed out after 300 seconds.", "error": "Evaluation timed out after 300 seconds.", "parent_ids": ["79cce7be-a121-4a1c-815e-61b8946185f0", "993188b5-7f1b-4df5-b4d9-f6bb1ade6759"], "operator": "crossover", "metadata": {"llm_response": "# Description: Adaptive DE with moving average and separate smoothing for F and Cr adaptation, with an archive to maintain diversity.\n# Code: \n```python\nimport numpy as np\n\nclass AdaptiveDEArchive:\n    def __init__(self, budget=10000, dim=10, pop_size=None, F_init=0.5, Cr_init=0.9, memory_size=10, archive_size=5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size if pop_size is not None else 10 * dim\n        self.F = F_init\n        self.Cr = Cr_init\n        self.lb = -5.0\n        self.ub = 5.0\n        self.memory_size = memory_size\n        self.archive_size = archive_size\n        self.F_memory = np.full(self.memory_size, F_init)\n        self.Cr_memory = np.full(self.memory_size, Cr_init)\n        self.F_idx = 0\n        self.Cr_idx = 0\n        self.success_F = []\n        self.success_Cr = []\n        self.archive = []\n        self.archive_fitness = []\n        self.smoothing_rate_F = 0.1  # Smoothing rate for F\n        self.smoothing_rate_Cr = 0.1 # Smoothing rate for Cr\n\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.pop = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.nevals = self.pop_size\n\n        while self.nevals < self.budget:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = self.pop[idxs]\n\n                x_mutated = self.pop[i] + self.F * (x_r2 - x_r3)\n                x_mutated = np.clip(x_mutated, self.lb, self.ub)\n\n                # Crossover\n                x_trial = np.copy(self.pop[i])\n                crossover_points = np.random.rand(self.dim) < self.Cr\n                if not np.any(crossover_points):\n                    crossover_points[np.random.randint(0, self.dim)] = True\n                x_trial[crossover_points] = x_mutated[crossover_points]\n\n                # Evaluation\n                f_trial = func(x_trial)\n                self.nevals += 1\n\n                # Selection\n                if f_trial < self.fitness[i]:\n                    # Store successful F and Cr\n                    self.success_F.append(self.F)\n                    self.success_Cr.append(self.Cr)\n                    \n                    self.fitness[i] = f_trial\n                    self.pop[i] = x_trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = x_trial\n                else:\n                    # Add to archive if not better\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(x_trial)\n                        self.archive_fitness.append(f_trial)\n                    else:\n                        # Replace worst in archive\n                        max_archive_fitness_idx = np.argmax(self.archive_fitness)\n                        if f_trial < self.archive_fitness[max_archive_fitness_idx]:\n                            self.archive[max_archive_fitness_idx] = x_trial\n                            self.archive_fitness[max_archive_fitness_idx] = f_trial\n\n            # Adapt F and Cr (after each generation)\n            self.adapt_parameters()\n\n        return self.f_opt, self.x_opt\n\n    def adapt_parameters(self):\n        # Adapt F with smoothing\n        if self.success_F:\n            mean_F = np.mean(self.success_F)\n            self.F_memory[self.F_idx] = mean_F\n            self.F_idx = (self.F_idx + 1) % self.memory_size\n            self.F = (1 - self.smoothing_rate_F) * self.F + self.smoothing_rate_F * np.mean(self.F_memory)\n            self.F = np.clip(self.F, 0.0, 1.0)\n            self.success_F = []\n\n        # Adapt Cr with smoothing\n        if self.success_Cr:\n            mean_Cr = np.mean(self.success_Cr)\n            self.Cr_memory[self.Cr_idx] = mean_Cr\n            self.Cr_idx = (self.Cr_idx + 1) % self.memory_size\n            self.Cr = (1 - self.smoothing_rate_Cr) * self.Cr + self.smoothing_rate_Cr * np.mean(self.Cr_memory)\n            self.Cr = np.clip(self.Cr, 0.0, 1.0)\n            self.success_Cr = []\n```", "operator": "crossover", "theta_sampled": 0.03817096126394858, "generation": 4}, "task_prompt": ""}
{"id": "8d3a322b-b5bf-417d-934d-d79838f675a8", "fitness": 0.2605233387098984, "name": "AdaptiveDE", "description": "Adaptively adjusts DE parameters F and Cr based on the mean of successful values using a decaying average, promoting exploration in early stages.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=None, F_init=0.5, Cr_init=0.9, decay_rate=0.95):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size if pop_size is not None else 10 * dim\n        self.F = F_init\n        self.Cr = Cr_init\n        self.lb = -5.0\n        self.ub = 5.0\n        self.decay_rate = decay_rate\n        self.success_F_sum = 0.0\n        self.success_Cr_sum = 0.0\n        self.success_count = 0\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.pop = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.nevals = self.pop_size\n\n        while self.nevals < self.budget:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = self.pop[idxs]\n\n                x_mutated = self.pop[i] + self.F * (x_r2 - x_r3)\n                x_mutated = np.clip(x_mutated, self.lb, self.ub)\n\n                # Crossover\n                x_trial = np.copy(self.pop[i])\n                crossover_points = np.random.rand(self.dim) < self.Cr\n                if not np.any(crossover_points):\n                    crossover_points[np.random.randint(0, self.dim)] = True\n                x_trial[crossover_points] = x_mutated[crossover_points]\n\n                # Evaluation\n                f_trial = func(x_trial)\n                self.nevals += 1\n\n                # Selection\n                if f_trial < self.fitness[i]:\n                    self.success_F_sum = self.decay_rate * self.success_F_sum + (1 - self.decay_rate) * self.F\n                    self.success_Cr_sum = self.decay_rate * self.success_Cr_sum + (1 - self.decay_rate) * self.Cr\n                    self.success_count += 1\n                    \n                    self.fitness[i] = f_trial\n                    self.pop[i] = x_trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = x_trial\n\n            # Adapt F and Cr (after each generation)\n            self.adapt_parameters()\n\n        return self.f_opt, self.x_opt\n\n    def adapt_parameters(self):\n        if self.success_count > 0:\n            self.F = self.success_F_sum\n            self.Cr = self.success_Cr_sum\n            self.F = np.clip(self.F, 0.0, 1.0)\n            self.Cr = np.clip(self.Cr, 0.0, 1.0)\n            self.success_F_sum = 0.0\n            self.success_Cr_sum = 0.0\n            self.success_count = 0\n        else:\n            # If no improvement, increase exploration\n            self.F = np.clip(self.F * 1.1, 0.0, 1.0)\n            self.Cr = np.clip(self.Cr * 0.9, 0.0, 1.0)", "configspace": "", "generation": 4, "feedback": "The algorithm AdaptiveDE scored 0.261 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["79cce7be-a121-4a1c-815e-61b8946185f0"], "operator": "simplify", "metadata": {"llm_response": "# Description: Adaptively adjusts DE parameters F and Cr based on the mean of successful values using a decaying average, promoting exploration in early stages.\n# Code: \n```python\nimport numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=None, F_init=0.5, Cr_init=0.9, decay_rate=0.95):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size if pop_size is not None else 10 * dim\n        self.F = F_init\n        self.Cr = Cr_init\n        self.lb = -5.0\n        self.ub = 5.0\n        self.decay_rate = decay_rate\n        self.success_F_sum = 0.0\n        self.success_Cr_sum = 0.0\n        self.success_count = 0\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.pop = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.nevals = self.pop_size\n\n        while self.nevals < self.budget:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = self.pop[idxs]\n\n                x_mutated = self.pop[i] + self.F * (x_r2 - x_r3)\n                x_mutated = np.clip(x_mutated, self.lb, self.ub)\n\n                # Crossover\n                x_trial = np.copy(self.pop[i])\n                crossover_points = np.random.rand(self.dim) < self.Cr\n                if not np.any(crossover_points):\n                    crossover_points[np.random.randint(0, self.dim)] = True\n                x_trial[crossover_points] = x_mutated[crossover_points]\n\n                # Evaluation\n                f_trial = func(x_trial)\n                self.nevals += 1\n\n                # Selection\n                if f_trial < self.fitness[i]:\n                    self.success_F_sum = self.decay_rate * self.success_F_sum + (1 - self.decay_rate) * self.F\n                    self.success_Cr_sum = self.decay_rate * self.success_Cr_sum + (1 - self.decay_rate) * self.Cr\n                    self.success_count += 1\n                    \n                    self.fitness[i] = f_trial\n                    self.pop[i] = x_trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = x_trial\n\n            # Adapt F and Cr (after each generation)\n            self.adapt_parameters()\n\n        return self.f_opt, self.x_opt\n\n    def adapt_parameters(self):\n        if self.success_count > 0:\n            self.F = self.success_F_sum\n            self.Cr = self.success_Cr_sum\n            self.F = np.clip(self.F, 0.0, 1.0)\n            self.Cr = np.clip(self.Cr, 0.0, 1.0)\n            self.success_F_sum = 0.0\n            self.success_Cr_sum = 0.0\n            self.success_count = 0\n        else:\n            # If no improvement, increase exploration\n            self.F = np.clip(self.F * 1.1, 0.0, 1.0)\n            self.Cr = np.clip(self.Cr * 0.9, 0.0, 1.0)\n```", "operator": "simplify", "theta_sampled": 0.021460010211922814, "generation": 4, "aucs": [0.09869434720778247, 0.17139844125921266, 0.2267797232294222, 0.15684127006481796, 0.18981370838040001, 0.18971480379153716, 0.21261106063801505, 0.15834153861275613, 0.19068548366718174, 0.1455418138389779, 0.19482525258246297, 0.9974593987385337, 0.31427230292911124, 0.14574241289162204, 0.5449086733505646, 0.2576971766385986, 0.22624095722489268, 0.20455764489992667, 0.16206137447928737, 0.42227938977286517]}, "task_prompt": ""}
{"id": "1802944e-ab14-496e-a55d-4490a8b8fa87", "fitness": "-inf", "name": "AdaptiveDELearningRate", "description": "Adaptive DE with moving average for parameter adaptation and a separate learning rate for exploration vs. exploitation.", "code": "import numpy as np\n\nclass AdaptiveDELearningRate:\n    def __init__(self, budget=10000, dim=10, pop_size=None, F_init=0.5, Cr_init=0.9, memory_size=10, explore_rate=0.1, exploit_rate=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size if pop_size is not None else 10 * dim\n        self.F = F_init\n        self.Cr = Cr_init\n        self.lb = -5.0\n        self.ub = 5.0\n        self.memory_size = memory_size\n        self.F_memory = np.full(self.memory_size, F_init)\n        self.Cr_memory = np.full(self.memory_size, Cr_init)\n        self.F_idx = 0\n        self.Cr_idx = 0\n        self.success_F = []\n        self.success_Cr = []\n        self.explore_rate = explore_rate\n        self.exploit_rate = exploit_rate\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.pop = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.nevals = self.pop_size\n\n        while self.nevals < self.budget:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = self.pop[idxs]\n\n                # Use different learning rates for exploration and exploitation\n                if np.random.rand() < self.explore_rate:\n                  F = self.F * 2 # Increase exploration\n                else:\n                  F = self.F * self.exploit_rate # Dampen exploitation\n                  \n                x_mutated = self.pop[i] + F * (x_r2 - x_r3)\n                x_mutated = np.clip(x_mutated, self.lb, self.ub)\n\n                # Crossover\n                x_trial = np.copy(self.pop[i])\n                crossover_points = np.random.rand(self.dim) < self.Cr\n                if not np.any(crossover_points):\n                    crossover_points[np.random.randint(0, self.dim)] = True\n                x_trial[crossover_points] = x_mutated[crossover_points]\n\n                # Evaluation\n                f_trial = func(x_trial)\n                self.nevals += 1\n\n                # Selection\n                if f_trial < self.fitness[i]:\n                    # Store successful F and Cr\n                    self.success_F.append(self.F)\n                    self.success_Cr.append(self.Cr)\n                    \n                    self.fitness[i] = f_trial\n                    self.pop[i] = x_trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = x_trial\n\n            # Adapt F and Cr (after each generation)\n            self.adapt_parameters()\n\n        return self.f_opt, self.x_opt\n\n    def adapt_parameters(self):\n        if self.success_F:\n            # Update F using a moving average of successful values\n            self.F_memory[self.F_idx] = np.mean(self.success_F)\n            self.F_idx = (self.F_idx + 1) % self.memory_size\n            self.F = np.mean(self.F_memory)\n            self.F = np.clip(self.F, 0.0, 1.0)\n            self.success_F = []  # Reset successful F values for the next generation\n\n        if self.success_Cr:\n            # Update Cr using a moving average of successful values\n            self.Cr_memory[self.Cr_idx] = np.mean(self.success_Cr)\n            self.Cr_idx = (self.Cr_idx + 1) % self.memory_size\n            self.Cr = np.mean(self.Cr_memory)\n            self.Cr = np.clip(self.Cr, 0.0, 1.0)\n            self.success_Cr = []  # Reset successful Cr values for the next generation", "configspace": "", "generation": 4, "feedback": "Evaluation timed out after 300 seconds.", "error": "Evaluation timed out after 300 seconds.", "parent_ids": ["79cce7be-a121-4a1c-815e-61b8946185f0", "993188b5-7f1b-4df5-b4d9-f6bb1ade6759"], "operator": "crossover", "metadata": {"llm_response": "# Description: Adaptive DE with moving average for parameter adaptation and a separate learning rate for exploration vs. exploitation.\n# Code: \n```python\nimport numpy as np\n\nclass AdaptiveDELearningRate:\n    def __init__(self, budget=10000, dim=10, pop_size=None, F_init=0.5, Cr_init=0.9, memory_size=10, explore_rate=0.1, exploit_rate=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size if pop_size is not None else 10 * dim\n        self.F = F_init\n        self.Cr = Cr_init\n        self.lb = -5.0\n        self.ub = 5.0\n        self.memory_size = memory_size\n        self.F_memory = np.full(self.memory_size, F_init)\n        self.Cr_memory = np.full(self.memory_size, Cr_init)\n        self.F_idx = 0\n        self.Cr_idx = 0\n        self.success_F = []\n        self.success_Cr = []\n        self.explore_rate = explore_rate\n        self.exploit_rate = exploit_rate\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.pop = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.nevals = self.pop_size\n\n        while self.nevals < self.budget:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = self.pop[idxs]\n\n                # Use different learning rates for exploration and exploitation\n                if np.random.rand() < self.explore_rate:\n                  F = self.F * 2 # Increase exploration\n                else:\n                  F = self.F * self.exploit_rate # Dampen exploitation\n                  \n                x_mutated = self.pop[i] + F * (x_r2 - x_r3)\n                x_mutated = np.clip(x_mutated, self.lb, self.ub)\n\n                # Crossover\n                x_trial = np.copy(self.pop[i])\n                crossover_points = np.random.rand(self.dim) < self.Cr\n                if not np.any(crossover_points):\n                    crossover_points[np.random.randint(0, self.dim)] = True\n                x_trial[crossover_points] = x_mutated[crossover_points]\n\n                # Evaluation\n                f_trial = func(x_trial)\n                self.nevals += 1\n\n                # Selection\n                if f_trial < self.fitness[i]:\n                    # Store successful F and Cr\n                    self.success_F.append(self.F)\n                    self.success_Cr.append(self.Cr)\n                    \n                    self.fitness[i] = f_trial\n                    self.pop[i] = x_trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = x_trial\n\n            # Adapt F and Cr (after each generation)\n            self.adapt_parameters()\n\n        return self.f_opt, self.x_opt\n\n    def adapt_parameters(self):\n        if self.success_F:\n            # Update F using a moving average of successful values\n            self.F_memory[self.F_idx] = np.mean(self.success_F)\n            self.F_idx = (self.F_idx + 1) % self.memory_size\n            self.F = np.mean(self.F_memory)\n            self.F = np.clip(self.F, 0.0, 1.0)\n            self.success_F = []  # Reset successful F values for the next generation\n\n        if self.success_Cr:\n            # Update Cr using a moving average of successful values\n            self.Cr_memory[self.Cr_idx] = np.mean(self.success_Cr)\n            self.Cr_idx = (self.Cr_idx + 1) % self.memory_size\n            self.Cr = np.mean(self.Cr_memory)\n            self.Cr = np.clip(self.Cr, 0.0, 1.0)\n            self.success_Cr = []  # Reset successful Cr values for the next generation\n```", "operator": "crossover", "theta_sampled": 0.0015730706292482154, "generation": 4}, "task_prompt": ""}
{"id": "5b1df5a7-569f-4c9e-843e-61cb5a8a3f00", "fitness": "-inf", "name": "AdaptiveDEPopSize", "description": "An adaptive differential evolution algorithm that uses a dynamically adjusted population size and mutation strategy based on the progress of the optimization.", "code": "import numpy as np\n\nclass AdaptiveDEPopSize:\n    def __init__(self, budget=10000, dim=10, pop_factor=5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_factor = pop_factor  # Factor to determine initial population size\n        self.pop_size = int(self.pop_factor * self.dim)\n        self.cr = 0.7  # Initial crossover rate\n        self.f = 0.8  # Initial mutation factor\n        self.archive_factor = 2.0 # Factor for archive size relative to population size\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        \n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        # Initialize archive\n        archive = []\n\n        # Optimization loop\n        while self.budget > 0:\n            # Sort population by fitness\n            idx = np.argsort(fitness)\n            population = population[idx]\n            fitness = fitness[idx]\n\n            if fitness[0] < self.f_opt:\n                self.f_opt = fitness[0]\n                self.x_opt = population[0]\n            \n            # Adjust population size based on stagnation (example: if improvement is slow)\n            improvement = np.abs(self.f_opt - np.min(fitness))\n            \n            if improvement < 1e-6 and self.pop_size > self.dim + 1:  #Stagnation detected, decrease pop size\n                self.pop_size = max(self.dim+1, int(self.pop_size * 0.9))\n                population = population[:self.pop_size]\n                fitness = fitness[:self.pop_size]\n            elif improvement > 1e-2 and self.pop_size < self.pop_factor * self.dim *2: # Pop size is small, try increasing\n                self.pop_size = min(int(self.pop_size * 1.1), int(self.pop_factor* self.dim *2))\n                new_population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size - len(population), self.dim))\n                new_fitness = np.array([func(x) for x in new_population])\n                population = np.concatenate([population, new_population])\n                fitness = np.concatenate([fitness, new_fitness])\n                self.budget -= (self.pop_size - len(population) + len(new_population))\n\n            \n            # Differential Evolution operations\n            for i in range(len(population)):\n                # Mutation\n                idxs = np.random.choice(len(population), 3, replace=False)\n                x_r1, x_r2, x_r3 = population[idxs]\n                x_mutated = x_r1 + self.f * (x_r2 - x_r3)\n                x_mutated = np.clip(x_mutated, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                x_trial = population[i].copy()\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < self.cr or j == j_rand:\n                        x_trial[j] = x_mutated[j]\n\n                # Selection\n                f_trial = func(x_trial)\n                self.budget -= 1\n                \n                if f_trial < fitness[i]:\n                    \n                    fitness[i] = f_trial\n                    population[i] = x_trial\n                    archive.append(population[i].copy())\n                    if len(archive) > int(self.pop_size * self.archive_factor):\n                        archive.pop(0)\n\n            # Adjust F and CR (example: adapt based on success rate)\n            success_indices = fitness < np.mean(fitness)\n            if np.any(success_indices):\n                successful_fs = self.f * np.ones(len(success_indices))[success_indices]\n                successful_crs = self.cr * np.ones(len(success_indices))[success_indices]\n\n                self.f = np.mean(successful_fs) if len(successful_fs) > 0 else self.f\n                self.cr = np.mean(successful_crs) if len(successful_crs) > 0 else self.cr\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "Evaluation timed out after 300 seconds.", "error": "Evaluation timed out after 300 seconds.", "parent_ids": [], "operator": "random_new", "metadata": {"llm_response": "# Description: An adaptive differential evolution algorithm that uses a dynamically adjusted population size and mutation strategy based on the progress of the optimization.\n# Code: \n```python\nimport numpy as np\n\nclass AdaptiveDEPopSize:\n    def __init__(self, budget=10000, dim=10, pop_factor=5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_factor = pop_factor  # Factor to determine initial population size\n        self.pop_size = int(self.pop_factor * self.dim)\n        self.cr = 0.7  # Initial crossover rate\n        self.f = 0.8  # Initial mutation factor\n        self.archive_factor = 2.0 # Factor for archive size relative to population size\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        \n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        # Initialize archive\n        archive = []\n\n        # Optimization loop\n        while self.budget > 0:\n            # Sort population by fitness\n            idx = np.argsort(fitness)\n            population = population[idx]\n            fitness = fitness[idx]\n\n            if fitness[0] < self.f_opt:\n                self.f_opt = fitness[0]\n                self.x_opt = population[0]\n            \n            # Adjust population size based on stagnation (example: if improvement is slow)\n            improvement = np.abs(self.f_opt - np.min(fitness))\n            \n            if improvement < 1e-6 and self.pop_size > self.dim + 1:  #Stagnation detected, decrease pop size\n                self.pop_size = max(self.dim+1, int(self.pop_size * 0.9))\n                population = population[:self.pop_size]\n                fitness = fitness[:self.pop_size]\n            elif improvement > 1e-2 and self.pop_size < self.pop_factor * self.dim *2: # Pop size is small, try increasing\n                self.pop_size = min(int(self.pop_size * 1.1), int(self.pop_factor* self.dim *2))\n                new_population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size - len(population), self.dim))\n                new_fitness = np.array([func(x) for x in new_population])\n                population = np.concatenate([population, new_population])\n                fitness = np.concatenate([fitness, new_fitness])\n                self.budget -= (self.pop_size - len(population) + len(new_population))\n\n            \n            # Differential Evolution operations\n            for i in range(len(population)):\n                # Mutation\n                idxs = np.random.choice(len(population), 3, replace=False)\n                x_r1, x_r2, x_r3 = population[idxs]\n                x_mutated = x_r1 + self.f * (x_r2 - x_r3)\n                x_mutated = np.clip(x_mutated, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                x_trial = population[i].copy()\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < self.cr or j == j_rand:\n                        x_trial[j] = x_mutated[j]\n\n                # Selection\n                f_trial = func(x_trial)\n                self.budget -= 1\n                \n                if f_trial < fitness[i]:\n                    \n                    fitness[i] = f_trial\n                    population[i] = x_trial\n                    archive.append(population[i].copy())\n                    if len(archive) > int(self.pop_size * self.archive_factor):\n                        archive.pop(0)\n\n            # Adjust F and CR (example: adapt based on success rate)\n            success_indices = fitness < np.mean(fitness)\n            if np.any(success_indices):\n                successful_fs = self.f * np.ones(len(success_indices))[success_indices]\n                successful_crs = self.cr * np.ones(len(success_indices))[success_indices]\n\n                self.f = np.mean(successful_fs) if len(successful_fs) > 0 else self.f\n                self.cr = np.mean(successful_crs) if len(successful_crs) > 0 else self.cr\n\n        return self.f_opt, self.x_opt\n```", "operator": "random_new", "theta_sampled": 0.02517794856756408, "generation": 5}, "task_prompt": ""}
{"id": "f101f71d-d098-4450-8ac4-ac8371b50270", "fitness": "-inf", "name": "DynamicSubpopulationOptimizer", "description": "Population-based algorithm with dynamic subpopulation allocation based on performance and adaptive exploration-exploitation balance.", "code": "import numpy as np\n\nclass DynamicSubpopulationOptimizer:\n    def __init__(self, budget=10000, dim=10, num_subpopulations=4, initial_subpopulation_size=None):\n        self.budget = budget\n        self.dim = dim\n        self.num_subpopulations = num_subpopulations\n        self.lb = -5.0\n        self.ub = 5.0\n        if initial_subpopulation_size is None:\n            self.subpopulation_sizes = [budget // num_subpopulations] * num_subpopulations\n        else:\n            self.subpopulation_sizes = initial_subpopulation_size\n\n        self.populations = [np.random.uniform(self.lb, self.ub, size=(self.subpopulation_sizes[i], self.dim)) for i in range(self.num_subpopulations)]\n        self.fitness_values = [np.full(self.subpopulation_sizes[i], np.inf) for i in range(self.num_subpopulations)]\n        self.mutation_rates = [0.5] * self.num_subpopulations\n        self.crossover_rates = [0.7] * self.num_subpopulations\n        self.learning_rates = [0.1] * self.num_subpopulations\n        self.best_fitness = np.inf\n        self.best_solution = None\n        self.eval_count = 0\n\n    def __call__(self, func):\n        \n        while self.eval_count < self.budget:\n            for i in range(self.num_subpopulations):\n                if self.eval_count >= self.budget:\n                  break\n\n                for j in range(self.subpopulation_sizes[i]):\n                    if self.eval_count >= self.budget:\n                        break\n                    \n                    # Mutation and Crossover (DE-like)\n                    idxs = np.random.choice(self.subpopulation_sizes[i], 3, replace=False)\n                    mutant = self.populations[i][idxs[0]] + self.mutation_rates[i] * (self.populations[i][idxs[1]] - self.populations[i][idxs[2]])\n                    mutant = np.clip(mutant, self.lb, self.ub)\n                    \n                    trial = np.copy(self.populations[i][j])\n                    for k in range(self.dim):\n                        if np.random.rand() < self.crossover_rates[i] or k == np.random.randint(self.dim):\n                            trial[k] = mutant[k]\n\n                    f = func(trial)\n                    self.eval_count += 1\n\n                    if f < self.fitness_values[i][j]:\n                        self.fitness_values[i][j] = f\n                        self.populations[i][j] = trial\n\n                        if f < self.best_fitness:\n                            self.best_fitness = f\n                            self.best_solution = trial\n                            \n\n            # Dynamic Subpopulation Allocation (simplified): relocate individuals from worst to best performing subpopulation\n            subpopulation_performance = [np.mean(self.fitness_values[i]) for i in range(self.num_subpopulations)]\n            best_subpopulation = np.argmin(subpopulation_performance)\n            worst_subpopulation = np.argmax(subpopulation_performance)\n\n            transfer_size = max(1, self.subpopulation_sizes[worst_subpopulation] // 10)\n\n            # Move 'transfer_size' individuals from worst to best subpopulation\n\n            worst_indices = np.argsort(self.fitness_values[worst_subpopulation])[::-1][:transfer_size] #worst indices\n            best_indices = np.argsort(self.fitness_values[best_subpopulation])[:transfer_size]\n\n            self.populations[best_subpopulation][best_indices] = np.copy(self.populations[worst_subpopulation][worst_indices])\n            self.fitness_values[best_subpopulation][best_indices] = np.copy(self.fitness_values[worst_subpopulation][worst_indices])\n            # Adaptation of mutation rates based on subpopulation performance\n            for i in range(self.num_subpopulations):\n                if subpopulation_performance[i] < np.mean(subpopulation_performance):\n                    self.mutation_rates[i] *= (1 + self.learning_rates[i])\n                else:\n                    self.mutation_rates[i] *= (1 - self.learning_rates[i])\n                self.mutation_rates[i] = np.clip(self.mutation_rates[i], 0.1, 1.0)  # Keep mutation rates within bounds\n        \n        return self.best_fitness, self.best_solution", "configspace": "", "generation": 5, "feedback": "Evaluation timed out after 300 seconds.", "error": "Evaluation timed out after 300 seconds.", "parent_ids": [], "operator": "random_new", "metadata": {"llm_response": "# Description: Population-based algorithm with dynamic subpopulation allocation based on performance and adaptive exploration-exploitation balance.\n# Code: \n```python\nimport numpy as np\n\nclass DynamicSubpopulationOptimizer:\n    def __init__(self, budget=10000, dim=10, num_subpopulations=4, initial_subpopulation_size=None):\n        self.budget = budget\n        self.dim = dim\n        self.num_subpopulations = num_subpopulations\n        self.lb = -5.0\n        self.ub = 5.0\n        if initial_subpopulation_size is None:\n            self.subpopulation_sizes = [budget // num_subpopulations] * num_subpopulations\n        else:\n            self.subpopulation_sizes = initial_subpopulation_size\n\n        self.populations = [np.random.uniform(self.lb, self.ub, size=(self.subpopulation_sizes[i], self.dim)) for i in range(self.num_subpopulations)]\n        self.fitness_values = [np.full(self.subpopulation_sizes[i], np.inf) for i in range(self.num_subpopulations)]\n        self.mutation_rates = [0.5] * self.num_subpopulations\n        self.crossover_rates = [0.7] * self.num_subpopulations\n        self.learning_rates = [0.1] * self.num_subpopulations\n        self.best_fitness = np.inf\n        self.best_solution = None\n        self.eval_count = 0\n\n    def __call__(self, func):\n        \n        while self.eval_count < self.budget:\n            for i in range(self.num_subpopulations):\n                if self.eval_count >= self.budget:\n                  break\n\n                for j in range(self.subpopulation_sizes[i]):\n                    if self.eval_count >= self.budget:\n                        break\n                    \n                    # Mutation and Crossover (DE-like)\n                    idxs = np.random.choice(self.subpopulation_sizes[i], 3, replace=False)\n                    mutant = self.populations[i][idxs[0]] + self.mutation_rates[i] * (self.populations[i][idxs[1]] - self.populations[i][idxs[2]])\n                    mutant = np.clip(mutant, self.lb, self.ub)\n                    \n                    trial = np.copy(self.populations[i][j])\n                    for k in range(self.dim):\n                        if np.random.rand() < self.crossover_rates[i] or k == np.random.randint(self.dim):\n                            trial[k] = mutant[k]\n\n                    f = func(trial)\n                    self.eval_count += 1\n\n                    if f < self.fitness_values[i][j]:\n                        self.fitness_values[i][j] = f\n                        self.populations[i][j] = trial\n\n                        if f < self.best_fitness:\n                            self.best_fitness = f\n                            self.best_solution = trial\n                            \n\n            # Dynamic Subpopulation Allocation (simplified): relocate individuals from worst to best performing subpopulation\n            subpopulation_performance = [np.mean(self.fitness_values[i]) for i in range(self.num_subpopulations)]\n            best_subpopulation = np.argmin(subpopulation_performance)\n            worst_subpopulation = np.argmax(subpopulation_performance)\n\n            transfer_size = max(1, self.subpopulation_sizes[worst_subpopulation] // 10)\n\n            # Move 'transfer_size' individuals from worst to best subpopulation\n\n            worst_indices = np.argsort(self.fitness_values[worst_subpopulation])[::-1][:transfer_size] #worst indices\n            best_indices = np.argsort(self.fitness_values[best_subpopulation])[:transfer_size]\n\n            self.populations[best_subpopulation][best_indices] = np.copy(self.populations[worst_subpopulation][worst_indices])\n            self.fitness_values[best_subpopulation][best_indices] = np.copy(self.fitness_values[worst_subpopulation][worst_indices])\n            # Adaptation of mutation rates based on subpopulation performance\n            for i in range(self.num_subpopulations):\n                if subpopulation_performance[i] < np.mean(subpopulation_performance):\n                    self.mutation_rates[i] *= (1 + self.learning_rates[i])\n                else:\n                    self.mutation_rates[i] *= (1 - self.learning_rates[i])\n                self.mutation_rates[i] = np.clip(self.mutation_rates[i], 0.1, 1.0)  # Keep mutation rates within bounds\n        \n        return self.best_fitness, self.best_solution\n```", "operator": "random_new", "theta_sampled": 0.0019100707391434745, "generation": 5}, "task_prompt": ""}
{"id": "861b7b24-5a18-48bb-9496-0e17601db506", "fitness": "-inf", "name": "AdaptiveDESmoothedWindow", "description": "Adaptive Differential Evolution with windowed parameter adaptation and moving average smoothing for exploration.", "code": "import numpy as np\n\nclass AdaptiveDESmoothedWindow:\n    def __init__(self, budget=10000, dim=10, pop_size=40, F_init=0.5, Cr_init=0.9, window_size=50, memory_size=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F_init  # Initial scaling factor\n        self.Cr = Cr_init  # Initial crossover rate\n        self.window_size = window_size\n        self.memory_size = memory_size\n        self.F_history = []\n        self.Cr_history = []\n        self.success_F = []\n        self.success_Cr = []\n        self.F_memory = np.full(self.memory_size, F_init)\n        self.Cr_memory = np.full(self.memory_size, Cr_init)\n        self.F_idx = 0\n        self.Cr_idx = 0\n        self.archive_rate = 0.1\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in self.population])\n        self.n_evals = self.pop_size\n        \n        while self.n_evals < self.budget:\n            for i in range(self.pop_size):\n                # Parameter adaptation based on history (windowed for exploration)\n                if len(self.F_history) > self.window_size:\n                    recent_F = self.F_history[-self.window_size:]\n                    self.F = np.mean(recent_F) if len(recent_F) > 0 else 0.5\n                    self.F = np.clip(self.F, 0.1, 1.0)\n\n                    recent_Cr = self.Cr_history[-self.window_size:]\n                    self.Cr = np.mean(recent_Cr) if len(recent_Cr) > 0 else 0.9\n                    self.Cr = np.clip(self.Cr, 0.1, 1.0)\n\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_1, x_2, x_3 = self.population[idxs]\n                v = self.population[i] + self.F * (x_2 - x_3)\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n                \n                # Crossover\n                j_rand = np.random.randint(self.dim)\n                u = np.array([v[j] if (np.random.rand() < self.Cr or j == j_rand) else self.population[i, j] for j in range(self.dim)])\n\n                # Selection\n                f_u = func(u)\n                self.n_evals += 1\n                \n                if f_u < fitness[i]:\n                    # Store successful F and Cr for moving average smoothing\n                    self.success_F.append(self.F)\n                    self.success_Cr.append(self.Cr)\n                    \n                    self.population[i] = u\n                    fitness[i] = f_u\n                    self.F_history.append(self.F)\n                    self.Cr_history.append(self.Cr)\n                \n                    if f_u < self.f_opt:\n                        self.f_opt = f_u\n                        self.x_opt = u\n                else:\n                    self.F_history.append(self.F)\n                    self.Cr_history.append(self.Cr)\n\n                if self.n_evals >= self.budget:\n                    break\n\n            # Adapt F and Cr using moving average (after each generation)\n            if self.success_F:\n                self.F_memory[self.F_idx] = np.mean(self.success_F)\n                self.F_idx = (self.F_idx + 1) % self.memory_size\n                self.F = np.mean(self.F_memory)\n                self.F = np.clip(self.F, 0.0, 1.0)\n                self.success_F = []\n\n            if self.success_Cr:\n                self.Cr_memory[self.Cr_idx] = np.mean(self.success_Cr)\n                self.Cr_idx = (self.Cr_idx + 1) % self.memory_size\n                self.Cr = np.mean(self.Cr_memory)\n                self.Cr = np.clip(self.Cr, 0.0, 1.0)\n                self.success_Cr = []\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "Evaluation timed out after 300 seconds.", "error": "Evaluation timed out after 300 seconds.", "parent_ids": ["40bd89f0-32fe-4e6f-8887-5e032130b8e1", "79cce7be-a121-4a1c-815e-61b8946185f0"], "operator": "crossover", "metadata": {"llm_response": "# Description: Adaptive Differential Evolution with windowed parameter adaptation and moving average smoothing for exploration.\n# Code: \n```python\nimport numpy as np\n\nclass AdaptiveDESmoothedWindow:\n    def __init__(self, budget=10000, dim=10, pop_size=40, F_init=0.5, Cr_init=0.9, window_size=50, memory_size=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F_init  # Initial scaling factor\n        self.Cr = Cr_init  # Initial crossover rate\n        self.window_size = window_size\n        self.memory_size = memory_size\n        self.F_history = []\n        self.Cr_history = []\n        self.success_F = []\n        self.success_Cr = []\n        self.F_memory = np.full(self.memory_size, F_init)\n        self.Cr_memory = np.full(self.memory_size, Cr_init)\n        self.F_idx = 0\n        self.Cr_idx = 0\n        self.archive_rate = 0.1\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in self.population])\n        self.n_evals = self.pop_size\n        \n        while self.n_evals < self.budget:\n            for i in range(self.pop_size):\n                # Parameter adaptation based on history (windowed for exploration)\n                if len(self.F_history) > self.window_size:\n                    recent_F = self.F_history[-self.window_size:]\n                    self.F = np.mean(recent_F) if len(recent_F) > 0 else 0.5\n                    self.F = np.clip(self.F, 0.1, 1.0)\n\n                    recent_Cr = self.Cr_history[-self.window_size:]\n                    self.Cr = np.mean(recent_Cr) if len(recent_Cr) > 0 else 0.9\n                    self.Cr = np.clip(self.Cr, 0.1, 1.0)\n\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_1, x_2, x_3 = self.population[idxs]\n                v = self.population[i] + self.F * (x_2 - x_3)\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n                \n                # Crossover\n                j_rand = np.random.randint(self.dim)\n                u = np.array([v[j] if (np.random.rand() < self.Cr or j == j_rand) else self.population[i, j] for j in range(self.dim)])\n\n                # Selection\n                f_u = func(u)\n                self.n_evals += 1\n                \n                if f_u < fitness[i]:\n                    # Store successful F and Cr for moving average smoothing\n                    self.success_F.append(self.F)\n                    self.success_Cr.append(self.Cr)\n                    \n                    self.population[i] = u\n                    fitness[i] = f_u\n                    self.F_history.append(self.F)\n                    self.Cr_history.append(self.Cr)\n                \n                    if f_u < self.f_opt:\n                        self.f_opt = f_u\n                        self.x_opt = u\n                else:\n                    self.F_history.append(self.F)\n                    self.Cr_history.append(self.Cr)\n\n                if self.n_evals >= self.budget:\n                    break\n\n            # Adapt F and Cr using moving average (after each generation)\n            if self.success_F:\n                self.F_memory[self.F_idx] = np.mean(self.success_F)\n                self.F_idx = (self.F_idx + 1) % self.memory_size\n                self.F = np.mean(self.F_memory)\n                self.F = np.clip(self.F, 0.0, 1.0)\n                self.success_F = []\n\n            if self.success_Cr:\n                self.Cr_memory[self.Cr_idx] = np.mean(self.success_Cr)\n                self.Cr_idx = (self.Cr_idx + 1) % self.memory_size\n                self.Cr = np.mean(self.Cr_memory)\n                self.Cr = np.clip(self.Cr, 0.0, 1.0)\n                self.success_Cr = []\n\n        return self.f_opt, self.x_opt\n```", "operator": "crossover", "theta_sampled": 0.06041631861692587, "generation": 5}, "task_prompt": ""}
{"id": "2fa45af8-e020-4e0b-9f07-43390b8ca505", "fitness": 0.46083392623879316, "name": "AdaptiveDEWindowMemory", "description": "Adaptive Differential Evolution with window-based parameter adaptation and a memory of successful parameters.", "code": "import numpy as np\n\nclass AdaptiveDEWindowMemory:\n    def __init__(self, budget=10000, dim=10, pop_size=40, F_init=0.5, Cr_init=0.9, window_size=50, memory_size=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F_init\n        self.Cr = Cr_init\n        self.window_size = window_size\n        self.memory_size = memory_size\n        self.F_memory = np.full(self.memory_size, F_init)\n        self.Cr_memory = np.full(self.memory_size, Cr_init)\n        self.F_idx = 0\n        self.Cr_idx = 0\n        self.success_F = []\n        self.success_Cr = []\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in self.population])\n        self.n_evals = self.pop_size\n\n        while self.n_evals < self.budget:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_1, x_2, x_3 = self.population[idxs]\n                v = self.population[i] + self.F * (x_2 - x_3)\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                j_rand = np.random.randint(self.dim)\n                u = np.array([v[j] if (np.random.rand() < self.Cr or j == j_rand) else self.population[i, j] for j in range(self.dim)])\n\n                # Selection\n                f_u = func(u)\n                self.n_evals += 1\n\n                if f_u < fitness[i]:\n                    # Store successful F and Cr\n                    self.success_F.append(self.F)\n                    self.success_Cr.append(self.Cr)\n\n                    self.population[i] = u\n                    fitness[i] = f_u\n\n                    if f_u < self.f_opt:\n                        self.f_opt = f_u\n                        self.x_opt = u\n\n                if self.n_evals >= self.budget:\n                    break\n\n            # Adapt F and Cr (after each generation)\n            self.adapt_parameters()\n\n        return self.f_opt, self.x_opt\n\n    def adapt_parameters(self):\n        # Window-based averaging for F and Cr\n        if self.success_F:\n            self.F_memory[self.F_idx] = np.mean(self.success_F)\n            self.F_idx = (self.F_idx + 1) % self.memory_size\n            self.F = np.mean(self.F_memory)\n            self.F = np.clip(self.F, 0.1, 1.0)\n            self.success_F = []\n\n        if self.success_Cr:\n            self.Cr_memory[self.Cr_idx] = np.mean(self.success_Cr)\n            self.Cr_idx = (self.Cr_idx + 1) % self.memory_size\n            self.Cr = np.mean(self.Cr_memory)\n            self.Cr = np.clip(self.Cr, 0.1, 1.0)\n            self.success_Cr = []", "configspace": "", "generation": 5, "feedback": "The algorithm AdaptiveDEWindowMemory scored 0.461 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["40bd89f0-32fe-4e6f-8887-5e032130b8e1", "79cce7be-a121-4a1c-815e-61b8946185f0"], "operator": "crossover", "metadata": {"llm_response": "# Description: Adaptive Differential Evolution with window-based parameter adaptation and a memory of successful parameters.\n# Code: \n```python\nimport numpy as np\n\nclass AdaptiveDEWindowMemory:\n    def __init__(self, budget=10000, dim=10, pop_size=40, F_init=0.5, Cr_init=0.9, window_size=50, memory_size=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F_init\n        self.Cr = Cr_init\n        self.window_size = window_size\n        self.memory_size = memory_size\n        self.F_memory = np.full(self.memory_size, F_init)\n        self.Cr_memory = np.full(self.memory_size, Cr_init)\n        self.F_idx = 0\n        self.Cr_idx = 0\n        self.success_F = []\n        self.success_Cr = []\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in self.population])\n        self.n_evals = self.pop_size\n\n        while self.n_evals < self.budget:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_1, x_2, x_3 = self.population[idxs]\n                v = self.population[i] + self.F * (x_2 - x_3)\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                j_rand = np.random.randint(self.dim)\n                u = np.array([v[j] if (np.random.rand() < self.Cr or j == j_rand) else self.population[i, j] for j in range(self.dim)])\n\n                # Selection\n                f_u = func(u)\n                self.n_evals += 1\n\n                if f_u < fitness[i]:\n                    # Store successful F and Cr\n                    self.success_F.append(self.F)\n                    self.success_Cr.append(self.Cr)\n\n                    self.population[i] = u\n                    fitness[i] = f_u\n\n                    if f_u < self.f_opt:\n                        self.f_opt = f_u\n                        self.x_opt = u\n\n                if self.n_evals >= self.budget:\n                    break\n\n            # Adapt F and Cr (after each generation)\n            self.adapt_parameters()\n\n        return self.f_opt, self.x_opt\n\n    def adapt_parameters(self):\n        # Window-based averaging for F and Cr\n        if self.success_F:\n            self.F_memory[self.F_idx] = np.mean(self.success_F)\n            self.F_idx = (self.F_idx + 1) % self.memory_size\n            self.F = np.mean(self.F_memory)\n            self.F = np.clip(self.F, 0.1, 1.0)\n            self.success_F = []\n\n        if self.success_Cr:\n            self.Cr_memory[self.Cr_idx] = np.mean(self.success_Cr)\n            self.Cr_idx = (self.Cr_idx + 1) % self.memory_size\n            self.Cr = np.mean(self.Cr_memory)\n            self.Cr = np.clip(self.Cr, 0.1, 1.0)\n            self.success_Cr = []\n```", "operator": "crossover", "theta_sampled": 0.007782355199961526, "generation": 5, "aucs": [0.15629637442798516, 0.37316473385427196, 0.4214099251653832, 0.6855116030428258, 0.3724077767237105, 0.5239602813250052, 0.3021928447958897, 0.39358975129512885, 0.3817263533392288, 0.1945396532980065, 0.7307384538876851, 0.9997333668926104, 0.4192476503719639, 0.3160594954073065, 0.8166578842560042, 0.4920810894816675, 0.3544525094503064, 0.6107119516013528, 0.1839034888785429, 0.4882933372809881]}, "task_prompt": ""}
{"id": "a5611c32-bed4-4ec0-9aa1-bf509a3dc71f", "fitness": 0.45069569014229965, "name": "AdaptiveDESmoothedArchive", "description": "Adaptive Differential Evolution with smoothed parameter adaptation and archive.", "code": "import numpy as np\n\nclass AdaptiveDESmoothedArchive:\n    def __init__(self, budget=10000, dim=10, pop_size=40, F_init=0.5, Cr_init=0.9, adapt_rate=0.1, archive_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F_init\n        self.Cr = Cr_init\n        self.lb = -5.0\n        self.ub = 5.0\n        self.adapt_rate = adapt_rate\n        self.success_F = []\n        self.success_Cr = []\n        self.archive_rate = archive_rate\n        self.archive = []\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.pop = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.nevals = self.pop_size\n\n        while self.nevals < self.budget:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = self.pop[idxs]\n                \n                # Incorporate archive\n                if len(self.archive) > 0 and np.random.rand() < self.archive_rate:\n                    x_arc = self.archive[np.random.randint(0, len(self.archive))]\n                    x_mutated = self.pop[i] + self.F * (x_arc - x_r3)\n                else:\n                    x_mutated = self.pop[i] + self.F * (x_r2 - x_r3)\n                \n                x_mutated = np.clip(x_mutated, self.lb, self.ub)\n\n                # Crossover\n                x_trial = np.copy(self.pop[i])\n                crossover_points = np.random.rand(self.dim) < self.Cr\n                if not np.any(crossover_points):\n                    crossover_points[np.random.randint(0, self.dim)] = True\n                x_trial[crossover_points] = x_mutated[crossover_points]\n\n                # Evaluation\n                f_trial = func(x_trial)\n                self.nevals += 1\n\n                # Selection\n                if f_trial < self.fitness[i]:\n                    # Store successful F and Cr\n                    self.success_F.append(self.F)\n                    self.success_Cr.append(self.Cr)\n                    \n                    # Add replaced individual to archive\n                    if len(self.archive) < self.pop_size:\n                        self.archive.append(self.pop[i].copy())\n                    else:\n                        self.archive[np.random.randint(0, self.pop_size)] = self.pop[i].copy()\n                    \n                    self.fitness[i] = f_trial\n                    self.pop[i] = x_trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = x_trial\n\n            # Adapt F and Cr (after each generation)\n            self.adapt_parameters()\n\n        return self.f_opt, self.x_opt\n\n    def adapt_parameters(self):\n        if self.success_F:\n            self.F = (1 - self.adapt_rate) * self.F + self.adapt_rate * np.mean(self.success_F)\n            self.F = np.clip(self.F, 0.0, 1.0)\n            self.success_F = []\n\n        if self.success_Cr:\n            self.Cr = (1 - self.adapt_rate) * self.Cr + self.adapt_rate * np.mean(self.success_Cr)\n            self.Cr = np.clip(self.Cr, 0.0, 1.0)\n            self.success_Cr = []", "configspace": "", "generation": 5, "feedback": "The algorithm AdaptiveDESmoothedArchive scored 0.451 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["40bd89f0-32fe-4e6f-8887-5e032130b8e1", "b3d902a5-e4f8-4f92-81fe-5f30bec513b6"], "operator": "crossover", "metadata": {"llm_response": "# Description: Adaptive Differential Evolution with smoothed parameter adaptation and archive.\n# Code: \n```python\nimport numpy as np\n\nclass AdaptiveDESmoothedArchive:\n    def __init__(self, budget=10000, dim=10, pop_size=40, F_init=0.5, Cr_init=0.9, adapt_rate=0.1, archive_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F_init\n        self.Cr = Cr_init\n        self.lb = -5.0\n        self.ub = 5.0\n        self.adapt_rate = adapt_rate\n        self.success_F = []\n        self.success_Cr = []\n        self.archive_rate = archive_rate\n        self.archive = []\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.pop = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.nevals = self.pop_size\n\n        while self.nevals < self.budget:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = self.pop[idxs]\n                \n                # Incorporate archive\n                if len(self.archive) > 0 and np.random.rand() < self.archive_rate:\n                    x_arc = self.archive[np.random.randint(0, len(self.archive))]\n                    x_mutated = self.pop[i] + self.F * (x_arc - x_r3)\n                else:\n                    x_mutated = self.pop[i] + self.F * (x_r2 - x_r3)\n                \n                x_mutated = np.clip(x_mutated, self.lb, self.ub)\n\n                # Crossover\n                x_trial = np.copy(self.pop[i])\n                crossover_points = np.random.rand(self.dim) < self.Cr\n                if not np.any(crossover_points):\n                    crossover_points[np.random.randint(0, self.dim)] = True\n                x_trial[crossover_points] = x_mutated[crossover_points]\n\n                # Evaluation\n                f_trial = func(x_trial)\n                self.nevals += 1\n\n                # Selection\n                if f_trial < self.fitness[i]:\n                    # Store successful F and Cr\n                    self.success_F.append(self.F)\n                    self.success_Cr.append(self.Cr)\n                    \n                    # Add replaced individual to archive\n                    if len(self.archive) < self.pop_size:\n                        self.archive.append(self.pop[i].copy())\n                    else:\n                        self.archive[np.random.randint(0, self.pop_size)] = self.pop[i].copy()\n                    \n                    self.fitness[i] = f_trial\n                    self.pop[i] = x_trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = x_trial\n\n            # Adapt F and Cr (after each generation)\n            self.adapt_parameters()\n\n        return self.f_opt, self.x_opt\n\n    def adapt_parameters(self):\n        if self.success_F:\n            self.F = (1 - self.adapt_rate) * self.F + self.adapt_rate * np.mean(self.success_F)\n            self.F = np.clip(self.F, 0.0, 1.0)\n            self.success_F = []\n\n        if self.success_Cr:\n            self.Cr = (1 - self.adapt_rate) * self.Cr + self.adapt_rate * np.mean(self.success_Cr)\n            self.Cr = np.clip(self.Cr, 0.0, 1.0)\n            self.success_Cr = []\n```", "operator": "crossover", "theta_sampled": 0.03246489861542617, "generation": 5, "aucs": [0.14830147821688022, 0.3293531664644518, 0.41637612784812306, 0.7020056597979067, 0.31975165843937525, 0.5022157067070324, 0.298430817521221, 0.3914287338271769, 0.3871451487393647, 0.1976144813941879, 0.722773921645222, 0.9838301415575968, 0.4234561996036592, 0.31006586465230956, 0.7524968941813411, 0.4789841108066567, 0.37222238364840854, 0.6102893919722201, 0.177273071454226, 0.48989884436863196]}, "task_prompt": ""}
{"id": "10c97c75-dcad-4057-8ce4-b360ff4db78a", "fitness": "-inf", "name": "AdaptiveDE", "description": "An adaptive Differential Evolution algorithm that adjusts both the crossover rate and mutation factor based on the success of recent generations. It also incorporates a population diversity metric to encourage exploration when the population converges too quickly.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, cr_init=0.5, f_init=0.7):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.cr_init = cr_init\n        self.f_init = f_init\n        self.cr_history = []\n        self.f_history = []\n        self.diversity_threshold = 0.01 # Threshold for considering the population as converged\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n\n        self.cr = self.cr_init\n        self.f = self.f_init\n\n        while self.budget > 0:\n            new_population = np.zeros_like(self.population)\n            new_fitness = np.zeros_like(self.fitness)\n\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = self.population[idxs]\n                x_mutated = self.population[i] + self.f * (x_r1 - x_r2)\n\n                # Crossover\n                x_trial = np.copy(self.population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < self.cr or j == j_rand:\n                        x_trial[j] = x_mutated[j]\n                        x_trial[j] = np.clip(x_trial[j], func.bounds.lb[0], func.bounds.ub[0])  # Clip to bounds\n\n                # Selection\n                f_trial = func(x_trial)\n                self.budget -= 1\n                if f_trial < self.fitness[i]:\n                    new_population[i] = x_trial\n                    new_fitness[i] = f_trial\n                else:\n                    new_population[i] = self.population[i]\n                    new_fitness[i] = self.fitness[i]\n\n                if new_fitness[i] < self.f_opt:\n                    self.f_opt = new_fitness[i]\n                    self.x_opt = new_population[i]\n            \n            # Update population and fitness\n            self.population = new_population\n            self.fitness = new_fitness\n\n            # Adapt CR and F\n            self.cr = self.adapt_cr()\n            self.f = self.adapt_f()\n\n            # Check for population diversity\n            diversity = np.std(self.population)\n            if diversity < self.diversity_threshold:\n                # If population converges, re-initialize a portion of it\n                num_reinit = int(0.2 * self.pop_size)  # Re-initialize 20% of population\n                reinit_indices = np.random.choice(self.pop_size, num_reinit, replace=False)\n                self.population[reinit_indices] = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(num_reinit, self.dim))\n                for i in reinit_indices:\n                    self.fitness[i] = func(self.population[i])\n                    self.budget -= 1\n                    if self.fitness[i] < self.f_opt:\n                        self.f_opt = self.fitness[i]\n                        self.x_opt = self.population[i]\n\n\n        return self.f_opt, self.x_opt\n\n    def adapt_cr(self):\n        # Simple adaptive strategy: adjust CR based on recent success\n        if len(self.cr_history) < 5:\n            return self.cr\n        \n        success_rate = np.mean(self.cr_history[-5:])\n        self.cr = np.clip(success_rate, 0.1, 0.9)  # Stay within reasonable bounds\n        return self.cr\n\n    def adapt_f(self):\n        # Simple adaptive strategy: adjust F based on recent success\n        if len(self.f_history) < 5:\n            return self.f\n        \n        success_rate = np.mean(self.f_history[-5:])\n        self.f = np.clip(success_rate, 0.1, 1.0)  # Stay within reasonable bounds\n        return self.f", "configspace": "", "generation": 5, "feedback": "Evaluation timed out after 300 seconds.", "error": "Evaluation timed out after 300 seconds.", "parent_ids": [], "operator": "random_new", "metadata": {"llm_response": "# Description: An adaptive Differential Evolution algorithm that adjusts both the crossover rate and mutation factor based on the success of recent generations. It also incorporates a population diversity metric to encourage exploration when the population converges too quickly.\n# Code: \n```python\nimport numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, cr_init=0.5, f_init=0.7):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.cr_init = cr_init\n        self.f_init = f_init\n        self.cr_history = []\n        self.f_history = []\n        self.diversity_threshold = 0.01 # Threshold for considering the population as converged\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n\n        self.cr = self.cr_init\n        self.f = self.f_init\n\n        while self.budget > 0:\n            new_population = np.zeros_like(self.population)\n            new_fitness = np.zeros_like(self.fitness)\n\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = self.population[idxs]\n                x_mutated = self.population[i] + self.f * (x_r1 - x_r2)\n\n                # Crossover\n                x_trial = np.copy(self.population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < self.cr or j == j_rand:\n                        x_trial[j] = x_mutated[j]\n                        x_trial[j] = np.clip(x_trial[j], func.bounds.lb[0], func.bounds.ub[0])  # Clip to bounds\n\n                # Selection\n                f_trial = func(x_trial)\n                self.budget -= 1\n                if f_trial < self.fitness[i]:\n                    new_population[i] = x_trial\n                    new_fitness[i] = f_trial\n                else:\n                    new_population[i] = self.population[i]\n                    new_fitness[i] = self.fitness[i]\n\n                if new_fitness[i] < self.f_opt:\n                    self.f_opt = new_fitness[i]\n                    self.x_opt = new_population[i]\n            \n            # Update population and fitness\n            self.population = new_population\n            self.fitness = new_fitness\n\n            # Adapt CR and F\n            self.cr = self.adapt_cr()\n            self.f = self.adapt_f()\n\n            # Check for population diversity\n            diversity = np.std(self.population)\n            if diversity < self.diversity_threshold:\n                # If population converges, re-initialize a portion of it\n                num_reinit = int(0.2 * self.pop_size)  # Re-initialize 20% of population\n                reinit_indices = np.random.choice(self.pop_size, num_reinit, replace=False)\n                self.population[reinit_indices] = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(num_reinit, self.dim))\n                for i in reinit_indices:\n                    self.fitness[i] = func(self.population[i])\n                    self.budget -= 1\n                    if self.fitness[i] < self.f_opt:\n                        self.f_opt = self.fitness[i]\n                        self.x_opt = self.population[i]\n\n\n        return self.f_opt, self.x_opt\n\n    def adapt_cr(self):\n        # Simple adaptive strategy: adjust CR based on recent success\n        if len(self.cr_history) < 5:\n            return self.cr\n        \n        success_rate = np.mean(self.cr_history[-5:])\n        self.cr = np.clip(success_rate, 0.1, 0.9)  # Stay within reasonable bounds\n        return self.cr\n\n    def adapt_f(self):\n        # Simple adaptive strategy: adjust F based on recent success\n        if len(self.f_history) < 5:\n            return self.f\n        \n        success_rate = np.mean(self.f_history[-5:])\n        self.f = np.clip(success_rate, 0.1, 1.0)  # Stay within reasonable bounds\n        return self.f\n```", "operator": "random_new", "theta_sampled": 0.009374838000139859, "generation": 5}, "task_prompt": ""}
{"id": "706f7e4c-c317-4911-a172-eff794f60510", "fitness": 0.3369663166240959, "name": "AdaptiveDERestart", "description": "An adaptive Differential Evolution algorithm that adjusts the crossover rate and mutation factor based on the success of previous generations, using a moving average to smooth the adaptation. It also incorporates a restart mechanism to escape local optima.", "code": "import numpy as np\n\nclass AdaptiveDERestart:\n    def __init__(self, budget=10000, dim=10, pop_size=50, restart_prob=0.05):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.restart_prob = restart_prob\n        self.F = 0.5  # Initial mutation factor\n        self.CR = 0.7 # Initial crossover rate\n        self.F_history = []\n        self.CR_history = []\n        self.history_length = 10\n        \n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.n_evals = self.pop_size\n\n        while self.n_evals < self.budget:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = self.population[idxs]\n                x_mutated = self.population[i] + self.F * (x_r2 - x_r3)\n\n                # Crossover\n                x_trial = np.copy(self.population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR or j == j_rand:\n                        x_trial[j] = x_mutated[j]\n\n                x_trial = np.clip(x_trial, func.bounds.lb, func.bounds.ub)\n\n                # Evaluation\n                f_trial = func(x_trial)\n                self.n_evals += 1\n\n                # Selection\n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.population[i] = x_trial\n\n                    #Update F and CR history\n                    self.F_history.append(self.F)\n                    self.CR_history.append(self.CR)\n\n\n                if self.fitness[i] < self.f_opt:\n                    self.f_opt = self.fitness[i]\n                    self.x_opt = self.population[i]\n                    \n                if self.n_evals >= self.budget:\n                  break\n\n            # Adapt F and CR based on the recent history\n            if len(self.F_history) > self.history_length:\n                self.F = np.mean(self.F_history[-self.history_length:])\n                self.CR = np.mean(self.CR_history[-self.history_length:])\n                self.F = np.clip(self.F, 0.1, 0.9)\n                self.CR = np.clip(self.CR, 0.1, 0.9)\n            else:\n                self.F = 0.5 #default\n                self.CR = 0.7 #default\n\n            # Restart mechanism\n            if np.random.rand() < self.restart_prob:\n                self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                self.fitness = np.array([func(x) for x in self.population])\n                self.n_evals += self.pop_size\n                \n                if np.min(self.fitness) < self.f_opt:\n                    self.f_opt = np.min(self.fitness)\n                    self.x_opt = self.population[np.argmin(self.fitness)]\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "The algorithm AdaptiveDERestart scored 0.337 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": "random_new", "metadata": {"llm_response": "# Description: An adaptive Differential Evolution algorithm that adjusts the crossover rate and mutation factor based on the success of previous generations, using a moving average to smooth the adaptation. It also incorporates a restart mechanism to escape local optima.\n# Code:\n```python\nimport numpy as np\n\nclass AdaptiveDERestart:\n    def __init__(self, budget=10000, dim=10, pop_size=50, restart_prob=0.05):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.restart_prob = restart_prob\n        self.F = 0.5  # Initial mutation factor\n        self.CR = 0.7 # Initial crossover rate\n        self.F_history = []\n        self.CR_history = []\n        self.history_length = 10\n        \n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.n_evals = self.pop_size\n\n        while self.n_evals < self.budget:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = self.population[idxs]\n                x_mutated = self.population[i] + self.F * (x_r2 - x_r3)\n\n                # Crossover\n                x_trial = np.copy(self.population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR or j == j_rand:\n                        x_trial[j] = x_mutated[j]\n\n                x_trial = np.clip(x_trial, func.bounds.lb, func.bounds.ub)\n\n                # Evaluation\n                f_trial = func(x_trial)\n                self.n_evals += 1\n\n                # Selection\n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.population[i] = x_trial\n\n                    #Update F and CR history\n                    self.F_history.append(self.F)\n                    self.CR_history.append(self.CR)\n\n\n                if self.fitness[i] < self.f_opt:\n                    self.f_opt = self.fitness[i]\n                    self.x_opt = self.population[i]\n                    \n                if self.n_evals >= self.budget:\n                  break\n\n            # Adapt F and CR based on the recent history\n            if len(self.F_history) > self.history_length:\n                self.F = np.mean(self.F_history[-self.history_length:])\n                self.CR = np.mean(self.CR_history[-self.history_length:])\n                self.F = np.clip(self.F, 0.1, 0.9)\n                self.CR = np.clip(self.CR, 0.1, 0.9)\n            else:\n                self.F = 0.5 #default\n                self.CR = 0.7 #default\n\n            # Restart mechanism\n            if np.random.rand() < self.restart_prob:\n                self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                self.fitness = np.array([func(x) for x in self.population])\n                self.n_evals += self.pop_size\n                \n                if np.min(self.fitness) < self.f_opt:\n                    self.f_opt = np.min(self.fitness)\n                    self.x_opt = self.population[np.argmin(self.fitness)]\n\n\n        return self.f_opt, self.x_opt\n```", "operator": "random_new", "theta_sampled": 0.00930737278030979, "generation": 5, "aucs": [0.13407510381318954, 0.2510377689665382, 0.3553184581870983, 0.3570560381839064, 0.22458234118740183, 0.34715753748994216, 0.2790235942917769, 0.2758955937215857, 0.23189848357211518, 0.1777238490675851, 0.32990775457734656, 0.9989351961576212, 0.2937510431768229, 0.24701424629006352, 0.6720953514634704, 0.3303040701133948, 0.2735838699856664, 0.32228342739599247, 0.1691236842716458, 0.46855892056875437]}, "task_prompt": ""}
{"id": "1473b160-8b76-4e8c-93f2-a9cc2a22e698", "fitness": 0.38655671769652494, "name": "AdaptiveDE", "description": "An adaptive Differential Evolution algorithm that adjusts scaling factor F and crossover rate Cr based on the recent success rate of previous generations, incorporating an archive to maintain diversity.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=40, archive_size=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.F = 0.5  # Initial scaling factor\n        self.Cr = 0.9  # Initial crossover rate\n        self.archive = []\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in self.population])\n        self.n_evals = self.pop_size\n\n        while self.n_evals < self.budget:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_1, x_2, x_3 = self.population[idxs]\n\n                # Use archive with small probability\n                if len(self.archive) > 0 and np.random.rand() < 0.1:\n                    idx_archive = np.random.randint(len(self.archive))\n                    x_3 = self.archive[idx_archive]\n\n                v = self.population[i] + self.F * (x_2 - x_3)\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                j_rand = np.random.randint(self.dim)\n                u = np.array([v[j] if (np.random.rand() < self.Cr or j == j_rand) else self.population[i, j] for j in range(self.dim)])\n\n                # Selection\n                f_u = func(u)\n                self.n_evals += 1\n\n                if f_u < fitness[i]:\n                    # Update population\n                    self.population[i] = u\n                    fitness[i] = f_u\n\n                    # Update archive\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(self.population[i].copy())\n                    else:\n                        # Replace a random element in the archive\n                        idx_replace = np.random.randint(self.archive_size)\n                        self.archive[idx_replace] = self.population[i].copy()\n\n                    # Adapt F and Cr - Simplified adaptation based on success\n                    self.F = np.clip(self.F * np.random.normal(1, 0.1), 0.1, 1.0)\n                    self.Cr = np.clip(self.Cr * np.random.normal(1, 0.1), 0.1, 1.0)\n\n                if f_u < self.f_opt:\n                    self.f_opt = f_u\n                    self.x_opt = u\n\n                if self.n_evals >= self.budget:\n                    break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "The algorithm AdaptiveDE scored 0.387 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["40bd89f0-32fe-4e6f-8887-5e032130b8e1"], "operator": "simplify", "metadata": {"llm_response": "# Description: An adaptive Differential Evolution algorithm that adjusts scaling factor F and crossover rate Cr based on the recent success rate of previous generations, incorporating an archive to maintain diversity.\n# Code: \n```python\nimport numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=40, archive_size=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.F = 0.5  # Initial scaling factor\n        self.Cr = 0.9  # Initial crossover rate\n        self.archive = []\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in self.population])\n        self.n_evals = self.pop_size\n\n        while self.n_evals < self.budget:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_1, x_2, x_3 = self.population[idxs]\n\n                # Use archive with small probability\n                if len(self.archive) > 0 and np.random.rand() < 0.1:\n                    idx_archive = np.random.randint(len(self.archive))\n                    x_3 = self.archive[idx_archive]\n\n                v = self.population[i] + self.F * (x_2 - x_3)\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                j_rand = np.random.randint(self.dim)\n                u = np.array([v[j] if (np.random.rand() < self.Cr or j == j_rand) else self.population[i, j] for j in range(self.dim)])\n\n                # Selection\n                f_u = func(u)\n                self.n_evals += 1\n\n                if f_u < fitness[i]:\n                    # Update population\n                    self.population[i] = u\n                    fitness[i] = f_u\n\n                    # Update archive\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(self.population[i].copy())\n                    else:\n                        # Replace a random element in the archive\n                        idx_replace = np.random.randint(self.archive_size)\n                        self.archive[idx_replace] = self.population[i].copy()\n\n                    # Adapt F and Cr - Simplified adaptation based on success\n                    self.F = np.clip(self.F * np.random.normal(1, 0.1), 0.1, 1.0)\n                    self.Cr = np.clip(self.Cr * np.random.normal(1, 0.1), 0.1, 1.0)\n\n                if f_u < self.f_opt:\n                    self.f_opt = f_u\n                    self.x_opt = u\n\n                if self.n_evals >= self.budget:\n                    break\n\n        return self.f_opt, self.x_opt\n```", "operator": "simplify", "theta_sampled": 0.01812816555241526, "generation": 5, "aucs": [0.14637588192716589, 0.27584600552960103, 0.40265969544970703, 0.3957992019099902, 0.2681889679372903, 0.4323206759281857, 0.29047618799163366, 0.32549471324183776, 0.3363675732303719, 0.20119086912756867, 0.35913505830516457, 0.9955346917281078, 0.30923615908560775, 0.2851491567553386, 0.7258492283551892, 0.47387095338257657, 0.32384143899652296, 0.48833559283640704, 0.21012111395671174, 0.4853411882555225]}, "task_prompt": ""}
{"id": "5d65fd08-df10-4000-9f37-77516ffce740", "fitness": "-inf", "name": "AdaptiveSOMA", "description": "Implement a Self-Organizing Migrating Algorithm (SOMA) with adaptive step size control and a perturbation strategy to enhance exploration.", "code": "import numpy as np\n\nclass AdaptiveSOMA:\n    def __init__(self, budget=10000, dim=10, path_length=3.0, step_size=0.1, perturbation_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.path_length = path_length  # Maximum distance to move towards the leader\n        self.step_size = step_size      # Discretization of the path\n        self.perturbation_rate = perturbation_rate # Probability of random perturbation\n        self.archive = []\n        self.archive_size = 10\n\n    def __call__(self, func):\n        # Initialize population randomly\n        population_size = min(100, self.budget // 100)\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(population_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= population_size\n        \n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)]\n\n        # Evolution loop\n        while self.budget > 0:\n            # Select leader (best individual)\n            leader_index = np.argmin(fitness)\n            leader = population[leader_index]\n\n            for i in range(population_size):\n                if i == leader_index:\n                    continue\n\n                # Calculate displacement vector\n                displacement = leader - population[i]\n\n                # Move along the path towards the leader\n                for step in np.arange(self.step_size, self.path_length + self.step_size, self.step_size):\n                    if self.budget <= 0:\n                        break\n                    \n                    new_position = population[i] + step * displacement\n                    \n                    # Perturbation\n                    for j in range(self.dim):\n                        if np.random.rand() < self.perturbation_rate:\n                            new_position[j] = np.random.uniform(func.bounds.lb[j], func.bounds.ub[j])\n\n                    # Clip to boundaries\n                    new_position = np.clip(new_position, func.bounds.lb, func.bounds.ub)\n\n                    # Evaluate new position\n                    new_fitness = func(new_position)\n                    self.budget -= 1\n                    \n                    # Update if better\n                    if new_fitness < fitness[i]:\n                        fitness[i] = new_fitness\n                        population[i] = new_position\n\n                        if new_fitness < self.f_opt:\n                            self.f_opt = new_fitness\n                            self.x_opt = new_position\n\n            # Adaptive step size (optional): Reduce step size if no improvement for a while\n            if len(self.archive) > self.archive_size and self.f_opt >= self.archive[-self.archive_size]:\n                self.step_size *= 0.9\n            else:\n                self.step_size = min(0.1, self.step_size * 1.1)\n                \n            self.archive.append(self.f_opt)\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "Evaluation timed out after 300 seconds.", "error": "Evaluation timed out after 300 seconds.", "parent_ids": [], "operator": "random_new", "metadata": {"llm_response": "# Description: Implement a Self-Organizing Migrating Algorithm (SOMA) with adaptive step size control and a perturbation strategy to enhance exploration.\n# Code: \n```python\nimport numpy as np\n\nclass AdaptiveSOMA:\n    def __init__(self, budget=10000, dim=10, path_length=3.0, step_size=0.1, perturbation_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.path_length = path_length  # Maximum distance to move towards the leader\n        self.step_size = step_size      # Discretization of the path\n        self.perturbation_rate = perturbation_rate # Probability of random perturbation\n        self.archive = []\n        self.archive_size = 10\n\n    def __call__(self, func):\n        # Initialize population randomly\n        population_size = min(100, self.budget // 100)\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(population_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= population_size\n        \n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)]\n\n        # Evolution loop\n        while self.budget > 0:\n            # Select leader (best individual)\n            leader_index = np.argmin(fitness)\n            leader = population[leader_index]\n\n            for i in range(population_size):\n                if i == leader_index:\n                    continue\n\n                # Calculate displacement vector\n                displacement = leader - population[i]\n\n                # Move along the path towards the leader\n                for step in np.arange(self.step_size, self.path_length + self.step_size, self.step_size):\n                    if self.budget <= 0:\n                        break\n                    \n                    new_position = population[i] + step * displacement\n                    \n                    # Perturbation\n                    for j in range(self.dim):\n                        if np.random.rand() < self.perturbation_rate:\n                            new_position[j] = np.random.uniform(func.bounds.lb[j], func.bounds.ub[j])\n\n                    # Clip to boundaries\n                    new_position = np.clip(new_position, func.bounds.lb, func.bounds.ub)\n\n                    # Evaluate new position\n                    new_fitness = func(new_position)\n                    self.budget -= 1\n                    \n                    # Update if better\n                    if new_fitness < fitness[i]:\n                        fitness[i] = new_fitness\n                        population[i] = new_position\n\n                        if new_fitness < self.f_opt:\n                            self.f_opt = new_fitness\n                            self.x_opt = new_position\n\n            # Adaptive step size (optional): Reduce step size if no improvement for a while\n            if len(self.archive) > self.archive_size and self.f_opt >= self.archive[-self.archive_size]:\n                self.step_size *= 0.9\n            else:\n                self.step_size = min(0.1, self.step_size * 1.1)\n                \n            self.archive.append(self.f_opt)\n        return self.f_opt, self.x_opt\n```", "operator": "random_new", "theta_sampled": 0.00725180505137051, "generation": 6}, "task_prompt": ""}
{"id": "986d1f94-c3b8-4da1-8570-00081de694c4", "fitness": "-inf", "name": "AdaptiveDEArchiveWindowMemory", "description": "Adaptive Differential Evolution with a dynamically updated archive to enhance exploration around promising regions.", "code": "import numpy as np\n\nclass AdaptiveDEArchiveWindowMemory:\n    def __init__(self, budget=10000, dim=10, pop_size=40, F_init=0.5, Cr_init=0.9, window_size=50, memory_size=10, archive_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F_init\n        self.Cr = Cr_init\n        self.window_size = window_size\n        self.memory_size = memory_size\n        self.F_memory = np.full(self.memory_size, F_init)\n        self.Cr_memory = np.full(self.memory_size, Cr_init)\n        self.F_idx = 0\n        self.Cr_idx = 0\n        self.success_F = []\n        self.success_Cr = []\n        self.archive_rate = archive_rate\n        self.archive = []\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        self.population = np.random.uniform(lb, ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in self.population])\n        self.n_evals = self.pop_size\n\n        while self.n_evals < self.budget:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_1, x_2, x_3 = self.population[idxs]\n\n                # Incorporate archive\n                if len(self.archive) > 0 and np.random.rand() < self.archive_rate:\n                    x_arc = self.archive[np.random.randint(0, len(self.archive))]\n                    v = self.population[i] + self.F * (x_arc - x_3)\n                else:\n                    v = self.population[i] + self.F * (x_2 - x_3)\n\n                v = np.clip(v, lb, ub)\n\n                # Crossover\n                j_rand = np.random.randint(self.dim)\n                u = np.array([v[j] if (np.random.rand() < self.Cr or j == j_rand) else self.population[i, j] for j in range(self.dim)])\n\n                # Selection\n                f_u = func(u)\n                self.n_evals += 1\n\n                if f_u < fitness[i]:\n                    # Store successful F and Cr\n                    self.success_F.append(self.F)\n                    self.success_Cr.append(self.Cr)\n                    \n                    # Add replaced individual to archive\n                    if len(self.archive) < self.pop_size:\n                        self.archive.append(self.population[i].copy())\n                    else:\n                        self.archive[np.random.randint(0, self.pop_size)] = self.population[i].copy()\n\n                    self.population[i] = u\n                    fitness[i] = f_u\n\n                    if f_u < self.f_opt:\n                        self.f_opt = f_u\n                        self.x_opt = u\n\n                if self.n_evals >= self.budget:\n                    break\n\n            # Adapt F and Cr (after each generation)\n            self.adapt_parameters()\n\n        return self.f_opt, self.x_opt\n\n    def adapt_parameters(self):\n        # Window-based averaging for F and Cr\n        if self.success_F:\n            self.F_memory[self.F_idx] = np.mean(self.success_F)\n            self.F_idx = (self.F_idx + 1) % self.memory_size\n            self.F = np.mean(self.F_memory)\n            self.F = np.clip(self.F, 0.1, 1.0)\n            self.success_F = []\n\n        if self.success_Cr:\n            self.Cr_memory[self.Cr_idx] = np.mean(self.success_Cr)\n            self.Cr_idx = (self.Cr_idx + 1) % self.memory_size\n            self.Cr = np.mean(self.Cr_memory)\n            self.Cr = np.clip(self.Cr, 0.1, 1.0)\n            self.success_Cr = []", "configspace": "", "generation": 6, "feedback": "Evaluation timed out after 300 seconds.", "error": "Evaluation timed out after 300 seconds.", "parent_ids": ["2fa45af8-e020-4e0b-9f07-43390b8ca505", "a5611c32-bed4-4ec0-9aa1-bf509a3dc71f"], "operator": "crossover", "metadata": {"llm_response": "# Description: Adaptive Differential Evolution with a dynamically updated archive to enhance exploration around promising regions.\n# Code: \n```python\nimport numpy as np\n\nclass AdaptiveDEArchiveWindowMemory:\n    def __init__(self, budget=10000, dim=10, pop_size=40, F_init=0.5, Cr_init=0.9, window_size=50, memory_size=10, archive_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F_init\n        self.Cr = Cr_init\n        self.window_size = window_size\n        self.memory_size = memory_size\n        self.F_memory = np.full(self.memory_size, F_init)\n        self.Cr_memory = np.full(self.memory_size, Cr_init)\n        self.F_idx = 0\n        self.Cr_idx = 0\n        self.success_F = []\n        self.success_Cr = []\n        self.archive_rate = archive_rate\n        self.archive = []\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        self.population = np.random.uniform(lb, ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in self.population])\n        self.n_evals = self.pop_size\n\n        while self.n_evals < self.budget:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_1, x_2, x_3 = self.population[idxs]\n\n                # Incorporate archive\n                if len(self.archive) > 0 and np.random.rand() < self.archive_rate:\n                    x_arc = self.archive[np.random.randint(0, len(self.archive))]\n                    v = self.population[i] + self.F * (x_arc - x_3)\n                else:\n                    v = self.population[i] + self.F * (x_2 - x_3)\n\n                v = np.clip(v, lb, ub)\n\n                # Crossover\n                j_rand = np.random.randint(self.dim)\n                u = np.array([v[j] if (np.random.rand() < self.Cr or j == j_rand) else self.population[i, j] for j in range(self.dim)])\n\n                # Selection\n                f_u = func(u)\n                self.n_evals += 1\n\n                if f_u < fitness[i]:\n                    # Store successful F and Cr\n                    self.success_F.append(self.F)\n                    self.success_Cr.append(self.Cr)\n                    \n                    # Add replaced individual to archive\n                    if len(self.archive) < self.pop_size:\n                        self.archive.append(self.population[i].copy())\n                    else:\n                        self.archive[np.random.randint(0, self.pop_size)] = self.population[i].copy()\n\n                    self.population[i] = u\n                    fitness[i] = f_u\n\n                    if f_u < self.f_opt:\n                        self.f_opt = f_u\n                        self.x_opt = u\n\n                if self.n_evals >= self.budget:\n                    break\n\n            # Adapt F and Cr (after each generation)\n            self.adapt_parameters()\n\n        return self.f_opt, self.x_opt\n\n    def adapt_parameters(self):\n        # Window-based averaging for F and Cr\n        if self.success_F:\n            self.F_memory[self.F_idx] = np.mean(self.success_F)\n            self.F_idx = (self.F_idx + 1) % self.memory_size\n            self.F = np.mean(self.F_memory)\n            self.F = np.clip(self.F, 0.1, 1.0)\n            self.success_F = []\n\n        if self.success_Cr:\n            self.Cr_memory[self.Cr_idx] = np.mean(self.success_Cr)\n            self.Cr_idx = (self.Cr_idx + 1) % self.memory_size\n            self.Cr = np.mean(self.Cr_memory)\n            self.Cr = np.clip(self.Cr, 0.1, 1.0)\n            self.success_Cr = []\n```", "operator": "crossover", "theta_sampled": 0.02606021468238492, "generation": 6}, "task_prompt": ""}
{"id": "ee4e029d-3182-4722-bc81-ee476b8cffa8", "fitness": 0.25267508345857514, "name": "CooperativeCoevolution", "description": "Implements a cooperative co-evolutionary algorithm, where sub-populations optimize different dimensions independently and periodically exchange information.", "code": "import numpy as np\n\nclass CooperativeCoevolution:\n    def __init__(self, budget=10000, dim=10, pop_size=20, sub_pop_size=5, exchange_interval=50):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.sub_pop_size = sub_pop_size\n        self.exchange_interval = exchange_interval\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        eval_count = 0\n\n        # Initialize sub-populations\n        sub_populations = []\n        for i in range(self.dim):\n            sub_populations.append(np.random.uniform(func.bounds.lb[0], func.bounds.ub[0], size=self.sub_pop_size))\n\n        # Create a complete solution using a random vector\n        best_solution = np.random.uniform(func.bounds.lb, func.bounds.ub)\n        best_fitness = func(best_solution)\n        eval_count += 1\n            \n\n        while eval_count < self.budget:\n            # Iterate over dimensions\n            for dim_index in range(self.dim):\n                # Optimize each dimension using a simple strategy (e.g., random search within each sub-population)\n                for i in range(self.sub_pop_size):\n\n                    #Modify dimension and create a complete solution\n                    temp_solution = best_solution.copy()\n                    mutation = np.random.uniform(-0.1, 0.1) # Small mutation step\n                    new_val = sub_populations[dim_index][i] + mutation\n                    new_val = np.clip(new_val, func.bounds.lb[0], func.bounds.ub[0])\n                    temp_solution[dim_index] = new_val\n\n                    if eval_count < self.budget:\n                        fitness = func(temp_solution)\n                        eval_count += 1\n                        \n                        if fitness < best_fitness:\n                            best_fitness = fitness\n                            best_solution = temp_solution\n                            sub_populations[dim_index][i] = new_val #Update subpop\n                    else:\n                        break  # Budget exceeded\n                \n                if eval_count >= self.budget:\n                    break\n\n            # Periodic Exchange of Information\n            if eval_count % self.exchange_interval == 0:\n                # Replace worst individuals in each sub-population with corresponding dimension of best solution\n                for dim_index in range(self.dim):\n                    worst_index = np.argmax(np.abs(sub_populations[dim_index] - best_solution[dim_index]))\n                    sub_populations[dim_index][worst_index] = best_solution[dim_index]\n\n            if best_fitness < self.f_opt:\n                self.f_opt = best_fitness\n                self.x_opt = best_solution\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "The algorithm CooperativeCoevolution scored 0.253 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": "random_new", "metadata": {"llm_response": "# Description: Implements a cooperative co-evolutionary algorithm, where sub-populations optimize different dimensions independently and periodically exchange information.\n# Code: \n```python\nimport numpy as np\n\nclass CooperativeCoevolution:\n    def __init__(self, budget=10000, dim=10, pop_size=20, sub_pop_size=5, exchange_interval=50):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.sub_pop_size = sub_pop_size\n        self.exchange_interval = exchange_interval\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        eval_count = 0\n\n        # Initialize sub-populations\n        sub_populations = []\n        for i in range(self.dim):\n            sub_populations.append(np.random.uniform(func.bounds.lb[0], func.bounds.ub[0], size=self.sub_pop_size))\n\n        # Create a complete solution using a random vector\n        best_solution = np.random.uniform(func.bounds.lb, func.bounds.ub)\n        best_fitness = func(best_solution)\n        eval_count += 1\n            \n\n        while eval_count < self.budget:\n            # Iterate over dimensions\n            for dim_index in range(self.dim):\n                # Optimize each dimension using a simple strategy (e.g., random search within each sub-population)\n                for i in range(self.sub_pop_size):\n\n                    #Modify dimension and create a complete solution\n                    temp_solution = best_solution.copy()\n                    mutation = np.random.uniform(-0.1, 0.1) # Small mutation step\n                    new_val = sub_populations[dim_index][i] + mutation\n                    new_val = np.clip(new_val, func.bounds.lb[0], func.bounds.ub[0])\n                    temp_solution[dim_index] = new_val\n\n                    if eval_count < self.budget:\n                        fitness = func(temp_solution)\n                        eval_count += 1\n                        \n                        if fitness < best_fitness:\n                            best_fitness = fitness\n                            best_solution = temp_solution\n                            sub_populations[dim_index][i] = new_val #Update subpop\n                    else:\n                        break  # Budget exceeded\n                \n                if eval_count >= self.budget:\n                    break\n\n            # Periodic Exchange of Information\n            if eval_count % self.exchange_interval == 0:\n                # Replace worst individuals in each sub-population with corresponding dimension of best solution\n                for dim_index in range(self.dim):\n                    worst_index = np.argmax(np.abs(sub_populations[dim_index] - best_solution[dim_index]))\n                    sub_populations[dim_index][worst_index] = best_solution[dim_index]\n\n            if best_fitness < self.f_opt:\n                self.f_opt = best_fitness\n                self.x_opt = best_solution\n\n        return self.f_opt, self.x_opt\n```", "operator": "random_new", "theta_sampled": 0.0005058289015630372, "generation": 6, "aucs": [0.12422911197492081, 0.14899231557039005, 0.2505453904433761, 0.1441442750692593, 0.14373239067075771, 0.30580233955537206, 0.21317328186294915, 0.21381364286869986, 0.15061970340231678, 0.15933212079464043, 0.1753729326805885, 0.2100671891624143, 0.22073075696001143, 0.19366073405103124, 0.6156185783508723, 0.641439474720224, 0.20852481654507382, 0.2908714569144226, 0.16999528376033612, 0.4728358738138462]}, "task_prompt": ""}
{"id": "2175dde7-397c-4cd3-9989-cd8a40f6026d", "fitness": 0.4515446357123204, "name": "AdaptiveDEArchiveWindow", "description": "Combines adaptive DE with an archive of past solutions for enhanced exploration and exploitation.", "code": "import numpy as np\n\nclass AdaptiveDEArchiveWindow:\n    def __init__(self, budget=10000, dim=10, pop_size=40, F=0.5, Cr=0.9, window_size=50, archive_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F  # Initial scaling factor\n        self.Cr = Cr  # Initial crossover rate\n        self.window_size = window_size\n        self.F_history = []\n        self.Cr_history = []\n        self.success_history = []\n        self.archive_rate = archive_rate\n        self.archive = []\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in self.population])\n        self.n_evals = self.pop_size\n        \n        while self.n_evals < self.budget:\n            for i in range(self.pop_size):\n                # Parameter adaptation based on history\n                if len(self.F_history) > 0:\n                    recent_F = self.F_history[-self.window_size:]\n                    self.F = np.mean(recent_F) if len(recent_F) > 0 else 0.5\n                    self.F = np.clip(self.F, 0.1, 1.0)  # Keep F within bounds\n\n                    recent_Cr = self.Cr_history[-self.window_size:]\n                    self.Cr = np.mean(recent_Cr) if len(recent_Cr) > 0 else 0.9\n                    self.Cr = np.clip(self.Cr, 0.1, 1.0)  # Keep Cr within bounds\n                \n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_1, x_2, x_3 = self.population[idxs]\n\n                # Incorporate archive\n                if len(self.archive) > 0 and np.random.rand() < self.archive_rate:\n                    x_arc = self.archive[np.random.randint(0, len(self.archive))]\n                    v = self.population[i] + self.F * (x_arc - x_3)\n                else:\n                    v = self.population[i] + self.F * (x_2 - x_3)\n                \n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n                \n                # Crossover\n                j_rand = np.random.randint(self.dim)\n                u = np.array([v[j] if (np.random.rand() < self.Cr or j == j_rand) else self.population[i, j] for j in range(self.dim)])\n\n                # Selection\n                f_u = func(u)\n                self.n_evals += 1\n                \n                if f_u < fitness[i]:\n                    # Add replaced individual to archive\n                    if len(self.archive) < self.pop_size:\n                        self.archive.append(self.population[i].copy())\n                    else:\n                        self.archive[np.random.randint(0, self.pop_size)] = self.population[i].copy()\n                    \n                    self.population[i] = u\n                    fitness[i] = f_u\n                    self.F_history.append(self.F)\n                    self.Cr_history.append(self.Cr)\n                    self.success_history.append(1)  # Success\n                else:\n                    self.success_history.append(0) # Failure\n                    \n                if f_u < self.f_opt:\n                    self.f_opt = f_u\n                    self.x_opt = u\n\n                if self.n_evals >= self.budget:\n                    break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "The algorithm AdaptiveDEArchiveWindow scored 0.452 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["40bd89f0-32fe-4e6f-8887-5e032130b8e1", "a5611c32-bed4-4ec0-9aa1-bf509a3dc71f"], "operator": "crossover", "metadata": {"llm_response": "# Description: Combines adaptive DE with an archive of past solutions for enhanced exploration and exploitation.\n# Code: \n```python\nimport numpy as np\n\nclass AdaptiveDEArchiveWindow:\n    def __init__(self, budget=10000, dim=10, pop_size=40, F=0.5, Cr=0.9, window_size=50, archive_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F  # Initial scaling factor\n        self.Cr = Cr  # Initial crossover rate\n        self.window_size = window_size\n        self.F_history = []\n        self.Cr_history = []\n        self.success_history = []\n        self.archive_rate = archive_rate\n        self.archive = []\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in self.population])\n        self.n_evals = self.pop_size\n        \n        while self.n_evals < self.budget:\n            for i in range(self.pop_size):\n                # Parameter adaptation based on history\n                if len(self.F_history) > 0:\n                    recent_F = self.F_history[-self.window_size:]\n                    self.F = np.mean(recent_F) if len(recent_F) > 0 else 0.5\n                    self.F = np.clip(self.F, 0.1, 1.0)  # Keep F within bounds\n\n                    recent_Cr = self.Cr_history[-self.window_size:]\n                    self.Cr = np.mean(recent_Cr) if len(recent_Cr) > 0 else 0.9\n                    self.Cr = np.clip(self.Cr, 0.1, 1.0)  # Keep Cr within bounds\n                \n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_1, x_2, x_3 = self.population[idxs]\n\n                # Incorporate archive\n                if len(self.archive) > 0 and np.random.rand() < self.archive_rate:\n                    x_arc = self.archive[np.random.randint(0, len(self.archive))]\n                    v = self.population[i] + self.F * (x_arc - x_3)\n                else:\n                    v = self.population[i] + self.F * (x_2 - x_3)\n                \n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n                \n                # Crossover\n                j_rand = np.random.randint(self.dim)\n                u = np.array([v[j] if (np.random.rand() < self.Cr or j == j_rand) else self.population[i, j] for j in range(self.dim)])\n\n                # Selection\n                f_u = func(u)\n                self.n_evals += 1\n                \n                if f_u < fitness[i]:\n                    # Add replaced individual to archive\n                    if len(self.archive) < self.pop_size:\n                        self.archive.append(self.population[i].copy())\n                    else:\n                        self.archive[np.random.randint(0, self.pop_size)] = self.population[i].copy()\n                    \n                    self.population[i] = u\n                    fitness[i] = f_u\n                    self.F_history.append(self.F)\n                    self.Cr_history.append(self.Cr)\n                    self.success_history.append(1)  # Success\n                else:\n                    self.success_history.append(0) # Failure\n                    \n                if f_u < self.f_opt:\n                    self.f_opt = f_u\n                    self.x_opt = u\n\n                if self.n_evals >= self.budget:\n                    break\n\n        return self.f_opt, self.x_opt\n```", "operator": "crossover", "theta_sampled": 0.0065710077137257315, "generation": 6, "aucs": [0.15227935420757788, 0.32510904362428716, 0.4262647365196711, 0.6057194314721603, 0.35454893055889525, 0.49416170441998486, 0.3015804158754122, 0.36968117879367823, 0.4045776031776167, 0.20116928596218975, 0.7263912260205347, 0.9972955967458262, 0.4477755545676566, 0.33761258751467316, 0.7836580183050825, 0.4723322398557891, 0.35317103980622033, 0.5867129271266971, 0.2000235498679821, 0.49082828982447213]}, "task_prompt": ""}
{"id": "6e225775-931a-49ec-b703-e6ac6c530016", "fitness": "-inf", "name": "AdaptiveDEWithSuccess", "description": "Adaptive DE with memory for parameter adaptation and success-based parameter adjustment.", "code": "import numpy as np\n\nclass AdaptiveDEWithSuccess:\n    def __init__(self, budget=10000, dim=10, pop_size=40, F_init=0.5, Cr_init=0.9, memory_size=10, success_rate_threshold=0.2):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F_init\n        self.Cr = Cr_init\n        self.memory_size = memory_size\n        self.F_memory = np.full(self.memory_size, F_init)\n        self.Cr_memory = np.full(self.memory_size, Cr_init)\n        self.F_idx = 0\n        self.Cr_idx = 0\n        self.success_F = []\n        self.success_Cr = []\n        self.success_rate_threshold = success_rate_threshold\n        self.success_history = []\n\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in self.population])\n        self.n_evals = self.pop_size\n\n        while self.n_evals < self.budget:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_1, x_2, x_3 = self.population[idxs]\n                v = self.population[i] + self.F * (x_2 - x_3)\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                j_rand = np.random.randint(self.dim)\n                u = np.array([v[j] if (np.random.rand() < self.Cr or j == j_rand) else self.population[i, j] for j in range(self.dim)])\n\n                # Selection\n                f_u = func(u)\n                self.n_evals += 1\n\n                if f_u < fitness[i]:\n                    # Store successful F and Cr\n                    self.success_F.append(self.F)\n                    self.success_Cr.append(self.Cr)\n                    self.success_history.append(1)\n\n                    self.population[i] = u\n                    fitness[i] = f_u\n\n                    if f_u < self.f_opt:\n                        self.f_opt = f_u\n                        self.x_opt = u\n                else:\n                    self.success_history.append(0)\n\n\n                if self.n_evals >= self.budget:\n                    break\n\n            # Adapt F and Cr (after each generation)\n            self.adapt_parameters()\n\n        return self.f_opt, self.x_opt\n\n    def adapt_parameters(self):\n        # Window-based averaging for F and Cr\n        if self.success_F:\n            self.F_memory[self.F_idx] = np.mean(self.success_F)\n            self.F_idx = (self.F_idx + 1) % self.memory_size\n            self.F = np.mean(self.F_memory)\n            self.F = np.clip(self.F, 0.1, 1.0)\n            self.success_F = []\n\n        if self.success_Cr:\n            self.Cr_memory[self.Cr_idx] = np.mean(self.success_Cr)\n            self.Cr_idx = (self.Cr_idx + 1) % self.memory_size\n            self.Cr = np.mean(self.Cr_memory)\n            self.Cr = np.clip(self.Cr, 0.1, 1.0)\n            self.success_Cr = []\n        \n        # Adjust F and Cr based on recent success rate\n        if len(self.success_history) > self.pop_size:\n            recent_success_rate = np.mean(self.success_history[-self.pop_size:])\n            if recent_success_rate < self.success_rate_threshold:\n                self.F = min(self.F * 1.1, 1.0)  # Increase F to explore more\n                self.Cr = max(self.Cr * 0.9, 0.1)  # Decrease Cr to rely more on mutation\n            else:\n                self.F = max(self.F * 0.9, 0.1)  # Decrease F to exploit more\n                self.Cr = min(self.Cr * 1.1, 1.0) # Increase Cr to rely more on crossover", "configspace": "", "generation": 6, "feedback": "Evaluation timed out after 300 seconds.", "error": "Evaluation timed out after 300 seconds.", "parent_ids": ["2fa45af8-e020-4e0b-9f07-43390b8ca505", "40bd89f0-32fe-4e6f-8887-5e032130b8e1"], "operator": "crossover", "metadata": {"llm_response": "# Description: Adaptive DE with memory for parameter adaptation and success-based parameter adjustment.\n# Code: \n```python\nimport numpy as np\n\nclass AdaptiveDEWithSuccess:\n    def __init__(self, budget=10000, dim=10, pop_size=40, F_init=0.5, Cr_init=0.9, memory_size=10, success_rate_threshold=0.2):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F_init\n        self.Cr = Cr_init\n        self.memory_size = memory_size\n        self.F_memory = np.full(self.memory_size, F_init)\n        self.Cr_memory = np.full(self.memory_size, Cr_init)\n        self.F_idx = 0\n        self.Cr_idx = 0\n        self.success_F = []\n        self.success_Cr = []\n        self.success_rate_threshold = success_rate_threshold\n        self.success_history = []\n\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in self.population])\n        self.n_evals = self.pop_size\n\n        while self.n_evals < self.budget:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_1, x_2, x_3 = self.population[idxs]\n                v = self.population[i] + self.F * (x_2 - x_3)\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                j_rand = np.random.randint(self.dim)\n                u = np.array([v[j] if (np.random.rand() < self.Cr or j == j_rand) else self.population[i, j] for j in range(self.dim)])\n\n                # Selection\n                f_u = func(u)\n                self.n_evals += 1\n\n                if f_u < fitness[i]:\n                    # Store successful F and Cr\n                    self.success_F.append(self.F)\n                    self.success_Cr.append(self.Cr)\n                    self.success_history.append(1)\n\n                    self.population[i] = u\n                    fitness[i] = f_u\n\n                    if f_u < self.f_opt:\n                        self.f_opt = f_u\n                        self.x_opt = u\n                else:\n                    self.success_history.append(0)\n\n\n                if self.n_evals >= self.budget:\n                    break\n\n            # Adapt F and Cr (after each generation)\n            self.adapt_parameters()\n\n        return self.f_opt, self.x_opt\n\n    def adapt_parameters(self):\n        # Window-based averaging for F and Cr\n        if self.success_F:\n            self.F_memory[self.F_idx] = np.mean(self.success_F)\n            self.F_idx = (self.F_idx + 1) % self.memory_size\n            self.F = np.mean(self.F_memory)\n            self.F = np.clip(self.F, 0.1, 1.0)\n            self.success_F = []\n\n        if self.success_Cr:\n            self.Cr_memory[self.Cr_idx] = np.mean(self.success_Cr)\n            self.Cr_idx = (self.Cr_idx + 1) % self.memory_size\n            self.Cr = np.mean(self.Cr_memory)\n            self.Cr = np.clip(self.Cr, 0.1, 1.0)\n            self.success_Cr = []\n        \n        # Adjust F and Cr based on recent success rate\n        if len(self.success_history) > self.pop_size:\n            recent_success_rate = np.mean(self.success_history[-self.pop_size:])\n            if recent_success_rate < self.success_rate_threshold:\n                self.F = min(self.F * 1.1, 1.0)  # Increase F to explore more\n                self.Cr = max(self.Cr * 0.9, 0.1)  # Decrease Cr to rely more on mutation\n            else:\n                self.F = max(self.F * 0.9, 0.1)  # Decrease F to exploit more\n                self.Cr = min(self.Cr * 1.1, 1.0) # Increase Cr to rely more on crossover\n```", "operator": "crossover", "theta_sampled": 0.013999777353992474, "generation": 6}, "task_prompt": ""}
{"id": "ef47cff3-6048-4579-9394-79f8886d7037", "fitness": "-inf", "name": "AdaptiveDEWindowMemoryArchive", "description": "Adaptive Differential Evolution with Window Memory and Archive Incorporation for enhanced exploration.", "code": "import numpy as np\n\nclass AdaptiveDEWindowMemoryArchive:\n    def __init__(self, budget=10000, dim=10, pop_size=40, F_init=0.5, Cr_init=0.9, window_size=50, memory_size=10, archive_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F_init\n        self.Cr = Cr_init\n        self.window_size = window_size\n        self.memory_size = memory_size\n        self.F_memory = np.full(self.memory_size, F_init)\n        self.Cr_memory = np.full(self.memory_size, Cr_init)\n        self.F_idx = 0\n        self.Cr_idx = 0\n        self.success_F = []\n        self.success_Cr = []\n        self.archive_rate = archive_rate\n        self.archive = []\n        self.lb = -5.0\n        self.ub = 5.0\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in self.population])\n        self.n_evals = self.pop_size\n\n        while self.n_evals < self.budget:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_1, x_2, x_3 = self.population[idxs]\n\n                # Incorporate archive\n                if len(self.archive) > 0 and np.random.rand() < self.archive_rate:\n                    x_arc = self.archive[np.random.randint(0, len(self.archive))]\n                    v = self.population[i] + self.F * (x_arc - x_3)\n                else:\n                    v = self.population[i] + self.F * (x_2 - x_3)\n                \n                v = np.clip(v, self.lb, self.ub)\n\n                # Crossover\n                j_rand = np.random.randint(self.dim)\n                u = np.array([v[j] if (np.random.rand() < self.Cr or j == j_rand) else self.population[i, j] for j in range(self.dim)])\n\n                # Selection\n                f_u = func(u)\n                self.n_evals += 1\n\n                if f_u < fitness[i]:\n                    # Store successful F and Cr\n                    self.success_F.append(self.F)\n                    self.success_Cr.append(self.Cr)\n                    \n                    # Add replaced individual to archive\n                    if len(self.archive) < self.pop_size:\n                        self.archive.append(self.population[i].copy())\n                    else:\n                        self.archive[np.random.randint(0, self.pop_size)] = self.population[i].copy()\n                    \n                    self.population[i] = u\n                    fitness[i] = f_u\n\n                    if f_u < self.f_opt:\n                        self.f_opt = f_u\n                        self.x_opt = u\n\n                if self.n_evals >= self.budget:\n                    break\n\n            # Adapt F and Cr (after each generation)\n            self.adapt_parameters()\n\n        return self.f_opt, self.x_opt\n\n    def adapt_parameters(self):\n        # Window-based averaging for F and Cr\n        if self.success_F:\n            self.F_memory[self.F_idx] = np.mean(self.success_F)\n            self.F_idx = (self.F_idx + 1) % self.memory_size\n            self.F = np.mean(self.F_memory)\n            self.F = np.clip(self.F, 0.1, 1.0)\n            self.success_F = []\n\n        if self.success_Cr:\n            self.Cr_memory[self.Cr_idx] = np.mean(self.success_Cr)\n            self.Cr_idx = (self.Cr_idx + 1) % self.memory_size\n            self.Cr = np.mean(self.Cr_memory)\n            self.Cr = np.clip(self.Cr, 0.1, 1.0)\n            self.success_Cr = []", "configspace": "", "generation": 6, "feedback": "Evaluation timed out after 300 seconds.", "error": "Evaluation timed out after 300 seconds.", "parent_ids": ["2fa45af8-e020-4e0b-9f07-43390b8ca505", "a5611c32-bed4-4ec0-9aa1-bf509a3dc71f"], "operator": "crossover", "metadata": {"llm_response": "# Description: Adaptive Differential Evolution with Window Memory and Archive Incorporation for enhanced exploration.\n# Code: \n```python\nimport numpy as np\n\nclass AdaptiveDEWindowMemoryArchive:\n    def __init__(self, budget=10000, dim=10, pop_size=40, F_init=0.5, Cr_init=0.9, window_size=50, memory_size=10, archive_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F_init\n        self.Cr = Cr_init\n        self.window_size = window_size\n        self.memory_size = memory_size\n        self.F_memory = np.full(self.memory_size, F_init)\n        self.Cr_memory = np.full(self.memory_size, Cr_init)\n        self.F_idx = 0\n        self.Cr_idx = 0\n        self.success_F = []\n        self.success_Cr = []\n        self.archive_rate = archive_rate\n        self.archive = []\n        self.lb = -5.0\n        self.ub = 5.0\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in self.population])\n        self.n_evals = self.pop_size\n\n        while self.n_evals < self.budget:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_1, x_2, x_3 = self.population[idxs]\n\n                # Incorporate archive\n                if len(self.archive) > 0 and np.random.rand() < self.archive_rate:\n                    x_arc = self.archive[np.random.randint(0, len(self.archive))]\n                    v = self.population[i] + self.F * (x_arc - x_3)\n                else:\n                    v = self.population[i] + self.F * (x_2 - x_3)\n                \n                v = np.clip(v, self.lb, self.ub)\n\n                # Crossover\n                j_rand = np.random.randint(self.dim)\n                u = np.array([v[j] if (np.random.rand() < self.Cr or j == j_rand) else self.population[i, j] for j in range(self.dim)])\n\n                # Selection\n                f_u = func(u)\n                self.n_evals += 1\n\n                if f_u < fitness[i]:\n                    # Store successful F and Cr\n                    self.success_F.append(self.F)\n                    self.success_Cr.append(self.Cr)\n                    \n                    # Add replaced individual to archive\n                    if len(self.archive) < self.pop_size:\n                        self.archive.append(self.population[i].copy())\n                    else:\n                        self.archive[np.random.randint(0, self.pop_size)] = self.population[i].copy()\n                    \n                    self.population[i] = u\n                    fitness[i] = f_u\n\n                    if f_u < self.f_opt:\n                        self.f_opt = f_u\n                        self.x_opt = u\n\n                if self.n_evals >= self.budget:\n                    break\n\n            # Adapt F and Cr (after each generation)\n            self.adapt_parameters()\n\n        return self.f_opt, self.x_opt\n\n    def adapt_parameters(self):\n        # Window-based averaging for F and Cr\n        if self.success_F:\n            self.F_memory[self.F_idx] = np.mean(self.success_F)\n            self.F_idx = (self.F_idx + 1) % self.memory_size\n            self.F = np.mean(self.F_memory)\n            self.F = np.clip(self.F, 0.1, 1.0)\n            self.success_F = []\n\n        if self.success_Cr:\n            self.Cr_memory[self.Cr_idx] = np.mean(self.success_Cr)\n            self.Cr_idx = (self.Cr_idx + 1) % self.memory_size\n            self.Cr = np.mean(self.Cr_memory)\n            self.Cr = np.clip(self.Cr, 0.1, 1.0)\n            self.success_Cr = []\n```", "operator": "crossover", "theta_sampled": 0.01937572054770598, "generation": 6}, "task_prompt": ""}
{"id": "b79b55b4-0bca-4566-8a02-5bd33b804c8a", "fitness": 0.44549177275912577, "name": "AdaptiveDEArchiveWindow", "description": "Adaptive Differential Evolution with archive and window-based parameter adaptation, prioritizing recent successes and exploration.", "code": "import numpy as np\n\nclass AdaptiveDEArchiveWindow:\n    def __init__(self, budget=10000, dim=10, pop_size=40, F_init=0.5, Cr_init=0.9, window_size=50, archive_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F_init\n        self.Cr = Cr_init\n        self.lb = -5.0\n        self.ub = 5.0\n        self.window_size = window_size\n        self.archive_rate = archive_rate\n        self.archive = []\n        self.success_F = []\n        self.success_Cr = []\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.pop = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.nevals = self.pop_size\n\n        while self.nevals < self.budget:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = self.pop[idxs]\n\n                # Incorporate archive\n                if len(self.archive) > 0 and np.random.rand() < self.archive_rate:\n                    x_arc = self.archive[np.random.randint(0, len(self.archive))]\n                    x_mutated = self.pop[i] + self.F * (x_arc - x_r3)\n                else:\n                    x_mutated = self.pop[i] + self.F * (x_r2 - x_r3)\n\n                x_mutated = np.clip(x_mutated, self.lb, self.ub)\n\n                # Crossover\n                x_trial = np.copy(self.pop[i])\n                crossover_points = np.random.rand(self.dim) < self.Cr\n                if not np.any(crossover_points):\n                    crossover_points[np.random.randint(0, self.dim)] = True\n                x_trial[crossover_points] = x_mutated[crossover_points]\n\n                # Evaluation\n                f_trial = func(x_trial)\n                self.nevals += 1\n\n                # Selection\n                if f_trial < self.fitness[i]:\n                    # Store successful F and Cr\n                    self.success_F.append(self.F)\n                    self.success_Cr.append(self.Cr)\n\n                    # Add replaced individual to archive\n                    if len(self.archive) < self.pop_size:\n                        self.archive.append(self.pop[i].copy())\n                    else:\n                        self.archive[np.random.randint(0, len(self.archive))] = self.pop[i].copy()\n\n                    self.fitness[i] = f_trial\n                    self.pop[i] = x_trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = x_trial\n\n            # Adapt F and Cr (after each generation)\n            self.adapt_parameters()\n\n        return self.f_opt, self.x_opt\n\n    def adapt_parameters(self):\n        if self.success_F:\n            self.F = np.mean(np.clip(self.success_F, 0.1, 1.0))\n            self.success_F = []\n\n        if self.success_Cr:\n            self.Cr = np.mean(np.clip(self.success_Cr, 0.1, 1.0))\n            self.success_Cr = []", "configspace": "", "generation": 6, "feedback": "The algorithm AdaptiveDEArchiveWindow scored 0.445 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["2fa45af8-e020-4e0b-9f07-43390b8ca505", "a5611c32-bed4-4ec0-9aa1-bf509a3dc71f"], "operator": "crossover", "metadata": {"llm_response": "# Description: Adaptive Differential Evolution with archive and window-based parameter adaptation, prioritizing recent successes and exploration.\n# Code: \n```python\nimport numpy as np\n\nclass AdaptiveDEArchiveWindow:\n    def __init__(self, budget=10000, dim=10, pop_size=40, F_init=0.5, Cr_init=0.9, window_size=50, archive_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F_init\n        self.Cr = Cr_init\n        self.lb = -5.0\n        self.ub = 5.0\n        self.window_size = window_size\n        self.archive_rate = archive_rate\n        self.archive = []\n        self.success_F = []\n        self.success_Cr = []\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.pop = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.nevals = self.pop_size\n\n        while self.nevals < self.budget:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = self.pop[idxs]\n\n                # Incorporate archive\n                if len(self.archive) > 0 and np.random.rand() < self.archive_rate:\n                    x_arc = self.archive[np.random.randint(0, len(self.archive))]\n                    x_mutated = self.pop[i] + self.F * (x_arc - x_r3)\n                else:\n                    x_mutated = self.pop[i] + self.F * (x_r2 - x_r3)\n\n                x_mutated = np.clip(x_mutated, self.lb, self.ub)\n\n                # Crossover\n                x_trial = np.copy(self.pop[i])\n                crossover_points = np.random.rand(self.dim) < self.Cr\n                if not np.any(crossover_points):\n                    crossover_points[np.random.randint(0, self.dim)] = True\n                x_trial[crossover_points] = x_mutated[crossover_points]\n\n                # Evaluation\n                f_trial = func(x_trial)\n                self.nevals += 1\n\n                # Selection\n                if f_trial < self.fitness[i]:\n                    # Store successful F and Cr\n                    self.success_F.append(self.F)\n                    self.success_Cr.append(self.Cr)\n\n                    # Add replaced individual to archive\n                    if len(self.archive) < self.pop_size:\n                        self.archive.append(self.pop[i].copy())\n                    else:\n                        self.archive[np.random.randint(0, len(self.archive))] = self.pop[i].copy()\n\n                    self.fitness[i] = f_trial\n                    self.pop[i] = x_trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = x_trial\n\n            # Adapt F and Cr (after each generation)\n            self.adapt_parameters()\n\n        return self.f_opt, self.x_opt\n\n    def adapt_parameters(self):\n        if self.success_F:\n            self.F = np.mean(np.clip(self.success_F, 0.1, 1.0))\n            self.success_F = []\n\n        if self.success_Cr:\n            self.Cr = np.mean(np.clip(self.success_Cr, 0.1, 1.0))\n            self.success_Cr = []\n```", "operator": "crossover", "theta_sampled": 0.023051981497476268, "generation": 6, "aucs": [0.166468895600288, 0.32026024321876845, 0.405879136037653, 0.6685690054070278, 0.33828982146258735, 0.5201644876286607, 0.31408506903205635, 0.3659673107272551, 0.36427051658414, 0.1965694927470676, 0.6715841413050554, 0.9905843009763, 0.42236788111147805, 0.32191162829139053, 0.798263859149711, 0.47793809036789503, 0.3485645716909035, 0.5494951204265428, 0.17786571817632246, 0.49073616524141184]}, "task_prompt": ""}
{"id": "a59cff4e-6fd7-4a83-8c32-672cedb278d9", "fitness": "-inf", "name": "AdaptiveDEWindowMemoryArchive", "description": "Adaptive DE with windowed memory for F and Cr, and an archive to promote diversity by sometimes replacing a random vector in mutation.", "code": "import numpy as np\n\nclass AdaptiveDEWindowMemoryArchive:\n    def __init__(self, budget=10000, dim=10, pop_size=40, F_init=0.5, Cr_init=0.9, window_size=50, memory_size=10, archive_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F_init\n        self.Cr = Cr_init\n        self.window_size = window_size\n        self.memory_size = memory_size\n        self.F_memory = np.full(self.memory_size, F_init)\n        self.Cr_memory = np.full(self.memory_size, Cr_init)\n        self.F_idx = 0\n        self.Cr_idx = 0\n        self.success_F = []\n        self.success_Cr = []\n        self.archive_rate = archive_rate\n        self.archive = []\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in self.population])\n        self.n_evals = self.pop_size\n\n        while self.n_evals < self.budget:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_1, x_2, x_3 = self.population[idxs]\n                \n                # Incorporate archive\n                if len(self.archive) > 0 and np.random.rand() < self.archive_rate:\n                    x_rand_arc = self.archive[np.random.randint(0, len(self.archive))]\n                    v = self.population[i] + self.F * (x_rand_arc - x_3)\n                else:\n                    v = self.population[i] + self.F * (x_2 - x_3)\n\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                j_rand = np.random.randint(self.dim)\n                u = np.array([v[j] if (np.random.rand() < self.Cr or j == j_rand) else self.population[i, j] for j in range(self.dim)])\n\n                # Selection\n                f_u = func(u)\n                self.n_evals += 1\n\n                if f_u < fitness[i]:\n                    # Store successful F and Cr\n                    self.success_F.append(self.F)\n                    self.success_Cr.append(self.Cr)\n\n                    # Add replaced individual to archive\n                    if len(self.archive) < self.pop_size:\n                        self.archive.append(self.population[i].copy())\n                    else:\n                        self.archive[np.random.randint(0, self.pop_size)] = self.population[i].copy()\n                    \n                    self.population[i] = u\n                    fitness[i] = f_u\n\n                    if f_u < self.f_opt:\n                        self.f_opt = f_u\n                        self.x_opt = u\n\n                if self.n_evals >= self.budget:\n                    break\n\n            # Adapt F and Cr (after each generation)\n            self.adapt_parameters()\n\n        return self.f_opt, self.x_opt\n\n    def adapt_parameters(self):\n        # Window-based averaging for F and Cr\n        if self.success_F:\n            self.F_memory[self.F_idx] = np.mean(self.success_F)\n            self.F_idx = (self.F_idx + 1) % self.memory_size\n            self.F = np.mean(self.F_memory)\n            self.F = np.clip(self.F, 0.1, 1.0)\n            self.success_F = []\n\n        if self.success_Cr:\n            self.Cr_memory[self.Cr_idx] = np.mean(self.success_Cr)\n            self.Cr_idx = (self.Cr_idx + 1) % self.memory_size\n            self.Cr = np.mean(self.Cr_memory)\n            self.Cr = np.clip(self.Cr, 0.1, 1.0)\n            self.success_Cr = []", "configspace": "", "generation": 6, "feedback": "Evaluation timed out after 300 seconds.", "error": "Evaluation timed out after 300 seconds.", "parent_ids": ["2fa45af8-e020-4e0b-9f07-43390b8ca505", "a5611c32-bed4-4ec0-9aa1-bf509a3dc71f"], "operator": "crossover", "metadata": {"llm_response": "# Description: Adaptive DE with windowed memory for F and Cr, and an archive to promote diversity by sometimes replacing a random vector in mutation.\n# Code: \n```python\nimport numpy as np\n\nclass AdaptiveDEWindowMemoryArchive:\n    def __init__(self, budget=10000, dim=10, pop_size=40, F_init=0.5, Cr_init=0.9, window_size=50, memory_size=10, archive_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F_init\n        self.Cr = Cr_init\n        self.window_size = window_size\n        self.memory_size = memory_size\n        self.F_memory = np.full(self.memory_size, F_init)\n        self.Cr_memory = np.full(self.memory_size, Cr_init)\n        self.F_idx = 0\n        self.Cr_idx = 0\n        self.success_F = []\n        self.success_Cr = []\n        self.archive_rate = archive_rate\n        self.archive = []\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in self.population])\n        self.n_evals = self.pop_size\n\n        while self.n_evals < self.budget:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_1, x_2, x_3 = self.population[idxs]\n                \n                # Incorporate archive\n                if len(self.archive) > 0 and np.random.rand() < self.archive_rate:\n                    x_rand_arc = self.archive[np.random.randint(0, len(self.archive))]\n                    v = self.population[i] + self.F * (x_rand_arc - x_3)\n                else:\n                    v = self.population[i] + self.F * (x_2 - x_3)\n\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                j_rand = np.random.randint(self.dim)\n                u = np.array([v[j] if (np.random.rand() < self.Cr or j == j_rand) else self.population[i, j] for j in range(self.dim)])\n\n                # Selection\n                f_u = func(u)\n                self.n_evals += 1\n\n                if f_u < fitness[i]:\n                    # Store successful F and Cr\n                    self.success_F.append(self.F)\n                    self.success_Cr.append(self.Cr)\n\n                    # Add replaced individual to archive\n                    if len(self.archive) < self.pop_size:\n                        self.archive.append(self.population[i].copy())\n                    else:\n                        self.archive[np.random.randint(0, self.pop_size)] = self.population[i].copy()\n                    \n                    self.population[i] = u\n                    fitness[i] = f_u\n\n                    if f_u < self.f_opt:\n                        self.f_opt = f_u\n                        self.x_opt = u\n\n                if self.n_evals >= self.budget:\n                    break\n\n            # Adapt F and Cr (after each generation)\n            self.adapt_parameters()\n\n        return self.f_opt, self.x_opt\n\n    def adapt_parameters(self):\n        # Window-based averaging for F and Cr\n        if self.success_F:\n            self.F_memory[self.F_idx] = np.mean(self.success_F)\n            self.F_idx = (self.F_idx + 1) % self.memory_size\n            self.F = np.mean(self.F_memory)\n            self.F = np.clip(self.F, 0.1, 1.0)\n            self.success_F = []\n\n        if self.success_Cr:\n            self.Cr_memory[self.Cr_idx] = np.mean(self.success_Cr)\n            self.Cr_idx = (self.Cr_idx + 1) % self.memory_size\n            self.Cr = np.mean(self.Cr_memory)\n            self.Cr = np.clip(self.Cr, 0.1, 1.0)\n            self.success_Cr = []\n```", "operator": "crossover", "theta_sampled": 0.0195480752930277, "generation": 6}, "task_prompt": ""}
{"id": "31d5b322-9fb4-4946-aa24-272dd4c6f7e3", "fitness": 0.17335203102109153, "name": "CMAES_DynamicPop", "description": "Covariance Matrix Adaptation Evolution Strategy with a dynamically adjusted population size based on success rate and a simplified rank-one update.", "code": "import numpy as np\n\nclass CMAES_DynamicPop:\n    def __init__(self, budget=10000, dim=10, initial_pop_size=None):\n        self.budget = budget\n        self.dim = dim\n        self.mu = dim // 2 if initial_pop_size is None else initial_pop_size  # Initial population size\n        self.lambda_ = 4 + int(3 * np.log(self.dim)) # Population size\n        self.C = np.eye(self.dim)  # Covariance matrix\n        self.m = np.zeros(self.dim)  # Mean\n        self.sigma = 0.3  # Step size\n        self.c_sigma = (self.mu + 2) / (self.dim + self.mu + 5)\n        self.d_sigma = 1 + 2 * max(0, np.sqrt((self.mu - 1) / (self.dim + 1)) - 1) + self.c_sigma\n        self.c_rank_one = 2 / ((self.dim + 1.3)**2 + self.mu)\n        self.success_rate = 0.5  # Initialize success rate\n        self.learning_rate = 0.2 # Learning rate for population size update\n        self.min_pop = 4\n        self.max_pop = 4 * dim\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        evals = 0\n        while evals < self.budget:\n            # Generate population\n            z = np.random.randn(self.dim, self.lambda_)\n            x = self.m[:, np.newaxis] + self.sigma * np.dot(np.linalg.cholesky(self.C), z)\n\n            # Evaluate population, handling bound constraints\n            f = np.array([func(np.clip(x[:, i], func.bounds.lb, func.bounds.ub)) for i in range(self.lambda_)])\n            evals += self.lambda_\n            if evals > self.budget:\n                self.lambda_ = self.lambda_ - (evals - self.budget)\n                f = f[:self.lambda_]\n\n            # Sort population\n            idx = np.argsort(f)\n            x = x[:, idx]\n            f = f[idx]\n\n            # Update optimal solution\n            if f[0] < self.f_opt:\n                self.f_opt = f[0]\n                self.x_opt = x[:, 0]\n\n            # Update mean\n            weights = np.ones(self.mu) / self.mu\n            delta_m = np.sum(x[:, :self.mu] * weights[np.newaxis, :], axis=1) - self.m\n            self.m = self.m + delta_m\n            \n            # Simplified Rank-one update\n            z_mean = np.mean(z[:, idx[:self.mu]], axis=1)\n            self.C = (1 - self.c_rank_one) * self.C + self.c_rank_one * np.outer(z_mean, z_mean)\n            \n            # Ensure C remains positive definite\n            try:\n                np.linalg.cholesky(self.C)\n            except np.linalg.LinAlgError:\n                self.C = self.C + 1e-6 * np.eye(self.dim)  # Add a small multiple of the identity matrix\n\n\n            # Update step size\n            self.sigma = self.sigma * np.exp(self.c_sigma / self.d_sigma * (np.linalg.norm(z_mean) / np.sqrt(self.dim) - 1))\n\n            # Dynamic population size adjustment\n            success = np.mean(f[:-1] > f[1:])  # approximate success rate\n            self.success_rate = 0.9 * self.success_rate + 0.1 * success # Smoothed success rate\n            if self.success_rate > 0.7:\n                self.lambda_ = max(self.min_pop, int(self.lambda_ * (1 - self.learning_rate)))\n            elif self.success_rate < 0.3:\n                self.lambda_ = min(self.max_pop, int(self.lambda_ * (1 + self.learning_rate)))\n            self.lambda_ = max(self.min_pop, min(self.max_pop, self.lambda_))\n\n            self.mu = min(self.lambda_ // 2, self.dim) #Adjust mu based on lambda\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "The algorithm CMAES_DynamicPop scored 0.173 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": "random_new", "metadata": {"llm_response": "# Description: Covariance Matrix Adaptation Evolution Strategy with a dynamically adjusted population size based on success rate and a simplified rank-one update.\n# Code: \n```python\nimport numpy as np\n\nclass CMAES_DynamicPop:\n    def __init__(self, budget=10000, dim=10, initial_pop_size=None):\n        self.budget = budget\n        self.dim = dim\n        self.mu = dim // 2 if initial_pop_size is None else initial_pop_size  # Initial population size\n        self.lambda_ = 4 + int(3 * np.log(self.dim)) # Population size\n        self.C = np.eye(self.dim)  # Covariance matrix\n        self.m = np.zeros(self.dim)  # Mean\n        self.sigma = 0.3  # Step size\n        self.c_sigma = (self.mu + 2) / (self.dim + self.mu + 5)\n        self.d_sigma = 1 + 2 * max(0, np.sqrt((self.mu - 1) / (self.dim + 1)) - 1) + self.c_sigma\n        self.c_rank_one = 2 / ((self.dim + 1.3)**2 + self.mu)\n        self.success_rate = 0.5  # Initialize success rate\n        self.learning_rate = 0.2 # Learning rate for population size update\n        self.min_pop = 4\n        self.max_pop = 4 * dim\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        evals = 0\n        while evals < self.budget:\n            # Generate population\n            z = np.random.randn(self.dim, self.lambda_)\n            x = self.m[:, np.newaxis] + self.sigma * np.dot(np.linalg.cholesky(self.C), z)\n\n            # Evaluate population, handling bound constraints\n            f = np.array([func(np.clip(x[:, i], func.bounds.lb, func.bounds.ub)) for i in range(self.lambda_)])\n            evals += self.lambda_\n            if evals > self.budget:\n                self.lambda_ = self.lambda_ - (evals - self.budget)\n                f = f[:self.lambda_]\n\n            # Sort population\n            idx = np.argsort(f)\n            x = x[:, idx]\n            f = f[idx]\n\n            # Update optimal solution\n            if f[0] < self.f_opt:\n                self.f_opt = f[0]\n                self.x_opt = x[:, 0]\n\n            # Update mean\n            weights = np.ones(self.mu) / self.mu\n            delta_m = np.sum(x[:, :self.mu] * weights[np.newaxis, :], axis=1) - self.m\n            self.m = self.m + delta_m\n            \n            # Simplified Rank-one update\n            z_mean = np.mean(z[:, idx[:self.mu]], axis=1)\n            self.C = (1 - self.c_rank_one) * self.C + self.c_rank_one * np.outer(z_mean, z_mean)\n            \n            # Ensure C remains positive definite\n            try:\n                np.linalg.cholesky(self.C)\n            except np.linalg.LinAlgError:\n                self.C = self.C + 1e-6 * np.eye(self.dim)  # Add a small multiple of the identity matrix\n\n\n            # Update step size\n            self.sigma = self.sigma * np.exp(self.c_sigma / self.d_sigma * (np.linalg.norm(z_mean) / np.sqrt(self.dim) - 1))\n\n            # Dynamic population size adjustment\n            success = np.mean(f[:-1] > f[1:])  # approximate success rate\n            self.success_rate = 0.9 * self.success_rate + 0.1 * success # Smoothed success rate\n            if self.success_rate > 0.7:\n                self.lambda_ = max(self.min_pop, int(self.lambda_ * (1 - self.learning_rate)))\n            elif self.success_rate < 0.3:\n                self.lambda_ = min(self.max_pop, int(self.lambda_ * (1 + self.learning_rate)))\n            self.lambda_ = max(self.min_pop, min(self.max_pop, self.lambda_))\n\n            self.mu = min(self.lambda_ // 2, self.dim) #Adjust mu based on lambda\n        return self.f_opt, self.x_opt\n```", "operator": "random_new", "theta_sampled": -0.0017126057520784864, "generation": 7, "aucs": [0.07539094264257873, 0.18142312492822787, 0.3056188623854409, 0.10448821786804985, 0.11492620316515278, 0.15581525691518527, 0.17830664673105334, 0.16277210601013736, 0.16372103761720713, 0.15745312168686598, 0.16704325381035234, 0.2152687819823711, 0.24831506455487828, 0.12545940287934465, 0.14018965130228478, 0.25848302340079, 0.21097065322230646, 0.18811293523575456, 0.15768140767263017, 0.15560092641121903]}, "task_prompt": ""}
{"id": "1ac0706b-929f-49ef-9eb1-3a9f2423ec93", "fitness": 0.4131812498263767, "name": "WeightedAdaptiveDE", "description": "Adaptive DE with a windowed approach for F and Cr adaptation, prioritizing recent successful parameter values by weighting them more heavily in the average.", "code": "import numpy as np\n\nclass WeightedAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=40, F_init=0.5, Cr_init=0.9, window_size=50):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F_init\n        self.Cr = Cr_init\n        self.window_size = window_size\n        self.F_history = []\n        self.Cr_history = []\n        self.success_history = []  # 1 for success, 0 for failure\n        self.weights = np.linspace(0.1, 1.0, window_size)  # Linear weights, recent values have more influence\n        self.weights /= np.sum(self.weights) # Normalize\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in self.population])\n        self.n_evals = self.pop_size\n\n        while self.n_evals < self.budget:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_1, x_2, x_3 = self.population[idxs]\n                v = self.population[i] + self.F * (x_2 - x_3)\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                j_rand = np.random.randint(self.dim)\n                u = np.array([v[j] if (np.random.rand() < self.Cr or j == j_rand) else self.population[i, j] for j in range(self.dim)])\n\n                # Selection\n                f_u = func(u)\n                self.n_evals += 1\n\n                if f_u < fitness[i]:\n                    self.population[i] = u\n                    fitness[i] = f_u\n                    self.F_history.append(self.F)\n                    self.Cr_history.append(self.Cr)\n                    self.success_history.append(1)  # Success\n                    if f_u < self.f_opt:\n                        self.f_opt = f_u\n                        self.x_opt = u\n                else:\n                    self.success_history.append(0) # Failure\n\n                if self.n_evals >= self.budget:\n                    break\n\n            # Adapt F and Cr (after each generation)\n            self.adapt_parameters()\n\n        return self.f_opt, self.x_opt\n\n    def adapt_parameters(self):\n        # Weighted window-based averaging for F and Cr\n        if len(self.F_history) > 0:\n            windowed_F = self.F_history[-self.window_size:]\n            windowed_Cr = self.Cr_history[-self.window_size:]\n            windowed_success = self.success_history[-self.window_size:]\n            \n            if len(windowed_F) > 0:\n                weights_to_use = self.weights[-len(windowed_F):]\n                self.F = np.clip(np.sum(np.array(windowed_F) * weights_to_use), 0.1, 1.0)\n            if len(windowed_Cr) > 0:\n                weights_to_use = self.weights[-len(windowed_Cr):]\n                self.Cr = np.clip(np.sum(np.array(windowed_Cr) * weights_to_use), 0.1, 1.0)", "configspace": "", "generation": 7, "feedback": "The algorithm WeightedAdaptiveDE scored 0.413 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["2fa45af8-e020-4e0b-9f07-43390b8ca505", "40bd89f0-32fe-4e6f-8887-5e032130b8e1"], "operator": "crossover", "metadata": {"llm_response": "# Description: Adaptive DE with a windowed approach for F and Cr adaptation, prioritizing recent successful parameter values by weighting them more heavily in the average.\n# Code: \n```python\nimport numpy as np\n\nclass WeightedAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=40, F_init=0.5, Cr_init=0.9, window_size=50):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F_init\n        self.Cr = Cr_init\n        self.window_size = window_size\n        self.F_history = []\n        self.Cr_history = []\n        self.success_history = []  # 1 for success, 0 for failure\n        self.weights = np.linspace(0.1, 1.0, window_size)  # Linear weights, recent values have more influence\n        self.weights /= np.sum(self.weights) # Normalize\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in self.population])\n        self.n_evals = self.pop_size\n\n        while self.n_evals < self.budget:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_1, x_2, x_3 = self.population[idxs]\n                v = self.population[i] + self.F * (x_2 - x_3)\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                j_rand = np.random.randint(self.dim)\n                u = np.array([v[j] if (np.random.rand() < self.Cr or j == j_rand) else self.population[i, j] for j in range(self.dim)])\n\n                # Selection\n                f_u = func(u)\n                self.n_evals += 1\n\n                if f_u < fitness[i]:\n                    self.population[i] = u\n                    fitness[i] = f_u\n                    self.F_history.append(self.F)\n                    self.Cr_history.append(self.Cr)\n                    self.success_history.append(1)  # Success\n                    if f_u < self.f_opt:\n                        self.f_opt = f_u\n                        self.x_opt = u\n                else:\n                    self.success_history.append(0) # Failure\n\n                if self.n_evals >= self.budget:\n                    break\n\n            # Adapt F and Cr (after each generation)\n            self.adapt_parameters()\n\n        return self.f_opt, self.x_opt\n\n    def adapt_parameters(self):\n        # Weighted window-based averaging for F and Cr\n        if len(self.F_history) > 0:\n            windowed_F = self.F_history[-self.window_size:]\n            windowed_Cr = self.Cr_history[-self.window_size:]\n            windowed_success = self.success_history[-self.window_size:]\n            \n            if len(windowed_F) > 0:\n                weights_to_use = self.weights[-len(windowed_F):]\n                self.F = np.clip(np.sum(np.array(windowed_F) * weights_to_use), 0.1, 1.0)\n            if len(windowed_Cr) > 0:\n                weights_to_use = self.weights[-len(windowed_Cr):]\n                self.Cr = np.clip(np.sum(np.array(windowed_Cr) * weights_to_use), 0.1, 1.0)\n```", "operator": "crossover", "theta_sampled": 0.004870097389590939, "generation": 7, "aucs": [0.14245161595199463, 0.26681856627818223, 0.4165053418078557, 0.5268795059530358, 0.31105430322614913, 0.4474807125542418, 0.3039894984971717, 0.35701080396300267, 0.3103625798673415, 0.17186646872228628, 0.6125314310315371, 0.9983765396929193, 0.32280840217720164, 0.2903246952324883, 0.7600022496205683, 0.4406951738170275, 0.34755041055297053, 0.5558028375974777, 0.18834881651381918, 0.49276504347026384]}, "task_prompt": ""}
{"id": "e9d12a74-d7eb-414d-af36-a79ce4db3124", "fitness": 0.374649137963097, "name": "AdaptiveDESuccessRate", "description": "Adaptive Differential Evolution with a success-rate based adaptation of F and Cr.", "code": "import numpy as np\n\nclass AdaptiveDESuccessRate:\n    def __init__(self, budget=10000, dim=10, pop_size=40, F_init=0.5, Cr_init=0.9, success_rate_window=50):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F_init\n        self.Cr = Cr_init\n        self.success_rate_window = success_rate_window\n        self.success_history = []\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in self.population])\n        self.n_evals = self.pop_size\n\n        while self.n_evals < self.budget:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_1, x_2, x_3 = self.population[idxs]\n                v = self.population[i] + self.F * (x_2 - x_3)\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                j_rand = np.random.randint(self.dim)\n                u = np.array([v[j] if (np.random.rand() < self.Cr or j == j_rand) else self.population[i, j] for j in range(self.dim)])\n\n                # Selection\n                f_u = func(u)\n                self.n_evals += 1\n\n                if f_u < fitness[i]:\n                    self.population[i] = u\n                    fitness[i] = f_u\n                    self.success_history.append(1)\n\n                    if f_u < self.f_opt:\n                        self.f_opt = f_u\n                        self.x_opt = u\n                else:\n                    self.success_history.append(0)\n\n                if self.n_evals >= self.budget:\n                    break\n\n            # Adapt F and Cr based on success rate\n            self.adapt_parameters()\n\n        return self.f_opt, self.x_opt\n\n    def adapt_parameters(self):\n        # Calculate success rate over the window\n        if len(self.success_history) > self.success_rate_window:\n            recent_successes = self.success_history[-self.success_rate_window:]\n            success_rate = np.mean(recent_successes)\n\n            # Adjust F and Cr based on success rate\n            if success_rate > 0.5:\n                self.F *= 0.95  # Reduce F if success rate is high\n                self.Cr *= 1.05 # Increase Cr if success rate is high\n            else:\n                self.F *= 1.05  # Increase F if success rate is low\n                self.Cr *= 0.95 # Decrease Cr if success rate is low\n\n            self.F = np.clip(self.F, 0.1, 1.0)\n            self.Cr = np.clip(self.Cr, 0.1, 1.0)", "configspace": "", "generation": 7, "feedback": "The algorithm AdaptiveDESuccessRate scored 0.375 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["2fa45af8-e020-4e0b-9f07-43390b8ca505", "40bd89f0-32fe-4e6f-8887-5e032130b8e1"], "operator": "crossover", "metadata": {"llm_response": "# Description: Adaptive Differential Evolution with a success-rate based adaptation of F and Cr.\n# Code: \n```python\nimport numpy as np\n\nclass AdaptiveDESuccessRate:\n    def __init__(self, budget=10000, dim=10, pop_size=40, F_init=0.5, Cr_init=0.9, success_rate_window=50):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F_init\n        self.Cr = Cr_init\n        self.success_rate_window = success_rate_window\n        self.success_history = []\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in self.population])\n        self.n_evals = self.pop_size\n\n        while self.n_evals < self.budget:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_1, x_2, x_3 = self.population[idxs]\n                v = self.population[i] + self.F * (x_2 - x_3)\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                j_rand = np.random.randint(self.dim)\n                u = np.array([v[j] if (np.random.rand() < self.Cr or j == j_rand) else self.population[i, j] for j in range(self.dim)])\n\n                # Selection\n                f_u = func(u)\n                self.n_evals += 1\n\n                if f_u < fitness[i]:\n                    self.population[i] = u\n                    fitness[i] = f_u\n                    self.success_history.append(1)\n\n                    if f_u < self.f_opt:\n                        self.f_opt = f_u\n                        self.x_opt = u\n                else:\n                    self.success_history.append(0)\n\n                if self.n_evals >= self.budget:\n                    break\n\n            # Adapt F and Cr based on success rate\n            self.adapt_parameters()\n\n        return self.f_opt, self.x_opt\n\n    def adapt_parameters(self):\n        # Calculate success rate over the window\n        if len(self.success_history) > self.success_rate_window:\n            recent_successes = self.success_history[-self.success_rate_window:]\n            success_rate = np.mean(recent_successes)\n\n            # Adjust F and Cr based on success rate\n            if success_rate > 0.5:\n                self.F *= 0.95  # Reduce F if success rate is high\n                self.Cr *= 1.05 # Increase Cr if success rate is high\n            else:\n                self.F *= 1.05  # Increase F if success rate is low\n                self.Cr *= 0.95 # Decrease Cr if success rate is low\n\n            self.F = np.clip(self.F, 0.1, 1.0)\n            self.Cr = np.clip(self.Cr, 0.1, 1.0)\n```", "operator": "crossover", "theta_sampled": 0.01499343527239144, "generation": 7, "aucs": [0.1484736746741422, 0.19810044825719864, 0.3686390703175487, 0.2982725898962646, 0.3059918888189085, 0.4283918425235992, 0.3119951988198424, 0.34739813672639697, 0.3292444187344207, 0.18765954505156668, 0.32970711920417795, 0.9831709285803485, 0.29096929906669844, 0.2821992711586596, 0.7679847028137693, 0.4479347508118888, 0.3082325032003541, 0.4790096313387201, 0.19402977952780887, 0.4855779597396256]}, "task_prompt": ""}
{"id": "50da3b0c-3976-4868-b42f-20dbc4959d29", "fitness": 0.4036190790491114, "name": "AdaptiveDESmoothRestart", "description": "Adaptive Differential Evolution with parameter smoothing and population restart to escape local optima.", "code": "import numpy as np\n\nclass AdaptiveDESmoothRestart:\n    def __init__(self, budget=10000, dim=10, pop_size=40, F=0.5, Cr=0.9, smoothing_window=20, restart_trigger=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.Cr = Cr\n        self.smoothing_window = smoothing_window\n        self.restart_trigger = restart_trigger\n        self.F_history = []\n        self.Cr_history = []\n        self.fitness_history = []\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in self.population])\n        self.n_evals = self.pop_size\n        self.fitness_history.append(np.mean(fitness))\n\n        while self.n_evals < self.budget:\n            for i in range(self.pop_size):\n                # Parameter adaptation (smoothed)\n                if len(self.F_history) > self.smoothing_window:\n                    self.F = np.mean(self.F_history[-self.smoothing_window:])\n                    self.Cr = np.mean(self.Cr_history[-self.smoothing_window:])\n                self.F = np.clip(self.F + np.random.normal(0, 0.1), 0.1, 1.0)\n                self.Cr = np.clip(self.Cr + np.random.normal(0, 0.1), 0.1, 1.0)\n                \n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_1, x_2, x_3 = self.population[idxs]\n                v = self.population[i] + self.F * (x_2 - x_3)\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n                \n                # Crossover\n                j_rand = np.random.randint(self.dim)\n                u = np.array([v[j] if (np.random.rand() < self.Cr or j == j_rand) else self.population[i, j] for j in range(self.dim)])\n\n                # Selection\n                f_u = func(u)\n                self.n_evals += 1\n                \n                if f_u < fitness[i]:\n                    self.population[i] = u\n                    fitness[i] = f_u\n                    self.F_history.append(self.F)\n                    self.Cr_history.append(self.Cr)\n\n                    if f_u < self.f_opt:\n                        self.f_opt = f_u\n                        self.x_opt = u\n\n                if self.n_evals >= self.budget:\n                    break\n            \n            # Population Restart Condition (Check if fitness stagnates)\n            self.fitness_history.append(np.mean(fitness))\n            if len(self.fitness_history) > self.smoothing_window:\n                recent_fitness = self.fitness_history[-self.smoothing_window:]\n                fitness_change = np.abs(recent_fitness[-1] - recent_fitness[0]) / np.abs(recent_fitness[0]) if recent_fitness[0] != 0 else 0\n                if fitness_change < self.restart_trigger:\n                    self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                    fitness = np.array([func(x) for x in self.population])\n                    self.n_evals += self.pop_size\n                    self.fitness_history = [np.mean(fitness)] # Reset fitness history\n                    \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "The algorithm AdaptiveDESmoothRestart scored 0.404 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["40bd89f0-32fe-4e6f-8887-5e032130b8e1"], "operator": "simplify", "metadata": {"llm_response": "# Description: Adaptive Differential Evolution with parameter smoothing and population restart to escape local optima.\n# Code: \n```python\nimport numpy as np\n\nclass AdaptiveDESmoothRestart:\n    def __init__(self, budget=10000, dim=10, pop_size=40, F=0.5, Cr=0.9, smoothing_window=20, restart_trigger=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.Cr = Cr\n        self.smoothing_window = smoothing_window\n        self.restart_trigger = restart_trigger\n        self.F_history = []\n        self.Cr_history = []\n        self.fitness_history = []\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in self.population])\n        self.n_evals = self.pop_size\n        self.fitness_history.append(np.mean(fitness))\n\n        while self.n_evals < self.budget:\n            for i in range(self.pop_size):\n                # Parameter adaptation (smoothed)\n                if len(self.F_history) > self.smoothing_window:\n                    self.F = np.mean(self.F_history[-self.smoothing_window:])\n                    self.Cr = np.mean(self.Cr_history[-self.smoothing_window:])\n                self.F = np.clip(self.F + np.random.normal(0, 0.1), 0.1, 1.0)\n                self.Cr = np.clip(self.Cr + np.random.normal(0, 0.1), 0.1, 1.0)\n                \n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_1, x_2, x_3 = self.population[idxs]\n                v = self.population[i] + self.F * (x_2 - x_3)\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n                \n                # Crossover\n                j_rand = np.random.randint(self.dim)\n                u = np.array([v[j] if (np.random.rand() < self.Cr or j == j_rand) else self.population[i, j] for j in range(self.dim)])\n\n                # Selection\n                f_u = func(u)\n                self.n_evals += 1\n                \n                if f_u < fitness[i]:\n                    self.population[i] = u\n                    fitness[i] = f_u\n                    self.F_history.append(self.F)\n                    self.Cr_history.append(self.Cr)\n\n                    if f_u < self.f_opt:\n                        self.f_opt = f_u\n                        self.x_opt = u\n\n                if self.n_evals >= self.budget:\n                    break\n            \n            # Population Restart Condition (Check if fitness stagnates)\n            self.fitness_history.append(np.mean(fitness))\n            if len(self.fitness_history) > self.smoothing_window:\n                recent_fitness = self.fitness_history[-self.smoothing_window:]\n                fitness_change = np.abs(recent_fitness[-1] - recent_fitness[0]) / np.abs(recent_fitness[0]) if recent_fitness[0] != 0 else 0\n                if fitness_change < self.restart_trigger:\n                    self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                    fitness = np.array([func(x) for x in self.population])\n                    self.n_evals += self.pop_size\n                    self.fitness_history = [np.mean(fitness)] # Reset fitness history\n                    \n        return self.f_opt, self.x_opt\n```", "operator": "simplify", "theta_sampled": 0.004408969362981154, "generation": 7, "aucs": [0.14416274083107405, 0.36200959887063566, 0.43591337054963464, 0.4838312039527288, 0.2736836524353885, 0.4209899604992703, 0.2833039623865722, 0.33697766372509896, 0.2782366190084433, 0.2181358363829301, 0.4044576168885925, 0.9993358436046059, 0.35270351688840995, 0.33674016186179545, 0.7136768827026954, 0.4877429386113479, 0.31652504442885676, 0.531951005747119, 0.1974698641226118, 0.4945340974844169]}, "task_prompt": ""}
{"id": "f9cc8de2-dec8-445d-ae60-ed9c73bb3993", "fitness": 0.0, "name": "AdaptiveCoordinateDescent", "description": "An Adaptive Coordinate Descent method that focuses on promising dimensions by tracking improvement rates and adjusting step sizes.", "code": "import numpy as np\n\nclass AdaptiveCoordinateDescent:\n    def __init__(self, budget=10000, dim=10, initial_step_size=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.initial_step_size = initial_step_size\n        self.step_sizes = np.full(dim, initial_step_size)\n        self.improvement_rates = np.zeros(dim)  # Track improvement rate for each dimension\n        self.x_opt = None\n        self.f_opt = np.inf\n        self.eval_count = 0\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        \n        # Initialize solution randomly\n        if self.x_opt is None:\n            self.x_opt = np.random.uniform(lb, ub, self.dim)\n            self.f_opt = func(self.x_opt)\n            self.eval_count += 1\n        \n        while self.eval_count < self.budget:\n            # Choose a dimension to optimize\n            dimension = np.argmax(self.improvement_rates)  # Focus on the dimension with highest improvement rate\n\n            # Coordinate descent along selected dimension\n            step = self.step_sizes[dimension]\n            \n            # Try positive step\n            x_new = self.x_opt.copy()\n            x_new[dimension] = np.clip(x_new[dimension] + step, lb[dimension], ub[dimension])\n            f_new = func(x_new)\n            self.eval_count += 1\n\n            if f_new < self.f_opt:\n                self.f_opt = f_new\n                self.x_opt = x_new\n                self.improvement_rates[dimension] += 0.1 # Boost improvement rate\n            else:\n                # Try negative step if positive step failed.\n                x_new = self.x_opt.copy()\n                x_new[dimension] = np.clip(x_new[dimension] - step, lb[dimension], ub[dimension])\n                f_new = func(x_new)\n                self.eval_count += 1\n                \n                if f_new < self.f_opt:\n                    self.f_opt = f_new\n                    self.x_opt = x_new\n                    self.improvement_rates[dimension] += 0.1 # Boost improvement rate\n                else:\n                    self.improvement_rates[dimension] -= 0.05 # Reduce improvement rate if no improvement\n                    self.step_sizes[dimension] *= 0.9  # Reduce step size if no improvement\n\n            # Reduce all improvement rates slightly\n            self.improvement_rates *= 0.98\n            \n            if self.eval_count >= self.budget:\n                break\n        \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "The algorithm AdaptiveCoordinateDescent scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": "random_new", "metadata": {"llm_response": "# Description: An Adaptive Coordinate Descent method that focuses on promising dimensions by tracking improvement rates and adjusting step sizes.\n# Code: \n```python\nimport numpy as np\n\nclass AdaptiveCoordinateDescent:\n    def __init__(self, budget=10000, dim=10, initial_step_size=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.initial_step_size = initial_step_size\n        self.step_sizes = np.full(dim, initial_step_size)\n        self.improvement_rates = np.zeros(dim)  # Track improvement rate for each dimension\n        self.x_opt = None\n        self.f_opt = np.inf\n        self.eval_count = 0\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        \n        # Initialize solution randomly\n        if self.x_opt is None:\n            self.x_opt = np.random.uniform(lb, ub, self.dim)\n            self.f_opt = func(self.x_opt)\n            self.eval_count += 1\n        \n        while self.eval_count < self.budget:\n            # Choose a dimension to optimize\n            dimension = np.argmax(self.improvement_rates)  # Focus on the dimension with highest improvement rate\n\n            # Coordinate descent along selected dimension\n            step = self.step_sizes[dimension]\n            \n            # Try positive step\n            x_new = self.x_opt.copy()\n            x_new[dimension] = np.clip(x_new[dimension] + step, lb[dimension], ub[dimension])\n            f_new = func(x_new)\n            self.eval_count += 1\n\n            if f_new < self.f_opt:\n                self.f_opt = f_new\n                self.x_opt = x_new\n                self.improvement_rates[dimension] += 0.1 # Boost improvement rate\n            else:\n                # Try negative step if positive step failed.\n                x_new = self.x_opt.copy()\n                x_new[dimension] = np.clip(x_new[dimension] - step, lb[dimension], ub[dimension])\n                f_new = func(x_new)\n                self.eval_count += 1\n                \n                if f_new < self.f_opt:\n                    self.f_opt = f_new\n                    self.x_opt = x_new\n                    self.improvement_rates[dimension] += 0.1 # Boost improvement rate\n                else:\n                    self.improvement_rates[dimension] -= 0.05 # Reduce improvement rate if no improvement\n                    self.step_sizes[dimension] *= 0.9  # Reduce step size if no improvement\n\n            # Reduce all improvement rates slightly\n            self.improvement_rates *= 0.98\n            \n            if self.eval_count >= self.budget:\n                break\n        \n        return self.f_opt, self.x_opt\n```", "operator": "random_new", "theta_sampled": 0.005330408036190629, "generation": 7, "aucs": [0]}, "task_prompt": ""}
{"id": "e3fb1403-4620-48a7-8209-b6d1ad71f1b8", "fitness": 0.4565818846275639, "name": "AdaptiveDEWindowSimple", "description": "Adaptive Differential Evolution with weighted historical parameter adaptation and simplified parameter update rules.", "code": "import numpy as np\n\nclass AdaptiveDEWindowSimple:\n    def __init__(self, budget=10000, dim=10, pop_size=40, F=0.5, Cr=0.9, window_size=50):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F  # Initial scaling factor\n        self.Cr = Cr  # Initial crossover rate\n        self.window_size = window_size\n        self.F_history = []\n        self.Cr_history = []\n        self.archive_rate = 0.1\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in self.population])\n        self.n_evals = self.pop_size\n        \n        weights_F = np.zeros(self.window_size) + 1e-6 # Avoid division by zero\n        weights_Cr = np.zeros(self.window_size) + 1e-6 # Avoid division by zero\n        \n        while self.n_evals < self.budget:\n            for i in range(self.pop_size):\n                # Parameter adaptation based on history\n                if len(self.F_history) > 0:\n                    recent_F = np.array(self.F_history[-self.window_size:])\n                    recent_Cr = np.array(self.Cr_history[-self.window_size:])\n                    recent_F_weights = weights_F[:len(recent_F)] / np.sum(weights_F[:len(recent_F)]) if len(recent_F) > 0 else np.array([1.0])\n                    recent_Cr_weights = weights_Cr[:len(recent_Cr)] / np.sum(weights_Cr[:len(recent_Cr)]) if len(recent_Cr) > 0 else np.array([1.0])\n\n                    self.F = np.clip(np.sum(recent_F * recent_F_weights), 0.1, 1.0)\n                    self.Cr = np.clip(np.sum(recent_Cr * recent_Cr_weights), 0.1, 1.0)\n                \n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_1, x_2, x_3 = self.population[idxs]\n                v = self.population[i] + self.F * (x_2 - x_3)\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n                \n                # Crossover\n                j_rand = np.random.randint(self.dim)\n                u = np.array([v[j] if (np.random.rand() < self.Cr or j == j_rand) else self.population[i, j] for j in range(self.dim)])\n\n                # Selection\n                f_u = func(u)\n                self.n_evals += 1\n                \n                if f_u < fitness[i]:\n                    self.population[i] = u\n                    fitness[i] = f_u\n                    self.F_history.append(self.F)\n                    self.Cr_history.append(self.Cr)\n                    if len(self.F_history) > self.window_size:\n                         self.F_history.pop(0)\n                         self.Cr_history.pop(0)\n                    \n                    weights_F = np.concatenate(([1.1], weights_F[:-1])) # shift all entries and add new weight\n                    weights_Cr = np.concatenate(([1.1], weights_Cr[:-1]))\n\n                else:\n                    weights_F = np.concatenate(([0.9], weights_F[:-1])) # shift all entries and add new weight\n                    weights_Cr = np.concatenate(([0.9], weights_Cr[:-1]))\n                    \n                if f_u < self.f_opt:\n                    self.f_opt = f_u\n                    self.x_opt = u\n\n                if self.n_evals >= self.budget:\n                    break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "The algorithm AdaptiveDEWindowSimple scored 0.457 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["40bd89f0-32fe-4e6f-8887-5e032130b8e1"], "operator": "simplify", "metadata": {"llm_response": "# Description: Adaptive Differential Evolution with weighted historical parameter adaptation and simplified parameter update rules.\n# Code:\n```python\nimport numpy as np\n\nclass AdaptiveDEWindowSimple:\n    def __init__(self, budget=10000, dim=10, pop_size=40, F=0.5, Cr=0.9, window_size=50):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F  # Initial scaling factor\n        self.Cr = Cr  # Initial crossover rate\n        self.window_size = window_size\n        self.F_history = []\n        self.Cr_history = []\n        self.archive_rate = 0.1\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in self.population])\n        self.n_evals = self.pop_size\n        \n        weights_F = np.zeros(self.window_size) + 1e-6 # Avoid division by zero\n        weights_Cr = np.zeros(self.window_size) + 1e-6 # Avoid division by zero\n        \n        while self.n_evals < self.budget:\n            for i in range(self.pop_size):\n                # Parameter adaptation based on history\n                if len(self.F_history) > 0:\n                    recent_F = np.array(self.F_history[-self.window_size:])\n                    recent_Cr = np.array(self.Cr_history[-self.window_size:])\n                    recent_F_weights = weights_F[:len(recent_F)] / np.sum(weights_F[:len(recent_F)]) if len(recent_F) > 0 else np.array([1.0])\n                    recent_Cr_weights = weights_Cr[:len(recent_Cr)] / np.sum(weights_Cr[:len(recent_Cr)]) if len(recent_Cr) > 0 else np.array([1.0])\n\n                    self.F = np.clip(np.sum(recent_F * recent_F_weights), 0.1, 1.0)\n                    self.Cr = np.clip(np.sum(recent_Cr * recent_Cr_weights), 0.1, 1.0)\n                \n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_1, x_2, x_3 = self.population[idxs]\n                v = self.population[i] + self.F * (x_2 - x_3)\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n                \n                # Crossover\n                j_rand = np.random.randint(self.dim)\n                u = np.array([v[j] if (np.random.rand() < self.Cr or j == j_rand) else self.population[i, j] for j in range(self.dim)])\n\n                # Selection\n                f_u = func(u)\n                self.n_evals += 1\n                \n                if f_u < fitness[i]:\n                    self.population[i] = u\n                    fitness[i] = f_u\n                    self.F_history.append(self.F)\n                    self.Cr_history.append(self.Cr)\n                    if len(self.F_history) > self.window_size:\n                         self.F_history.pop(0)\n                         self.Cr_history.pop(0)\n                    \n                    weights_F = np.concatenate(([1.1], weights_F[:-1])) # shift all entries and add new weight\n                    weights_Cr = np.concatenate(([1.1], weights_Cr[:-1]))\n\n                else:\n                    weights_F = np.concatenate(([0.9], weights_F[:-1])) # shift all entries and add new weight\n                    weights_Cr = np.concatenate(([0.9], weights_Cr[:-1]))\n                    \n                if f_u < self.f_opt:\n                    self.f_opt = f_u\n                    self.x_opt = u\n\n                if self.n_evals >= self.budget:\n                    break\n\n        return self.f_opt, self.x_opt\n```", "operator": "simplify", "theta_sampled": 0.001521745172234855, "generation": 7, "aucs": [0.16582553029350078, 0.33542721189095026, 0.4017660341377034, 0.5975446551924055, 0.3489934159991994, 0.5162761271655446, 0.3200698423717998, 0.39025467412447046, 0.3628958504902775, 0.20605810787809253, 0.7191356924928392, 0.9998262749165641, 0.43806935882704234, 0.3381299512444019, 0.8055374536421354, 0.4911334446937097, 0.3409350532639963, 0.6377874527787137, 0.23070029997872266, 0.48527126116920727]}, "task_prompt": ""}
{"id": "6ba6bc14-f646-4f68-864f-ea0c2f4927a4", "fitness": 0.412077594493918, "name": "ArchiveDE", "description": "Self-adaptive Differential Evolution with archive and stochastic ranking to handle constraints.", "code": "import numpy as np\n\nclass ArchiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=10, F_init=0.5, Cr_init=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.F = F_init\n        self.Cr = Cr_init\n        self.archive = []\n        self.archive_fitness = []\n        self.p = 0.1 # Probability of using archive individual\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in self.population])\n        self.n_evals = self.pop_size\n\n        while self.n_evals < self.budget:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 2, replace=False)\n                x_1, x_2 = self.population[idxs]\n\n                if np.random.rand() < self.p and len(self.archive) > 0:\n                    arc_idx = np.random.randint(len(self.archive))\n                    x_3 = self.archive[arc_idx]\n                else:\n                    idxs2 = np.random.choice(self.pop_size, 1, replace=False)\n                    x_3 = self.population[idxs2[0]]\n\n                v = self.population[i] + self.F * (x_1 - x_2)\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                j_rand = np.random.randint(self.dim)\n                u = np.array([v[j] if (np.random.rand() < self.Cr or j == j_rand) else self.population[i, j] for j in range(self.dim)])\n\n                # Selection\n                f_u = func(u)\n                self.n_evals += 1\n\n                if f_u < fitness[i]:\n                    #Add replaced individual to archive\n                    if len(self.archive) < self.archive_size:\n                         self.archive.append(self.population[i].copy())\n                         self.archive_fitness.append(fitness[i])\n                    else:\n                         #Replace oldest\n                         self.archive[self.n_evals % self.archive_size] = self.population[i].copy()\n                         self.archive_fitness[self.n_evals % self.archive_size] = fitness[i]\n\n                    self.population[i] = u\n                    fitness[i] = f_u\n\n                    if f_u < self.f_opt:\n                        self.f_opt = f_u\n                        self.x_opt = u\n\n\n                if self.n_evals >= self.budget:\n                    break\n\n            # Adapt F and Cr (after each generation)\n            self.adapt_parameters(fitness)\n\n        return self.f_opt, self.x_opt\n\n    def adapt_parameters(self, fitness):\n         # Adapt F and Cr based on success\n         success_indices = np.where(fitness < np.mean(fitness))[0]\n         if len(success_indices) > 0:\n              self.F = np.clip(np.random.normal(0.5, 0.3), 0.1, 1.0)\n              self.Cr = np.clip(np.random.normal(0.9, 0.2), 0.1, 1.0)\n         else:\n              self.F = np.clip(np.random.normal(self.F, 0.1), 0.1, 1.0)\n              self.Cr = np.clip(np.random.normal(self.Cr, 0.1), 0.1, 1.0)", "configspace": "", "generation": 7, "feedback": "The algorithm ArchiveDE scored 0.412 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["2fa45af8-e020-4e0b-9f07-43390b8ca505"], "operator": "simplify", "metadata": {"llm_response": "# Description: Self-adaptive Differential Evolution with archive and stochastic ranking to handle constraints.\n# Code: \n```python\nimport numpy as np\n\nclass ArchiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=10, F_init=0.5, Cr_init=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.F = F_init\n        self.Cr = Cr_init\n        self.archive = []\n        self.archive_fitness = []\n        self.p = 0.1 # Probability of using archive individual\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in self.population])\n        self.n_evals = self.pop_size\n\n        while self.n_evals < self.budget:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 2, replace=False)\n                x_1, x_2 = self.population[idxs]\n\n                if np.random.rand() < self.p and len(self.archive) > 0:\n                    arc_idx = np.random.randint(len(self.archive))\n                    x_3 = self.archive[arc_idx]\n                else:\n                    idxs2 = np.random.choice(self.pop_size, 1, replace=False)\n                    x_3 = self.population[idxs2[0]]\n\n                v = self.population[i] + self.F * (x_1 - x_2)\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                j_rand = np.random.randint(self.dim)\n                u = np.array([v[j] if (np.random.rand() < self.Cr or j == j_rand) else self.population[i, j] for j in range(self.dim)])\n\n                # Selection\n                f_u = func(u)\n                self.n_evals += 1\n\n                if f_u < fitness[i]:\n                    #Add replaced individual to archive\n                    if len(self.archive) < self.archive_size:\n                         self.archive.append(self.population[i].copy())\n                         self.archive_fitness.append(fitness[i])\n                    else:\n                         #Replace oldest\n                         self.archive[self.n_evals % self.archive_size] = self.population[i].copy()\n                         self.archive_fitness[self.n_evals % self.archive_size] = fitness[i]\n\n                    self.population[i] = u\n                    fitness[i] = f_u\n\n                    if f_u < self.f_opt:\n                        self.f_opt = f_u\n                        self.x_opt = u\n\n\n                if self.n_evals >= self.budget:\n                    break\n\n            # Adapt F and Cr (after each generation)\n            self.adapt_parameters(fitness)\n\n        return self.f_opt, self.x_opt\n\n    def adapt_parameters(self, fitness):\n         # Adapt F and Cr based on success\n         success_indices = np.where(fitness < np.mean(fitness))[0]\n         if len(success_indices) > 0:\n              self.F = np.clip(np.random.normal(0.5, 0.3), 0.1, 1.0)\n              self.Cr = np.clip(np.random.normal(0.9, 0.2), 0.1, 1.0)\n         else:\n              self.F = np.clip(np.random.normal(self.F, 0.1), 0.1, 1.0)\n              self.Cr = np.clip(np.random.normal(self.Cr, 0.1), 0.1, 1.0)\n```", "operator": "simplify", "theta_sampled": 0.008506957850456815, "generation": 7, "aucs": [0.1572062624183923, 0.3435691577534714, 0.38355238623534726, 0.4751033282127337, 0.3296297364850235, 0.39273784483672336, 0.2948809163389685, 0.33648703834486426, 0.33642055682298644, 0.19811323935707537, 0.5566779725320179, 0.9920474113649226, 0.3681309062214887, 0.32961764176515085, 0.76722483214398, 0.41720452022268795, 0.32735489424160025, 0.5398855363601482, 0.20614328397648118, 0.4895644242442976]}, "task_prompt": ""}
{"id": "6c636ef7-f81d-4df4-8ccb-23fc4d5d9e6c", "fitness": 0.07279769640040579, "name": "AdaptiveDERejuv", "description": "Adaptive Differential Evolution with a decaying exploration rate and periodic population rejuvenation.", "code": "import numpy as np\n\nclass AdaptiveDERejuv:\n    def __init__(self, budget=10000, dim=10, pop_size=40, F=0.5, Cr=0.9, decay_rate=0.99, rejuv_prob=0.01):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.Cr = Cr\n        self.decay_rate = decay_rate\n        self.rejuv_prob = rejuv_prob\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in self.population])\n        self.n_evals = self.pop_size\n\n        while self.n_evals < self.budget:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_1, x_2, x_3 = self.population[idxs]\n                v = self.population[i] + self.F * (x_2 - x_3)\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                j_rand = np.random.randint(self.dim)\n                u = np.array([v[j] if (np.random.rand() < self.Cr or j == j_rand) else self.population[i, j] for j in range(self.dim)])\n\n                # Selection\n                f_u = func(u)\n                self.n_evals += 1\n\n                if f_u < fitness[i]:\n                    self.population[i] = u\n                    fitness[i] = f_u\n\n                    if f_u < self.f_opt:\n                        self.f_opt = f_u\n                        self.x_opt = u\n\n                if self.n_evals >= self.budget:\n                    break\n            \n            # Decay F and Cr\n            self.F *= self.decay_rate\n            self.Cr *= self.decay_rate\n            self.F = np.clip(self.F, 0.1, 1.0)\n            self.Cr = np.clip(self.Cr, 0.1, 1.0)\n\n            # Population Rejuvenation\n            for i in range(self.pop_size):\n                if np.random.rand() < self.rejuv_prob:\n                    self.population[i] = np.random.uniform(func.bounds.lb, func.bounds.ub)\n                    fitness[i] = func(self.population[i])\n                    self.n_evals += 1\n                    if fitness[i] < self.f_opt:\n                        self.f_opt = fitness[i]\n                        self.x_opt = self.population[i]\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "The algorithm AdaptiveDERejuv scored 0.073 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["2fa45af8-e020-4e0b-9f07-43390b8ca505"], "operator": "simplify", "metadata": {"llm_response": "# Description: Adaptive Differential Evolution with a decaying exploration rate and periodic population rejuvenation.\n# Code: \n```python\nimport numpy as np\n\nclass AdaptiveDERejuv:\n    def __init__(self, budget=10000, dim=10, pop_size=40, F=0.5, Cr=0.9, decay_rate=0.99, rejuv_prob=0.01):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.Cr = Cr\n        self.decay_rate = decay_rate\n        self.rejuv_prob = rejuv_prob\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in self.population])\n        self.n_evals = self.pop_size\n\n        while self.n_evals < self.budget:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_1, x_2, x_3 = self.population[idxs]\n                v = self.population[i] + self.F * (x_2 - x_3)\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                j_rand = np.random.randint(self.dim)\n                u = np.array([v[j] if (np.random.rand() < self.Cr or j == j_rand) else self.population[i, j] for j in range(self.dim)])\n\n                # Selection\n                f_u = func(u)\n                self.n_evals += 1\n\n                if f_u < fitness[i]:\n                    self.population[i] = u\n                    fitness[i] = f_u\n\n                    if f_u < self.f_opt:\n                        self.f_opt = f_u\n                        self.x_opt = u\n\n                if self.n_evals >= self.budget:\n                    break\n            \n            # Decay F and Cr\n            self.F *= self.decay_rate\n            self.Cr *= self.decay_rate\n            self.F = np.clip(self.F, 0.1, 1.0)\n            self.Cr = np.clip(self.Cr, 0.1, 1.0)\n\n            # Population Rejuvenation\n            for i in range(self.pop_size):\n                if np.random.rand() < self.rejuv_prob:\n                    self.population[i] = np.random.uniform(func.bounds.lb, func.bounds.ub)\n                    fitness[i] = func(self.population[i])\n                    self.n_evals += 1\n                    if fitness[i] < self.f_opt:\n                        self.f_opt = fitness[i]\n                        self.x_opt = self.population[i]\n\n        return self.f_opt, self.x_opt\n```", "operator": "simplify", "theta_sampled": -0.0007046015504732, "generation": 7, "aucs": [0.14559539280081157, 0]}, "task_prompt": ""}
{"id": "4ef611c8-8520-44f8-bb49-1a325d9d7f3c", "fitness": 0.4567224489075287, "name": "AdaptiveDEPerDimensionF", "description": "Adaptive Differential Evolution with per-dimension learning of scaling factor F.", "code": "import numpy as np\n\nclass AdaptiveDEPerDimensionF:\n    def __init__(self, budget=10000, dim=10, pop_size=40, F_init=0.5, Cr_init=0.9, window_size=50):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = np.full(dim, F_init)  # F is now per-dimension\n        self.Cr = Cr_init\n        self.window_size = window_size\n        self.F_history = [[] for _ in range(dim)] # Store F history for each dimension\n        self.Cr_history = []\n        self.success_history = []\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in self.population])\n        self.n_evals = self.pop_size\n\n        while self.n_evals < self.budget:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_1, x_2, x_3 = self.population[idxs]\n\n                # Adapt F for each dimension based on its history\n                F_current = np.zeros(self.dim)\n                for j in range(self.dim):\n                     if len(self.F_history[j]) > 0:\n                         recent_F = self.F_history[j][-self.window_size:]\n                         F_current[j] = np.mean(recent_F) if len(recent_F) > 0 else 0.5\n                         F_current[j] = np.clip(F_current[j], 0.1, 1.0)\n                     else:\n                         F_current[j] = self.F[j] # Default initialization\n                self.F = F_current\n                \n                v = self.population[i] + self.F * (x_2 - x_3)\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                j_rand = np.random.randint(self.dim)\n                u = np.array([v[j] if (np.random.rand() < self.Cr or j == j_rand) else self.population[i, j] for j in range(self.dim)])\n\n                # Selection\n                f_u = func(u)\n                self.n_evals += 1\n\n                if f_u < fitness[i]:\n                    self.population[i] = u\n                    fitness[i] = f_u\n                    \n                    # Store successful F values for each dimension\n                    for j in range(self.dim):\n                        self.F_history[j].append(self.F[j])\n                    \n                    self.Cr_history.append(self.Cr)\n                    self.success_history.append(1)\n\n                    if f_u < self.f_opt:\n                        self.f_opt = f_u\n                        self.x_opt = u\n                else:\n                    self.success_history.append(0)\n                    \n                if self.n_evals >= self.budget:\n                    break\n                    \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "The algorithm AdaptiveDEPerDimensionF scored 0.457 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["2fa45af8-e020-4e0b-9f07-43390b8ca505", "40bd89f0-32fe-4e6f-8887-5e032130b8e1"], "operator": "crossover", "metadata": {"llm_response": "# Description: Adaptive Differential Evolution with per-dimension learning of scaling factor F.\n# Code: \n```python\nimport numpy as np\n\nclass AdaptiveDEPerDimensionF:\n    def __init__(self, budget=10000, dim=10, pop_size=40, F_init=0.5, Cr_init=0.9, window_size=50):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = np.full(dim, F_init)  # F is now per-dimension\n        self.Cr = Cr_init\n        self.window_size = window_size\n        self.F_history = [[] for _ in range(dim)] # Store F history for each dimension\n        self.Cr_history = []\n        self.success_history = []\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in self.population])\n        self.n_evals = self.pop_size\n\n        while self.n_evals < self.budget:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_1, x_2, x_3 = self.population[idxs]\n\n                # Adapt F for each dimension based on its history\n                F_current = np.zeros(self.dim)\n                for j in range(self.dim):\n                     if len(self.F_history[j]) > 0:\n                         recent_F = self.F_history[j][-self.window_size:]\n                         F_current[j] = np.mean(recent_F) if len(recent_F) > 0 else 0.5\n                         F_current[j] = np.clip(F_current[j], 0.1, 1.0)\n                     else:\n                         F_current[j] = self.F[j] # Default initialization\n                self.F = F_current\n                \n                v = self.population[i] + self.F * (x_2 - x_3)\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                j_rand = np.random.randint(self.dim)\n                u = np.array([v[j] if (np.random.rand() < self.Cr or j == j_rand) else self.population[i, j] for j in range(self.dim)])\n\n                # Selection\n                f_u = func(u)\n                self.n_evals += 1\n\n                if f_u < fitness[i]:\n                    self.population[i] = u\n                    fitness[i] = f_u\n                    \n                    # Store successful F values for each dimension\n                    for j in range(self.dim):\n                        self.F_history[j].append(self.F[j])\n                    \n                    self.Cr_history.append(self.Cr)\n                    self.success_history.append(1)\n\n                    if f_u < self.f_opt:\n                        self.f_opt = f_u\n                        self.x_opt = u\n                else:\n                    self.success_history.append(0)\n                    \n                if self.n_evals >= self.budget:\n                    break\n                    \n        return self.f_opt, self.x_opt\n```", "operator": "crossover", "theta_sampled": 0.01334541478697533, "generation": 8, "aucs": [0.1775120955298093, 0.3299742097951178, 0.42207303497137816, 0.630864940333473, 0.3611175643606894, 0.5254614267019737, 0.3093122453368464, 0.38432872596027146, 0.39930939327487636, 0.20049473104437232, 0.6728943432044283, 0.9909638453636962, 0.4302796960046983, 0.3231019965779517, 0.8360537043311586, 0.49540351678307515, 0.3664335633780563, 0.6062726572794064, 0.1886712207161143, 0.48392606720318176]}, "task_prompt": ""}
{"id": "83ce7ba4-6fc3-4427-8ab8-dee7e5727158", "fitness": 0.4082501428853719, "name": "AdaptiveDEWindowSuccess", "description": "Adaptive DE with window-based parameter adaptation and a simplified success history to influence F and Cr.", "code": "import numpy as np\n\nclass AdaptiveDEWindowSuccess:\n    def __init__(self, budget=10000, dim=10, pop_size=40, F_init=0.5, Cr_init=0.9, window_size=50):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F_init\n        self.Cr = Cr_init\n        self.window_size = window_size\n        self.success_history = []  # 1 for success, 0 for failure\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in self.population])\n        self.n_evals = self.pop_size\n\n        while self.n_evals < self.budget:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_1, x_2, x_3 = self.population[idxs]\n                v = self.population[i] + self.F * (x_2 - x_3)\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                j_rand = np.random.randint(self.dim)\n                u = np.array([v[j] if (np.random.rand() < self.Cr or j == j_rand) else self.population[i, j] for j in range(self.dim)])\n\n                # Selection\n                f_u = func(u)\n                self.n_evals += 1\n\n                if f_u < fitness[i]:\n                    self.population[i] = u\n                    fitness[i] = f_u\n                    self.success_history.append(1)\n                    if f_u < self.f_opt:\n                        self.f_opt = f_u\n                        self.x_opt = u\n                else:\n                    self.success_history.append(0)\n\n                if self.n_evals >= self.budget:\n                    break\n\n            # Adapt F and Cr (after each generation)\n            self.adapt_parameters()\n\n        return self.f_opt, self.x_opt\n\n    def adapt_parameters(self):\n        if len(self.success_history) > self.window_size:\n            self.success_history = self.success_history[-self.window_size:]\n        \n        if self.success_history:\n            success_rate = np.mean(self.success_history)\n            \n            # Adjust F and Cr based on success rate\n            self.F = np.clip(self.F * (1 + 0.2 * (success_rate - 0.5)), 0.1, 1.0) # if more than 50% success, increase F, else decrease\n            self.Cr = np.clip(self.Cr * (1 + 0.2 * (success_rate - 0.5)), 0.1, 1.0) # same for Cr", "configspace": "", "generation": 8, "feedback": "The algorithm AdaptiveDEWindowSuccess scored 0.408 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["2fa45af8-e020-4e0b-9f07-43390b8ca505", "40bd89f0-32fe-4e6f-8887-5e032130b8e1"], "operator": "crossover", "metadata": {"llm_response": "# Description: Adaptive DE with window-based parameter adaptation and a simplified success history to influence F and Cr.\n# Code: \n```python\nimport numpy as np\n\nclass AdaptiveDEWindowSuccess:\n    def __init__(self, budget=10000, dim=10, pop_size=40, F_init=0.5, Cr_init=0.9, window_size=50):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F_init\n        self.Cr = Cr_init\n        self.window_size = window_size\n        self.success_history = []  # 1 for success, 0 for failure\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in self.population])\n        self.n_evals = self.pop_size\n\n        while self.n_evals < self.budget:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_1, x_2, x_3 = self.population[idxs]\n                v = self.population[i] + self.F * (x_2 - x_3)\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                j_rand = np.random.randint(self.dim)\n                u = np.array([v[j] if (np.random.rand() < self.Cr or j == j_rand) else self.population[i, j] for j in range(self.dim)])\n\n                # Selection\n                f_u = func(u)\n                self.n_evals += 1\n\n                if f_u < fitness[i]:\n                    self.population[i] = u\n                    fitness[i] = f_u\n                    self.success_history.append(1)\n                    if f_u < self.f_opt:\n                        self.f_opt = f_u\n                        self.x_opt = u\n                else:\n                    self.success_history.append(0)\n\n                if self.n_evals >= self.budget:\n                    break\n\n            # Adapt F and Cr (after each generation)\n            self.adapt_parameters()\n\n        return self.f_opt, self.x_opt\n\n    def adapt_parameters(self):\n        if len(self.success_history) > self.window_size:\n            self.success_history = self.success_history[-self.window_size:]\n        \n        if self.success_history:\n            success_rate = np.mean(self.success_history)\n            \n            # Adjust F and Cr based on success rate\n            self.F = np.clip(self.F * (1 + 0.2 * (success_rate - 0.5)), 0.1, 1.0) # if more than 50% success, increase F, else decrease\n            self.Cr = np.clip(self.Cr * (1 + 0.2 * (success_rate - 0.5)), 0.1, 1.0) # same for Cr\n```", "operator": "crossover", "theta_sampled": 0.01220402250289254, "generation": 8, "aucs": [0.14914357689830127, 0.29138151298759485, 0.41873575629235826, 0.4728775321003985, 0.32476144957234454, 0.4334340511423418, 0.29216519459108503, 0.340733469635761, 0.32295303464370795, 0.23916353395432, 0.43665964861843987, 0.9785072082719733, 0.2838551314501262, 0.2973238045977442, 0.785068230393553, 0.5227187833077439, 0.3139451563242246, 0.517269078362194, 0.21451598804060845, 0.5297907165226191]}, "task_prompt": ""}
{"id": "eb41a721-9d42-4372-934c-1d0501893a1b", "fitness": "-inf", "name": "ClusteredDifferentialEvolution", "description": "A population-based algorithm that combines aspects of Differential Evolution with a clustering-based mutation strategy to diversify the search.", "code": "import numpy as np\nfrom sklearn.cluster import KMeans\n\nclass ClusteredDifferentialEvolution:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.7, CR=0.9, num_clusters=5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.num_clusters = num_clusters\n\n    def __call__(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n        self.eval_count = self.pop_size\n\n        while self.eval_count < self.budget:\n            # Clustering to diversify search\n            kmeans = KMeans(n_clusters=self.num_clusters, random_state=0, n_init='auto').fit(self.population)\n            cluster_labels = kmeans.labels_\n\n            for i in range(self.pop_size):\n                # Mutation strategy based on cluster\n                cluster_indices = np.where(cluster_labels == cluster_labels[i])[0]\n\n                if len(cluster_indices) > 1:\n                    # Select two other individuals within the same cluster\n                    indices = np.random.choice(cluster_indices, 2, replace=False)\n                    x_r1 = self.population[indices[0]]\n                    x_r2 = self.population[indices[1]]\n                else:\n                    # If only one member in cluster, select randomly from entire population\n                    indices = np.random.choice(self.pop_size, 2, replace=False)\n                    x_r1 = self.population[indices[0]]\n                    x_r2 = self.population[indices[1]]\n\n                x_current = self.population[i]\n                x_mutated = x_current + self.F * (x_r1 - x_r2)\n                x_mutated = np.clip(x_mutated, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                x_trial = np.zeros(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR:\n                        x_trial[j] = x_mutated[j]\n                    else:\n                        x_trial[j] = x_current[j]\n                \n\n                f_trial = func(x_trial)\n                self.eval_count += 1\n\n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.population[i] = x_trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = x_trial\n\n                if self.eval_count >= self.budget:\n                    break\n                    \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "", "error": "Validation failed: Forbidden import: 'sklearn' is not available in the evaluation environment", "parent_ids": [], "operator": "random_new", "metadata": {"llm_response": "# Description: A population-based algorithm that combines aspects of Differential Evolution with a clustering-based mutation strategy to diversify the search.\n# Code: \n```python\nimport numpy as np\nfrom sklearn.cluster import KMeans\n\nclass ClusteredDifferentialEvolution:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.7, CR=0.9, num_clusters=5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.num_clusters = num_clusters\n\n    def __call__(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n        self.eval_count = self.pop_size\n\n        while self.eval_count < self.budget:\n            # Clustering to diversify search\n            kmeans = KMeans(n_clusters=self.num_clusters, random_state=0, n_init='auto').fit(self.population)\n            cluster_labels = kmeans.labels_\n\n            for i in range(self.pop_size):\n                # Mutation strategy based on cluster\n                cluster_indices = np.where(cluster_labels == cluster_labels[i])[0]\n\n                if len(cluster_indices) > 1:\n                    # Select two other individuals within the same cluster\n                    indices = np.random.choice(cluster_indices, 2, replace=False)\n                    x_r1 = self.population[indices[0]]\n                    x_r2 = self.population[indices[1]]\n                else:\n                    # If only one member in cluster, select randomly from entire population\n                    indices = np.random.choice(self.pop_size, 2, replace=False)\n                    x_r1 = self.population[indices[0]]\n                    x_r2 = self.population[indices[1]]\n\n                x_current = self.population[i]\n                x_mutated = x_current + self.F * (x_r1 - x_r2)\n                x_mutated = np.clip(x_mutated, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                x_trial = np.zeros(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR:\n                        x_trial[j] = x_mutated[j]\n                    else:\n                        x_trial[j] = x_current[j]\n                \n\n                f_trial = func(x_trial)\n                self.eval_count += 1\n\n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.population[i] = x_trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = x_trial\n\n                if self.eval_count >= self.budget:\n                    break\n                    \n        return self.f_opt, self.x_opt\n```"}, "task_prompt": ""}
{"id": "fcdcbf48-6523-411e-bb4a-e8762f71fb52", "fitness": 0.46102706648912595, "name": "AdaptiveDEWindowWeighted", "description": "Adaptive Differential Evolution with window-based parameter adaptation using success-based weighting.", "code": "import numpy as np\n\nclass AdaptiveDEWindowWeighted:\n    def __init__(self, budget=10000, dim=10, pop_size=40, F_init=0.5, Cr_init=0.9, window_size=20):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F_init\n        self.Cr = Cr_init\n        self.window_size = window_size\n        self.success_F = []\n        self.success_Cr = []\n        self.weights = np.zeros(window_size) + 1e-6\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in self.population])\n        self.n_evals = self.pop_size\n\n        while self.n_evals < self.budget:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_1, x_2, x_3 = self.population[idxs]\n                v = self.population[i] + self.F * (x_2 - x_3)\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                j_rand = np.random.randint(self.dim)\n                u = np.array([v[j] if (np.random.rand() < self.Cr or j == j_rand) else self.population[i, j] for j in range(self.dim)])\n\n                # Selection\n                f_u = func(u)\n                self.n_evals += 1\n\n                if f_u < fitness[i]:\n                    # Store successful F and Cr\n                    self.success_F.append(self.F)\n                    self.success_Cr.append(self.Cr)\n\n                    self.population[i] = u\n                    fitness[i] = f_u\n\n                    if f_u < self.f_opt:\n                        self.f_opt = f_u\n                        self.x_opt = u\n                    \n                    self.weights = np.concatenate(([1.1], self.weights[:-1]))\n\n                else:\n                    self.weights = np.concatenate(([0.9], self.weights[:-1]))\n\n                if self.n_evals >= self.budget:\n                    break\n\n            # Adapt F and Cr (after each generation)\n            self.adapt_parameters()\n\n        return self.f_opt, self.x_opt\n\n    def adapt_parameters(self):\n        # Window-based weighted averaging for F and Cr\n        if self.success_F:\n            recent_F = np.array(self.success_F[-self.window_size:])\n            recent_F_weights = self.weights[:len(recent_F)] / np.sum(self.weights[:len(recent_F)]) if len(recent_F) > 0 else np.array([1.0])\n            self.F = np.clip(np.sum(recent_F * recent_F_weights), 0.1, 1.0)\n            self.success_F = []\n\n        if self.success_Cr:\n            recent_Cr = np.array(self.success_Cr[-self.window_size:])\n            recent_Cr_weights = self.weights[:len(recent_Cr)] / np.sum(self.weights[:len(recent_Cr)]) if len(recent_Cr) > 0 else np.array([1.0])\n            self.Cr = np.clip(np.sum(recent_Cr * recent_Cr_weights), 0.1, 1.0)\n            self.success_Cr = []", "configspace": "", "generation": 8, "feedback": "The algorithm AdaptiveDEWindowWeighted scored 0.461 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["2fa45af8-e020-4e0b-9f07-43390b8ca505", "e3fb1403-4620-48a7-8209-b6d1ad71f1b8"], "operator": "crossover", "metadata": {"llm_response": "# Description: Adaptive Differential Evolution with window-based parameter adaptation using success-based weighting.\n# Code: \n```python\nimport numpy as np\n\nclass AdaptiveDEWindowWeighted:\n    def __init__(self, budget=10000, dim=10, pop_size=40, F_init=0.5, Cr_init=0.9, window_size=20):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F_init\n        self.Cr = Cr_init\n        self.window_size = window_size\n        self.success_F = []\n        self.success_Cr = []\n        self.weights = np.zeros(window_size) + 1e-6\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in self.population])\n        self.n_evals = self.pop_size\n\n        while self.n_evals < self.budget:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_1, x_2, x_3 = self.population[idxs]\n                v = self.population[i] + self.F * (x_2 - x_3)\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                j_rand = np.random.randint(self.dim)\n                u = np.array([v[j] if (np.random.rand() < self.Cr or j == j_rand) else self.population[i, j] for j in range(self.dim)])\n\n                # Selection\n                f_u = func(u)\n                self.n_evals += 1\n\n                if f_u < fitness[i]:\n                    # Store successful F and Cr\n                    self.success_F.append(self.F)\n                    self.success_Cr.append(self.Cr)\n\n                    self.population[i] = u\n                    fitness[i] = f_u\n\n                    if f_u < self.f_opt:\n                        self.f_opt = f_u\n                        self.x_opt = u\n                    \n                    self.weights = np.concatenate(([1.1], self.weights[:-1]))\n\n                else:\n                    self.weights = np.concatenate(([0.9], self.weights[:-1]))\n\n                if self.n_evals >= self.budget:\n                    break\n\n            # Adapt F and Cr (after each generation)\n            self.adapt_parameters()\n\n        return self.f_opt, self.x_opt\n\n    def adapt_parameters(self):\n        # Window-based weighted averaging for F and Cr\n        if self.success_F:\n            recent_F = np.array(self.success_F[-self.window_size:])\n            recent_F_weights = self.weights[:len(recent_F)] / np.sum(self.weights[:len(recent_F)]) if len(recent_F) > 0 else np.array([1.0])\n            self.F = np.clip(np.sum(recent_F * recent_F_weights), 0.1, 1.0)\n            self.success_F = []\n\n        if self.success_Cr:\n            recent_Cr = np.array(self.success_Cr[-self.window_size:])\n            recent_Cr_weights = self.weights[:len(recent_Cr)] / np.sum(self.weights[:len(recent_Cr)]) if len(recent_Cr) > 0 else np.array([1.0])\n            self.Cr = np.clip(np.sum(recent_Cr * recent_Cr_weights), 0.1, 1.0)\n            self.success_Cr = []\n```", "operator": "crossover", "theta_sampled": 0.007961666616959226, "generation": 8, "aucs": [0.1773158348208681, 0.34888410427769945, 0.43195089463578995, 0.7338281216332323, 0.3303715829716227, 0.5107334193420858, 0.30164299935919503, 0.39462360073526626, 0.3725142211651782, 0.19377567725029599, 0.704590368080571, 0.9999111625021494, 0.42247907307016686, 0.3311958954148385, 0.8074645804463608, 0.4777467791634987, 0.35386023451022663, 0.6188690774850552, 0.21132601365170645, 0.49745768926671097]}, "task_prompt": ""}
{"id": "1e4120b9-5dcb-4d7a-9145-a0dbc211bd88", "fitness": 0.3739943870377734, "name": "AdaptiveDE", "description": "Adaptive Differential Evolution with a decaying exploration rate and simplified parameter adaptation based on recent success.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=40, initial_F=0.5, initial_Cr=0.9, exploration_decay=0.99):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = initial_F  # Scaling factor\n        self.Cr = initial_Cr  # Crossover rate\n        self.exploration_decay = exploration_decay\n        self.best_fitness_history = []\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in self.population])\n        self.n_evals = self.pop_size\n        best_idx = np.argmin(fitness)\n        self.f_opt = fitness[best_idx]\n        self.x_opt = self.population[best_idx]\n        self.best_fitness_history.append(self.f_opt)\n\n\n        while self.n_evals < self.budget:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_1, x_2, x_3 = self.population[idxs]\n                v = self.population[i] + self.F * (x_2 - x_3)\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n                \n                # Crossover\n                j_rand = np.random.randint(self.dim)\n                u = np.array([v[j] if (np.random.rand() < self.Cr or j == j_rand) else self.population[i, j] for j in range(self.dim)])\n\n                # Selection\n                f_u = func(u)\n                self.n_evals += 1\n                \n                if f_u < fitness[i]:\n                    self.population[i] = u\n                    fitness[i] = f_u\n                    # If successful, slightly increase Cr and decrease F to exploit more\n                    self.Cr = min(1.0, self.Cr + 0.05)\n                    self.F = max(0.1, self.F - 0.05)\n                else:\n                    # If unsuccessful, slightly decrease Cr and increase F to explore more\n                    self.Cr = max(0.1, self.Cr - 0.05)\n                    self.F = min(0.9, self.F + 0.05)\n                    \n                if f_u < self.f_opt:\n                    self.f_opt = f_u\n                    self.x_opt = u\n\n                if self.n_evals >= self.budget:\n                    break\n            \n            # Decay exploration to fine-tune towards the end\n            self.F *= self.exploration_decay\n            self.Cr *= self.exploration_decay\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "The algorithm AdaptiveDE scored 0.374 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["40bd89f0-32fe-4e6f-8887-5e032130b8e1"], "operator": "simplify", "metadata": {"llm_response": "# Description: Adaptive Differential Evolution with a decaying exploration rate and simplified parameter adaptation based on recent success.\n# Code: \n```python\nimport numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=40, initial_F=0.5, initial_Cr=0.9, exploration_decay=0.99):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = initial_F  # Scaling factor\n        self.Cr = initial_Cr  # Crossover rate\n        self.exploration_decay = exploration_decay\n        self.best_fitness_history = []\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in self.population])\n        self.n_evals = self.pop_size\n        best_idx = np.argmin(fitness)\n        self.f_opt = fitness[best_idx]\n        self.x_opt = self.population[best_idx]\n        self.best_fitness_history.append(self.f_opt)\n\n\n        while self.n_evals < self.budget:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_1, x_2, x_3 = self.population[idxs]\n                v = self.population[i] + self.F * (x_2 - x_3)\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n                \n                # Crossover\n                j_rand = np.random.randint(self.dim)\n                u = np.array([v[j] if (np.random.rand() < self.Cr or j == j_rand) else self.population[i, j] for j in range(self.dim)])\n\n                # Selection\n                f_u = func(u)\n                self.n_evals += 1\n                \n                if f_u < fitness[i]:\n                    self.population[i] = u\n                    fitness[i] = f_u\n                    # If successful, slightly increase Cr and decrease F to exploit more\n                    self.Cr = min(1.0, self.Cr + 0.05)\n                    self.F = max(0.1, self.F - 0.05)\n                else:\n                    # If unsuccessful, slightly decrease Cr and increase F to explore more\n                    self.Cr = max(0.1, self.Cr - 0.05)\n                    self.F = min(0.9, self.F + 0.05)\n                    \n                if f_u < self.f_opt:\n                    self.f_opt = f_u\n                    self.x_opt = u\n\n                if self.n_evals >= self.budget:\n                    break\n            \n            # Decay exploration to fine-tune towards the end\n            self.F *= self.exploration_decay\n            self.Cr *= self.exploration_decay\n\n        return self.f_opt, self.x_opt\n```", "operator": "simplify", "theta_sampled": 0.010287313464409606, "generation": 8, "aucs": [0.15282484907558314, 0.198939835847864, 0.3893899210124414, 0.30121704616622724, 0.29578781891741224, 0.421807320539231, 0.2937239704435691, 0.32662383371550385, 0.3065086069223566, 0.19705849377819618, 0.3120693558447434, 0.997352591839212, 0.26765379923003507, 0.3112645478628475, 0.7484041457386161, 0.42811490571675304, 0.31415165928801225, 0.5270222036976744, 0.19477249538030195, 0.4952003397388869]}, "task_prompt": ""}
{"id": "db0a7488-5693-4570-8097-896a973058db", "fitness": 0.4567298737361768, "name": "AdaptiveDEWindowSuccess", "description": "Adaptive DE with weighted history for F and Cr, and a success-based update for weights.", "code": "import numpy as np\n\nclass AdaptiveDEWindowSuccess:\n    def __init__(self, budget=10000, dim=10, pop_size=40, F=0.5, Cr=0.9, window_size=50):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F  # Initial scaling factor\n        self.Cr = Cr  # Initial crossover rate\n        self.window_size = window_size\n        self.F_history = []\n        self.Cr_history = []\n        self.success_history = []\n        self.archive_rate = 0.1\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in self.population])\n        self.n_evals = self.pop_size\n        \n        weights_F = np.zeros(self.window_size) + 1e-6 # Avoid division by zero\n        weights_Cr = np.zeros(self.window_size) + 1e-6 # Avoid division by zero\n        \n        while self.n_evals < self.budget:\n            for i in range(self.pop_size):\n                # Parameter adaptation based on history\n                if len(self.F_history) > 0:\n                    recent_F = np.array(self.F_history[-self.window_size:])\n                    recent_Cr = np.array(self.Cr_history[-self.window_size:])\n                    recent_F_weights = weights_F[:len(recent_F)] / np.sum(weights_F[:len(recent_F)]) if len(recent_F) > 0 else np.array([1.0])\n                    recent_Cr_weights = weights_Cr[:len(recent_Cr)] / np.sum(weights_Cr[:len(recent_Cr)]) if len(recent_Cr) > 0 else np.array([1.0])\n\n                    self.F = np.clip(np.sum(recent_F * recent_F_weights), 0.1, 1.0)\n                    self.Cr = np.clip(np.sum(recent_Cr * recent_Cr_weights), 0.1, 1.0)\n                \n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_1, x_2, x_3 = self.population[idxs]\n                v = self.population[i] + self.F * (x_2 - x_3)\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n                \n                # Crossover\n                j_rand = np.random.randint(self.dim)\n                u = np.array([v[j] if (np.random.rand() < self.Cr or j == j_rand) else self.population[i, j] for j in range(self.dim)])\n\n                # Selection\n                f_u = func(u)\n                self.n_evals += 1\n                \n                if f_u < fitness[i]:\n                    self.population[i] = u\n                    fitness[i] = f_u\n                    self.F_history.append(self.F)\n                    self.Cr_history.append(self.Cr)\n                    \n                    weights_F = np.concatenate(([1.1], weights_F[:-1])) # shift all entries and add new weight\n                    weights_Cr = np.concatenate(([1.1], weights_Cr[:-1]))\n\n                else:\n                    weights_F = np.concatenate(([0.9], weights_F[:-1])) # shift all entries and add new weight\n                    weights_Cr = np.concatenate(([0.9], weights_Cr[:-1]))\n                    \n                if f_u < self.f_opt:\n                    self.f_opt = f_u\n                    self.x_opt = u\n\n                if self.n_evals >= self.budget:\n                    break\n                \n                if len(self.F_history) > self.window_size:\n                    self.F_history.pop(0)\n                    self.Cr_history.pop(0)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "The algorithm AdaptiveDEWindowSuccess scored 0.457 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["e3fb1403-4620-48a7-8209-b6d1ad71f1b8", "40bd89f0-32fe-4e6f-8887-5e032130b8e1"], "operator": "crossover", "metadata": {"llm_response": "# Description: Adaptive DE with weighted history for F and Cr, and a success-based update for weights.\n# Code: \n```python\nimport numpy as np\n\nclass AdaptiveDEWindowSuccess:\n    def __init__(self, budget=10000, dim=10, pop_size=40, F=0.5, Cr=0.9, window_size=50):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F  # Initial scaling factor\n        self.Cr = Cr  # Initial crossover rate\n        self.window_size = window_size\n        self.F_history = []\n        self.Cr_history = []\n        self.success_history = []\n        self.archive_rate = 0.1\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in self.population])\n        self.n_evals = self.pop_size\n        \n        weights_F = np.zeros(self.window_size) + 1e-6 # Avoid division by zero\n        weights_Cr = np.zeros(self.window_size) + 1e-6 # Avoid division by zero\n        \n        while self.n_evals < self.budget:\n            for i in range(self.pop_size):\n                # Parameter adaptation based on history\n                if len(self.F_history) > 0:\n                    recent_F = np.array(self.F_history[-self.window_size:])\n                    recent_Cr = np.array(self.Cr_history[-self.window_size:])\n                    recent_F_weights = weights_F[:len(recent_F)] / np.sum(weights_F[:len(recent_F)]) if len(recent_F) > 0 else np.array([1.0])\n                    recent_Cr_weights = weights_Cr[:len(recent_Cr)] / np.sum(weights_Cr[:len(recent_Cr)]) if len(recent_Cr) > 0 else np.array([1.0])\n\n                    self.F = np.clip(np.sum(recent_F * recent_F_weights), 0.1, 1.0)\n                    self.Cr = np.clip(np.sum(recent_Cr * recent_Cr_weights), 0.1, 1.0)\n                \n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_1, x_2, x_3 = self.population[idxs]\n                v = self.population[i] + self.F * (x_2 - x_3)\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n                \n                # Crossover\n                j_rand = np.random.randint(self.dim)\n                u = np.array([v[j] if (np.random.rand() < self.Cr or j == j_rand) else self.population[i, j] for j in range(self.dim)])\n\n                # Selection\n                f_u = func(u)\n                self.n_evals += 1\n                \n                if f_u < fitness[i]:\n                    self.population[i] = u\n                    fitness[i] = f_u\n                    self.F_history.append(self.F)\n                    self.Cr_history.append(self.Cr)\n                    \n                    weights_F = np.concatenate(([1.1], weights_F[:-1])) # shift all entries and add new weight\n                    weights_Cr = np.concatenate(([1.1], weights_Cr[:-1]))\n\n                else:\n                    weights_F = np.concatenate(([0.9], weights_F[:-1])) # shift all entries and add new weight\n                    weights_Cr = np.concatenate(([0.9], weights_Cr[:-1]))\n                    \n                if f_u < self.f_opt:\n                    self.f_opt = f_u\n                    self.x_opt = u\n\n                if self.n_evals >= self.budget:\n                    break\n                \n                if len(self.F_history) > self.window_size:\n                    self.F_history.pop(0)\n                    self.Cr_history.pop(0)\n\n        return self.f_opt, self.x_opt\n```", "operator": "crossover", "theta_sampled": 0.007762283216042255, "generation": 8, "aucs": [0.16971071244494962, 0.3289726547995683, 0.4124000018597336, 0.6548406380429922, 0.3597684158590778, 0.5161165811150978, 0.3201938287795042, 0.3767384792503443, 0.3794177321358295, 0.1974507747646692, 0.7305393116033254, 0.99364873600517, 0.4530823976392341, 0.339629726008581, 0.8177825598799271, 0.47261554644022563, 0.3541221548536235, 0.5746547852736684, 0.18934168685687114, 0.4935707511111429]}, "task_prompt": ""}
{"id": "4f323c71-7ecb-465d-b3d2-4542ac704786", "fitness": "-inf", "name": "PSODE_NN", "description": "A population-based algorithm that combines aspects of particle swarm optimization (PSO) and differential evolution (DE) with a diversity maintenance strategy using nearest neighbor distance.", "code": "import numpy as np\nfrom scipy.spatial.distance import cdist\n\nclass PSODE_NN:\n    def __init__(self, budget=10000, dim=10, pop_size=50, inertia=0.7, c1=1.5, c2=1.5, F=0.8, cr=0.7, diversity_threshold=0.01):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.inertia = inertia\n        self.c1 = c1\n        self.c2 = c2\n        self.F = F\n        self.cr = cr\n        self.diversity_threshold = diversity_threshold\n\n    def __call__(self, func):\n        # Initialization\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        velocities = np.random.uniform(-1, 1, size=(self.pop_size, self.dim))  # Initialize velocities\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n        \n        personal_best_positions = population.copy()\n        personal_best_fitness = fitness.copy()\n        global_best_index = np.argmin(fitness)\n        global_best_position = population[global_best_index].copy()\n        global_best_fitness = fitness[global_best_index]\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # PSO Update\n                velocities[i] = (self.inertia * velocities[i] +\n                                 self.c1 * np.random.rand(self.dim) * (personal_best_positions[i] - population[i]) +\n                                 self.c2 * np.random.rand(self.dim) * (global_best_position - population[i]))\n                \n                # DE Mutation and Crossover\n                r1, r2, r3 = np.random.choice(self.pop_size, 3, replace=False)\n                while r1 == i or r2 == i or r3 == i:\n                    r1, r2, r3 = np.random.choice(self.pop_size, 3, replace=False)\n\n                mutant = population[r1] + self.F * (population[r2] - population[r3])\n                \n                # Crossover\n                trial_vector = population[i].copy()\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < self.cr or j == j_rand:\n                        trial_vector[j] = mutant[j]\n\n                # Boundary Handling (clip to bounds)\n                trial_vector = np.clip(trial_vector, func.bounds.lb, func.bounds.ub)\n                \n                # Evaluate trial vector\n                f_trial = func(trial_vector)\n                self.budget -= 1\n                if self.budget <= 0:\n                    break\n\n                # Selection\n                if f_trial < fitness[i]:\n                    population[i] = trial_vector\n                    fitness[i] = f_trial\n\n                    # Update personal best\n                    if f_trial < personal_best_fitness[i]:\n                        personal_best_fitness[i] = f_trial\n                        personal_best_positions[i] = trial_vector.copy()\n\n                        # Update global best\n                        if f_trial < global_best_fitness:\n                            global_best_fitness = f_trial\n                            global_best_position = trial_vector.copy()\n            \n            if self.budget <= 0:\n                break\n\n            # Diversity Maintenance using Nearest Neighbor Distance\n            distances = cdist(population, population)\n            np.fill_diagonal(distances, np.inf)  # Avoid self-comparison\n            min_distances = np.min(distances, axis=1)\n            mean_min_distance = np.mean(min_distances)\n\n            if mean_min_distance < self.diversity_threshold:\n                # Reset the worst performing particle to a random location\n                worst_index = np.argmax(fitness)\n                population[worst_index] = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                fitness[worst_index] = func(population[worst_index])\n                self.budget -= 1\n                \n                # Update personal and global best if necessary\n                personal_best_positions[worst_index] = population[worst_index].copy()\n                personal_best_fitness[worst_index] = fitness[worst_index]\n                if fitness[worst_index] < global_best_fitness:\n                    global_best_fitness = fitness[worst_index]\n                    global_best_position = population[worst_index].copy()\n\n        return global_best_fitness, global_best_position", "configspace": "", "generation": 8, "feedback": "In the code, line 78, in __call__, the following error occurred:\nNameError: name 'cdist' is not defined. Did you mean: 'dict'?\nOn line: distances = cdist(population, population)", "error": "In the code, line 78, in __call__, the following error occurred:\nNameError: name 'cdist' is not defined. Did you mean: 'dict'?\nOn line: distances = cdist(population, population)", "parent_ids": [], "operator": "random_new", "metadata": {"llm_response": "# Description: A population-based algorithm that combines aspects of particle swarm optimization (PSO) and differential evolution (DE) with a diversity maintenance strategy using nearest neighbor distance.\n# Code: \n```python\nimport numpy as np\nfrom scipy.spatial.distance import cdist\n\nclass PSODE_NN:\n    def __init__(self, budget=10000, dim=10, pop_size=50, inertia=0.7, c1=1.5, c2=1.5, F=0.8, cr=0.7, diversity_threshold=0.01):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.inertia = inertia\n        self.c1 = c1\n        self.c2 = c2\n        self.F = F\n        self.cr = cr\n        self.diversity_threshold = diversity_threshold\n\n    def __call__(self, func):\n        # Initialization\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        velocities = np.random.uniform(-1, 1, size=(self.pop_size, self.dim))  # Initialize velocities\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n        \n        personal_best_positions = population.copy()\n        personal_best_fitness = fitness.copy()\n        global_best_index = np.argmin(fitness)\n        global_best_position = population[global_best_index].copy()\n        global_best_fitness = fitness[global_best_index]\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # PSO Update\n                velocities[i] = (self.inertia * velocities[i] +\n                                 self.c1 * np.random.rand(self.dim) * (personal_best_positions[i] - population[i]) +\n                                 self.c2 * np.random.rand(self.dim) * (global_best_position - population[i]))\n                \n                # DE Mutation and Crossover\n                r1, r2, r3 = np.random.choice(self.pop_size, 3, replace=False)\n                while r1 == i or r2 == i or r3 == i:\n                    r1, r2, r3 = np.random.choice(self.pop_size, 3, replace=False)\n\n                mutant = population[r1] + self.F * (population[r2] - population[r3])\n                \n                # Crossover\n                trial_vector = population[i].copy()\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < self.cr or j == j_rand:\n                        trial_vector[j] = mutant[j]\n\n                # Boundary Handling (clip to bounds)\n                trial_vector = np.clip(trial_vector, func.bounds.lb, func.bounds.ub)\n                \n                # Evaluate trial vector\n                f_trial = func(trial_vector)\n                self.budget -= 1\n                if self.budget <= 0:\n                    break\n\n                # Selection\n                if f_trial < fitness[i]:\n                    population[i] = trial_vector\n                    fitness[i] = f_trial\n\n                    # Update personal best\n                    if f_trial < personal_best_fitness[i]:\n                        personal_best_fitness[i] = f_trial\n                        personal_best_positions[i] = trial_vector.copy()\n\n                        # Update global best\n                        if f_trial < global_best_fitness:\n                            global_best_fitness = f_trial\n                            global_best_position = trial_vector.copy()\n            \n            if self.budget <= 0:\n                break\n\n            # Diversity Maintenance using Nearest Neighbor Distance\n            distances = cdist(population, population)\n            np.fill_diagonal(distances, np.inf)  # Avoid self-comparison\n            min_distances = np.min(distances, axis=1)\n            mean_min_distance = np.mean(min_distances)\n\n            if mean_min_distance < self.diversity_threshold:\n                # Reset the worst performing particle to a random location\n                worst_index = np.argmax(fitness)\n                population[worst_index] = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                fitness[worst_index] = func(population[worst_index])\n                self.budget -= 1\n                \n                # Update personal and global best if necessary\n                personal_best_positions[worst_index] = population[worst_index].copy()\n                personal_best_fitness[worst_index] = fitness[worst_index]\n                if fitness[worst_index] < global_best_fitness:\n                    global_best_fitness = fitness[worst_index]\n                    global_best_position = population[worst_index].copy()\n\n        return global_best_fitness, global_best_position\n```", "operator": "random_new", "theta_sampled": 0.011253613575881142, "generation": 8}, "task_prompt": ""}
{"id": "9b31f0cb-4a43-41b4-9efe-baf135bd9e2c", "fitness": 0.45231497956346456, "name": "AdaptiveDESuccessFrequency", "description": "Adaptive Differential Evolution with window-based parameter averaging and success-based parameter update frequency.", "code": "import numpy as np\n\nclass AdaptiveDESuccessFrequency:\n    def __init__(self, budget=10000, dim=10, pop_size=40, F_init=0.5, Cr_init=0.9, window_size=50):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F_init\n        self.Cr = Cr_init\n        self.window_size = window_size\n        self.success_F = []\n        self.success_Cr = []\n        self.F_history = []\n        self.Cr_history = []\n        self.success_history = []\n        self.update_frequency = 5  # Update F and Cr every n generations\n        self.generation_counter = 0\n\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in self.population])\n        self.n_evals = self.pop_size\n\n        while self.n_evals < self.budget:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_1, x_2, x_3 = self.population[idxs]\n                v = self.population[i] + self.F * (x_2 - x_3)\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                j_rand = np.random.randint(self.dim)\n                u = np.array([v[j] if (np.random.rand() < self.Cr or j == j_rand) else self.population[i, j] for j in range(self.dim)])\n\n                # Selection\n                f_u = func(u)\n                self.n_evals += 1\n\n                if f_u < fitness[i]:\n                    # Store successful F and Cr\n                    self.success_F.append(self.F)\n                    self.success_Cr.append(self.Cr)\n                    self.success_history.append(1)\n\n                    self.population[i] = u\n                    fitness[i] = f_u\n\n                    if f_u < self.f_opt:\n                        self.f_opt = f_u\n                        self.x_opt = u\n                else:\n                    self.success_history.append(0)\n\n                if self.n_evals >= self.budget:\n                    break\n\n            # Adapt F and Cr (every update_frequency generations)\n            self.generation_counter += 1\n            if self.generation_counter % self.update_frequency == 0:\n                self.adapt_parameters()\n\n        return self.f_opt, self.x_opt\n\n    def adapt_parameters(self):\n        # Window-based averaging for F and Cr\n        if self.success_F:\n            self.F = np.mean(self.success_F)\n            self.F = np.clip(self.F, 0.1, 1.0)\n            self.success_F = []\n\n        if self.success_Cr:\n            self.Cr = np.mean(self.success_Cr)\n            self.Cr = np.clip(self.Cr, 0.1, 1.0)\n            self.success_Cr = []", "configspace": "", "generation": 8, "feedback": "The algorithm AdaptiveDESuccessFrequency scored 0.452 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["2fa45af8-e020-4e0b-9f07-43390b8ca505", "40bd89f0-32fe-4e6f-8887-5e032130b8e1"], "operator": "crossover", "metadata": {"llm_response": "# Description: Adaptive Differential Evolution with window-based parameter averaging and success-based parameter update frequency.\n\n# Code:\n```python\nimport numpy as np\n\nclass AdaptiveDESuccessFrequency:\n    def __init__(self, budget=10000, dim=10, pop_size=40, F_init=0.5, Cr_init=0.9, window_size=50):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F_init\n        self.Cr = Cr_init\n        self.window_size = window_size\n        self.success_F = []\n        self.success_Cr = []\n        self.F_history = []\n        self.Cr_history = []\n        self.success_history = []\n        self.update_frequency = 5  # Update F and Cr every n generations\n        self.generation_counter = 0\n\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in self.population])\n        self.n_evals = self.pop_size\n\n        while self.n_evals < self.budget:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_1, x_2, x_3 = self.population[idxs]\n                v = self.population[i] + self.F * (x_2 - x_3)\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                j_rand = np.random.randint(self.dim)\n                u = np.array([v[j] if (np.random.rand() < self.Cr or j == j_rand) else self.population[i, j] for j in range(self.dim)])\n\n                # Selection\n                f_u = func(u)\n                self.n_evals += 1\n\n                if f_u < fitness[i]:\n                    # Store successful F and Cr\n                    self.success_F.append(self.F)\n                    self.success_Cr.append(self.Cr)\n                    self.success_history.append(1)\n\n                    self.population[i] = u\n                    fitness[i] = f_u\n\n                    if f_u < self.f_opt:\n                        self.f_opt = f_u\n                        self.x_opt = u\n                else:\n                    self.success_history.append(0)\n\n                if self.n_evals >= self.budget:\n                    break\n\n            # Adapt F and Cr (every update_frequency generations)\n            self.generation_counter += 1\n            if self.generation_counter % self.update_frequency == 0:\n                self.adapt_parameters()\n\n        return self.f_opt, self.x_opt\n\n    def adapt_parameters(self):\n        # Window-based averaging for F and Cr\n        if self.success_F:\n            self.F = np.mean(self.success_F)\n            self.F = np.clip(self.F, 0.1, 1.0)\n            self.success_F = []\n\n        if self.success_Cr:\n            self.Cr = np.mean(self.success_Cr)\n            self.Cr = np.clip(self.Cr, 0.1, 1.0)\n            self.success_Cr = []\n```", "operator": "crossover", "theta_sampled": 0.0033469038549753995, "generation": 8, "aucs": [0.15574772175035545, 0.35070757189839485, 0.4143057150216366, 0.6730303224050334, 0.3343180680091302, 0.5186264360926571, 0.30958795682414, 0.37208405028805225, 0.3424205621638504, 0.19707427509045117, 0.6744793802666535, 0.9970185814505902, 0.43919420181192204, 0.3538716458494692, 0.7749542306014867, 0.48093371985944866, 0.36266839267396767, 0.6272924877042367, 0.17853374832930946, 0.48945052317850524]}, "task_prompt": ""}
{"id": "a6a11c7b-0e23-4a06-96da-4c73b330ade1", "fitness": "-inf", "name": "AdaptiveDEWindowWeightedMemory", "description": "Adaptive DE with window-weighted parameter adaptation and memory-based re-initialization for improved exploration.", "code": "import numpy as np\n\nclass AdaptiveDEWindowWeightedMemory:\n    def __init__(self, budget=10000, dim=10, pop_size=40, F_init=0.5, Cr_init=0.9, window_size=20, memory_size=5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F_init\n        self.Cr = Cr_init\n        self.window_size = window_size\n        self.memory_size = memory_size\n        self.success_F = []\n        self.success_Cr = []\n        self.weights = np.zeros(window_size) + 1e-6\n        self.F_memory = np.full(self.memory_size, F_init)\n        self.Cr_memory = np.full(self.memory_size, Cr_init)\n        self.F_idx = 0\n        self.Cr_idx = 0\n\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in self.population])\n        self.n_evals = self.pop_size\n\n        while self.n_evals < self.budget:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_1, x_2, x_3 = self.population[idxs]\n                v = self.population[i] + self.F * (x_2 - x_3)\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                j_rand = np.random.randint(self.dim)\n                u = np.array([v[j] if (np.random.rand() < self.Cr or j == j_rand) else self.population[i, j] for j in range(self.dim)])\n\n                # Selection\n                f_u = func(u)\n                self.n_evals += 1\n\n                if f_u < fitness[i]:\n                    # Store successful F and Cr\n                    self.success_F.append(self.F)\n                    self.success_Cr.append(self.Cr)\n\n                    self.population[i] = u\n                    fitness[i] = f_u\n\n                    if f_u < self.f_opt:\n                        self.f_opt = f_u\n                        self.x_opt = u\n                    \n                    self.weights = np.concatenate(([1.1], self.weights[:-1]))\n\n                else:\n                    self.weights = np.concatenate(([0.9], self.weights[:-1]))\n\n\n                if self.n_evals >= self.budget:\n                    break\n\n            # Adapt F and Cr (after each generation)\n            self.adapt_parameters()\n            self.reinitialize_parameters()\n\n\n        return self.f_opt, self.x_opt\n\n    def adapt_parameters(self):\n        # Window-based weighted averaging for F and Cr\n        if self.success_F:\n            recent_F = np.array(self.success_F[-self.window_size:])\n            recent_F_weights = self.weights[:len(recent_F)] / np.sum(self.weights[:len(recent_F)]) if len(recent_F) > 0 else np.array([1.0])\n            self.F = np.clip(np.sum(recent_F * recent_F_weights), 0.1, 1.0)\n            self.success_F = []\n\n        if self.success_Cr:\n            recent_Cr = np.array(self.success_Cr[-self.window_size:])\n            recent_Cr_weights = self.weights[:len(recent_Cr)] / np.sum(self.weights[:len(recent_Cr)]) if len(recent_Cr) > 0 else np.array([1.0])\n            self.Cr = np.clip(np.sum(recent_Cr * recent_Cr_weights), 0.1, 1.0)\n            self.success_Cr = []\n\n    def reinitialize_parameters(self):\n        if np.random.rand() < 0.1:  # Probability of re-initialization (e.g., 10%)\n             self.F = np.random.uniform(0.1, 0.9)\n             self.Cr = np.random.uniform(0.1, 0.9)", "configspace": "", "generation": 9, "feedback": "Evaluation timed out after 300 seconds.", "error": "Evaluation timed out after 300 seconds.", "parent_ids": ["fcdcbf48-6523-411e-bb4a-e8762f71fb52", "2fa45af8-e020-4e0b-9f07-43390b8ca505"], "operator": "crossover", "metadata": {"llm_response": "# Description: Adaptive DE with window-weighted parameter adaptation and memory-based re-initialization for improved exploration.\n# Code: \n```python\nimport numpy as np\n\nclass AdaptiveDEWindowWeightedMemory:\n    def __init__(self, budget=10000, dim=10, pop_size=40, F_init=0.5, Cr_init=0.9, window_size=20, memory_size=5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F_init\n        self.Cr = Cr_init\n        self.window_size = window_size\n        self.memory_size = memory_size\n        self.success_F = []\n        self.success_Cr = []\n        self.weights = np.zeros(window_size) + 1e-6\n        self.F_memory = np.full(self.memory_size, F_init)\n        self.Cr_memory = np.full(self.memory_size, Cr_init)\n        self.F_idx = 0\n        self.Cr_idx = 0\n\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in self.population])\n        self.n_evals = self.pop_size\n\n        while self.n_evals < self.budget:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_1, x_2, x_3 = self.population[idxs]\n                v = self.population[i] + self.F * (x_2 - x_3)\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                j_rand = np.random.randint(self.dim)\n                u = np.array([v[j] if (np.random.rand() < self.Cr or j == j_rand) else self.population[i, j] for j in range(self.dim)])\n\n                # Selection\n                f_u = func(u)\n                self.n_evals += 1\n\n                if f_u < fitness[i]:\n                    # Store successful F and Cr\n                    self.success_F.append(self.F)\n                    self.success_Cr.append(self.Cr)\n\n                    self.population[i] = u\n                    fitness[i] = f_u\n\n                    if f_u < self.f_opt:\n                        self.f_opt = f_u\n                        self.x_opt = u\n                    \n                    self.weights = np.concatenate(([1.1], self.weights[:-1]))\n\n                else:\n                    self.weights = np.concatenate(([0.9], self.weights[:-1]))\n\n\n                if self.n_evals >= self.budget:\n                    break\n\n            # Adapt F and Cr (after each generation)\n            self.adapt_parameters()\n            self.reinitialize_parameters()\n\n\n        return self.f_opt, self.x_opt\n\n    def adapt_parameters(self):\n        # Window-based weighted averaging for F and Cr\n        if self.success_F:\n            recent_F = np.array(self.success_F[-self.window_size:])\n            recent_F_weights = self.weights[:len(recent_F)] / np.sum(self.weights[:len(recent_F)]) if len(recent_F) > 0 else np.array([1.0])\n            self.F = np.clip(np.sum(recent_F * recent_F_weights), 0.1, 1.0)\n            self.success_F = []\n\n        if self.success_Cr:\n            recent_Cr = np.array(self.success_Cr[-self.window_size:])\n            recent_Cr_weights = self.weights[:len(recent_Cr)] / np.sum(self.weights[:len(recent_Cr)]) if len(recent_Cr) > 0 else np.array([1.0])\n            self.Cr = np.clip(np.sum(recent_Cr * recent_Cr_weights), 0.1, 1.0)\n            self.success_Cr = []\n\n    def reinitialize_parameters(self):\n        if np.random.rand() < 0.1:  # Probability of re-initialization (e.g., 10%)\n             self.F = np.random.uniform(0.1, 0.9)\n             self.Cr = np.random.uniform(0.1, 0.9)\n\n```", "operator": "crossover", "theta_sampled": 0.006185946803307903, "generation": 9}, "task_prompt": ""}
{"id": "a5f62931-6991-475c-b7ca-4ae0a70b1a84", "fitness": "-inf", "name": "AdaptiveDEWindowWeightedMemory", "description": "Adaptive Differential Evolution with weighted window for parameter adaptation and memory of successful F/Cr values for exploration.", "code": "import numpy as np\n\nclass AdaptiveDEWindowWeightedMemory:\n    def __init__(self, budget=10000, dim=10, pop_size=40, F_init=0.5, Cr_init=0.9, window_size=20, memory_size=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F_init\n        self.Cr = Cr_init\n        self.window_size = window_size\n        self.memory_size = memory_size\n        self.F_memory = np.full(self.memory_size, F_init)\n        self.Cr_memory = np.full(self.memory_size, Cr_init)\n        self.F_idx = 0\n        self.Cr_idx = 0\n        self.success_F = []\n        self.success_Cr = []\n        self.weights = np.zeros(window_size) + 1e-6\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in self.population])\n        self.n_evals = self.pop_size\n\n        while self.n_evals < self.budget:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_1, x_2, x_3 = self.population[idxs]\n                v = self.population[i] + self.F * (x_2 - x_3)\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                j_rand = np.random.randint(self.dim)\n                u = np.array([v[j] if (np.random.rand() < self.Cr or j == j_rand) else self.population[i, j] for j in range(self.dim)])\n\n                # Selection\n                f_u = func(u)\n                self.n_evals += 1\n\n                if f_u < fitness[i]:\n                    # Store successful F and Cr\n                    self.success_F.append(self.F)\n                    self.success_Cr.append(self.Cr)\n\n                    self.population[i] = u\n                    fitness[i] = f_u\n\n                    if f_u < self.f_opt:\n                        self.f_opt = f_u\n                        self.x_opt = u\n                    \n                    self.weights = np.concatenate(([1.1], self.weights[:-1]))\n\n                else:\n                    self.weights = np.concatenate(([0.9], self.weights[:-1]))\n\n                if self.n_evals >= self.budget:\n                    break\n\n            # Adapt F and Cr (after each generation)\n            self.adapt_parameters()\n\n        return self.f_opt, self.x_opt\n\n    def adapt_parameters(self):\n        # Window-based weighted averaging for F and Cr\n        if self.success_F:\n            recent_F = np.array(self.success_F[-self.window_size:])\n            recent_F_weights = self.weights[:len(recent_F)] / np.sum(self.weights[:len(recent_F)]) if len(recent_F) > 0 else np.array([1.0])\n            weighted_F = np.sum(recent_F * recent_F_weights)\n            self.F_memory[self.F_idx] = weighted_F\n            self.F_idx = (self.F_idx + 1) % self.memory_size\n            self.F = np.clip(np.mean(self.F_memory), 0.1, 1.0)\n            self.success_F = []\n\n        if self.success_Cr:\n            recent_Cr = np.array(self.success_Cr[-self.window_size:])\n            recent_Cr_weights = self.weights[:len(recent_Cr)] / np.sum(self.weights[:len(recent_Cr)]) if len(recent_Cr) > 0 else np.array([1.0])\n            weighted_Cr = np.sum(recent_Cr * recent_Cr_weights)\n            self.Cr_memory[self.Cr_idx] = weighted_Cr\n            self.Cr_idx = (self.Cr_idx + 1) % self.memory_size\n            self.Cr = np.clip(np.mean(self.Cr_memory), 0.1, 1.0)\n            self.success_Cr = []", "configspace": "", "generation": 9, "feedback": "Evaluation timed out after 300 seconds.", "error": "Evaluation timed out after 300 seconds.", "parent_ids": ["fcdcbf48-6523-411e-bb4a-e8762f71fb52", "2fa45af8-e020-4e0b-9f07-43390b8ca505"], "operator": "crossover", "metadata": {"llm_response": "# Description: Adaptive Differential Evolution with weighted window for parameter adaptation and memory of successful F/Cr values for exploration.\n# Code: \n```python\nimport numpy as np\n\nclass AdaptiveDEWindowWeightedMemory:\n    def __init__(self, budget=10000, dim=10, pop_size=40, F_init=0.5, Cr_init=0.9, window_size=20, memory_size=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F_init\n        self.Cr = Cr_init\n        self.window_size = window_size\n        self.memory_size = memory_size\n        self.F_memory = np.full(self.memory_size, F_init)\n        self.Cr_memory = np.full(self.memory_size, Cr_init)\n        self.F_idx = 0\n        self.Cr_idx = 0\n        self.success_F = []\n        self.success_Cr = []\n        self.weights = np.zeros(window_size) + 1e-6\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in self.population])\n        self.n_evals = self.pop_size\n\n        while self.n_evals < self.budget:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_1, x_2, x_3 = self.population[idxs]\n                v = self.population[i] + self.F * (x_2 - x_3)\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                j_rand = np.random.randint(self.dim)\n                u = np.array([v[j] if (np.random.rand() < self.Cr or j == j_rand) else self.population[i, j] for j in range(self.dim)])\n\n                # Selection\n                f_u = func(u)\n                self.n_evals += 1\n\n                if f_u < fitness[i]:\n                    # Store successful F and Cr\n                    self.success_F.append(self.F)\n                    self.success_Cr.append(self.Cr)\n\n                    self.population[i] = u\n                    fitness[i] = f_u\n\n                    if f_u < self.f_opt:\n                        self.f_opt = f_u\n                        self.x_opt = u\n                    \n                    self.weights = np.concatenate(([1.1], self.weights[:-1]))\n\n                else:\n                    self.weights = np.concatenate(([0.9], self.weights[:-1]))\n\n                if self.n_evals >= self.budget:\n                    break\n\n            # Adapt F and Cr (after each generation)\n            self.adapt_parameters()\n\n        return self.f_opt, self.x_opt\n\n    def adapt_parameters(self):\n        # Window-based weighted averaging for F and Cr\n        if self.success_F:\n            recent_F = np.array(self.success_F[-self.window_size:])\n            recent_F_weights = self.weights[:len(recent_F)] / np.sum(self.weights[:len(recent_F)]) if len(recent_F) > 0 else np.array([1.0])\n            weighted_F = np.sum(recent_F * recent_F_weights)\n            self.F_memory[self.F_idx] = weighted_F\n            self.F_idx = (self.F_idx + 1) % self.memory_size\n            self.F = np.clip(np.mean(self.F_memory), 0.1, 1.0)\n            self.success_F = []\n\n        if self.success_Cr:\n            recent_Cr = np.array(self.success_Cr[-self.window_size:])\n            recent_Cr_weights = self.weights[:len(recent_Cr)] / np.sum(self.weights[:len(recent_Cr)]) if len(recent_Cr) > 0 else np.array([1.0])\n            weighted_Cr = np.sum(recent_Cr * recent_Cr_weights)\n            self.Cr_memory[self.Cr_idx] = weighted_Cr\n            self.Cr_idx = (self.Cr_idx + 1) % self.memory_size\n            self.Cr = np.clip(np.mean(self.Cr_memory), 0.1, 1.0)\n            self.success_Cr = []\n```", "operator": "crossover", "theta_sampled": 0.003164344283567122, "generation": 9}, "task_prompt": ""}
{"id": "8b0957b5-bef6-4c2f-a596-8577bd7becb0", "fitness": "-inf", "name": "AdaptiveDEWindowMemoryWeighted", "description": "Adaptive Differential Evolution with window-weighted parameter adaptation and memory of successful parameters.", "code": "import numpy as np\n\nclass AdaptiveDEWindowMemoryWeighted:\n    def __init__(self, budget=10000, dim=10, pop_size=40, F_init=0.5, Cr_init=0.9, window_size=20, memory_size=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F_init\n        self.Cr = Cr_init\n        self.window_size = window_size\n        self.memory_size = memory_size\n        self.F_memory = np.full(self.memory_size, F_init)\n        self.Cr_memory = np.full(self.memory_size, Cr_init)\n        self.F_idx = 0\n        self.Cr_idx = 0\n        self.success_F = []\n        self.success_Cr = []\n        self.weights = np.zeros(self.window_size) + 1e-6\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in self.population])\n        self.n_evals = self.pop_size\n\n        while self.n_evals < self.budget:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_1, x_2, x_3 = self.population[idxs]\n                v = self.population[i] + self.F * (x_2 - x_3)\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                j_rand = np.random.randint(self.dim)\n                u = np.array([v[j] if (np.random.rand() < self.Cr or j == j_rand) else self.population[i, j] for j in range(self.dim)])\n\n                # Selection\n                f_u = func(u)\n                self.n_evals += 1\n\n                if f_u < fitness[i]:\n                    # Store successful F and Cr\n                    self.success_F.append(self.F)\n                    self.success_Cr.append(self.Cr)\n\n                    self.population[i] = u\n                    fitness[i] = f_u\n\n                    if f_u < self.f_opt:\n                        self.f_opt = f_u\n                        self.x_opt = u\n                    \n                    self.weights = np.concatenate(([1.1], self.weights[:-1]))\n\n                else:\n                    self.weights = np.concatenate(([0.9], self.weights[:-1]))\n\n\n                if self.n_evals >= self.budget:\n                    break\n\n            # Adapt F and Cr (after each generation)\n            self.adapt_parameters()\n\n        return self.f_opt, self.x_opt\n\n    def adapt_parameters(self):\n        # Window-based weighted averaging for F and Cr\n        if self.success_F:\n            # Update memory with mean of successful F values\n            self.F_memory[self.F_idx] = np.mean(self.success_F)\n            self.F_idx = (self.F_idx + 1) % self.memory_size\n\n            #Calculate weighted average using the memory\n            recent_F = self.F_memory\n            recent_F_weights = self.weights[:len(recent_F)] / np.sum(self.weights[:len(recent_F)]) if len(recent_F) > 0 else np.array([1.0/self.memory_size]*self.memory_size)\n            self.F = np.clip(np.sum(recent_F * recent_F_weights), 0.1, 1.0)\n\n            self.success_F = []\n\n        if self.success_Cr:\n            # Update memory with mean of successful Cr values\n            self.Cr_memory[self.Cr_idx] = np.mean(self.success_Cr)\n            self.Cr_idx = (self.Cr_idx + 1) % self.memory_size\n\n            #Calculate weighted average using memory\n            recent_Cr = self.Cr_memory\n            recent_Cr_weights = self.weights[:len(recent_Cr)] / np.sum(self.weights[:len(recent_Cr)]) if len(recent_Cr) > 0 else np.array([1.0/self.memory_size]*self.memory_size)\n            self.Cr = np.clip(np.sum(recent_Cr * recent_Cr_weights), 0.1, 1.0)\n\n            self.success_Cr = []", "configspace": "", "generation": 9, "feedback": "Evaluation timed out after 300 seconds.", "error": "Evaluation timed out after 300 seconds.", "parent_ids": ["fcdcbf48-6523-411e-bb4a-e8762f71fb52", "2fa45af8-e020-4e0b-9f07-43390b8ca505"], "operator": "crossover", "metadata": {"llm_response": "# Description: Adaptive Differential Evolution with window-weighted parameter adaptation and memory of successful parameters.\n# Code: \n```python\nimport numpy as np\n\nclass AdaptiveDEWindowMemoryWeighted:\n    def __init__(self, budget=10000, dim=10, pop_size=40, F_init=0.5, Cr_init=0.9, window_size=20, memory_size=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F_init\n        self.Cr = Cr_init\n        self.window_size = window_size\n        self.memory_size = memory_size\n        self.F_memory = np.full(self.memory_size, F_init)\n        self.Cr_memory = np.full(self.memory_size, Cr_init)\n        self.F_idx = 0\n        self.Cr_idx = 0\n        self.success_F = []\n        self.success_Cr = []\n        self.weights = np.zeros(self.window_size) + 1e-6\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in self.population])\n        self.n_evals = self.pop_size\n\n        while self.n_evals < self.budget:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_1, x_2, x_3 = self.population[idxs]\n                v = self.population[i] + self.F * (x_2 - x_3)\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                j_rand = np.random.randint(self.dim)\n                u = np.array([v[j] if (np.random.rand() < self.Cr or j == j_rand) else self.population[i, j] for j in range(self.dim)])\n\n                # Selection\n                f_u = func(u)\n                self.n_evals += 1\n\n                if f_u < fitness[i]:\n                    # Store successful F and Cr\n                    self.success_F.append(self.F)\n                    self.success_Cr.append(self.Cr)\n\n                    self.population[i] = u\n                    fitness[i] = f_u\n\n                    if f_u < self.f_opt:\n                        self.f_opt = f_u\n                        self.x_opt = u\n                    \n                    self.weights = np.concatenate(([1.1], self.weights[:-1]))\n\n                else:\n                    self.weights = np.concatenate(([0.9], self.weights[:-1]))\n\n\n                if self.n_evals >= self.budget:\n                    break\n\n            # Adapt F and Cr (after each generation)\n            self.adapt_parameters()\n\n        return self.f_opt, self.x_opt\n\n    def adapt_parameters(self):\n        # Window-based weighted averaging for F and Cr\n        if self.success_F:\n            # Update memory with mean of successful F values\n            self.F_memory[self.F_idx] = np.mean(self.success_F)\n            self.F_idx = (self.F_idx + 1) % self.memory_size\n\n            #Calculate weighted average using the memory\n            recent_F = self.F_memory\n            recent_F_weights = self.weights[:len(recent_F)] / np.sum(self.weights[:len(recent_F)]) if len(recent_F) > 0 else np.array([1.0/self.memory_size]*self.memory_size)\n            self.F = np.clip(np.sum(recent_F * recent_F_weights), 0.1, 1.0)\n\n            self.success_F = []\n\n        if self.success_Cr:\n            # Update memory with mean of successful Cr values\n            self.Cr_memory[self.Cr_idx] = np.mean(self.success_Cr)\n            self.Cr_idx = (self.Cr_idx + 1) % self.memory_size\n\n            #Calculate weighted average using memory\n            recent_Cr = self.Cr_memory\n            recent_Cr_weights = self.weights[:len(recent_Cr)] / np.sum(self.weights[:len(recent_Cr)]) if len(recent_Cr) > 0 else np.array([1.0/self.memory_size]*self.memory_size)\n            self.Cr = np.clip(np.sum(recent_Cr * recent_Cr_weights), 0.1, 1.0)\n\n            self.success_Cr = []\n```", "operator": "crossover", "theta_sampled": 0.009644909989106476, "generation": 9}, "task_prompt": ""}
{"id": "02e16495-6211-41b3-a92f-9f70a23b30ed", "fitness": 0.44969361930171087, "name": "AdaptiveDEDualWindow", "description": "Adaptive Differential Evolution with success-based parameter adaptation and a dual window for exploration and exploitation.", "code": "import numpy as np\n\nclass AdaptiveDEDualWindow:\n    def __init__(self, budget=10000, dim=10, pop_size=40, F_init=0.5, Cr_init=0.9, window_size_success=20, window_size_weights=20):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F_init\n        self.Cr = Cr_init\n        self.window_size_success = window_size_success\n        self.window_size_weights = window_size_weights\n        self.success_F = []\n        self.success_Cr = []\n        self.weights = np.zeros(window_size_weights) + 1e-6\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in self.population])\n        self.n_evals = self.pop_size\n\n        while self.n_evals < self.budget:\n            for i in range(self.pop_size):\n                # Adaptation of F and Cr based on recent success\n                if self.success_F:\n                    recent_F = np.array(self.success_F[-self.window_size_success:])\n                    self.F = np.clip(np.mean(recent_F), 0.1, 1.0)  # Simple mean for F\n\n                if self.success_Cr:\n                    recent_Cr = np.array(self.success_Cr[-self.window_size_success:])\n                    self.Cr = np.clip(np.mean(recent_Cr), 0.1, 1.0)  # Simple mean for Cr\n                \n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_1, x_2, x_3 = self.population[idxs]\n                v = self.population[i] + self.F * (x_2 - x_3)\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                j_rand = np.random.randint(self.dim)\n                u = np.array([v[j] if (np.random.rand() < self.Cr or j == j_rand) else self.population[i, j] for j in range(self.dim)])\n\n                # Selection\n                f_u = func(u)\n                self.n_evals += 1\n\n                if f_u < fitness[i]:\n                    # Store successful F and Cr\n                    self.success_F.append(self.F)\n                    self.success_Cr.append(self.Cr)\n\n                    self.population[i] = u\n                    fitness[i] = f_u\n\n                    if f_u < self.f_opt:\n                        self.f_opt = f_u\n                        self.x_opt = u\n                    \n                    self.weights = np.concatenate(([1.1], self.weights[:-1]))\n\n                else:\n                    self.weights = np.concatenate(([0.9], self.weights[:-1]))\n                    \n                if len(self.success_F) > self.window_size_success:\n                    self.success_F.pop(0)\n                    self.success_Cr.pop(0)\n                \n                if self.n_evals >= self.budget:\n                    break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 9, "feedback": "The algorithm AdaptiveDEDualWindow scored 0.450 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["fcdcbf48-6523-411e-bb4a-e8762f71fb52", "db0a7488-5693-4570-8097-896a973058db"], "operator": "crossover", "metadata": {"llm_response": "# Description: Adaptive Differential Evolution with success-based parameter adaptation and a dual window for exploration and exploitation.\n# Code:\n```python\nimport numpy as np\n\nclass AdaptiveDEDualWindow:\n    def __init__(self, budget=10000, dim=10, pop_size=40, F_init=0.5, Cr_init=0.9, window_size_success=20, window_size_weights=20):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F_init\n        self.Cr = Cr_init\n        self.window_size_success = window_size_success\n        self.window_size_weights = window_size_weights\n        self.success_F = []\n        self.success_Cr = []\n        self.weights = np.zeros(window_size_weights) + 1e-6\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in self.population])\n        self.n_evals = self.pop_size\n\n        while self.n_evals < self.budget:\n            for i in range(self.pop_size):\n                # Adaptation of F and Cr based on recent success\n                if self.success_F:\n                    recent_F = np.array(self.success_F[-self.window_size_success:])\n                    self.F = np.clip(np.mean(recent_F), 0.1, 1.0)  # Simple mean for F\n\n                if self.success_Cr:\n                    recent_Cr = np.array(self.success_Cr[-self.window_size_success:])\n                    self.Cr = np.clip(np.mean(recent_Cr), 0.1, 1.0)  # Simple mean for Cr\n                \n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_1, x_2, x_3 = self.population[idxs]\n                v = self.population[i] + self.F * (x_2 - x_3)\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                j_rand = np.random.randint(self.dim)\n                u = np.array([v[j] if (np.random.rand() < self.Cr or j == j_rand) else self.population[i, j] for j in range(self.dim)])\n\n                # Selection\n                f_u = func(u)\n                self.n_evals += 1\n\n                if f_u < fitness[i]:\n                    # Store successful F and Cr\n                    self.success_F.append(self.F)\n                    self.success_Cr.append(self.Cr)\n\n                    self.population[i] = u\n                    fitness[i] = f_u\n\n                    if f_u < self.f_opt:\n                        self.f_opt = f_u\n                        self.x_opt = u\n                    \n                    self.weights = np.concatenate(([1.1], self.weights[:-1]))\n\n                else:\n                    self.weights = np.concatenate(([0.9], self.weights[:-1]))\n                    \n                if len(self.success_F) > self.window_size_success:\n                    self.success_F.pop(0)\n                    self.success_Cr.pop(0)\n                \n                if self.n_evals >= self.budget:\n                    break\n\n        return self.f_opt, self.x_opt\n```", "operator": "crossover", "theta_sampled": 0.002736941009665182, "generation": 9, "aucs": [0.15895488326102836, 0.31419983156305775, 0.43722639475307346, 0.6011335326692053, 0.3566027189490987, 0.5405311950476239, 0.308154267873191, 0.3801521438011446, 0.3736358946684143, 0.20569563365469934, 0.6822536319478325, 0.9980762368292414, 0.426766349446725, 0.34173857904304106, 0.7596521625729723, 0.4857245252331478, 0.34987676460611705, 0.6042387323876619, 0.17605422902756962, 0.4932046786993728]}, "task_prompt": ""}
{"id": "02a3fc44-7f1e-4c3a-aab8-042c2fa04b74", "fitness": 0.4555126105890081, "name": "AdaptiveDEWindowSimple", "description": "Adaptive Differential Evolution with a simplified parameter adaptation scheme based on success history and a fixed window size.", "code": "import numpy as np\n\nclass AdaptiveDEWindowSimple:\n    def __init__(self, budget=10000, dim=10, pop_size=40, F_init=0.5, Cr_init=0.9, window_size=20):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F_init\n        self.Cr = Cr_init\n        self.window_size = window_size\n        self.success_F = []\n        self.success_Cr = []\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in self.population])\n        self.n_evals = self.pop_size\n\n        while self.n_evals < self.budget:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_1, x_2, x_3 = self.population[idxs]\n                v = self.population[i] + self.F * (x_2 - x_3)\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                j_rand = np.random.randint(self.dim)\n                u = np.array([v[j] if (np.random.rand() < self.Cr or j == j_rand) else self.population[i, j] for j in range(self.dim)])\n\n                # Selection\n                f_u = func(u)\n                self.n_evals += 1\n\n                if f_u < fitness[i]:\n                    # Store successful F and Cr\n                    self.success_F.append(self.F)\n                    self.success_Cr.append(self.Cr)\n\n                    self.population[i] = u\n                    fitness[i] = f_u\n\n                    if f_u < self.f_opt:\n                        self.f_opt = f_u\n                        self.x_opt = u\n\n                if self.n_evals >= self.budget:\n                    break\n\n            # Adapt F and Cr (after each generation)\n            self.adapt_parameters()\n\n        return self.f_opt, self.x_opt\n\n    def adapt_parameters(self):\n        # Window-based averaging for F and Cr\n        if self.success_F:\n            recent_F = self.success_F[-self.window_size:]\n            self.F = np.clip(np.mean(recent_F), 0.1, 1.0) if recent_F else self.F\n            self.success_F = []\n\n        if self.success_Cr:\n            recent_Cr = self.success_Cr[-self.window_size:]\n            self.Cr = np.clip(np.mean(recent_Cr), 0.1, 1.0) if recent_Cr else self.Cr\n            self.success_Cr = []", "configspace": "", "generation": 9, "feedback": "The algorithm AdaptiveDEWindowSimple scored 0.456 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["fcdcbf48-6523-411e-bb4a-e8762f71fb52"], "operator": "simplify", "metadata": {"llm_response": "# Description: Adaptive Differential Evolution with a simplified parameter adaptation scheme based on success history and a fixed window size.\n# Code: \n```python\nimport numpy as np\n\nclass AdaptiveDEWindowSimple:\n    def __init__(self, budget=10000, dim=10, pop_size=40, F_init=0.5, Cr_init=0.9, window_size=20):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F_init\n        self.Cr = Cr_init\n        self.window_size = window_size\n        self.success_F = []\n        self.success_Cr = []\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in self.population])\n        self.n_evals = self.pop_size\n\n        while self.n_evals < self.budget:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_1, x_2, x_3 = self.population[idxs]\n                v = self.population[i] + self.F * (x_2 - x_3)\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                j_rand = np.random.randint(self.dim)\n                u = np.array([v[j] if (np.random.rand() < self.Cr or j == j_rand) else self.population[i, j] for j in range(self.dim)])\n\n                # Selection\n                f_u = func(u)\n                self.n_evals += 1\n\n                if f_u < fitness[i]:\n                    # Store successful F and Cr\n                    self.success_F.append(self.F)\n                    self.success_Cr.append(self.Cr)\n\n                    self.population[i] = u\n                    fitness[i] = f_u\n\n                    if f_u < self.f_opt:\n                        self.f_opt = f_u\n                        self.x_opt = u\n\n                if self.n_evals >= self.budget:\n                    break\n\n            # Adapt F and Cr (after each generation)\n            self.adapt_parameters()\n\n        return self.f_opt, self.x_opt\n\n    def adapt_parameters(self):\n        # Window-based averaging for F and Cr\n        if self.success_F:\n            recent_F = self.success_F[-self.window_size:]\n            self.F = np.clip(np.mean(recent_F), 0.1, 1.0) if recent_F else self.F\n            self.success_F = []\n\n        if self.success_Cr:\n            recent_Cr = self.success_Cr[-self.window_size:]\n            self.Cr = np.clip(np.mean(recent_Cr), 0.1, 1.0) if recent_Cr else self.Cr\n            self.success_Cr = []\n```", "operator": "simplify", "theta_sampled": 0.005837168523193161, "generation": 9, "aucs": [0.16330542510109058, 0.3408165972365642, 0.4108189041988647, 0.6341054196458329, 0.3477459907175937, 0.5161677065008936, 0.29700607947495095, 0.38485031895390653, 0.3863960307725476, 0.2001072711903974, 0.7101528465622624, 0.9963617072197108, 0.43882892098054904, 0.33649264826744607, 0.8279761802733292, 0.47826483725142166, 0.36389602494678897, 0.6057881205640896, 0.18167651702132093, 0.48949466490060056]}, "task_prompt": ""}
{"id": "d67de366-6e6f-43de-b06b-fa3c0046ad31", "fitness": "-inf", "name": "AdaptiveDEWindowMemoryWeighted", "description": "Adaptive Differential Evolution with weighted updates and memory for historical success rates of F and Cr.", "code": "import numpy as np\n\nclass AdaptiveDEWindowMemoryWeighted:\n    def __init__(self, budget=10000, dim=10, pop_size=40, F_init=0.5, Cr_init=0.9, window_size=20, memory_size=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F_init\n        self.Cr = Cr_init\n        self.window_size = window_size\n        self.memory_size = memory_size\n        self.F_memory = np.full(self.memory_size, F_init)\n        self.Cr_memory = np.full(self.memory_size, Cr_init)\n        self.F_idx = 0\n        self.Cr_idx = 0\n        self.success_F = []\n        self.success_Cr = []\n        self.weights = np.zeros(window_size) + 1e-6\n\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in self.population])\n        self.n_evals = self.pop_size\n\n        while self.n_evals < self.budget:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_1, x_2, x_3 = self.population[idxs]\n                v = self.population[i] + self.F * (x_2 - x_3)\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                j_rand = np.random.randint(self.dim)\n                u = np.array([v[j] if (np.random.rand() < self.Cr or j == j_rand) else self.population[i, j] for j in range(self.dim)])\n\n                # Selection\n                f_u = func(u)\n                self.n_evals += 1\n\n                if f_u < fitness[i]:\n                    # Store successful F and Cr\n                    self.success_F.append(self.F)\n                    self.success_Cr.append(self.Cr)\n\n                    self.population[i] = u\n                    fitness[i] = f_u\n\n                    if f_u < self.f_opt:\n                        self.f_opt = f_u\n                        self.x_opt = u\n\n                    self.weights = np.concatenate(([1.1], self.weights[:-1]))\n\n                else:\n                    self.weights = np.concatenate(([0.9], self.weights[:-1]))\n\n                if self.n_evals >= self.budget:\n                    break\n\n            # Adapt F and Cr (after each generation)\n            self.adapt_parameters()\n\n        return self.f_opt, self.x_opt\n\n    def adapt_parameters(self):\n         # Window-based weighted averaging for successful F and Cr values, combined with memory.\n        if self.success_F:\n            recent_F = np.array(self.success_F[-self.window_size:])\n            recent_F_weights = self.weights[:len(recent_F)] / np.sum(self.weights[:len(recent_F)]) if len(recent_F) > 0 else np.array([1.0])\n            weighted_avg_F = np.sum(recent_F * recent_F_weights)\n            self.F_memory[self.F_idx] = weighted_avg_F\n            self.F_idx = (self.F_idx + 1) % self.memory_size\n            self.F = np.mean(self.F_memory)\n            self.F = np.clip(self.F, 0.1, 1.0)\n            self.success_F = []\n\n        if self.success_Cr:\n            recent_Cr = np.array(self.success_Cr[-self.window_size:])\n            recent_Cr_weights = self.weights[:len(recent_Cr)] / np.sum(self.weights[:len(recent_Cr)]) if len(recent_Cr) > 0 else np.array([1.0])\n            weighted_avg_Cr = np.sum(recent_Cr * recent_Cr_weights)\n            self.Cr_memory[self.Cr_idx] = weighted_avg_Cr\n            self.Cr_idx = (self.Cr_idx + 1) % self.memory_size\n            self.Cr = np.mean(self.Cr_memory)\n            self.Cr = np.clip(self.Cr, 0.1, 1.0)\n            self.success_Cr = []", "configspace": "", "generation": 9, "feedback": "Evaluation timed out after 300 seconds.", "error": "Evaluation timed out after 300 seconds.", "parent_ids": ["fcdcbf48-6523-411e-bb4a-e8762f71fb52", "2fa45af8-e020-4e0b-9f07-43390b8ca505"], "operator": "crossover", "metadata": {"llm_response": "# Description: Adaptive Differential Evolution with weighted updates and memory for historical success rates of F and Cr.\n# Code: \n```python\nimport numpy as np\n\nclass AdaptiveDEWindowMemoryWeighted:\n    def __init__(self, budget=10000, dim=10, pop_size=40, F_init=0.5, Cr_init=0.9, window_size=20, memory_size=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F_init\n        self.Cr = Cr_init\n        self.window_size = window_size\n        self.memory_size = memory_size\n        self.F_memory = np.full(self.memory_size, F_init)\n        self.Cr_memory = np.full(self.memory_size, Cr_init)\n        self.F_idx = 0\n        self.Cr_idx = 0\n        self.success_F = []\n        self.success_Cr = []\n        self.weights = np.zeros(window_size) + 1e-6\n\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in self.population])\n        self.n_evals = self.pop_size\n\n        while self.n_evals < self.budget:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_1, x_2, x_3 = self.population[idxs]\n                v = self.population[i] + self.F * (x_2 - x_3)\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                j_rand = np.random.randint(self.dim)\n                u = np.array([v[j] if (np.random.rand() < self.Cr or j == j_rand) else self.population[i, j] for j in range(self.dim)])\n\n                # Selection\n                f_u = func(u)\n                self.n_evals += 1\n\n                if f_u < fitness[i]:\n                    # Store successful F and Cr\n                    self.success_F.append(self.F)\n                    self.success_Cr.append(self.Cr)\n\n                    self.population[i] = u\n                    fitness[i] = f_u\n\n                    if f_u < self.f_opt:\n                        self.f_opt = f_u\n                        self.x_opt = u\n\n                    self.weights = np.concatenate(([1.1], self.weights[:-1]))\n\n                else:\n                    self.weights = np.concatenate(([0.9], self.weights[:-1]))\n\n                if self.n_evals >= self.budget:\n                    break\n\n            # Adapt F and Cr (after each generation)\n            self.adapt_parameters()\n\n        return self.f_opt, self.x_opt\n\n    def adapt_parameters(self):\n         # Window-based weighted averaging for successful F and Cr values, combined with memory.\n        if self.success_F:\n            recent_F = np.array(self.success_F[-self.window_size:])\n            recent_F_weights = self.weights[:len(recent_F)] / np.sum(self.weights[:len(recent_F)]) if len(recent_F) > 0 else np.array([1.0])\n            weighted_avg_F = np.sum(recent_F * recent_F_weights)\n            self.F_memory[self.F_idx] = weighted_avg_F\n            self.F_idx = (self.F_idx + 1) % self.memory_size\n            self.F = np.mean(self.F_memory)\n            self.F = np.clip(self.F, 0.1, 1.0)\n            self.success_F = []\n\n        if self.success_Cr:\n            recent_Cr = np.array(self.success_Cr[-self.window_size:])\n            recent_Cr_weights = self.weights[:len(recent_Cr)] / np.sum(self.weights[:len(recent_Cr)]) if len(recent_Cr) > 0 else np.array([1.0])\n            weighted_avg_Cr = np.sum(recent_Cr * recent_Cr_weights)\n            self.Cr_memory[self.Cr_idx] = weighted_avg_Cr\n            self.Cr_idx = (self.Cr_idx + 1) % self.memory_size\n            self.Cr = np.mean(self.Cr_memory)\n            self.Cr = np.clip(self.Cr, 0.1, 1.0)\n            self.success_Cr = []\n```", "operator": "crossover", "theta_sampled": 0.0017007400281672466, "generation": 9}, "task_prompt": ""}
{"id": "16cf4ad5-3c68-408e-9139-36d1a25d72b8", "fitness": "-inf", "name": "PSODE", "description": "Population-based algorithm that combines aspects of particle swarm optimization with differential evolution, dynamically adjusting parameters based on swarm diversity.", "code": "import numpy as np\n\nclass PSODE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, w=0.7, c1=1.5, c2=1.5, f=0.8, cr=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.w = w  # Inertia weight\n        self.c1 = c1  # Cognitive coefficient\n        self.c2 = c2  # Social coefficient\n        self.f = f  # Differential evolution F parameter\n        self.cr = cr  # Differential evolution CR parameter\n        self.population = None\n        self.velocities = None\n        self.fitness = None\n        self.personal_best_positions = None\n        self.personal_best_fitness = None\n        self.global_best_position = None\n        self.global_best_fitness = np.inf\n        self.eval_count = 0\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, size=(self.pop_size, self.dim))  # Initialize velocities\n        self.fitness = np.array([func(x) for x in self.population])\n        self.eval_count += self.pop_size\n        self.personal_best_positions = self.population.copy()\n        self.personal_best_fitness = self.fitness.copy()\n        self.global_best_position = self.population[np.argmin(self.fitness)].copy()\n        self.global_best_fitness = np.min(self.fitness)\n    \n    def update_parameters(self):\n        # Adjust parameters based on population diversity\n        diversity = np.std(self.population)\n        \n        # Dynamically adjust w, c1, c2\n        self.w = 0.7 + 0.2 * np.tanh(diversity)\n        self.c1 = 1.5 - 0.5 * np.tanh(diversity)\n        self.c2 = 1.5 - 0.5 * np.tanh(diversity)\n        \n        # Dynamically adjust F and CR\n        self.f = 0.8 + 0.1 * np.tanh(diversity)\n        self.cr = 0.9 - 0.1 * np.tanh(diversity)\n        \n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.eval_count < self.budget:\n            self.update_parameters()  # Adapt parameters\n            for i in range(self.pop_size):\n                # PSO update\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                self.velocities[i] = (self.w * self.velocities[i] +\n                                      self.c1 * r1 * (self.personal_best_positions[i] - self.population[i]) +\n                                      self.c2 * r2 * (self.global_best_position - self.population[i]))\n\n                # DE mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x1, x2, x3 = self.population[idxs]\n                v = x1 + self.f * (x2 - x3)\n\n                # Crossover\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() > self.cr and j != j_rand:\n                        v[j] = self.population[i, j]\n\n                # Clip to bounds\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n\n                # Evaluate\n                f_v = func(v)\n                self.eval_count += 1\n\n                # Update individual and global best\n                if f_v < self.personal_best_fitness[i]:\n                    self.personal_best_fitness[i] = f_v\n                    self.personal_best_positions[i] = v.copy()\n\n                    if f_v < self.global_best_fitness:\n                        self.global_best_fitness = f_v\n                        self.global_best_position = v.copy()\n\n                # Selection: Replace individual with mutated vector if it's better\n                if f_v < self.fitness[i]:\n                    self.population[i] = v.copy()\n                    self.fitness[i] = f_v\n                \n\n                if self.eval_count >= self.budget:\n                    break\n\n        return self.global_best_fitness, self.global_best_position", "configspace": "", "generation": 9, "feedback": "Evaluation timed out after 300 seconds.", "error": "Evaluation timed out after 300 seconds.", "parent_ids": [], "operator": "random_new", "metadata": {"llm_response": "# Description: Population-based algorithm that combines aspects of particle swarm optimization with differential evolution, dynamically adjusting parameters based on swarm diversity.\n# Code: \n```python\nimport numpy as np\n\nclass PSODE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, w=0.7, c1=1.5, c2=1.5, f=0.8, cr=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.w = w  # Inertia weight\n        self.c1 = c1  # Cognitive coefficient\n        self.c2 = c2  # Social coefficient\n        self.f = f  # Differential evolution F parameter\n        self.cr = cr  # Differential evolution CR parameter\n        self.population = None\n        self.velocities = None\n        self.fitness = None\n        self.personal_best_positions = None\n        self.personal_best_fitness = None\n        self.global_best_position = None\n        self.global_best_fitness = np.inf\n        self.eval_count = 0\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, size=(self.pop_size, self.dim))  # Initialize velocities\n        self.fitness = np.array([func(x) for x in self.population])\n        self.eval_count += self.pop_size\n        self.personal_best_positions = self.population.copy()\n        self.personal_best_fitness = self.fitness.copy()\n        self.global_best_position = self.population[np.argmin(self.fitness)].copy()\n        self.global_best_fitness = np.min(self.fitness)\n    \n    def update_parameters(self):\n        # Adjust parameters based on population diversity\n        diversity = np.std(self.population)\n        \n        # Dynamically adjust w, c1, c2\n        self.w = 0.7 + 0.2 * np.tanh(diversity)\n        self.c1 = 1.5 - 0.5 * np.tanh(diversity)\n        self.c2 = 1.5 - 0.5 * np.tanh(diversity)\n        \n        # Dynamically adjust F and CR\n        self.f = 0.8 + 0.1 * np.tanh(diversity)\n        self.cr = 0.9 - 0.1 * np.tanh(diversity)\n        \n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.eval_count < self.budget:\n            self.update_parameters()  # Adapt parameters\n            for i in range(self.pop_size):\n                # PSO update\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                self.velocities[i] = (self.w * self.velocities[i] +\n                                      self.c1 * r1 * (self.personal_best_positions[i] - self.population[i]) +\n                                      self.c2 * r2 * (self.global_best_position - self.population[i]))\n\n                # DE mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x1, x2, x3 = self.population[idxs]\n                v = x1 + self.f * (x2 - x3)\n\n                # Crossover\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() > self.cr and j != j_rand:\n                        v[j] = self.population[i, j]\n\n                # Clip to bounds\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n\n                # Evaluate\n                f_v = func(v)\n                self.eval_count += 1\n\n                # Update individual and global best\n                if f_v < self.personal_best_fitness[i]:\n                    self.personal_best_fitness[i] = f_v\n                    self.personal_best_positions[i] = v.copy()\n\n                    if f_v < self.global_best_fitness:\n                        self.global_best_fitness = f_v\n                        self.global_best_position = v.copy()\n\n                # Selection: Replace individual with mutated vector if it's better\n                if f_v < self.fitness[i]:\n                    self.population[i] = v.copy()\n                    self.fitness[i] = f_v\n                \n\n                if self.eval_count >= self.budget:\n                    break\n\n        return self.global_best_fitness, self.global_best_position\n```", "operator": "random_new", "theta_sampled": 0.006993836382102341, "generation": 9}, "task_prompt": ""}
{"id": "13c7de64-3298-4550-93e3-428b7f9ace9d", "fitness": 0.5606717280705357, "name": "AdaptiveDESelf", "description": "Self-adaptive Differential Evolution with weighted parameter adaptation and improved exploration.", "code": "import numpy as np\n\nclass AdaptiveDESelf:\n    def __init__(self, budget=10000, dim=10, pop_size=40, F_init=0.5, Cr_init=0.9, adapt_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F_init * np.ones(pop_size)  # Individual F values\n        self.Cr = Cr_init * np.ones(pop_size) # Individual Cr values\n        self.adapt_rate = adapt_rate\n        self.archive = [] # Archive for storing discarded solutions\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in self.population])\n        self.n_evals = self.pop_size\n        \n        while self.n_evals < self.budget:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 4, replace=False) # Sample 4 distinct indices\n                x_1, x_2, x_3, x_4 = self.population[idxs]\n                \n                # DE/rand/1 scheme with archive consideration to enhance exploration\n                if np.random.rand() < 0.2 and len(self.archive) > 0: # 20% chance to use archive\n                    arch_idx = np.random.randint(len(self.archive))\n                    x_4 = self.archive[arch_idx] \n\n                v = self.population[i] + self.F[i] * (x_2 - x_3) + self.F[i] * (x_4 - self.population[i]) # DE/rand/1\n\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                j_rand = np.random.randint(self.dim)\n                u = np.array([v[j] if (np.random.rand() < self.Cr[i] or j == j_rand) else self.population[i, j] for j in range(self.dim)])\n\n                # Selection\n                f_u = func(u)\n                self.n_evals += 1\n\n                if f_u < fitness[i]:\n                    # Update successful F and Cr with a simple adaptation rule\n                    self.F[i] = self.F[i] * (1 - self.adapt_rate) + np.random.rand() * self.adapt_rate\n                    self.Cr[i] = self.Cr[i] * (1 - self.adapt_rate) + np.random.rand() * self.adapt_rate\n\n                    self.archive.append(self.population[i])  # Add replaced solution to archive\n                    if len(self.archive) > self.pop_size:\n                         self.archive.pop(0) # Maintain archive size\n                    \n                    self.population[i] = u\n                    fitness[i] = f_u\n\n                    if f_u < self.f_opt:\n                        self.f_opt = f_u\n                        self.x_opt = u\n                        \n                else:\n                    # If the trial vector is worse, adapt F and Cr, but with less intensity\n                    self.F[i] = self.F[i] * (1 - self.adapt_rate * 0.5) + np.random.rand() * self.adapt_rate * 0.5\n                    self.Cr[i] = self.Cr[i] * (1 - self.adapt_rate * 0.5) + np.random.rand() * self.adapt_rate * 0.5\n\n                self.F[i] = np.clip(self.F[i], 0.1, 1.0)\n                self.Cr[i] = np.clip(self.Cr[i], 0.1, 1.0)\n\n                if self.n_evals >= self.budget:\n                    break\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 9, "feedback": "The algorithm AdaptiveDESelf scored 0.561 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["fcdcbf48-6523-411e-bb4a-e8762f71fb52"], "operator": "simplify", "metadata": {"llm_response": "# Description: Self-adaptive Differential Evolution with weighted parameter adaptation and improved exploration.\n# Code: \n```python\nimport numpy as np\n\nclass AdaptiveDESelf:\n    def __init__(self, budget=10000, dim=10, pop_size=40, F_init=0.5, Cr_init=0.9, adapt_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F_init * np.ones(pop_size)  # Individual F values\n        self.Cr = Cr_init * np.ones(pop_size) # Individual Cr values\n        self.adapt_rate = adapt_rate\n        self.archive = [] # Archive for storing discarded solutions\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in self.population])\n        self.n_evals = self.pop_size\n        \n        while self.n_evals < self.budget:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 4, replace=False) # Sample 4 distinct indices\n                x_1, x_2, x_3, x_4 = self.population[idxs]\n                \n                # DE/rand/1 scheme with archive consideration to enhance exploration\n                if np.random.rand() < 0.2 and len(self.archive) > 0: # 20% chance to use archive\n                    arch_idx = np.random.randint(len(self.archive))\n                    x_4 = self.archive[arch_idx] \n\n                v = self.population[i] + self.F[i] * (x_2 - x_3) + self.F[i] * (x_4 - self.population[i]) # DE/rand/1\n\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                j_rand = np.random.randint(self.dim)\n                u = np.array([v[j] if (np.random.rand() < self.Cr[i] or j == j_rand) else self.population[i, j] for j in range(self.dim)])\n\n                # Selection\n                f_u = func(u)\n                self.n_evals += 1\n\n                if f_u < fitness[i]:\n                    # Update successful F and Cr with a simple adaptation rule\n                    self.F[i] = self.F[i] * (1 - self.adapt_rate) + np.random.rand() * self.adapt_rate\n                    self.Cr[i] = self.Cr[i] * (1 - self.adapt_rate) + np.random.rand() * self.adapt_rate\n\n                    self.archive.append(self.population[i])  # Add replaced solution to archive\n                    if len(self.archive) > self.pop_size:\n                         self.archive.pop(0) # Maintain archive size\n                    \n                    self.population[i] = u\n                    fitness[i] = f_u\n\n                    if f_u < self.f_opt:\n                        self.f_opt = f_u\n                        self.x_opt = u\n                        \n                else:\n                    # If the trial vector is worse, adapt F and Cr, but with less intensity\n                    self.F[i] = self.F[i] * (1 - self.adapt_rate * 0.5) + np.random.rand() * self.adapt_rate * 0.5\n                    self.Cr[i] = self.Cr[i] * (1 - self.adapt_rate * 0.5) + np.random.rand() * self.adapt_rate * 0.5\n\n                self.F[i] = np.clip(self.F[i], 0.1, 1.0)\n                self.Cr[i] = np.clip(self.Cr[i], 0.1, 1.0)\n\n                if self.n_evals >= self.budget:\n                    break\n\n\n        return self.f_opt, self.x_opt\n```", "operator": "simplify", "theta_sampled": 0.006024930109540513, "generation": 9, "aucs": [0.1798864096179268, 0.3346608376989799, 0.45843188375037514, 0.8486778947188759, 0.5916290603237458, 0.7602837391694627, 0.5022800303797641, 0.5874688287022803, 0.7973832308041634, 0.18361667658372238, 0.8106681406201405, 0.9941135736869937, 0.364496609175748, 0.5435190067751556, 0.8195381257324833, 0.3672573852619657, 0.4926899673094064, 0.8531592455471856, 0.21007418133806754, 0.5135997342142715]}, "task_prompt": ""}
{"id": "c3fa81b1-a451-4b43-92ce-9eccb5effdb1", "fitness": 0.5948584013532349, "name": "AdaptiveDE", "description": "Adaptive Differential Evolution with a simplified parameter adaptation based on exponential moving averages and a dynamic population size.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size_init=40, F_init=0.5, Cr_init=0.9, pop_size_min=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size_init\n        self.pop_size_min = pop_size_min\n        self.F = F_init\n        self.Cr = Cr_init\n        self.F_ema = F_init  # Exponential moving average of F\n        self.Cr_ema = Cr_init # Exponential moving average of Cr\n        self.alpha = 0.2      # EMA smoothing factor\n        self.success_count = 0\n        self.pop_size_factor = 1.0\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in self.population])\n        self.n_evals = self.pop_size\n\n        while self.n_evals < self.budget:\n            self.success_count = 0 # reset success counter for population size control\n\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_1, x_2, x_3 = self.population[idxs]\n                v = self.population[i] + self.F * (x_2 - x_3)\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                j_rand = np.random.randint(self.dim)\n                u = np.array([v[j] if (np.random.rand() < self.Cr or j == j_rand) else self.population[i, j] for j in range(self.dim)])\n\n                # Selection\n                f_u = func(u)\n                self.n_evals += 1\n\n                if f_u < fitness[i]:\n                    self.population[i] = u\n                    fitness[i] = f_u\n                    self.success_count += 1\n\n                    if f_u < self.f_opt:\n                        self.f_opt = f_u\n                        self.x_opt = u\n\n                    # Update EMA of F and Cr\n                    self.F_ema = self.alpha * self.F + (1 - self.alpha) * self.F_ema\n                    self.Cr_ema = self.alpha * self.Cr + (1 - self.alpha) * self.Cr_ema\n\n                if self.n_evals >= self.budget:\n                    break\n\n            # Parameter Adaptation (Simplified EMA)\n            self.F = np.clip(np.random.normal(self.F_ema, 0.1), 0.1, 1.0)\n            self.Cr = np.clip(np.random.normal(self.Cr_ema, 0.1), 0.1, 1.0)\n\n            # Dynamic Population Size Adjustment (simplified)\n            success_rate = self.success_count / self.pop_size\n            if success_rate > 0.2:  # Increase population if doing well\n                self.pop_size_factor *= 1.1\n            elif success_rate < 0.1: # Decrease population if stagnating\n                self.pop_size_factor *= 0.9\n\n            self.pop_size = int(np.clip(self.pop_size_factor * self.pop_size, self.pop_size_min, 2*self.dim)) #Cap pop size\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 10, "feedback": "The algorithm AdaptiveDE scored 0.595 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["fcdcbf48-6523-411e-bb4a-e8762f71fb52"], "operator": "simplify", "metadata": {"llm_response": "# Description: Adaptive Differential Evolution with a simplified parameter adaptation based on exponential moving averages and a dynamic population size.\n# Code:\n```python\nimport numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size_init=40, F_init=0.5, Cr_init=0.9, pop_size_min=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size_init\n        self.pop_size_min = pop_size_min\n        self.F = F_init\n        self.Cr = Cr_init\n        self.F_ema = F_init  # Exponential moving average of F\n        self.Cr_ema = Cr_init # Exponential moving average of Cr\n        self.alpha = 0.2      # EMA smoothing factor\n        self.success_count = 0\n        self.pop_size_factor = 1.0\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in self.population])\n        self.n_evals = self.pop_size\n\n        while self.n_evals < self.budget:\n            self.success_count = 0 # reset success counter for population size control\n\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_1, x_2, x_3 = self.population[idxs]\n                v = self.population[i] + self.F * (x_2 - x_3)\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                j_rand = np.random.randint(self.dim)\n                u = np.array([v[j] if (np.random.rand() < self.Cr or j == j_rand) else self.population[i, j] for j in range(self.dim)])\n\n                # Selection\n                f_u = func(u)\n                self.n_evals += 1\n\n                if f_u < fitness[i]:\n                    self.population[i] = u\n                    fitness[i] = f_u\n                    self.success_count += 1\n\n                    if f_u < self.f_opt:\n                        self.f_opt = f_u\n                        self.x_opt = u\n\n                    # Update EMA of F and Cr\n                    self.F_ema = self.alpha * self.F + (1 - self.alpha) * self.F_ema\n                    self.Cr_ema = self.alpha * self.Cr + (1 - self.alpha) * self.Cr_ema\n\n                if self.n_evals >= self.budget:\n                    break\n\n            # Parameter Adaptation (Simplified EMA)\n            self.F = np.clip(np.random.normal(self.F_ema, 0.1), 0.1, 1.0)\n            self.Cr = np.clip(np.random.normal(self.Cr_ema, 0.1), 0.1, 1.0)\n\n            # Dynamic Population Size Adjustment (simplified)\n            success_rate = self.success_count / self.pop_size\n            if success_rate > 0.2:  # Increase population if doing well\n                self.pop_size_factor *= 1.1\n            elif success_rate < 0.1: # Decrease population if stagnating\n                self.pop_size_factor *= 0.9\n\n            self.pop_size = int(np.clip(self.pop_size_factor * self.pop_size, self.pop_size_min, 2*self.dim)) #Cap pop size\n\n        return self.f_opt, self.x_opt\n```", "operator": "simplify", "theta_sampled": 0.10465057703186081, "generation": 10, "aucs": [0.2558044761771928, 0.6314711178056469, 0.8111781026194194, 0.9067690497071588, 0.25060675542384936, 0.7673850535146138, 0.3374149846790928, 0.4659154761325458, 0.46152623267947934, 0.19432964127620977, 0.9311388760058851, 0.997610609991442, 0.775370698130826, 0.41641727194561506, 0.8930390227093332, 0.7994367652319643, 0.39620596775482353, 0.9213423411333059, 0.19861960688468283, 0.485585977261611]}, "task_prompt": ""}
{"id": "17b6eb69-41cd-40f5-8f81-e9ae11b4596f", "fitness": 0.4947355909729378, "name": "AdaptiveDESelfMirror", "description": "Self-adaptive Differential Evolution with simplified parameter adaptation and enhanced exploration using a mirrored population.", "code": "import numpy as np\n\nclass AdaptiveDESelfMirror:\n    def __init__(self, budget=10000, dim=10, pop_size=40, F_init=0.5, Cr_init=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F_init * np.ones(pop_size)  # Individual F values\n        self.Cr = Cr_init * np.ones(pop_size) # Individual Cr values\n\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in self.population])\n        self.n_evals = self.pop_size\n        \n        # Create mirrored population\n        mirrored_population = func.bounds.ub + func.bounds.lb - self.population\n        mirrored_fitness = np.array([func(x) for x in mirrored_population])\n        self.n_evals += self.pop_size\n\n        combined_population = np.concatenate((self.population, mirrored_population))\n        combined_fitness = np.concatenate((fitness, mirrored_fitness))\n\n        best_index = np.argmin(combined_fitness)\n        if best_index < self.pop_size:\n            self.f_opt = combined_fitness[best_index]\n            self.x_opt = combined_population[best_index]\n        else:\n            self.f_opt = combined_fitness[best_index]\n            self.x_opt = combined_population[best_index]\n\n\n        while self.n_evals < self.budget:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False) # Sample 3 distinct indices\n                x_1, x_2, x_3 = self.population[idxs]\n                \n                # DE/rand/1 scheme - simplified\n                v = self.population[i] + self.F[i] * (x_1 - x_2) + self.F[i] * (x_3 - self.population[i])\n\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                j_rand = np.random.randint(self.dim)\n                u = np.array([v[j] if (np.random.rand() < self.Cr[i] or j == j_rand) else self.population[i, j] for j in range(self.dim)])\n\n                # Selection\n                f_u = func(u)\n                self.n_evals += 1\n\n                if f_u < fitness[i]:\n                    # Update successful F and Cr with a simple adaptation rule\n                    self.F[i] = 0.9 * self.F[i] + 0.1\n                    self.Cr[i] = 0.9 * self.Cr[i] + 0.1\n                    \n                    self.population[i] = u\n                    fitness[i] = f_u\n\n                    if f_u < self.f_opt:\n                        self.f_opt = f_u\n                        self.x_opt = u\n                        \n                else:\n                    # If the trial vector is worse, adapt F and Cr, but with less intensity\n                    self.F[i] = 0.9 * self.F[i]\n                    self.Cr[i] = 0.9 * self.Cr[i]\n\n                self.F[i] = np.clip(self.F[i], 0.1, 1.0)\n                self.Cr[i] = np.clip(self.Cr[i], 0.1, 1.0)\n\n                if self.n_evals >= self.budget:\n                    break\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 10, "feedback": "The algorithm AdaptiveDESelfMirror scored 0.495 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["13c7de64-3298-4550-93e3-428b7f9ace9d"], "operator": "simplify", "metadata": {"llm_response": "# Description: Self-adaptive Differential Evolution with simplified parameter adaptation and enhanced exploration using a mirrored population.\n# Code: \n```python\nimport numpy as np\n\nclass AdaptiveDESelfMirror:\n    def __init__(self, budget=10000, dim=10, pop_size=40, F_init=0.5, Cr_init=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F_init * np.ones(pop_size)  # Individual F values\n        self.Cr = Cr_init * np.ones(pop_size) # Individual Cr values\n\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in self.population])\n        self.n_evals = self.pop_size\n        \n        # Create mirrored population\n        mirrored_population = func.bounds.ub + func.bounds.lb - self.population\n        mirrored_fitness = np.array([func(x) for x in mirrored_population])\n        self.n_evals += self.pop_size\n\n        combined_population = np.concatenate((self.population, mirrored_population))\n        combined_fitness = np.concatenate((fitness, mirrored_fitness))\n\n        best_index = np.argmin(combined_fitness)\n        if best_index < self.pop_size:\n            self.f_opt = combined_fitness[best_index]\n            self.x_opt = combined_population[best_index]\n        else:\n            self.f_opt = combined_fitness[best_index]\n            self.x_opt = combined_population[best_index]\n\n\n        while self.n_evals < self.budget:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False) # Sample 3 distinct indices\n                x_1, x_2, x_3 = self.population[idxs]\n                \n                # DE/rand/1 scheme - simplified\n                v = self.population[i] + self.F[i] * (x_1 - x_2) + self.F[i] * (x_3 - self.population[i])\n\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                j_rand = np.random.randint(self.dim)\n                u = np.array([v[j] if (np.random.rand() < self.Cr[i] or j == j_rand) else self.population[i, j] for j in range(self.dim)])\n\n                # Selection\n                f_u = func(u)\n                self.n_evals += 1\n\n                if f_u < fitness[i]:\n                    # Update successful F and Cr with a simple adaptation rule\n                    self.F[i] = 0.9 * self.F[i] + 0.1\n                    self.Cr[i] = 0.9 * self.Cr[i] + 0.1\n                    \n                    self.population[i] = u\n                    fitness[i] = f_u\n\n                    if f_u < self.f_opt:\n                        self.f_opt = f_u\n                        self.x_opt = u\n                        \n                else:\n                    # If the trial vector is worse, adapt F and Cr, but with less intensity\n                    self.F[i] = 0.9 * self.F[i]\n                    self.Cr[i] = 0.9 * self.Cr[i]\n\n                self.F[i] = np.clip(self.F[i], 0.1, 1.0)\n                self.Cr[i] = np.clip(self.Cr[i], 0.1, 1.0)\n\n                if self.n_evals >= self.budget:\n                    break\n\n\n        return self.f_opt, self.x_opt\n```", "operator": "simplify", "theta_sampled": 0.19397961156910973, "generation": 10, "aucs": [0.144274023597318, 0.3446509740407252, 0.404771299730336, 0.823283416031155, 0.36747130458543986, 0.8022200934003367, 0.3309582141027849, 0.4567201208989702, 0.4324702671259123, 0.19575228485849738, 0.7016478721852666, 0.9916901045778874, 0.26510961043489256, 0.4123676665041247, 0.7734027495450029, 0.5312779881017142, 0.3893791919398868, 0.8248576474677028, 0.204057168253991, 0.49834982207681355]}, "task_prompt": ""}
{"id": "5213650d-9212-4424-8cd1-9da558449d5f", "fitness": 0.3930442780380351, "name": "AdaptiveDEMomentum", "description": "Adaptive Differential Evolution with a decaying exploration rate and momentum-based parameter adaptation for enhanced exploration-exploitation balance.", "code": "import numpy as np\n\nclass AdaptiveDEMomentum:\n    def __init__(self, budget=10000, dim=10, pop_size=40, F_init=0.5, Cr_init=0.9, momentum=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F_init\n        self.Cr = Cr_init\n        self.momentum = momentum\n        self.F_velocity = 0.0\n        self.Cr_velocity = 0.0\n        self.exploration_rate = 1.0  # Initial exploration rate\n        self.exploration_decay = 0.9995  # Decay factor for exploration rate\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in self.population])\n        self.n_evals = self.pop_size\n\n        while self.n_evals < self.budget:\n            for i in range(self.pop_size):\n                # Mutation with Exploration\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_1, x_2, x_3 = self.population[idxs]\n\n                # Add exploration component\n                exploration_vector = np.random.uniform(-self.exploration_rate, self.exploration_rate, size=self.dim)\n                v = self.population[i] + self.F * (x_2 - x_3) + exploration_vector\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                j_rand = np.random.randint(self.dim)\n                u = np.array([v[j] if (np.random.rand() < self.Cr or j == j_rand) else self.population[i, j] for j in range(self.dim)])\n\n                # Selection\n                f_u = func(u)\n                self.n_evals += 1\n\n                if f_u < fitness[i]:\n                    self.population[i] = u\n                    fitness[i] = f_u\n\n                    if f_u < self.f_opt:\n                        self.f_opt = f_u\n                        self.x_opt = u\n\n                    # Adapt F and Cr with momentum\n                    delta_F = np.random.normal(0, 0.1)  # Small random change\n                    self.F_velocity = self.momentum * self.F_velocity + (1 - self.momentum) * delta_F\n                    self.F = np.clip(self.F + self.F_velocity, 0.1, 1.0)\n\n                    delta_Cr = np.random.normal(0, 0.1)\n                    self.Cr_velocity = self.momentum * self.Cr_velocity + (1 - self.momentum) * delta_Cr\n                    self.Cr = np.clip(self.Cr + self.Cr_velocity, 0.1, 1.0)\n                \n                #Decay exploration rate\n                self.exploration_rate *= self.exploration_decay\n\n\n                if self.n_evals >= self.budget:\n                    break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 10, "feedback": "The algorithm AdaptiveDEMomentum scored 0.393 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["fcdcbf48-6523-411e-bb4a-e8762f71fb52"], "operator": "simplify", "metadata": {"llm_response": "# Description: Adaptive Differential Evolution with a decaying exploration rate and momentum-based parameter adaptation for enhanced exploration-exploitation balance.\n# Code: \n```python\nimport numpy as np\n\nclass AdaptiveDEMomentum:\n    def __init__(self, budget=10000, dim=10, pop_size=40, F_init=0.5, Cr_init=0.9, momentum=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F_init\n        self.Cr = Cr_init\n        self.momentum = momentum\n        self.F_velocity = 0.0\n        self.Cr_velocity = 0.0\n        self.exploration_rate = 1.0  # Initial exploration rate\n        self.exploration_decay = 0.9995  # Decay factor for exploration rate\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in self.population])\n        self.n_evals = self.pop_size\n\n        while self.n_evals < self.budget:\n            for i in range(self.pop_size):\n                # Mutation with Exploration\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_1, x_2, x_3 = self.population[idxs]\n\n                # Add exploration component\n                exploration_vector = np.random.uniform(-self.exploration_rate, self.exploration_rate, size=self.dim)\n                v = self.population[i] + self.F * (x_2 - x_3) + exploration_vector\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                j_rand = np.random.randint(self.dim)\n                u = np.array([v[j] if (np.random.rand() < self.Cr or j == j_rand) else self.population[i, j] for j in range(self.dim)])\n\n                # Selection\n                f_u = func(u)\n                self.n_evals += 1\n\n                if f_u < fitness[i]:\n                    self.population[i] = u\n                    fitness[i] = f_u\n\n                    if f_u < self.f_opt:\n                        self.f_opt = f_u\n                        self.x_opt = u\n\n                    # Adapt F and Cr with momentum\n                    delta_F = np.random.normal(0, 0.1)  # Small random change\n                    self.F_velocity = self.momentum * self.F_velocity + (1 - self.momentum) * delta_F\n                    self.F = np.clip(self.F + self.F_velocity, 0.1, 1.0)\n\n                    delta_Cr = np.random.normal(0, 0.1)\n                    self.Cr_velocity = self.momentum * self.Cr_velocity + (1 - self.momentum) * delta_Cr\n                    self.Cr = np.clip(self.Cr + self.Cr_velocity, 0.1, 1.0)\n                \n                #Decay exploration rate\n                self.exploration_rate *= self.exploration_decay\n\n\n                if self.n_evals >= self.budget:\n                    break\n\n        return self.f_opt, self.x_opt\n```", "operator": "simplify", "theta_sampled": 0.12509569200253126, "generation": 10, "aucs": [0.15690063798485665, 0.19816173996971742, 0.3980490986434203, 0.3696708797481678, 0.3061788558476787, 0.4358426630126855, 0.2996766104401709, 0.3369578713654082, 0.326961261637132, 0.2061932244310376, 0.4677997723238525, 0.9973188978531157, 0.32842313920400834, 0.31730776562282503, 0.7801688340243897, 0.4355649003005898, 0.32233183572625135, 0.5047629710339339, 0.18648867906300548, 0.48612592252845455]}, "task_prompt": ""}
{"id": "56caf88b-b963-42d5-9f92-fde2bb56c95b", "fitness": 0.4264419385515671, "name": "AdaptiveDE", "description": "Adaptive Differential Evolution with a simplified parameter adaptation scheme using a success rate based moving average.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=40, F_init=0.5, Cr_init=0.9, learning_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F_init\n        self.Cr = Cr_init\n        self.learning_rate = learning_rate\n        self.success_rate_F = 0.5  # Initialize success rates\n        self.success_rate_Cr = 0.5\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in self.population])\n        self.n_evals = self.pop_size\n        best_idx = np.argmin(fitness)\n        self.f_opt = fitness[best_idx]\n        self.x_opt = self.population[best_idx]\n\n\n        while self.n_evals < self.budget:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_1, x_2, x_3 = self.population[idxs]\n                v = self.population[i] + self.F * (x_2 - x_3)\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                j_rand = np.random.randint(self.dim)\n                u = np.array([v[j] if (np.random.rand() < self.Cr or j == j_rand) else self.population[i, j] for j in range(self.dim)])\n\n                # Selection\n                f_u = func(u)\n                self.n_evals += 1\n\n                if f_u < fitness[i]:\n                    # Update success rates\n                    self.success_rate_F = (1 - self.learning_rate) * self.success_rate_F + self.learning_rate * 1\n                    self.success_rate_Cr = (1 - self.learning_rate) * self.success_rate_Cr + self.learning_rate * 1\n\n                    self.population[i] = u\n                    fitness[i] = f_u\n\n                    if f_u < self.f_opt:\n                        self.f_opt = f_u\n                        self.x_opt = u\n\n                else:\n                    # Update success rates\n                    self.success_rate_F = (1 - self.learning_rate) * self.success_rate_F\n                    self.success_rate_Cr = (1 - self.learning_rate) * self.success_rate_Cr\n\n                if self.n_evals >= self.budget:\n                    break\n\n            # Adapt F and Cr (after each generation)\n            self.adapt_parameters()\n\n        return self.f_opt, self.x_opt\n\n    def adapt_parameters(self):\n        # Adjust F and Cr based on success rates\n        self.F = np.clip(self.F * (1 + self.learning_rate * (self.success_rate_F - 0.5)), 0.1, 1.0)\n        self.Cr = np.clip(self.Cr * (1 + self.learning_rate * (self.success_rate_Cr - 0.5)), 0.1, 1.0)", "configspace": "", "generation": 10, "feedback": "The algorithm AdaptiveDE scored 0.426 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["fcdcbf48-6523-411e-bb4a-e8762f71fb52"], "operator": "simplify", "metadata": {"llm_response": "# Description: Adaptive Differential Evolution with a simplified parameter adaptation scheme using a success rate based moving average.\n# Code: \n```python\nimport numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=40, F_init=0.5, Cr_init=0.9, learning_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F_init\n        self.Cr = Cr_init\n        self.learning_rate = learning_rate\n        self.success_rate_F = 0.5  # Initialize success rates\n        self.success_rate_Cr = 0.5\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in self.population])\n        self.n_evals = self.pop_size\n        best_idx = np.argmin(fitness)\n        self.f_opt = fitness[best_idx]\n        self.x_opt = self.population[best_idx]\n\n\n        while self.n_evals < self.budget:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_1, x_2, x_3 = self.population[idxs]\n                v = self.population[i] + self.F * (x_2 - x_3)\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                j_rand = np.random.randint(self.dim)\n                u = np.array([v[j] if (np.random.rand() < self.Cr or j == j_rand) else self.population[i, j] for j in range(self.dim)])\n\n                # Selection\n                f_u = func(u)\n                self.n_evals += 1\n\n                if f_u < fitness[i]:\n                    # Update success rates\n                    self.success_rate_F = (1 - self.learning_rate) * self.success_rate_F + self.learning_rate * 1\n                    self.success_rate_Cr = (1 - self.learning_rate) * self.success_rate_Cr + self.learning_rate * 1\n\n                    self.population[i] = u\n                    fitness[i] = f_u\n\n                    if f_u < self.f_opt:\n                        self.f_opt = f_u\n                        self.x_opt = u\n\n                else:\n                    # Update success rates\n                    self.success_rate_F = (1 - self.learning_rate) * self.success_rate_F\n                    self.success_rate_Cr = (1 - self.learning_rate) * self.success_rate_Cr\n\n                if self.n_evals >= self.budget:\n                    break\n\n            # Adapt F and Cr (after each generation)\n            self.adapt_parameters()\n\n        return self.f_opt, self.x_opt\n\n    def adapt_parameters(self):\n        # Adjust F and Cr based on success rates\n        self.F = np.clip(self.F * (1 + self.learning_rate * (self.success_rate_F - 0.5)), 0.1, 1.0)\n        self.Cr = np.clip(self.Cr * (1 + self.learning_rate * (self.success_rate_Cr - 0.5)), 0.1, 1.0)\n```", "operator": "simplify", "theta_sampled": 0.04353616443808091, "generation": 10, "aucs": [0.19037893255208282, 0.30428067509121703, 0.42014671778342605, 0.5655670570004905, 0.3541334516654052, 0.44214610135625876, 0.3080624176519813, 0.36930204718713644, 0.31053692489526497, 0.230992168194873, 0.5143860635069308, 0.9898503287304524, 0.35168869273892256, 0.2895252319142745, 0.7462758945838969, 0.5068638040981297, 0.34832115917984874, 0.5652950865949422, 0.21794816250614524, 0.5031378537996647]}, "task_prompt": ""}
{"id": "8d959b91-9c55-45af-8094-857292633186", "fitness": "-inf", "name": "AdaptiveDEMemorySelf", "description": "Adaptive Differential Evolution with individual parameter adaptation and memory-assisted global F/Cr update.", "code": "import numpy as np\n\nclass AdaptiveDEMemorySelf:\n    def __init__(self, budget=10000, dim=10, pop_size=40, F_init=0.5, Cr_init=0.9, adapt_rate=0.1, memory_size=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F_init * np.ones(pop_size)  # Individual F values\n        self.Cr = Cr_init * np.ones(pop_size) # Individual Cr values\n        self.adapt_rate = adapt_rate\n        self.archive = [] # Archive for storing discarded solutions\n        self.memory_size = memory_size\n        self.F_memory = np.full(self.memory_size, F_init)\n        self.Cr_memory = np.full(self.memory_size, Cr_init)\n        self.F_idx = 0\n        self.Cr_idx = 0\n        self.success_F = []\n        self.success_Cr = []\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in self.population])\n        self.n_evals = self.pop_size\n        \n        while self.n_evals < self.budget:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 4, replace=False) # Sample 4 distinct indices\n                x_1, x_2, x_3, x_4 = self.population[idxs]\n                \n                # DE/rand/1 scheme with archive consideration to enhance exploration\n                if np.random.rand() < 0.2 and len(self.archive) > 0: # 20% chance to use archive\n                    arch_idx = np.random.randint(len(self.archive))\n                    x_4 = self.archive[arch_idx] \n\n                v = self.population[i] + self.F[i] * (x_2 - x_3) + self.F[i] * (x_4 - self.population[i]) # DE/rand/1\n\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                j_rand = np.random.randint(self.dim)\n                u = np.array([v[j] if (np.random.rand() < self.Cr[i] or j == j_rand) else self.population[i, j] for j in range(self.dim)])\n\n                # Selection\n                f_u = func(u)\n                self.n_evals += 1\n\n                if f_u < fitness[i]:\n                    # Update successful F and Cr with a simple adaptation rule\n                    # Store successful F and Cr for memory update\n                    self.success_F.append(self.F[i])\n                    self.success_Cr.append(self.Cr[i])\n\n                    self.F[i] = self.F[i] * (1 - self.adapt_rate) + np.random.rand() * self.adapt_rate\n                    self.Cr[i] = self.Cr[i] * (1 - self.adapt_rate) + np.random.rand() * self.adapt_rate\n\n                    self.archive.append(self.population[i])  # Add replaced solution to archive\n                    if len(self.archive) > self.pop_size:\n                         self.archive.pop(0) # Maintain archive size\n                    \n                    self.population[i] = u\n                    fitness[i] = f_u\n\n                    if f_u < self.f_opt:\n                        self.f_opt = f_u\n                        self.x_opt = u\n                        \n                else:\n                    # If the trial vector is worse, adapt F and Cr, but with less intensity\n                    self.F[i] = self.F[i] * (1 - self.adapt_rate * 0.5) + np.random.rand() * self.adapt_rate * 0.5\n                    self.Cr[i] = self.Cr[i] * (1 - self.adapt_rate * 0.5) + np.random.rand() * self.adapt_rate * 0.5\n\n                self.F[i] = np.clip(self.F[i], 0.1, 1.0)\n                self.Cr[i] = np.clip(self.Cr[i], 0.1, 1.0)\n\n                if self.n_evals >= self.budget:\n                    break\n\n            # Adapt F and Cr globally using memory\n            self.adapt_parameters()\n\n\n        return self.f_opt, self.x_opt\n\n    def adapt_parameters(self):\n        # Window-based averaging for F and Cr\n        if self.success_F:\n            self.F_memory[self.F_idx] = np.mean(self.success_F)\n            self.F_idx = (self.F_idx + 1) % self.memory_size\n            global_F = np.mean(self.F_memory)\n            global_F = np.clip(global_F, 0.1, 1.0)\n            self.F = global_F * np.ones(self.pop_size) # Apply global F to all individuals\n            self.success_F = []\n\n        if self.success_Cr:\n            self.Cr_memory[self.Cr_idx] = np.mean(self.success_Cr)\n            self.Cr_idx = (self.Cr_idx + 1) % self.memory_size\n            global_Cr = np.mean(self.Cr_memory)\n            global_Cr = np.clip(global_Cr, 0.1, 1.0)\n            self.Cr = global_Cr * np.ones(self.pop_size) # Apply global Cr to all individuals\n            self.success_Cr = []", "configspace": "", "generation": 10, "feedback": "Evaluation failed without an exception.", "error": "Evaluation failed without an exception.", "parent_ids": ["13c7de64-3298-4550-93e3-428b7f9ace9d", "2fa45af8-e020-4e0b-9f07-43390b8ca505"], "operator": "crossover", "metadata": {"llm_response": "# Description: Adaptive Differential Evolution with individual parameter adaptation and memory-assisted global F/Cr update.\n# Code: \n```python\nimport numpy as np\n\nclass AdaptiveDEMemorySelf:\n    def __init__(self, budget=10000, dim=10, pop_size=40, F_init=0.5, Cr_init=0.9, adapt_rate=0.1, memory_size=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F_init * np.ones(pop_size)  # Individual F values\n        self.Cr = Cr_init * np.ones(pop_size) # Individual Cr values\n        self.adapt_rate = adapt_rate\n        self.archive = [] # Archive for storing discarded solutions\n        self.memory_size = memory_size\n        self.F_memory = np.full(self.memory_size, F_init)\n        self.Cr_memory = np.full(self.memory_size, Cr_init)\n        self.F_idx = 0\n        self.Cr_idx = 0\n        self.success_F = []\n        self.success_Cr = []\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in self.population])\n        self.n_evals = self.pop_size\n        \n        while self.n_evals < self.budget:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 4, replace=False) # Sample 4 distinct indices\n                x_1, x_2, x_3, x_4 = self.population[idxs]\n                \n                # DE/rand/1 scheme with archive consideration to enhance exploration\n                if np.random.rand() < 0.2 and len(self.archive) > 0: # 20% chance to use archive\n                    arch_idx = np.random.randint(len(self.archive))\n                    x_4 = self.archive[arch_idx] \n\n                v = self.population[i] + self.F[i] * (x_2 - x_3) + self.F[i] * (x_4 - self.population[i]) # DE/rand/1\n\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                j_rand = np.random.randint(self.dim)\n                u = np.array([v[j] if (np.random.rand() < self.Cr[i] or j == j_rand) else self.population[i, j] for j in range(self.dim)])\n\n                # Selection\n                f_u = func(u)\n                self.n_evals += 1\n\n                if f_u < fitness[i]:\n                    # Update successful F and Cr with a simple adaptation rule\n                    # Store successful F and Cr for memory update\n                    self.success_F.append(self.F[i])\n                    self.success_Cr.append(self.Cr[i])\n\n                    self.F[i] = self.F[i] * (1 - self.adapt_rate) + np.random.rand() * self.adapt_rate\n                    self.Cr[i] = self.Cr[i] * (1 - self.adapt_rate) + np.random.rand() * self.adapt_rate\n\n                    self.archive.append(self.population[i])  # Add replaced solution to archive\n                    if len(self.archive) > self.pop_size:\n                         self.archive.pop(0) # Maintain archive size\n                    \n                    self.population[i] = u\n                    fitness[i] = f_u\n\n                    if f_u < self.f_opt:\n                        self.f_opt = f_u\n                        self.x_opt = u\n                        \n                else:\n                    # If the trial vector is worse, adapt F and Cr, but with less intensity\n                    self.F[i] = self.F[i] * (1 - self.adapt_rate * 0.5) + np.random.rand() * self.adapt_rate * 0.5\n                    self.Cr[i] = self.Cr[i] * (1 - self.adapt_rate * 0.5) + np.random.rand() * self.adapt_rate * 0.5\n\n                self.F[i] = np.clip(self.F[i], 0.1, 1.0)\n                self.Cr[i] = np.clip(self.Cr[i], 0.1, 1.0)\n\n                if self.n_evals >= self.budget:\n                    break\n\n            # Adapt F and Cr globally using memory\n            self.adapt_parameters()\n\n\n        return self.f_opt, self.x_opt\n\n    def adapt_parameters(self):\n        # Window-based averaging for F and Cr\n        if self.success_F:\n            self.F_memory[self.F_idx] = np.mean(self.success_F)\n            self.F_idx = (self.F_idx + 1) % self.memory_size\n            global_F = np.mean(self.F_memory)\n            global_F = np.clip(global_F, 0.1, 1.0)\n            self.F = global_F * np.ones(self.pop_size) # Apply global F to all individuals\n            self.success_F = []\n\n        if self.success_Cr:\n            self.Cr_memory[self.Cr_idx] = np.mean(self.success_Cr)\n            self.Cr_idx = (self.Cr_idx + 1) % self.memory_size\n            global_Cr = np.mean(self.Cr_memory)\n            global_Cr = np.clip(global_Cr, 0.1, 1.0)\n            self.Cr = global_Cr * np.ones(self.pop_size) # Apply global Cr to all individuals\n            self.success_Cr = []\n```", "operator": "crossover", "theta_sampled": 0.0012969627232409361, "generation": 10}, "task_prompt": ""}
