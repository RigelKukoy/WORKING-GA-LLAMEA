{"id": "bff7ed35-f277-4b67-ab53-e19454146c01", "fitness": 0.6154460678810344, "name": "HybridPSO_DE", "description": "No description provided.", "code": "import numpy as np\n\nclass HybridPSO_DE:\n    def __init__(self, budget=10000, dim=10, pop_size=20, w=0.7, c1=1.5, c2=1.5, cr=0.7, f=0.8):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.w = w  # Inertia weight\n        self.c1 = c1  # Cognitive coefficient\n        self.c2 = c2  # Social coefficient\n        self.cr = cr  # Crossover rate\n        self.f = f  # Mutation factor\n        self.lb = -5.0\n        self.ub = 5.0\n\n    def __call__(self, func):\n        # Initialization\n        population = np.random.uniform(self.lb, self.ub, (self.pop_size, self.dim))\n        velocities = np.random.uniform(-1, 1, (self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)].copy()\n        personal_best_positions = population.copy()\n        personal_best_fitness = fitness.copy()\n        \n        eval_count = self.pop_size\n\n        while eval_count < self.budget:\n            # PSO update\n            r1 = np.random.rand(self.pop_size, self.dim)\n            r2 = np.random.rand(self.pop_size, self.dim)\n            global_best_position = self.x_opt \n\n            velocities = (self.w * velocities +\n                          self.c1 * r1 * (personal_best_positions - population) +\n                          self.c2 * r2 * (global_best_position - population))\n\n            population = population + velocities\n            population = np.clip(population, self.lb, self.ub)\n            \n            # DE mutation\n            for i in range(self.pop_size):\n                # Select three random individuals (excluding the current one)\n                indices = np.random.choice(self.pop_size, 3, replace=False)\n                while i in indices:\n                    indices = np.random.choice(self.pop_size, 3, replace=False)\n                    \n                x_r1, x_r2, x_r3 = population[indices]\n                \n                # Mutation\n                v_mutation = x_r1 + self.f * (x_r2 - x_r3)\n                v_mutation = np.clip(v_mutation, self.lb, self.ub)\n\n                # Crossover\n                j_rand = np.random.randint(self.dim)\n                u = np.random.rand(self.dim)\n                mask = (u <= self.cr) | (np.arange(self.dim) == j_rand)\n                u_crossover = np.where(mask, v_mutation, population[i])\n                \n                # Evaluation\n                f_trial = func(u_crossover)\n                eval_count += 1\n                \n                # Selection\n                if f_trial < fitness[i]:\n                    population[i] = u_crossover\n                    fitness[i] = f_trial\n                    \n                    if f_trial < personal_best_fitness[i]:\n                        personal_best_positions[i] = u_crossover\n                        personal_best_fitness[i] = f_trial\n                        \n                        if f_trial < self.f_opt:\n                            self.f_opt = f_trial\n                            self.x_opt = u_crossover.copy()\n                \n                if eval_count >= self.budget:\n                    break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm HybridPSO_DE scored 0.615 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.12938589326096694, 0.5037494709583915, 0.7439113107241202, 0.8222704379971336, 0.7633996862068553, 0.8287116566897561, 0.32490773029858067, 0.6904108351862037, 0.7724365635660831, 0.3183923498512955, 0.8892212795995134, 0.9980915171981991, 0.27603140366759293, 0.3040498979107822, 0.7411867230586812, 0.8157172446552386, 0.6772186217563414, 0.8608931887971448, 0.3299032100117377, 0.5190323362260706]}, "task_prompt": ""}
{"id": "f9d60ece-8365-4fe7-beaa-0d7c7f2a3c80", "fitness": 0.0, "name": "AdaptiveDE", "description": "No description provided.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, CR=0.7, local_search_prob=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.local_search_prob = local_search_prob\n\n        self.lb = -5.0\n        self.ub = 5.0\n        self.population = None\n        self.fitness = None\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.evals = 0\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals += self.pop_size\n\n        best_idx = np.argmin(self.fitness)\n        if self.fitness[best_idx] < self.f_opt:\n            self.f_opt = self.fitness[best_idx]\n            self.x_opt = self.population[best_idx]\n\n    def mutate(self):\n        for i in range(self.pop_size):\n            idxs = np.random.choice(self.pop_size, 3, replace=False)\n            a, b, c = self.population[idxs]\n            mutant = a + self.F * (b - c)\n            mutant = np.clip(mutant, self.lb, self.ub)\n            yield mutant\n\n    def crossover(self, mutant, i):\n        trial = np.copy(self.population[i])\n        for j in range(self.dim):\n            if np.random.rand() < self.CR or j == np.random.randint(self.dim):\n                trial[j] = mutant[j]\n        return trial\n    \n    def local_search(self, x, func):\n        # Simple local search around x\n        delta = np.random.uniform(-0.1, 0.1, size=self.dim)\n        x_new = np.clip(x + delta, self.lb, self.ub)\n        f_new = func(x_new)\n        self.evals += 1\n        if f_new < func(x):\n            return x_new, f_new\n        else:\n            return x, func(x)\n\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.evals = 0\n        \n        self.initialize_population(func)\n        \n        generation = 0\n        stagnation_counter = 0\n        last_improvement = 0\n\n        while self.evals < self.budget:\n            \n            new_population = []\n            new_fitness = []\n            \n            for i, mutant in enumerate(self.mutate()):\n                trial = self.crossover(mutant, i)\n                \n                f_trial = func(trial)\n                self.evals += 1\n                \n                if np.random.rand() < self.local_search_prob:\n                  trial, f_trial = self.local_search(trial, func)\n                \n                if f_trial < self.fitness[i]:\n                    new_population.append(trial)\n                    new_fitness.append(f_trial)\n                else:\n                    new_population.append(self.population[i])\n                    new_fitness.append(self.fitness[i])\n                    \n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = trial\n                    last_improvement = generation\n\n            self.population = np.array(new_population)\n            self.fitness = np.array(new_fitness)\n\n            generation += 1\n            \n            if generation - last_improvement > 50:\n              stagnation_counter +=1\n            else:\n              stagnation_counter = 0\n\n            if stagnation_counter > 2:\n                self.initialize_population(func)  # Restart population\n                stagnation_counter = 0\n                last_improvement = generation\n        \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveDE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0]}, "task_prompt": ""}
{"id": "378a6b5b-d7fa-4ddb-a080-8368b9d5ffa7", "fitness": 0.5166370190385764, "name": "AdaptiveDE", "description": "No description provided.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=20, archive_size=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.F = 0.5\n        self.CR = 0.9\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.archive = []\n        self.success_F = []\n        self.success_CR = []\n        self.success_fitness_diff = []\n        self.mu_F = 0.5\n        self.mu_CR = 0.9\n        self.func_evals = self.pop_size\n\n        while self.func_evals < self.budget:\n            for i in range(self.pop_size):\n                # Mutation\n                indices = np.random.choice(self.pop_size + len(self.archive), 3, replace=False)\n                if indices[0] < self.pop_size:\n                    a = self.pop[indices[0]]\n                else:\n                    a = self.archive[indices[0] - self.pop_size]\n                if indices[1] < self.pop_size:\n                    b = self.pop[indices[1]]\n                else:\n                    b = self.archive[indices[1] - self.pop_size]\n                if indices[2] < self.pop_size:\n                    c = self.pop[indices[2]]\n                else:\n                    c = self.archive[indices[2] - self.pop_size]\n\n                mutant = self.pop[i] + self.F * (b - c)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, self.pop[i])\n\n                # Selection\n                f = func(trial)\n                self.func_evals += 1\n\n                if f < self.fitness[i]:\n                    self.archive.append(self.pop[i].copy())\n                    if len(self.archive) > self.archive_size:\n                        self.archive.pop(0)\n                    \n                    self.success_F.append(self.F)\n                    self.success_CR.append(self.CR)\n                    self.success_fitness_diff.append(abs(f - self.fitness[i]))\n\n                    self.pop[i] = trial\n                    self.fitness[i] = f\n\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = trial\n\n            # Parameter Adaptation\n            if self.success_F:\n                self.mu_F = sum(f**2 for f in self.success_F) / sum(self.success_F)\n                self.mu_CR = np.mean(self.success_CR)\n\n            self.F = np.clip(np.random.normal(self.mu_F, 0.1), 0.1, 1.0)\n            self.CR = np.clip(np.random.normal(self.mu_CR, 0.1), 0.1, 1.0)\n            \n            self.success_F = []\n            self.success_CR = []\n            self.success_fitness_diff = []\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveDE scored 0.517 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.18119682284860483, 0.3751626582670923, 0.5304136758859838, 0.7555800393281984, 0.3504273848661055, 0.6323253369949939, 0.3646265919143008, 0.44269006490235374, 0.43028425972789086, 0.1894674677147069, 0.7744990525757116, 0.991342307747365, 0.5086381162168081, 0.44804480447728257, 0.8298391828361107, 0.6747223833869453, 0.44126101985188404, 0.6719918251599142, 0.2322769464504003, 0.5079504396188759]}, "task_prompt": ""}
{"id": "7485d855-36a7-484e-9307-2228c01c302d", "fitness": 0.0, "name": "GradientCMAES", "description": "No description provided.", "code": "import numpy as np\n\nclass GradientCMAES:\n    def __init__(self, budget=10000, dim=10, pop_size=None, step_size=0.1, local_search_iterations=5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size if pop_size is not None else 4 + int(3 * np.log(dim))\n        self.step_size = step_size\n        self.local_search_iterations = local_search_iterations\n\n        self.mean = None\n        self.C = None\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.eval_count = 0\n\n        self.mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        self.C = np.eye(self.dim)\n        \n        while self.eval_count < self.budget:\n            # Sample population\n            z = np.random.multivariate_normal(np.zeros(self.dim), self.C, size=self.pop_size)\n            x = self.mean + self.step_size * z\n            \n            # Clip to bounds\n            x = np.clip(x, func.bounds.lb, func.bounds.ub)\n            \n            f = np.array([func(xi) for xi in x])\n            self.eval_count += self.pop_size\n\n            # Update best solution\n            best_idx = np.argmin(f)\n            if f[best_idx] < self.f_opt:\n                self.f_opt = f[best_idx]\n                self.x_opt = x[best_idx]\n\n            # Update mean\n            weights = np.exp(-self.pop_size * f / np.sum(f))\n            weights /= np.sum(weights) # Normalize\n\n            self.mean = np.sum(x * weights[:, None], axis=0)\n            \n            # Update covariance matrix (simplified)\n            self.C = np.cov(x.T) #Covariance matrix of the population\n        \n            # Local search around the best solution\n            if self.x_opt is not None:\n                x_local = self.x_opt.copy()\n                for _ in range(self.local_search_iterations):\n                    # Estimate gradient using finite differences\n                    gradient = np.zeros(self.dim)\n                    delta = 1e-3  # Small perturbation\n                    for i in range(self.dim):\n                        x_plus = x_local.copy()\n                        x_minus = x_local.copy()\n                        x_plus[i] += delta\n                        x_minus[i] -= delta\n\n                        x_plus = np.clip(x_plus, func.bounds.lb, func.bounds.ub)\n                        x_minus = np.clip(x_minus, func.bounds.lb, func.bounds.ub)\n\n                        f_plus = func(x_plus)\n                        f_minus = func(x_minus)\n                        \n                        self.eval_count += 2 #Accounting for function evaluations for gradient estimation\n                        \n                        gradient[i] = (f_plus - f_minus) / (2 * delta)\n                    \n                    # Update position based on gradient\n                    x_local -= 0.01 * gradient  # Small step along the negative gradient\n                    x_local = np.clip(x_local, func.bounds.lb, func.bounds.ub)\n                    \n                    f_local = func(x_local)\n                    self.eval_count += 1\n                    \n                    if f_local < self.f_opt:\n                        self.f_opt = f_local\n                        self.x_opt = x_local.copy()\n                        \n                    if self.eval_count >= self.budget:\n                        break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm GradientCMAES scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0]}, "task_prompt": ""}
{"id": "c4acc981-1576-45c5-bcf8-c19ec47a5ad6", "fitness": 0.48626220578778534, "name": "AdaptiveDifferentialEvolution", "description": "No description provided.", "code": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget=10000, dim=10, pop_size=None, F=0.7, CR=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size if pop_size else 10 * dim # Default population size\n        self.F = F  # Mutation factor\n        self.CR = CR # Crossover rate\n        self.lb = -5.0\n        self.ub = 5.0\n        self.history_length = 10\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        \n        # Initialize population\n        population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n        \n        # Store history of fitness improvements to adjust F and CR\n        fitness_history = []\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + self.F * (b - c), self.lb, self.ub)\n                \n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                \n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                \n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    \n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n            \n            # Dynamic Adjustment of F and CR (example: based on success history)\n            fitness_history.append(np.min(fitness))\n            if len(fitness_history) > self.history_length:\n                fitness_history.pop(0)\n                \n                # Example: Decrease F if little improvement\n                if fitness_history[-1] >= fitness_history[0]:\n                    self.F = max(0.1, self.F * 0.9)\n                else:\n                    self.F = min(0.9, self.F * 1.1)\n\n                # Example: Increase CR if improvement\n                if fitness_history[-1] < fitness_history[0]:\n                    self.CR = min(0.99, self.CR * 1.1)\n                else:\n                    self.CR = max(0.1, self.CR * 0.9)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveDifferentialEvolution scored 0.486 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.17885212596943179, 0.30685132480777877, 0.4436560791309294, 0.7436687919803517, 0.40967330047784567, 0.5454242513230193, 0.34376051615187087, 0.4134295010984306, 0.4332274178602634, 0.35526617991444387, 0.7295949846299574, 0.9974736535052224, 0.269900815025501, 0.4210901708812833, 0.8606878616345377, 0.5228760431011859, 0.41600151302152066, 0.6436889023363135, 0.19199828022198273, 0.4981224026838379]}, "task_prompt": ""}
{"id": "ca77c5d5-5458-45df-b340-027d2667e7d8", "fitness": 0.0, "name": "DifferentialEvolutionLocalSearch", "description": "No description provided.", "code": "import numpy as np\n\nclass DifferentialEvolutionLocalSearch:\n    def __init__(self, budget=10000, dim=10, pop_size=20, F=0.7, CR=0.9, local_search_iters=5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.local_search_iters = local_search_iters\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        \n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        # Find best initial individual\n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index]\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + self.F * (b - c), func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                f = func(trial)\n                self.budget -= 1\n                if f < fitness[i]:\n                    fitness[i] = f\n                    population[i] = trial\n\n                    # Update best\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = trial\n            \n            #Local Search around current best solution\n            for _ in range(min(self.local_search_iters, self.budget)):\n                step_size = 0.1 * (func.bounds.ub - func.bounds.lb)\n                x_neighbor = np.clip(self.x_opt + np.random.normal(0, step_size, self.dim), func.bounds.lb, func.bounds.ub)\n                f_neighbor = func(x_neighbor)\n                self.budget -= 1\n\n                if f_neighbor < self.f_opt:\n                    self.f_opt = f_neighbor\n                    self.x_opt = x_neighbor\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm DifferentialEvolutionLocalSearch scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0]}, "task_prompt": ""}
{"id": "bdf9e8c0-e18d-45ea-bc93-f99cba284441", "fitness": 0.1609240005769754, "name": "AdaptiveSimulatedAnnealing", "description": "No description provided.", "code": "import numpy as np\n\nclass AdaptiveSimulatedAnnealing:\n    def __init__(self, budget=10000, dim=10, initial_temp=1.0, cooling_rate=0.95, restart_prob=0.05):\n        self.budget = budget\n        self.dim = dim\n        self.initial_temp = initial_temp\n        self.cooling_rate = cooling_rate\n        self.restart_prob = restart_prob\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        x = np.random.uniform(func.bounds.lb, func.bounds.ub, self.dim)\n        f = func(x)\n        self.f_opt = f\n        self.x_opt = x\n        temp = self.initial_temp\n        eval_count = 1\n\n        while eval_count < self.budget:\n            x_new = x + np.random.normal(0, temp, self.dim)\n            x_new = np.clip(x_new, func.bounds.lb, func.bounds.ub)\n            f_new = func(x_new)\n            eval_count += 1\n\n            delta_f = f_new - f\n\n            if delta_f < 0:\n                x = x_new\n                f = f_new\n                if f < self.f_opt:\n                    self.f_opt = f\n                    self.x_opt = x\n            else:\n                acceptance_prob = np.exp(-delta_f / temp)\n                if np.random.rand() < acceptance_prob:\n                    x = x_new\n                    f = f_new\n\n            temp *= self.cooling_rate\n\n            if np.random.rand() < self.restart_prob:\n                 x = np.random.uniform(func.bounds.lb, func.bounds.ub, self.dim)\n                 f = func(x)\n                 eval_count +=1\n                 if f < self.f_opt:\n                    self.f_opt = f\n                    self.x_opt = x\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveSimulatedAnnealing scored 0.161 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.08232994983752617, 0.1349333393759291, 0.2421559139363224, 0.18847072612270754, 0.19576777246616484, 0.18316519883543925, 0.2155485193261545, 0.1870952726601326, 0.193997808842733, 0.14669950494362005, 0]}, "task_prompt": ""}
{"id": "7132a2dc-c4ff-41cf-b27c-f175d38eda8b", "fitness": 0.4321750003262723, "name": "AdaptiveDE", "description": "No description provided.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=20, F=0.5, CR=0.7, local_search_prob=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.local_search_prob = local_search_prob\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        \n        # Initialize population\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        \n        # Find best initial solution\n        best_idx = np.argmin(fitness)\n        if fitness[best_idx] < self.f_opt:\n            self.f_opt = fitness[best_idx]\n            self.x_opt = self.population[best_idx]\n\n        success_count = 0\n        eval_count = 0\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                while i in idxs:\n                    idxs = np.random.choice(self.pop_size, 3, replace=False)\n\n                x_r1, x_r2, x_r3 = self.population[idxs]\n                x_mutated = self.population[i] + self.F * (x_r2 - x_r3)\n\n                # Crossover\n                x_trial = np.copy(self.population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR or j == j_rand:\n                        x_trial[j] = x_mutated[j]\n\n                # Clip to bounds\n                x_trial = np.clip(x_trial, func.bounds.lb, func.bounds.ub)\n\n                # Local Search\n                if np.random.rand() < self.local_search_prob:\n                    step_size = 0.01 * (func.bounds.ub - func.bounds.lb)\n                    x_trial += np.random.uniform(-step_size, step_size, size=self.dim)\n                    x_trial = np.clip(x_trial, func.bounds.lb, func.bounds.ub)\n\n                # Selection\n                f_trial = func(x_trial)\n                self.budget -= 1\n                eval_count += 1\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    self.population[i] = x_trial\n                    success_count += 1\n\n                    # Update best solution\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = x_trial\n\n            # Adaptive parameter control\n            if success_count > 0:\n                success_rate = success_count / self.pop_size\n                self.F = np.clip(self.F * (1 + 0.2 * (success_rate - 0.5)), 0.1, 1.0)\n                self.CR = np.clip(self.CR * (1 + 0.2 * (success_rate - 0.5)), 0.1, 0.9)\n            success_count = 0 # Reset success_count\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveDE scored 0.432 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.13264458711752503, 0.31753334250904686, 0.45733345175224493, 0.6115978543139025, 0.3618071973498146, 0.48522882479506146, 0.2826037394475497, 0.34891107909148966, 0.35044026029213415, 0.20688217497282457, 0.3790154495026179, 0.9962198596423949, 0.30418235105981273, 0.44129177704656475, 0.7654533899523481, 0.536962590117974, 0.3292519840745042, 0.5967947229875206, 0.25068559072675656, 0.48865977977335895]}, "task_prompt": ""}
{"id": "2b7b8420-23db-4af9-a17c-53ea44ef3907", "fitness": 0.0, "name": "NeighborhoodParticleSwarm", "description": "No description provided.", "code": "import numpy as np\n\nclass NeighborhoodParticleSwarm:\n    def __init__(self, budget=10000, dim=10, swarm_size=30, neighborhood_size=5, inertia=0.7, cognitive_coeff=1.4, social_coeff=1.4, velocity_clamp=1.0):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = swarm_size\n        self.neighborhood_size = neighborhood_size\n        self.inertia = inertia\n        self.cognitive_coeff = cognitive_coeff\n        self.social_coeff = social_coeff\n        self.velocity_clamp = velocity_clamp\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        \n        # Initialize particles\n        particles = np.random.uniform(lb, ub, size=(self.swarm_size, self.dim))\n        velocities = np.random.uniform(-self.velocity_clamp, self.velocity_clamp, size=(self.swarm_size, self.dim))\n        personal_best_positions = particles.copy()\n        personal_best_fitnesses = np.array([func(x) for x in particles])\n        self.budget -= self.swarm_size\n\n        # Initialize global best\n        best_index = np.argmin(personal_best_fitnesses)\n        global_best_position = personal_best_positions[best_index].copy()\n        self.f_opt = personal_best_fitnesses[best_index]\n        self.x_opt = global_best_position.copy()\n        \n\n        while self.budget > 0:\n            for i in range(self.swarm_size):\n                # Determine neighborhood\n                neighborhood_indices = list(range(max(0, i - self.neighborhood_size // 2), min(self.swarm_size, i + self.neighborhood_size // 2 + 1)))\n\n                # Find best particle in neighborhood\n                neighborhood_best_index = neighborhood_indices[np.argmin(personal_best_fitnesses[neighborhood_indices])]\n                neighborhood_best_position = personal_best_positions[neighborhood_best_index]\n\n                # Update velocity\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                velocities[i] = (self.inertia * velocities[i]\n                                 + self.cognitive_coeff * r1 * (personal_best_positions[i] - particles[i])\n                                 + self.social_coeff * r2 * (neighborhood_best_position - particles[i]))\n                velocities[i] = np.clip(velocities[i], -self.velocity_clamp, self.velocity_clamp)\n\n                # Update position\n                new_position = particles[i] + velocities[i]\n                new_position = np.clip(new_position, lb, ub)\n\n                # Evaluate new position\n                new_fitness = func(new_position)\n                self.budget -= 1\n\n                # Update personal best\n                if new_fitness < personal_best_fitnesses[i]:\n                    personal_best_fitnesses[i] = new_fitness\n                    personal_best_positions[i] = new_position.copy()\n\n                    # Update global best\n                    if new_fitness < self.f_opt:\n                        self.f_opt = new_fitness\n                        self.x_opt = new_position.copy()\n                \n                particles[i] = new_position\n\n            # Adapt neighborhood size (example: shrink if no improvement)\n            if np.min(personal_best_fitnesses) >= self.f_opt:\n                self.neighborhood_size = max(1, self.neighborhood_size - 1)\n            else:\n                self.neighborhood_size = min(self.swarm_size, self.neighborhood_size + 1)\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm NeighborhoodParticleSwarm scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0]}, "task_prompt": ""}
{"id": "817d60a8-8164-4f98-afb7-f2c7c54ce70a", "fitness": 0.12076074285824626, "name": "AdaptiveStepSizeRandomSearch", "description": "No description provided.", "code": "import numpy as np\n\nclass AdaptiveStepSizeRandomSearch:\n    def __init__(self, budget=10000, dim=10, initial_step_size=0.1, success_rate_threshold=0.4, step_size_multiplier=1.2, step_size_divisor=2.0):\n        self.budget = budget\n        self.dim = dim\n        self.initial_step_size = initial_step_size\n        self.success_rate_threshold = success_rate_threshold\n        self.step_size_multiplier = step_size_multiplier\n        self.step_size_divisor = step_size_divisor\n        self.lb = -5.0\n        self.ub = 5.0\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        x = np.random.uniform(self.lb, self.ub, size=self.dim)\n        self.f_opt = func(x)\n        self.x_opt = x.copy()\n        step_size = self.initial_step_size\n        successes = 0\n        iterations = 0\n        eval_count = 1\n\n        while eval_count < self.budget:\n            iterations += 1\n            x_new = x + np.random.uniform(-step_size, step_size, size=self.dim)\n            x_new = np.clip(x_new, self.lb, self.ub)\n            f_new = func(x_new)\n            eval_count += 1\n\n            if f_new < self.f_opt:\n                self.f_opt = f_new\n                self.x_opt = x_new.copy()\n                x = x_new.copy()\n                successes += 1\n\n            if iterations % 10 == 0:\n                success_rate = successes / 10\n                if success_rate > self.success_rate_threshold:\n                    step_size *= self.step_size_multiplier\n                else:\n                    step_size /= self.step_size_divisor\n                successes = 0\n                step_size = min(max(step_size, 1e-6), (self.ub - self.lb)/2)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveStepSizeRandomSearch scored 0.121 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [9.999999999998899e-05, 0.03228369588157742, 0.18664889395825623, 0.11881751760198989, 0.025564510993936862, 0.08167639226881496, 0.19756629576146, 0.1043035080380339, 0.15315769641208066, 0.11537671238024427, 0.06533144891223097, 0.1526022643541135, 0.30237836956868624, 0.12272877473267385, 0.12437960787598279, 0.12101352094934992, 0.1632365528371389, 0.16206394136183022, 0.048341112678722986, 0.13764404059780133]}, "task_prompt": ""}
{"id": "893c3847-ebb8-4d84-b848-4d5d732e7baa", "fitness": "-inf", "name": "CMAES", "description": "No description provided.", "code": "import numpy as np\n\nclass CMAES:\n    def __init__(self, budget=10000, dim=10, pop_size=None, sigma=0.5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size if pop_size else 4 + int(3 * np.log(dim))\n        self.sigma = sigma\n        self.mean = np.zeros(dim)\n        self.C = np.eye(dim)\n        self.lb = -5.0\n        self.ub = 5.0\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.mu = self.pop_size // 2\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        self.cs = (self.mueff + 2) / (self.dim + self.mueff + 5)\n        self.damps = 1 + 2 * max(0, np.sqrt((self.mueff - 1)/(self.dim + 1)) - 1) + self.cs\n        self.ccov1 = (1 / self.mueff) * min(1, (self.mueff + 2) / (self.dim + self.mueff + 5))\n        self.ccovmu = min(1 - self.ccov1, (2 * (self.mueff - 2 + 1/self.mueff)) / ((self.dim + 2)**2 + self.mueff))\n        self.chiN = self.dim**0.5 * (1 - 1/(4*self.dim) + 1/(21*self.dim**2))\n        self.ps = np.zeros(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.B = None\n        self.D = None\n        self.C_updated = 0\n        \n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        \n        while self.budget > 0:\n            # Sample population\n            if self.C_updated == 0:\n                self.D, self.B = np.linalg.eigh(self.C)\n                self.D = np.sqrt(self.D)\n                self.C_updated = 1\n            \n            z = np.random.normal(0, 1, size=(self.dim, self.pop_size))\n            y = self.B @ np.diag(self.D) @ z\n            x = self.mean[:, np.newaxis] + self.sigma * y\n            x = np.clip(x, self.lb, self.ub)\n            \n            # Evaluate population\n            fitness = np.array([func(xi) for xi in x.T])\n            self.budget -= self.pop_size\n            \n            # Sort population\n            idx = np.argsort(fitness)\n            fitness = fitness[idx]\n            x = x[:, idx]\n            \n            # Update optimal solution\n            if fitness[0] < self.f_opt:\n                self.f_opt = fitness[0]\n                self.x_opt = x[:, 0]\n            \n            # Update CMA-ES parameters\n            xmean = np.sum(x[:, :self.mu] * self.weights, axis=1)\n            y_mean = xmean - self.mean\n            y_mean = y_mean / self.sigma\n\n            self.ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs) * self.mueff) * (self.B @ z[:, :self.mu] @ self.weights)\n            norm_ps = np.linalg.norm(self.ps)\n            self.sigma *= np.exp((self.cs / self.damps) * (norm_ps / self.chiN - 1))\n\n            hsig = norm_ps / np.sqrt(1 - (1 - self.cs)**2 * self.budget / self.budget) < (1.4 + 2 / (self.dim + 1)) * self.chiN\n            self.pc = (1 - self.ccov1) * self.pc + hsig * np.sqrt(self.ccov1 * (2 - self.ccov1) * self.mueff) * y_mean\n            \n            artmp = (x[:, :self.mu] - self.mean[:, np.newaxis]) / self.sigma\n            self.C = (1 - self.ccov1 - self.ccovmu + self.ccov1 * (self.ccovmu/self.ccov1) * hsig**2) * self.C + self.ccov1 * self.pc[:, np.newaxis] @ self.pc[np.newaxis, :] + self.ccovmu * artmp @ np.diag(self.weights) @ artmp.T\n            \n            self.mean = xmean\n            self.C = np.triu(self.C) + np.triu(self.C, 1).T\n            self.C = self.C / np.linalg.norm(self.C)\n            self.C_updated = 0\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "In the code, line 151, in _raise_linalgerror_eigenvalues_nonconvergence, the following error occurred:\nLinAlgError: Eigenvalues did not converge", "error": "In the code, line 151, in _raise_linalgerror_eigenvalues_nonconvergence, the following error occurred:\nLinAlgError: Eigenvalues did not converge", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "263c6fd6-1191-40bc-b03a-19726fe3654c", "fitness": 0.49813859576223934, "name": "AdaptiveSimulatedAnnealing", "description": "No description provided.", "code": "import numpy as np\n\nclass AdaptiveSimulatedAnnealing:\n    def __init__(self, budget=10000, dim=10, temp_init=100.0, alpha=0.99, step_init=1.0):\n        self.budget = budget\n        self.dim = dim\n        self.temp = temp_init\n        self.alpha = alpha\n        self.step_size = step_init\n        self.lb = -5.0\n        self.ub = 5.0\n        self.acceptance_rate = 0.0\n        self.acceptance_count = 0\n        self.iteration = 0\n\n    def __call__(self, func):\n        x = np.random.uniform(self.lb, self.ub, self.dim)\n        self.f_opt = func(x)\n        self.x_opt = x.copy()\n        eval_count = 1\n\n        while eval_count < self.budget:\n            self.iteration += 1\n            x_new = x + np.random.normal(0, self.step_size, self.dim)\n            x_new = np.clip(x_new, self.lb, self.ub)\n            f_new = func(x_new)\n            eval_count += 1\n\n            delta_f = f_new - self.f_opt\n            if delta_f < 0 or np.random.rand() < np.exp(-delta_f / self.temp):\n                x = x_new.copy()\n                self.f_opt = f_new\n                if f_new < self.f_opt:\n                    self.x_opt = x_new.copy()\n                self.acceptance_count += 1\n\n            # Temperature update\n            self.temp *= self.alpha\n\n            # Step size adaptation\n            if self.iteration % 100 == 0:\n                self.acceptance_rate = self.acceptance_count / 100\n                self.acceptance_count = 0\n\n                if self.acceptance_rate > 0.6:\n                    self.step_size *= 1.2\n                elif self.acceptance_rate < 0.4:\n                    self.step_size *= 0.8\n                self.step_size = np.clip(self.step_size, 0.01, 2.0)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveSimulatedAnnealing scored 0.498 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.16751327125135762, 0.5769183356567534, 0.5038549963601884, 0.8794868992420071, 0.31410694911398207, 0.5571380822462406, 0.3015757894827691, 0.44142358878318355, 0.4966550028593103, 0.20677871975087725, 0.8647516091548066, 0.9748743450420229, 0.23148738098636423, 0.3000046209546625, 0.8597686945804585, 0.5835098250439439, 0.3115935342300775, 0.687179577655646, 0.21848252344270647, 0.4856681694074295]}, "task_prompt": ""}
{"id": "5531f6e9-83bd-4d17-b4d1-ebcad9357d2f", "fitness": 0.4157238800528683, "name": "HarmonySearch", "description": "No description provided.", "code": "import numpy as np\n\nclass HarmonySearch:\n    def __init__(self, budget=10000, dim=10, HMS=10, HMCR=0.9, PAR=0.3, BW=0.01):\n        self.budget = budget\n        self.dim = dim\n        self.HMS = HMS  # Harmony Memory Size\n        self.HMCR = HMCR # Harmony Memory Consideration Rate\n        self.PAR = PAR   # Pitch Adjusting Rate\n        self.BW = BW     # Bandwidth\n        self.lb = -5.0\n        self.ub = 5.0\n\n    def __call__(self, func):\n        # {Initialize a harmony memory with random solutions, then iteratively improvise new harmonies by considering memory, pitch adjustment, and randomization, accepting better harmonies and updating parameters dynamically.}\n        self.f_opt = np.inf\n        self.x_opt = None\n        \n        # Initialize Harmony Memory\n        HM = np.random.uniform(self.lb, self.ub, size=(self.HMS, self.dim))\n        HM_fitness = np.array([func(x) for x in HM])\n        self.budget -= self.HMS\n        \n        # Find the best harmony in the initial HM\n        best_index = np.argmin(HM_fitness)\n        if HM_fitness[best_index] < self.f_opt:\n            self.f_opt = HM_fitness[best_index]\n            self.x_opt = HM[best_index]\n\n        while self.budget > 0:\n            # Improvise a new harmony\n            new_harmony = np.zeros(self.dim)\n            for i in range(self.dim):\n                if np.random.rand() < self.HMCR:\n                    # Memory Consideration\n                    new_harmony[i] = HM[np.random.randint(0, self.HMS), i]\n                    # Pitch Adjustment\n                    if np.random.rand() < self.PAR:\n                        new_harmony[i] += np.random.uniform(-self.BW, self.BW)\n                        new_harmony[i] = np.clip(new_harmony[i], self.lb, self.ub)\n                else:\n                    # Random Selection\n                    new_harmony[i] = np.random.uniform(self.lb, self.ub)\n            \n            # Evaluate the new harmony\n            new_fitness = func(new_harmony)\n            self.budget -= 1\n            \n            # Replace the worst harmony in HM if the new harmony is better\n            worst_index = np.argmax(HM_fitness)\n            if new_fitness < HM_fitness[worst_index]:\n                HM[worst_index] = new_harmony\n                HM_fitness[worst_index] = new_fitness\n                \n                # Update the best harmony found so far\n                if new_fitness < self.f_opt:\n                    self.f_opt = new_fitness\n                    self.x_opt = new_harmony\n\n            # Adaptive bandwidth adjustment\n            if np.random.rand() < 0.1: # Adjust BW with 10% probability\n                self.BW *= np.random.uniform(0.9, 1.1) # Reduce or increase BW by 10%\n                self.BW = np.clip(self.BW, 0.0001, 0.1)  # Keep BW within reasonable bounds\n        \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm HarmonySearch scored 0.416 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.15622615486596236, 0.23077380671307157, 0.36701568384345207, 0.3933273109470701, 0.27269455593617586, 0.597353450520034, 0.26755492940973, 0.3059975796552362, 0.2972779416001613, 0.19231654274614252, 0.5582687825859771, 0.9982898572086755, 0.2314828311718785, 0.23613995930408205, 0.8263660014135303, 0.6054525053480097, 0.31693041072295214, 0.7453779522693357, 0.22226499634906616, 0.49336634844682203]}, "task_prompt": ""}
{"id": "e4e1fea8-4cec-43b2-937c-4148155f43c1", "fitness": 0.5835680683050416, "name": "HybridDE", "description": "No description provided.", "code": "import numpy as np\n\nclass HybridDE:\n    def __init__(self, budget=10000, dim=10, pop_size=20, local_search_prob=0.1, stagnation_threshold=500):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.local_search_prob = local_search_prob\n        self.stagnation_threshold = stagnation_threshold\n        self.lb = -5.0\n        self.ub = 5.0\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.pop = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.func_evals = self.pop_size\n        self.stagnation_counter = 0\n\n        while self.func_evals < self.budget:\n            best_index = np.argmin(self.fitness)\n            if self.fitness[best_index] < self.f_opt:\n                self.f_opt = self.fitness[best_index]\n                self.x_opt = self.pop[best_index].copy()\n                self.stagnation_counter = 0\n            else:\n                self.stagnation_counter += 1\n\n            if self.stagnation_counter > self.stagnation_threshold:\n                self.pop = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n                self.fitness = np.array([func(x) for x in self.pop])\n                self.func_evals += self.pop_size\n                self.stagnation_counter = 0\n                continue\n\n            for i in range(self.pop_size):\n                # Differential Evolution Mutation\n                indices = np.random.choice(self.pop_size, 3, replace=False)\n                a, b, c = self.pop[indices]\n                mutant = self.pop[i] + 0.5 * (b - c)\n                mutant = np.clip(mutant, self.lb, self.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < 0.9\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, self.pop[i])\n\n                # Local Search\n                if np.random.rand() < self.local_search_prob:\n                    trial = trial + np.random.normal(0, 0.1, self.dim)\n                    trial = np.clip(trial, self.lb, self.ub)\n\n                # Selection\n                f = func(trial)\n                self.func_evals += 1\n\n                if f < self.fitness[i]:\n                    self.pop[i] = trial\n                    self.fitness[i] = f\n\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = trial\n                        self.stagnation_counter = 0\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDE scored 0.584 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.19049690670359076, 0.5373959807233676, 0.5692045628285693, 0.8433995435241647, 0.5802798272705741, 0.723373600662562, 0.34218217130100315, 0.539052396749535, 0.672163043882249, 0.19637754853911615, 0.8532710754207511, 0.9994865306280105, 0.5793185978717383, 0.4451678937081497, 0.8990349779114514, 0.68373265221417, 0.5018287751362938, 0.8205170155071044, 0.20077814337026056, 0.494300122148169]}, "task_prompt": ""}
{"id": "8bc75881-2e77-439b-bd51-201b163e5f94", "fitness": 0.49922491773393307, "name": "AdaptiveSearch", "description": "No description provided.", "code": "import numpy as np\n\nclass AdaptiveSearch:\n    def __init__(self, budget=10000, dim=10, pop_size=20, local_ratio=0.3):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.local_ratio = local_ratio\n        self.lb = -5.0\n        self.ub = 5.0\n        self.population = None\n        self.fitness = None\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)].copy()\n        self.budget -= self.pop_size\n\n    def global_search(self, func):\n        # Implement a simple DE mutation strategy for global exploration\n        for i in range(self.pop_size):\n            if self.budget <= 0:\n                break\n            idxs = [idx for idx in range(self.pop_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            mutant = np.clip(a + 0.7 * (b - c), self.lb, self.ub)\n\n            f_mutant = func(mutant)\n            self.budget -= 1\n            if f_mutant < self.fitness[i]:\n                self.population[i] = mutant\n                self.fitness[i] = f_mutant\n                if f_mutant < self.f_opt:\n                    self.f_opt = f_mutant\n                    self.x_opt = mutant.copy()\n        return\n\n    def local_search(self, func):\n        # Implement a simple gradient-based local search\n        step_size = 0.1\n        for i in range(self.pop_size):\n            if self.budget <= 0:\n                break\n            x = self.population[i].copy()\n            for j in range(self.dim):\n                delta = np.zeros(self.dim)\n                delta[j] = step_size\n                x_plus = np.clip(x + delta, self.lb, self.ub)\n                f_plus = func(x_plus)\n                self.budget -= 1\n                if self.budget <= 0:\n                    break\n\n                if f_plus < self.fitness[i]:\n                    x = x_plus\n                    self.fitness[i] = f_plus\n                    self.population[i] = x\n                    if f_plus < self.f_opt:\n                        self.f_opt = f_plus\n                        self.x_opt = x.copy()\n                \n                delta[j] = -step_size\n                x_minus = np.clip(x + delta, self.lb, self.ub)\n                f_minus = func(x_minus)\n                self.budget -= 1\n                if self.budget <= 0:\n                  break\n\n                if f_minus < self.fitness[i]:\n                    x = x_minus\n                    self.fitness[i] = f_minus\n                    self.population[i] = x\n                    if f_minus < self.f_opt:\n                        self.f_opt = f_minus\n                        self.x_opt = x.copy()\n                \n                if self.budget <= 0:\n                  break\n\n        return\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        while self.budget > 0:\n            if np.random.rand() < self.local_ratio:\n                self.local_search(func)\n            else:\n                self.global_search(func)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveSearch scored 0.499 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.17354002774699362, 0.4131514894262547, 0.46154747999639467, 0.8373993920383069, 0.2591867377501156, 0.5778187575508391, 0.31249653085637463, 0.3722294506540993, 0.46003941562979966, 0.20630199986948072, 0.7929009136204013, 0.9470986539180082, 0.4252542057343812, 0.44533108530735066, 0.8956615016383909, 0.5717177988239335, 0.40295917832329786, 0.7409128184308069, 0.2154011520025758, 0.47354976536085647]}, "task_prompt": ""}
{"id": "fcc83016-1beb-4741-a4b4-42525d777ebc", "fitness": 0.37625831131333054, "name": "FitnessWeightedSampling", "description": "No description provided.", "code": "import numpy as np\n\nclass FitnessWeightedSampling:\n    def __init__(self, budget=10000, dim=10, pop_size=20, sample_size=5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.sample_size = sample_size\n        self.lb = -5.0\n        self.ub = 5.0\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.pop = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.eval_count = self.pop_size\n\n        while self.eval_count < self.budget:\n            # Calculate fitness weights\n            fitness_min = np.min(self.fitness)\n            fitness_max = np.max(self.fitness)\n\n            if fitness_max == fitness_min:\n                weights = np.ones(self.pop_size) / self.pop_size\n            else:\n                # Scale fitness to be between 0 and 1, higher fitness gets higher weight\n                weights = (fitness_max - self.fitness) / (fitness_max - fitness_min)\n                weights = weights / np.sum(weights)  # Normalize to sum to 1\n\n            # Generate new solutions by sampling from the population\n            new_pop = []\n            for _ in range(self.pop_size):\n                # Sample indices based on fitness weights\n                indices = np.random.choice(self.pop_size, self.sample_size, replace=False, p=weights)\n                sample = self.pop[indices]\n                \n                # Create a new solution by averaging the sampled solutions and adding noise\n                new_x = np.mean(sample, axis=0) + np.random.normal(0, 0.5, self.dim)  # Adjust noise scale as needed\n                new_x = np.clip(new_x, self.lb, self.ub)\n                new_pop.append(new_x)\n\n            new_pop = np.array(new_pop)\n            new_fitness = np.array([func(x) for x in new_pop])\n            self.eval_count += self.pop_size\n\n            # Update the population\n            for i in range(self.pop_size):\n                if new_fitness[i] < self.fitness[i]:\n                    self.pop[i] = new_pop[i]\n                    self.fitness[i] = new_fitness[i]\n\n                    if new_fitness[i] < self.f_opt:\n                        self.f_opt = new_fitness[i]\n                        self.x_opt = new_pop[i]\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm FitnessWeightedSampling scored 0.376 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.1359401554787979, 0.31090351574901887, 0.3743841922664627, 0.419318577622279, 0.2862389200228499, 0.36797143595257376, 0.29459186652702674, 0.3382908351424949, 0.3089879277354952, 0.19697632178222613, 0.44313463427034305, 0.9904645554253856, 0.2623341301108919, 0.296740595706832, 0.7297515712539782, 0.3524943383178384, 0.3120328696675728, 0.4285399829958154, 0.17857721086186917, 0.49749258937685936]}, "task_prompt": ""}
{"id": "9d7bf849-4ac2-4f67-be1f-2b562fe62d5e", "fitness": 0.27035725497394403, "name": "BudgetAwareCMAES", "description": "No description provided.", "code": "import numpy as np\n\nclass BudgetAwareCMAES:\n    def __init__(self, budget=10000, dim=10, initial_pop_size=None, restart_factor=3):\n        self.budget = budget\n        self.dim = dim\n        self.initial_pop_size = initial_pop_size if initial_pop_size is not None else 4 + int(3 * np.log(dim))\n        self.restart_factor = restart_factor\n        self.x_mean = None\n        self.sigma = None\n        self.C = None\n        self.pc = None\n        self.ps = None\n        self.chiN = None\n        self.mu = None\n        self.weights = None\n        self.mueff = None\n        self.cc = None\n        self.cs = None\n        self.damps = None\n        self.ccov1 = None\n        self.ccovmu = None\n        self.pop_size = self.initial_pop_size\n        self.func_evals = 0\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.restarts = 0\n\n    def initialize(self, func):\n        self.x_mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        self.sigma = 0.3\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.chiN = self.dim**0.5 * (1 - 1/(4*self.dim) + 1/(21*self.dim**2))\n        self.mu = self.pop_size // 2\n        self.weights = np.log(self.mu+1/2) - np.log(np.arange(1, self.mu+1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        self.cc = (4 + self.mueff/self.dim) / (self.dim + 4 + 2*self.mueff/self.dim)\n        self.cs = (self.mueff + 2) / (self.dim + self.mueff + 5)\n        self.damps = (1 + 2*max(0, np.sqrt((self.mueff-1)/(self.dim+1))-1) + self.cs)\n        self.ccov1 = 2 / ((self.dim+1.3)**2 + self.mueff)\n        self.ccovmu = 2 * (self.mueff-2+1/self.mueff) / ((self.dim+2)**2 + self.mueff)\n        self.ccovmu = min(1-self.ccov1, self.ccovmu)\n\n    def __call__(self, func):\n        self.initialize(func)\n        while self.func_evals < self.budget:\n            try:\n                z = np.random.normal(0, 1, size=(self.pop_size, self.dim))\n                B = None\n                D = None\n                C = self.C\n                eigen_decomposition = np.linalg.eigh(C)\n                D = np.diag(np.sqrt(eigen_decomposition[0]))\n                B = eigen_decomposition[1]\n                x = self.x_mean + self.sigma * np.dot(z, np.dot(B, D))\n                x = np.clip(x, func.bounds.lb, func.bounds.ub)\n                f = np.array([func(xi) for xi in x])\n                self.func_evals += self.pop_size\n\n                if np.any(f < self.f_opt):\n                    best_index = np.argmin(f)\n                    if f[best_index] < self.f_opt:\n                        self.f_opt = f[best_index]\n                        self.x_opt = x[best_index]\n\n                idx = np.argsort(f)\n                x_best = x[idx[:self.mu]]\n                z_best = z[idx[:self.mu]]\n\n                x_mean_old = self.x_mean.copy()\n                self.x_mean = np.sum(x_best * self.weights[:, None], axis=0)\n\n                self.ps = (1-self.cs) * self.ps + np.sqrt(self.cs*(2-self.cs)*self.mueff) * np.dot(B, z_best[0].T) #z_best weighted sum removed\n                hsig = (np.linalg.norm(self.ps)/np.sqrt(1-(1-self.cs)**(2*(self.budget/self.pop_size)))/self.chiN < 1.4 + 2/(self.dim+1))\n                self.pc = (1-self.cc) * self.pc + hsig * np.sqrt(self.cc*(2-self.cc)*self.mueff) * (self.x_mean - x_mean_old) / self.sigma\n\n                self.C = (1-self.ccov1-self.ccovmu) * self.C + self.ccov1 * np.outer(self.pc, self.pc) + self.ccovmu * np.dot(z_best.T, np.dot(np.diag(self.weights), z_best))\n                self.sigma *= np.exp((self.cs/self.damps) * (np.linalg.norm(self.ps)/self.chiN - 1))\n            except np.linalg.LinAlgError:\n                self.restarts += 1\n                self.initialize(func)\n                self.pop_size = min(self.initial_pop_size * self.restart_factor, self.budget - self.func_evals)\n                if self.pop_size <= 0:\n                    break\n                self.mu = self.pop_size // 2\n                self.weights = np.log(self.mu+1/2) - np.log(np.arange(1, self.mu+1))\n                self.weights = self.weights / np.sum(self.weights)\n                self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n                self.cc = (4 + self.mueff/self.dim) / (self.dim + 4 + 2*self.mueff/self.dim)\n                self.cs = (self.mueff + 2) / (self.dim + self.mueff + 5)\n                self.damps = (1 + 2*max(0, np.sqrt((self.mueff-1)/(self.dim+1))-1) + self.cs)\n                self.ccov1 = 2 / ((self.dim+1.3)**2 + self.mueff)\n                self.ccovmu = 2 * (self.mueff-2+1/self.mueff) / ((self.dim+2)**2 + self.mueff)\n                self.ccovmu = min(1-self.ccov1, self.ccovmu)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm BudgetAwareCMAES scored 0.270 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.11638975114746586, 0.1429261331359296, 0.26756377215858207, 0.20707537147688637, 0.28943698523200057, 0.22753571709213716, 0.19957472404857812, 0.2692381360559538, 0.17413784172592317, 0.14955541375691195, 0.16248681038510882, 0.9963449525944195, 0.23348817319796544, 0.1661315559422557, 0.5429579800374366, 0.24481108932564244, 0.19962768096942263, 0.25541641918334823, 0.13439553316547603, 0.4280510588474372]}, "task_prompt": ""}
{"id": "cb56bcb5-f69f-47ae-a16b-03faedf01605", "fitness": 0.672922215684701, "name": "AdaptiveDE", "description": "No description provided.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, mutation_factor=0.5, crossover_rate=0.7, reduction_factor=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.mutation_factor = mutation_factor\n        self.crossover_rate = crossover_rate\n        self.reduction_factor = reduction_factor\n        self.lb = -5.0\n        self.ub = 5.0\n        self.population = None\n        self.fitness = None\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)].copy()\n        self.budget -= self.pop_size\n\n    def evolve(self, func):\n        new_population = np.copy(self.population)\n        new_fitness = np.copy(self.fitness)\n        for i in range(self.pop_size):\n            if self.budget <= 0:\n                break\n\n            idxs = [idx for idx in range(self.pop_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n\n            # Adaptive mutation factor\n            mutant = np.clip(a + self.mutation_factor * (b - c), self.lb, self.ub)\n\n            # Crossover\n            cross_points = np.random.rand(self.dim) < self.crossover_rate\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial_vector = np.where(cross_points, mutant, self.population[i])\n\n            f_trial = func(trial_vector)\n            self.budget -= 1\n\n            if f_trial < self.fitness[i]:\n                new_population[i] = trial_vector\n                new_fitness[i] = f_trial\n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = trial_vector.copy()\n\n        self.population = new_population\n        self.fitness = new_fitness\n        \n        # Population size reduction (optional)\n        if self.budget > 0 and len(self.population) > 10 and np.random.rand() < 0.1:\n            indices = np.argsort(self.fitness)[::-1] #sort from worst to best\n            remove_count = int(len(self.population) * (1 - self.reduction_factor))\n            indices_to_keep = indices[:-remove_count] #indices of the best solutions\n            \n            self.population = self.population[indices_to_keep]\n            self.fitness = self.fitness[indices_to_keep]\n            self.pop_size = len(self.population)\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        while self.budget > 0:\n            self.evolve(func)\n            # Adapt mutation factor\n            if np.random.rand() < 0.2:\n                self.mutation_factor = np.random.uniform(0.3, 0.9)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveDE scored 0.673 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.2602156910747113, 0.6302258941515779, 0.45535589041055013, 0.8037402272238257, 0.7480302044874105, 0.7964814473957638, 0.6411432638691488, 0.6588001817229356, 0.7099439956885453, 0.6914060665738305, 0.7228020796300632, 0.9918820793509175, 0.2850179484026829, 0.7570963081501221, 0.859567341870315, 0.7831930044938532, 0.6579742230519996, 0.814371918406068, 0.6784861663397904, 0.5127103813999085]}, "task_prompt": ""}
{"id": "d5d67ed3-a552-4267-a64b-d9b7b5909c28", "fitness": "-inf", "name": "SelfAdaptiveDE", "description": "No description provided.", "code": "import numpy as np\n\nclass SelfAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, lb=-5.0, ub=5.0, shrink_factor = 0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lb = lb\n        self.ub = ub\n        self.f = 0.5\n        self.cr = 0.7\n        self.shrink_factor = shrink_factor\n\n    def __call__(self, func):\n        population = np.random.uniform(self.lb, self.ub, (self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)].copy()\n        eval_count = self.pop_size\n\n        successful_mutations = 0\n\n        while eval_count < self.budget and self.pop_size > 2:\n\n            for i in range(self.pop_size):\n                # Mutation\n                indices = np.random.choice(self.pop_size, 3, replace=False)\n                while i in indices:\n                    indices = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = population[indices]\n                v_mutation = x_r1 + self.f * (x_r2 - x_r3)\n                v_mutation = np.clip(v_mutation, self.lb, self.ub)\n\n                # Crossover\n                j_rand = np.random.randint(self.dim)\n                u = np.random.rand(self.dim)\n                mask = (u <= self.cr) | (np.arange(self.dim) == j_rand)\n                u_crossover = np.where(mask, v_mutation, population[i])\n\n                # Evaluation\n                f_trial = func(u_crossover)\n                eval_count += 1\n\n                # Selection\n                if f_trial < fitness[i]:\n                    population[i] = u_crossover\n                    fitness[i] = f_trial\n                    successful_mutations += 1\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = u_crossover.copy()\n\n                if eval_count >= self.budget:\n                    break\n\n            # Parameter Adaptation\n            if successful_mutations / self.pop_size < 0.1:\n                self.f *= 1.1\n            elif successful_mutations / self.pop_size > 0.9:\n                self.f *= 0.9\n            self.f = np.clip(self.f, 0.1, 1.0)\n            \n            successful_mutations = 0 # Reset counter\n\n            # Population size reduction\n            if eval_count > self.budget * 0.7:\n                 self.pop_size = int(self.pop_size * self.shrink_factor)\n                 if self.pop_size < 2:\n                     self.pop_size = 2\n\n                 population = population[:self.pop_size]\n                 fitness = fitness[:self.pop_size]\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "Evaluation timed out after 300 seconds.", "error": "Evaluation timed out after 300 seconds.", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "e835bf8e-34aa-4ca6-a166-90a4fe573391", "fitness": "-inf", "name": "CMAES_DE", "description": "No description provided.", "code": "import numpy as np\nimport cma\n\nclass CMAES_DE:\n    def __init__(self, budget=10000, dim=10, pop_size=20, archive_size=10, cmaes_sigma=0.5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.F = 0.5\n        self.CR = 0.9\n        self.cmaes_sigma = cmaes_sigma\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.func_evals = 0\n\n        # Initialize CMA-ES\n        es = cma.purecma.CMAES(np.zeros(self.dim), self.cmaes_sigma,\n                              {'bounds': [func.bounds.lb, func.bounds.ub],\n                               'popsize': self.pop_size})\n\n        while self.func_evals < self.budget and not es.stop():\n            # Ask CMA-ES for new points\n            solutions = es.ask()\n            \n            # Evaluate solutions from CMA-ES\n            fitness = np.array([func(x) for x in solutions])\n            self.func_evals += self.pop_size\n            \n            # Update CMA-ES\n            es.tell(solutions, fitness)\n\n            # Differential Evolution refinement\n            for i in range(self.pop_size):\n                # Mutation\n                indices = np.random.choice(self.pop_size, 3, replace=False)\n                a, b, c = solutions[indices]\n                mutant = solutions[i] + self.F * (b - c)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, solutions[i])\n\n                # Selection\n                f = func(trial)\n                self.func_evals += 1\n\n                if f < fitness[i]:\n                    solutions[i] = trial\n                    fitness[i] = f\n\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = trial\n            \n            best_index = np.argmin(fitness)\n            if fitness[best_index] < self.f_opt:\n                self.f_opt = fitness[best_index]\n                self.x_opt = solutions[best_index]\n            \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "In the code, line 2, in <module>, the following error occurred:\nModuleNotFoundError: No module named 'cma'\nOn line: import cma", "error": "In the code, line 2, in <module>, the following error occurred:\nModuleNotFoundError: No module named 'cma'\nOn line: import cma", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "5a9a849f-bad8-42ef-81f9-d4a6e23c2073", "fitness": 0.5992278978177559, "name": "AdaptiveDE", "description": "No description provided.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=20, cr_init=0.5, f_init=0.7):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.cr = cr_init  # Initial crossover rate\n        self.f = f_init  # Initial mutation factor\n        self.lb = -5.0\n        self.ub = 5.0\n        self.cr_memory = []\n        self.f_memory = []\n        self.memory_size = 10\n        self.success_rate = 0.0\n\n    def __call__(self, func):\n        # Initialization\n        population = np.random.uniform(self.lb, self.ub, (self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)].copy()\n        eval_count = self.pop_size\n\n        while eval_count < self.budget:\n            new_population = np.zeros_like(population)\n            new_fitness = np.zeros_like(fitness)\n            success_count = 0\n\n            for i in range(self.pop_size):\n                # Select three random individuals (excluding the current one)\n                indices = np.random.choice(self.pop_size, 3, replace=False)\n                while i in indices:\n                    indices = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = population[indices]\n\n                # Mutation\n                v_mutation = x_r1 + self.f * (x_r2 - x_r3)\n                v_mutation = np.clip(v_mutation, self.lb, self.ub)\n\n                # Crossover\n                j_rand = np.random.randint(self.dim)\n                u = np.random.rand(self.dim)\n                mask = (u <= self.cr) | (np.arange(self.dim) == j_rand)\n                u_crossover = np.where(mask, v_mutation, population[i])\n\n                # Evaluation\n                f_trial = func(u_crossover)\n                eval_count += 1\n\n                # Selection\n                if f_trial < fitness[i]:\n                    new_population[i] = u_crossover\n                    new_fitness[i] = f_trial\n                    success_count += 1\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = u_crossover.copy()\n                else:\n                    new_population[i] = population[i]\n                    new_fitness[i] = fitness[i]\n\n                if eval_count >= self.budget:\n                    break\n\n            population = new_population\n            fitness = new_fitness\n\n            # Adaptive parameter control\n            self.success_rate = success_count / self.pop_size\n            self.cr = np.clip(self.cr + 0.1 * (self.success_rate - 0.5), 0.1, 0.9) # Adjust CR\n            self.f = np.clip(self.f + 0.1 * (self.success_rate - 0.5), 0.3, 1.0)   # Adjust F\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveDE scored 0.599 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.2000134900106777, 0.23604053412698733, 0.5530466388222754, 0.8229184536940479, 0.5933844859396853, 0.877510910284362, 0.39338586576294177, 0.5476967314419923, 0.8052247939808546, 0.6393155000868722, 0.716671482879289, 0.9915142646988218, 0.26260171118532294, 0.48570919198068874, 0.8228205392438284, 0.8887482889383245, 0.45173622481516973, 0.8743114914536617, 0.31783819139921277, 0.5040691656101011]}, "task_prompt": ""}
{"id": "81361b4d-8fc5-4c54-89d9-98ae86a70f1a", "fitness": 0.0, "name": "AdaptiveDE", "description": "No description provided.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=15, restart_prob=0.05):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.restart_prob = restart_prob\n        self.lb = -5.0\n        self.ub = 5.0\n        self.mutation_factor = 0.5\n        self.crossover_prob = 0.7\n        self.success_history = []\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.pop = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.func_evals = self.pop_size\n\n        while self.func_evals < self.budget:\n            \n            if np.random.rand() < self.restart_prob:\n                self.pop = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n                self.fitness = np.array([func(x) for x in self.pop])\n                self.func_evals += self.pop_size\n                continue\n            \n            for i in range(self.pop_size):\n                # Differential Evolution Mutation\n                indices = np.random.choice(self.pop_size, 3, replace=False)\n                a, b, c = self.pop[indices]\n                mutant = self.pop[i] + self.mutation_factor * (b - c)\n                mutant = np.clip(mutant, self.lb, self.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.crossover_prob\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, self.pop[i])\n\n                # Selection\n                f = func(trial)\n                self.func_evals += 1\n\n                if f < self.fitness[i]:\n                    self.success_history.append(1)\n                    self.pop[i] = trial\n                    self.fitness[i] = f\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = trial\n                else:\n                    self.success_history.append(0)\n\n            # Adaptive Parameter Control\n            if len(self.success_history) > self.pop_size:\n                success_rate = np.mean(self.success_history[-self.pop_size:])\n                self.mutation_factor = np.clip(self.mutation_factor + 0.1 * (success_rate - 0.5), 0.1, 0.9)\n                self.crossover_prob = np.clip(self.crossover_prob + 0.1 * (success_rate - 0.5), 0.1, 0.9)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveDE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0]}, "task_prompt": ""}
{"id": "153f5a48-3959-4d0c-bdc4-2989d05d846e", "fitness": 0.0, "name": "BudgetAwareCMAES", "description": "No description provided.", "code": "import numpy as np\n\nclass BudgetAwareCMAES:\n    def __init__(self, budget=10000, dim=10, pop_size=None, initial_step_size=1.0):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size if pop_size else 4 + int(3 * np.log(dim))\n        self.mu = self.pop_size // 2\n        self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n        self.weights /= np.sum(self.weights)\n        self.m = np.zeros(self.dim)\n        self.sigma = initial_step_size\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.chiN = np.sqrt(self.dim) * (1 - (1 / (4 * self.dim)) + (1 / (21 * self.dim**2)))\n        self.c_sigma = (self.mu / (self.dim + self.mu))**0.5\n        self.d_sigma = 1 + 2 * max(0, np.sqrt((self.mu - 1) / (self.dim + 1)) - 1) + self.c_sigma\n        self.c_c = (4 + self.mu / self.dim) / (self.dim + 4 + 2 * self.mu / self.dim)\n        self.c_1 = 2 / ((self.dim + 1.3)**2 + self.mu)\n        self.c_mu = min(1 - self.c_1, 2 * (self.mu - 2 + 1 / self.mu) / ((self.dim + 2)**2 + self.mu))\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.func_evals = 0\n        self.lb = -5.0\n        self.ub = 5.0\n\n    def __call__(self, func):\n        while self.func_evals < self.budget:\n            z = np.random.randn(self.dim)\n            try:\n                A = np.linalg.cholesky(self.C)\n            except np.linalg.LinAlgError:\n                A = np.eye(self.dim)\n                self.C = np.eye(self.dim)\n\n            x = self.m + self.sigma * A @ z\n\n            x = np.clip(x, self.lb, self.ub)\n            f = func(x)\n            self.func_evals += 1\n            \n            X = np.zeros((self.pop_size, self.dim))\n            F = np.zeros(self.pop_size)\n\n            for k in range(self.pop_size):\n                z = np.random.randn(self.dim)\n                x = self.m + self.sigma * A @ z\n                x = np.clip(x, self.lb, self.ub)\n                X[k, :] = x\n                F[k] = func(x)\n                self.func_evals += 1\n                if self.func_evals >= self.budget:\n                    break\n                    \n            if self.func_evals >= self.budget:\n                break\n            \n            idx = np.argsort(F)\n            X = X[idx]\n            F = F[idx]\n\n            if F[0] < self.f_opt:\n                self.f_opt = F[0]\n                self.x_opt = X[0]\n            \n            xmean = np.sum(self.weights[:, None] * X[:self.mu], axis=0)\n            \n            self.ps = (1 - self.c_sigma) * self.ps + np.sqrt(self.c_sigma * (2 - self.c_sigma)) * (A @ ((xmean - self.m) / self.sigma))\n            self.pc = (1 - self.c_c) * self.pc + np.sqrt(self.c_c * (2 - self.c_c)) * ((xmean - self.m) / self.sigma) * (np.linalg.norm(self.ps) < (1.4 + 2 / (self.dim + 1)) * self.chiN)\n            self.C = (1 - self.c_1 - self.c_mu) * self.C + self.c_1 * np.outer(self.pc, self.pc)\n            \n            for k in range(self.mu):\n                y = (X[k] - self.m) / self.sigma\n                self.C += self.c_mu * self.weights[k] * np.outer(y, y)\n\n            self.m = xmean\n            \n            # Budget-aware step size adaptation\n            fraction_spent = self.func_evals / self.budget\n            target_sigma = 1.0  # Example target, can be adjusted\n            self.sigma *= np.exp(self.c_sigma / self.d_sigma * ((np.linalg.norm(self.ps) / self.chiN) - 1))\n            self.sigma = min(self.sigma, (1 - fraction_spent) * 2.0) # Reduce step size as budget is used\n            self.C = np.triu(self.C) + np.triu(self.C, 1).T\n            \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm BudgetAwareCMAES scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0]}, "task_prompt": ""}
{"id": "2696df97-8c1b-453d-b6f6-307722ad4f33", "fitness": 0.4899498109557479, "name": "AdaptiveDE_SHADE", "description": "No description provided.", "code": "import numpy as np\n\nclass AdaptiveDE_SHADE:\n    def __init__(self, budget=10000, dim=10, pop_size=100, memory_size=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.memory_size = memory_size\n        self.lb = -5.0\n        self.ub = 5.0\n        self.population = None\n        self.fitness = None\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.memory_cr = np.full(self.memory_size, 0.5)\n        self.memory_f = np.full(self.memory_size, 0.5)\n        self.archive = []\n        self.p = 0.1  # Percentile value for selecting p-best solutions\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)].copy()\n        self.budget -= self.pop_size\n\n    def evolve(self, func):\n        new_population = np.copy(self.population)\n        new_fitness = np.copy(self.fitness)\n        success_cr = []\n        success_f = []\n        \n        ranked_indices = np.argsort(self.fitness)\n        p_best_count = max(1, int(self.p * self.pop_size))\n        p_best_indices = ranked_indices[:p_best_count]\n\n        for i in range(self.pop_size):\n            if self.budget <= 0:\n                break\n            \n            # Select memory index\n            memory_index = np.random.randint(self.memory_size)\n            \n            # Sample CR and F\n            cr = np.random.normal(self.memory_cr[memory_index], 0.1)\n            cr = np.clip(cr, 0, 1)\n            \n            f = np.random.normal(self.memory_f[memory_index], 0.1)\n            while f <= 0:\n                f = np.random.normal(self.memory_f[memory_index], 0.1)\n            f = np.clip(f, 0.00001, 1.0)\n\n            # Selection of p-best individual\n            p_best_idx = np.random.choice(p_best_indices)\n            \n            # Selection of random individuals\n            idxs = [idx for idx in range(self.pop_size) if idx != i]\n            r1_idx = np.random.choice(idxs)\n            \n            archive_size = len(self.archive)\n            if archive_size > 0:\n                r2_idx = np.random.randint(archive_size)\n                mutant = np.clip(self.population[i] + f * (self.population[p_best_idx] - self.population[i]) + f * (self.population[r1_idx] - self.archive[r2_idx]), self.lb, self.ub)\n            else:\n                 mutant = np.clip(self.population[i] + f * (self.population[p_best_idx] - self.population[i]) + f * (self.population[r1_idx] - self.population[np.random.choice(idxs)]), self.lb, self.ub)\n\n            # Crossover\n            cross_points = np.random.rand(self.dim) < cr\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial_vector = np.where(cross_points, mutant, self.population[i])\n\n            f_trial = func(trial_vector)\n            self.budget -= 1\n\n            if f_trial < self.fitness[i]:\n                new_population[i] = trial_vector\n                new_fitness[i] = f_trial\n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = trial_vector.copy()\n                success_cr.append(cr)\n                success_f.append(f)\n                self.archive.append(self.population[i].copy())\n                if len(self.archive) > self.pop_size:\n                    self.archive.pop(0)\n\n        self.population = new_population\n        self.fitness = new_fitness\n        \n        # Update memory\n        if success_cr:\n            self.memory_cr[memory_index] = np.mean(success_cr)\n        if success_f:\n            self.memory_f[memory_index] = np.mean(success_f)\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        while self.budget > 0:\n            self.evolve(func)\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveDE_SHADE scored 0.490 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.18402892277002758, 0.29565350066782525, 0.4746251952573487, 0.7289263622854776, 0.4213957789159626, 0.5614895161739457, 0.3353909970649356, 0.42834693227445464, 0.46702434102736146, 0.2624415258873639, 0.6938859550149082, 0.9956945312018796, 0.35433583423741666, 0.4345325915621713, 0.8127581915818426, 0.5634602366026922, 0.38802175889850155, 0.6812860389278195, 0.21375435810307752, 0.501943650659946]}, "task_prompt": ""}
{"id": "b861b142-06eb-4588-8d84-f5aab6d8c416", "fitness": 0.18125212792195028, "name": "GradientAnnealing", "description": "No description provided.", "code": "import numpy as np\n\nclass GradientAnnealing:\n    def __init__(self, budget=10000, dim=10, initial_temp=1.0, cooling_rate=0.001, step_size=0.1, num_neighbors=5):\n        self.budget = budget\n        self.dim = dim\n        self.initial_temp = initial_temp\n        self.cooling_rate = cooling_rate\n        self.step_size = step_size\n        self.num_neighbors = num_neighbors\n        self.lb = -5.0\n        self.ub = 5.0\n        self.x_opt = None\n        self.f_opt = np.inf\n\n    def estimate_gradient(self, func, x, num_neighbors):\n        gradient = np.zeros(self.dim)\n        for _ in range(num_neighbors):\n            direction = np.random.randn(self.dim)\n            direction /= np.linalg.norm(direction)\n            x_plus = np.clip(x + self.step_size * direction, self.lb, self.ub)\n            x_minus = np.clip(x - self.step_size * direction, self.lb, self.ub)\n\n            if self.budget >= 2:\n                f_plus = func(x_plus)\n                f_minus = func(x_minus)\n                self.budget -= 2\n                gradient += (f_plus - f_minus) * direction\n            else:\n                break #if budget runs out, return zero gradient\n            \n        return gradient\n\n    def __call__(self, func):\n        x = np.random.uniform(self.lb, self.ub, self.dim)\n        self.f_opt = func(x)\n        self.x_opt = x.copy()\n        self.budget -= 1\n        temperature = self.initial_temp\n\n        while self.budget > 0:\n            gradient = self.estimate_gradient(func, x, self.num_neighbors)\n            \n            if self.budget <= 0:\n                break\n\n            #move against gradient, with annealing\n            x_new = x - temperature * self.step_size * gradient\n            x_new = np.clip(x_new, self.lb, self.ub)\n\n            f_new = func(x_new)\n            self.budget -= 1\n\n            delta_f = f_new - self.f_opt\n\n            if delta_f < 0:\n                x = x_new.copy()\n                self.f_opt = f_new\n                self.x_opt = x.copy()\n            else:\n                # Simulated annealing acceptance probability\n                acceptance_prob = np.exp(-delta_f / temperature)\n                if np.random.rand() < acceptance_prob:\n                    x = x_new.copy()\n\n            temperature *= (1 - self.cooling_rate)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm GradientAnnealing scored 0.181 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.2168007235131877, 0.21428123568239166, 0.17234327854513432, 0.12098524183883053, 0.1558964540413048, 0.12603746305824393, 0.1883741606955971, 0.17800180458423875, 0.1472500267670548, 0.14818122377388532, 0.12094951765736806, 0.4590270615953672, 0.19654294610520717, 0.17255031796328624, 0.16307087447451563, 0.23683554226374115, 0.171041218064109, 0.154101869926757, 0.13348936645392517, 0.14928223143486063]}, "task_prompt": ""}
{"id": "8d16ea50-5743-42ab-87d5-7674948b6fe9", "fitness": "-inf", "name": "GaussianAdaptation", "description": "No description provided.", "code": "import numpy as np\nfrom scipy.stats import truncnorm\n\nclass GaussianAdaptation:\n    def __init__(self, budget=10000, dim=10, pop_size=20, selection_threshold=0.2):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.selection_threshold = selection_threshold\n        self.lb = -5.0\n        self.ub = 5.0\n        self.mean = np.zeros(dim)\n        self.sigma = np.ones(dim) * 2  # Initial standard deviation\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def sample(self):\n        samples = np.zeros((self.pop_size, self.dim))\n        for i in range(self.dim):\n            lower = (self.lb - self.mean[i]) / self.sigma[i]\n            upper = (self.ub - self.mean[i]) / self.sigma[i]\n            samples[:, i] = truncnorm.rvs(lower, upper, loc=self.mean[i], scale=self.sigma[i], size=self.pop_size)\n        return samples\n\n    def __call__(self, func):\n        eval_count = 0\n        while eval_count < self.budget:\n            samples = self.sample()\n            fitness = np.array([func(x) for x in samples])\n            eval_count += self.pop_size\n\n            if np.min(fitness) < self.f_opt:\n                self.f_opt = np.min(fitness)\n                self.x_opt = samples[np.argmin(fitness)].copy()\n\n            # Select top samples\n            threshold = np.quantile(fitness, self.selection_threshold)\n            selected_samples = samples[fitness <= threshold]\n\n            if len(selected_samples) > 0:\n                self.mean = np.mean(selected_samples, axis=0)\n                self.sigma = np.std(selected_samples, axis=0) + 1e-8 #avoid zero std\n            else:\n                # If no samples are selected, re-initialize the mean\n                self.mean = np.random.uniform(self.lb, self.ub, self.dim)\n                self.sigma = np.ones(self.dim) * 2  # Reset standard deviation\n\n            # Ensure sigma stays within reasonable bounds\n            self.sigma = np.clip(self.sigma, 0.1, 5)\n\n            if eval_count >= self.budget:\n                break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "In the code, line 22, in sample, the following error occurred:\nNameError: name 'truncnorm' is not defined\nOn line: samples[:, i] = truncnorm.rvs(lower, upper, loc=self.mean[i], scale=self.sigma[i], size=self.pop_size)", "error": "In the code, line 22, in sample, the following error occurred:\nNameError: name 'truncnorm' is not defined\nOn line: samples[:, i] = truncnorm.rvs(lower, upper, loc=self.mean[i], scale=self.sigma[i], size=self.pop_size)", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "4ff0005e-437d-4550-8baf-bbb780a3d220", "fitness": "-inf", "name": "NicheClearing", "description": "No description provided.", "code": "import numpy as np\nfrom sklearn.cluster import KMeans\n\nclass NicheClearing:\n    def __init__(self, budget=10000, dim=10, pop_size=50, n_clusters=5, niche_radius=0.5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.n_clusters = n_clusters\n        self.niche_radius = niche_radius\n        self.lb = -5.0\n        self.ub = 5.0\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        # Initialization\n        population = np.random.uniform(self.lb, self.ub, (self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)].copy()\n        eval_count = self.pop_size\n\n        while eval_count < self.budget:\n            # Clustering\n            kmeans = KMeans(n_clusters=self.n_clusters, n_init=1, random_state=0)\n            clusters = kmeans.fit_predict(population)\n\n            # Niche Clearing\n            for i in range(self.n_clusters):\n                cluster_indices = np.where(clusters == i)[0]\n                if len(cluster_indices) > 0:\n                    # Find the best individual in the niche\n                    best_index = cluster_indices[np.argmin(fitness[cluster_indices])]\n                    best_individual = population[best_index]\n\n                    # Sample new solutions around the best individual in the niche\n                    for _ in range(len(cluster_indices)):\n                        if eval_count >= self.budget:\n                            break\n\n                        new_solution = best_individual + np.random.normal(0, self.niche_radius, self.dim)\n                        new_solution = np.clip(new_solution, self.lb, self.ub)\n                        new_fitness = func(new_solution)\n                        eval_count += 1\n\n                        # Replace a random member of the niche if the new solution is better\n                        replace_index = np.random.choice(cluster_indices)\n                        if new_fitness < fitness[replace_index]:\n                            population[replace_index] = new_solution\n                            fitness[replace_index] = new_fitness\n\n                            if new_fitness < self.f_opt:\n                                self.f_opt = new_fitness\n                                self.x_opt = new_solution.copy()\n            \n            #Global Search\n            if eval_count < self.budget and np.random.rand() < 0.1:\n                new_x = np.random.uniform(self.lb, self.ub, self.dim)\n                new_f = func(new_x)\n                eval_count+=1\n\n                worst_index = np.argmax(fitness)\n                if new_f < fitness[worst_index]:\n                    population[worst_index] = new_x\n                    fitness[worst_index] = new_f\n                    if new_f < self.f_opt:\n                        self.f_opt = new_f\n                        self.x_opt = new_x.copy()\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "In the code, line 26, in __call__, the following error occurred:\nNameError: name 'KMeans' is not defined. Did you mean: 'kmeans'?\nOn line: kmeans = KMeans(n_clusters=self.n_clusters, n_init=1, random_state=0)", "error": "In the code, line 26, in __call__, the following error occurred:\nNameError: name 'KMeans' is not defined. Did you mean: 'kmeans'?\nOn line: kmeans = KMeans(n_clusters=self.n_clusters, n_init=1, random_state=0)", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "584bff77-bde7-4da3-b20f-da0aebf924a2", "fitness": 0.3001744227560155, "name": "ProbabilisticSampling", "description": "No description provided.", "code": "import numpy as np\n\nclass ProbabilisticSampling:\n    def __init__(self, budget=10000, dim=10, pop_size=20, alpha=0.1, beta=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.alpha = alpha  # Weight for the best solution\n        self.beta = beta    # Weight for the population average\n        self.lb = -5.0\n        self.ub = 5.0\n        self.population = None\n        self.fitness = None\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(self.lb, self.ub, (self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)].copy()\n        return self.pop_size\n\n    def __call__(self, func):\n        eval_count = 0\n        eval_count += self.initialize_population(func)\n        \n        while eval_count < self.budget:\n            new_population = np.zeros_like(self.population)\n            new_fitness = np.zeros_like(self.fitness)\n\n            for i in range(self.pop_size):\n                # Find the best solution\n                best_index = np.argmin(self.fitness)\n                x_best = self.population[best_index]\n\n                # Calculate the weighted average of the population\n                x_avg = np.mean(self.population, axis=0)\n\n                # Generate a random vector for exploration\n                x_rand = np.random.uniform(self.lb, self.ub, self.dim)\n\n                # Create a new candidate solution by combining the best solution, population average, and random exploration\n                new_x = (\n                    self.alpha * x_best\n                    + self.beta * x_avg\n                    + (1 - self.alpha - self.beta) * x_rand\n                )\n                new_x = np.clip(new_x, self.lb, self.ub)\n\n                # Evaluate the new solution\n                f_trial = func(new_x)\n                eval_count += 1\n\n                # Selection: replace the old solution if the new one is better\n                if f_trial < self.fitness[i]:\n                    new_population[i] = new_x\n                    new_fitness[i] = f_trial\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = new_x.copy()\n                else:\n                    new_population[i] = self.population[i]\n                    new_fitness[i] = self.fitness[i]\n\n                if eval_count >= self.budget:\n                    break\n\n            self.population = new_population\n            self.fitness = new_fitness\n\n            # Adapt the weights (optional)\n            self.alpha = np.clip(self.alpha + 0.01 * np.random.randn(), 0.05, 0.2)\n            self.beta = np.clip(self.beta + 0.01 * np.random.randn(), 0.05, 0.2)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm ProbabilisticSampling scored 0.300 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.1422865114358749, 0.18766230225659497, 0.288987935477269, 0.24509666373576722, 0.20526654019361945, 0.23322044448341017, 0.2499001684454698, 0.22380768160216935, 0.20803597025159004, 0.17100225979543882, 0.28088673538105025, 0.9954531747110276, 0.2502829477624493, 0.2241328807656019, 0.6607736534606027, 0.28460671182180486, 0.23536951361682268, 0.2711438352347082, 0.17178800661835814, 0.4737845180706821]}, "task_prompt": ""}
{"id": "d2bc4115-66f7-4bfb-ba97-799a99ad549b", "fitness": 0.5008264933489975, "name": "AdaptiveVarianceSampling", "description": "No description provided.", "code": "import numpy as np\n\nclass AdaptiveVarianceSampling:\n    def __init__(self, budget=10000, dim=10, pop_size=20, initial_variance=1.0, success_rate_threshold=0.2):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.initial_variance = initial_variance\n        self.success_rate_threshold = success_rate_threshold\n        self.lb = -5.0\n        self.ub = 5.0\n\n    def __call__(self, func):\n        # Initialization\n        population = np.random.uniform(self.lb, self.ub, (self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)].copy()\n        variance = np.full(self.dim, self.initial_variance)\n        eval_count = self.pop_size\n        successes = 0\n        iterations = 0\n\n        while eval_count < self.budget:\n            iterations += 1\n            # Sample new points around the best individual\n            new_points = np.random.normal(loc=self.x_opt, scale=np.sqrt(variance), size=(self.pop_size, self.dim))\n            new_points = np.clip(new_points, self.lb, self.ub)\n            \n            new_fitness = np.array([func(x) for x in new_points])\n            eval_count += self.pop_size\n\n            # Update the best solution\n            for i in range(self.pop_size):\n                if new_fitness[i] < self.f_opt:\n                    self.f_opt = new_fitness[i]\n                    self.x_opt = new_points[i].copy()\n                    successes += 1\n\n            # Adapt the variance\n            success_rate = successes / iterations\n            if success_rate > self.success_rate_threshold:\n                variance *= 1.1  # Increase variance\n            else:\n                variance *= 0.9  # Decrease variance\n            \n            variance = np.clip(variance, 1e-6, (self.ub - self.lb)**2) # Ensure variance stays reasonable\n\n            if eval_count >= self.budget:\n                break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveVarianceSampling scored 0.501 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.19974424550393055, 0.16028915492274354, 0.5802382542321984, 0.8490252368684801, 0.3146695841536551, 0.6401564720094091, 0.2650955116978183, 0.49126869471915735, 0.561654938617205, 0.2351182288957122, 0.7911194055130386, 0.9949590871471482, 0.2510507183379753, 0.4754952389934559, 0.8198416963168489, 0.6089952515280852, 0.32449814212054306, 0.7460559920440764, 0.21279630671858585, 0.49445770663988275]}, "task_prompt": ""}
{"id": "326e183c-d6e4-4a5f-9ef8-c527a5a16795", "fitness": 0.34603347173353177, "name": "StochasticAveraging", "description": "No description provided.", "code": "import numpy as np\n\nclass StochasticAveraging:\n    def __init__(self, budget=10000, dim=10, pop_size=20, step_size=0.1, sample_size=5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.step_size = step_size\n        self.sample_size = sample_size\n        self.lb = -5.0\n        self.ub = 5.0\n        self.population = None\n        self.fitness = None\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)].copy()\n        self.budget -= self.pop_size\n\n    def refine_population(self, func):\n        new_population = np.zeros_like(self.population)\n        new_fitness = np.zeros_like(self.fitness)\n\n        for i in range(self.pop_size):\n            if self.budget <= 0:\n                break\n\n            # Sample best individuals\n            indices = np.argsort(self.fitness)[:self.sample_size]\n            best_samples = self.population[indices]\n\n            # Calculate average position\n            average_position = np.mean(best_samples, axis=0)\n\n            # Adaptive step size based on fitness variance\n            fitness_variance = np.var(self.fitness[indices])\n            adaptive_step_size = self.step_size / (1 + fitness_variance) # Lower variance means smaller step size.\n            \n            # Generate new solution\n            new_solution = average_position + adaptive_step_size * np.random.normal(0, 1, self.dim)\n            new_solution = np.clip(new_solution, self.lb, self.ub)\n\n            # Evaluate new solution\n            f_new = func(new_solution)\n            self.budget -= 1\n\n            # Acceptance criterion: Always accept.\n            new_population[i] = new_solution\n            new_fitness[i] = f_new\n            if f_new < self.f_opt:\n                self.f_opt = f_new\n                self.x_opt = new_solution.copy()\n\n        self.population = new_population\n        self.fitness = new_fitness\n        \n        # Replace worst with best\n        worst_index = np.argmax(self.fitness)\n        best_index = np.argmin(self.fitness)\n        if self.fitness[worst_index] > self.fitness[best_index]:\n            self.population[worst_index] = self.population[best_index].copy()\n            self.fitness[worst_index] = self.fitness[best_index]\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        while self.budget > 0:\n            self.refine_population(func)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm StochasticAveraging scored 0.346 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.11952070567988504, 0.35003383375038766, 0.28571097945169555, 0.1818368105375936, 0.2795926676035161, 0.3813153867256871, 0.30809498788265344, 0.20144458510713326, 0.1713226194199804, 0.15248959460158618, 0.8428375860481384, 0.8000291892720202, 0.27752281118831257, 0.29780523897154465, 0.3105478581468587, 0.40867891811397916, 0.36219751240788267, 0.5520695960428105, 0.19287848849804867, 0.44474006522092224]}, "task_prompt": ""}
{"id": "b476b928-c9ac-4581-9beb-07120695685f", "fitness": 0.28584375492164826, "name": "ExtremalOptimization", "description": "No description provided.", "code": "import numpy as np\n\nclass ExtremalOptimization:\n    def __init__(self, budget=10000, dim=10, tau=1.5):\n        self.budget = budget\n        self.dim = dim\n        self.tau = tau\n        self.lb = -5.0\n        self.ub = 5.0\n\n    def __call__(self, func):\n        x = np.random.uniform(self.lb, self.ub, self.dim)\n        self.f_opt = func(x)\n        self.x_opt = x.copy()\n        eval_count = 1\n\n        while eval_count < self.budget:\n            # Evaluate each component of the solution\n            component_fitness = np.zeros(self.dim)\n            for i in range(self.dim):\n                x_temp = x.copy()\n                x_temp[i] = np.random.uniform(self.lb, self.ub)\n                component_fitness[i] = func(x_temp)\n                eval_count += 1\n                if eval_count >= self.budget:\n                    break\n\n            if eval_count >= self.budget:\n                break\n            \n            # Rank components based on fitness\n            ranking = np.argsort(component_fitness)\n\n            # Select the worst component with probability based on rank\n            probabilities = (np.arange(1, self.dim + 1) / self.dim) ** self.tau\n            probabilities /= np.sum(probabilities)\n            \n            selected_component = np.random.choice(self.dim, p=probabilities)\n\n            # Optimize the selected component\n            x[selected_component] = np.random.uniform(self.lb, self.ub)\n            f = func(x)\n            eval_count += 1\n\n            if f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = x.copy()\n            \n            if eval_count >= self.budget:\n                break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm ExtremalOptimization scored 0.286 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.10572246423703113, 0.18161486197551857, 0.267507316525607, 0.19524913952506007, 0.20990647387033867, 0.24895836226353885, 0.2623700162978866, 0.19368342464528643, 0.20527792877630913, 0.15706357536897864, 0.2717248778005307, 0.9952238622359899, 0.26739038992006614, 0.1990440585747869, 0.5860906266001357, 0.2841550703127417, 0.2355704608867114, 0.2360647949957767, 0.16568612787555226, 0.44857126574511974]}, "task_prompt": ""}
{"id": "828f1268-7018-41e8-8c5e-0c12222b2728", "fitness": 0.22075116990603988, "name": "RankCentroidDE", "description": "No description provided.", "code": "import numpy as np\n\nclass RankCentroidDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, centroid_learning_rate=0.1, rank_selection_pressure=2.0, reduction_factor=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.centroid_learning_rate = centroid_learning_rate\n        self.rank_selection_pressure = rank_selection_pressure\n        self.reduction_factor = reduction_factor\n        self.lb = -5.0\n        self.ub = 5.0\n        self.population = None\n        self.fitness = None\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)].copy()\n        self.budget -= self.pop_size\n\n    def evolve(self, func):\n        new_population = np.copy(self.population)\n        new_fitness = np.copy(self.fitness)\n\n        # Rank-based selection probabilities\n        ranked_indices = np.argsort(self.fitness)\n        selection_probs = np.power(np.arange(1, self.pop_size + 1), -self.rank_selection_pressure)\n        selection_probs /= np.sum(selection_probs)\n\n        centroid = np.mean(self.population, axis=0)\n\n        for i in range(self.pop_size):\n            if self.budget <= 0:\n                break\n\n            # Parent selection\n            parent_indices = np.random.choice(self.pop_size, 2, replace=False, p=selection_probs)\n            parent1, parent2 = self.population[parent_indices]\n\n            # Offspring generation towards centroid\n            offspring = parent1 + 0.5 * (parent2 - parent1) + self.centroid_learning_rate * (centroid - parent1)\n            offspring = np.clip(offspring, self.lb, self.ub)\n\n            f_offspring = func(offspring)\n            self.budget -= 1\n\n            if f_offspring < self.fitness[i]:\n                new_population[i] = offspring\n                new_fitness[i] = f_offspring\n                if f_offspring < self.f_opt:\n                    self.f_opt = f_offspring\n                    self.x_opt = offspring.copy()\n\n        self.population = new_population\n        self.fitness = new_fitness\n\n        # Dynamic population size reduction\n        if self.budget > 0 and len(self.population) > 10 and np.random.rand() < 0.1:\n            indices = np.argsort(self.fitness)[::-1]\n            remove_count = int(len(self.population) * (1 - self.reduction_factor))\n            indices_to_keep = indices[:-remove_count]\n\n            self.population = self.population[indices_to_keep]\n            self.fitness = self.fitness[indices_to_keep]\n            self.pop_size = len(self.population)\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        while self.budget > 0:\n            self.evolve(func)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm RankCentroidDE scored 0.221 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.09542291704698869, 0.12240423359565333, 0.2343595202642027, 0.15452937009288326, 0.15232668466232901, 0.17818315711872024, 0.2464142033706761, 0.26244961984505943, 0.16203838107997814, 0.1420625310124013, 0.16294311462508881, 0.9983339965887792, 0.23112513909331145, 0.13706508577973653, 0.14519079458718787, 0.2458306743077504, 0.2038524476077247, 0.1984738179934311, 0.17556702158127568, 0.16645068786761985]}, "task_prompt": ""}
{"id": "b235fc76-899c-459a-b564-ecb0ffdce1b4", "fitness": "-inf", "name": "AdaptiveDE_CMA", "description": "No description provided.", "code": "import numpy as np\n\nclass AdaptiveDE_CMA:\n    def __init__(self, budget=10000, dim=10, pop_size=None, mutation_factor=0.5, crossover_rate=0.7, initial_sigma=0.5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size if pop_size is not None else 4 + int(3 * np.log(dim))\n        self.mutation_factor = mutation_factor\n        self.crossover_rate = crossover_rate\n        self.lb = -5.0\n        self.ub = 5.0\n        self.population = None\n        self.fitness = None\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.sigma = initial_sigma\n        self.C = np.eye(dim)  # Covariance matrix\n        self.pc = np.zeros(dim) # Evolution path for C\n        self.ps = np.zeros(dim) # Evolution path for sigma\n        self.damps = 1 + (dim / 2)\n        self.cs = (self.damps - 1) / (np.linalg.norm(self.ps)**2 + self.damps)\n        self.cc = (4 + (dim / 3)) / (dim + 4)\n        self.mu = self.pop_size // 2 # Number of parents\n        self.weights = np.log(self.mu+1/2) - np.log(np.arange(1, self.mu+1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.chiN = dim**0.5 * (1 - (1/(4*dim)) + (1/(21 * dim**2)))\n\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)].copy()\n        self.budget -= self.pop_size\n\n    def evolve(self, func):\n        if self.budget <= 0:\n            return\n\n        z = np.random.multivariate_normal(np.zeros(self.dim), self.C, size=self.pop_size)\n        new_population = self.x_opt + self.sigma * z\n        new_population = np.clip(new_population, self.lb, self.ub)\n        new_fitness = np.array([func(x) for x in new_population])\n        self.budget -= self.pop_size\n\n        if self.budget <= 0:\n            return\n\n        indices = np.argsort(new_fitness)\n        best_individuals = new_population[indices[:self.mu]]\n        best_fitnesses = new_fitness[indices[:self.mu]]\n\n        # Update optimal solution\n        if np.min(best_fitnesses) < self.f_opt:\n            self.f_opt = np.min(best_fitnesses)\n            self.x_opt = best_individuals[np.argmin(best_fitnesses)].copy()\n        \n        # CMA-ES update\n        y = (best_individuals - self.x_opt) / self.sigma\n        self.ps = (1-self.cs) * self.ps + (self.cs**0.5) * (np.linalg.norm(self.ps)/self.chiN)**-1 * ((best_individuals[0] - self.x_opt) / self.sigma)\n        hsig = int(np.linalg.norm(self.ps)/((1-self.cs)**(self.budget/self.budget))) < (2+self.dim/3)**0.5\n        self.pc = (1-self.cc) * self.pc + hsig*(self.cc*(2-self.cc))**0.5 * np.sum(self.weights[:, None] * y, axis=0)\n        \n        C_temp = np.sum(self.weights[:,None,None] * y[:,:,None] * y[:, None,:], axis=0)\n\n        self.C = (1 - self.cc) * self.C + self.cc * C_temp + self.cc * self.pc[:,None] * self.pc[None,:]\n\n        self.sigma *= np.exp((self.cs/self.damps) * (np.linalg.norm(self.ps)/self.chiN - 1))\n        \n        self.population = new_population\n        self.fitness = new_fitness\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        while self.budget > self.pop_size:\n            self.evolve(func)\n\n        # Final refinement (optional, if budget remains)\n        if self.budget > 0:\n            final_pop = np.random.uniform(self.lb, self.ub, size=(min(self.budget, self.pop_size), self.dim))\n            final_fitness = np.array([func(x) for x in final_pop])\n            self.budget -= len(final_pop)\n            if np.min(final_fitness) < self.f_opt:\n                self.f_opt = np.min(final_fitness)\n                self.x_opt = final_pop[np.argmin(final_fitness)].copy()\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "In the code, line 61, in evolve, the following error occurred:\nOverflowError: cannot convert float infinity to integer\nOn line: hsig = int(np.linalg.norm(self.ps)/((1-self.cs)**(self.budget/self.budget))) < (2+self.dim/3)**0.5", "error": "In the code, line 61, in evolve, the following error occurred:\nOverflowError: cannot convert float infinity to integer\nOn line: hsig = int(np.linalg.norm(self.ps)/((1-self.cs)**(self.budget/self.budget))) < (2+self.dim/3)**0.5", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "44d0d3a6-e0b0-4096-b743-ee632dac7aec", "fitness": 0.5681502353066403, "name": "EnhancedDE", "description": "No description provided.", "code": "import numpy as np\n\nclass EnhancedDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, mutation_factor=0.5, crossover_rate=0.7, reduction_factor=0.9, local_search_prob=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.mutation_factor = mutation_factor\n        self.crossover_rate = crossover_rate\n        self.reduction_factor = reduction_factor\n        self.local_search_prob = local_search_prob\n        self.lb = -5.0\n        self.ub = 5.0\n        self.population = None\n        self.fitness = None\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)].copy()\n        self.budget -= self.pop_size\n\n    def evolve(self, func):\n        new_population = np.copy(self.population)\n        new_fitness = np.copy(self.fitness)\n        improvements = 0\n        for i in range(self.pop_size):\n            if self.budget <= 0:\n                break\n\n            idxs = [idx for idx in range(self.pop_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n\n            # Adaptive mutation factor\n            mutant = np.clip(a + self.mutation_factor * (b - c), self.lb, self.ub)\n\n            # Crossover\n            cross_points = np.random.rand(self.dim) < self.crossover_rate\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial_vector = np.where(cross_points, mutant, self.population[i])\n\n            f_trial = func(trial_vector)\n            self.budget -= 1\n\n            if f_trial < self.fitness[i]:\n                new_population[i] = trial_vector\n                new_fitness[i] = f_trial\n                improvements +=1\n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = trial_vector.copy()\n            elif np.random.rand() < self.local_search_prob:\n                # Local search around the best solution\n                scale = 0.1 * (self.ub - self.lb)\n                x_local = np.clip(self.x_opt + np.random.normal(0, scale, size=self.dim), self.lb, self.ub)\n                f_local = func(x_local)\n                self.budget -= 1\n\n                if f_local < self.f_opt:\n                    self.f_opt = f_local\n                    self.x_opt = x_local.copy()\n                    \n        if improvements > 0:\n             self.mutation_factor = min(0.9, self.mutation_factor * 1.1)\n        else:\n             self.mutation_factor = max(0.1, self.mutation_factor * 0.9)\n\n        self.population = new_population\n        self.fitness = new_fitness\n\n        # Population size reduction (optional)\n        if self.budget > 0 and len(self.population) > 10 and np.random.rand() < 0.1:\n            indices = np.argsort(self.fitness)[::-1] #sort from worst to best\n            remove_count = int(len(self.population) * (1 - self.reduction_factor))\n            indices_to_keep = indices[:-remove_count] #indices of the best solutions\n\n            self.population = self.population[indices_to_keep]\n            self.fitness = self.fitness[indices_to_keep]\n            self.pop_size = len(self.population)\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        while self.budget > 0:\n            self.evolve(func)\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm EnhancedDE scored 0.568 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.1999695462830089, 0.230228521411186, 0.6102931652058037, 0.76583641155079, 0.601112078733007, 0.5417042855886725, 0.5965886657643732, 0.5614326382523542, 0.5452564450116995, 0.6854119807146073, 0.665072979986763, 0.9996465424387898, 0.3182619089475661, 0.6495572184792455, 0.8535482174015205, 0.6941344359429558, 0.4540937852974324, 0.6941095986744881, 0.20250340477893014, 0.4942428756696128]}, "task_prompt": ""}
{"id": "273a7b90-65eb-42b8-861c-e750c886393e", "fitness": 0.19269244312118727, "name": "EnsembleAdaptiveDE", "description": "No description provided.", "code": "import numpy as np\n\nclass EnsembleAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, crossover_rate=0.7, restart_probability=0.05):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.crossover_rate = crossover_rate\n        self.restart_probability = restart_probability\n        self.lb = -5.0\n        self.ub = 5.0\n        self.population = None\n        self.fitness = None\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.mutation_factors = [0.5, 0.7, 0.9] # Ensemble of mutation factors\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)].copy()\n        self.budget -= self.pop_size\n\n    def evolve(self, func):\n        new_population = np.copy(self.population)\n        new_fitness = np.copy(self.fitness)\n        for i in range(self.pop_size):\n            if self.budget <= 0:\n                break\n\n            idxs = [idx for idx in range(self.pop_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n\n            # Ensemble of mutation strategies\n            mutation_factor = np.random.choice(self.mutation_factors)\n            mutant = np.clip(a + mutation_factor * (b - c), self.lb, self.ub)\n\n            # Crossover\n            cross_points = np.random.rand(self.dim) < self.crossover_rate\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial_vector = np.where(cross_points, mutant, self.population[i])\n\n            f_trial = func(trial_vector)\n            self.budget -= 1\n\n            if f_trial < self.fitness[i]:\n                new_population[i] = trial_vector\n                new_fitness[i] = f_trial\n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = trial_vector.copy()\n\n        self.population = new_population\n        self.fitness = new_fitness\n\n    def restart(self, func):\n        # Restart population if stagnation is detected\n        if np.random.rand() < self.restart_probability:\n            self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n            self.fitness = np.array([func(x) for x in self.population])\n            self.budget -= self.pop_size\n            best_index = np.argmin(self.fitness)\n            if self.fitness[best_index] < self.f_opt:\n                 self.f_opt = self.fitness[best_index]\n                 self.x_opt = self.population[best_index].copy()\n    def __call__(self, func):\n        self.initialize_population(func)\n        while self.budget > 0:\n            self.evolve(func)\n            self.restart(func) #apply the restart mechanism\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm EnsembleAdaptiveDE scored 0.193 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.14595743127158878, 0.2143848866139597, 0.3415139100984901, 0.2616059876218978, 0]}, "task_prompt": ""}
{"id": "dd2fe81c-38c2-4cca-ae7d-3dbd01b7d657", "fitness": 0.5894699059698267, "name": "SelfAdaptiveDE", "description": "No description provided.", "code": "import numpy as np\n\nclass SelfAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=20, F=0.5, CR=0.9, local_search_prob=0.05):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F  # Mutation factor\n        self.CR = CR  # Crossover rate\n        self.local_search_prob = local_search_prob\n        self.lb = -5.0\n        self.ub = 5.0\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.pop = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.func_evals = self.pop_size\n\n        while self.func_evals < self.budget:\n            for i in range(self.pop_size):\n                # Mutation\n                indices = np.random.choice(self.pop_size, 3, replace=False)\n                a, b, c = self.pop[indices]\n                mutant = self.pop[i] + self.F * (b - c)\n\n                # Repair mechanism\n                mutant = np.clip(mutant, self.lb, self.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, self.pop[i])\n\n                # Simplified Local Search\n                if np.random.rand() < self.local_search_prob:\n                    trial = trial + 0.01 * np.random.randn(self.dim)\n                    trial = np.clip(trial, self.lb, self.ub)\n\n                # Selection\n                f = func(trial)\n                self.func_evals += 1\n\n                if f < self.fitness[i]:\n                    self.pop[i] = trial\n                    self.fitness[i] = f\n\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = trial.copy()\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm SelfAdaptiveDE scored 0.589 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.20473003427014935, 0.49375425627701697, 0.5601281371787219, 0.8541579768577724, 0.5848894360691101, 0.7573580055284512, 0.3322874094631335, 0.5770312789643166, 0.6517455266309513, 0.28266589363922967, 0.8596357634863669, 0.9965605791570067, 0.654016436049637, 0.46176859025403094, 0.8653378291252998, 0.7133055191816873, 0.46793342911168934, 0.7911530092857224, 0.1893534780050914, 0.4915855308611513]}, "task_prompt": ""}
{"id": "3dbfb4c9-2f59-4729-a04c-8d07caccb33f", "fitness": 0.2780764142625079, "name": "AdaptiveDifferentialEvolution", "description": "No description provided.", "code": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget=10000, dim=10, pop_size=None, F=0.5, CR=0.9, pop_size_adaptation_rate=0.05):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size if pop_size is not None else int(4 + 3 * np.log(dim))\n        self.F = F  # Mutation factor\n        self.CR = CR  # Crossover rate\n        self.lb = -5.0\n        self.ub = 5.0\n        self.pop_size_adaptation_rate = pop_size_adaptation_rate\n        self.min_pop_size = 4\n        self.max_pop_size = 100\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.pop = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.func_evals = self.pop_size\n\n        while self.func_evals < self.budget:\n            for i in range(self.pop_size):\n                # Mutation based on distance to nearest neighbor\n                distances = np.linalg.norm(self.pop - self.pop[i], axis=1)\n                distances[i] = np.inf  # Exclude itself\n                nearest_neighbor_idx = np.argmin(distances)\n\n                indices = np.random.choice(self.pop_size, 2, replace=False)\n                a, b = self.pop[indices]\n\n                mutant = self.pop[i] + self.F * (self.pop[nearest_neighbor_idx] - self.pop[i]) + self.F * (a - b)\n\n                # Repair mechanism\n                mutant = np.clip(mutant, self.lb, self.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, self.pop[i])\n\n                # Selection\n                f = func(trial)\n                self.func_evals += 1\n\n                if f < self.fitness[i]:\n                    self.pop[i] = trial\n                    self.fitness[i] = f\n\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = trial.copy()\n\n            # Adapt population size\n            if self.func_evals / self.budget > 0.5:  # Adapt after a certain point\n                success_rate = np.sum(self.fitness < np.mean(self.fitness)) / self.pop_size\n                if success_rate > 0.2:\n                    self.pop_size = min(self.max_pop_size, int(self.pop_size * (1 + self.pop_size_adaptation_rate)))\n                elif success_rate < 0.1:\n                    self.pop_size = max(self.min_pop_size, int(self.pop_size * (1 - self.pop_size_adaptation_rate)))\n                \n                self.pop_size = int(self.pop_size)\n                \n                if self.pop_size != self.pop.shape[0]:\n\n                    #resize population\n                    if self.pop_size > self.pop.shape[0]:\n                         new_individuals = np.random.uniform(self.lb, self.ub, size=(self.pop_size-self.pop.shape[0], self.dim))\n                         self.pop = np.vstack((self.pop,new_individuals))\n                         new_fitness = np.array([func(x) for x in new_individuals])\n                         self.fitness = np.concatenate((self.fitness,new_fitness))\n                         self.func_evals += new_individuals.shape[0]\n                    else:\n\n                         indices_to_keep = np.argsort(self.fitness)[:self.pop_size]\n                         self.pop = self.pop[indices_to_keep]\n                         self.fitness = self.fitness[indices_to_keep]\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveDifferentialEvolution scored 0.278 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.18046714536442232, 0.2528860476371717, 0.3184677585337937, 0.14976778535084978, 0.2105938822191754, 0.16421359258363433, 0.18061635286216882, 0.3945764296118873, 0.1675328337311266, 0.15975178123916323, 0.15986189039178667, 0.8820621602576413, 0.27243599100871885, 0.3081844044759874, 0.645226012530655, 0.2806549746644519, 0]}, "task_prompt": ""}
{"id": "26ad2449-056d-4ebd-9a93-e759d9b27f5a", "fitness": 0.47912673212531576, "name": "AdaptiveClampingPSO", "description": "No description provided.", "code": "import numpy as np\n\nclass AdaptiveClampingPSO:\n    def __init__(self, budget=10000, dim=10, pop_size=20, w_max=0.9, w_min=0.4, c1=2.0, c2=2.0, v_max_ratio=0.2):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.w_max = w_max  # Max inertia weight\n        self.w_min = w_min  # Min inertia weight\n        self.c1 = c1  # Cognitive coefficient\n        self.c2 = c2  # Social coefficient\n        self.lb = -5.0\n        self.ub = 5.0\n        self.v_max_ratio = v_max_ratio\n\n    def __call__(self, func):\n        # Initialization\n        population = np.random.uniform(self.lb, self.ub, (self.pop_size, self.dim))\n        v_max = self.v_max_ratio * (self.ub - self.lb)\n        velocities = np.random.uniform(-v_max, v_max, (self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)].copy()\n        personal_best_positions = population.copy()\n        personal_best_fitness = fitness.copy()\n\n        eval_count = self.pop_size\n\n        while eval_count < self.budget:\n            # Adaptive inertia weight (linearly decreasing)\n            w = self.w_max - (self.w_max - self.w_min) * (eval_count / self.budget)\n\n            # PSO update\n            r1 = np.random.rand(self.pop_size, self.dim)\n            r2 = np.random.rand(self.pop_size, self.dim)\n            global_best_position = self.x_opt\n\n            velocities = (w * velocities +\n                          self.c1 * r1 * (personal_best_positions - population) +\n                          self.c2 * r2 * (global_best_position - population))\n\n            # Velocity clamping\n            velocities = np.clip(velocities, -v_max, v_max)\n\n            population = population + velocities\n            population = np.clip(population, self.lb, self.ub)\n\n            # Evaluation\n            new_fitness = np.array([func(x) for x in population])\n            eval_count += self.pop_size\n            fitness = new_fitness\n\n            # Update personal best\n            for i in range(self.pop_size):\n                if fitness[i] < personal_best_fitness[i]:\n                    personal_best_positions[i] = population[i].copy()\n                    personal_best_fitness[i] = fitness[i]\n\n                    # Update global best\n                    if fitness[i] < self.f_opt:\n                        self.f_opt = fitness[i]\n                        self.x_opt = population[i].copy()\n\n            if eval_count >= self.budget:\n                eval_count = self.budget #correction\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveClampingPSO scored 0.479 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.1838931784597022, 0.18386501753265982, 0.5885606562768158, 0.7689197434003601, 0.25211578363518206, 0.6902149460112303, 0.3174103288826533, 0.4252447691135002, 0.626000050449111, 0.18890435932239913, 0.7801622493962657, 0.9966978647033959, 0.3274716608918792, 0.25794728262243616, 0.649000713949426, 0.3770758739147192, 0.5280834165535198, 0.7192858668166292, 0.20928750195997414, 0.5123933786144577]}, "task_prompt": ""}
{"id": "e432ed9e-af65-4a67-916d-31e61fc1f8f6", "fitness": 0.4104520295828629, "name": "EnhancedDE", "description": "No description provided.", "code": "import numpy as np\n\nclass EnhancedDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.3, CR=0.95, local_search_prob=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.local_search_prob = local_search_prob\n        self.lb = -5.0\n        self.ub = 5.0\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.pop = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.func_evals = self.pop_size\n\n        while self.func_evals < self.budget:\n            for i in range(self.pop_size):\n                # Mutation\n                indices = np.random.choice(self.pop_size, 3, replace=False)\n                a, b, c = self.pop[indices]\n                mutant = self.pop[i] + self.F * (b - c)\n                mutant = np.clip(mutant, self.lb, self.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, self.pop[i])\n\n                # Aggressive Local Search\n                if np.random.rand() < self.local_search_prob:\n                    trial = trial + 0.1 * np.random.randn(self.dim)\n                    trial = np.clip(trial, self.lb, self.ub)\n\n                # Selection\n                f = func(trial)\n                self.func_evals += 1\n\n                if f < self.fitness[i]:\n                    self.pop[i] = trial\n                    self.fitness[i] = f\n\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = trial.copy()\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm EnhancedDE scored 0.410 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.14646258190892936, 0.31043250234966735, 0.3816117284051538, 0.39994583793879435, 0.2929627869436112, 0.4242477999114188, 0.2955187695761059, 0.35377544639644376, 0.3091907513957979, 0.1811649401415003, 0.6091914771852704, 0.9989772953615932, 0.42464585617603734, 0.30386228023001405, 0.788659493206165, 0.41794602395309943, 0.3514089907755621, 0.5567752425353623, 0.17567300801329888, 0.4865877792534319]}, "task_prompt": ""}
{"id": "6e18d5e8-dfe1-46c8-a773-e2ab18df4b3e", "fitness": 0.40090026355204716, "name": "AdaptiveDE", "description": "No description provided.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.7, CR=0.8, stagnation_threshold=0.0001, stagnation_iters=500, reduction_factor=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F  # Mutation factor\n        self.CR = CR  # Crossover rate\n        self.lb = -5.0\n        self.ub = 5.0\n        self.stagnation_threshold = stagnation_threshold\n        self.stagnation_iters = stagnation_iters\n        self.reduction_factor = reduction_factor\n        self.best_fitness_history = []\n        self.learning_rate = 0.1\n        self.restart_counter = 0\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.pop = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.func_evals = self.pop_size\n        self.best_fitness_history.append(np.min(self.fitness))\n\n        while self.func_evals < self.budget:\n            prev_best_fitness = self.f_opt\n\n            for i in range(self.pop_size):\n                # Mutation\n                indices = np.random.choice(self.pop_size, 3, replace=False)\n                a, b, c = self.pop[indices]\n                mutant = self.pop[i] + self.F * (b - c)\n\n                # Repair mechanism\n                mutant = np.clip(mutant, self.lb, self.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, self.pop[i])\n\n                # Selection\n                f = func(trial)\n                self.func_evals += 1\n\n                if f < self.fitness[i]:\n                    self.pop[i] = trial\n                    self.fitness[i] = f\n\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = trial.copy()\n                        # Adaptive Learning Rate based on improvement\n                        improvement = prev_best_fitness - self.f_opt\n                        self.learning_rate = min(0.1 + improvement, 1.0)\n\n            self.best_fitness_history.append(np.min(self.fitness))\n            \n            #Stagnation Check and Restart/Reduction\n            if len(self.best_fitness_history) > self.stagnation_iters:\n                if abs(self.best_fitness_history[-1] - self.best_fitness_history[-self.stagnation_iters]) < self.stagnation_threshold:\n                    self.restart_counter += 1\n                    if self.restart_counter < 3:\n                        # Reduce population size\n                        self.pop_size = int(self.pop_size * self.reduction_factor)\n                        self.pop_size = max(10, self.pop_size) #Ensure minimum pop size\n                        self.pop = self.pop[np.argsort(self.fitness)[:self.pop_size]]  # Keep best individuals\n                        self.fitness = self.fitness[np.argsort(self.fitness)[:self.pop_size]]\n                        \n                        # Repopulate with random individuals\n                        num_new = 50 - self.pop_size if 50 - self.pop_size > 0 else 0\n                        if num_new > 0:\n                            new_pop = np.random.uniform(self.lb, self.ub, size=(num_new, self.dim))\n                            new_fitness = np.array([func(x) for x in new_pop])\n                            self.func_evals += num_new\n                            self.pop = np.concatenate((self.pop, new_pop))\n                            self.fitness = np.concatenate((self.fitness, new_fitness))\n                        self.pop_size = 50\n\n\n                    else:\n                        #Restart with new random population\n                        self.pop = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n                        self.fitness = np.array([func(x) for x in self.pop])\n                        self.func_evals += self.pop_size\n                        self.restart_counter = 0\n\n                    self.best_fitness_history = [np.min(self.fitness)] #Reset history after restart.\n                    \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveDE scored 0.401 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.15254791421517822, 0.25355413393766557, 0.37242310825098823, 0.5415088096687121, 0.30154912305588555, 0.4193039508123012, 0.2946202502260675, 0.3462150175660129, 0.3182204875642841, 0.1885914959757985, 0.5049380814616355, 0.9997448693812921, 0.3144536280596709, 0.33551836688257264, 0.7133989058018794, 0.4428118477108067, 0.31501050042105105, 0.5383569037487388, 0.17483848760091025, 0.4903993886994914]}, "task_prompt": ""}
{"id": "008cdf3e-f21d-40af-aeae-41dab83fb5bc", "fitness": 0.13775741793605567, "name": "AdaptiveSimulatedAnnealing", "description": "No description provided.", "code": "import numpy as np\n\nclass AdaptiveSimulatedAnnealing:\n    def __init__(self, budget=10000, dim=10, initial_temp=100.0, cooling_rate=0.95):\n        self.budget = budget\n        self.dim = dim\n        self.initial_temp = initial_temp\n        self.cooling_rate = cooling_rate\n        self.lb = -5.0\n        self.ub = 5.0\n        self.temp = initial_temp\n        self.step_size = 1.0\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        x = np.random.uniform(self.lb, self.ub, self.dim)\n        self.f_opt = func(x)\n        self.x_opt = x.copy()\n        eval_count = 1\n\n        while eval_count < self.budget:\n            # Generate neighbor\n            x_new = x + np.random.normal(0, self.step_size, self.dim)\n            x_new = np.clip(x_new, self.lb, self.ub)\n\n            # Evaluate neighbor\n            f_new = func(x_new)\n            eval_count += 1\n\n            # Acceptance probability\n            delta_f = f_new - self.f_opt\n            if delta_f < 0 or np.random.rand() < np.exp(-delta_f / self.temp):\n                x = x_new.copy()\n                if f_new < self.f_opt:\n                    self.f_opt = f_new\n                    self.x_opt = x_new.copy()\n                    #Increase step size if improving\n                    self.step_size *= 1.1\n                else:\n                    #Decrease step size if not improving\n                    self.step_size *= 0.9\n\n            #Cooling Schedule\n            self.temp *= self.cooling_rate\n            self.step_size = np.clip(self.step_size, 0.01, 2.0)\n            if eval_count >= self.budget:\n                break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveSimulatedAnnealing scored 0.138 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.16257495378346498, 0.07598890238286571, 0.19000308505015384, 0.19640780770590072, 0.07049484930242511, 0.10288031086944305, 0.21586665561811047, 0.11937170130651265, 0.15631880298560663, 0.12348942609502678, 0.1491914047615055, 0.12411556391921275, 0.2953610100778511, 0.07321410609084256, 0.09427184629924523, 0.1250800743161955, 0.09106509798226148, 0.11476807672879474, 0.09535646587398283, 0.17932821757171225]}, "task_prompt": ""}
{"id": "84291775-23c0-48c7-bc08-d4b6955ee3b4", "fitness": "-inf", "name": "SimplifiedCMAES", "description": "No description provided.", "code": "import numpy as np\n\nclass SimplifiedCMAES:\n    def __init__(self, budget=10000, dim=10, pop_size=None, sigma0=0.2):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size if pop_size is not None else 4 + int(3 * np.log(dim))\n        self.sigma = sigma0\n        self.mean = np.zeros(dim)\n        self.C = np.eye(dim)\n        self.lb = -5.0\n        self.ub = 5.0\n        self.restart_trigger = 100 * self.dim # Trigger restart after this many evaluations without improvement\n        self.no_improvement_count = 0\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        self.mean = np.random.uniform(self.lb, self.ub, self.dim)\n        self.f_opt = np.inf\n        self.x_opt = None\n        eval_count = 0\n\n        while eval_count < self.budget:\n            # Sample population\n            z = np.random.randn(self.pop_size, self.dim)\n            x = self.mean + self.sigma * np.dot(z, np.linalg.cholesky(self.C).T)\n            x = np.clip(x, self.lb, self.ub)\n\n            # Evaluate population\n            fitness = np.array([func(xi) for xi in x])\n            eval_count += self.pop_size\n\n            # Find best individual\n            best_idx = np.argmin(fitness)\n            if fitness[best_idx] < self.f_opt:\n                self.f_opt = fitness[best_idx]\n                self.x_opt = x[best_idx].copy()\n                self.no_improvement_count = 0\n            else:\n                self.no_improvement_count += self.pop_size\n\n            # Update mean\n            weights = np.zeros(self.pop_size)\n            weights[best_idx] = 1.0  # Give all weight to the best individual\n            delta_mean = np.sum(weights[:, None] * (x - self.mean), axis=0)\n            self.mean += delta_mean\n\n            # Simplified Covariance Matrix Adaptation\n            self.C = (1 - 0.1) * self.C + 0.1 * np.outer(delta_mean / self.sigma, delta_mean / self.sigma)\n            self.sigma *= np.exp(0.2 * (self.success_rate(fitness) - 0.2)) # adapt step size\n\n            # Hard Restart mechanism\n            if self.no_improvement_count > self.restart_trigger or not np.all(np.isfinite(self.C)):\n                self.mean = np.random.uniform(self.lb, self.ub, self.dim)\n                self.C = np.eye(self.dim)\n                self.sigma = 0.2\n                self.no_improvement_count = 0\n\n            if eval_count >= self.budget:\n                break\n\n        return self.f_opt, self.x_opt\n\n    def success_rate(self, fitness):\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "In the code, line 51, in __call__, the following error occurred:\nTypeError: unsupported operand type(s) for -: 'tuple' and 'float'\nOn line: self.sigma *= np.exp(0.2 * (self.success_rate(fitness) - 0.2)) # adapt step size", "error": "In the code, line 51, in __call__, the following error occurred:\nTypeError: unsupported operand type(s) for -: 'tuple' and 'float'\nOn line: self.sigma *= np.exp(0.2 * (self.success_rate(fitness) - 0.2)) # adapt step size", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "493c83b4-21c7-445f-941b-db5ede37947c", "fitness": 0.09173767458410842, "name": "SimpleCMAES", "description": "No description provided.", "code": "import numpy as np\n\nclass SimpleCMAES:\n    def __init__(self, budget=10000, dim=10, pop_size=None, sigma=0.5, mu_factor=0.25, restart_factor=0.75):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size if pop_size is not None else 4 + int(3 * np.log(dim))\n        self.sigma = sigma\n        self.mu = max(1, int(self.pop_size * mu_factor))\n        self.weights = np.log(self.mu + 1) - np.log(np.arange(1, self.mu + 1))\n        self.weights /= np.sum(self.weights)\n        self.mean = np.random.uniform(-5, 5, size=dim)\n        self.C = np.eye(dim)\n        self.lb = -5.0\n        self.ub = 5.0\n        self.restart_factor = restart_factor\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.eval_count = 0\n\n    def sample_population(self):\n        z = np.random.randn(self.pop_size, self.dim)\n        return self.mean + self.sigma * np.dot(z, np.linalg.cholesky(self.C).T)\n\n    def __call__(self, func):\n        while self.eval_count < self.budget:\n            population = self.sample_population()\n            population = np.clip(population, self.lb, self.ub)\n            fitness = np.array([func(x) for x in population])\n            self.eval_count += self.pop_size\n            if self.eval_count > self.budget:\n                fitness = fitness[:self.budget - (self.eval_count - self.pop_size)]\n                population = population[:self.budget - (self.eval_count - self.pop_size)]\n                \n            indices = np.argsort(fitness)\n            best_indices = indices[:self.mu]\n\n            best_population = population[best_indices]\n            best_fitness = fitness[best_indices]\n            \n            if np.min(best_fitness) < self.f_opt:\n                self.f_opt = np.min(best_fitness)\n                self.x_opt = best_population[np.argmin(best_fitness)].copy()\n\n            # Update mean\n            self.mean = np.sum(self.weights[:, None] * best_population, axis=0)\n\n            # Rank-one update of covariance matrix\n            y = best_population - self.mean\n            C_update = np.sum(self.weights[:, None, None] * y[:, :, None] * y[:, None, :], axis=0)\n            self.C = (1 - self.restart_factor) * self.C + self.restart_factor * C_update\n\n            # Keep C positive definite\n            self.C = np.triu(self.C) + np.triu(self.C, k=1).T\n\n            #Simple check for positive definiteness and fix\n            try:\n                np.linalg.cholesky(self.C)\n            except np.linalg.LinAlgError:\n                self.C = self.C + 1e-6 * np.eye(self.dim)\n\n            # Adjust step size\n            self.sigma *= np.exp(0.5 * (np.mean(best_fitness) - np.mean(fitness)) / self.dim)\n            if self.sigma < 1e-6:\n                self.sigma = 0.5 #Reset sigma if too small\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm SimpleCMAES scored 0.092 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.0002609668247781638, 0.020169298671889324, 0.21972375064224958, 0.11053431113802048, 0.029615493022243422, 0.14844248304624985, 0.025528823419749602, 0.06423223797456046, 0.052560630200299774, 0.09366721116917975, 0.13032161677537757, 0.19712868484024493, 0.013211090788148083, 0.10882726372270612, 0.13567632125291607, 0.11254700843633836, 0.07086853167860796, 0.09593974534769512, 0.07484545338295723, 0.13065256934795655]}, "task_prompt": ""}
{"id": "a4b63ab6-2bcb-4671-b974-a67ad9ef985f", "fitness": 0.0, "name": "AdaptivePerturbation", "description": "No description provided.", "code": "import numpy as np\n\nclass AdaptivePerturbation:\n    def __init__(self, budget=10000, dim=10, step_size=0.1, num_samples=10):\n        self.budget = budget\n        self.dim = dim\n        self.step_size = step_size\n        self.num_samples = num_samples\n        self.lb = -5.0\n        self.ub = 5.0\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = np.random.uniform(self.lb, self.ub, self.dim)\n        self.f_opt = func(self.x_opt)\n        eval_count = 1\n        \n        while eval_count < self.budget:\n            # Estimate local gradient\n            perturbations = np.random.normal(0, self.step_size, size=(self.num_samples, self.dim))\n            \n            # Evaluate perturbed points\n            perturbed_points = np.clip(self.x_opt + perturbations, self.lb, self.ub)\n            fitness_values = np.array([func(x) for x in perturbed_points])\n            eval_count += self.num_samples\n            \n            if eval_count > self.budget:\n                fitness_values = fitness_values[:self.budget - (eval_count - self.num_samples)]\n                perturbed_points = perturbed_points[:self.budget - (eval_count - self.num_samples)]\n                \n            # Calculate fitness differences\n            fitness_diffs = fitness_values - self.f_opt\n            \n            # Update solution based on fitness differences and perturbations\n            if np.any(fitness_diffs < 0):\n                best_index = np.argmin(fitness_diffs)\n                if fitness_values[best_index] < self.f_opt:\n                  self.f_opt = fitness_values[best_index]\n                  self.x_opt = perturbed_points[best_index].copy()\n\n                # Adapt step size\n                self.step_size *= 0.95 # Reduce step size if improvement\n            else:\n                # Adapt step size\n                self.step_size *= 1.05 # Increase step size if no improvement\n                self.step_size = min(self.step_size, 1.0) # limit to 1.0\n            \n            if eval_count >= self.budget:\n                break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptivePerturbation scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0]}, "task_prompt": ""}
{"id": "c9a21fb8-2867-46fb-a4c0-d590405ae755", "fitness": 0.1603515648142121, "name": "AdaptiveSimulatedAnnealing", "description": "No description provided.", "code": "import numpy as np\n\nclass AdaptiveSimulatedAnnealing:\n    def __init__(self, budget=10000, dim=10, initial_temp=100.0, cooling_rate=0.95, temp_adjust_freq=100):\n        self.budget = budget\n        self.dim = dim\n        self.initial_temp = initial_temp\n        self.cooling_rate = cooling_rate\n        self.temp_adjust_freq = temp_adjust_freq\n        self.lb = -5.0\n        self.ub = 5.0\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        x = np.random.uniform(self.lb, self.ub, self.dim)\n        f = func(x)\n        self.f_opt = f\n        self.x_opt = x.copy()\n\n        temp = self.initial_temp\n        eval_count = 1\n        accept_count = 0\n\n        while eval_count < self.budget:\n            x_new = x + np.random.normal(0, 0.1, self.dim)\n            x_new = np.clip(x_new, self.lb, self.ub)\n            f_new = func(x_new)\n            eval_count += 1\n\n            delta_f = f_new - f\n            if delta_f < 0 or np.random.rand() < np.exp(-delta_f / temp):\n                x = x_new\n                f = f_new\n                accept_count += 1\n\n                if f < self.f_opt:\n                    self.f_opt = f\n                    self.x_opt = x.copy()\n\n            if eval_count % self.temp_adjust_freq == 0:\n                if accept_count / self.temp_adjust_freq > 0.6:\n                    temp *= 1.1  # Increase temp if accepting too many\n                elif accept_count / self.temp_adjust_freq < 0.4:\n                    temp *= 0.9  # Decrease temp if accepting too few\n                else:\n                    temp *= self.cooling_rate # default cooling\n\n                accept_count = 0\n            \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveSimulatedAnnealing scored 0.160 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.06434691001885695, 0.12254467837576977, 0.21468809214776863, 0.1223894296633935, 0.12179558454304529, 0.15732582974088305, 0.1818384264651155, 0.18839495637442183, 0.15786319067508936, 0.13899086991046627, 0.12734443133876217, 0.22316642121506858, 0.2471302474193603, 0.09077007542201898, 0.11955221188014253, 0.22376144436788503, 0.25311631276739577, 0.15460818414833288, 0.15016672283380483, 0.14723727697666023]}, "task_prompt": ""}
{"id": "5bb8f5ca-f61f-4c0e-99cf-7c25d1abcf42", "fitness": 0.27467879103640697, "name": "SimplifiedEvolutionaryAlgorithm", "description": "No description provided.", "code": "import numpy as np\n\nclass SimplifiedEvolutionaryAlgorithm:\n    def __init__(self, budget=10000, dim=10, pop_size=40, mutation_rate=0.1, tournament_size=4, initial_step_size=1.0):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.mutation_rate = mutation_rate\n        self.tournament_size = tournament_size\n        self.lb = -5.0\n        self.ub = 5.0\n        self.population = None\n        self.fitness = None\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.step_size = initial_step_size\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)].copy()\n        self.budget -= self.pop_size\n\n    def tournament_selection(self):\n        participants = np.random.choice(self.pop_size, self.tournament_size, replace=False)\n        winner = participants[np.argmin(self.fitness[participants])]\n        return winner\n\n    def evolve(self, func):\n        new_population = np.copy(self.population)\n        new_fitness = np.copy(self.fitness)\n\n        for i in range(self.pop_size):\n            if self.budget <= 0:\n                break\n\n            # Selection\n            parent_idx = self.tournament_selection()\n            parent = self.population[parent_idx]\n\n            # Mutation with self-adaptive step size\n            mutation = np.random.normal(0, self.step_size, self.dim) * np.random.binomial(1, self.mutation_rate, self.dim)\n            child = np.clip(parent + mutation, self.lb, self.ub)\n           \n            f_child = func(child)\n            self.budget -= 1\n\n            # Replacement - replace the worst in the population\n            worst_idx = np.argmax(self.fitness) \n            if f_child < self.fitness[worst_idx]:\n                new_population[worst_idx] = child\n                new_fitness[worst_idx] = f_child\n\n                if f_child < self.f_opt:\n                    self.f_opt = f_child\n                    self.x_opt = child.copy()\n        \n        # Adapt step size\n        if np.random.rand() < 0.1:\n            if np.mean(new_fitness) < np.mean(self.fitness):\n                self.step_size *= 1.1\n            else:\n                self.step_size *= 0.9\n            self.step_size = np.clip(self.step_size, 0.01, 1.5)\n        \n        self.population = new_population\n        self.fitness = new_fitness\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        while self.budget > 0:\n            self.evolve(func)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm SimplifiedEvolutionaryAlgorithm scored 0.275 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.10324215263619552, 0.18100574757960208, 0.3064561128819904, 0.21028706940562225, 0.22583083482980215, 0.20410873256343198, 0.24232753464416357, 0.26677925101924427, 0.19439278178562014, 0.16097371789976933, 0.18762689645092723, 0.9423919553652882, 0.262081090319102, 0.22079067249037032, 0.41991164924508373, 0.2735719001938699, 0.24460433221268574, 0.23527327598463876, 0.16140662946296036, 0.45051348375777156]}, "task_prompt": ""}
{"id": "3044025f-9ef4-4946-b9e8-4277bda16f02", "fitness": 0.25962262974573835, "name": "AdaptiveGradientDescent", "description": "No description provided.", "code": "import numpy as np\n\nclass AdaptiveGradientDescent:\n    def __init__(self, budget=10000, dim=10, step_size=0.1, momentum=0.9, exploration_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.step_size = step_size\n        self.momentum = momentum\n        self.exploration_rate = exploration_rate\n        self.lb = -5.0\n        self.ub = 5.0\n        self.velocity = np.zeros(dim)\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        x = np.random.uniform(self.lb, self.ub, self.dim)\n        fx = func(x)\n        self.f_opt = fx\n        self.x_opt = x.copy()\n        \n        eval_count = 1\n\n        while eval_count < self.budget:\n            # Estimate gradient (using finite differences)\n            gradient = np.zeros(self.dim)\n            delta = 1e-6\n            for i in range(self.dim):\n                x_plus = x.copy()\n                x_plus[i] += delta\n                x_plus = np.clip(x_plus, self.lb, self.ub)\n                f_plus = func(x_plus)\n                eval_count += 1\n                if eval_count >= self.budget:\n                  break\n                \n                x_minus = x.copy()\n                x_minus[i] -= delta\n                x_minus = np.clip(x_minus, self.lb, self.ub)\n                f_minus = func(x_minus)\n                eval_count += 1\n                if eval_count >= self.budget:\n                  break\n                \n                gradient[i] = (f_plus - f_minus) / (2 * delta)\n\n            # Update velocity with momentum\n            self.velocity = self.momentum * self.velocity - self.step_size * gradient\n\n            # Update position\n            x_new = x + self.velocity\n            x_new = np.clip(x_new, self.lb, self.ub)\n            \n\n            # Exploration: Random jump with probability exploration_rate\n            if np.random.rand() < self.exploration_rate:\n                x_new = np.random.uniform(self.lb, self.ub, self.dim)\n\n            f_new = func(x_new)\n            eval_count += 1\n\n            if f_new < fx:\n                x = x_new\n                fx = f_new\n\n                if f_new < self.f_opt:\n                    self.f_opt = f_new\n                    self.x_opt = x.copy()\n            \n            #Adaptive step size update\n            if eval_count % 100 == 0:\n              if f_new > fx:\n                self.step_size *= 0.9\n              else:\n                self.step_size *= 1.1\n              self.step_size = np.clip(self.step_size, 1e-6, 0.5)\n\n            if eval_count >= self.budget:\n              break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveGradientDescent scored 0.260 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.07154621836510688, 0.09020857852996345, 0.22428092745260253, 0.16207002954312788, 0.1540736840475283, 0.1711763789283638, 0.20461513663591913, 0.1877322207866935, 0.15320653091600067, 0.13470342139882374, 0.1606112437256817, 0.8846165614721597, 0.04322352054612366, 0.14785334104225967, 0.5365078922423936, 0.2735282932647669, 0.20574449833952424, 0.8908913807128194, 0.10977490026982506, 0.38608783669508395]}, "task_prompt": ""}
{"id": "574c75d1-2f4d-4afe-a164-e835495f95d1", "fitness": 0.3282465926177144, "name": "AdaptiveES", "description": "No description provided.", "code": "import numpy as np\n\nclass AdaptiveES:\n    def __init__(self, budget=10000, dim=10, pop_size=20, tau=None, tau_prime=None, initial_step_size=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lb = -5.0\n        self.ub = 5.0\n        if tau is None:\n            self.tau = 1 / np.sqrt(2 * self.dim)\n        else:\n            self.tau = tau\n        if tau_prime is None:\n            self.tau_prime = 1 / np.sqrt(2 * np.sqrt(self.dim))\n        else:\n            self.tau_prime = tau_prime\n        self.initial_step_size = initial_step_size\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        # Initialize population and step sizes\n        population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        step_sizes = np.full((self.pop_size, self.dim), self.initial_step_size)\n\n        fitness = np.array([func(x) for x in population])\n        self.func_evals = self.pop_size\n        \n        while self.func_evals < self.budget:\n            for i in range(self.pop_size):\n                # Mutate step sizes\n                global_factor = np.exp(self.tau_prime * np.random.normal(0, 1))\n                local_factors = np.exp(self.tau * np.random.normal(0, 1, size=self.dim))\n                mutated_step_sizes = step_sizes[i] * global_factor * local_factors\n                \n                # Mutate individual\n                mutated_individual = population[i] + mutated_step_sizes * np.random.normal(0, 1, size=self.dim)\n                mutated_individual = np.clip(mutated_individual, self.lb, self.ub)\n\n                # Evaluate offspring\n                f = func(mutated_individual)\n                self.func_evals += 1\n\n                # Selection: compare parent and offspring\n                if f < fitness[i]:\n                    population[i] = mutated_individual\n                    fitness[i] = f\n                    step_sizes[i] = mutated_step_sizes\n\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = mutated_individual.copy()\n                \n                if self.func_evals >= self.budget:\n                    break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveES scored 0.328 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.11667727201274702, 0.2007071155609973, 0.36152799137692504, 0.2356781916057804, 0.2701315140878401, 0.2921310172273316, 0.282590160181619, 0.2689221211055871, 0.26444780129129275, 0.16929171549541255, 0.2790820590354991, 0.956153691091522, 0.3155348049054203, 0.27865469402584697, 0.6391984713910828, 0.34081755018448734, 0.27990997198259526, 0.3259875689581323, 0.20720183842011541, 0.4802863024140531]}, "task_prompt": ""}
{"id": "43f0baac-5311-4be2-badd-646f0ff4fc21", "fitness": "-inf", "name": "CMAES", "description": "No description provided.", "code": "import numpy as np\n\nclass CMAES:\n    def __init__(self, budget=10000, dim=10, pop_size=None, sigma=0.5, cs=0.3, damps=None, c_cov_mean=None, c_cov_sigma=None):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size if pop_size is not None else 4 + int(3 * np.log(dim))\n        self.sigma = sigma\n        self.mean = np.random.uniform(-5, 5, size=dim)\n        self.C = np.eye(dim)\n        self.ps = np.zeros(dim)\n        self.pc = np.zeros(dim)\n        self.chiN = dim**0.5 * (1 - 1/(4*dim) + 1/(21*dim**2))\n\n        self.cs = cs\n        self.damps = damps if damps is not None else 1 + 2 * max(0, np.sqrt((self.pop_size - 1) / (dim + 1)) - 1) + self.cs\n        self.c_cov_mean = c_cov_mean if c_cov_mean is not None else 2 / ((dim + np.sqrt(2))**2)\n        self.c_cov_sigma = c_cov_sigma if c_cov_sigma is not None else 2 / ((dim + np.sqrt(2))**2) + (1 - 2 / ((dim + np.sqrt(2))**2)) * min(1, (2 * self.mu_eff - 1) / ((dim + 2)**2 + self.mu_eff))\n\n        self.mu = self.pop_size // 2\n        self.weights = np.log(self.pop_size + 1/2) - np.log(np.arange(1, self.pop_size + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mu_eff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n\n        self.lb = -5.0\n        self.ub = 5.0\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def sample_population(self):\n        z = np.random.normal(0, 1, size=(self.pop_size, self.dim))\n        return self.mean + self.sigma * np.dot(z, np.linalg.cholesky(self.C).T), z\n\n    def update_parameters(self, solutions, z):\n        fitness_values = np.array([f for f, _ in solutions])\n        indices = np.argsort(fitness_values)\n        solutions = [solutions[i][1] for i in indices]\n\n        y = solutions[:self.mu] - self.mean\n        z = z[indices[:self.mu]]\n\n        self.mean = np.sum(self.weights[:, None] * solutions[:self.mu], axis=0)\n\n        self.ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs) * self.mu_eff) * np.dot(np.linalg.inv(np.linalg.cholesky(self.C)), (self.mean - self.mean)) / self.sigma\n        self.pc = (1 - self.c_cov_mean) * self.pc + np.sqrt(self.c_cov_mean * (2 - self.c_cov_mean) * self.mu_eff) * y.sum(axis=0)\n\n        delta_hsigma = (np.linalg.norm(self.ps) / self.chiN - 1)\n        self.sigma *= np.exp((self.cs / self.damps) * delta_hsigma)\n\n        C_temp = (1 - self.c_cov_sigma) * self.C + self.c_cov_sigma * (self.pc[:, None] @ self.pc[None, :])\n\n        for i in range(self.mu):\n            C_temp += self.c_cov_sigma * self.weights[i] * y[i][:, None] @ y[i][None, :]\n\n        self.C = np.triu(C_temp) + np.triu(C_temp, 1).T  # Ensure symmetry\n\n    def __call__(self, func):\n        while self.budget > 0:\n            population, z = self.sample_population()\n            \n            # Clip the population within bounds\n            population = np.clip(population, self.lb, self.ub)\n\n            fitness_values = []\n            for i in range(len(population)):\n              if self.budget > 0:\n                f = func(population[i])\n                self.budget -= 1\n                fitness_values.append((f, population[i]))\n\n                if f < self.f_opt:\n                    self.f_opt = f\n                    self.x_opt = population[i].copy()\n              else:\n                break\n            \n            if len(fitness_values) > 0:\n              self.update_parameters(fitness_values, z)\n            else:\n              break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "In the code, line 18, in __init__, the following error occurred:\nAttributeError: 'CMAES' object has no attribute 'mu_eff'\nOn line: self.c_cov_sigma = c_cov_sigma if c_cov_sigma is not None else 2 / ((dim + np.sqrt(2))**2) + (1 - 2 / ((dim + np.sqrt(2))**2)) * min(1, (2 * self.mu_eff - 1) / ((dim + 2)**2 + self.mu_eff))", "error": "In the code, line 18, in __init__, the following error occurred:\nAttributeError: 'CMAES' object has no attribute 'mu_eff'\nOn line: self.c_cov_sigma = c_cov_sigma if c_cov_sigma is not None else 2 / ((dim + np.sqrt(2))**2) + (1 - 2 / ((dim + np.sqrt(2))**2)) * min(1, (2 * self.mu_eff - 1) / ((dim + 2)**2 + self.mu_eff))", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "9fd9b5c1-060e-48a7-a115-8945eabe87d5", "fitness": 0.6340505000495549, "name": "SelfAdaptiveDE", "description": "No description provided.", "code": "import numpy as np\n\nclass SelfAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=20, f_init=0.5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.f = f_init  # Initial mutation factor\n        self.lb = -5.0\n        self.ub = 5.0\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        # Initialization\n        population = np.random.uniform(self.lb, self.ub, (self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index].copy()\n        eval_count = self.pop_size\n\n        mutation_factors = np.full(self.pop_size, self.f)\n\n        while eval_count < self.budget:\n            for i in range(self.pop_size):\n                # Select three random individuals (excluding the current one)\n                indices = np.random.choice(self.pop_size, 3, replace=False)\n                while i in indices:\n                    indices = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = population[indices]\n\n                # Mutation: Use individual-specific mutation factor\n                v_mutation = x_r1 + mutation_factors[i] * (x_r2 - x_r3)\n                v_mutation = np.clip(v_mutation, self.lb, self.ub)\n\n                # Crossover (Binomial)\n                j_rand = np.random.randint(self.dim)\n                u = np.random.rand(self.dim)\n                mask = (u <= 0.9) | (np.arange(self.dim) == j_rand)  # Fixed CR = 0.9\n                u_crossover = np.where(mask, v_mutation, population[i])\n\n                # Evaluation\n                f_trial = func(u_crossover)\n                eval_count += 1\n\n                # Selection\n                if f_trial < fitness[i]:\n                    # Success! Update individual and adjust mutation factor\n                    fitness_improvement = fitness[i] - f_trial\n                    population[i] = u_crossover\n                    fitness[i] = f_trial\n                    mutation_factors[i] = np.clip(mutation_factors[i] * (1 + 0.2 * fitness_improvement / abs(fitness[i])), 0.1, 1.0) #Adaptive F\n                    \n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = u_crossover.copy()\n                else:\n                    # Failure: Reduce mutation factor slightly\n                    mutation_factors[i] = np.clip(mutation_factors[i] * 0.95, 0.1, 1.0)\n\n\n                if eval_count >= self.budget:\n                    break\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm SelfAdaptiveDE scored 0.634 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.1980427280049315, 0.8724103284455698, 0.5698335459924053, 0.9300787250397998, 0.8679684108812631, 0.8857006757657583, 0.3692696760976353, 0.7380962235953916, 0.8706605542056652, 0.22430960669112965, 0.9312380380130558, 0.9966806088866876, 0.35868892845593014, 0.3350392590390072, 0.7930128106933457, 0.8856215401731208, 0.24232274594239012, 0.9001606255129563, 0.20720836567633316, 0.5046666038787189]}, "task_prompt": ""}
