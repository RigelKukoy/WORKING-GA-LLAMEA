{"id": "42c80570-cec8-4427-bf7a-7db1656f2ce6", "fitness": 0.6991877278575359, "name": "AdaptiveDE", "description": "No description provided.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, Cr=0.9, local_search_iterations=5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.Cr = Cr\n        self.local_search_iterations = local_search_iterations\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.budget -= self.pop_size\n\n        if np.min(self.fitness) < self.f_opt:\n            self.f_opt = np.min(self.fitness)\n            self.x_opt = self.pop[np.argmin(self.fitness)]\n\n        stagnation_counter = 0\n        best_fitness_history = [self.f_opt]\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                if self.budget <= 0:\n                    break\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                a, b, c = self.pop[idxs]\n                \n                mutant = np.clip(a + self.F * (b - c), func.bounds.lb, func.bounds.ub)\n                \n                cross_mask = np.random.rand(self.dim) < self.Cr\n                trial = np.copy(self.pop[i])\n                trial[cross_mask] = mutant[cross_mask]\n\n                f_trial = func(trial)\n                self.budget -= 1\n\n                if f_trial < self.fitness[i]:\n                    self.pop[i] = trial\n                    self.fitness[i] = f_trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n            if self.f_opt <= min(best_fitness_history):\n                stagnation_counter = 0\n            else:\n                stagnation_counter += 1\n\n            best_fitness_history.append(self.f_opt)\n            if len(best_fitness_history) > 100:\n                best_fitness_history.pop(0)\n\n            if stagnation_counter > 50:\n                # Local Search on the best solution\n                for _ in range(min(self.local_search_iterations, self.budget)):\n                    if self.budget <= 0:\n                        break\n                    \n                    perturbation = np.random.normal(0, 0.1, size=self.dim)\n                    x_local = np.clip(self.x_opt + perturbation, func.bounds.lb, func.bounds.ub)\n                    f_local = func(x_local)\n                    self.budget -= 1\n\n                    if f_local < self.f_opt:\n                        self.f_opt = f_local\n                        self.x_opt = x_local\n                stagnation_counter = 0\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveDE scored 0.699 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.33501596069598605, 0.6171326062363471, 0.6761549827970317, 0.850270550763351, 0.725945492316461, 0.7792110314290891, 0.617180169083952, 0.6589703769025489, 0.7096811304791132, 0.6872094076342905, 0.8648782722070127, 0.9993292221928174, 0.6460817379292987, 0.7156598900648998, 0.9093553058989705, 0.7773660030233481, 0.5806606719346584, 0.8516646919762322, 0.4592602627451089, 0.5227267908401995]}, "task_prompt": ""}
{"id": "ba3e3784-4548-4abf-9f0b-2ca6e350f0b4", "fitness": 0.60868778255176, "name": "AdaptiveDE", "description": "No description provided.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.F = 0.5\n        self.CR = 0.7\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.archive = []\n        self.eval_count = self.pop_size\n        \n        while self.eval_count < self.budget:\n            for i in range(self.pop_size):\n                if self.eval_count >= self.budget:\n                    break\n                \n                # Adaptive F and CR\n                if np.random.rand() < 0.1:\n                  self.F = np.random.uniform(0.1, 0.9)\n                if np.random.rand() < 0.1:\n                  self.CR = np.random.uniform(0.1, 0.9)\n\n                # Mutation\n                idx = np.random.choice(self.pop_size, 3, replace=False)\n                x1, x2, x3 = self.population[idx]\n\n                if len(self.archive) > 0 and np.random.rand() < 0.1:\n                    xa = self.archive[np.random.randint(len(self.archive))]\n                    v = x1 + self.F * (xa - x3)\n                else:\n                    v = x1 + self.F * (x2 - x3)\n                \n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n                \n                # Crossover\n                u = np.copy(self.population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR or j == j_rand:\n                        u[j] = v[j]\n\n                # Selection\n                f_u = func(u)\n                self.eval_count += 1\n                \n                if f_u < self.fitness[i]:\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(self.population[i].copy())\n                    else:\n                        self.archive[np.random.randint(self.archive_size)] = self.population[i].copy()\n                    self.population[i] = u\n                    self.fitness[i] = f_u\n\n                    if f_u < self.f_opt:\n                        self.f_opt = f_u\n                        self.x_opt = u\n        \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveDE scored 0.609 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.23789892583638916, 0.4541800173130738, 0.5565827853392702, 0.7863330502054785, 0.6476499912090374, 0.7260187980941168, 0.5735191101360875, 0.5991761534111231, 0.6367977869886214, 0.6378458977895667, 0.7197996342580695, 0.9991667644678869, 0.33006383077320456, 0.6737837636991408, 0.8716280399154067, 0.7479847169876384, 0.4741503795102321, 0.7815619814938406, 0.21068029406169786, 0.5089337295453187]}, "task_prompt": ""}
{"id": "40005f2e-2f87-4fff-8af2-3e4eea6f07b2", "fitness": 0.3289289372251841, "name": "AdaptiveExplorationOptimization", "description": "No description provided.", "code": "import numpy as np\n\nclass AdaptiveExplorationOptimization:\n    def __init__(self, budget=10000, dim=10, pop_size=20, alpha=0.9, beta=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.alpha = alpha  # Weight for global best\n        self.beta = beta    # Weight for local best\n        self.step_size = 1.0 # Initial step size, decays over time\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        \n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n        \n        best_index = np.argmin(fitness)\n        global_best_x = population[best_index]\n        global_best_f = fitness[best_index]\n        \n        if global_best_f < self.f_opt:\n                self.f_opt = global_best_f\n                self.x_opt = global_best_x\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Local search: find the best neighbor\n                neighbor_index = np.random.choice(self.pop_size)\n                if fitness[neighbor_index] < fitness[i]:\n                    local_best_x = population[neighbor_index]\n                else:\n                    local_best_x = population[i]\n\n                # Update position based on global best, local best, and random movement\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n\n                new_position = population[i] + self.step_size * (\n                    self.alpha * r1 * (global_best_x - population[i]) +\n                    self.beta * r2 * (local_best_x - population[i]) +\n                    0.1 * np.random.uniform(-1, 1, self.dim) # Random exploration\n                )\n\n                # Clip to bounds\n                new_position = np.clip(new_position, func.bounds.lb, func.bounds.ub)\n\n                new_fitness = func(new_position)\n                self.budget -= 1\n\n                if new_fitness < fitness[i]:\n                    population[i] = new_position\n                    fitness[i] = new_fitness\n\n                # Update global best\n                if new_fitness < global_best_f:\n                    global_best_f = new_fitness\n                    global_best_x = new_position\n                    if global_best_f < self.f_opt:\n                        self.f_opt = global_best_f\n                        self.x_opt = global_best_x\n            \n            # Decay step size\n            self.step_size *= 0.99\n            \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveExplorationOptimization scored 0.329 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.14062608988098635, 0.1871873281527774, 0.548347070097956, 0.3355317693077925, 0.23531588311579477, 0.23527217189544714, 0.23360075526778168, 0.3089655357500586, 0.32010175104043437, 0.21211858294295272, 0.23119713148876653, 0.9953659596992136, 0.2541241530588536, 0.21695313300874253, 0.17766747636041347, 0.27187505369393006, 0.25468606093971724, 0.7774468789216117, 0.18828791116150112, 0.4539080487189523]}, "task_prompt": ""}
{"id": "9c18a554-fabc-4b94-ac1c-7e9dd97d0536", "fitness": 0.27938435597352235, "name": "AdaptiveCauchySearch", "description": "No description provided.", "code": "import numpy as np\n\nclass AdaptiveCauchySearch:\n    def __init__(self, budget=10000, dim=10, pop_size=20, initial_step_size=1.0):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.initial_step_size = initial_step_size\n        self.lb = -5.0\n        self.ub = 5.0\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.eval_count = 0\n\n        # Initialize population\n        population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.eval_count += self.pop_size\n\n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index]\n\n        step_size = self.initial_step_size\n\n        while self.eval_count < self.budget:\n            new_population = np.zeros_like(population)\n            new_fitness = np.zeros_like(fitness)\n\n            for i in range(self.pop_size):\n                # Cauchy mutation around the best solution\n                cauchy_vector = step_size * np.random.standard_cauchy(size=self.dim)\n                new_x = population[best_index] + cauchy_vector\n                \n                # Clip to bounds\n                new_x = np.clip(new_x, self.lb, self.ub)\n\n                new_f = func(new_x)\n                self.eval_count += 1\n\n                if new_f < fitness[i]:\n                    new_population[i] = new_x\n                    new_fitness[i] = new_f\n                else:\n                    new_population[i] = population[i]\n                    new_fitness[i] = fitness[i]\n\n\n                if new_f < self.f_opt:\n                    self.f_opt = new_f\n                    self.x_opt = new_x\n\n                if self.eval_count >= self.budget:\n                    break\n\n\n            population = new_population\n            fitness = new_fitness\n\n            best_index = np.argmin(fitness)\n            \n            # Adapt step size\n            if np.std(fitness) < 1e-6:\n                step_size *= 0.9\n            else:\n                step_size *= 1.1\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveCauchySearch scored 0.279 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.13600162267563132, 0.15107897273878568, 0.2654269339902823, 0.2221439730653877, 0.20074184743773116, 0.2618305568432241, 0.23091127768728714, 0.18548612225402628, 0.21816162052124932, 0.1482770122065682, 0.22341460388967493, 0.9999142385836555, 0.2139289912388913, 0.20921586944728854, 0.5553424124238886, 0.27478614848103855, 0.21724985337903013, 0.26204687960199313, 0.13849643933830869, 0.473231743666504]}, "task_prompt": ""}
{"id": "ec8d1019-05de-4f78-8e2f-b5cebb5b104f", "fitness": 0.3514028838269002, "name": "AdaptiveDifferentialEvolution", "description": "No description provided.", "code": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget=10000, dim=10, pop_size=20, f=0.5, cr=0.7):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.f = f  # Mutation factor\n        self.cr = cr  # Crossover rate\n        self.lb = -5.0\n        self.ub = 5.0\n        self.population = None\n        self.fitness = None\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.eval_count = 0\n\n    def initialize_population(self):\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.zeros(self.pop_size)\n\n    def evaluate_population(self, func):\n        for i in range(self.pop_size):\n            if self.eval_count < self.budget:\n                self.fitness[i] = func(self.population[i])\n                self.eval_count += 1\n                if self.fitness[i] < self.f_opt:\n                    self.f_opt = self.fitness[i]\n                    self.x_opt = self.population[i]\n            else:\n                return False #budget exceeded\n        return True\n    \n    def reinitialize_population(self, top_individuals, num_new_individuals=5):\n        \"\"\"Re-initializes a portion of the population around the best individuals.\"\"\"\n        new_population = np.copy(self.population)  # Start with the existing population\n        \n        for i in range(num_new_individuals):\n            # Select a random top individual\n            idx = np.random.randint(0, len(top_individuals))\n            best_x = top_individuals[idx]\n            \n            # Generate a new individual around this best individual\n            new_x = best_x + np.random.normal(0, 0.5, size=self.dim)  # Gaussian perturbation\n            new_x = np.clip(new_x, self.lb, self.ub)  # Clip to bounds\n            \n            # Replace a random individual in the population\n            replace_idx = np.random.randint(0, self.pop_size)\n            new_population[replace_idx] = new_x\n        \n        self.population = new_population\n        \n\n    def __call__(self, func):\n        self.initialize_population()\n        \n        if not self.evaluate_population(func):\n            return self.f_opt, self.x_opt\n        \n        generations = 0\n        while self.eval_count < self.budget:\n            generations += 1\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = self.population[idxs]\n                x_mutated = self.population[i] + self.f * (x_r2 - x_r3)\n                x_mutated = np.clip(x_mutated, self.lb, self.ub)\n                \n                # Crossover\n                x_trial = np.copy(self.population[i])\n                for j in range(self.dim):\n                    if np.random.rand() < self.cr or j == np.random.randint(0, self.dim):\n                        x_trial[j] = x_mutated[j]\n                \n                # Selection\n                f_trial = func(x_trial)\n                self.eval_count += 1\n\n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.population[i] = x_trial\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = x_trial\n            \n            # Adaptive F and CR (simple version)\n            self.f = 0.5 + 0.2 * np.random.randn()\n            self.cr = 0.7 + 0.1 * np.random.randn()\n            self.f = np.clip(self.f, 0.1, 1.0)\n            self.cr = np.clip(self.cr, 0.1, 1.0)\n\n            # Re-initialize the population based on top individuals\n            num_top = int(self.pop_size * 0.2)  # e.g., top 20%\n            top_indices = np.argsort(self.fitness)[:num_top]\n            top_individuals = self.population[top_indices]\n            self.reinitialize_population(top_individuals)\n            \n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveDifferentialEvolution scored 0.351 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.1614076821347633, 0.2366839885048766, 0.3491040765679867, 0.3687720345045473, 0.2579871701948905, 0.30958454721762296, 0.27517635223804415, 0.2868463321695416, 0.2772375414472401, 0.18716085569957053, 0.3731101746709794, 0.9954867411080425, 0.26907647458989337, 0.2767706392960818, 0.7150000593515493, 0.32737100167873756, 0.28380897635848246, 0.40889609727456266, 0.18505116766471907, 0.48352576386587276]}, "task_prompt": ""}
{"id": "342c7840-1139-4db6-8bd9-349cfcf9e2eb", "fitness": 0.5959520129331451, "name": "AdaptiveDifferentialEvolution", "description": "No description provided.", "code": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget=10000, dim=10, pop_size=20, cr=0.9, f=0.5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.cr = cr\n        self.f = f\n        self.lb = -5.0\n        self.ub = 5.0\n\n    def __call__(self, func):\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n        \n        evals = self.pop_size\n\n        while evals < self.budget:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n                mutant = a + self.f * (b - c)\n                mutant = np.clip(mutant, self.lb, self.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.cr\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, self.population[i])\n\n                # Selection\n                f = func(trial)\n                evals += 1\n\n                if f < self.fitness[i]:\n                    self.fitness[i] = f\n                    self.population[i] = trial\n\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = trial\n\n                # Adapt F\n                if evals % (self.pop_size * 5) == 0:\n                    success_indices = self.fitness < np.mean(self.fitness)\n                    if np.sum(success_indices) > 0:\n                        self.f = np.clip(np.random.normal(0.5, 0.3), 0.1, 1.0)\n\n                if evals >= self.budget:\n                    break\n                    \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveDifferentialEvolution scored 0.596 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.3843666127584118, 0.23123087518346686, 0.4649960340885517, 0.9483039297956335, 0.8752154400974762, 0.5009544192471663, 0.8844892937033277, 0.7564643652736465, 0.2865627258727721, 0.2617801284252337, 0.9155006036694, 0.9897483452761107, 0.39359726985559496, 0.8176099390252678, 0.7817203244037416, 0.9231783048811487, 0.45169297569323485, 0.3436543164143807, 0.19211148131346012, 0.515862873684878]}, "task_prompt": ""}
{"id": "11636074-e868-4f2e-87e6-27bd65536b9d", "fitness": 0.7135158713437029, "name": "AdaptiveDE", "description": "No description provided.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=25):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.F = 0.5\n        self.CR = 0.9\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.nevals = self.pop_size  # account for initial population evaluations\n        self.archive = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.archive_size, self.dim))\n\n        while self.nevals < self.budget:\n            for i in range(self.pop_size):\n                if self.nevals >= self.budget:\n                    break\n\n                # Mutation\n                a, b, c = np.random.choice(self.pop_size, 3, replace=False)\n                mutant = self.pop[a] + self.F * (self.pop[b] - self.pop[c])\n\n                # Add archive vector to the choice of parents\n                if np.random.rand() < 0.1:\n                    d = np.random.choice(self.archive_size, 1, replace=False)[0]\n                    mutant = self.pop[a] + self.F * (self.pop[b] - self.archive[d])\n\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                trial = np.copy(self.pop[i])\n                jrand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR or j == jrand:\n                        trial[j] = mutant[j]\n\n                # Selection\n                f_trial = func(trial)\n                self.nevals += 1\n\n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.pop[i] = trial\n\n                    # Update archive: random replacement\n                    idx = np.random.randint(self.archive_size)\n                    self.archive[idx] = trial\n\n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = trial\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveDE scored 0.714 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.32027023221907747, 0.7061296115143885, 0.6945769930255514, 0.8493069148189354, 0.7563708398238995, 0.7924903670596982, 0.6242609041497424, 0.6464130377218965, 0.7493338665556644, 0.6859606749786156, 0.8653177622105297, 0.9995609888838127, 0.6616040676545349, 0.7139369324945146, 0.9157634933568582, 0.8141932700964125, 0.6043618319101447, 0.8406640967609299, 0.5087685719554118, 0.5210329696834386]}, "task_prompt": ""}
{"id": "a67ccd43-6e7a-494a-81c1-7ff1621b587b", "fitness": "-inf", "name": "HybridDE", "description": "No description provided.", "code": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.7, Cr=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.Cr = Cr\n        self.lb = -5.0\n        self.ub = 5.0\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        \n        # Initialize population\n        population = np.random.uniform(self.lb, self.ub, (self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        evals = self.pop_size\n        \n        # Main loop\n        while evals < self.budget:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + self.F * (b - c), self.lb, self.ub)\n                \n                # Crossover\n                cross_mask = np.random.rand(self.dim) < self.Cr\n                trial = np.where(cross_mask, mutant, population[i])\n                \n                # Selection\n                f = func(trial)\n                evals += 1\n                \n                if f < fitness[i]:\n                    # Local Search with Nelder-Mead on improved solution\n                    res = minimize(func, trial, method='Nelder-Mead',\n                                   bounds=[(self.lb, self.ub)] * self.dim,\n                                   options={'maxfev': max(1, int((self.budget - evals) / (self.pop_size - i)))}\n                                  )\n                    \n                    if res.fun < f:\n                        trial = res.x\n                        f = res.fun\n                        evals += res.nfev\n                    \n                    population[i] = trial\n                    fitness[i] = f\n                    \n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = trial\n                \n                if evals >= self.budget:\n                    break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "In the code, line 41, in __call__, the following error occurred:\nNameError: name 'minimize' is not defined\nOn line: res = minimize(func, trial, method='Nelder-Mead',", "error": "In the code, line 41, in __call__, the following error occurred:\nNameError: name 'minimize' is not defined\nOn line: res = minimize(func, trial, method='Nelder-Mead',", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "09abcaa6-700f-411c-ab89-bab38c0b3145", "fitness": 0.14219617208096386, "name": "AdaptiveSimulatedAnnealing", "description": "No description provided.", "code": "import numpy as np\n\nclass AdaptiveSimulatedAnnealing:\n    def __init__(self, budget=10000, dim=10, initial_temp=1.0, cooling_rate=0.95):\n        self.budget = budget\n        self.dim = dim\n        self.initial_temp = initial_temp\n        self.cooling_rate = cooling_rate\n        self.temp = initial_temp\n        self.x_opt = None\n        self.f_opt = np.inf\n\n    def __call__(self, func):\n        x = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        f = func(x)\n        self.budget -= 1\n        self.f_opt = f\n        self.x_opt = x\n\n        while self.budget > 0:\n            # Adaptive perturbation size\n            perturbation_size = 0.1 * self.temp  # Perturbation scales with temperature\n            \n            x_new = x + np.random.normal(0, perturbation_size, size=self.dim)\n            x_new = np.clip(x_new, func.bounds.lb, func.bounds.ub)\n            f_new = func(x_new)\n            self.budget -= 1\n            \n            delta_e = f_new - f\n\n            if delta_e < 0:\n                x = x_new\n                f = f_new\n                if f < self.f_opt:\n                    self.f_opt = f\n                    self.x_opt = x\n            else:\n                acceptance_probability = np.exp(-delta_e / self.temp)\n                if np.random.rand() < acceptance_probability:\n                    x = x_new\n                    f = f_new\n\n            # Adaptive temperature schedule\n            self.temp *= self.cooling_rate\n            if self.temp < 1e-5: # prevent temperature collapse\n              self.temp = self.initial_temp \n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveSimulatedAnnealing scored 0.142 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [9.999999999998899e-05, 0.15141868731484087, 0.17171998270052047, 0.1313314939265643, 0.11728424456415487, 0.11414400321753948, 0.14248374729026347, 0.08710454492585562, 0.15780918897166762, 0.13340026048532472, 0.13817906525972923, 0.20965372884425748, 0.2647680888830385, 0.15587571890529905, 0.13164150916438555, 0.17249769815096694, 0.13957661446589786, 0.1672057039439826, 0.10726812011293008, 0.15046104049205877]}, "task_prompt": ""}
{"id": "b92f26cf-2c76-488b-8499-3a2fa561efb6", "fitness": "-inf", "name": "GaussianMixtureOptimisation", "description": "No description provided.", "code": "import numpy as np\nfrom sklearn.mixture import GaussianMixture\n\nclass GaussianMixtureOptimisation:\n    def __init__(self, budget=10000, dim=10, pop_size=20, n_components=5, lb=-5.0, ub=5.0):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.n_components = n_components\n        self.lb = lb\n        self.ub = ub\n        self.gmm = None\n        self.population = None\n        self.fitness = None\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        # Initialize population\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.nevals = self.pop_size\n\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n\n        while self.nevals < self.budget:\n            # Fit GMM to the best solutions\n            num_elites = min(self.pop_size // 2, self.nevals // 10)\n            elite_indices = np.argsort(self.fitness)[:num_elites]\n            elite_solutions = self.population[elite_indices]\n\n            if len(elite_solutions) > 1:\n                self.gmm = GaussianMixture(n_components=min(self.n_components, len(elite_solutions)), covariance_type='full', random_state=0, max_iter=100, n_init=1)\n                try:\n                    self.gmm.fit(elite_solutions)\n                except:\n                    self.gmm = None\n\n            # Sample new solutions from GMM\n            new_solutions = []\n            if self.gmm is not None:\n                try:\n                    new_solutions = self.gmm.sample(n_samples=self.pop_size)[0]\n                    new_solutions = np.clip(new_solutions, self.lb, self.ub)\n                except:\n                    new_solutions = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n            else:\n                 new_solutions = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n\n\n            # Evaluate new solutions\n            new_fitness = np.array([func(x) for x in new_solutions])\n            self.nevals += self.pop_size\n\n            # Replace old population with new solutions based on fitness\n            combined_population = np.concatenate((self.population, new_solutions))\n            combined_fitness = np.concatenate((self.fitness, new_fitness))\n            \n            sorted_indices = np.argsort(combined_fitness)[:self.pop_size]\n            self.population = combined_population[sorted_indices]\n            self.fitness = combined_fitness[sorted_indices]\n\n            # Update optimal solution\n            if np.min(self.fitness) < self.f_opt:\n                self.f_opt = np.min(self.fitness)\n                self.x_opt = self.population[np.argmin(self.fitness)]\n\n            if self.nevals >= self.budget:\n                break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "In the code, line 34, in __call__, the following error occurred:\nNameError: name 'GaussianMixture' is not defined\nOn line: self.gmm = GaussianMixture(n_components=min(self.n_components, len(elite_solutions)), covariance_type='full', random_state=0, max_iter=100, n_init=1)", "error": "In the code, line 34, in __call__, the following error occurred:\nNameError: name 'GaussianMixture' is not defined\nOn line: self.gmm = GaussianMixture(n_components=min(self.n_components, len(elite_solutions)), covariance_type='full', random_state=0, max_iter=100, n_init=1)", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "f8fec72e-a33e-455f-a655-b89ee5881ee7", "fitness": 0.35211668650094374, "name": "AdaptiveSimulatedAnnealing", "description": "No description provided.", "code": "import numpy as np\n\nclass AdaptiveSimulatedAnnealing:\n    def __init__(self, budget=10000, dim=10, initial_temp=1.0, cooling_rate=0.95, min_temp=1e-5, restart_patience=500):\n        self.budget = budget\n        self.dim = dim\n        self.initial_temp = initial_temp\n        self.cooling_rate = cooling_rate\n        self.min_temp = min_temp\n        self.restart_patience = restart_patience\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        x = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        f = func(x)\n        self.budget -= 1\n        if f < self.f_opt:\n            self.f_opt = f\n            self.x_opt = x\n\n        temp = self.initial_temp\n        stagnation_counter = 0\n        \n        while self.budget > 0 and temp > self.min_temp:\n            x_new = x + np.random.normal(0, temp**(1/self.dim), size=self.dim)\n            x_new = np.clip(x_new, func.bounds.lb, func.bounds.ub)\n            f_new = func(x_new)\n            self.budget -= 1\n\n            if f_new < f or np.random.rand() < np.exp((f - f_new) / temp):\n                x = x_new\n                f = f_new\n                if f < self.f_opt:\n                    self.f_opt = f\n                    self.x_opt = x\n                    stagnation_counter = 0\n            else:\n                stagnation_counter += 1\n            \n            temp *= self.cooling_rate\n\n            if stagnation_counter > self.restart_patience:\n                x = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                f = func(x)\n                self.budget -= 1\n                if f < self.f_opt:\n                    self.f_opt = f\n                    self.x_opt = x\n                temp = self.initial_temp\n                stagnation_counter = 0\n        \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveSimulatedAnnealing scored 0.352 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.07093635382445751, 0.17631189132745761, 0.3688927770917021, 0.5827137948907896, 0.2309029376275249, 0.43521810009193507, 0.2707540713724349, 0.3280322011682628, 0.2932848677563641, 0.16098527191971101, 0.36056663809573497, 0.9913505466166458, 0.2289242690690455, 0.33077942324307474, 0.595821998534977, 0.2850921520716385, 0.2826899383701449, 0.467224246446295, 0.1317100639606037, 0.4501421865400759]}, "task_prompt": ""}
{"id": "af3f892e-4a2e-40fe-8a81-17fe10d31361", "fitness": 0.49910959136436706, "name": "ParticleSwarmOptimizer", "description": "No description provided.", "code": "import numpy as np\n\nclass ParticleSwarmOptimizer:\n    def __init__(self, budget=10000, dim=10, pop_size=50, inertia_max=0.9, inertia_min=0.4, c1=2, c2=2):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.inertia_max = inertia_max\n        self.inertia_min = inertia_min\n        self.c1 = c1\n        self.c2 = c2\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.particles = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, size=(self.pop_size, self.dim))  # Initialize velocities\n        self.fitness = np.array([func(x) for x in self.particles])\n        self.nevals = self.pop_size\n        self.personal_best_positions = np.copy(self.particles)\n        self.personal_best_fitness = np.copy(self.fitness)\n        self.global_best_index = np.argmin(self.fitness)\n        self.global_best_position = np.copy(self.particles[self.global_best_index])\n        self.global_best_fitness = self.fitness[self.global_best_index]\n\n        while self.nevals < self.budget:\n            # Adapt inertia weight linearly\n            inertia = self.inertia_max - (self.inertia_max - self.inertia_min) * (self.nevals / self.budget)\n\n            for i in range(self.pop_size):\n                if self.nevals >= self.budget:\n                    break\n\n                # Update velocity\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                cognitive_velocity = self.c1 * r1 * (self.personal_best_positions[i] - self.particles[i])\n                social_velocity = self.c2 * r2 * (self.global_best_position - self.particles[i])\n                self.velocities[i] = inertia * self.velocities[i] + cognitive_velocity + social_velocity\n\n                # Velocity clamping\n                v_max = (func.bounds.ub - func.bounds.lb) * 0.1  # Example clamping value\n                self.velocities[i] = np.clip(self.velocities[i], -v_max, v_max)\n\n                # Update position\n                self.particles[i] = self.particles[i] + self.velocities[i]\n                self.particles[i] = np.clip(self.particles[i], func.bounds.lb, func.bounds.ub)\n\n                # Evaluate fitness\n                fitness = func(self.particles[i])\n                self.nevals += 1\n\n                # Update personal best\n                if fitness < self.personal_best_fitness[i]:\n                    self.personal_best_fitness[i] = fitness\n                    self.personal_best_positions[i] = np.copy(self.particles[i])\n\n                # Update global best\n                if fitness < self.global_best_fitness:\n                    self.global_best_fitness = fitness\n                    self.global_best_position = np.copy(self.particles[i])\n                    self.global_best_index = i\n\n                if fitness < self.f_opt:\n                    self.f_opt = fitness\n                    self.x_opt = self.particles[i]\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm ParticleSwarmOptimizer scored 0.499 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.2207701669377512, 0.43790262542929725, 0.5416230907645015, 0.7890734536728536, 0.45903172069742404, 0.6003643196220572, 0.3328800596797402, 0.44647847710266153, 0.503922669394106, 0.48730605992971743, 0.7036884521655269, 0.9934397588045955, 0.27950000952052345, 0.28955961535500685, 0.7312573041828563, 0.6062735355588882, 0.42768303581477485, 0.37664743301254844, 0.22525685389482875, 0.5295331857476815]}, "task_prompt": ""}
{"id": "25f9ba8f-6a62-48bf-856b-4c507a9b9e70", "fitness": 0.5927505295817592, "name": "PerturbedParticleSwarm", "description": "No description provided.", "code": "import numpy as np\n\nclass PerturbedParticleSwarm:\n    def __init__(self, budget=10000, dim=10, pop_size=30, inertia=0.7, cognitive_coeff=1.4, social_coeff=1.4, restart_prob=0.01):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.inertia = inertia\n        self.cognitive_coeff = cognitive_coeff\n        self.social_coeff = social_coeff\n        self.restart_prob = restart_prob\n        self.lb = -5.0\n        self.ub = 5.0\n        self.constriction_factor = 1 # 2 / abs(2 - (cognitive_coeff + social_coeff) - np.sqrt((cognitive_coeff + social_coeff)**2 - 4*(cognitive_coeff + social_coeff))) if (cognitive_coeff + social_coeff) > 4 else 1\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.particles = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, size=(self.pop_size, self.dim))\n        self.personal_best_positions = self.particles.copy()\n        self.personal_best_fitness = np.array([func(x) for x in self.particles])\n        self.evals = self.pop_size\n\n        if np.min(self.personal_best_fitness) < self.f_opt:\n            self.f_opt = np.min(self.personal_best_fitness)\n            self.x_opt = self.personal_best_positions[np.argmin(self.personal_best_fitness)]\n\n        while self.evals < self.budget:\n            global_best_index = np.argmin(self.personal_best_fitness)\n            global_best_position = self.personal_best_positions[global_best_index]\n\n            for i in range(self.pop_size):\n                # Update velocity\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                \n                cognitive_velocity = self.cognitive_coeff * r1 * (self.personal_best_positions[i] - self.particles[i])\n                social_velocity = self.social_coeff * r2 * (global_best_position - self.particles[i])\n                \n                self.velocities[i] = self.constriction_factor * (self.inertia * self.velocities[i] + cognitive_velocity + social_velocity)\n\n                # Clamp velocity\n                v_max = (self.ub - self.lb) / 2\n                self.velocities[i] = np.clip(self.velocities[i], -v_max, v_max)\n                \n\n                # Update position\n                self.particles[i] = self.particles[i] + self.velocities[i]\n                self.particles[i] = np.clip(self.particles[i], self.lb, self.ub)\n                \n                # Evaluate new position\n                f = func(self.particles[i])\n                self.evals += 1\n\n                # Update personal best\n                if f < self.personal_best_fitness[i]:\n                    self.personal_best_fitness[i] = f\n                    self.personal_best_positions[i] = self.particles[i].copy()\n\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = self.particles[i].copy()\n\n                # Random Restart\n                if np.random.rand() < self.restart_prob:\n                    self.particles[i] = np.random.uniform(self.lb, self.ub, size=self.dim)\n                    self.velocities[i] = np.random.uniform(-1, 1, size=self.dim)\n                    f = func(self.particles[i])\n                    self.evals += 1\n\n                    self.personal_best_fitness[i] = f\n                    self.personal_best_positions[i] = self.particles[i].copy()\n\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = self.particles[i].copy()\n\n                if self.evals >= self.budget:\n                    break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm PerturbedParticleSwarm scored 0.593 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.13911409238596573, 0.31821510383729734, 0.7784910004358755, 0.9320067441319861, 0.2592627767649245, 0.8512528298039557, 0.3284853603495317, 0.5020860316828373, 0.8339776507633545, 0.8341281125705095, 0.8954137103418669, 0.9955478858706428, 0.23239881714556865, 0.293265583771084, 0.7360724460145442, 0.8547933259908639, 0.4089456374185555, 0.8998343528686825, 0.22226961595260364, 0.5394495135345332]}, "task_prompt": ""}
{"id": "a99b563e-ed7f-4ea0-afa6-56140d851aa5", "fitness": 0.3547821712825871, "name": "AdaptivePSO", "description": "No description provided.", "code": "import numpy as np\n\nclass AdaptivePSO:\n    def __init__(self, budget=10000, dim=10, swarm_size=50, inertia=0.7, cognitive_coeff=1.4, social_coeff=1.4, velocity_clamp=0.5):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = swarm_size\n        self.inertia = inertia\n        self.cognitive_coeff = cognitive_coeff\n        self.social_coeff = social_coeff\n        self.velocity_clamp = velocity_clamp\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        # Initialize swarm\n        self.swarm = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-self.velocity_clamp * (func.bounds.ub - func.bounds.lb), self.velocity_clamp * (func.bounds.ub - func.bounds.lb), size=(self.swarm_size, self.dim))\n        self.personal_best_positions = self.swarm.copy()\n        self.personal_best_fitnesses = np.array([func(x) for x in self.swarm])\n        self.budget -= self.swarm_size\n\n        if np.min(self.personal_best_fitnesses) < self.f_opt:\n            self.f_opt = np.min(self.personal_best_fitnesses)\n            self.x_opt = self.personal_best_positions[np.argmin(self.personal_best_fitnesses)]\n        \n        global_best_index = np.argmin(self.personal_best_fitnesses)\n        self.global_best_position = self.personal_best_positions[global_best_index].copy()\n\n        while self.budget > 0:\n            # Update velocities and positions\n            inertia_weight = self.inertia + (1.0 - self.inertia) * (self.budget / 10000) # Adaptive inertia\n            for i in range(self.swarm_size):\n                if self.budget <= 0:\n                    break\n\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                r3 = np.random.rand(self.dim) #Diversity promoting\n\n                self.velocities[i] = (inertia_weight * self.velocities[i] +\n                                     self.cognitive_coeff * r1 * (self.personal_best_positions[i] - self.swarm[i]) +\n                                     self.social_coeff * r2 * (self.global_best_position - self.swarm[i]) +\n                                     0.1 * r3 * (np.random.uniform(func.bounds.lb, func.bounds.ub) - self.swarm[i]))\n                \n                # Velocity Clamping\n                self.velocities[i] = np.clip(self.velocities[i], -self.velocity_clamp * (func.bounds.ub - func.bounds.lb), self.velocity_clamp * (func.bounds.ub - func.bounds.lb))\n                \n                self.swarm[i] += self.velocities[i]\n                self.swarm[i] = np.clip(self.swarm[i], func.bounds.lb, func.bounds.ub)\n                \n                fitness = func(self.swarm[i])\n                self.budget -= 1\n\n                if fitness < self.personal_best_fitnesses[i]:\n                    self.personal_best_fitnesses[i] = fitness\n                    self.personal_best_positions[i] = self.swarm[i].copy()\n\n                    if fitness < self.f_opt:\n                        self.f_opt = fitness\n                        self.x_opt = self.swarm[i].copy()\n\n                        \n            global_best_index = np.argmin(self.personal_best_fitnesses)\n            self.global_best_position = self.personal_best_positions[global_best_index].copy()\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptivePSO scored 0.355 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.1632078969555365, 0.24747507961085125, 0.31812959505200256, 0.3892943343357088, 0.26408524275482015, 0.3264721212720403, 0.26715231600323575, 0.2732217589631243, 0.26432790250692284, 0.20698414909519736, 0.3746515096823999, 0.9946978204878822, 0.32893307468432254, 0.2744268763267349, 0.6960897554694467, 0.33948848400194065, 0.284385809051372, 0.39934882953277007, 0.19189251827006482, 0.4913783515953688]}, "task_prompt": ""}
{"id": "d9d4a78f-cbe9-4349-9b64-65f1adc20065", "fitness": 0.32190554033825797, "name": "AdaptiveSimulatedAnnealing", "description": "No description provided.", "code": "import numpy as np\n\nclass AdaptiveSimulatedAnnealing:\n    def __init__(self, budget=10000, dim=10, initial_temp=1.0, cooling_rate=0.95):\n        self.budget = budget\n        self.dim = dim\n        self.initial_temp = initial_temp\n        self.cooling_rate = cooling_rate\n        self.lb = -5.0\n        self.ub = 5.0\n        self.step_size = 1.0\n\n    def __call__(self, func):\n        self.x_opt = np.random.uniform(self.lb, self.ub, size=self.dim)\n        self.f_opt = func(self.x_opt)\n        evals = 1\n        temp = self.initial_temp\n\n        while evals < self.budget:\n            # Generate neighbor\n            x_new = self.x_opt + np.random.normal(0, self.step_size, size=self.dim)\n            x_new = np.clip(x_new, self.lb, self.ub)\n            f_new = func(x_new)\n            evals += 1\n\n            # Acceptance probability\n            delta_e = f_new - self.f_opt\n            if delta_e < 0 or np.random.rand() < np.exp(-delta_e / temp):\n                self.x_opt = x_new\n                self.f_opt = f_new\n                \n                #Increase step size if improvement\n                self.step_size *= 1.05\n\n            else:\n                #Decrease step size if no improvement\n                self.step_size *= 0.95\n            \n            self.step_size = np.clip(self.step_size, 0.01, 2.0) # Limit step size\n            temp *= self.cooling_rate  #Cooling\n            \n            if evals >= self.budget:\n                break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveSimulatedAnnealing scored 0.322 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.06013230967265659, 0.17322672534426597, 0.5566937612442202, 0.17501696477495132, 0.35057317689644163, 0.6458767990830534, 0.29824759285198976, 0.301271757332498, 0.5797834564968412, 0.21126488107968444, 0.22811664517444774, 0.190370140493488, 0.25432023378975466, 0.2901444872425557, 0.8892965858435419, 0.3623037145581637, 0.27983628044749265, 0.1571141855319449, 0.21921645990129934, 0.21530464900586854]}, "task_prompt": ""}
{"id": "f2ed02f2-d6dd-4795-8d45-73cb3e792b16", "fitness": "-inf", "name": "PSOSimplex", "description": "No description provided.", "code": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass PSOSimplex:\n    def __init__(self, budget=10000, dim=10, pop_size=30, inertia=0.7, cognitive_coeff=1.4, social_coeff=1.4, simplex_iterations=3):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.inertia = inertia\n        self.cognitive_coeff = cognitive_coeff\n        self.social_coeff = social_coeff\n        self.simplex_iterations = simplex_iterations\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        # Initialize population and velocities\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, size=(self.pop_size, self.dim))\n        self.personal_best_positions = self.population.copy()\n        self.personal_best_fitness = np.array([func(x) for x in self.population])\n        self.eval_count = self.pop_size\n\n        # Initialize global best\n        self.global_best_index = np.argmin(self.personal_best_fitness)\n        self.global_best_position = self.personal_best_positions[self.global_best_index].copy()\n        self.f_opt = self.personal_best_fitness[self.global_best_index].copy()\n        self.x_opt = self.global_best_position.copy()\n\n        while self.eval_count < self.budget:\n            for i in range(self.pop_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                # Update velocity and position\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                self.velocities[i] = (self.inertia * self.velocities[i] +\n                                      self.cognitive_coeff * r1 * (self.personal_best_positions[i] - self.population[i]) +\n                                      self.social_coeff * r2 * (self.global_best_position - self.population[i]))\n                self.population[i] = np.clip(self.population[i] + self.velocities[i], func.bounds.lb, func.bounds.ub)\n\n                # Evaluate fitness\n                fitness = func(self.population[i])\n                self.eval_count += 1\n\n                # Update personal best\n                if fitness < self.personal_best_fitness[i]:\n                    self.personal_best_fitness[i] = fitness\n                    self.personal_best_positions[i] = self.population[i].copy()\n\n                    # Update global best\n                    if fitness < self.f_opt:\n                        self.f_opt = fitness\n                        self.x_opt = self.population[i].copy()\n                        self.global_best_position = self.population[i].copy()\n\n            # Local search with Nelder-Mead on global best\n            if self.eval_count + self.simplex_iterations * (self.dim + 1) <= self.budget:\n                res = minimize(func, self.global_best_position, method='Nelder-Mead',\n                               options={'maxfev': self.simplex_iterations * (self.dim + 1)})\n                if res.fun < self.f_opt:\n                    self.f_opt = res.fun\n                    self.x_opt = res.x\n                    self.global_best_position = res.x\n                self.eval_count += res.nfev\n            \n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "In the code, line 61, in __call__, the following error occurred:\nNameError: name 'minimize' is not defined\nOn line: res = minimize(func, self.global_best_position, method='Nelder-Mead',", "error": "In the code, line 61, in __call__, the following error occurred:\nNameError: name 'minimize' is not defined\nOn line: res = minimize(func, self.global_best_position, method='Nelder-Mead',", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "cb0614ad-87dc-459e-83ed-16198d879ec4", "fitness": "-inf", "name": "BudgetAwareCMAES", "description": "No description provided.", "code": "import numpy as np\n\nclass BudgetAwareCMAES:\n    def __init__(self, budget=10000, dim=10, pop_size=None, sigma0=0.5, mu_factor=0.25, cs=0.3, damps=1.0, ccov1=None, ccovmu=None):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size if pop_size is not None else 4 + int(3 * np.log(self.dim))\n        self.mu = int(self.pop_size * mu_factor)\n        self.sigma = sigma0\n        self.mean = None\n        self.C = None\n        self.pc = None\n        self.ps = None\n        self.chiN = None\n        self.eval_count = 0\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.cs = cs\n        self.damps = damps + 2 * max(0, np.sqrt((self.mu / self.pop_size) - 1))\n        self.ccov1 = 2 / ((self.dim + 1.3)**2 + self.mu) if ccov1 is None else ccov1\n        self.ccovmu = 2 * (self.mu / self.pop_size) / ((self.dim + 2)**2 + self.mu) if ccovmu is None else ccovmu\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.chiN = np.sqrt(self.dim) * (1 - (1 / (4 * self.dim)) + (1 / (21 * self.dim**2)))\n        self.restart_trigger = 1000  # Initial restart trigger\n        self.restart_count = 0\n\n    def __call__(self, func):\n        self.mean = np.random.uniform(func.bounds.lb, func.bounds.ub, self.dim)\n        while self.eval_count < self.budget:\n            if self.eval_count + self.pop_size > self.budget:\n                self.pop_size = self.budget - self.eval_count\n                if self.pop_size <= 0:\n                    break\n\n            z = np.random.multivariate_normal(np.zeros(self.dim), self.C, self.pop_size)\n            x = self.mean + self.sigma * z\n            x = np.clip(x, func.bounds.lb, func.bounds.ub)\n            fitness = np.array([func(xi) for xi in x])\n            self.eval_count += self.pop_size\n            \n            idx = np.argsort(fitness)\n            x_sorted = x[idx]\n            fitness_sorted = fitness[idx]\n\n            if fitness_sorted[0] < self.f_opt:\n                self.f_opt = fitness_sorted[0]\n                self.x_opt = x_sorted[0]\n            \n            x_mu = x_sorted[:self.mu]\n            weights = np.log(self.pop_size + 1) - np.log(1 + np.arange(self.mu))\n            weights /= np.sum(weights)\n            \n            old_mean = self.mean.copy()\n            self.mean += self.sigma * np.sum(weights[:, None] * z[idx[:self.mu]], axis=0)\n\n            self.ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs) * self.mu) * (self.mean - old_mean) / self.sigma\n            hsig = np.linalg.norm(self.ps) / np.sqrt(1 - (1 - self.cs)**(2 * (self.eval_count / self.pop_size))) < (1.4 + (2 / (self.dim + 1))) * self.chiN\n            self.pc = (1 - self.ccov1) * self.pc + hsig * np.sqrt(self.ccov1 * (2 - self.ccov1) * self.mu) * ((self.mean - old_mean) / self.sigma)\n\n            artmp = (1 / self.sigma) * (x_mu - old_mean).T\n            self.C = (1 - self.ccov1 - self.ccovmu + self.ccov1 * np.sum(self.pc[:, None] * self.pc[None, :])) * self.C + self.ccovmu * artmp @ np.diag(weights) @ artmp.T\n            \n            self.sigma *= np.exp((self.cs / self.damps) * (np.linalg.norm(self.ps) / self.chiN - 1))\n            self.C = np.triu(self.C) + np.triu(self.C, 1).T\n            try:\n                np.linalg.cholesky(self.C)\n            except np.linalg.LinAlgError:\n                self.C = np.eye(self.dim)\n                self.pc = np.zeros(self.dim)\n                self.ps = np.zeros(self.dim)\n\n            # Budget-aware adaptation of restart trigger\n            if self.eval_count > self.restart_trigger:\n              self.restart_count += 1\n              self.restart_trigger += 1000 * (1 + self.restart_count)\n              self.mean = np.random.uniform(func.bounds.lb, func.bounds.ub, self.dim)\n              self.sigma = 0.5  # Reset sigma\n              self.C = np.eye(self.dim)\n              self.pc = np.zeros(self.dim)\n              self.ps = np.zeros(self.dim)\n              \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "In the code, line 154, in _raise_linalgerror_svd_nonconvergence, the following error occurred:\nLinAlgError: SVD did not converge", "error": "In the code, line 154, in _raise_linalgerror_svd_nonconvergence, the following error occurred:\nLinAlgError: SVD did not converge", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "55fd4632-c18e-4579-8544-605e20151603", "fitness": 0.13812367859166857, "name": "CmaDifferentialEvolution", "description": "No description provided.", "code": "import numpy as np\n\nclass CmaDifferentialEvolution:\n    def __init__(self, budget=10000, dim=10, pop_size=20, cr=0.9, initial_sigma=0.5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.cr = cr\n        self.initial_sigma = initial_sigma\n        self.lb = -5.0\n        self.ub = 5.0\n\n        self.mean = np.random.uniform(self.lb, self.ub, size=self.dim)\n        self.sigma = self.initial_sigma\n        self.C = np.eye(self.dim)  # Covariance matrix\n\n    def __call__(self, func):\n        self.population = np.random.multivariate_normal(self.mean, self.sigma**2 * self.C, size=self.pop_size)\n        self.population = np.clip(self.population, self.lb, self.ub)\n        self.fitness = np.array([func(x) for x in self.population])\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n        \n        evals = self.pop_size\n\n        while evals < self.budget:\n            offspring = []\n            offspring_fitness = []\n\n            for i in range(self.pop_size):\n                # Differential Mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n                mutant = a + self.sigma * (b - c)\n                mutant = np.clip(mutant, self.lb, self.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.cr\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, self.population[i])\n                \n                # Evaluate\n                f = func(trial)\n                evals += 1\n                offspring.append(trial)\n                offspring_fitness.append(f)\n\n                if f < self.f_opt:\n                    self.f_opt = f\n                    self.x_opt = trial\n\n                if evals >= self.budget:\n                    break\n            \n            offspring = np.array(offspring)\n            offspring_fitness = np.array(offspring_fitness)\n\n            # Selection: Replace the worst individuals in the population with the best offspring\n            for i in np.argsort(self.fitness)[::-1]:\n                j = np.argmin(offspring_fitness)\n                if offspring_fitness[j] < self.fitness[i]:\n                    self.fitness[i] = offspring_fitness[j]\n                    self.population[i] = offspring[j]\n                    offspring_fitness = np.delete(offspring_fitness, j)\n                    offspring = np.delete(offspring, obj=j, axis=0)\n                    if offspring.size == 0:\n                        break\n            \n            # CMA-ES-like Adaptation\n            self.mean = np.mean(self.population, axis=0)\n            diff = self.population - self.mean\n            self.C = np.cov(diff.T) + 1e-8 * np.eye(self.dim) # Add a small value to avoid singular matrix\n            self.sigma *= np.exp(0.1 * (np.mean(self.fitness) - np.mean(offspring_fitness)) / self.f_opt)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm CmaDifferentialEvolution scored 0.138 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.15548580214044783, 0.14392021689574763, 0.21556528431728827, 0.13619892191981142, 0.08691576148048441, 0.11638008942439515, 0.058662458932172146, 0.09302530117324825, 0.11716421283830203, 0.20120301298401955, 0.12431242808613663, 0.21161687127112938, 0.042815646761704684, 0.08039277787527321, 0.09291249718571959, 0.26788373817305755, 0.1961443342264868, 0.1278491916555673, 0.14307272322942577, 0.1509523012629539]}, "task_prompt": ""}
{"id": "3d22cf66-882e-4cd8-a5d1-b69f5bc02db7", "fitness": 0.3769302281702085, "name": "EnhancedDE", "description": "No description provided.", "code": "import numpy as np\n\nclass EnhancedDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, CR=0.9, restart_prob=0.05):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.restart_prob = restart_prob\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.nevals = self.pop_size\n\n        while self.nevals < self.budget:\n            for i in range(self.pop_size):\n                if self.nevals >= self.budget:\n                    break\n\n                # Mutation\n                a, b, c = np.random.choice(self.pop_size, 3, replace=False)\n                mutant = self.pop[a] + self.F * (self.pop[b] - self.pop[c])\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                trial = np.copy(self.pop[i])\n                jrand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR or j == jrand:\n                        trial[j] = mutant[j]\n\n                # Selection\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n                f_trial = func(trial)\n                self.nevals += 1\n\n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.pop[i] = trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                else:\n                    # Parameter Adaptation (adjust F and CR based on success)\n                    if np.random.rand() < 0.1:\n                        self.F = np.clip(np.random.normal(self.F, 0.1), 0.1, 0.9)\n                        self.CR = np.clip(np.random.normal(self.CR, 0.1), 0.1, 0.9)\n\n                # Restart mechanism\n                if np.random.rand() < self.restart_prob:\n                    idx = np.random.randint(self.pop_size)\n                    self.pop[idx] = np.random.uniform(func.bounds.lb, func.bounds.ub)\n                    self.fitness[idx] = func(self.pop[idx])\n                    self.nevals += 1\n\n                    if self.fitness[idx] < self.f_opt:\n                        self.f_opt = self.fitness[idx]\n                        self.x_opt = self.pop[idx]\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm EnhancedDE scored 0.377 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.1787443452919455, 0.26764033039760593, 0.479451715772381, 0.6360424255948978, 0.5108698308660388, 0.5240112887188056, 0.41868188871999323, 0]}, "task_prompt": ""}
{"id": "95463fb0-2092-41d3-b690-fe73c5a99cef", "fitness": 0.5018704220734258, "name": "DynamicAdaptiveDE", "description": "No description provided.", "code": "import numpy as np\n\nclass DynamicAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size_min=10, pop_size_max=100, archive_size=20, learning_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size_min = pop_size_min\n        self.pop_size_max = pop_size_max\n        self.pop_size = pop_size_max # Start with larger population for exploration\n        self.archive_size = archive_size\n        self.learning_rate = learning_rate\n        self.F = 0.5\n        self.CR = 0.7\n        self.successful_F = []\n        self.successful_CR = []\n        self.archive = []\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.eval_count = self.pop_size\n        \n        while self.eval_count < self.budget:\n            # Dynamically adjust population size\n            if self.eval_count > self.budget * 0.7:\n                self.pop_size = max(self.pop_size_min, int(self.pop_size * 0.95)) # Reduce population size near the end for exploitation\n            else:\n                self.pop_size = min(self.pop_size_max, self.pop_size)\n\n            for i in range(self.pop_size):\n                if self.eval_count >= self.budget:\n                    break\n                \n                # Mutation\n                idx = np.random.choice(self.pop_size, 3, replace=False)\n                x1, x2, x3 = self.population[idx]\n\n                if len(self.archive) > 0 and np.random.rand() < 0.1:\n                    xa = self.archive[np.random.randint(len(self.archive))]\n                    v = x1 + self.F * (xa - x3)\n                else:\n                    v = x1 + self.F * (x2 - x3)\n                \n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n                \n                # Crossover\n                u = np.copy(self.population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR or j == j_rand:\n                        u[j] = v[j]\n\n                # Selection\n                f_u = func(u)\n                self.eval_count += 1\n                \n                if f_u < self.fitness[i]:\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(self.population[i].copy())\n                    else:\n                        self.archive[np.random.randint(self.archive_size)] = self.population[i].copy()\n                    \n                    # Store successful parameters\n                    self.successful_F.append(self.F)\n                    self.successful_CR.append(self.CR)\n\n                    self.population[i] = u\n                    self.fitness[i] = f_u\n\n                    if f_u < self.f_opt:\n                        self.f_opt = f_u\n                        self.x_opt = u\n\n            # Update F and CR based on successful values\n            if len(self.successful_F) > 0:\n                self.F = (1 - self.learning_rate) * self.F + self.learning_rate * np.mean(self.successful_F)\n                self.CR = (1 - self.learning_rate) * self.CR + self.learning_rate * np.mean(self.successful_CR)\n                self.successful_F = []\n                self.successful_CR = []\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm DynamicAdaptiveDE scored 0.502 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.19872767784088197, 0.3110109569322237, 0.44095314976251443, 0.6953340352115844, 0.514617518482196, 0.5810663323361873, 0.3759330313180699, 0.4559959242477597, 0.4924615908721628, 0.39159304233705305, 0.6429436392820593, 0.9918183847623196, 0.3211122302061914, 0.47647886360497804, 0.7895395939254333, 0.5857206065699108, 0.42089832748567524, 0.64759057070368, 0.19841129506765776, 0.5052016705199784]}, "task_prompt": ""}
{"id": "e2f2bd89-d586-4da9-889e-8221d6b5a58a", "fitness": "-inf", "name": "BudgetAwareCMAES", "description": "No description provided.", "code": "import numpy as np\n\nclass BudgetAwareCMAES:\n    def __init__(self, budget=10000, dim=10, initial_pop_size=None, cs=0.3, damps=1, ccov1=0.1, ccovmu=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.mu = int(np.floor(self.dim / 2))\n        self.initial_pop_size = initial_pop_size if initial_pop_size is not None else 4 + int(3 * np.log(self.dim))\n        self.pop_size = self.initial_pop_size\n\n        self.m = np.zeros(self.dim)\n        self.sigma = 0.2\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.chiN = self.dim**0.5 * (1 - 1/(4*self.dim) + 1/(21*self.dim**2))\n\n        self.cs = cs\n        self.damps = damps\n        self.ccov1 = ccov1 / (self.dim + 1)\n        self.ccovmu = ccovmu / (self.dim + 1)\n\n        self.weights = np.log(self.pop_size + 1/2) - np.log(np.arange(1, self.pop_size + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        self.ccovmu = min(1-self.ccov1, self.ccovmu * self.mueff)\n\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        while self.budget > 0:\n            if self.budget < self.pop_size:\n                self.pop_size = self.budget\n                self.weights = np.log(self.pop_size + 1/2) - np.log(np.arange(1, self.pop_size + 1))\n                self.weights = self.weights / np.sum(self.weights)\n                self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n                self.ccovmu = min(1-self.ccov1, self.ccovmu * self.mueff)\n\n            z = np.random.randn(self.dim, self.pop_size)\n            x = self.m[:, np.newaxis] + self.sigma * np.dot(np.linalg.cholesky(self.C), z)\n            x = np.clip(x, func.bounds.lb, func.bounds.ub)\n\n            fitness = np.array([func(x[:,i]) for i in range(self.pop_size)])\n            self.budget -= self.pop_size\n\n            if np.min(fitness) < self.f_opt:\n                self.f_opt = np.min(fitness)\n                self.x_opt = x[:, np.argmin(fitness)]\n\n            idx = np.argsort(fitness)\n            x_sorted = x[:, idx]\n\n            m_old = self.m.copy()\n            self.m = np.dot(x_sorted[:, :self.mu], self.weights[:self.mu])\n\n            self.ps = (1 - self.cs) * self.ps + (self.cs * (2 - self.cs) * self.mueff)**0.5 * np.dot(np.linalg.inv(np.linalg.cholesky(self.C)), (self.m - m_old)) / self.sigma\n            hsig = (np.sum(self.ps**2) / (self.dim * (1 - self.cs)**(2 * (self.budget / self.pop_size))) < 2 + 4/(self.dim + 1))\n            self.pc = (1 - self.ccov1) * self.pc + hsig * (self.ccov1 * (2 - self.ccov1) * self.mueff)**0.5 * (self.m - m_old) / self.sigma\n\n            artmp = (1/self.sigma) * (x_sorted[:, :self.mu] - m_old[:, np.newaxis])\n            self.C = (1 - self.ccov1 - self.ccovmu + self.ccov1 * (1/self.mueff)) * self.C + self.ccov1 * np.outer(self.pc, self.pc) + self.ccovmu * np.dot(artmp, np.diag(self.weights[:self.mu])).dot(artmp.T)\n\n            self.sigma *= np.exp((self.cs/self.damps) * (np.linalg.norm(self.ps)/self.chiN - 1))\n\n            self.C = np.triu(self.C) + np.triu(self.C, 1).T\n            self.C = self.C + 1e-8 * np.eye(self.dim)\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "In the code, line 113, in _clip, the following error occurred:\nValueError: operands could not be broadcast together with shapes (2,6) (2,) (2,) ", "error": "In the code, line 113, in _clip, the following error occurred:\nValueError: operands could not be broadcast together with shapes (2,6) (2,) (2,) ", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "6d2765fe-c6b2-4a55-91e7-66b7a613f5ba", "fitness": 0.3095381356428037, "name": "EnhancedDE", "description": "No description provided.", "code": "import numpy as np\n\nclass EnhancedDE:\n    def __init__(self, budget=10000, dim=10, pop_size=100, restart_interval=2000):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.restart_interval = restart_interval\n        self.F = 0.9\n        self.CR = 0.3\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.eval_count = self.pop_size\n\n        while self.eval_count < self.budget:\n            # Restart population periodically\n            if self.eval_count % self.restart_interval < self.pop_size and self.eval_count > 0:\n                self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                self.fitness = np.array([func(x) for x in self.population])\n                self.eval_count += self.pop_size\n                if self.eval_count >= self.budget:\n                  break\n\n\n            for i in range(self.pop_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                # Mutation\n                idx = np.random.choice(self.pop_size, 3, replace=False)\n                x1, x2, x3 = self.population[idx]\n                v = x1 + self.F * (x2 - x3)\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                u = np.copy(self.population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR or j == j_rand:\n                        u[j] = v[j]\n\n                # Selection\n                f_u = func(u)\n                self.eval_count += 1\n\n                if f_u < self.fitness[i]:\n                    self.population[i] = u\n                    self.fitness[i] = f_u\n\n                    if f_u < self.f_opt:\n                        self.f_opt = f_u\n                        self.x_opt = u\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm EnhancedDE scored 0.310 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.1355827603454407, 0.1964519285012568, 0.2909563874302439, 0.24647972707906562, 0.24182670486143254, 0.25706022741951107, 0.2659361978202802, 0.2389255007878287, 0.22845748185567827, 0.17936547027338912, 0.24639334116106348, 0.9955115812703685, 0.2576965352687266, 0.2425794153464006, 0.6562227035548536, 0.3054458969463184, 0.2596304225586996, 0.3073907192551265, 0.1676769026311129, 0.4711728084892767]}, "task_prompt": ""}
{"id": "1284b326-77d3-4244-a3db-bc4c2fa50461", "fitness": 0.3747748467154987, "name": "HybridDEPSO", "description": "No description provided.", "code": "import numpy as np\n\nclass HybridDEPSO:\n    def __init__(self, budget=10000, dim=10, pop_size=20, w=0.7, c1=1.5, c2=1.5, cr=0.9, f=0.5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.w = w  # Inertia weight\n        self.c1 = c1  # Cognitive coefficient\n        self.c2 = c2  # Social coefficient\n        self.cr = cr\n        self.f = f\n        self.lb = -5.0\n        self.ub = 5.0\n        self.population = None\n        self.fitness = None\n        self.velocities = None\n        self.personal_best_positions = None\n        self.personal_best_fitness = None\n        self.global_best_position = None\n        self.global_best_fitness = np.inf\n\n    def __call__(self, func):\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, size=(self.pop_size, self.dim))  # Initialize velocities\n        self.fitness = np.array([func(x) for x in self.population])\n        self.personal_best_positions = self.population.copy()\n        self.personal_best_fitness = self.fitness.copy()\n\n        self.global_best_position = self.population[np.argmin(self.fitness)].copy()\n        self.global_best_fitness = np.min(self.fitness)\n\n        self.f_opt = self.global_best_fitness\n        self.x_opt = self.global_best_position.copy()\n        \n        evals = self.pop_size\n\n        while evals < self.budget:\n            for i in range(self.pop_size):\n                # PSO Velocity Update\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n\n                cognitive_component = self.c1 * r1 * (self.personal_best_positions[i] - self.population[i])\n                social_component = self.c2 * r2 * (self.global_best_position - self.population[i])\n\n                self.velocities[i] = self.w * self.velocities[i] + cognitive_component + social_component\n\n                # Differential Evolution Mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n                mutant = a + self.f * (b - c)\n                mutant = np.clip(mutant, self.lb, self.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.cr\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, self.population[i])\n                \n                # Update position with velocity\n                trial = self.population[i] + self.velocities[i]\n                trial = np.clip(trial, self.lb, self.ub)\n\n\n                # Selection\n                f = func(trial)\n                evals += 1\n\n                if f < self.fitness[i]:\n                    self.fitness[i] = f\n                    self.population[i] = trial\n\n                    if f < self.personal_best_fitness[i]:\n                        self.personal_best_fitness[i] = f\n                        self.personal_best_positions[i] = trial.copy()\n\n\n                    if f < self.global_best_fitness:\n                        self.global_best_fitness = f\n                        self.global_best_position = trial.copy()\n                        self.f_opt = f\n                        self.x_opt = trial.copy()\n                        \n                if evals >= self.budget:\n                    break\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDEPSO scored 0.375 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.12028068948110582, 0.21041541259723306, 0.3855138913268429, 0.9668855921178033, 0.2887852893388828, 0.24385969843923083, 0.25921619977990984, 0.3706591649448564, 0.22679559995532816, 0.1480786597917335, 0.28327215220891644, 0.9963477795914012, 0.27208011014180467, 0.25674581497034843, 0.5634914112320455, 0.3834459116175757, 0.36774781798724654, 0.380886533877158, 0.3124679940163466, 0.45852121089420583]}, "task_prompt": ""}
{"id": "5164390a-0ac7-4f4d-9520-e12c3b34d4d1", "fitness": 0.2867522461928686, "name": "CMAES", "description": "No description provided.", "code": "import numpy as np\n\nclass CMAES:\n    def __init__(self, budget=10000, dim=10, pop_size=None, sigma=0.5, cs=0.3, damps=None, ccov1=None, ccovmu=None):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size if pop_size is not None else 4 + int(3 * np.log(self.dim))\n        self.sigma = sigma\n        self.mu = self.pop_size // 2\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.m = None\n        self.C = None\n        self.pc = None\n        self.ps = None\n        self.chiN = self.dim**0.5 * (1 - 1/(4*self.dim) + 1/(21*self.dim**2))\n\n        self.cs = cs\n        self.damps = damps if damps is not None else 1 + 2 * max(0, np.sqrt((self.mu-1)/(self.dim+1)) - 1) + self.cs\n        self.ccov1 = ccov1 if ccov1 is not None else 2 / ((self.dim + 1.3)**2 + self.mu)\n        self.ccovmu = ccovmu if ccovmu is not None else 2 * (self.mu - 2 + 1/self.mu) / ((self.dim + 2)**2 + self.mu)\n        self.ccovmu = min(1-self.ccov1, self.ccovmu)\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.m = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.nevals = 0\n\n        while self.nevals < self.budget:\n            z = np.random.multivariate_normal(np.zeros(self.dim), self.C, size=self.pop_size)\n            x = self.m + self.sigma * z\n            x = np.clip(x, func.bounds.lb, func.bounds.ub)\n            \n            f = np.array([func(xi) for xi in x])\n            self.nevals += self.pop_size\n\n            idx = np.argsort(f)\n            x_sorted = x[idx]\n            f_sorted = f[idx]\n            \n            m_old = self.m.copy()\n            self.m = np.sum(self.weights[:, None] * x_sorted[:self.mu], axis=0)\n            \n            z_sorted = z[idx]\n            z_mean = np.sum(self.weights[:, None] * z_sorted[:self.mu], axis=0)\n            \n            self.ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs)) * z_mean\n            hsig = (np.linalg.norm(self.ps) / np.sqrt(1 - (1 - self.cs)**(2 * self.nevals / self.pop_size)) / self.chiN) < (1.4 + 2/(self.dim+1))\n            dhsig = (1-hsig) * self.cs * (2-self.cs)\n            \n            self.pc = (1 - 1) * self.pc + hsig * np.sqrt(1) * (self.m - m_old) / self.sigma\n            \n            artmp = (1 / self.sigma) * (x_sorted[:self.mu] - m_old)\n            self.C = (1-self.ccov1-self.ccovmu+self.ccov1*dhsig) * self.C + self.ccov1 * np.outer(self.pc, self.pc) \\\n                        + self.ccovmu * artmp.T @ np.diag(self.weights) @ artmp\n            \n            self.sigma *= np.exp((self.cs/self.damps) * (np.linalg.norm(self.ps)/self.chiN - 1))\n\n            if f_sorted[0] < self.f_opt:\n                self.f_opt = f_sorted[0]\n                self.x_opt = x_sorted[0]\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm CMAES scored 0.287 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.07708235699235921, 0.18357008373473493, 0.5453004434519237, 0.9896034851268826, 0.21005707288026576, 0.15698910309950043, 0.23965709800681545, 0.2998548397780554, 0.14812194706976756, 0.15573292878299239, 0.3371708388447816, 0.19069885491009164, 0.31647176626213847, 0.2187437670978637, 0.1766524999533643, 0.3324506807783245, 0.2762740014899656, 0.4070007935630756, 0.30834406436816075, 0.16526829766630868]}, "task_prompt": ""}
{"id": "7484b89f-9179-4daa-bcca-8b763dc37ff3", "fitness": 0.15340933854119312, "name": "SimpleCMAES", "description": "No description provided.", "code": "import numpy as np\n\nclass SimpleCMAES:\n    def __init__(self, budget=10000, dim=10, pop_size=None, sigma=0.5, mu_factor=0.25):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size if pop_size is not None else 4 + int(3 * np.log(dim))\n        self.sigma = sigma\n        self.mu = max(1, int(self.pop_size * mu_factor))\n        self.weights = np.log(self.pop_size + 1) - np.log(np.arange(1, self.pop_size + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.C = np.eye(dim)\n        self.m = np.zeros(dim)\n        self.pc = np.zeros(dim)\n        self.ps = np.zeros(dim)\n        self.chiN = np.sqrt(dim) * (1 - (1/(4*dim)) + 1/(21*dim**2))\n        self.cc = 4/(dim + 4)\n        self.cs = (self.cc + (budget/self.pop_size))/ (dim + (budget/self.pop_size))\n        self.damps = 1 + 2*max(0, np.sqrt((self.mu/self.pop_size) - 1)) + self.cs\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.nevals = 0\n        self.restart_criterion = 1e-12  # added restart criterion\n        self.restart_iterations = 50  # added restart iterations\n\n    def __call__(self, func):\n        iteration = 0\n        while self.nevals < self.budget:\n            iteration += 1\n            z = np.random.normal(0, 1, size=(self.pop_size, self.dim))\n            x = self.m + self.sigma * np.dot(z, np.linalg.cholesky(self.C).T)\n            x = np.clip(x, func.bounds.lb, func.bounds.ub)\n            fitness = np.array([func(xi) for xi in x])\n            self.nevals += self.pop_size\n\n            idx = np.argsort(fitness)\n            fitness = fitness[idx]\n            x = x[idx]\n\n            if fitness[0] < self.f_opt:\n                self.f_opt = fitness[0]\n                self.x_opt = x[0]\n            \n            m_old = self.m\n            self.m = np.sum(x[:self.mu].T * self.weights[:self.mu], axis=1)\n            \n            self.ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs)) * (np.linalg.inv(np.linalg.cholesky(self.C)) @ (self.m - m_old)) / self.sigma\n            \n            self.pc = (1 - self.cc) * self.pc + np.sqrt(self.cc * (2 - self.cc)) * (self.m - m_old) / self.sigma\n            \n            self.C = (1 - self.cc) * self.C + self.cc * (np.outer(self.pc, self.pc) + (self.cc/(2-self.cc)) * self.C) \n\n            self.sigma *= np.exp((self.cs/self.damps)*(np.linalg.norm(self.ps)/self.chiN - 1))\n            \n            # Restart mechanism\n            if np.min(np.diag(self.C)) < self.restart_criterion or iteration > self.restart_iterations:  \n                self.C = np.eye(self.dim)\n                self.pc = np.zeros(self.dim)\n                self.ps = np.zeros(self.dim)\n                self.sigma = 0.5  # Reset sigma\n                iteration = 0\n                \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm SimpleCMAES scored 0.153 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.05635390850015953, 0.15273406758727748, 0.27698510870927207, 0.1140480276684408, 0.11433374962563103, 0.1467510669855474, 0.1826724161536163, 0.144984539043571, 0.14508866226104622, 0.13956476952484098, 0.1566795820210366, 0.18466957360330638, 0.253664186150536, 0.10490235423492034, 0.1155498835623252, 0.2179609185399609, 0.13596563139001705, 0.1451656552456534, 0.13191164815268452, 0.1482010218640195]}, "task_prompt": ""}
{"id": "ff2fb850-dad6-4973-8c61-1816a2694fd4", "fitness": "-inf", "name": "CMAES", "description": "No description provided.", "code": "import numpy as np\nfrom scipy.linalg import sqrtm\n\nclass CMAES:\n    def __init__(self, budget=10000, dim=10, pop_size=None, sigma=0.5, cs=0.3, damps=None, c_cov_mean=None, c_cov_rank_one=None, c_cov_rank_mu=None, mu=None):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size if pop_size is not None else 4 + int(3 * np.log(self.dim))\n        self.mu = mu if mu is not None else self.pop_size // 2\n        self.sigma = sigma\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights /= np.sum(self.weights)\n        self.m = np.zeros(self.dim)\n        self.p_sigma = np.zeros(self.dim)\n        self.p_c = np.zeros(self.dim)\n        self.C = np.eye(self.dim)\n        self.cs = cs\n        self.damps = damps if damps is not None else 1 + 2 * max(0, np.sqrt((self.mu - 1)/(self.dim + 1)) - 1) + self.cs\n        self.c_cov_mean = c_cov_mean if c_cov_mean is not None else 1\n        self.c_cov_rank_one = c_cov_rank_one if c_cov_rank_one is not None else 2 / ((self.dim + 1.3)**2 + self.mu)\n        self.c_cov_rank_mu = c_cov_rank_mu if c_cov_rank_mu is not None else 2 * (self.mu - 1) / ((self.dim + 2)**2 + self.mu)\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        self.nevals = 0\n        while self.nevals < self.budget:\n            z = np.random.normal(0, 1, size=(self.pop_size, self.dim))\n            x = self.m + self.sigma * (sqrtm(self.C) @ z.T).T\n            x = np.clip(x, func.bounds.lb, func.bounds.ub)\n            f = np.array([func(xi) for xi in x])\n            self.nevals += self.pop_size\n\n            if np.min(f) < self.f_opt:\n                self.f_opt = np.min(f)\n                self.x_opt = x[np.argmin(f)]\n\n            idx = np.argsort(f)\n            x_mu = x[idx[:self.mu]]\n            z_mu = z[idx[:self.mu]]\n\n            y_w = np.sum(self.weights[:, None] * z_mu, axis=0)\n            m_new = np.sum(self.weights[:, None] * x_mu, axis=0)\n            \n            self.p_sigma = (1 - self.cs) * self.p_sigma + np.sqrt(self.cs * (2 - self.cs)) * y_w\n            self.p_c = (1 - self.c_cov_mean) * self.p_c + np.sqrt(self.c_cov_mean * (2 - self.c_cov_mean)) * np.sqrt(self.mu) * (m_new - self.m) / self.sigma\n            \n            self.C = (1 - self.c_cov_rank_one - self.c_cov_rank_mu) * self.C + \\\n                     self.c_cov_rank_one * np.outer(self.p_c, self.p_c) + \\\n                     self.c_cov_rank_mu * np.sum(self.weights[:, None, None] * np.array([np.outer(z_mu[i], z_mu[i]) for i in range(self.mu)]), axis=0)\n\n            self.sigma *= np.exp((self.cs / self.damps) * (np.linalg.norm(self.p_sigma) / np.sqrt(self.dim) - 1))\n            self.m = m_new\n            \n            if self.nevals >= self.budget:\n                break\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "In the code, line 29, in __call__, the following error occurred:\nNameError: name 'sqrtm' is not defined\nOn line: x = self.m + self.sigma * (sqrtm(self.C) @ z.T).T", "error": "In the code, line 29, in __call__, the following error occurred:\nNameError: name 'sqrtm' is not defined\nOn line: x = self.m + self.sigma * (sqrtm(self.C) @ z.T).T", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "ab9316fb-5169-4d77-87ca-ce3fb4584a01", "fitness": "-inf", "name": "CMAES", "description": "No description provided.", "code": "import numpy as np\n\nclass CMAES:\n    def __init__(self, budget=10000, dim=10, pop_size=None, sigma=0.5, cs=0.3, c1=None, cmu=None):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size if pop_size is not None else 4 + int(3 * np.log(dim))\n        self.sigma = sigma\n        self.cs = cs\n        self.c1 = c1 if c1 is not None else 2 / ((dim + 1.3)**2 + 1)\n        self.cmu = cmu if cmu is not None else min(1 - self.c1, 2 * (self.pop_size - 2 + 1 / self.pop_size) / ((dim + 2)**2 + 1))\n        self.mu = (func.bounds.ub + func.bounds.lb) / 2 * np.ones(dim)\n        self.C = np.eye(dim)\n        self.pc = np.zeros(dim)\n        self.ps = np.zeros(dim)\n        self.chiN = np.sqrt(dim) * (1 - 1/(4*dim) + 1/(21*dim**2))\n        self.weights = np.log(self.pop_size + 1/2) - np.log(np.arange(1, self.pop_size + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mueff = 1 / np.sum(self.weights**2)\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        while self.budget > 0:\n            z = np.random.normal(0, 1, size=(self.pop_size, self.dim))\n            x = self.mu + self.sigma * np.dot(z, np.linalg.cholesky(self.C).T)\n            x = np.clip(x, func.bounds.lb, func.bounds.ub)\n            \n            f = np.array([func(xi) for xi in x])\n            self.budget -= self.pop_size\n            if self.budget <= 0:\n              f = f[:self.pop_size+self.budget]\n              x = x[:self.pop_size+self.budget]\n              self.pop_size = len(x)\n\n            idx = np.argsort(f)\n            x = x[idx]\n            f = f[idx]\n            \n            if f[0] < self.f_opt:\n                self.f_opt = f[0]\n                self.x_opt = x[0]\n            \n            z = z[idx]\n            z = z[:self.pop_size]\n            x = x[:self.pop_size]\n            f = f[:self.pop_size]\n\n            delta_mu = np.sum(self.weights[:, None] * z[:self.pop_size], axis=0)\n            self.pc = (1 - self.cs) * self.pc + np.sqrt(self.cs * (2 - self.cs) * self.mueff) * delta_mu\n            self.ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs) * self.mueff) * np.linalg.inv(np.linalg.cholesky(self.C)) @ delta_mu\n            \n            self.C = (1 - self.c1 - self.cmu) * self.C + self.c1 * (self.pc[:, None] @ self.pc[None, :])\n            self.C += self.cmu * np.sum(self.weights[:, None, None] * (z[:, :, None] @ z[:, None, :]), axis=0)\n            self.mu = self.mu + self.sigma * np.sum(self.weights[:, None] * z[:self.pop_size], axis=0)\n            self.sigma = self.sigma * np.exp((self.cs / self.dim) * (np.linalg.norm(self.ps) / self.chiN - 1))\n\n            self.C = np.triu(self.C) + np.triu(self.C, 1).T\n            \n            try:\n                np.linalg.cholesky(self.C)\n            except np.linalg.LinAlgError:\n                self.C = self.C + 1e-6 * np.eye(self.dim)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "In the code, line 12, in __init__, the following error occurred:\nNameError: name 'func' is not defined\nOn line: self.mu = (func.bounds.ub + func.bounds.lb) / 2 * np.ones(dim)", "error": "In the code, line 12, in __init__, the following error occurred:\nNameError: name 'func' is not defined\nOn line: self.mu = (func.bounds.ub + func.bounds.lb) / 2 * np.ones(dim)", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "571bf2a6-48b2-4813-9efe-30be5ce3f48a", "fitness": 0.42184053768424573, "name": "ShrinkingPSO", "description": "No description provided.", "code": "import numpy as np\n\nclass ShrinkingPSO:\n    def __init__(self, budget=10000, dim=10, pop_size=20, inertia=0.7, cognitive_coeff=1.5, social_coeff=1.5, shrinkage_rate=0.95):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.inertia = inertia\n        self.cognitive_coeff = cognitive_coeff\n        self.social_coeff = social_coeff\n        self.shrinkage_rate = shrinkage_rate\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.personal_best_positions = self.population.copy()\n        self.personal_best_fitness = self.fitness.copy()\n        self.global_best_index = np.argmin(self.fitness)\n        self.global_best_position = self.population[self.global_best_index].copy()\n\n        self.eval_count = self.pop_size\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n\n        while self.eval_count < self.budget:\n            for i in range(self.pop_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                # Update velocity\n                self.velocities[i] = (self.inertia * self.velocities[i] +\n                                      self.cognitive_coeff * np.random.rand() * (self.personal_best_positions[i] - self.population[i]) +\n                                      self.social_coeff * np.random.rand() * (self.global_best_position - self.population[i]))\n\n                # Update position\n                new_position = self.population[i] + self.velocities[i]\n                new_position = np.clip(new_position, lb, ub)\n                \n                # Evaluate new position\n                f_new = func(new_position)\n                self.eval_count += 1\n                \n\n                # Update personal best\n                if f_new < self.personal_best_fitness[i]:\n                    self.personal_best_fitness[i] = f_new\n                    self.personal_best_positions[i] = new_position.copy()\n\n                    # Update global best\n                    if f_new < self.f_opt:\n                        self.f_opt = f_new\n                        self.x_opt = new_position.copy()\n                        self.global_best_position = new_position.copy()\n                        self.global_best_index = i\n\n                self.population[i] = new_position\n\n            # Shrink the search space around the global best\n            center = self.global_best_position\n            range_val = (ub - lb) / 2.0 * self.shrinkage_rate\n            lb = np.maximum(func.bounds.lb, center - range_val)\n            ub = np.minimum(func.bounds.ub, center + range_val)\n            self.population = np.clip(self.population, lb, ub)\n            \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm ShrinkingPSO scored 0.422 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.2077449693770197, 0.23395391177106617, 0.7320010491196622, 0.17267758088012086, 0.24623544592219626, 0.2212201755749552, 0.2982929773229399, 0.35155073802439785, 0.8950853194515672, 0.18321917402711774, 0.8526154916146794, 0.9756789738918706, 0.2513289945876631, 0.2439756922892825, 0.6926965445163396, 0.34730184583766, 0.32429273382246515, 0.5311529218264337, 0.22775711694482248, 0.4480290968826549]}, "task_prompt": ""}
{"id": "48ad7022-6055-4982-8188-5d213d7069c7", "fitness": 0.4046018945117579, "name": "GaussianAdaptation", "description": "No description provided.", "code": "import numpy as np\n\nclass GaussianAdaptation:\n    def __init__(self, budget=10000, dim=10, pop_size=50, initial_step_size=1.0, success_rate_memory=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.step_size = initial_step_size\n        self.success_rate_memory = success_rate_memory\n        self.success_history = []\n        self.restart_trigger = 100\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.budget -= self.pop_size\n\n        if np.min(self.fitness) < self.f_opt:\n            self.f_opt = np.min(self.fitness)\n            self.x_opt = self.pop[np.argmin(self.fitness)]\n\n        iteration = 0\n        stagnation_counter = 0\n        best_fitness_history = [self.f_opt]\n\n        while self.budget > 0:\n            iteration += 1\n            successful_mutations = 0\n            for i in range(self.pop_size):\n                if self.budget <= 0:\n                    break\n\n                mutant = np.clip(self.pop[i] + self.step_size * np.random.normal(0, 1, size=self.dim), func.bounds.lb, func.bounds.ub)\n\n                f_mutant = func(mutant)\n                self.budget -= 1\n\n                if f_mutant < self.fitness[i]:\n                    self.pop[i] = mutant\n                    self.fitness[i] = f_mutant\n                    successful_mutations += 1\n                    if f_mutant < self.f_opt:\n                        self.f_opt = f_mutant\n                        self.x_opt = mutant\n\n            success_rate = successful_mutations / self.pop_size if self.pop_size > 0 else 0.0\n            self.success_history.append(success_rate)\n            if len(self.success_history) > self.success_rate_memory:\n                self.success_history.pop(0)\n\n            avg_success_rate = np.mean(self.success_history) if self.success_history else 0.0\n            if avg_success_rate > 0.2:\n                self.step_size *= 1.1\n            else:\n                self.step_size *= 0.9\n\n            if self.f_opt <= min(best_fitness_history):\n                stagnation_counter = 0\n            else:\n                stagnation_counter += 1\n\n            best_fitness_history.append(self.f_opt)\n            if len(best_fitness_history) > 100:\n                best_fitness_history.pop(0)\n            \n            if stagnation_counter > self.restart_trigger:\n                self.pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                self.fitness = np.array([func(x) for x in self.pop])\n                self.budget -= self.pop_size\n                if np.min(self.fitness) < self.f_opt:\n                    self.f_opt = np.min(self.fitness)\n                    self.x_opt = self.pop[np.argmin(self.fitness)]\n                stagnation_counter = 0\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm GaussianAdaptation scored 0.405 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.1669660994850627, 0.232647339828293, 0.4270557644728017, 0.6299373506552978, 0.32787165647947847, 0.43066287524373625, 0.29865080657995535, 0.4009285139013876, 0.3209830720493998, 0.1962609511368386, 0.5196439007791289, 0.9994838809465191, 0.27599707831923026, 0.30118937118237454, 0.7128465324522453, 0.37326418441363585, 0.27740149146738546, 0.482962929156054, 0.2251070311562493, 0.4921770605300857]}, "task_prompt": ""}
{"id": "1b87ca82-dc29-4f82-855a-290b9a12332e", "fitness": "-inf", "name": "BayesianOptimization", "description": "No description provided.", "code": "import numpy as np\nfrom sklearn.gaussian_process import GaussianProcessRegressor\nfrom sklearn.gaussian_process.kernels import RBF, ConstantKernel\nfrom scipy.stats import norm\nfrom scipy.optimize import minimize\n\nclass BayesianOptimization:\n    def __init__(self, budget=10000, dim=10, n_initial_points=10, n_iter=10):\n        self.budget = budget\n        self.dim = dim\n        self.n_initial_points = min(n_initial_points, budget // 2)\n        self.n_iter = n_iter\n        self.kernel = ConstantKernel(1.0, constant_value_bounds=\"fixed\") * RBF(length_scale=1.0, length_scale_bounds=\"fixed\")\n        self.gp = GaussianProcessRegressor(kernel=self.kernel, n_restarts_optimizer=5)\n        self.X = None\n        self.y = None\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.nevals = 0\n\n    def acquisition_function(self, x, gp, bounds):\n        x = x.reshape(-1, self.dim)\n        mu, sigma = gp.predict(x, return_std=True)\n        \n        # Ensure sigma is not zero to avoid division by zero\n        sigma = np.maximum(sigma, 1e-9)\n        \n        best = self.y.min()\n        z = (best - mu) / sigma\n        return - (mu - best) - sigma * norm.pdf(z)\n    \n    def propose_location(self, gp, bounds, n_restarts=25):\n        \"\"\"Proposes the next sampling point by optimizing the acquisition function.\"\"\"\n        \n        def min_obj(x):\n            \"\"\"Returns the value of the acquisition function at x.\"\"\"\n            return self.acquisition_function(x.reshape(-1, self.dim), gp, bounds)\n\n        # Start with bounds on x\n        cons = ({'type': 'ineq', 'fun': lambda x: x - bounds.lb},\n                {'type': 'ineq', 'fun': lambda x: bounds.ub - x})\n\n        # Run local search n_restarts times\n        x_starts = []\n        f_values = []\n        for x0 in np.random.uniform(bounds.lb, bounds.ub, size=(n_restarts, self.dim)):\n            res = minimize(min_obj, x0=x0, bounds=bounds, method='L-BFGS-B', constraints=cons)\n            x_starts.append(res.x.reshape(-1, self.dim))\n            f_values.append(res.fun)\n\n        # Pick the best among all local search results\n        ind = np.argmin(f_values)\n        return x_starts[ind].reshape(-1, self.dim)\n    \n    def __call__(self, func):\n        # Initial exploration\n        self.X = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.n_initial_points, self.dim))\n        self.y = np.array([func(x) for x in self.X])\n        self.nevals = self.n_initial_points\n\n        if np.min(self.y) < self.f_opt:\n            self.f_opt = np.min(self.y)\n            self.x_opt = self.X[np.argmin(self.y)]\n\n        # Bayesian optimization loop\n        for i in range(self.n_iter):\n            if self.nevals >= self.budget:\n                break\n\n            self.gp.fit(self.X, self.y)\n\n            # Propose the next location\n            x_new = self.propose_location(self.gp, func.bounds)\n\n            # Evaluate the function at the new location\n            f_new = func(x_new)\n            self.nevals += 1\n\n            # Update data\n            self.X = np.vstack((self.X, x_new))\n            self.y = np.append(self.y, f_new)\n\n            # Update best\n            if f_new < self.f_opt:\n                self.f_opt = f_new\n                self.x_opt = x_new\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "In the code, line 13, in __init__, the following error occurred:\nNameError: name 'ConstantKernel' is not defined\nOn line: self.kernel = ConstantKernel(1.0, constant_value_bounds=\"fixed\") * RBF(length_scale=1.0, length_scale_bounds=\"fixed\")", "error": "In the code, line 13, in __init__, the following error occurred:\nNameError: name 'ConstantKernel' is not defined\nOn line: self.kernel = ConstantKernel(1.0, constant_value_bounds=\"fixed\") * RBF(length_scale=1.0, length_scale_bounds=\"fixed\")", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "cbc4e04e-ac7f-4bca-8b71-439ca4cdeb24", "fitness": 0.5711726485943152, "name": "CMAES", "description": "No description provided.", "code": "import numpy as np\n\nclass CMAES:\n    def __init__(self, budget=10000, dim=10, pop_size=None, sigma=0.5, restart_factor=3):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size if pop_size else 4 + int(3 * np.log(dim))\n        self.sigma = sigma\n        self.restart_factor = restart_factor\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.chiN = self.dim**0.5 * (1 - 1/(4*self.dim) + 1/(21*self.dim**2))\n        self.count_evals = 0\n        self.mu = self.pop_size // 2\n        self.weights = np.log(self.mu+1/2) - np.log(np.arange(1, self.mu+1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        self.cc = (4 + self.mueff/self.dim) / (self.dim + 4 + 2*self.mueff/self.dim)\n        self.cs = (self.mueff + 2) / (self.dim + self.mueff + 5)\n        self.c1 = 2 / ((self.dim + 1.3)**2 + self.mueff)\n        self.cmu = min(1 - self.c1, 2 * (self.mueff - 2 + 1/self.mueff) / ((self.dim + 2.0)**2 + self.mueff))\n        self.damps = 1 + 2*max(0, np.sqrt((self.mueff-1)/(self.dim+1)) - 1) + self.cs\n        best_fitness_history = []\n        stagnation_counter = 0\n\n        while self.count_evals < self.budget:\n            z = np.random.multivariate_normal(np.zeros(self.dim), self.C, self.pop_size)\n            samples = self.mean + self.sigma * z\n            samples = np.clip(samples, func.bounds.lb, func.bounds.ub)\n\n            fitness = np.array([func(x) for x in samples])\n            self.count_evals += self.pop_size\n            if self.count_evals > self.budget:\n                fitness = fitness[:self.pop_size - (self.count_evals - self.budget)]\n                samples = samples[:self.pop_size - (self.count_evals - self.budget)]\n\n            idx = np.argsort(fitness)\n            fitness = fitness[idx]\n            samples = samples[idx]\n\n            if fitness[0] < self.f_opt:\n                self.f_opt = fitness[0]\n                self.x_opt = samples[0]\n\n            delta_mean = np.sum(self.weights[:, None] * z[idx[:self.mu]], axis=0)\n            self.ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs) * self.mueff) * delta_mean\n            hsig = (np.linalg.norm(self.ps)/np.sqrt(1-(1-self.cs)**(2*(self.budget/self.pop_size)))/self.chiN < 1.4 + 2/(self.dim+1))\n            self.pc = (1 - self.cc) * self.pc + hsig * np.sqrt(self.cc * (2 - self.cc) * self.mueff) * self.sigma**-1 * (self.mean - samples[0])\n            self.C = (1 - self.c1 - self.cmu) * self.C + self.c1 * (self.pc[:, None] * self.pc) + self.cmu * np.sum(self.weights[:, None, None] * (z[idx[:self.mu], :, None] * z[idx[:self.mu], None, :]), axis=0)\n\n            self.sigma *= np.exp((self.cs/self.damps) * (np.linalg.norm(self.ps)/self.chiN - 1))\n            self.mean = np.sum(self.weights[:, None] * samples[:self.mu], axis=0)\n\n            if np.any(np.linalg.eigvalsh(self.C) < 0):\n                self.C = np.eye(self.dim)\n\n            if self.f_opt <= min(best_fitness_history + [np.inf]):\n              stagnation_counter = 0\n            else:\n                stagnation_counter +=1\n\n            best_fitness_history.append(self.f_opt)\n            if len(best_fitness_history) > 50:\n              best_fitness_history.pop(0)\n\n            if stagnation_counter > 25:\n              self.mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n              self.C = np.eye(self.dim)\n              self.pc = np.zeros(self.dim)\n              self.ps = np.zeros(self.dim)\n              stagnation_counter = 0\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm CMAES scored 0.571 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.19850899293926694, 0.1854033385984286, 0.9271501222226974, 0.9116212012327463, 0.36118695271915235, 0.33109188871417405, 0.3227121399348418, 0.4410988970085459, 0.25044214646299723, 0.20102435188346202, 0.7558995098225068, 0.9956066709467535, 0.8760288248079282, 0.8929852011105932, 0.9464008410198018, 0.93980992074781, 0.2997542649786661, 0.9132067216280247, 0.19546386554727513, 0.4780571195606287]}, "task_prompt": ""}
{"id": "e5a1813a-a514-4380-952c-e03450eb79be", "fitness": 0.2963564116623299, "name": "AdaptiveSimulatedAnnealing", "description": "No description provided.", "code": "import numpy as np\n\nclass AdaptiveSimulatedAnnealing:\n    def __init__(self, budget=10000, dim=10, initial_temp=100.0, cooling_rate=0.95, perturbation_scale=1.0):\n        self.budget = budget\n        self.dim = dim\n        self.initial_temp = initial_temp\n        self.cooling_rate = cooling_rate\n        self.perturbation_scale = perturbation_scale\n        self.lb = -5.0\n        self.ub = 5.0\n\n    def __call__(self, func):\n        self.x = np.random.uniform(self.lb, self.ub, size=self.dim)\n        self.f_current = func(self.x)\n        self.f_opt = self.f_current\n        self.x_opt = self.x.copy()\n        self.temp = self.initial_temp\n        evals = 1\n        acceptance_rate = 0.0\n\n        while evals < self.budget:\n            # Perturbation using Cauchy distribution\n            x_new = self.x + self.perturbation_scale * self.temp * np.random.standard_cauchy(size=self.dim)\n            x_new = np.clip(x_new, self.lb, self.ub)\n\n            f_new = func(x_new)\n            evals += 1\n\n            delta_f = f_new - self.f_current\n\n            if delta_f < 0 or np.random.rand() < np.exp(-delta_f / self.temp):\n                self.x = x_new\n                self.f_current = f_new\n                acceptance_rate += 1.0\n\n                if f_new < self.f_opt:\n                    self.f_opt = f_new\n                    self.x_opt = x_new.copy()\n\n            # Adaptive temperature adjustment\n            if evals % 100 == 0:\n                acceptance_ratio = acceptance_rate / 100.0\n                if acceptance_ratio > 0.5:\n                    self.temp *= 0.95\n                elif acceptance_ratio < 0.1:\n                    self.temp *= 1.05\n                acceptance_rate = 0.0\n            else:\n                self.temp *= self.cooling_rate\n\n            if evals >= self.budget:\n                break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveSimulatedAnnealing scored 0.296 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.11830932065396427, 0.17049702168165737, 0.22309763077920308, 0.37647597895039664, 0.27238809814978526, 0.2704260579693817, 0.2382146343401249, 0.22330820023349784, 0.17064649062081316, 0.20734576456397946, 0.3686874356563413, 0.9989619614130452, 0.20487440591730433, 0.20638883765012994, 0.5631093219511727, 0.2617945709523547, 0.2051740472652368, 0.22538424957782444, 0.16088821852785817, 0.46115598639252764]}, "task_prompt": ""}
{"id": "b91e7de3-c2d9-4b58-a4fd-33e55b55990a", "fitness": 0.4906082258987713, "name": "SelfAdaptiveDE", "description": "No description provided.", "code": "import numpy as np\n\nclass SelfAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = 0.5\n        self.CR = 0.9\n        self.F_history = []\n        self.CR_history = []\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.nevals = self.pop_size\n\n        while self.nevals < self.budget:\n            for i in range(self.pop_size):\n                if self.nevals >= self.budget:\n                    break\n\n                # Mutation using Cauchy distribution\n                a, b, c = np.random.choice(self.pop_size, 3, replace=False)\n                mutant = self.pop[a] + self.F * (self.pop[b] - self.pop[c]) * np.random.standard_cauchy(size=self.dim)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                trial = np.copy(self.pop[i])\n                jrand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR or j == jrand:\n                        trial[j] = mutant[j]\n\n                # Selection\n                f_trial = func(trial)\n                self.nevals += 1\n\n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.pop[i] = trial\n                    \n                    # Adapt F and CR\n                    if len(self.F_history) > 10:\n                        self.F = np.mean(self.F_history[-10:])\n                        self.CR = np.mean(self.CR_history[-10:])\n\n                    self.F = np.clip(self.F + 0.1 * np.random.normal(), 0.1, 1.0)\n                    self.CR = np.clip(self.CR + 0.1 * np.random.normal(), 0.1, 1.0)\n                    self.F_history.append(self.F)\n                    self.CR_history.append(self.CR)\n\n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = trial\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm SelfAdaptiveDE scored 0.491 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.17041873070532987, 0.4180107243158794, 0.3897880703276102, 0.7034141343642011, 0.5615310118836958, 0.48271812651843515, 0.3031492921297174, 0.4133944472873895, 0.46880645545237043, 0.24860244359550165, 0.6566607551154529, 0.9904953925407449, 0.2660376261983811, 0.5458151650081493, 0.7376454405146844, 0.6331671337337752, 0.4056050156672253, 0.7136115750698044, 0.19408118504691352, 0.509211792500166]}, "task_prompt": ""}
{"id": "881a159b-b162-4067-9762-a4cc75c6b01c", "fitness": "-inf", "name": "CMAES", "description": "No description provided.", "code": "import numpy as np\n\nclass CMAES:\n    def __init__(self, budget=10000, dim=10, pop_size=None, sigma=0.5, cs=0.3, damps=None, ccov1=None, ccovmu=None, mu=None):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size if pop_size is not None else 4 + int(3 * np.log(self.dim))\n        self.sigma = sigma\n        self.mu = mu if mu is not None else self.pop_size // 2\n\n        self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n        self.weights /= np.sum(self.weights)\n\n        self.m = None\n        self.C = None\n        self.pc = None\n        self.ps = None\n        self.chiN = None\n\n        self.cs = cs\n        self.damps = damps if damps is not None else 1 + 2 * max(0, np.sqrt((self.mu - 1) / (self.dim + 1)) - 1) + self.cs\n        self.ccov1 = ccov1 if ccov1 is not None else 2 / ((self.dim + 1.3)**2 + self.mu)\n        self.ccovmu = ccovmu if ccovmu is not None else 2 * (self.mu - 1 + 1e-8) / ((self.dim + 2)**2 + self.mu)\n        self.ccovmu = min(1 - self.ccov1, self.ccovmu)\n\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.nevals = 0\n\n    def __call__(self, func):\n        self.m = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.chiN = self.dim**0.5 * (1 - 1 / (4 * self.dim) + 1 / (21 * self.dim**2))\n\n        while self.nevals < self.budget:\n            z = np.random.normal(size=(self.dim, self.pop_size))\n            y = np.linalg.cholesky(self.C) @ z\n            x = self.m + self.sigma * y\n            x = np.clip(x, func.bounds.lb, func.bounds.ub)\n\n            fitness = np.array([func(xi) for xi in x.T])\n            self.nevals += self.pop_size\n\n            if self.nevals > self.budget:\n                fitness = fitness[:self.budget - (self.nevals - self.pop_size)]\n                x = x[:, :self.budget - (self.nevals - self.pop_size)]\n                self.nevals = self.budget\n\n            idx = np.argsort(fitness)\n            x = x[:, idx]\n            fitness = fitness[idx]\n\n            if fitness[0] < self.f_opt:\n                self.f_opt = fitness[0]\n                self.x_opt = x[:, 0]\n\n            m_old = self.m.copy()\n            self.m = x[:, :self.mu] @ self.weights\n\n            y_mean = self.m - m_old\n            self.ps = (1 - self.cs) * self.ps + (np.sqrt(self.cs * (2 - self.cs) * self.mu) / self.sigma) * (np.linalg.inv(np.linalg.cholesky(self.C)) @ y_mean)\n            hsig = (np.linalg.norm(self.ps) / np.sqrt(1 - (1 - self.cs)**(2 * self.nevals / self.pop_size)) / self.chiN < 1.4 + 2 / (self.dim + 1))\n\n            self.pc = (1 - self.ccov1) * self.pc + hsig * np.sqrt(self.ccov1 * (2 - self.ccov1) * self.mu) * y_mean / self.sigma\n\n            self.C = (1 - self.ccov1 - self.ccovmu) * self.C + self.ccov1 * np.outer(self.pc, self.pc) + self.ccovmu * (x[:, :self.mu] - m_old) @ np.diag(self.weights) @ (x[:, :self.mu] - m_old).T / self.sigma**2\n\n            self.sigma *= np.exp((self.cs / self.damps) * (np.linalg.norm(self.ps) / self.chiN - 1))\n\n            self.C = np.triu(self.C) + np.triu(self.C, 1).T\n            self.C = np.linalg.cholesky(self.C @ self.C.T) # Repair C if it's close to singular\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "In the code, line 40, in __call__, the following error occurred:\nValueError: operands could not be broadcast together with shapes (2,) (2,6) \nOn line: x = self.m + self.sigma * y", "error": "In the code, line 40, in __call__, the following error occurred:\nValueError: operands could not be broadcast together with shapes (2,) (2,6) \nOn line: x = self.m + self.sigma * y", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "5528ad8f-300f-4b91-8fd4-076fcbd5d215", "fitness": 0.4194380932049603, "name": "BudgetAwareCMAES", "description": "No description provided.", "code": "import numpy as np\n\nclass BudgetAwareCMAES:\n    def __init__(self, budget=10000, dim=10, pop_size=None, initial_step_size=1.0):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size if pop_size is not None else 4 + int(3 * np.log(dim))\n        self.mu = self.pop_size // 2\n        self.initial_step_size = initial_step_size\n        self.c_sigma = (self.mu / (self.dim + self.mu))**0.5\n        self.d_sigma = 1 + 2 * max(0, ((self.mu - 1) / (self.dim + 1)) - 1) + self.c_sigma\n        self.c_cov = (1 / (self.mu * (self.dim + 1.3)**2 + 0.1 * (budget/dim) ) )\n        self.c_m = 1\n        self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n        self.weights /= np.sum(self.weights)\n        self.m = None\n        self.sigma = None\n        self.C = None\n        self.P_sigma = None\n        self.eigen_decomposition = None\n        self.x_opt = None\n        self.f_opt = np.inf\n\n    def __call__(self, func):\n        self.nevals = 0\n        self.m = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        self.sigma = self.initial_step_size\n        self.C = np.eye(self.dim)\n        self.P_sigma = np.zeros(self.dim)\n\n        while self.nevals < self.budget:\n            z = np.random.multivariate_normal(np.zeros(self.dim), self.C, size=self.pop_size)\n            x = self.m + self.sigma * z\n            x = np.clip(x, func.bounds.lb, func.bounds.ub)\n            f = np.array([func(xi) for xi in x])\n            self.nevals += self.pop_size\n            \n            if self.nevals > self.budget:\n                f = f[:self.pop_size - (self.nevals-self.budget)]\n                x = x[:self.pop_size - (self.nevals-self.budget)]\n\n            idx = np.argsort(f)\n            x = x[idx]\n            f = f[idx]\n            \n            if f[0] < self.f_opt:\n                self.f_opt = f[0]\n                self.x_opt = x[0]\n\n            m_old = self.m\n            self.m = np.sum(self.weights[:, None] * x[:self.mu], axis=0)\n\n            z_m = (self.m - m_old) / self.sigma\n\n            self.P_sigma = (1 - self.c_sigma) * self.P_sigma + np.sqrt(self.c_sigma * (2 - self.c_sigma)) * z_m\n\n            norm_P_sigma = np.linalg.norm(self.P_sigma)\n\n            self.sigma *= np.exp((self.c_sigma / self.d_sigma) * (norm_P_sigma / (np.sqrt(self.dim)) - 1))\n\n            if self.sigma < 1e-10:\n                self.sigma = 1e-10\n\n            C_tmp = (x[:self.mu] - m_old).T @ (x[:self.mu] - m_old)\n            \n            self.C = (1 - self.c_cov) * self.C + self.c_cov * C_tmp / (self.sigma**2 * self.mu)\n            \n            if np.linalg.det(self.C) <= 0:\n                self.C = np.eye(self.dim) # Restart Covariance Matrix when it is no longer positive definite\n                self.P_sigma = np.zeros(self.dim)\n                self.sigma = self.initial_step_size # Reset step-size\n            \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm BudgetAwareCMAES scored 0.419 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.135415071919739, 0.20302879380951488, 0.30752365272075166, 0.19458638721958, 0.27786470928951024, 0.977308332635779, 0.2614652705966496, 0.5194983689172903, 0.8238426409747857, 0.15651087030744582, 0.17702565072946475, 0.9970002468724755, 0.22505151239389376, 0.3636800096235584, 0.6433029007549498, 0.24438510435229932, 0.27859707493629626, 0.9790985694002898, 0.13850001255259414, 0.48507668409233684]}, "task_prompt": ""}
{"id": "83ede628-a268-4b88-8ff8-fce1f9bb6600", "fitness": "-inf", "name": "HybridDE", "description": "No description provided.", "code": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridDE:\n    def __init__(self, budget=10000, dim=10, pop_size=20, cr=0.9, f=0.5, local_search_interval=500):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.cr = cr\n        self.f = f\n        self.lb = -5.0\n        self.ub = 5.0\n        self.local_search_interval = local_search_interval\n\n    def __call__(self, func):\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n        \n        evals = self.pop_size\n\n        while evals < self.budget:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n                mutant = a + self.f * (b - c)\n                mutant = np.clip(mutant, self.lb, self.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.cr\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, self.population[i])\n\n                # Selection\n                f = func(trial)\n                evals += 1\n\n                if f < self.fitness[i]:\n                    self.fitness[i] = f\n                    self.population[i] = trial\n\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = trial\n\n            # Local Search\n            if evals % self.local_search_interval == 0:\n                best_index = np.argmin(self.fitness)\n                x_best = self.population[best_index].copy()\n\n                def local_func(x):\n                    return func(x)\n                \n                local_result = minimize(local_func, x_best, method='Nelder-Mead', bounds=[(self.lb, self.ub)] * self.dim, options={'maxfev': self.local_search_interval // 2})\n                \n                if local_result.fun < self.f_opt:\n                    self.f_opt = local_result.fun\n                    self.x_opt = local_result.x\n                    self.fitness[best_index] = local_result.fun\n                    self.population[best_index] = local_result.x\n                evals += local_result.nfev\n\n                \n            if evals >= self.budget:\n                break\n                    \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "In the code, line 57, in __call__, the following error occurred:\nNameError: name 'minimize' is not defined\nOn line: local_result = minimize(local_func, x_best, method='Nelder-Mead', bounds=[(self.lb, self.ub)] * self.dim, options={'maxfev': self.local_search_interval // 2})", "error": "In the code, line 57, in __call__, the following error occurred:\nNameError: name 'minimize' is not defined\nOn line: local_result = minimize(local_func, x_best, method='Nelder-Mead', bounds=[(self.lb, self.ub)] * self.dim, options={'maxfev': self.local_search_interval // 2})", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "83914ffe-2552-4f94-a0c2-2c89da9ba5a7", "fitness": 0.15530346147537138, "name": "AdaptiveSimulatedAnnealing", "description": "No description provided.", "code": "import numpy as np\n\nclass AdaptiveSimulatedAnnealing:\n    def __init__(self, budget=10000, dim=10, initial_temp=100.0, cooling_rate=0.99, temp_adjust_freq=50):\n        self.budget = budget\n        self.dim = dim\n        self.initial_temp = initial_temp\n        self.cooling_rate = cooling_rate\n        self.temp_adjust_freq = temp_adjust_freq\n        self.lb = -5.0\n        self.ub = 5.0\n        self.acceptance_history = []\n\n    def __call__(self, func):\n        self.x_opt = np.random.uniform(self.lb, self.ub, size=self.dim)\n        self.f_opt = func(self.x_opt)\n        self.current_x = self.x_opt.copy()\n        self.current_f = self.f_opt\n        self.temperature = self.initial_temp\n        evals = 1\n\n        while evals < self.budget:\n            # Generate a neighbor\n            neighbor_x = self.current_x + np.random.normal(0, 0.1, size=self.dim)\n            neighbor_x = np.clip(neighbor_x, self.lb, self.ub)\n            neighbor_f = func(neighbor_x)\n            evals += 1\n\n            # Acceptance probability\n            delta_f = neighbor_f - self.current_f\n            if delta_f < 0 or np.random.rand() < np.exp(-delta_f / self.temperature):\n                self.current_x = neighbor_x\n                self.current_f = neighbor_f\n                self.acceptance_history.append(1)\n                if neighbor_f < self.f_opt:\n                    self.f_opt = neighbor_f\n                    self.x_opt = neighbor_x\n            else:\n                 self.acceptance_history.append(0)\n            # Temperature update\n            if evals % self.temp_adjust_freq == 0:\n                acceptance_rate = np.mean(self.acceptance_history[-self.temp_adjust_freq:]) if len(self.acceptance_history) >= self.temp_adjust_freq else np.mean(self.acceptance_history) if len(self.acceptance_history)>0 else 0.5\n                if acceptance_rate > 0.7:\n                    self.temperature *= 1.1\n                elif acceptance_rate < 0.3:\n                    self.temperature *= 0.9\n                else:\n                    self.temperature *= self.cooling_rate\n                self.temperature = max(self.temperature, 1e-6)\n                \n\n            if evals >= self.budget:\n                break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveSimulatedAnnealing scored 0.155 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.05711980868476507, 0.05951786273775184, 0.21345805547586538, 0.20890904259808962, 0.09102470608874869, 0.15414162196304182, 0.15349727437215954, 0.11278618577566957, 0.13000540407176397, 0.1423236355704881, 0.12307459018442335, 0.1879106993032016, 0.22861086319031299, 0.14736492429995707, 0.142137036388707, 0.1857402144420569, 0.18632288285495546, 0.15504630269467756, 0.11732055605011715, 0.309757562760675]}, "task_prompt": ""}
{"id": "de6e7afc-75a2-4a88-a289-c68c34e4a0a2", "fitness": 0.7179200104079493, "name": "SelfAdaptiveDE", "description": "No description provided.", "code": "import numpy as np\n\nclass SelfAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=20):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.F = 0.5\n        self.Cr = 0.9\n        self.archive = []\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.budget -= self.pop_size\n\n        if np.min(self.fitness) < self.f_opt:\n            self.f_opt = np.min(self.fitness)\n            self.x_opt = self.pop[np.argmin(self.fitness)]\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                if self.budget <= 0:\n                    break\n                # Parameter Adaptation\n                self.F = np.random.normal(0.5, 0.33)\n                self.F = np.clip(self.F, 0.1, 1.0)\n                self.Cr = np.random.normal(0.9, 0.1)\n                self.Cr = np.clip(self.Cr, 0.1, 1.0)\n\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                a, b, c = self.pop[idxs]\n\n                # Use archive with a small probability\n                if len(self.archive) > 0 and np.random.rand() < 0.1:\n                    arc_idx = np.random.randint(len(self.archive))\n                    c = self.archive[arc_idx]\n\n                mutant = np.clip(a + self.F * (b - c), func.bounds.lb, func.bounds.ub)\n                \n                cross_mask = np.random.rand(self.dim) < self.Cr\n                trial = np.copy(self.pop[i])\n                trial[cross_mask] = mutant[cross_mask]\n\n                f_trial = func(trial)\n                self.budget -= 1\n\n                if f_trial < self.fitness[i]:\n                    # Update population\n                    self.pop[i] = trial\n                    self.fitness[i] = f_trial\n\n                    # Update archive\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(self.pop[i])\n                    else:\n                        idx_to_replace = np.random.randint(self.archive_size)\n                        self.archive[idx_to_replace] = self.pop[i]\n                    \n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm SelfAdaptiveDE scored 0.718 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.431867302310471, 0.6919280025806899, 0.6618027867639824, 0.8654279843635176, 0.7555530274765787, 0.7899706705704405, 0.6330242562471199, 0.6520964106559115, 0.7424749419946806, 0.7114258633759685, 0.8580658564187021, 0.9906092630164499, 0.7079736813974891, 0.7147523872902181, 0.9086695497096142, 0.7788930272193842, 0.6262293300157304, 0.8474937232553664, 0.49154667794820917, 0.4985954655484599]}, "task_prompt": ""}
{"id": "2149bc12-6a27-4dec-9e0a-da20dee6a9e9", "fitness": 0.7569439724919318, "name": "SelfAdaptiveDifferentialEvolution", "description": "No description provided.", "code": "import numpy as np\n\nclass SelfAdaptiveDifferentialEvolution:\n    def __init__(self, budget=10000, dim=10, pop_size=20, cr_init=0.5, f_init=0.5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.cr_init = cr_init\n        self.f_init = f_init\n        self.lb = -5.0\n        self.ub = 5.0\n        self.cr_memory = np.ones(self.pop_size) * self.cr_init\n        self.f_memory = np.ones(self.pop_size) * self.f_init\n\n    def __call__(self, func):\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n        \n        evals = self.pop_size\n\n        while evals < self.budget:\n            for i in range(self.pop_size):\n                # Adaptation of F and CR\n                self.cr = np.random.normal(self.cr_memory[i], 0.1)\n                self.cr = np.clip(self.cr, 0.0, 1.0)\n                self.f = np.random.normal(self.f_memory[i], 0.3)\n                self.f = np.clip(self.f, 0.1, 1.0)\n\n                # Mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n                mutant = a + self.f * (b - c)\n                mutant = np.clip(mutant, self.lb, self.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.cr\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, self.population[i])\n\n                # Selection\n                f = func(trial)\n                evals += 1\n\n                if f < self.fitness[i]:\n                    delta_fitness = self.fitness[i] - f\n                    self.fitness[i] = f\n                    self.population[i] = trial\n\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = trial\n                    \n                    # Update memory of F and CR based on success\n                    self.cr_memory[i] = 0.9 * self.cr_memory[i] + 0.1 * self.cr\n                    self.f_memory[i] = 0.9 * self.f_memory[i] + 0.1 * self.f\n                else:\n                    # Even if selection fails, slightly perturb the CR and F\n                    self.cr_memory[i] = 0.9 * self.cr_memory[i] + 0.1 * np.random.rand()\n                    self.f_memory[i] = 0.9 * self.f_memory[i] + 0.1 * np.random.rand()\n\n                if evals >= self.budget:\n                    break\n                    \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm SelfAdaptiveDifferentialEvolution scored 0.757 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.33275951081818034, 0.6241406359395463, 0.764380662625752, 0.8874361547066609, 0.8246785945729007, 0.8836321999777208, 0.8119630734261007, 0.78973239967947, 0.8330830354497946, 0.786531378834639, 0.8896977610281255, 0.9965740914056272, 0.2714742879981912, 0.8331877156347167, 0.9574648937907524, 0.8930507061926733, 0.6888436732687464, 0.912092947724347, 0.6374270193840641, 0.5207287073806285]}, "task_prompt": ""}
{"id": "f732743a-1327-4dc0-a8dd-6623169618c5", "fitness": 0.6139992242317927, "name": "ParticleSwarmOptimizer", "description": "No description provided.", "code": "import numpy as np\n\nclass ParticleSwarmOptimizer:\n    def __init__(self, budget=10000, dim=10, pop_size=20, w=0.7, c1=1.5, c2=1.5, v_max=1.0):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.w = w  # Inertia weight\n        self.c1 = c1  # Cognitive coefficient\n        self.c2 = c2  # Social coefficient\n        self.v_max = v_max\n        self.lb = -5.0\n        self.ub = 5.0\n\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.velocities = np.random.uniform(-self.v_max, self.v_max, size=(self.pop_size, self.dim))\n        self.fitness = np.array([np.inf] * self.pop_size)  # Initialize with infinity\n        self.personal_best_positions = self.population.copy()\n        self.personal_best_fitness = np.array([np.inf] * self.pop_size) # Initialize with infinity\n        self.global_best_position = None\n        self.global_best_fitness = np.inf\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        evals = 0\n        for i in range(self.pop_size):\n            self.fitness[i] = func(self.population[i])\n            evals += 1\n            if self.fitness[i] < self.personal_best_fitness[i]:\n                self.personal_best_fitness[i] = self.fitness[i]\n                self.personal_best_positions[i] = self.population[i].copy()\n\n                if self.fitness[i] < self.global_best_fitness:\n                    self.global_best_fitness = self.fitness[i]\n                    self.global_best_position = self.population[i].copy()\n                    self.f_opt = self.global_best_fitness\n                    self.x_opt = self.global_best_position\n\n\n        while evals < self.budget:\n            for i in range(self.pop_size):\n                # Update velocity\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                cognitive_component = self.c1 * r1 * (self.personal_best_positions[i] - self.population[i])\n                social_component = self.c2 * r2 * (self.global_best_position - self.population[i])\n                self.velocities[i] = self.w * self.velocities[i] + cognitive_component + social_component\n                self.velocities[i] = np.clip(self.velocities[i], -self.v_max, self.v_max)\n\n                # Update position\n                self.population[i] = self.population[i] + self.velocities[i]\n                self.population[i] = np.clip(self.population[i], self.lb, self.ub)\n\n                # Evaluate fitness\n                fitness = func(self.population[i])\n                evals += 1\n\n                # Update personal and global best\n                if fitness < self.personal_best_fitness[i]:\n                    self.personal_best_fitness[i] = fitness\n                    self.personal_best_positions[i] = self.population[i].copy()\n\n                    if fitness < self.global_best_fitness:\n                        self.global_best_fitness = fitness\n                        self.global_best_position = self.population[i].copy()\n                        self.f_opt = self.global_best_fitness\n                        self.x_opt = self.global_best_position\n\n                if evals >= self.budget:\n                    break\n\n            # Decay inertia weight (optional)\n            self.w *= 0.99\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm ParticleSwarmOptimizer scored 0.614 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.2566274457727664, 0.25530694576289026, 0.9314113636495633, 0.9657837697210248, 0.9234333749509703, 0.9475801452853956, 0.33053468509120365, 0.4245662617677619, 0.9334369210532132, 0.19461759389265776, 0.8945870398398941, 0.9988333050404142, 0.24358306792758733, 0.2733978501898443, 0.7415434669761585, 0.928588184388771, 0.33590980588425234, 0.9484753618454799, 0.24905690529448343, 0.5027109903015169]}, "task_prompt": ""}
{"id": "f9b6ca40-8ac5-4257-b1e6-ca9733a031b1", "fitness": "-inf", "name": "BayesianOptimization", "description": "No description provided.", "code": "import numpy as np\nfrom sklearn.gaussian_process import GaussianProcessRegressor\nfrom sklearn.gaussian_process.kernels import RBF, ConstantKernel as C\n\nclass BayesianOptimization:\n    def __init__(self, budget=10000, dim=10, n_initial_points=10, exploration_factor=2.0):\n        self.budget = budget\n        self.dim = dim\n        self.n_initial_points = n_initial_points\n        self.exploration_factor = exploration_factor\n        self.lb = -5.0\n        self.ub = 5.0\n        self.X_samples = []\n        self.Y_samples = []\n\n    def acquisition_function(self, x, gp):\n        mu, sigma = gp.predict(x.reshape(1, -1), return_std=True)\n        best = np.min(self.Y_samples)\n        imp = best - mu\n        Z = imp / sigma\n        ei = imp * norm.cdf(Z) + sigma * norm.pdf(Z)\n        return ei\n\n    def __call__(self, func):\n        from scipy.stats import norm\n        from scipy.optimize import minimize\n\n        self.f_opt = np.inf\n        self.x_opt = None\n        \n        # Initial sampling\n        X_init = np.random.uniform(self.lb, self.ub, size=(self.n_initial_points, self.dim))\n        Y_init = np.array([func(x) for x in X_init])\n        self.X_samples = X_init.tolist()\n        self.Y_samples = Y_init.tolist()\n        \n        self.budget -= self.n_initial_points\n        \n        if np.min(Y_init) < self.f_opt:\n            self.f_opt = np.min(Y_init)\n            self.x_opt = X_init[np.argmin(Y_init)]\n\n        # Define Gaussian Process\n        kernel = C(1.0, (1e-3, 1e3)) * RBF(1.0, (1e-3, 1e3))\n        gp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=10)\n\n        while self.budget > 0:\n            # Fit GP model\n            gp.fit(self.X_samples, self.Y_samples)\n\n            # Find next point to evaluate using Expected Improvement\n            def neg_ei(x):\n                return -self.acquisition_function(x, gp)\n\n            bounds = [(self.lb, self.ub)] * self.dim\n            x0 = np.random.uniform(self.lb, self.ub, size=self.dim) # Initial guess\n            res = minimize(neg_ei, x0, method='L-BFGS-B', bounds=bounds)\n            x_next = res.x\n\n            # Evaluate the function\n            f_next = func(x_next)\n            self.budget -= 1\n            \n            # Append sample\n            self.X_samples.append(x_next)\n            self.Y_samples.append(f_next)\n\n            # Update best\n            if f_next < self.f_opt:\n                self.f_opt = f_next\n                self.x_opt = x_next\n        \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "In the code, line 44, in __call__, the following error occurred:\nNameError: name 'C' is not defined\nOn line: kernel = C(1.0, (1e-3, 1e3)) * RBF(1.0, (1e-3, 1e3))", "error": "In the code, line 44, in __call__, the following error occurred:\nNameError: name 'C' is not defined\nOn line: kernel = C(1.0, (1e-3, 1e3)) * RBF(1.0, (1e-3, 1e3))", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "92b0e54e-445d-4807-aa0d-8eaf650c12fe", "fitness": "-inf", "name": "PSO_GM", "description": "No description provided.", "code": "import numpy as np\n\nclass PSO_GM:\n    def __init__(self, budget=10000, dim=10, pop_size=50, w=0.7, c1=1.5, c2=1.5, mutation_rate=0.05, reset_ratio=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.w = w  # Inertia weight\n        self.c1 = c1  # Cognitive coefficient\n        self.c2 = c2  # Social coefficient\n        self.mutation_rate = mutation_rate\n        self.reset_ratio = reset_ratio\n        self.pop = None\n        self.velocity = None\n        self.fitness = None\n        self.personal_best_positions = None\n        self.personal_best_fitness = None\n        self.global_best_position = None\n        self.global_best_fitness = np.inf\n\n    def initialize_population(self, func):\n        self.pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.velocity = np.random.uniform(-1, 1, size=(self.pop_size, self.dim))  # Initialize velocities\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.budget -= self.pop_size\n        self.personal_best_positions = np.copy(self.pop)\n        self.personal_best_fitness = np.copy(self.fitness)\n        self.global_best_position = self.pop[np.argmin(self.fitness)]\n        self.global_best_fitness = np.min(self.fitness)\n        self.f_opt = self.global_best_fitness\n        self.x_opt = self.global_best_position\n        \n    def gaussian_mutation(self, x, func):\n        mask = np.random.rand(self.dim) < self.mutation_rate\n        x[mask] = np.clip(x[mask] + np.random.normal(0, 0.1, size=np.sum(mask)), func.bounds.lb, func.bounds.ub)\n        return x\n    \n    def reset_population_subset(self, func):\n        num_reset = int(self.pop_size * self.reset_ratio)\n        indices = np.random.choice(self.pop_size, num_reset, replace=False)\n        self.pop[indices] = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(num_reset, self.dim))\n        self.velocity[indices] = np.random.uniform(-1, 1, size=(num_reset, self.dim))\n        self.fitness[indices] = np.array([func(x) for x in self.pop[indices]])\n        self.budget -= num_reset\n        \n        for i in indices:\n            if self.fitness[i] < self.personal_best_fitness[i]:\n                self.personal_best_fitness[i] = self.fitness[i]\n                self.personal_best_positions[i] = self.pop[i]\n            \n            if self.fitness[i] < self.global_best_fitness:\n                self.global_best_fitness = self.fitness[i]\n                self.global_best_position = self.pop[i]\n                self.f_opt = self.global_best_fitness\n                self.x_opt = self.global_best_position\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        generation = 0\n        while self.budget > 0:\n            generation += 1\n            for i in range(self.pop_size):\n                if self.budget <= 0:\n                    break\n                # Update velocity\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                cognitive_component = self.c1 * r1 * (self.personal_best_positions[i] - self.pop[i])\n                social_component = self.c2 * r2 * (self.global_best_position - self.pop[i])\n                self.velocity[i] = self.w * self.velocity[i] + cognitive_component + social_component\n                \n                # Update position\n                self.pop[i] = np.clip(self.pop[i] + self.velocity[i], func.bounds.lb, func.bounds.ub)\n                \n                # Gaussian Mutation\n                self.pop[i] = self.gaussian_mutation(self.pop[i], func)\n\n                # Evaluate fitness\n                f = func(self.pop[i])\n                self.budget -= 1\n                self.fitness[i] = f\n\n                # Update personal best\n                if f < self.personal_best_fitness[i]:\n                    self.personal_best_fitness[i] = f\n                    self.personal_best_positions[i] = self.pop[i]\n\n                # Update global best\n                if f < self.global_best_fitness:\n                    self.global_best_fitness = f\n                    self.global_best_position = self.pop[i]\n                    self.f_opt = self.global_best_fitness\n                    self.x_opt = self.global_best_position\n            \n            if generation % 50 == 0:\n                self.reset_population_subset(func)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "In the code, line 35, in gaussian_mutation, the following error occurred:\nValueError: NumPy boolean array indexing assignment cannot assign 2 input values to the 1 output values where the mask is true\nOn line: x[mask] = np.clip(x[mask] + np.random.normal(0, 0.1, size=np.sum(mask)), func.bounds.lb, func.bounds.ub)", "error": "In the code, line 35, in gaussian_mutation, the following error occurred:\nValueError: NumPy boolean array indexing assignment cannot assign 2 input values to the 1 output values where the mask is true\nOn line: x[mask] = np.clip(x[mask] + np.random.normal(0, 0.1, size=np.sum(mask)), func.bounds.lb, func.bounds.ub)", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "0dd9054e-3ab4-4f9b-bd0d-e9b53ee2cda9", "fitness": 0.19540293495103395, "name": "CMAES", "description": "No description provided.", "code": "import numpy as np\n\nclass CMAES:\n    def __init__(self, budget=10000, dim=10, pop_size=None, sigma=0.5, cs=0.3, damps=1.0, c_cov=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size if pop_size else 4 + int(3 * np.log(self.dim))\n        self.sigma = sigma\n        self.lb = -5.0\n        self.ub = 5.0\n\n        self.mu = (self.ub + self.lb) / 2 * np.ones(self.dim)  # Mean value\n        self.C = np.eye(self.dim)  # Covariance matrix\n        self.p_sigma = np.zeros(self.dim)  # Evolution path for sigma\n        self.p_c = np.zeros(self.dim)  # Evolution path for covariance\n\n        self.cs = cs\n        self.damps = damps\n        self.c_cov = c_cov\n\n        self.chiN = self.dim**0.5 * (1 - 1/(4*self.dim) + 1/(21*self.dim**2)) # expectation of ||N(0,I)||\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        evals = 0\n        \n        while evals < self.budget:\n            # Generate population\n            z = np.random.multivariate_normal(np.zeros(self.dim), np.eye(self.dim), size=self.pop_size)\n            y = np.dot(z, np.linalg.cholesky(self.C).T)\n            x = self.mu + self.sigma * y\n            x = np.clip(x, self.lb, self.ub)  # clip to bounds\n\n            fitness = np.array([func(xi) for xi in x])\n            evals += self.pop_size\n\n            if np.min(fitness) < self.f_opt:\n                self.f_opt = np.min(fitness)\n                self.x_opt = x[np.argmin(fitness)]\n\n            # Sort by fitness\n            idx_sort = np.argsort(fitness)\n            x = x[idx_sort]\n\n            # Update mean\n            x_mean = np.mean(x[:self.pop_size//2], axis=0)\n            y_mean = (x_mean - self.mu) / self.sigma\n            self.mu = x_mean\n\n            # Update evolution paths\n            self.p_sigma = (1 - self.cs) * self.p_sigma + np.sqrt(self.cs * (2 - self.cs)) * y_mean\n            self.p_c = (1 - self.c_cov) * self.p_c + np.sqrt(self.c_cov * (2 - self.c_cov)) * np.sqrt(self.pop_size//2) * (x_mean - self.mu) / self.sigma\n\n            # Update covariance matrix\n            self.C = (1 - self.c_cov) * self.C + self.c_cov * np.outer(self.p_c, self.p_c) + self.c_cov * (1 - self.c_cov) * np.eye(self.dim)\n\n            # Update sigma\n            self.sigma *= np.exp((self.cs / self.damps) * (np.linalg.norm(self.p_sigma) / self.chiN - 1))\n            self.sigma = max(self.sigma, 1e-10)\n\n            if evals >= self.budget:\n                break\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm CMAES scored 0.195 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.09080542173908801, 0.18018678493931328, 0.3614736122156029, 0.18750797494839466, 0.14931928112217252, 0.15135963377325767, 0.264339770274009, 0.182275917675764, 0.16187746289282934, 0.15692224939552324, 0.1564157535589893, 0.20730985883967923, 0.2415475994854901, 0.23242480620198724, 0.20864007411034158, 0.25042876353421184, 0.21327388315967755, 0.16686092617965953, 0.19443122057516926, 0.15065770439951864]}, "task_prompt": ""}
{"id": "deb17059-dc31-427f-841b-0060ee5746b6", "fitness": 0.5737860268327754, "name": "CMAES", "description": "No description provided.", "code": "import numpy as np\n\nclass CMAES:\n    def __init__(self, budget=10000, dim=10, pop_size=None, sigma_init=0.5, cs=0.3, c_cov=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size if pop_size is not None else 4 + int(3 * np.log(self.dim))\n        self.sigma = sigma_init\n        self.mean = np.zeros(self.dim)\n        self.C = np.eye(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.cs = cs\n        self.c_cov = c_cov\n        self.mu = self.pop_size // 2\n        self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n        self.weights /= np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        self.c_sigma = (self.mueff + 2) / (self.dim + self.mueff + 5)\n        self.d_sigma = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1) + self.cs\n        self.chiN = self.dim**0.5 * (1 - 1 / (4 * self.dim) + 1 / (21 * self.dim**2))\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        evals = 0\n\n        while evals < self.budget:\n            z = np.random.randn(self.pop_size, self.dim)\n            x = self.mean + self.sigma * z @ np.linalg.cholesky(self.C).T\n            x = np.clip(x, func.bounds.lb, func.bounds.ub)\n            \n            fitness = np.array([func(xi) for xi in x])\n            evals += self.pop_size\n            if evals > self.budget:\n                fitness = fitness[:self.pop_size - (evals - self.budget)]\n                x = x[:self.pop_size - (evals - self.budget)]\n                evals = self.budget\n                self.pop_size = self.pop_size - (evals - self.budget)\n                \n\n            if np.min(fitness) < self.f_opt:\n                self.f_opt = np.min(fitness)\n                self.x_opt = x[np.argmin(fitness)]\n\n            idx = np.argsort(fitness)\n            x_sorted = x[idx[:self.mu]]\n            z_sorted = z[idx[:self.mu]]\n\n            mean_old = self.mean.copy()\n            self.mean += self.sigma * np.sum(self.weights[:, None] * z_sorted, axis=0)\n\n            self.ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs) * self.mueff) * (self.mean - mean_old) / self.sigma\n            self.pc = (1 - self.c_cov) * self.pc + np.sqrt(self.c_cov * (2 - self.c_cov) * self.mueff) * (self.mean - mean_old) / self.sigma\n\n            hsig = (np.linalg.norm(self.ps) / np.sqrt(1 - (1 - self.cs)**(2 * evals / self.pop_size)) / self.chiN < 1.4 + 2 / (self.dim + 1))\n            dhsig = (1 - hsig) * self.c_cov * (2 - self.c_cov)\n\n            self.C = (1 - self.c_cov) * self.C + dhsig * self.pc[:, None] @ self.pc[None, :]\n            for i in range(self.mu):\n                self.C += self.c_cov * self.weights[i] * ((z_sorted[i] -0)[:, None] @ (z_sorted[i]-0)[None, :])\n\n            self.sigma *= np.exp((self.c_sigma / self.d_sigma) * (np.linalg.norm(self.ps) / self.chiN - 1))\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm CMAES scored 0.574 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.37458689921923827, 0.9308306164611657, 0.928605642826785, 0.9632762275541932, 0.3152777172743719, 0.9444238544292302, 0.3199544121444906, 0.9344684580243647, 0.9531922767927621, 0.22763931458998343, 0.9649337146368008, 0.9958235325652176, 0.2517145712446943, 0.21049207439890383, 0.5954409735462483, 0.35815096873425734, 0.21874653553817103, 0.2890011163764741, 0.2006656825491887, 0.4984959477489658]}, "task_prompt": ""}
{"id": "6436ed52-398d-4dbb-8bd7-3c7af399d862", "fitness": 0.317938093039328, "name": "PopulationBasedAnnealing", "description": "No description provided.", "code": "import numpy as np\n\nclass PopulationBasedAnnealing:\n    def __init__(self, budget=10000, dim=10, pop_size=50, temp_init=1.0, temp_decay=0.95):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.temp = temp_init\n        self.temp_decay = temp_decay\n        self.f_opt = np.inf\n        self.x_opt = None\n    def __call__(self, func):\n        self.pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.budget -= self.pop_size\n        \n        if np.min(self.fitness) < self.f_opt:\n            self.f_opt = np.min(self.fitness)\n            self.x_opt = self.pop[np.argmin(self.fitness)]\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                if self.budget <= 0:\n                    break\n                # Generate a candidate solution by perturbing the current solution\n                perturbation = np.random.normal(0, self.temp, size=self.dim)\n                candidate = np.clip(self.pop[i] + perturbation, func.bounds.lb, func.bounds.ub)\n                f_candidate = func(candidate)\n                self.budget -= 1\n\n                # Acceptance criterion (Metropolis criterion)\n                delta_f = f_candidate - self.fitness[i]\n                if delta_f < 0 or np.random.rand() < np.exp(-delta_f / self.temp):\n                    self.pop[i] = candidate\n                    self.fitness[i] = f_candidate\n\n                    if f_candidate < self.f_opt:\n                        self.f_opt = f_candidate\n                        self.x_opt = candidate\n\n            self.temp *= self.temp_decay  # Cool down the temperature\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm PopulationBasedAnnealing scored 0.318 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.1277414633310262, 0.3316709523739688, 0.25610598273434393, 0.32731988761553343, 0.27227951650022497, 0.2218398971109815, 0.24570180386318063, 0.23920752119783573, 0.21563418100994636, 0.19225143252025456, 0.3192414567917313, 0.9985433171402842, 0.32838553297061557, 0.2925004232291877, 0.5478226138409376, 0.2889699532946345, 0.2549406818135188, 0.25858288542608565, 0.19098859148453373, 0.44903376653773586]}, "task_prompt": ""}
{"id": "b08b409e-aff5-47cb-b158-853fe18200e2", "fitness": "-inf", "name": "GMMAdaptation", "description": "No description provided.", "code": "import numpy as np\nfrom sklearn.mixture import GaussianMixture\n\nclass GMMAdaptation:\n    def __init__(self, budget=10000, dim=10, pop_size=20, n_components=5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.n_components = n_components\n        self.lb = -5.0\n        self.ub = 5.0\n        self.gmm = GaussianMixture(n_components=self.n_components, covariance_type='full', max_iter=100, random_state=42)\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        # Initial population\n        population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        evals = self.pop_size\n\n        if np.min(fitness) < self.f_opt:\n            self.f_opt = np.min(fitness)\n            self.x_opt = population[np.argmin(fitness)]\n\n        while evals < self.budget:\n            # Learn GMM from the best individuals\n            num_elites = min(self.pop_size // 2, population.shape[0]) #ensure no index out of bounds\n            elites_indices = np.argsort(fitness)[:num_elites]\n            elites = population[elites_indices]\n\n            try:\n                self.gmm.fit(elites)\n            except:\n                # Handle the case where GMM fitting fails, e.g., due to insufficient data or singular covariance\n                # Re-initialize GMM or skip the adaptation step\n                self.gmm = GaussianMixture(n_components=self.n_components, covariance_type='full', max_iter=100, random_state=42)\n                population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n                fitness = np.array([func(x) for x in population])\n                evals += self.pop_size # update evaluation counts\n\n                if np.min(fitness) < self.f_opt:\n                    self.f_opt = np.min(fitness)\n                    self.x_opt = population[np.argmin(fitness)]\n                continue\n            \n            # Sample new individuals from GMM\n            n_samples = min(self.budget - evals, self.pop_size)\n            new_samples = self.gmm.sample(n_samples)[0]\n            new_samples = np.clip(new_samples, self.lb, self.ub)\n            \n            new_fitness = np.array([func(x) for x in new_samples])\n            evals += n_samples\n            \n            # Update population\n            worst_idx = np.argmax(fitness)\n            for i in range(n_samples):\n                if new_fitness[i] < fitness[worst_idx]:\n                    fitness[worst_idx] = new_fitness[i]\n                    population[worst_idx] = new_samples[i]\n                    worst_idx = np.argmax(fitness) #Recalculate index of worst\n                \n                    if new_fitness[i] < self.f_opt:\n                        self.f_opt = new_fitness[i]\n                        self.x_opt = new_samples[i]\n\n            if evals >= self.budget:\n                break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "In the code, line 12, in __init__, the following error occurred:\nNameError: name 'GaussianMixture' is not defined\nOn line: self.gmm = GaussianMixture(n_components=self.n_components, covariance_type='full', max_iter=100, random_state=42)", "error": "In the code, line 12, in __init__, the following error occurred:\nNameError: name 'GaussianMixture' is not defined\nOn line: self.gmm = GaussianMixture(n_components=self.n_components, covariance_type='full', max_iter=100, random_state=42)", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "d40db0ff-db3c-4c08-be12-b5f65458b722", "fitness": "-inf", "name": "CMAES", "description": "No description provided.", "code": "import numpy as np\n\nclass CMAES:\n    def __init__(self, budget=10000, dim=10, pop_size=None, sigma0=0.5, restart_trigger=1e-12):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size if pop_size else 4 + int(3 * np.log(self.dim))\n        self.sigma0 = sigma0\n        self.mean = None\n        self.C = None\n        self.sigma = None\n        self.pc = None\n        self.ps = None\n        self.chiN = None\n        self.mu = None\n        self.weights = None\n        self.mueff = None\n        self.restart_trigger = restart_trigger\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.initialize()\n\n    def initialize(self):\n        self.mean = np.random.uniform(-1, 1, self.dim)\n        self.C = np.eye(self.dim)\n        self.sigma = self.sigma0\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.chiN = np.sqrt(self.dim) * (1 - (1 / (4 * self.dim)) + (1 / (21 * self.dim**2)))\n        self.mu = self.pop_size // 2\n        self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n        self.weights /= np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n\n    def __call__(self, func):\n        self.initialize()\n        c_sigma = (self.mueff + 2) / (self.dim + self.mueff + 5)\n        c_c = (4 + self.mueff / self.dim) / (self.dim + 4 + 2 * self.mueff / self.dim)\n        damps = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1) + c_sigma\n        c_1 = 2 / ((self.dim + 1.3)**2 + self.mueff)\n        c_mu = min(1 - c_1, 2 * (self.mueff - 2 + 1 / self.mueff) / ((self.dim + 2)**2 + self.mueff))\n\n        B = None\n        D = None\n        eigen_updated = 0\n\n        while self.budget > 0:\n            if eigen_updated > self.pop_size // 2:\n                eigen_updated = 0\n                self.C = np.triu(self.C) + np.triu(self.C, 1).T\n                D, B = np.linalg.eigh(self.C)\n                D = np.sqrt(D)\n                if np.min(D) < self.restart_trigger or np.max(D) > 1/self.restart_trigger:\n                   self.initialize()\n                   continue\n\n            z = np.random.normal(0, 1, size=(self.pop_size, self.dim))\n            y = B @ (D[:, None] * z.T)\n            x = self.mean + self.sigma * y.T\n            x = np.clip(x, func.bounds.lb, func.bounds.ub)\n\n            fitness = np.array([func(xi) for xi in x])\n            self.budget -= self.pop_size\n\n            if np.min(fitness) < self.f_opt:\n                self.f_opt = np.min(fitness)\n                self.x_opt = x[np.argmin(fitness)]\n\n            idx = np.argsort(fitness)\n            x_sorted = x[idx]\n\n            y_mean = np.sum(self.weights[:, None] * (x_sorted[:self.mu] - self.mean), axis=0)\n\n            self.ps = (1 - c_sigma) * self.ps + np.sqrt(c_sigma * (2 - c_sigma) * self.mueff) * (B @ y[:, :self.mu] @ self.weights)\n            self.pc = (1 - c_c) * self.pc + np.sqrt(c_c * (2 - c_c) * self.mueff) * y_mean / self.sigma\n            hsig = (np.linalg.norm(self.ps) / np.sqrt(1 - (1 - c_sigma)**(self.budget / self.pop_size)) / self.chiN) < (1.4 + 2 / (self.dim + 1))\n            self.C = (1 - c_1 - c_mu) * self.C + c_1 * (self.pc[:, None] @ self.pc[None, :]) + c_mu * (B @ (y[:, :self.mu] * self.weights) @ (y[:, :self.mu] @ self.weights).T @ B.T)\n\n            self.sigma *= np.exp((c_sigma / damps) * (np.linalg.norm(self.ps) / self.chiN - 1))\n            self.mean = x_sorted[0]\n            eigen_updated += 1\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "In the code, line 58, in __call__, the following error occurred:\nTypeError: 'NoneType' object is not subscriptable\nOn line: y = B @ (D[:, None] * z.T)", "error": "In the code, line 58, in __call__, the following error occurred:\nTypeError: 'NoneType' object is not subscriptable\nOn line: y = B @ (D[:, None] * z.T)", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "98b755f9-efc4-4170-be86-d26eb279ddbf", "fitness": 0.0, "name": "PopulationIteratedLocalSearch", "description": "No description provided.", "code": "import numpy as np\n\nclass PopulationIteratedLocalSearch:\n    def __init__(self, budget=10000, dim=10, pop_size=20, local_search_iterations=10, reinit_threshold=0.2):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.local_search_iterations = local_search_iterations\n        self.reinit_threshold = reinit_threshold\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.budget -= self.pop_size\n\n        if np.min(self.fitness) < self.f_opt:\n            self.f_opt = np.min(self.fitness)\n            self.x_opt = self.pop[np.argmin(self.fitness)]\n\n        while self.budget > 0:\n            # Local Search\n            for i in range(self.pop_size):\n                if self.budget <= 0:\n                    break\n                for _ in range(min(self.local_search_iterations, self.budget)):\n                    if self.budget <= 0:\n                        break\n                    perturbation = np.random.normal(0, 0.05, size=self.dim)\n                    x_local = np.clip(self.pop[i] + perturbation, func.bounds.lb, func.bounds.ub)\n                    f_local = func(x_local)\n                    self.budget -= 1\n\n                    if f_local < self.fitness[i]:\n                        self.pop[i] = x_local\n                        self.fitness[i] = f_local\n\n                        if f_local < self.f_opt:\n                            self.f_opt = f_local\n                            self.x_opt = x_local\n\n            # Re-initialization\n            fitness_threshold = np.mean(self.fitness) + self.reinit_threshold * np.std(self.fitness)\n            for i in range(self.pop_size):\n                if self.fitness[i] > fitness_threshold:\n                    self.pop[i] = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                    self.fitness[i] = func(self.pop[i])\n                    self.budget -= 1\n                    if self.fitness[i] < self.f_opt:\n                        self.f_opt = self.fitness[i]\n                        self.x_opt = self.pop[i]\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm PopulationIteratedLocalSearch scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0]}, "task_prompt": ""}
{"id": "fe3729ee-0d3d-462b-9ae3-c23c2628eb7c", "fitness": 0.07302371334773633, "name": "SelfAdaptiveDifferentialEvolutionCauchyArchive", "description": "No description provided.", "code": "import numpy as np\n\nclass SelfAdaptiveDifferentialEvolutionCauchyArchive:\n    def __init__(self, budget=10000, dim=10, pop_size=20, archive_size=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.lb = -5.0\n        self.ub = 5.0\n        self.population = None\n        self.fitness = None\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.archive = []\n        self.archive_fitness = []\n        self.cr = 0.5\n        self.f = 0.5\n\n    def __call__(self, func):\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n        evals = self.pop_size\n\n        while evals < self.budget:\n            for i in range(self.pop_size):\n                # Mutation using Cauchy distribution\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b = self.population[np.random.choice(idxs, 2, replace=False)]\n                mutant = self.population[i] + self.f * (a - b) * np.random.standard_cauchy(size=self.dim)\n                mutant = np.clip(mutant, self.lb, self.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.cr\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, self.population[i])\n\n                # Selection\n                f = func(trial)\n                evals += 1\n\n                if f < self.fitness[i]:\n                    self.fitness[i] = f\n                    self.population[i] = trial\n\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = trial\n                    #Update CR and F\n                    self.cr = 0.9 * self.cr + 0.1 * np.random.rand()\n                    self.f = 0.9 * self.f + 0.1 * np.random.rand()\n\n                    # Add to archive\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(self.population[i].copy())\n                    else:\n                        # Replace random element in archive\n                        idx_to_replace = np.random.randint(0, self.archive_size)\n                        self.archive[idx_to_replace] = self.population[i].copy()\n                elif len(self.archive) > 0:\n                    # Mutation using archive\n                    arch_idx = np.random.randint(0, len(self.archive))\n                    idxs = [idx for idx in range(self.pop_size) if idx != i]\n                    a = self.population[np.random.choice(idxs, 1, replace=False)][0]\n                    mutant = self.population[i] + self.f * (self.archive[arch_idx] - a) * np.random.standard_cauchy(size=self.dim)\n                    mutant = np.clip(mutant, self.lb, self.ub)\n                    \n                    # Crossover\n                    cross_points = np.random.rand(self.dim) < self.cr\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n                    trial = np.where(cross_points, mutant, self.population[i])\n                    \n                    f = func(trial)\n                    evals += 1\n\n                    if f < self.fitness[i]:\n                        self.fitness[i] = f\n                        self.population[i] = trial\n\n                        if f < self.f_opt:\n                            self.f_opt = f\n                            self.x_opt = trial\n                        #Update CR and F\n                        self.cr = 0.9 * self.cr + 0.1 * np.random.rand()\n                        self.f = 0.9 * self.f + 0.1 * np.random.rand()\n\n                        # Add to archive\n                        if len(self.archive) < self.archive_size:\n                            self.archive.append(self.population[i].copy())\n                        else:\n                            # Replace random element in archive\n                            idx_to_replace = np.random.randint(0, self.archive_size)\n                            self.archive[idx_to_replace] = self.population[i].copy()\n\n                if evals >= self.budget:\n                    break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm SelfAdaptiveDifferentialEvolutionCauchyArchive scored 0.073 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.14604742669547266, 0]}, "task_prompt": ""}
{"id": "4146fd10-eb15-443e-b187-2cfba2e857a1", "fitness": 0.3895317318329611, "name": "CMAES", "description": "No description provided.", "code": "import numpy as np\n\nclass CMAES:\n    def __init__(self, budget=10000, dim=10, pop_size=None, initial_sigma=0.5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size if pop_size is not None else 4 + int(3 * np.log(dim))\n        self.initial_sigma = initial_sigma\n        self.mu = self.pop_size // 2\n        self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.m = None\n        self.C = None\n        self.sigma = None\n        self.pc = None\n        self.ps = None\n        self.chiN = None\n        self.c_sigma = None\n        self.d_sigma = None\n        self.c_c = None\n        self.c_1 = None\n        self.c_mu = None\n        self.learning_rate = 1.0\n\n    def initialize(self):\n        self.m = np.random.uniform(-1, 1, size=self.dim)\n        self.C = np.eye(self.dim)\n        self.sigma = self.initial_sigma\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.chiN = self.dim**0.5 * (1 - 1/(4*self.dim) + 1/(21*self.dim**2))\n        self.c_sigma = (self.pop_size + 3) / (5 * self.dim**0.5)\n        self.d_sigma = 1 + 2 * max(0, ((np.sum(self.weights) * self.chiN / self.sigma) - 1)) + self.c_sigma\n        self.c_c = (4 + self.mu/self.dim) / (self.dim + 4 + 2*self.mu/self.dim)\n        self.c_1 = 2 / ((self.dim + 1.3)**2 + self.mu)\n        self.c_mu = min(1 - self.c_1, 2 * (self.mu - 2 + 1/self.mu) / ((self.dim + 2)**2 + self.mu))\n        self.learning_rate = 1.0\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.nevals = 0\n        self.initialize()\n\n        while self.nevals < self.budget:\n            z = np.random.normal(0, 1, size=(self.pop_size, self.dim))\n            y = np.dot(z, np.linalg.cholesky(self.C).T)\n            x = self.m + self.sigma * y\n            \n            # Boundary Handling (clipping)\n            for i in range(self.pop_size):\n                x[i] = np.clip(x[i], func.bounds.lb, func.bounds.ub)\n\n            fitness = np.array([func(xi) for xi in x])\n            self.nevals += self.pop_size\n            \n            idx = np.argsort(fitness)\n            fitness = fitness[idx]\n            x = x[idx]\n\n            self.m = np.sum(self.weights[:, None] * x[:self.mu], axis=0)\n\n            # Update evolution path for C\n            y_mean = np.mean(y[:self.mu], axis=0)\n            self.ps = (1 - self.c_sigma) * self.ps + (self.c_sigma * (2 - self.c_sigma))**0.5 * (np.linalg.inv(np.linalg.cholesky(self.C)) @ y_mean)\n            self.pc = (1 - self.c_c) * self.pc + (self.c_c * (2 - self.c_c))**0.5 * y_mean\n\n            # Update covariance matrix C\n            self.C = (1 - self.c_1 - self.c_mu) * self.C + self.c_1 * (self.pc[:, None] @ self.pc[None, :])\n            self.C += self.c_mu * np.sum(self.weights[:, None, None] * (y[:self.mu, :, None] @ y[:self.mu, None, :]), axis=0)\n\n            # Update step size sigma\n            self.sigma *= np.exp((self.c_sigma / self.d_sigma) * ((np.linalg.norm(self.ps) / self.chiN) - 1))\n            self.sigma = max(self.sigma, 1e-10)  # Prevent sigma from becoming too small\n\n            if fitness[0] < self.f_opt:\n                self.f_opt = fitness[0]\n                self.x_opt = x[0]\n\n            self.learning_rate *= 0.995 # Decaying learning rate\n\n            self.c_sigma *= self.learning_rate\n            self.c_c *= self.learning_rate\n            self.c_1 *= self.learning_rate\n            self.c_mu *= self.learning_rate\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm CMAES scored 0.390 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.20068797458441534, 0.35937718358832804, 0.44314220874105836, 0.1708539512075753, 0.35434686756785394, 0.46121284741157353, 0.30181584108135184, 0.3834425720514473, 0.3673880825833301, 0.20595962471713025, 0.5146229348900737, 0.9950397872724193, 0.26501628585141357, 0.344378790614712, 0.7802567131781122, 0.40893595194332477, 0.3701074560098665, 0.1739514111043301, 0.21499023884535684, 0.4751079134155478]}, "task_prompt": ""}
