{"id": "3b6588b5-7d9d-46dc-a57f-04fd0fa2fbbe", "fitness": 0.3732865086512087, "name": "AdaptiveDifferentialEvolution", "description": "An adaptive differential evolution strategy that adjusts mutation and crossover rates based on population diversity and success rate.", "code": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, CR=0.9):\n        \"\"\"\n        Initializes the Adaptive Differential Evolution optimizer.\n\n        Args:\n            budget (int): The maximum number of function evaluations.\n            dim (int): The dimensionality of the search space.\n            pop_size (int): The population size.\n            F (float): Initial mutation factor.\n            CR (float): Initial crossover rate.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.pop = None\n        self.fitness = None\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.eval_count = 0\n\n    def initialize_population(self, func):\n        \"\"\"Initializes the population randomly within the bounds.\"\"\"\n        self.pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.zeros(self.pop_size)\n        for i in range(self.pop_size):\n            self.fitness[i] = func(self.pop[i])\n            self.eval_count += 1\n            if self.fitness[i] < self.f_opt:\n                self.f_opt = self.fitness[i]\n                self.x_opt = self.pop[i]\n\n    def mutate(self, pop, F):\n        \"\"\"Performs the mutation operation.\"\"\"\n        mutated_pop = np.zeros_like(pop)\n        for i in range(self.pop_size):\n            idxs = [idx for idx in range(self.pop_size) if idx != i]\n            a, b, c = np.random.choice(idxs, 3, replace=False)\n            mutated_pop[i] = pop[a] + F * (pop[b] - pop[c])\n        return mutated_pop\n\n    def crossover(self, pop, mutated_pop, CR):\n        \"\"\"Performs the crossover operation.\"\"\"\n        crossed_pop = np.zeros_like(pop)\n        for i in range(self.pop_size):\n            for j in range(self.dim):\n                if np.random.rand() < CR:\n                    crossed_pop[i, j] = mutated_pop[i, j]\n                else:\n                    crossed_pop[i, j] = pop[i, j]\n        return crossed_pop\n\n    def selection(self, func, crossed_pop):\n        \"\"\"Performs the selection operation.\"\"\"\n        for i in range(self.pop_size):\n            f = func(crossed_pop[i])\n            self.eval_count += 1\n            if f < self.fitness[i]:\n                self.pop[i] = crossed_pop[i]\n                self.fitness[i] = f\n                if f < self.f_opt:\n                    self.f_opt = f\n                    self.x_opt = crossed_pop[i]\n\n    def adapt_parameters(self):\n        \"\"\"Adapts the mutation and crossover rates based on population diversity and success.\"\"\"\n        # Adjust F: If the population is too homogeneous, increase F\n        if np.std(self.fitness) < 1e-6:  # Population is very similar\n            self.F = min(self.F * 1.2, 1.0)  # Increase F\n\n        # Adjust CR: if many trials are successful, decrease CR and increase it otherwise\n        success_rate = np.sum(self.fitness < self.fitness.mean()) / self.pop_size\n\n        if success_rate > 0.5:\n            self.CR = max(self.CR * 0.9, 0.1)  # Decrease CR\n        else:\n            self.CR = min(self.CR * 1.1, 0.9)  # Increase CR\n\n    def __call__(self, func):\n        \"\"\"Optimizes the given function using Adaptive Differential Evolution.\"\"\"\n        self.initialize_population(func)\n\n        while self.eval_count < self.budget:\n            mutated_pop = self.mutate(self.pop, self.F)\n            crossed_pop = self.crossover(self.pop, mutated_pop, self.CR)\n\n            # Clip the values to stay within bounds\n            crossed_pop = np.clip(crossed_pop, func.bounds.lb, func.bounds.ub)\n\n            self.selection(func, crossed_pop)\n            self.adapt_parameters()\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveDifferentialEvolution scored 0.373 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.14815385792503166, 0.20723803798970397, 0.3958847924730299, 0.34801486909502244, 0.28805052773012074, 0.4499930249208125, 0.29576859732095406, 0.35429595655579826, 0.33378578178451024, 0.18709774469346163, 0.32052069435397956, 0.9958107805168807, 0.25110799466518907, 0.2984297250566831, 0.7025773112983551, 0.4287292417199572, 0.3053025557701371, 0.4635243621290972, 0.19652094654421992, 0.4949233704812308]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "ad0f6409-cff0-4bc8-87da-01a088bef245", "fitness": 0.5844282184304761, "name": "HybridPSO_DE", "description": "A population-based algorithm that combines elements of particle swarm optimization (PSO) and differential evolution (DE) with adaptive parameter control.", "code": "import numpy as np\n\nclass HybridPSO_DE:\n    def __init__(self, budget=10000, dim=10, pop_size=20, pso_weight=0.7, de_cr=0.9, de_f=0.8):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.pso_weight = pso_weight  # Inertia weight for PSO\n        self.de_cr = de_cr            # Crossover rate for DE\n        self.de_f = de_f              # Mutation factor for DE\n        self.population = None\n        self.fitness = None\n        self.velocities = None\n        self.best_positions = None # Personal best positions for PSO\n        self.global_best_fitness = np.inf\n        self.global_best_position = None\n        self.eval_count = 0\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.eval_count += self.pop_size\n        self.velocities = np.random.uniform(-1, 1, size=(self.pop_size, self.dim)) * 0.1  # Initialize velocities\n        self.best_positions = self.population.copy() # Initialize personal best positions\n        self.global_best_position = self.population[np.argmin(self.fitness)].copy()\n        self.global_best_fitness = np.min(self.fitness)\n\n    def pso_update(self, func):\n        cognitive_weight = 1.5\n        social_weight = 1.5\n\n        r1 = np.random.rand(self.pop_size, self.dim)\n        r2 = np.random.rand(self.pop_size, self.dim)\n\n        cognitive_component = cognitive_weight * r1 * (self.best_positions - self.population)\n        social_component = social_weight * r2 * (self.global_best_position - self.population)\n\n        self.velocities = self.pso_weight * self.velocities + cognitive_component + social_component\n        self.population = self.population + self.velocities\n\n        # Clip positions to bounds\n        self.population = np.clip(self.population, func.bounds.lb, func.bounds.ub)\n\n        new_fitness = np.array([func(x) for x in self.population])\n        self.eval_count += self.pop_size\n\n        # Update personal best positions\n        for i in range(self.pop_size):\n            if new_fitness[i] < self.fitness[i]:\n                self.best_positions[i] = self.population[i].copy()\n                self.fitness[i] = new_fitness[i]\n\n        # Update global best position\n        best_index = np.argmin(self.fitness)\n        if self.fitness[best_index] < self.global_best_fitness:\n            self.global_best_fitness = self.fitness[best_index]\n            self.global_best_position = self.population[best_index].copy()\n\n\n    def de_update(self, func):\n        for i in range(self.pop_size):\n            # Choose three random indices, distinct from each other and i\n            idxs = np.random.choice(self.pop_size, 3, replace=False)\n            while i in idxs:\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n\n            x1, x2, x3 = self.population[idxs]\n\n            # Mutation\n            v = x1 + self.de_f * (x2 - x3)\n            v = np.clip(v, func.bounds.lb, func.bounds.ub)\n\n            # Crossover\n            u = np.copy(self.population[i])\n            j_rand = np.random.randint(self.dim)\n            for j in range(self.dim):\n                if np.random.rand() < self.de_cr or j == j_rand:\n                    u[j] = v[j]\n\n            # Selection\n            f_u = func(u)\n            self.eval_count += 1\n\n            if f_u < self.fitness[i]:\n                self.population[i] = u\n                self.fitness[i] = f_u\n\n                if f_u < self.global_best_fitness:\n                    self.global_best_fitness = f_u\n                    self.global_best_position = u.copy()\n            elif self.eval_count >= self.budget:\n                break\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.eval_count < self.budget:\n            if self.eval_count < self.budget * 0.5:\n                self.pso_update(func)\n            else:\n                self.de_update(func)\n            if self.eval_count >= self.budget:\n                break\n\n        return self.global_best_fitness, self.global_best_position", "configspace": "", "generation": 0, "feedback": "The algorithm HybridPSO_DE scored 0.584 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.23233641700919816, 0.5510896288640739, 0.8680961489138704, 0.9565209307423083, 0.3109044535192902, 0.8946411646638626, 0.32648468002816244, 0.6315713810149729, 0.855760124789113, 0.213745224071058, 0.9335825289260093, 0.9982428825587983, 0.3406877147402815, 0.3059718830260285, 0.5550421924869486, 0.8784990874693536, 0.7735916875004076, 0.38093729755495354, 0.21077680898347662, 0.47008213174735225]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "bd6b3c28-b984-4fa3-be0f-156d74b61953", "fitness": 0.3726265304660994, "name": "AdaptiveDifferentialEvolution", "description": "An Adaptive Differential Evolution strategy with dynamically adjusted parameters and a restart mechanism.", "code": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, CR=0.9):\n        \"\"\"\n        Initialize the Adaptive Differential Evolution algorithm.\n\n        Args:\n            budget (int): The maximum number of function evaluations.\n            dim (int): The dimensionality of the search space.\n            pop_size (int): The population size.\n            F (float): The initial scaling factor.\n            CR (float): The initial crossover rate.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.pop = None\n        self.fitness = None\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.lb = None\n        self.ub = None\n\n    def initialize_population(self, func):\n        \"\"\"\n        Initialize the population with random values within the bounds.\n        \"\"\"\n        self.lb = func.bounds.lb\n        self.ub = func.bounds.ub\n        self.pop = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.eval_count += self.pop_size\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.pop[np.argmin(self.fitness)]\n\n\n    def mutate(self):\n        \"\"\"\n        Perform mutation operation.\n        \"\"\"\n        for i in range(self.pop_size):\n            idxs = np.random.choice(self.pop_size, 3, replace=False)\n            x_r1, x_r2, x_r3 = self.pop[idxs]\n            v = self.pop[i] + self.F * (x_r2 - x_r3)\n            v = np.clip(v, self.lb, self.ub)\n            yield v\n\n    def crossover(self, mutant, i):\n        \"\"\"\n        Perform crossover operation.\n        \"\"\"\n        u = np.copy(self.pop[i])\n        j_rand = np.random.randint(self.dim)\n        for j in range(self.dim):\n            if np.random.rand() < self.CR or j == j_rand:\n                u[j] = mutant[j]\n        return u\n\n    def selection(self, func, u, i):\n        \"\"\"\n        Perform selection operation.\n        \"\"\"\n        f_u = func(u)\n        self.eval_count += 1\n\n        if f_u < self.fitness[i]:\n            self.pop[i] = u\n            self.fitness[i] = f_u\n\n            if f_u < self.f_opt:\n                self.f_opt = f_u\n                self.x_opt = u\n\n    def adapt_parameters(self, gen):\n        \"\"\"\n        Adapt the parameters F and CR based on the generation number.\n        \"\"\"\n        self.F = 0.5 + 0.4 * np.exp(-gen / 100) # Decaying F\n        self.CR = 0.1 + 0.8 * np.sin(gen / 200) # Oscillating CR\n\n\n    def restart(self, func):\n        \"\"\"\n        Restart the population if no improvement has been made for a certain number of iterations.\n        \"\"\"\n        self.pop = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.eval_count += self.pop_size\n        \n        current_best_idx = np.argmin(self.fitness)\n        if self.fitness[current_best_idx] < self.f_opt:\n            self.f_opt = self.fitness[current_best_idx]\n            self.x_opt = self.pop[current_best_idx]\n\n\n    def __call__(self, func):\n        \"\"\"\n        Optimize the given function using Adaptive Differential Evolution.\n\n        Args:\n            func (callable): The function to optimize.\n\n        Returns:\n            tuple: The best function value found and the corresponding solution.\n        \"\"\"\n        self.initialize_population(func)\n        no_improvement_count = 0\n        best_f_so_far = self.f_opt\n        gen = 0 \n\n        while self.eval_count < self.budget:\n            \n            mutants = self.mutate()\n            for i, mutant in enumerate(mutants):\n                if self.eval_count >= self.budget:\n                    break\n                u = self.crossover(mutant, i)\n                self.selection(func, u, i)\n                \n            gen += 1\n            self.adapt_parameters(gen)\n            \n            if self.f_opt < best_f_so_far:\n                best_f_so_far = self.f_opt\n                no_improvement_count = 0\n            else:\n                no_improvement_count += 1\n\n            if no_improvement_count > 200:\n                self.restart(func)\n                no_improvement_count = 0\n                \n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveDifferentialEvolution scored 0.373 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.14487450592671958, 0.22928933305241916, 0.37021692952454355, 0.31356805908129903, 0.2979434710751232, 0.386607315260709, 0.29093959194488284, 0.33064266042709123, 0.29272321737236695, 0.19350391687822566, 0.3832541258708222, 0.9956056911996769, 0.3062070705367461, 0.30159903787658027, 0.7378744209038435, 0.4146314794748224, 0.3074437558327795, 0.48315647450198473, 0.1886119384642092, 0.4838376141171419]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "d83e47fb-f808-4342-af73-25d2c79f9fac", "fitness": 0.6288676199420782, "name": "AdaptiveDE", "description": "An adaptive differential evolution strategy with a self-adaptive mutation rate and crossover probability, coupled with a restart mechanism to escape local optima.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, CR=0.9, restart_patience=500):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F  # Initial mutation factor\n        self.CR = CR  # Initial crossover rate\n        self.restart_patience = restart_patience\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        # Initialize population\n        pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in pop])\n        self.budget -= self.pop_size\n        \n        best_idx = np.argmin(fitness)\n        self.f_opt = fitness[best_idx]\n        self.x_opt = pop[best_idx]\n\n        iteration = 0\n        no_improvement_count = 0\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = pop[idxs]\n                \n                # Adaptive F: perturb F with a small probability\n                F_current = self.F\n                if np.random.rand() < 0.1:\n                    F_current = np.random.normal(self.F, 0.1)\n                    F_current = np.clip(F_current, 0.1, 1.0) #Keep within reasonable bounds\n                \n                v = x_r1 + F_current * (x_r2 - x_r3)\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)  # Keep within bounds\n\n                # Crossover\n                j_rand = np.random.randint(self.dim)\n                u = np.copy(pop[i])\n                \n                # Adaptive CR: perturb CR with a small probability\n                CR_current = self.CR\n                if np.random.rand() < 0.1:\n                    CR_current = np.random.normal(self.CR, 0.1)\n                    CR_current = np.clip(CR_current, 0.1, 1.0) #Keep within reasonable bounds\n                \n                for j in range(self.dim):\n                    if np.random.rand() < CR_current or j == j_rand:\n                        u[j] = v[j]\n\n                # Selection\n                f_u = func(u)\n                self.budget -= 1\n\n                if f_u < fitness[i]:\n                    fitness[i] = f_u\n                    pop[i] = u\n\n                    if f_u < self.f_opt:\n                        self.f_opt = f_u\n                        self.x_opt = u\n                        no_improvement_count = 0 # Reset counter\n                    else:\n                        no_improvement_count += 1\n                else:\n                    no_improvement_count += 1\n\n            # Restart mechanism\n            if no_improvement_count > self.restart_patience:\n                pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                fitness = np.array([func(x) for x in pop])\n                self.budget -= self.pop_size\n                \n                best_idx = np.argmin(fitness)\n                self.f_opt = fitness[best_idx]\n                self.x_opt = pop[best_idx]\n                no_improvement_count = 0  # Reset counter\n            \n            iteration += 1\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveDE scored 0.629 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.18519657912281484, 0.5148637146987901, 0.7256245551256095, 0.873352957135056, 0.7307992084007313, 0.7543094272504847, 0.34479479827947346, 0.5263324131030589, 0.7490310230668658, 0.5475039970750044, 0.8616713876493778, 0.9971400824769371, 0.6306560389163707, 0.5142958338089685, 0.9255614186546374, 0.7611997937152485, 0.46658170444319114, 0.8395698459768648, 0]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "b2982b85-a98d-47c0-b280-36b73d533f3f", "fitness": 0.0, "name": "DecayingPopulationAdaptiveDE", "description": "A differential evolution strategy with a decaying population size and adaptive mutation/crossover based on the success rate of recent generations.", "code": "import numpy as np\n\nclass DecayingPopulationAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=100, F=0.5, CR=0.9, decay_rate=0.99):\n        \"\"\"\n        Initializes the Decaying Population Adaptive Differential Evolution optimizer.\n\n        Args:\n            budget (int): The maximum number of function evaluations.\n            dim (int): The dimensionality of the search space.\n            pop_size (int): Initial population size.\n            F (float): Initial mutation factor.\n            CR (float): Initial crossover rate.\n            decay_rate (float): Rate at which the population size decays.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.initial_pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.decay_rate = decay_rate\n        self.pop = None\n        self.fitness = None\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.success_history = []  # Store success rate of recent generations\n\n\n    def initialize_population(self, func):\n        \"\"\"Initializes the population randomly within the bounds.\"\"\"\n        self.pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.zeros(self.pop_size)\n        for i in range(self.pop_size):\n            self.fitness[i] = func(self.pop[i])\n            self.eval_count += 1\n            if self.fitness[i] < self.f_opt:\n                self.f_opt = self.fitness[i]\n                self.x_opt = self.pop[i]\n\n    def mutate(self, pop, F):\n        \"\"\"Performs the mutation operation.\"\"\"\n        mutated_pop = np.zeros_like(pop)\n        for i in range(self.pop_size):\n            idxs = [idx for idx in range(self.pop_size) if idx != i]\n            a, b, c = np.random.choice(idxs, 3, replace=False)\n            mutated_pop[i] = pop[a] + F * (pop[b] - pop[c])\n        return mutated_pop\n\n    def crossover(self, pop, mutated_pop, CR):\n        \"\"\"Performs the crossover operation.\"\"\"\n        crossed_pop = np.zeros_like(pop)\n        for i in range(self.pop_size):\n            for j in range(self.dim):\n                if np.random.rand() < CR:\n                    crossed_pop[i, j] = mutated_pop[i, j]\n                else:\n                    crossed_pop[i, j] = pop[i, j]\n        return crossed_pop\n\n    def selection(self, func, crossed_pop):\n        \"\"\"Performs the selection operation and updates success history.\"\"\"\n        improvements = 0\n        for i in range(self.pop_size):\n            f = func(crossed_pop[i])\n            self.eval_count += 1\n            if f < self.fitness[i]:\n                self.pop[i] = crossed_pop[i]\n                self.fitness[i] = f\n                if f < self.f_opt:\n                    self.f_opt = f\n                    self.x_opt = crossed_pop[i]\n                improvements += 1\n        return improvements/self.pop_size  #Return success rate\n\n    def adapt_parameters(self):\n        \"\"\"Adapts the mutation and crossover rates based on recent success.\"\"\"\n        if not self.success_history:\n            return\n\n        recent_success = np.mean(self.success_history[-5:]) #Average of last 5 generations\n\n        if recent_success > 0.4:\n            self.CR = max(self.CR * 0.95, 0.1)\n            self.F = min(self.F * 1.05, 0.9)\n        elif recent_success < 0.2:\n            self.CR = min(self.CR * 1.05, 0.9)\n            self.F = max(self.F * 0.95, 0.1)\n\n\n    def decay_population(self):\n        \"\"\"Reduces the population size.\"\"\"\n        self.pop_size = int(self.initial_pop_size * (self.decay_rate ** (self.eval_count / self.budget)))\n        self.pop_size = max(10, self.pop_size)  # Ensure a minimum population size\n\n        # Keep the best individuals\n        sorted_indices = np.argsort(self.fitness)[:self.pop_size]\n        self.pop = self.pop[sorted_indices]\n        self.fitness = self.fitness[sorted_indices]\n\n        # Refill the population if necessary\n        if self.pop.shape[0] < self.pop_size:\n          num_to_add = self.pop_size - self.pop.shape[0]\n          new_individuals = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(num_to_add, self.dim))\n          new_fitness = np.zeros(num_to_add)\n          for i in range(num_to_add):\n            new_fitness[i] = func(new_individuals[i])\n            self.eval_count += 1\n            if new_fitness[i] < self.f_opt:\n              self.f_opt = new_fitness[i]\n              self.x_opt = new_individuals[i]\n\n          self.pop = np.vstack((self.pop, new_individuals))\n          self.fitness = np.concatenate((self.fitness, new_fitness))\n          \n\n    def __call__(self, func):\n        \"\"\"Optimizes the given function using Decaying Population Adaptive Differential Evolution.\"\"\"\n        self.initialize_population(func)\n\n        while self.eval_count < self.budget:\n            mutated_pop = self.mutate(self.pop, self.F)\n            crossed_pop = self.crossover(self.pop, mutated_pop, self.CR)\n            crossed_pop = np.clip(crossed_pop, func.bounds.lb, func.bounds.ub)\n\n            success_rate = self.selection(func, crossed_pop)\n            self.success_history.append(success_rate)\n            self.adapt_parameters()\n            self.decay_population()\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "The algorithm DecayingPopulationAdaptiveDE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["3b6588b5-7d9d-46dc-a57f-04fd0fa2fbbe"], "operator": null, "metadata": {"aucs": [0]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "ce793343-56c3-4407-afe4-b814070ddbed", "fitness": "-inf", "name": "AdaptiveDE", "description": "An adaptive differential evolution strategy with self-adaptive parameters, orthogonal learning to improve population diversity, and a restart mechanism.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, CR=0.9, restart_patience=500, orthogonal_learning_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F  # Initial mutation factor\n        self.CR = CR  # Initial crossover rate\n        self.restart_patience = restart_patience\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.orthogonal_learning_rate = orthogonal_learning_rate #Rate of orthogonal learning.\n\n    def __call__(self, func):\n        # Initialize population\n        pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in pop])\n        self.budget -= self.pop_size\n        \n        best_idx = np.argmin(fitness)\n        self.f_opt = fitness[best_idx]\n        self.x_opt = pop[best_idx]\n\n        iteration = 0\n        no_improvement_count = 0\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = pop[idxs]\n                \n                # Adaptive F: perturb F with a small probability\n                F_current = self.F\n                if np.random.rand() < 0.1:\n                    F_current = np.random.normal(self.F, 0.1)\n                    F_current = np.clip(F_current, 0.1, 1.0) #Keep within reasonable bounds\n                \n                v = x_r1 + F_current * (x_r2 - x_r3)\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)  # Keep within bounds\n\n                # Crossover\n                j_rand = np.random.randint(self.dim)\n                u = np.copy(pop[i])\n                \n                # Adaptive CR: perturb CR with a small probability\n                CR_current = self.CR\n                if np.random.rand() < 0.1:\n                    CR_current = np.random.normal(self.CR, 0.1)\n                    CR_current = np.clip(CR_current, 0.1, 1.0) #Keep within reasonable bounds\n                \n                for j in range(self.dim):\n                    if np.random.rand() < CR_current or j == j_rand:\n                        u[j] = v[j]\n\n                # Selection\n                f_u = func(u)\n                self.budget -= 1\n\n                if f_u < fitness[i]:\n                    fitness[i] = f_u\n                    pop[i] = u\n\n                    if f_u < self.f_opt:\n                        self.f_opt = f_u\n                        self.x_opt = u\n                        no_improvement_count = 0 # Reset counter\n                    else:\n                        no_improvement_count += 1\n                else:\n                    no_improvement_count += 1\n            \n            #Orthogonal Learning\n            if np.random.rand() < self.orthogonal_learning_rate:\n                self.orthogonal_learning(func, pop, fitness)\n\n\n            # Restart mechanism\n            if no_improvement_count > self.restart_patience:\n                pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                fitness = np.array([func(x) for x in pop])\n                self.budget -= self.pop_size\n                \n                best_idx = np.argmin(fitness)\n                self.f_opt = fitness[best_idx]\n                self.x_opt = pop[best_idx]\n                no_improvement_count = 0  # Reset counter\n            \n            iteration += 1\n        return self.f_opt, self.x_opt\n\n    def orthogonal_learning(self, func, pop, fitness):\n        \"\"\"\n        Implements orthogonal learning to improve population diversity and potentially find better solutions.\n        \"\"\"\n        # Select a random individual from the population\n        idx = np.random.randint(self.pop_size)\n        x = pop[idx]\n\n        # Generate an orthogonal array (simplified version)\n        # Here, we create two new points based on perturbing each dimension\n        x_plus = np.copy(x)\n        x_minus = np.copy(x)\n\n        for j in range(self.dim):\n            delta = 0.1 * (func.bounds.ub - func.bounds.lb)  # Small perturbation\n            x_plus[j] = min(func.bounds.ub, x[j] + delta)\n            x_minus[j] = max(func.bounds.lb, x[j] - delta)\n\n        # Evaluate the new points\n        f_plus = func(x_plus)\n        self.budget -= 1\n        f_minus = func(x_minus)\n        self.budget -= 1\n\n        # Replace the current individual if either new point is better\n        if f_plus < fitness[idx]:\n            fitness[idx] = f_plus\n            pop[idx] = x_plus\n            if f_plus < self.f_opt:\n                self.f_opt = f_plus\n                self.x_opt = x_plus\n        elif f_minus < fitness[idx]:\n            fitness[idx] = f_minus\n            pop[idx] = x_minus\n            if f_minus < self.f_opt:\n                self.f_opt = f_minus\n                self.x_opt = x_minus", "configspace": "", "generation": 1, "feedback": "In the code, line 108, in orthogonal_learning, the following error occurred:\nValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\nOn line: x_plus[j] = min(func.bounds.ub, x[j] + delta)", "error": "In the code, line 108, in orthogonal_learning, the following error occurred:\nValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\nOn line: x_plus[j] = min(func.bounds.ub, x[j] + delta)", "parent_ids": ["d83e47fb-f808-4342-af73-25d2c79f9fac"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "177f1644-5b82-4808-91b9-224ebc51bb8e", "fitness": "-inf", "name": "HybridPSO_DE", "description": "Combines PSO and DE with self-adaptive parameter control and a local search phase triggered by stagnation detection to enhance exploration and exploitation.", "code": "import numpy as np\n\nclass HybridPSO_DE:\n    def __init__(self, budget=10000, dim=10, pop_size=20, pso_weight_start=0.9, pso_weight_end=0.4, de_cr_start=0.7, de_cr_end=0.9, de_f_start=0.6, de_f_end=0.8, stagnation_threshold=100):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.pso_weight_start = pso_weight_start\n        self.pso_weight_end = pso_weight_end\n        self.de_cr_start = de_cr_start\n        self.de_cr_end = de_cr_end\n        self.de_f_start = de_f_start\n        self.de_f_end = de_f_end\n        self.population = None\n        self.fitness = None\n        self.velocities = None\n        self.best_positions = None\n        self.global_best_fitness = np.inf\n        self.global_best_position = None\n        self.eval_count = 0\n        self.stagnation_counter = 0\n        self.stagnation_threshold = stagnation_threshold\n        self.previous_best_fitness = np.inf\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.eval_count += self.pop_size\n        self.velocities = np.random.uniform(-1, 1, size=(self.pop_size, self.dim)) * 0.1\n        self.best_positions = self.population.copy()\n        self.global_best_position = self.population[np.argmin(self.fitness)].copy()\n        self.global_best_fitness = np.min(self.fitness)\n        self.previous_best_fitness = self.global_best_fitness\n\n    def pso_update(self, func):\n        cognitive_weight = 1.5\n        social_weight = 1.5\n        # Linearly decreasing inertia weight\n        pso_weight = self.pso_weight_start - (self.pso_weight_start - self.pso_weight_end) * (self.eval_count / self.budget)\n\n\n        r1 = np.random.rand(self.pop_size, self.dim)\n        r2 = np.random.rand(self.pop_size, self.dim)\n\n        cognitive_component = cognitive_weight * r1 * (self.best_positions - self.population)\n        social_component = social_weight * r2 * (self.global_best_position - self.population)\n\n        self.velocities = pso_weight * self.velocities + cognitive_component + social_component\n        self.population = self.population + self.velocities\n\n        # Clip positions to bounds\n        self.population = np.clip(self.population, func.bounds.lb, func.bounds.ub)\n\n        new_fitness = np.array([func(x) for x in self.population])\n        self.eval_count += self.pop_size\n\n        # Update personal best positions\n        for i in range(self.pop_size):\n            if new_fitness[i] < self.fitness[i]:\n                self.best_positions[i] = self.population[i].copy()\n                self.fitness[i] = new_fitness[i]\n\n        # Update global best position\n        best_index = np.argmin(self.fitness)\n        if self.fitness[best_index] < self.global_best_fitness:\n            self.global_best_fitness = self.fitness[best_index]\n            self.global_best_position = self.population[best_index].copy()\n\n\n    def de_update(self, func):\n        # Linearly increasing crossover rate and mutation factor\n        de_cr = self.de_cr_start + (self.de_cr_end - self.de_cr_start) * (self.eval_count / self.budget)\n        de_f = self.de_f_start + (self.de_f_end - self.de_f_start) * (self.eval_count / self.budget)\n\n\n        for i in range(self.pop_size):\n            # Choose three random indices, distinct from each other and i\n            idxs = np.random.choice(self.pop_size, 3, replace=False)\n            while i in idxs:\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n\n            x1, x2, x3 = self.population[idxs]\n\n            # Mutation\n            v = x1 + de_f * (x2 - x3)\n            v = np.clip(v, func.bounds.lb, func.bounds.ub)\n\n            # Crossover\n            u = np.copy(self.population[i])\n            j_rand = np.random.randint(self.dim)\n            for j in range(self.dim):\n                if np.random.rand() < de_cr or j == j_rand:\n                    u[j] = v[j]\n\n            # Selection\n            f_u = func(u)\n            self.eval_count += 1\n\n            if f_u < self.fitness[i]:\n                self.population[i] = u\n                self.fitness[i] = f_u\n\n                if f_u < self.global_best_fitness:\n                    self.global_best_fitness = f_u\n                    self.global_best_position = u.copy()\n            elif self.eval_count >= self.budget:\n                break\n\n    def local_search(self, func, individual, step_size=0.1, num_steps=10):\n        \"\"\"Performs a simple local search around an individual.\"\"\"\n        best_fitness = func(individual)\n        best_position = individual.copy()\n        self.eval_count += 1\n\n        for _ in range(num_steps):\n            # Generate a random direction\n            direction = np.random.uniform(-1, 1, size=self.dim)\n            direction = direction / np.linalg.norm(direction)  # Normalize\n\n            # Take a step in that direction\n            new_position = individual + step_size * direction\n            new_position = np.clip(new_position, func.bounds.lb, func.bounds.ub)\n\n            new_fitness = func(new_position)\n            self.eval_count += 1\n\n            if new_fitness < best_fitness:\n                best_fitness = new_fitness\n                best_position = new_position.copy()\n            \n            if self.eval_count >= self.budget:\n                break\n\n        return best_fitness, best_position\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.eval_count < self.budget:\n            if self.eval_count < self.budget * 0.5:\n                self.pso_update(func)\n            else:\n                self.de_update(func)\n\n            # Stagnation Detection\n            if self.global_best_fitness >= self.previous_best_fitness:\n                self.stagnation_counter += 1\n            else:\n                self.stagnation_counter = 0\n\n            # Local Search\n            if self.stagnation_counter >= self.stagnation_threshold:\n                # Perform local search around the best individual\n                best_fitness, best_position = self.local_search(func, self.global_best_position)\n                if best_fitness < self.global_best_fitness:\n                  self.global_best_fitness = best_fitness\n                  self.global_best_position = best_position.copy()\n\n                # Reset stagnation counter\n                self.stagnation_counter = 0\n\n            self.previous_best_fitness = self.global_best_fitness\n\n            if self.eval_count >= self.budget:\n                break\n\n        return self.global_best_fitness, self.global_best_position", "configspace": "", "generation": 1, "feedback": "Evaluation timed out after 300 seconds.", "error": "Evaluation timed out after 300 seconds.", "parent_ids": ["ad0f6409-cff0-4bc8-87da-01a088bef245"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "dd65cebb-1f7b-4243-b33c-476d9a53195e", "fitness": 0.0, "name": "AdaptiveDifferentialEvolution", "description": "An adaptive differential evolution strategy that incorporates a local search component and dynamically adjusts parameters based on success history and population diversity.", "code": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, CR=0.9, local_search_prob=0.1):\n        \"\"\"\n        Initializes the Adaptive Differential Evolution optimizer.\n\n        Args:\n            budget (int): The maximum number of function evaluations.\n            dim (int): The dimensionality of the search space.\n            pop_size (int): The population size.\n            F (float): Initial mutation factor.\n            CR (float): Initial crossover rate.\n            local_search_prob (float): Probability of applying local search.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.local_search_prob = local_search_prob\n        self.pop = None\n        self.fitness = None\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.successful_cr = []\n        self.successful_f = []\n\n    def initialize_population(self, func):\n        \"\"\"Initializes the population randomly within the bounds.\"\"\"\n        self.pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.zeros(self.pop_size)\n        for i in range(self.pop_size):\n            self.fitness[i] = func(self.pop[i])\n            self.eval_count += 1\n            if self.fitness[i] < self.f_opt:\n                self.f_opt = self.fitness[i]\n                self.x_opt = self.pop[i]\n\n    def mutate(self, pop, F):\n        \"\"\"Performs the mutation operation.\"\"\"\n        mutated_pop = np.zeros_like(pop)\n        for i in range(self.pop_size):\n            idxs = [idx for idx in range(self.pop_size) if idx != i]\n            a, b, c = np.random.choice(idxs, 3, replace=False)\n            mutated_pop[i] = pop[a] + F * (pop[b] - pop[c])\n        return mutated_pop\n\n    def crossover(self, pop, mutated_pop, CR):\n        \"\"\"Performs the crossover operation.\"\"\"\n        crossed_pop = np.zeros_like(pop)\n        for i in range(self.pop_size):\n            for j in range(self.dim):\n                if np.random.rand() < CR:\n                    crossed_pop[i, j] = mutated_pop[i, j]\n                else:\n                    crossed_pop[i, j] = pop[i, j]\n        return crossed_pop\n\n    def local_search(self, func, x):\n        \"\"\"Performs a local search around the given solution.\"\"\"\n        delta = np.random.uniform(-0.1, 0.1, size=self.dim)  # Small perturbation\n        x_new = x + delta\n        x_new = np.clip(x_new, func.bounds.lb, func.bounds.ub)\n        f_new = func(x_new)\n        self.eval_count += 1\n        return x_new, f_new\n\n    def selection(self, func, crossed_pop):\n        \"\"\"Performs the selection operation.\"\"\"\n        for i in range(self.pop_size):\n            f = func(crossed_pop[i])\n            self.eval_count += 1\n            \n            # Local Search with probability\n            if np.random.rand() < self.local_search_prob:\n                x_local, f_local = self.local_search(func, crossed_pop[i])\n                if f_local < f:\n                    f = f_local\n                    crossed_pop[i] = x_local\n\n            if f < self.fitness[i]:\n                \n                self.successful_cr.append(self.CR)\n                self.successful_f.append(self.F)\n                \n                self.pop[i] = crossed_pop[i]\n                self.fitness[i] = f\n                if f < self.f_opt:\n                    self.f_opt = f\n                    self.x_opt = crossed_pop[i]\n\n    def adapt_parameters(self):\n        \"\"\"Adapts the mutation and crossover rates based on population diversity and success.\"\"\"\n        # Adjust F: If the population is too homogeneous, increase F\n        if np.std(self.fitness) < 1e-6:  # Population is very similar\n            self.F = min(self.F * 1.2, 1.0)  # Increase F\n        else:\n            self.F = 0.5\n        \n        if self.successful_cr:\n            self.CR = np.mean(self.successful_cr)\n            self.F = np.mean(self.successful_f)\n            self.successful_cr = []\n            self.successful_f = []\n        else:\n            self.CR = 0.9\n            self.F = 0.5\n\n        self.CR = np.clip(self.CR, 0.1, 0.9)\n        self.F = np.clip(self.F, 0.1, 1.0)\n\n    def __call__(self, func):\n        \"\"\"Optimizes the given function using Adaptive Differential Evolution.\"\"\"\n        self.initialize_population(func)\n\n        while self.eval_count < self.budget:\n            mutated_pop = self.mutate(self.pop, self.F)\n            crossed_pop = self.crossover(self.pop, mutated_pop, self.CR)\n\n            # Clip the values to stay within bounds\n            crossed_pop = np.clip(crossed_pop, func.bounds.lb, func.bounds.ub)\n\n            self.selection(func, crossed_pop)\n            self.adapt_parameters()\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "The algorithm AdaptiveDifferentialEvolution scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["3b6588b5-7d9d-46dc-a57f-04fd0fa2fbbe"], "operator": null, "metadata": {"aucs": [0]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "9ea2467f-3d16-44f7-806c-d37b73ed7a77", "fitness": 0.4501697595649246, "name": "EnhancedAdaptiveDifferentialEvolution", "description": "An enhanced Adaptive Differential Evolution with aging and a multi-strategy mutation to boost exploration and exploitation.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDifferentialEvolution:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, CR=0.9, age_limit=50):\n        \"\"\"\n        Initialize the Enhanced Adaptive Differential Evolution algorithm.\n\n        Args:\n            budget (int): The maximum number of function evaluations.\n            dim (int): The dimensionality of the search space.\n            pop_size (int): The population size.\n            F (float): The initial scaling factor.\n            CR (float): The initial crossover rate.\n            age_limit (int): Maximum age before restarting an individual.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.pop = None\n        self.fitness = None\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.lb = None\n        self.ub = None\n        self.age = None  # Track the age of each individual\n        self.age_limit = age_limit\n\n    def initialize_population(self, func):\n        \"\"\"\n        Initialize the population with random values within the bounds.\n        \"\"\"\n        self.lb = func.bounds.lb\n        self.ub = func.bounds.ub\n        self.pop = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.eval_count += self.pop_size\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.pop[np.argmin(self.fitness)]\n        self.age = np.zeros(self.pop_size, dtype=int)\n\n\n    def mutate(self, i):\n        \"\"\"\n        Perform mutation operation with multiple strategies.\n        \"\"\"\n        strategy = np.random.choice(['current-to-rand', 'rand-to-best', 'best-to-current'])\n\n        if strategy == 'current-to-rand':\n            idxs = np.random.choice(self.pop_size, 3, replace=False)\n            x_r1, x_r2, x_r3 = self.pop[idxs]\n            v = self.pop[i] + self.F * (x_r1 - self.pop[i]) + self.F * (x_r2 - x_r3)\n        elif strategy == 'rand-to-best':\n            idxs = np.random.choice(self.pop_size, 2, replace=False)\n            x_r1, x_r2 = self.pop[idxs]\n            v = self.pop[i] + self.F * (self.x_opt - self.pop[i]) + self.F * (x_r1 - x_r2)\n        else:  # 'best-to-current'\n            idxs = np.random.choice(self.pop_size, 2, replace=False)\n            x_r1, x_r2 = self.pop[idxs]\n            v = self.x_opt + self.F * (self.pop[i] - self.x_opt) + self.F * (x_r1 - x_r2)\n        \n        v = np.clip(v, self.lb, self.ub)\n        return v\n\n    def crossover(self, mutant, i):\n        \"\"\"\n        Perform crossover operation.\n        \"\"\"\n        u = np.copy(self.pop[i])\n        j_rand = np.random.randint(self.dim)\n        for j in range(self.dim):\n            if np.random.rand() < self.CR or j == j_rand:\n                u[j] = mutant[j]\n        return u\n\n    def selection(self, func, u, i):\n        \"\"\"\n        Perform selection operation.\n        \"\"\"\n        f_u = func(u)\n        self.eval_count += 1\n\n        if f_u < self.fitness[i]:\n            self.pop[i] = u\n            self.fitness[i] = f_u\n            self.age[i] = 0  # Reset age if improved\n\n            if f_u < self.f_opt:\n                self.f_opt = f_u\n                self.x_opt = u\n        else:\n            self.age[i] += 1  # Increase age if not improved\n\n\n    def adapt_parameters(self, gen):\n        \"\"\"\n        Adapt the parameters F and CR based on the generation number.\n        \"\"\"\n        self.F = 0.5 + 0.4 * np.exp(-gen / 100) # Decaying F\n        self.CR = 0.1 + 0.8 * np.sin(gen / 200) # Oscillating CR\n\n\n    def restart_individual(self, func, i):\n        \"\"\"\n        Restart an individual if it's too old.\n        \"\"\"\n        self.pop[i] = np.random.uniform(self.lb, self.ub, size=self.dim)\n        self.fitness[i] = func(self.pop[i])\n        self.eval_count += 1\n        self.age[i] = 0\n\n        if self.fitness[i] < self.f_opt:\n            self.f_opt = self.fitness[i]\n            self.x_opt = self.pop[i]\n\n    def __call__(self, func):\n        \"\"\"\n        Optimize the given function using Adaptive Differential Evolution.\n\n        Args:\n            func (callable): The function to optimize.\n\n        Returns:\n            tuple: The best function value found and the corresponding solution.\n        \"\"\"\n        self.initialize_population(func)\n\n        gen = 0 \n\n        while self.eval_count < self.budget:\n            \n            for i in range(self.pop_size):\n                if self.eval_count >= self.budget:\n                    break\n                \n                mutant = self.mutate(i)\n                u = self.crossover(mutant, i)\n                self.selection(func, u, i)\n                \n                if self.age[i] > self.age_limit:\n                    self.restart_individual(func, i)\n                \n            gen += 1\n            self.adapt_parameters(gen) # Adapt every generation.\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "The algorithm EnhancedAdaptiveDifferentialEvolution scored 0.450 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["bd6b3c28-b984-4fa3-be0f-156d74b61953"], "operator": null, "metadata": {"aucs": [0.17984633562946073, 0.3057394401085821, 0.5457530339471505, 0.6968236077321878, 0.5626713109120618, 0.677592559101905, 0.3572542332617066, 0.5096473421052915, 0.5950278953798684, 0.3266967099170729, 0.6449846466838084, 0]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "4da23f0e-6f00-43ef-a7d4-f6df57870753", "fitness": 0.660075314534043, "name": "EnhancedAdaptiveDE", "description": "Enhanced Adaptive Differential Evolution with orthogonal learning and covariance matrix adaptation for improved exploration and exploitation.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, CR=0.9, restart_patience=500, orthogonal_learning_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F  # Initial mutation factor\n        self.CR = CR  # Initial crossover rate\n        self.restart_patience = restart_patience\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.orthogonal_learning_rate = orthogonal_learning_rate\n        self.covariance_matrix = np.eye(dim)  # Initialize covariance matrix\n\n    def __call__(self, func):\n        # Initialize population\n        pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in pop])\n        self.budget -= self.pop_size\n        \n        best_idx = np.argmin(fitness)\n        self.f_opt = fitness[best_idx]\n        self.x_opt = pop[best_idx]\n\n        iteration = 0\n        no_improvement_count = 0\n\n        while self.budget > 0:\n            # Covariance Matrix Adaptation\n            if iteration % 10 == 0:\n                 self.covariance_matrix = np.cov(pop.T) + np.eye(self.dim) * 1e-6\n\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = pop[idxs]\n                \n                # Adaptive F: perturb F with a small probability\n                F_current = self.F\n                if np.random.rand() < 0.1:\n                    F_current = np.random.normal(self.F, 0.1)\n                    F_current = np.clip(F_current, 0.1, 1.0) #Keep within reasonable bounds\n\n                # Sample mutation vector from a multivariate normal distribution\n                mutation_vector = np.random.multivariate_normal(np.zeros(self.dim), self.covariance_matrix)\n                v = x_r1 + F_current * mutation_vector\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)  # Keep within bounds\n\n                # Crossover\n                j_rand = np.random.randint(self.dim)\n                u = np.copy(pop[i])\n                \n                # Adaptive CR: perturb CR with a small probability\n                CR_current = self.CR\n                if np.random.rand() < 0.1:\n                    CR_current = np.random.normal(self.CR, 0.1)\n                    CR_current = np.clip(CR_current, 0.1, 1.0) #Keep within reasonable bounds\n                \n                for j in range(self.dim):\n                    if np.random.rand() < CR_current or j == j_rand:\n                        u[j] = v[j]\n\n                # Orthogonal Learning\n                if np.random.rand() < self.orthogonal_learning_rate:\n                    direction = self.x_opt - u\n                    u = u + np.random.rand() * direction\n\n                u = np.clip(u, func.bounds.lb, func.bounds.ub)\n\n                # Selection\n                f_u = func(u)\n                self.budget -= 1\n\n                if f_u < fitness[i]:\n                    fitness[i] = f_u\n                    pop[i] = u\n\n                    if f_u < self.f_opt:\n                        self.f_opt = f_u\n                        self.x_opt = u\n                        no_improvement_count = 0 # Reset counter\n                    else:\n                        no_improvement_count += 1\n                else:\n                    no_improvement_count += 1\n\n            # Restart mechanism\n            if no_improvement_count > self.restart_patience:\n                pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                fitness = np.array([func(x) for x in pop])\n                self.budget -= self.pop_size\n                \n                best_idx = np.argmin(fitness)\n                self.f_opt = fitness[best_idx]\n                self.x_opt = pop[best_idx]\n                no_improvement_count = 0  # Reset counter\n                self.covariance_matrix = np.eye(self.dim)\n            \n            iteration += 1\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "The algorithm EnhancedAdaptiveDE scored 0.660 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["d83e47fb-f808-4342-af73-25d2c79f9fac"], "operator": null, "metadata": {"aucs": [0.27195437833472635, 0.7282588384258526, 0.6307039234576924, 0.8893853711418888, 0.7002280046136955, 0.7062434947665889, 0.4550043654244208, 0.5732119170415102, 0.6744752934773963, 0.6657198947542267, 0.8836628650849728, 0.9914711656673197, 0.49260217896849123, 0.6564054061630655, 0.9265463475101985, 0.6939144222660718, 0.5468880971998893, 0.8596020952292313, 0.2889149526567196, 0.5663132784969]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "7d60114b-d8ea-4a75-83d4-f176f20d45fe", "fitness": 0.43829254180881716, "name": "AdaptiveDifferentialEvolution", "description": "Self-adjusting Differential Evolution with orthogonal learning, population diversity maintenance, and a restart strategy.", "code": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, CR=0.9, ortho_group_size=5):\n        \"\"\"\n        Initialize the Adaptive Differential Evolution algorithm.\n\n        Args:\n            budget (int): The maximum number of function evaluations.\n            dim (int): The dimensionality of the search space.\n            pop_size (int): The population size.\n            F (float): The initial scaling factor.\n            CR (float): The initial crossover rate.\n            ortho_group_size (int): Number of individuals for orthogonal learning.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.pop = None\n        self.fitness = None\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.lb = None\n        self.ub = None\n        self.ortho_group_size = ortho_group_size\n\n    def initialize_population(self, func):\n        \"\"\"\n        Initialize the population with random values within the bounds.\n        \"\"\"\n        self.lb = func.bounds.lb\n        self.ub = func.bounds.ub\n        self.pop = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.eval_count += self.pop_size\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.pop[np.argmin(self.fitness)]\n\n\n    def mutate(self, i):\n        \"\"\"\n        Perform mutation operation with self-adjustment.\n        \"\"\"\n        idxs = np.random.choice(self.pop_size, 3, replace=False)\n        x_r1, x_r2, x_r3 = self.pop[idxs]\n        \n        # Self-adjusting F based on individual success\n        if self.fitness[i] < np.mean(self.fitness):\n            F_adaptive = self.F * np.random.uniform(0.5, 1.5)\n        else:\n            F_adaptive = self.F * np.random.uniform(0.0, 0.5)\n        \n        v = self.pop[i] + F_adaptive * (x_r2 - x_r3)\n        v = np.clip(v, self.lb, self.ub)\n        return v\n\n    def crossover(self, mutant, i):\n        \"\"\"\n        Perform crossover operation.\n        \"\"\"\n        u = np.copy(self.pop[i])\n        j_rand = np.random.randint(self.dim)\n        for j in range(self.dim):\n            if np.random.rand() < self.CR or j == j_rand:\n                u[j] = mutant[j]\n        return u\n\n    def selection(self, func, u, i):\n        \"\"\"\n        Perform selection operation.\n        \"\"\"\n        f_u = func(u)\n        self.eval_count += 1\n\n        if f_u < self.fitness[i]:\n            self.pop[i] = u\n            self.fitness[i] = f_u\n\n            if f_u < self.f_opt:\n                self.f_opt = f_u\n                self.x_opt = u\n                \n    def orthogonal_learning(self, func):\n        \"\"\"\n        Perform orthogonal learning to improve the population.\n        \"\"\"\n        indices = np.random.choice(self.pop_size, size=self.ortho_group_size, replace=False)\n        group = self.pop[indices]\n        \n        # Calculate the centroid of the group\n        centroid = np.mean(group, axis=0)\n        \n        # Generate new candidate solutions around the centroid\n        for i in range(self.ortho_group_size):\n            candidate = centroid + np.random.normal(0, 0.1, size=self.dim)  # Small perturbation\n            candidate = np.clip(candidate, self.lb, self.ub)\n            f_candidate = func(candidate)\n            self.eval_count += 1\n            \n            if f_candidate < self.fitness[indices[i]]:\n                self.pop[indices[i]] = candidate\n                self.fitness[indices[i]] = f_candidate\n                \n                if f_candidate < self.f_opt:\n                    self.f_opt = f_candidate\n                    self.x_opt = candidate\n\n    def restart(self, func):\n        \"\"\"\n        Restart the population if no improvement has been made for a certain number of iterations.\n        \"\"\"\n        self.pop = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.eval_count += self.pop_size\n        \n        current_best_idx = np.argmin(self.fitness)\n        if self.fitness[current_best_idx] < self.f_opt:\n            self.f_opt = self.fitness[current_best_idx]\n            self.x_opt = self.pop[current_best_idx]\n            \n    def check_diversity(self):\n        \"\"\"\n        Check the population diversity and increase mutation rate if low diversity.\n        \"\"\"\n        distances = np.linalg.norm(self.pop - np.mean(self.pop, axis=0), axis=1)\n        diversity_metric = np.mean(distances)\n        \n        if diversity_metric < 0.1 * (self.ub[0] - self.lb[0]):  # Check against a fraction of the search space range\n            self.F = min(self.F * 1.2, 1.0)  # Increase mutation rate\n\n    def __call__(self, func):\n        \"\"\"\n        Optimize the given function using Adaptive Differential Evolution.\n\n        Args:\n            func (callable): The function to optimize.\n\n        Returns:\n            tuple: The best function value found and the corresponding solution.\n        \"\"\"\n        self.initialize_population(func)\n        no_improvement_count = 0\n        best_f_so_far = self.f_opt\n\n        while self.eval_count < self.budget:\n            for i in range(self.pop_size):\n                if self.eval_count >= self.budget:\n                    break\n                \n                mutant = self.mutate(i)\n                u = self.crossover(mutant, i)\n                self.selection(func, u, i)\n            \n            if self.f_opt < best_f_so_far:\n                best_f_so_far = self.f_opt\n                no_improvement_count = 0\n            else:\n                no_improvement_count += 1\n\n            if no_improvement_count > 150:\n                self.restart(func)\n                no_improvement_count = 0\n                \n            if self.eval_count < self.budget:\n                 self.orthogonal_learning(func)\n                 self.check_diversity()\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "The algorithm AdaptiveDifferentialEvolution scored 0.438 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["bd6b3c28-b984-4fa3-be0f-156d74b61953"], "operator": null, "metadata": {"aucs": [0.1617171461994945, 0.3330088353232322, 0.406558746415254, 0.6019060403250168, 0.3407911624542307, 0.44741571304666405, 0.30906605112842056, 0.37541884827010996, 0.35110742687546925, 0.19674957706519203, 0.66375565744978, 0.9966269515107739, 0.4047042231767225, 0.36205738712124835, 0.8004212073840807, 0.4298488877112112, 0.3425706804605717, 0.5516240602287987, 0.20222860588647862, 0.48827362814359343]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "7d9cb530-1ec3-4f38-96b1-6dfba1e93783", "fitness": 0.6591790260499355, "name": "AdaptiveDE", "description": "Adaptive Differential Evolution with orthogonal learning and covariance matrix adaptation for enhanced exploration and exploitation.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, CR=0.9, restart_patience=500, ol_patience=100):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.restart_patience = restart_patience\n        self.ol_patience = ol_patience\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.rng = np.random.default_rng() # Use numpy random generator\n\n    def orthogonal_learning(self, pop, fitness, func):\n        \"\"\"\n        Performs orthogonal learning to generate a promising candidate.\n        \"\"\"\n        best_idx = np.argmin(fitness)\n        x_best = pop[best_idx]\n        \n        # Generate orthogonal array (simplified - can be replaced with a proper OA library if needed)\n        oa = np.array([[1, 1], [1, -1], [-1, 1], [-1, -1]]) # Very simple 2-factor OA\n        \n        candidate = np.copy(x_best)\n        for i in range(min(self.dim, oa.shape[0])): # Iterate until dimension exhausted\n            # Perturb each dimension based on OA\n            perturbed_candidate = x_best + 0.05 * oa[i % oa.shape[0]] * (func.bounds.ub - func.bounds.lb) # scale perturbation\n            perturbed_candidate = np.clip(perturbed_candidate, func.bounds.lb, func.bounds.ub)\n            \n            f_perturbed = func(perturbed_candidate)\n            self.budget -= 1\n            if f_perturbed < self.f_opt:\n                self.f_opt = f_perturbed\n                self.x_opt = perturbed_candidate\n            \n            if f_perturbed < func(candidate):\n                candidate = perturbed_candidate # keep best\n            \n            if self.budget <= 0:\n                break\n        return candidate\n\n    def __call__(self, func):\n        # Initialize population\n        pop = self.rng.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in pop])\n        self.budget -= self.pop_size\n\n        best_idx = np.argmin(fitness)\n        self.f_opt = fitness[best_idx]\n        self.x_opt = pop[best_idx]\n\n        iteration = 0\n        no_improvement_count = 0\n        ol_count = 0\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = self.rng.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = pop[idxs]\n\n                # Adaptive F\n                F_current = self.F * (1 + 0.1 * self.rng.normal(0, 1))\n                F_current = np.clip(F_current, 0.1, 1.0)\n\n                v = x_r1 + F_current * (x_r2 - x_r3)\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                j_rand = self.rng.integers(self.dim)\n                u = np.copy(pop[i])\n\n                # Adaptive CR\n                CR_current = self.CR * (1 + 0.1 * self.rng.normal(0, 1))\n                CR_current = np.clip(CR_current, 0.1, 1.0)\n\n                crossover_mask = self.rng.random(self.dim) < CR_current\n                u[crossover_mask] = v[crossover_mask]\n                u[j_rand] = v[j_rand] # Guarantee at least one change\n\n\n                # Selection\n                f_u = func(u)\n                self.budget -= 1\n\n                if f_u < fitness[i]:\n                    fitness[i] = f_u\n                    pop[i] = u\n\n                    if f_u < self.f_opt:\n                        self.f_opt = f_u\n                        self.x_opt = u\n                        no_improvement_count = 0\n                        ol_count = 0\n                    else:\n                        no_improvement_count += 1\n                else:\n                    no_improvement_count += 1\n\n            # Orthogonal Learning\n            if ol_count > self.ol_patience and self.budget > 0:\n              candidate = self.orthogonal_learning(pop, fitness, func)\n              ol_count = 0\n              best_fitness = np.min(fitness)\n              if func(candidate) < best_fitness: #Direct replacement if better\n                best_idx = np.argmax(fitness) #replace worst\n                pop[best_idx] = candidate\n                fitness[best_idx] = func(candidate)\n                self.budget -= 1\n            else:\n                ol_count +=1\n\n            # Restart mechanism\n            if no_improvement_count > self.restart_patience:\n                pop = self.rng.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                fitness = np.array([func(x) for x in pop])\n                self.budget -= self.pop_size\n\n                best_idx = np.argmin(fitness)\n                self.f_opt = fitness[best_idx]\n                self.x_opt = pop[best_idx]\n                no_improvement_count = 0\n                ol_count = 0\n\n            iteration += 1\n            if self.budget <= 0:\n                break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "The algorithm AdaptiveDE scored 0.659 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["d83e47fb-f808-4342-af73-25d2c79f9fac"], "operator": null, "metadata": {"aucs": [0.20388898677092615, 0.7216159730722584, 0.69654539470412, 0.8735324010478311, 0.752039787203268, 0.79805424718975, 0.4103135095079039, 0.5195140959889062, 0.734837931874986, 0.6788007980953961, 0.842448790915641, 0.9924447797938343, 0.46416434391296424, 0.7227943599683455, 0.8931411286322328, 0.7896044704584191, 0.5945434463659625, 0.8361170494460292, 0]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "242dae6c-7039-4e77-b945-1db8b3a90dbc", "fitness": "-inf", "name": "NovelAdaptiveDE", "description": "Adaptive Differential Evolution with Neighborhood Search and Lvy flight mutation for enhanced exploration and exploitation, especially in rugged landscapes.", "code": "import numpy as np\n\nclass NovelAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, CR=0.9, restart_patience=500, levy_exponent=1.5, neighborhood_size=5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F  # Initial mutation factor\n        self.CR = CR  # Initial crossover rate\n        self.restart_patience = restart_patience\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.levy_exponent = levy_exponent\n        self.neighborhood_size = neighborhood_size\n\n\n    def levy_flight(self, step_size=0.01):\n        \"\"\"Generates a Lvy flight step.\"\"\"\n        sigma = (np.math.gamma(1 + self.levy_exponent) * np.sin(np.pi * self.levy_exponent / 2) /\n                 (np.math.gamma((1 + self.levy_exponent) / 2) * self.levy_exponent * (2 ** ((self.levy_exponent - 1) / 2)))) ** (1 / self.levy_exponent)\n        u = np.random.normal(0, sigma, size=self.dim)\n        v = np.random.normal(0, 1, size=self.dim)\n        step = u / (abs(v) ** (1 / self.levy_exponent))\n        return step_size * step\n\n    def __call__(self, func):\n        # Initialize population\n        pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in pop])\n        self.budget -= self.pop_size\n        \n        best_idx = np.argmin(fitness)\n        self.f_opt = fitness[best_idx]\n        self.x_opt = pop[best_idx]\n\n        iteration = 0\n        no_improvement_count = 0\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation - Neighborhood Search and Lvy Flight\n                neighborhood_indices = np.random.choice(self.pop_size, self.neighborhood_size, replace=False)\n                neighborhood = pop[neighborhood_indices]\n                best_neighbor_idx = np.argmin(fitness[neighborhood_indices])\n                x_best_neighbor = neighborhood[best_neighbor_idx]\n\n                idxs = np.random.choice(self.pop_size, 2, replace=False)\n                x_r1, x_r2 = pop[idxs]\n\n                # Adaptive F: perturb F with a small probability\n                F_current = self.F\n                if np.random.rand() < 0.1:\n                    F_current = np.random.normal(self.F, 0.1)\n                    F_current = np.clip(F_current, 0.1, 1.0) #Keep within reasonable bounds\n                \n                levy_step = self.levy_flight()\n                v = x_best_neighbor + F_current * (x_r1 - x_r2) + levy_step # Add levy flight\n\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)  # Keep within bounds\n\n                # Crossover\n                j_rand = np.random.randint(self.dim)\n                u = np.copy(pop[i])\n                \n                # Adaptive CR: perturb CR with a small probability\n                CR_current = self.CR\n                if np.random.rand() < 0.1:\n                    CR_current = np.random.normal(self.CR, 0.1)\n                    CR_current = np.clip(CR_current, 0.1, 1.0) #Keep within reasonable bounds\n                \n                for j in range(self.dim):\n                    if np.random.rand() < CR_current or j == j_rand:\n                        u[j] = v[j]\n                \n                u = np.clip(u, func.bounds.lb, func.bounds.ub)\n\n                # Selection\n                f_u = func(u)\n                self.budget -= 1\n\n                if f_u < fitness[i]:\n                    fitness[i] = f_u\n                    pop[i] = u\n\n                    if f_u < self.f_opt:\n                        self.f_opt = f_u\n                        self.x_opt = u\n                        no_improvement_count = 0 # Reset counter\n                    else:\n                        no_improvement_count += 1\n                else:\n                    no_improvement_count += 1\n\n            # Restart mechanism\n            if no_improvement_count > self.restart_patience:\n                pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                fitness = np.array([func(x) for x in pop])\n                self.budget -= self.pop_size\n                \n                best_idx = np.argmin(fitness)\n                self.f_opt = fitness[best_idx]\n                self.x_opt = pop[best_idx]\n                no_improvement_count = 0  # Reset counter\n            \n            iteration += 1\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "In the code, line 805, in __getattr__, the following error occurred:\nAttributeError: module 'numpy' has no attribute 'math'. Did you mean: 'emath'?", "error": "In the code, line 805, in __getattr__, the following error occurred:\nAttributeError: module 'numpy' has no attribute 'math'. Did you mean: 'emath'?", "parent_ids": ["4da23f0e-6f00-43ef-a7d4-f6df57870753"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "2f6a31f4-7294-4c2a-8c8e-8be0e579e2d4", "fitness": "-inf", "name": "HybridPSO_DE", "description": "Combines PSO and DE with adaptive parameter control and a local search to enhance exploration and exploitation.", "code": "import numpy as np\n\nclass HybridPSO_DE:\n    def __init__(self, budget=10000, dim=10, pop_size=20, pso_weight=0.7, de_cr=0.9, de_f=0.8, local_search_prob=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.pso_weight = pso_weight  # Inertia weight for PSO\n        self.de_cr = de_cr            # Crossover rate for DE\n        self.de_f = de_f              # Mutation factor for DE\n        self.local_search_prob = local_search_prob\n        self.population = None\n        self.fitness = None\n        self.velocities = None\n        self.best_positions = None # Personal best positions for PSO\n        self.global_best_fitness = np.inf\n        self.global_best_position = None\n        self.eval_count = 0\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.eval_count += self.pop_size\n        self.velocities = np.random.uniform(-1, 1, size=(self.pop_size, self.dim)) * 0.1  # Initialize velocities\n        self.best_positions = self.population.copy() # Initialize personal best positions\n        self.global_best_position = self.population[np.argmin(self.fitness)].copy()\n        self.global_best_fitness = np.min(self.fitness)\n\n    def pso_update(self, func):\n        cognitive_weight = 1.5\n        social_weight = 1.5\n\n        r1 = np.random.rand(self.pop_size, self.dim)\n        r2 = np.random.rand(self.pop_size, self.dim)\n\n        cognitive_component = cognitive_weight * r1 * (self.best_positions - self.population)\n        social_component = social_weight * r2 * (self.global_best_position - self.population)\n\n        self.velocities = self.pso_weight * self.velocities + cognitive_component + social_component\n        self.population = self.population + self.velocities\n\n        # Clip positions to bounds\n        self.population = np.clip(self.population, func.bounds.lb, func.bounds.ub)\n\n        new_fitness = np.array([func(x) for x in self.population])\n        self.eval_count += self.pop_size\n\n        # Update personal best positions\n        for i in range(self.pop_size):\n            if new_fitness[i] < self.fitness[i]:\n                self.best_positions[i] = self.population[i].copy()\n                self.fitness[i] = new_fitness[i]\n\n        # Update global best position\n        best_index = np.argmin(self.fitness)\n        if self.fitness[best_index] < self.global_best_fitness:\n            self.global_best_fitness = self.fitness[best_index]\n            self.global_best_position = self.population[best_index].copy()\n\n\n    def de_update(self, func):\n        for i in range(self.pop_size):\n            # Choose three random indices, distinct from each other and i\n            idxs = np.random.choice(self.pop_size, 3, replace=False)\n            while i in idxs:\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n\n            x1, x2, x3 = self.population[idxs]\n\n            # Mutation\n            v = x1 + self.de_f * (x2 - x3)\n            v = np.clip(v, func.bounds.lb, func.bounds.ub)\n\n            # Crossover\n            u = np.copy(self.population[i])\n            j_rand = np.random.randint(self.dim)\n            for j in range(self.dim):\n                if np.random.rand() < self.de_cr or j == j_rand:\n                    u[j] = v[j]\n\n            # Selection\n            f_u = func(u)\n            self.eval_count += 1\n\n            if f_u < self.fitness[i]:\n                self.population[i] = u\n                self.fitness[i] = f_u\n\n                if f_u < self.global_best_fitness:\n                    self.global_best_fitness = f_u\n                    self.global_best_position = u.copy()\n            elif self.eval_count >= self.budget:\n                break\n    \n    def local_search(self, func):\n        # Apply local search around the global best\n        step_size = 0.01 * (func.bounds.ub - func.bounds.lb)  # Small step size\n        for i in range(self.dim):\n            # Perturb each dimension\n            perturbed_position = self.global_best_position.copy()\n            perturbed_position[i] += np.random.uniform(-step_size, step_size)\n            perturbed_position = np.clip(perturbed_position, func.bounds.lb, func.bounds.ub)\n\n            f_perturbed = func(perturbed_position)\n            self.eval_count += 1\n\n            if f_perturbed < self.global_best_fitness:\n                self.global_best_fitness = f_perturbed\n                self.global_best_position = perturbed_position.copy()\n\n            if self.eval_count >= self.budget:\n                break\n        \n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.eval_count < self.budget:\n            if self.eval_count < self.budget * 0.5:\n                self.pso_update(func)\n            else:\n                self.de_update(func)\n\n            # Apply local search with a small probability\n            if np.random.rand() < self.local_search_prob:\n                self.local_search(func)\n\n            if self.eval_count >= self.budget:\n                break\n\n        return self.global_best_fitness, self.global_best_position", "configspace": "", "generation": 2, "feedback": "In the code, line 101, in local_search, the following error occurred:\nValueError: setting an array element with a sequence.\nOn line: perturbed_position[i] += np.random.uniform(-step_size, step_size)", "error": "In the code, line 101, in local_search, the following error occurred:\nValueError: setting an array element with a sequence.\nOn line: perturbed_position[i] += np.random.uniform(-step_size, step_size)", "parent_ids": ["ad0f6409-cff0-4bc8-87da-01a088bef245"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "edbf797c-dcc3-460f-8549-de041651be38", "fitness": 0.6598515800639824, "name": "AdaptiveDE", "description": "Adaptive Differential Evolution with stochastic ranking and improved parameter adaptation for enhanced exploration and exploitation.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, CR=0.9, restart_patience=500):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F  # Initial mutation factor\n        self.CR = CR  # Initial crossover rate\n        self.restart_patience = restart_patience\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.archive_size = int(self.pop_size * 0.2)  # Archive size, e.g., 20% of population\n        self.archive = []\n\n    def __call__(self, func):\n        # Initialize population\n        pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in pop])\n        self.budget -= self.pop_size\n        \n        best_idx = np.argmin(fitness)\n        self.f_opt = fitness[best_idx]\n        self.x_opt = pop[best_idx]\n\n        iteration = 0\n        no_improvement_count = 0\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = pop[idxs]\n\n                # Optionally include an archive member\n                if len(self.archive) > 0 and np.random.rand() < 0.1:\n                    idx_a = np.random.randint(len(self.archive))\n                    x_r1 = self.archive[idx_a]\n                \n                # Adaptive F: perturb F with a small probability, use a better distribution\n                if np.random.rand() < 0.1:\n                    F_current = np.random.normal(self.F, 0.1)\n                else:\n                    F_current = self.F\n                F_current = np.clip(F_current, 0.1, 1.0)\n                \n                v = x_r1 + F_current * (x_r2 - x_r3)\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)  # Keep within bounds\n\n                # Crossover\n                j_rand = np.random.randint(self.dim)\n                u = np.copy(pop[i])\n                \n                # Adaptive CR: perturb CR with a small probability\n                if np.random.rand() < 0.1:\n                    CR_current = np.random.normal(self.CR, 0.1)\n                else:\n                    CR_current = self.CR\n                CR_current = np.clip(CR_current, 0.1, 1.0)\n                \n                for j in range(self.dim):\n                    if np.random.rand() < CR_current or j == j_rand:\n                        u[j] = v[j]\n\n                # Selection using stochastic ranking\n                f_u = func(u)\n                self.budget -= 1\n                \n                #Stochastic ranking\n                if (f_u < fitness[i]) or (np.random.rand() < 0.1): #Stochastic ranking\n                    if f_u < fitness[i]:\n                        fitness[i] = f_u\n                        pop[i] = u\n\n                        if f_u < self.f_opt:\n                            self.f_opt = f_u\n                            self.x_opt = u\n                            no_improvement_count = 0 # Reset counter\n                        else:\n                            no_improvement_count += 1\n                    else:\n                         #Accept even if worse, but update archive\n                         if len(self.archive) < self.archive_size:\n                            self.archive.append(pop[i])\n                         else:\n                            #Replace a random archive element\n                            idx_replace = np.random.randint(len(self.archive))\n                            self.archive[idx_replace] = pop[i]\n                else:\n                    no_improvement_count += 1\n\n            # Restart mechanism\n            if no_improvement_count > self.restart_patience:\n                pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                fitness = np.array([func(x) for x in pop])\n                self.budget -= self.pop_size\n                \n                best_idx = np.argmin(fitness)\n                self.f_opt = fitness[best_idx]\n                self.x_opt = pop[best_idx]\n                no_improvement_count = 0  # Reset counter\n                self.archive = [] #Clear archive\n            \n            iteration += 1\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "The algorithm AdaptiveDE scored 0.660 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["d83e47fb-f808-4342-af73-25d2c79f9fac"], "operator": null, "metadata": {"aucs": [0.21183058903325203, 0.42947439028843926, 0.7018963990380706, 0.8863577080470947, 0.7358339026912316, 0.7937178896154388, 0.6429915912711248, 0.6644765546754947, 0.6751252976305538, 0.6765128016595813, 0.8749392035017436, 0.9999157678232415, 0.5251464236351058, 0.7179581826848022, 0.9278513825819678, 0.7787236187585802, 0.4505166794825284, 0.8287682702072924, 0.1837100208424366, 0.491284927811667]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "5ff7b58b-fcf8-4de2-8d05-e5a6672d549e", "fitness": 0.6283823930922419, "name": "AdaptiveDE", "description": "Adaptive Differential Evolution with simplified orthogonal learning and a new niching strategy based on fitness similarity to enhance exploration and exploitation.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, CR=0.9, restart_patience=500, ol_patience=100, niching_threshold=0.01):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.restart_patience = restart_patience\n        self.ol_patience = ol_patience\n        self.niching_threshold = niching_threshold\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.rng = np.random.default_rng()\n\n    def orthogonal_learning(self, x_best, func):\n        \"\"\"\n        Simplified orthogonal learning around the best solution.\n        \"\"\"\n        candidate = np.copy(x_best)\n        for i in range(self.dim):\n            # Perturb each dimension randomly\n            perturbation = 0.05 * self.rng.uniform(-1, 1) * (func.bounds.ub - func.bounds.lb)\n            perturbed_candidate = x_best.copy()\n            perturbed_candidate[i] += perturbation\n            perturbed_candidate = np.clip(perturbed_candidate, func.bounds.lb, func.bounds.ub)\n\n            f_perturbed = func(perturbed_candidate)\n            self.budget -= 1\n            if f_perturbed < self.f_opt:\n                self.f_opt = f_perturbed\n                self.x_opt = perturbed_candidate\n            \n            if f_perturbed < func(candidate):\n                candidate = perturbed_candidate\n            \n            if self.budget <= 0:\n                break\n        return candidate\n\n    def niching(self, pop, fitness):\n        \"\"\"\n        Identifies and penalizes similar solutions to encourage diversity.\n        \"\"\"\n        for i in range(self.pop_size):\n            for j in range(i + 1, self.pop_size):\n                if np.abs(fitness[i] - fitness[j]) < self.niching_threshold:\n                    # Penalize the solution with the worse fitness\n                    if fitness[i] > fitness[j]:\n                        fitness[i] *= (1 + self.niching_threshold)\n                    else:\n                        fitness[j] *= (1 + self.niching_threshold)\n        return fitness\n\n    def __call__(self, func):\n        # Initialize population\n        pop = self.rng.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in pop])\n        self.budget -= self.pop_size\n\n        best_idx = np.argmin(fitness)\n        self.f_opt = fitness[best_idx]\n        self.x_opt = pop[best_idx]\n\n        iteration = 0\n        no_improvement_count = 0\n        ol_count = 0\n\n        while self.budget > 0:\n            # Apply niching strategy\n            fitness = self.niching(pop, fitness)\n\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = self.rng.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = pop[idxs]\n\n                # Adaptive F\n                F_current = self.F * (1 + 0.1 * self.rng.normal(0, 1))\n                F_current = np.clip(F_current, 0.1, 1.0)\n\n                v = x_r1 + F_current * (x_r2 - x_r3)\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                j_rand = self.rng.integers(self.dim)\n                u = np.copy(pop[i])\n\n                # Adaptive CR\n                CR_current = self.CR * (1 + 0.1 * self.rng.normal(0, 1))\n                CR_current = np.clip(CR_current, 0.1, 1.0)\n\n                crossover_mask = self.rng.random(self.dim) < CR_current\n                u[crossover_mask] = v[crossover_mask]\n                u[j_rand] = v[j_rand]\n\n                # Selection\n                f_u = func(u)\n                self.budget -= 1\n\n                if f_u < fitness[i]:\n                    fitness[i] = f_u\n                    pop[i] = u\n\n                    if f_u < self.f_opt:\n                        self.f_opt = f_u\n                        self.x_opt = u\n                        no_improvement_count = 0\n                        ol_count = 0\n                    else:\n                        no_improvement_count += 1\n                else:\n                    no_improvement_count += 1\n\n            # Orthogonal Learning\n            if ol_count > self.ol_patience and self.budget > 0:\n              candidate = self.orthogonal_learning(self.x_opt, func)\n              ol_count = 0\n              best_fitness = np.min(fitness)\n\n              f_candidate = func(candidate)\n              if f_candidate < best_fitness: #Direct replacement if better\n                worst_idx = np.argmax(fitness) #replace worst\n                pop[worst_idx] = candidate\n                fitness[worst_idx] = f_candidate\n                self.budget -= 1\n                if f_candidate < self.f_opt:\n                    self.f_opt = f_candidate\n                    self.x_opt = candidate\n            else:\n                ol_count +=1\n\n            # Restart mechanism\n            if no_improvement_count > self.restart_patience:\n                pop = self.rng.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                fitness = np.array([func(x) for x in pop])\n                self.budget -= self.pop_size\n\n                best_idx = np.argmin(fitness)\n                self.f_opt = fitness[best_idx]\n                self.x_opt = pop[best_idx]\n                no_improvement_count = 0\n                ol_count = 0\n\n            iteration += 1\n            if self.budget <= 0:\n                break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "The algorithm AdaptiveDE scored 0.628 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["7d9cb530-1ec3-4f38-96b1-6dfba1e93783"], "operator": null, "metadata": {"aucs": [0.20669283218862367, 0.5813280099273563, 0.6668351185795403, 0.872680822219041, 0.7459613341356561, 0.7829570012254716, 0.3774911800467662, 0.5216092466901585, 0.667176850785671, 0.508110202437186, 0.8554226509665861, 0.9933296242384735, 0.6586989255678031, 0.6108431385321751, 0.8792359430047296, 0.66438106395201, 0.46644431380020224, 0.8232221701372098, 0.18882117364865836, 0.4964062597615171]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "45aaf31d-007f-4368-89e6-dd88b28599cd", "fitness": 0.724260202235057, "name": "HybridPSO_DE_Adaptive", "description": "A hybrid algorithm combining PSO and DE with a self-adaptive switching mechanism based on performance feedback and orthogonal learning to refine promising solutions.", "code": "import numpy as np\n\nclass HybridPSO_DE_Adaptive:\n    def __init__(self, budget=10000, dim=10, pop_size=20, pso_weight=0.7, de_cr=0.9, de_f=0.8, ol_size=5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.pso_weight = pso_weight\n        self.de_cr = de_cr\n        self.de_f = de_f\n        self.ol_size = ol_size  # Number of individuals for orthogonal learning\n        self.population = None\n        self.fitness = None\n        self.velocities = None\n        self.best_positions = None\n        self.global_best_fitness = np.inf\n        self.global_best_position = None\n        self.eval_count = 0\n        self.pso_success_rate = 0.5 #initial pso success rate\n        self.de_success_rate = 0.5 # initial de success rate\n\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.eval_count += self.pop_size\n        self.velocities = np.random.uniform(-1, 1, size=(self.pop_size, self.dim)) * 0.1\n        self.best_positions = self.population.copy()\n        self.global_best_position = self.population[np.argmin(self.fitness)].copy()\n        self.global_best_fitness = np.min(self.fitness)\n\n    def pso_update(self, func):\n        cognitive_weight = 1.5\n        social_weight = 1.5\n        success_count = 0\n\n        r1 = np.random.rand(self.pop_size, self.dim)\n        r2 = np.random.rand(self.pop_size, self.dim)\n\n        cognitive_component = cognitive_weight * r1 * (self.best_positions - self.population)\n        social_component = social_weight * r2 * (self.global_best_position - self.population)\n\n        self.velocities = self.pso_weight * self.velocities + cognitive_component + social_component\n        self.population = self.population + self.velocities\n\n        self.population = np.clip(self.population, func.bounds.lb, func.bounds.ub)\n\n        new_fitness = np.array([func(x) for x in self.population])\n        self.eval_count += self.pop_size\n\n        for i in range(self.pop_size):\n            if new_fitness[i] < self.fitness[i]:\n                self.best_positions[i] = self.population[i].copy()\n                self.fitness[i] = new_fitness[i]\n                success_count +=1\n\n\n        best_index = np.argmin(self.fitness)\n        if self.fitness[best_index] < self.global_best_fitness:\n            self.global_best_fitness = self.fitness[best_index]\n            self.global_best_position = self.population[best_index].copy()\n\n        return success_count/self.pop_size\n\n    def de_update(self, func):\n        success_count = 0\n        for i in range(self.pop_size):\n            idxs = np.random.choice(self.pop_size, 3, replace=False)\n            while i in idxs:\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n\n            x1, x2, x3 = self.population[idxs]\n\n            v = x1 + self.de_f * (x2 - x3)\n            v = np.clip(v, func.bounds.lb, func.bounds.ub)\n\n            u = np.copy(self.population[i])\n            j_rand = np.random.randint(self.dim)\n            for j in range(self.dim):\n                if np.random.rand() < self.de_cr or j == j_rand:\n                    u[j] = v[j]\n\n            f_u = func(u)\n            self.eval_count += 1\n\n            if f_u < self.fitness[i]:\n                self.population[i] = u\n                self.fitness[i] = f_u\n                success_count += 1\n\n                if f_u < self.global_best_fitness:\n                    self.global_best_fitness = f_u\n                    self.global_best_position = u.copy()\n            elif self.eval_count >= self.budget:\n                break\n        return success_count/self.pop_size\n\n    def orthogonal_learning(self, func):\n        # Select top ol_size individuals\n        top_indices = np.argsort(self.fitness)[:self.ol_size]\n        basis = self.population[top_indices]\n\n        # Generate orthogonal array (simplified version - random perturbations)\n        oa = np.random.uniform(-0.1, 0.1, size=(self.ol_size, self.dim)) # scaled perturbations\n\n        # Create new solutions based on orthogonal array\n        for i in range(self.ol_size):\n            new_solution = basis[i] + oa[i]\n            new_solution = np.clip(new_solution, func.bounds.lb, func.bounds.ub)\n\n            f_new = func(new_solution)\n            self.eval_count += 1\n\n            # Replace the worst individual if the new solution is better\n            worst_index = np.argmax(self.fitness)\n            if f_new < self.fitness[worst_index]:\n                self.population[worst_index] = new_solution\n                self.fitness[worst_index] = f_new\n\n                if f_new < self.global_best_fitness:\n                    self.global_best_fitness = f_new\n                    self.global_best_position = new_solution.copy()\n            elif self.eval_count >= self.budget:\n                break\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.eval_count < self.budget:\n            # Adaptive switching mechanism\n            if self.pso_success_rate > self.de_success_rate:\n                success_rate = self.pso_update(func)\n                self.pso_success_rate = 0.9*self.pso_success_rate + 0.1*success_rate\n                self.de_success_rate = 0.9*self.de_success_rate # Decay the less successful rate\n\n            else:\n                success_rate = self.de_update(func)\n                self.de_success_rate = 0.9*self.de_success_rate + 0.1*success_rate\n                self.pso_success_rate = 0.9*self.pso_success_rate # Decay the less successful rate\n\n\n            if self.eval_count < self.budget * 0.8: # Reduce orthogonal learning budget\n                self.orthogonal_learning(func)\n            elif self.eval_count >= self.budget:\n                break\n\n        return self.global_best_fitness, self.global_best_position", "configspace": "", "generation": 2, "feedback": "The algorithm HybridPSO_DE_Adaptive scored 0.724 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["ad0f6409-cff0-4bc8-87da-01a088bef245"], "operator": null, "metadata": {"aucs": [0.48769621589843803, 0.7695746554717366, 0.7435262883211491, 0.9453662256940552, 0.7283281734088989, 0.8693031521567653, 0.7590522335874971, 0.706191100096405, 0.8427385591724146, 0.2274287422611595, 0.8921483963707841, 0.99902434149107, 0.6779423971194285, 0.8374005237657967, 0.885640206830898, 0.8559505461759831, 0.7306005220775325, 0.9024905332358046, 0.6248012315653233, 0]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "65911d3a-dd89-42a3-a7dc-fce5871cc776", "fitness": "-inf", "name": "AdaptiveHybridPSO_DE_NM", "description": "A hybrid algorithm combining PSO, DE, and Nelder-Mead simplex search, adaptively adjusting the contribution of each component based on their recent performance.", "code": "import numpy as np\n\nclass AdaptiveHybridPSO_DE_NM:\n    def __init__(self, budget=10000, dim=10, pop_size=20, pso_weight=0.7, de_cr=0.9, de_f=0.8, nm_freq=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.pso_weight = pso_weight\n        self.de_cr = de_cr\n        self.de_f = de_f\n        self.nm_freq = nm_freq  # Frequency of Nelder-Mead application\n        self.population = None\n        self.fitness = None\n        self.velocities = None\n        self.best_positions = None\n        self.global_best_fitness = np.inf\n        self.global_best_position = None\n        self.eval_count = 0\n\n        # Adaptive component weights\n        self.pso_prob = 0.33\n        self.de_prob = 0.33\n        self.nm_prob = 0.34\n        self.pso_success = 0\n        self.de_success = 0\n        self.nm_success = 0\n        self.adapt_rate = 0.1\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.eval_count += self.pop_size\n        self.velocities = np.random.uniform(-1, 1, size=(self.pop_size, self.dim)) * 0.1\n        self.best_positions = self.population.copy()\n        self.global_best_position = self.population[np.argmin(self.fitness)].copy()\n        self.global_best_fitness = np.min(self.fitness)\n\n    def pso_update(self, func):\n        cognitive_weight = 1.5\n        social_weight = 1.5\n\n        r1 = np.random.rand(self.pop_size, self.dim)\n        r2 = np.random.rand(self.pop_size, self.dim)\n\n        cognitive_component = cognitive_weight * r1 * (self.best_positions - self.population)\n        social_component = social_weight * r2 * (self.global_best_position - self.population)\n\n        self.velocities = self.pso_weight * self.velocities + cognitive_component + social_component\n        self.population = self.population + self.velocities\n\n        self.population = np.clip(self.population, func.bounds.lb, func.bounds.ub)\n\n        new_fitness = np.array([func(x) for x in self.population])\n        self.eval_count += self.pop_size\n\n        improved = False\n        for i in range(self.pop_size):\n            if new_fitness[i] < self.fitness[i]:\n                self.best_positions[i] = self.population[i].copy()\n                self.fitness[i] = new_fitness[i]\n                improved = True\n\n\n        best_index = np.argmin(self.fitness)\n        if self.fitness[best_index] < self.global_best_fitness:\n            self.global_best_fitness = self.fitness[best_index]\n            self.global_best_position = self.population[best_index].copy()\n            if improved:\n                self.pso_success += 1\n\n    def de_update(self, func):\n        improved = False\n        for i in range(self.pop_size):\n            idxs = np.random.choice(self.pop_size, 3, replace=False)\n            while i in idxs:\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n\n            x1, x2, x3 = self.population[idxs]\n\n            v = x1 + self.de_f * (x2 - x3)\n            v = np.clip(v, func.bounds.lb, func.bounds.ub)\n\n            u = np.copy(self.population[i])\n            j_rand = np.random.randint(self.dim)\n            for j in range(self.dim):\n                if np.random.rand() < self.de_cr or j == j_rand:\n                    u[j] = v[j]\n\n            f_u = func(u)\n            self.eval_count += 1\n\n            if f_u < self.fitness[i]:\n                self.population[i] = u\n                self.fitness[i] = f_u\n                improved = True\n\n                if f_u < self.global_best_fitness:\n                    self.global_best_fitness = f_u\n                    self.global_best_position = u.copy()\n            elif self.eval_count >= self.budget:\n                break\n        if improved:\n            self.de_success += 1\n\n    def nelder_mead(self, func, x0, step=0.1, no_improve_thr=10e-6, no_improve_break=10, max_iter=100):\n        \"\"\"\n        Nelder-Mead simplex algorithm.\n        \"\"\"\n        dim = len(x0)\n        simplex = np.zeros((dim + 1, dim))\n        simplex[0] = x0\n        for i in range(dim):\n            simplex[i + 1] = x0 + np.eye(dim)[i] * step\n\n        values = np.array([func(x) for x in simplex])\n        self.eval_count += dim + 1\n\n        iterations = 0\n        no_improve_count = 0\n\n        while iterations < max_iter and no_improve_count < no_improve_break and self.eval_count < self.budget:\n            iterations += 1\n\n            # 1. Order\n            order = values.argsort()\n            simplex = simplex[order]\n            values = values[order]\n\n            best = simplex[0]\n            worst = simplex[-1]\n\n            # 2. Calculate centroid\n            centroid = simplex[:-1].mean(axis=0)\n\n            # 3. Reflection\n            reflected = centroid + (centroid - worst)\n            reflected = np.clip(reflected, func.bounds.lb, func.bounds.ub)\n            reflected_value = func(reflected)\n            self.eval_count += 1\n\n            if values[0] <= reflected_value < values[-2]:\n                simplex[-1] = reflected\n                values[-1] = reflected_value\n            else:\n                # 4. Expansion\n                if reflected_value < values[0]:\n                    expanded = centroid + 2 * (centroid - worst)\n                    expanded = np.clip(expanded, func.bounds.lb, func.bounds.ub)\n                    expanded_value = func(expanded)\n                    self.eval_count += 1\n\n                    if expanded_value < reflected_value:\n                        simplex[-1] = expanded\n                        values[-1] = expanded_value\n                    else:\n                        simplex[-1] = reflected\n                        values[-1] = reflected_value\n                else:\n                    # 5. Contraction\n                    contracted = centroid + 0.5 * (worst - centroid)\n                    contracted = np.clip(contracted, func.bounds.lb, func.bounds.ub)\n                    contracted_value = func(contracted)\n                    self.eval_count += 1\n\n                    if contracted_value < values[-1]:\n                        simplex[-1] = contracted\n                        values[-1] = contracted_value\n                    else:\n                        # 6. Shrink\n                        for i in range(1, dim + 1):\n                            simplex[i] = best + 0.5 * (simplex[i] - best)\n                            simplex[i] = np.clip(simplex[i], func.bounds.lb, func.bounds.ub)\n                            values[i] = func(simplex[i])\n                            self.eval_count += 1\n\n            if np.std(values) < no_improve_thr:\n                no_improve_count += 1\n            else:\n                no_improve_count = 0\n\n            order = values.argsort()\n            simplex = simplex[order]\n            values = values[order]\n\n            if values[0] < self.global_best_fitness:\n                self.global_best_fitness = values[0]\n                self.global_best_position = simplex[0].copy()\n\n        return values[0], simplex[0]\n\n    def adapt_probabilities(self):\n      total_success = self.pso_success + self.de_success + self.nm_success\n      if total_success > 0:\n          self.pso_prob = self.pso_success / total_success\n          self.de_prob = self.de_success / total_success\n          self.nm_prob = self.nm_success / total_success\n      else:\n          self.pso_prob = 0.33\n          self.de_prob = 0.33\n          self.nm_prob = 0.34\n\n      self.pso_success = 0\n      self.de_success = 0\n      self.nm_success = 0\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        iteration = 0\n        while self.eval_count < self.budget:\n            iteration += 1\n            rand = np.random.rand()\n\n            if rand < self.pso_prob:\n                self.pso_update(func)\n            elif rand < self.pso_prob + self.de_prob:\n                self.de_update(func)\n            else:\n                # Apply Nelder-Mead to the best individual\n                _, best_pos = self.nelder_mead(func, self.global_best_position)\n                f_best = func(best_pos)\n                self.eval_count += 1\n\n                if f_best < self.global_best_fitness:\n                    self.global_best_fitness = f_best\n                    self.global_best_position = best_pos.copy()\n                    self.nm_success += 1\n\n\n            if iteration % 10 == 0:\n                self.adapt_probabilities()\n\n            if self.eval_count >= self.budget:\n                break\n\n        return self.global_best_fitness, self.global_best_position", "configspace": "", "generation": 2, "feedback": "Evaluation timed out after 300 seconds.", "error": "Evaluation timed out after 300 seconds.", "parent_ids": ["ad0f6409-cff0-4bc8-87da-01a088bef245"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "bc144b22-7ba7-4cbb-b922-7d669e896081", "fitness": 0.0, "name": "AdaptiveDE_NM", "description": "An adaptive Differential Evolution strategy that incorporates a Nelder-Mead local search component and dynamically adjusts its parameters based on success history.", "code": "import numpy as np\n\nclass AdaptiveDE_NM:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, CR=0.9, nm_frequency=50, nm_size=5, success_history_size=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.nm_frequency = nm_frequency\n        self.nm_size = nm_size\n        self.success_history_size = success_history_size\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.rng = np.random.default_rng()\n        self.success_history_F = np.ones(self.success_history_size) * self.F\n        self.success_history_CR = np.ones(self.success_history_size) * self.CR\n        self.success_idx = 0\n\n    def nelder_mead(self, func, x_start, bounds, budget):\n        \"\"\"\n        Performs Nelder-Mead simplex search around a starting point.\n        \"\"\"\n        dim = len(x_start)\n        simplex = [x_start + 0.05 * self.rng.normal(0, 1, dim) for _ in range(dim + 1)]\n        simplex = np.clip(simplex, bounds.lb, bounds.ub)\n        \n        fitness = np.array([func(x) for x in simplex])\n        budget -= (dim + 1)\n        \n        while budget > 0:\n            # Order the simplex points by fitness\n            idx = np.argsort(fitness)\n            simplex = simplex[idx]\n            fitness = fitness[idx]\n\n            best, worst = simplex[0], simplex[-1]\n            f_best, f_worst = fitness[0], fitness[-1]\n\n            # Reflection\n            centroid = np.mean(simplex[:-1], axis=0)\n            x_r = centroid + 1.0 * (centroid - worst)\n            x_r = np.clip(x_r, bounds.lb, bounds.ub)\n            f_r = func(x_r)\n            budget -= 1\n\n            if f_r < f_best:\n                # Expansion\n                x_e = centroid + 2.0 * (x_r - centroid)\n                x_e = np.clip(x_e, bounds.lb, bounds.ub)\n                f_e = func(x_e)\n                budget -= 1\n                if f_e < f_r:\n                    simplex[-1] = x_e\n                    fitness[-1] = f_e\n                else:\n                    simplex[-1] = x_r\n                    fitness[-1] = f_r\n            elif f_r < fitness[-2]:\n                simplex[-1] = x_r\n                fitness[-1] = f_r\n            else:\n                # Contraction\n                x_c = centroid + 0.5 * (worst - centroid)\n                x_c = np.clip(x_c, bounds.lb, bounds.ub)\n                f_c = func(x_c)\n                budget -= 1\n                if f_c < f_worst:\n                    simplex[-1] = x_c\n                    fitness[-1] = f_c\n                else:\n                    # Shrink\n                    for i in range(1, dim + 1):\n                        simplex[i] = best + 0.5 * (simplex[i] - best)\n                        simplex[i] = np.clip(simplex[i], bounds.lb, bounds.ub)\n                        fitness[i] = func(simplex[i])\n                        budget -= 1\n            \n            if budget <=0:\n              break\n\n\n        best_idx = np.argmin(fitness)\n        return fitness[best_idx], simplex[best_idx], budget\n\n    def __call__(self, func):\n        # Initialize population\n        pop = self.rng.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in pop])\n        self.budget -= self.pop_size\n\n        best_idx = np.argmin(fitness)\n        self.f_opt = fitness[best_idx]\n        self.x_opt = pop[best_idx]\n\n        iteration = 0\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = self.rng.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = pop[idxs]\n\n                # Adaptive F\n                F_current = self.success_history_F[self.success_idx % self.success_history_size]\n                F_current = np.clip(F_current, 0.1, 1.0)\n\n                v = x_r1 + F_current * (x_r2 - x_r3)\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                j_rand = self.rng.integers(self.dim)\n                u = np.copy(pop[i])\n\n                # Adaptive CR\n                CR_current = self.success_history_CR[self.success_idx % self.success_history_size]\n                CR_current = np.clip(CR_current, 0.1, 1.0)\n\n                crossover_mask = self.rng.random(self.dim) < CR_current\n                u[crossover_mask] = v[crossover_mask]\n                u[j_rand] = v[j_rand] # Guarantee at least one change\n\n                # Selection\n                f_u = func(u)\n                self.budget -= 1\n\n                if f_u < fitness[i]:\n                    fitness[i] = f_u\n                    pop[i] = u\n\n                    #Update optimal solution\n                    if f_u < self.f_opt:\n                        self.f_opt = f_u\n                        self.x_opt = u\n\n                        # Store successful F and CR\n                        self.success_history_F[self.success_idx % self.success_history_size] = F_current\n                        self.success_history_CR[self.success_idx % self.success_history_size] = CR_current\n                        self.success_idx += 1\n\n\n            # Nelder-Mead Local Search\n            if iteration % self.nm_frequency == 0 and self.budget > self.dim + 1:\n                best_idx = np.argmin(fitness)\n                x_nm_start = pop[best_idx]\n                f_nm, x_nm, self.budget = self.nelder_mead(func, x_nm_start, func.bounds, self.budget)\n\n                if f_nm < self.f_opt:\n                    self.f_opt = f_nm\n                    self.x_opt = x_nm\n                \n                worst_idx = np.argmax(fitness) #replace worst element\n                pop[worst_idx] = x_nm\n                fitness[worst_idx] = f_nm\n\n            iteration += 1\n            if self.budget <= 0:\n                break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "The algorithm AdaptiveDE_NM scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["7d9cb530-1ec3-4f38-96b1-6dfba1e93783"], "operator": null, "metadata": {"aucs": [0]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "53b58566-1279-4243-91d0-f819693d86aa", "fitness": 0.5176277838175565, "name": "HybridPSO_DE", "description": "A simplified hybrid PSO-DE algorithm with adaptive parameter control and a focus on exploration in early stages and exploitation in later stages.", "code": "import numpy as np\n\nclass HybridPSO_DE:\n    def __init__(self, budget=10000, dim=10, pop_size=20):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.population = None\n        self.fitness = None\n        self.velocities = None\n        self.best_positions = None\n        self.global_best_fitness = np.inf\n        self.global_best_position = None\n        self.eval_count = 0\n        self.pso_weight = 0.7\n        self.de_cr = 0.9\n        self.de_f = 0.8\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.eval_count += self.pop_size\n        self.velocities = np.random.uniform(-1, 1, size=(self.pop_size, self.dim)) * 0.1\n        self.best_positions = self.population.copy()\n        self.global_best_position = self.population[np.argmin(self.fitness)].copy()\n        self.global_best_fitness = np.min(self.fitness)\n\n    def pso_update(self, func, phase):\n        cognitive_weight = 1.5\n        social_weight = 1.5\n\n        # Adaptive PSO weight\n        pso_weight = self.pso_weight * (1 - phase)  # Gradually decrease inertia\n\n        r1 = np.random.rand(self.pop_size, self.dim)\n        r2 = np.random.rand(self.pop_size, self.dim)\n\n        cognitive_component = cognitive_weight * r1 * (self.best_positions - self.population)\n        social_component = social_weight * r2 * (self.global_best_position - self.population)\n\n        self.velocities = pso_weight * self.velocities + cognitive_component + social_component\n        self.population = self.population + self.velocities\n        self.population = np.clip(self.population, func.bounds.lb, func.bounds.ub)\n\n        new_fitness = np.array([func(x) for x in self.population])\n        self.eval_count += self.pop_size\n\n        for i in range(self.pop_size):\n            if new_fitness[i] < self.fitness[i]:\n                self.best_positions[i] = self.population[i].copy()\n                self.fitness[i] = new_fitness[i]\n\n        best_index = np.argmin(self.fitness)\n        if self.fitness[best_index] < self.global_best_fitness:\n            self.global_best_fitness = self.fitness[best_index]\n            self.global_best_position = self.population[best_index].copy()\n\n    def de_update(self, func, phase):\n        # Adaptive DE parameters\n        cr = self.de_cr * phase # Gradually increase crossover rate\n        f = self.de_f\n\n        for i in range(self.pop_size):\n            idxs = np.random.choice(self.pop_size, 3, replace=False)\n            while i in idxs:\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n\n            x1, x2, x3 = self.population[idxs]\n            v = x1 + f * (x2 - x3)\n            v = np.clip(v, func.bounds.lb, func.bounds.ub)\n            u = np.copy(self.population[i])\n            j_rand = np.random.randint(self.dim)\n            for j in range(self.dim):\n                if np.random.rand() < cr or j == j_rand:\n                    u[j] = v[j]\n\n            f_u = func(u)\n            self.eval_count += 1\n\n            if f_u < self.fitness[i]:\n                self.population[i] = u\n                self.fitness[i] = f_u\n\n                if f_u < self.global_best_fitness:\n                    self.global_best_fitness = f_u\n                    self.global_best_position = u.copy()\n            elif self.eval_count >= self.budget:\n                break\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.eval_count < self.budget:\n            phase = self.eval_count / self.budget  # Progress through the budget\n\n            if phase < 0.5:\n                self.pso_update(func, phase)  # PSO for exploration\n            else:\n                self.de_update(func, phase)   # DE for exploitation\n\n            if self.eval_count >= self.budget:\n                break\n\n        return self.global_best_fitness, self.global_best_position", "configspace": "", "generation": 2, "feedback": "The algorithm HybridPSO_DE scored 0.518 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["ad0f6409-cff0-4bc8-87da-01a088bef245"], "operator": null, "metadata": {"aucs": [0.1504861490245265, 0.40985999587640576, 0.8886187832445621, 0.3587758568225541, 0.37802222983396283, 0.24401857313073272, 0.32436508494280736, 0.5775013912614989, 0.9069319069550543, 0.23180924076654186, 0.9418043690205811, 0.9982884197385753, 0.2735015806370735, 0.31975379035770735, 0.7444318019816476, 0.9259593245815599, 0.5339215473014196, 0.3812028072846385, 0.28359656519615506, 0.47970625839312453]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "b3d551a2-11c6-4c74-a393-e657b3ebccdb", "fitness": 0.0733080599971575, "name": "DynamicNeighborhoodDE", "description": "A Differential Evolution strategy with a dynamically adjusted population size and a neighborhood-based mutation operator to enhance exploration and exploitation in complex landscapes.", "code": "import numpy as np\n\nclass DynamicNeighborhoodDE:\n    def __init__(self, budget=10000, dim=10, initial_pop_size=50, min_pop_size=10, max_pop_size=100, F=0.5, CR=0.9, neighborhood_size=5, pop_adapt_freq=50, pop_adapt_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.initial_pop_size = initial_pop_size\n        self.min_pop_size = min_pop_size\n        self.max_pop_size = max_pop_size\n        self.pop_size = initial_pop_size\n        self.F = F\n        self.CR = CR\n        self.neighborhood_size = neighborhood_size\n        self.pop_adapt_freq = pop_adapt_freq\n        self.pop_adapt_rate = pop_adapt_rate\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.rng = np.random.default_rng() # Use numpy random generator\n\n    def neighborhood_mutation(self, pop, i):\n        \"\"\"\n        Performs mutation within a defined neighborhood of the current individual.\n        \"\"\"\n        indices = np.arange(self.pop_size)\n        distances = np.linalg.norm(pop - pop[i], axis=1)\n        \n        # Find the nearest neighbors, excluding the current individual\n        neighbor_indices = np.argsort(distances)[1:self.neighborhood_size+1]\n\n        #Select two random neighbors\n        if len(neighbor_indices) < 2:\n          return pop[i] #Return original if not enough neighbors\n        \n        r1, r2 = self.rng.choice(neighbor_indices, 2, replace=False)\n\n        v = pop[i] + self.F * (pop[r1] - pop[r2])\n        return v\n\n    def adjust_population_size(self, fitness):\n        \"\"\"\n        Dynamically adjusts the population size based on the fitness variance.\n        \"\"\"\n        fitness_std = np.std(fitness)\n        \n        if fitness_std > np.mean(fitness):  # High variance, increase pop size\n            self.pop_size = min(self.max_pop_size, int(self.pop_size * (1 + self.pop_adapt_rate)))\n        else:  # Low variance, decrease pop size\n            self.pop_size = max(self.min_pop_size, int(self.pop_size * (1 - self.pop_adapt_rate)))\n        self.pop_size = int(self.pop_size)\n\n    def __call__(self, func):\n        # Initialize population\n        pop = self.rng.uniform(func.bounds.lb, func.bounds.ub, size=(self.initial_pop_size, self.dim))\n        fitness = np.array([func(x) for x in pop])\n        self.budget -= self.initial_pop_size\n        self.pop_size = self.initial_pop_size\n\n        best_idx = np.argmin(fitness)\n        self.f_opt = fitness[best_idx]\n        self.x_opt = pop[best_idx]\n\n        iteration = 0\n\n        while self.budget > 0:\n            new_pop = np.copy(pop)\n            new_fitness = np.copy(fitness)\n\n            for i in range(self.pop_size):\n                # Mutation (Neighborhood-based)\n                v = self.neighborhood_mutation(pop, i)\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                j_rand = self.rng.integers(self.dim)\n                u = np.copy(pop[i])\n                crossover_mask = self.rng.random(self.dim) < self.CR\n                u[crossover_mask] = v[crossover_mask]\n                u[j_rand] = v[j_rand] # Guarantee at least one change\n\n                # Selection\n                f_u = func(u)\n                self.budget -= 1\n\n                if f_u < fitness[i]:\n                    new_fitness[i] = f_u\n                    new_pop[i] = u\n\n                    if f_u < self.f_opt:\n                        self.f_opt = f_u\n                        self.x_opt = u\n\n            pop = new_pop\n            fitness = new_fitness\n\n            # Population Adjustment\n            if iteration % self.pop_adapt_freq == 0 and self.budget > 0:\n                self.adjust_population_size(fitness)\n\n                # Resize population - either add new random individuals or remove worst ones\n                if self.pop_size > pop.shape[0]:\n                    num_new = self.pop_size - pop.shape[0]\n                    new_individuals = self.rng.uniform(func.bounds.lb, func.bounds.ub, size=(num_new, self.dim))\n                    new_fitnesses = np.array([func(x) for x in new_individuals])\n                    self.budget -= num_new\n\n                    pop = np.vstack((pop, new_individuals))\n                    fitness = np.concatenate((fitness, new_fitnesses))\n                elif self.pop_size < pop.shape[0]:\n                    num_remove = pop.shape[0] - self.pop_size\n                    worst_indices = np.argsort(fitness)[-num_remove:] #remove worst\n                    mask = np.ones(pop.shape[0], dtype=bool)\n                    mask[worst_indices] = False\n                    pop = pop[mask]\n                    fitness = fitness[mask]\n\n            best_idx = np.argmin(fitness)\n            self.f_opt = fitness[best_idx]\n            self.x_opt = pop[best_idx]\n                \n            iteration += 1\n            if self.budget <= 0:\n                break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "The algorithm DynamicNeighborhoodDE scored 0.073 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["7d9cb530-1ec3-4f38-96b1-6dfba1e93783"], "operator": null, "metadata": {"aucs": [0.146616119994315, 0]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "206b5932-5c8b-4325-b5df-26a21fd7869a", "fitness": "-inf", "name": "AdaptiveDE", "description": "Adaptive Differential Evolution with improved orthogonal learning utilizing a larger orthogonal array and a self-adaptive population size adjustment based on stagnation detection.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, CR=0.9, restart_patience=500, ol_patience=100, pop_size_adapt_patience=300):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.restart_patience = restart_patience\n        self.ol_patience = ol_patience\n        self.pop_size_adapt_patience = pop_size_adapt_patience\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.rng = np.random.default_rng()\n        self.pop_sizes = [20,50,100]\n        self.current_pop_size_idx = 1\n\n    def generate_orthogonal_array(self, n_factors, strength=2):\n        \"\"\"Generates an orthogonal array.  Simplified implementation for demonstration.\"\"\"\n        if n_factors <= 4 and strength == 2:\n            if n_factors == 2:\n                return np.array([[1, 1], [1, -1], [-1, 1], [-1, -1]])\n            elif n_factors == 3:\n                return np.array([[1, 1, 1], [1, -1, -1], [-1, 1, -1], [-1, -1, 1]])\n            elif n_factors == 4:\n                 return np.array([[1, 1, 1, 1], [1, -1, -1, 1], [-1, 1, -1, 1], [-1, -1, 1, 1],\n                                  [1, 1, -1, -1], [1, -1, 1, -1], [-1, 1, 1, -1], [-1, -1, -1, -1]])\n        else:\n            #Placeholder for a more robust OA generator if needed.\n            #For example: pyDOE2 library\n            return self.rng.uniform(-1, 1, size=(8, min(n_factors,8))) # Placeholder\n\n    def orthogonal_learning(self, pop, fitness, func):\n        \"\"\"Performs orthogonal learning to generate a promising candidate.\"\"\"\n        best_idx = np.argmin(fitness)\n        x_best = pop[best_idx]\n\n        oa = self.generate_orthogonal_array(self.dim)\n        n_oa_rows = oa.shape[0]\n\n        candidates = np.zeros((n_oa_rows, self.dim))\n        f_candidates = np.zeros(n_oa_rows)\n\n        for i in range(n_oa_rows):\n            perturbed_candidate = x_best + 0.05 * oa[i] * (func.bounds.ub - func.bounds.lb)\n            perturbed_candidate = np.clip(perturbed_candidate, func.bounds.lb, func.bounds.ub)\n            candidates[i] = perturbed_candidate\n            f_candidates[i] = func(perturbed_candidate)\n            self.budget -= 1\n            if f_candidates[i] < self.f_opt:\n                self.f_opt = f_candidates[i]\n                self.x_opt = perturbed_candidate\n            if self.budget <= 0:\n                break\n\n        best_candidate_idx = np.argmin(f_candidates)\n        return candidates[best_candidate_idx]\n\n\n    def __call__(self, func):\n        # Initialize population\n        pop = self.rng.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in pop])\n        self.budget -= self.pop_size\n\n        best_idx = np.argmin(fitness)\n        self.f_opt = fitness[best_idx]\n        self.x_opt = pop[best_idx]\n\n        iteration = 0\n        no_improvement_count = 0\n        ol_count = 0\n        pop_size_no_improvement_count = 0\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = self.rng.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = pop[idxs]\n\n                # Adaptive F\n                F_current = self.F * (1 + 0.1 * self.rng.normal(0, 1))\n                F_current = np.clip(F_current, 0.1, 1.0)\n\n                v = x_r1 + F_current * (x_r2 - x_r3)\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                j_rand = self.rng.integers(self.dim)\n                u = np.copy(pop[i])\n\n                # Adaptive CR\n                CR_current = self.CR * (1 + 0.1 * self.rng.normal(0, 1))\n                CR_current = np.clip(CR_current, 0.1, 1.0)\n\n                crossover_mask = self.rng.random(self.dim) < CR_current\n                u[crossover_mask] = v[crossover_mask]\n                u[j_rand] = v[j_rand] # Guarantee at least one change\n\n\n                # Selection\n                f_u = func(u)\n                self.budget -= 1\n\n                if f_u < fitness[i]:\n                    fitness[i] = f_u\n                    pop[i] = u\n\n                    if f_u < self.f_opt:\n                        self.f_opt = f_u\n                        self.x_opt = u\n                        no_improvement_count = 0\n                        ol_count = 0\n                        pop_size_no_improvement_count = 0\n                    else:\n                        no_improvement_count += 1\n                        pop_size_no_improvement_count +=1\n                else:\n                    no_improvement_count += 1\n                    pop_size_no_improvement_count += 1\n\n            # Orthogonal Learning\n            if ol_count > self.ol_patience and self.budget > 0:\n              candidate = self.orthogonal_learning(pop, fitness, func)\n              ol_count = 0\n              best_fitness = np.min(fitness)\n              f_candidate = func(candidate)\n              self.budget -=1\n              if f_candidate < best_fitness: #Direct replacement if better\n                best_idx = np.argmax(fitness) #replace worst\n                pop[best_idx] = candidate\n                fitness[best_idx] = f_candidate\n                if f_candidate < self.f_opt:\n                  self.f_opt = f_candidate\n                  self.x_opt = candidate\n\n            else:\n                ol_count +=1\n\n            # Restart mechanism\n            if no_improvement_count > self.restart_patience:\n                pop = self.rng.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                fitness = np.array([func(x) for x in pop])\n                self.budget -= self.pop_size\n\n                best_idx = np.argmin(fitness)\n                self.f_opt = fitness[best_idx]\n                self.x_opt = pop[best_idx]\n                no_improvement_count = 0\n                ol_count = 0\n                pop_size_no_improvement_count = 0\n\n            #Population Size Adaptation\n            if pop_size_no_improvement_count > self.pop_size_adapt_patience:\n              self.current_pop_size_idx = (self.current_pop_size_idx + 1) % len(self.pop_sizes)\n              new_pop_size = self.pop_sizes[self.current_pop_size_idx]\n              if new_pop_size > self.pop_size: #increase\n                num_new_individuals = new_pop_size - self.pop_size\n                new_pop = self.rng.uniform(func.bounds.lb, func.bounds.ub, size=(num_new_individuals, self.dim))\n                new_fitness = np.array([func(x) for x in new_pop])\n                self.budget -= num_new_individuals\n                pop = np.vstack((pop, new_pop))\n                fitness = np.concatenate((fitness, new_fitness))\n\n              elif new_pop_size < self.pop_size: #decrease\n                num_remove_individuals = self.pop_size - new_pop_size\n                worst_indices = np.argsort(fitness)[-num_remove_individuals:]\n                mask = np.ones(self.pop_size, dtype=bool)\n                mask[worst_indices] = False\n                pop = pop[mask]\n                fitness = fitness[mask]\n\n              self.pop_size = new_pop_size\n              best_idx = np.argmin(fitness)\n              self.f_opt = fitness[best_idx]\n              self.x_opt = pop[best_idx]\n              pop_size_no_improvement_count = 0\n              no_improvement_count = 0\n              ol_count = 0\n\n\n            iteration += 1\n            if self.budget <= 0:\n                break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "Evaluation timed out after 300 seconds.", "error": "Evaluation timed out after 300 seconds.", "parent_ids": ["7d9cb530-1ec3-4f38-96b1-6dfba1e93783"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "72e46444-7307-42b1-aa0e-f5551499fc5a", "fitness": 0.23503320354783097, "name": "CMA_Neighborhood_AdaptiveDE", "description": "Covariance matrix adaptation DE with neighborhood search and diversity maintenance.", "code": "import numpy as np\n\nclass CMA_Neighborhood_AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, CR=0.9, restart_patience=500, neighborhood_size=5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.restart_patience = restart_patience\n        self.neighborhood_size = neighborhood_size  # Size of the neighborhood for local search\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.archive_size = int(self.pop_size * 0.2)\n        self.archive = []\n        self.C = np.eye(dim)  # Covariance matrix for CMA-ES like mutation\n\n    def __call__(self, func):\n        # Initialize population\n        pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in pop])\n        self.budget -= self.pop_size\n\n        best_idx = np.argmin(fitness)\n        self.f_opt = fitness[best_idx]\n        self.x_opt = pop[best_idx]\n\n        iteration = 0\n        no_improvement_count = 0\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation using CMA-ES-like approach\n                z = np.random.randn(self.dim)\n                mutation_vector = np.dot(np.linalg.cholesky(self.C), z) * self.F # Scale by F\n\n                # DE-like mutation within the neighborhood\n                neighborhood_indices = np.random.choice(self.pop_size, self.neighborhood_size, replace=False)\n                x_r1, x_r2 = pop[neighborhood_indices[0]], pop[neighborhood_indices[1]]\n\n                v = pop[i] + mutation_vector + self.F * (x_r1 - x_r2)\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                j_rand = np.random.randint(self.dim)\n                u = np.copy(pop[i])\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR or j == j_rand:\n                        u[j] = v[j]\n\n                # Selection\n                f_u = func(u)\n                self.budget -= 1\n                if f_u < fitness[i]:\n                    fitness[i] = f_u\n                    pop[i] = u\n                    if f_u < self.f_opt:\n                        self.f_opt = f_u\n                        self.x_opt = u\n                        no_improvement_count = 0\n                    else:\n                        no_improvement_count += 1\n\n                    # Update covariance matrix (simplified)\n                    d = u - pop[i]\n                    self.C = (1 - 0.05) * self.C + 0.05 * np.outer(d, d) #Simplified CMA update\n                else:\n                    no_improvement_count += 1\n                    \n                # Diversity maintenance using archive (only if worse)\n                if f_u >= fitness[i]:\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(pop[i])\n                    else:\n                        idx_replace = np.random.randint(len(self.archive))\n                        self.archive[idx_replace] = pop[i]\n\n\n            # Restart mechanism\n            if no_improvement_count > self.restart_patience:\n                pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                fitness = np.array([func(x) for x in pop])\n                self.budget -= self.pop_size\n                best_idx = np.argmin(fitness)\n                self.f_opt = fitness[best_idx]\n                self.x_opt = pop[best_idx]\n                no_improvement_count = 0\n                self.C = np.eye(self.dim) # Reset CMA\n                self.archive = []\n\n            iteration += 1\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "The algorithm CMA_Neighborhood_AdaptiveDE scored 0.235 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["edbf797c-dcc3-460f-8549-de041651be38"], "operator": null, "metadata": {"aucs": [0.13467642773680544, 0.23991248502016405, 0.3339936873230346, 0.32683662274942604, 0.25894453974666454, 0.3215622032480209, 0.26433966255853214, 0]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "36b76556-c02c-4ecf-b892-a37011e97d73", "fitness": "-inf", "name": "AdaptiveHybridPSO_DE", "description": "A hybrid algorithm combining PSO and DE with a self-adaptive population sizing and migration strategy to balance exploration and exploitation.", "code": "import numpy as np\n\nclass AdaptiveHybridPSO_DE:\n    def __init__(self, budget=10000, dim=10, initial_pop_size=50, min_pop_size=10, max_pop_size=100, pso_weight=0.7, de_cr=0.9, de_f=0.8, migration_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.initial_pop_size = initial_pop_size\n        self.min_pop_size = min_pop_size\n        self.max_pop_size = max_pop_size\n        self.pop_size = initial_pop_size\n        self.pso_weight = pso_weight\n        self.de_cr = de_cr\n        self.de_f = de_f\n        self.migration_rate = migration_rate\n        self.population = None\n        self.fitness = None\n        self.velocities = None\n        self.best_positions = None\n        self.global_best_fitness = np.inf\n        self.global_best_position = None\n        self.eval_count = 0\n        self.pso_success_rate = 0.5\n        self.de_success_rate = 0.5\n        self.exploration_phase = True #Initially start with exploration\n\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.eval_count += self.pop_size\n        self.velocities = np.random.uniform(-1, 1, size=(self.pop_size, self.dim)) * 0.1\n        self.best_positions = self.population.copy()\n        self.global_best_position = self.population[np.argmin(self.fitness)].copy()\n        self.global_best_fitness = np.min(self.fitness)\n\n    def pso_update(self, func):\n        cognitive_weight = 1.5\n        social_weight = 1.5\n        success_count = 0\n\n        r1 = np.random.rand(self.pop_size, self.dim)\n        r2 = np.random.rand(self.pop_size, self.dim)\n\n        cognitive_component = cognitive_weight * r1 * (self.best_positions - self.population)\n        social_component = social_weight * r2 * (self.global_best_position - self.population)\n\n        self.velocities = self.pso_weight * self.velocities + cognitive_component + social_component\n        self.population = self.population + self.velocities\n\n        self.population = np.clip(self.population, func.bounds.lb, func.bounds.ub)\n\n        new_fitness = np.array([func(x) for x in self.population])\n        self.eval_count += self.pop_size\n\n        for i in range(self.pop_size):\n            if new_fitness[i] < self.fitness[i]:\n                self.best_positions[i] = self.population[i].copy()\n                self.fitness[i] = new_fitness[i]\n                success_count +=1\n\n\n        best_index = np.argmin(self.fitness)\n        if self.fitness[best_index] < self.global_best_fitness:\n            self.global_best_fitness = self.fitness[best_index]\n            self.global_best_position = self.population[best_index].copy()\n\n        return success_count/self.pop_size\n\n    def de_update(self, func):\n        success_count = 0\n        for i in range(self.pop_size):\n            idxs = np.random.choice(self.pop_size, 3, replace=False)\n            while i in idxs:\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n\n            x1, x2, x3 = self.population[idxs]\n\n            v = x1 + self.de_f * (x2 - x3)\n            v = np.clip(v, func.bounds.lb, func.bounds.ub)\n\n            u = np.copy(self.population[i])\n            j_rand = np.random.randint(self.dim)\n            for j in range(self.dim):\n                if np.random.rand() < self.de_cr or j == j_rand:\n                    u[j] = v[j]\n\n            f_u = func(u)\n            self.eval_count += 1\n\n            if f_u < self.fitness[i]:\n                self.population[i] = u\n                self.fitness[i] = f_u\n                success_count += 1\n\n                if f_u < self.global_best_fitness:\n                    self.global_best_fitness = f_u\n                    self.global_best_position = u.copy()\n            elif self.eval_count >= self.budget:\n                break\n        return success_count/self.pop_size\n\n    def migrate_individuals(self):\n        # Identify worst individuals\n        num_to_migrate = int(self.pop_size * self.migration_rate)\n        worst_indices = np.argsort(self.fitness)[-num_to_migrate:]\n\n        # Replace them with new random individuals\n        self.population[worst_indices] = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(num_to_migrate, self.dim))\n        self.fitness[worst_indices] = np.array([func(x) for x in self.population[worst_indices]])\n        self.eval_count += num_to_migrate\n\n        # Update best positions if necessary\n        for i in worst_indices:\n            if self.fitness[i] < self.global_best_fitness:\n                self.global_best_fitness = self.fitness[i]\n                self.global_best_position = self.population[i].copy()\n\n    def adjust_population_size(self):\n          # Example strategy: If PSO is more successful, increase population size\n        if self.pso_success_rate > self.de_success_rate and self.pop_size < self.max_pop_size:\n            increase_amount = min(int(0.1 * self.pop_size), self.max_pop_size - self.pop_size)\n            if increase_amount > 0:\n                new_individuals = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(increase_amount, self.dim))\n                new_fitness = np.array([func(x) for x in new_individuals])\n                self.eval_count += increase_amount\n\n                self.population = np.vstack((self.population, new_individuals))\n                self.fitness = np.concatenate((self.fitness, new_fitness))\n                self.velocities = np.vstack((self.velocities, np.random.uniform(-1, 1, size=(increase_amount, self.dim)) * 0.1))\n                self.best_positions = np.vstack((self.best_positions, new_individuals))\n                self.pop_size += increase_amount\n                \n                # Update global best if necessary\n                best_index = np.argmin(self.fitness)\n                if self.fitness[best_index] < self.global_best_fitness:\n                    self.global_best_fitness = self.fitness[best_index]\n                    self.global_best_position = self.population[best_index].copy()\n\n        # If DE is more successful and population is large, decrease population size\n        elif self.de_success_rate > self.pso_success_rate and self.pop_size > self.min_pop_size:\n            decrease_amount = min(int(0.1 * self.pop_size), self.pop_size - self.min_pop_size)\n            if decrease_amount > 0:\n                worst_indices = np.argsort(self.fitness)[-decrease_amount:]\n                keep_indices = np.setdiff1d(np.arange(self.pop_size), worst_indices)\n                \n                self.population = self.population[keep_indices]\n                self.fitness = self.fitness[keep_indices]\n                self.velocities = self.velocities[keep_indices]\n                self.best_positions = self.best_positions[keep_indices]\n                self.pop_size -= decrease_amount\n    \n\n    def __call__(self, func):\n        self.func = func # Store the function for use in population adjustments\n        self.initialize_population(func)\n\n        while self.eval_count < self.budget:\n            # Exploration vs. Exploitation phase\n            if self.exploration_phase:\n                #Migration during exploration\n                self.migrate_individuals()\n                # Preferentially use DE for exploration (parameter tuning can change this)\n                success_rate = self.de_update(func)\n                self.de_success_rate = 0.9*self.de_success_rate + 0.1*success_rate\n                self.pso_success_rate = 0.9*self.pso_success_rate\n            else:\n                #Adaptive switching mechanism during exploitation\n                if self.pso_success_rate > self.de_success_rate:\n                    success_rate = self.pso_update(func)\n                    self.pso_success_rate = 0.9*self.pso_success_rate + 0.1*success_rate\n                    self.de_success_rate = 0.9*self.de_success_rate\n                else:\n                    success_rate = self.de_update(func)\n                    self.de_success_rate = 0.9*self.de_success_rate + 0.1*success_rate\n                    self.pso_success_rate = 0.9*self.pso_success_rate\n            \n            self.adjust_population_size()\n\n            # Switch from exploration to exploitation\n            if self.eval_count > self.budget * 0.3 and self.exploration_phase:\n                self.exploration_phase = False\n                \n            elif self.eval_count >= self.budget:\n                break\n\n        return self.global_best_fitness, self.global_best_position", "configspace": "", "generation": 3, "feedback": "In the code, line 108, in migrate_individuals, the following error occurred:\nNameError: name 'func' is not defined. Did you mean: 'self.func'?\nOn line: self.population[worst_indices] = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(num_to_migrate, self.dim))", "error": "In the code, line 108, in migrate_individuals, the following error occurred:\nNameError: name 'func' is not defined. Did you mean: 'self.func'?\nOn line: self.population[worst_indices] = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(num_to_migrate, self.dim))", "parent_ids": ["45aaf31d-007f-4368-89e6-dd88b28599cd"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "8d98956f-47b3-434f-a2e1-7e6dd64bceb7", "fitness": 0.0, "name": "HybridPSO_DE_Adaptive", "description": "A simplified hybrid algorithm combining PSO and DE with adaptive parameter tuning based on performance feedback and a simplified orthogonal learning strategy.", "code": "import numpy as np\n\nclass HybridPSO_DE_Adaptive:\n    def __init__(self, budget=10000, dim=10, pop_size=20, pso_weight=0.7, de_cr=0.9, de_f=0.8):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.pso_weight = pso_weight\n        self.de_cr = de_cr\n        self.de_f = de_f\n        self.population = None\n        self.fitness = None\n        self.velocities = None\n        self.best_positions = None\n        self.global_best_fitness = np.inf\n        self.global_best_position = None\n        self.eval_count = 0\n        self.pso_success_rate = 0.5\n        self.de_success_rate = 0.5\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.eval_count += self.pop_size\n        self.velocities = np.random.uniform(-1, 1, size=(self.pop_size, self.dim)) * 0.1\n        self.best_positions = self.population.copy()\n        self.global_best_position = self.population[np.argmin(self.fitness)].copy()\n        self.global_best_fitness = np.min(self.fitness)\n\n    def pso_update(self, func):\n        cognitive_weight = 1.5\n        social_weight = 1.5\n        r1 = np.random.rand(self.pop_size, self.dim)\n        r2 = np.random.rand(self.pop_size, self.dim)\n\n        self.velocities = self.pso_weight * self.velocities + \\\n                          cognitive_weight * r1 * (self.best_positions - self.population) + \\\n                          social_weight * r2 * (self.global_best_position - self.population)\n        self.population = np.clip(self.population + self.velocities, func.bounds.lb, func.bounds.ub)\n\n        new_fitness = np.array([func(x) for x in self.population])\n        self.eval_count += self.pop_size\n\n        improved = new_fitness < self.fitness\n        self.best_positions[improved] = self.population[improved].copy()\n        self.fitness[improved] = new_fitness[improved]\n\n        best_index = np.argmin(self.fitness)\n        if self.fitness[best_index] < self.global_best_fitness:\n            self.global_best_fitness = self.fitness[best_index]\n            self.global_best_position = self.population[best_index].copy()\n\n        return np.mean(improved)\n\n    def de_update(self, func):\n        for i in range(self.pop_size):\n            idxs = np.random.choice(self.pop_size, 3, replace=False)\n            x1, x2, x3 = self.population[idxs]\n\n            v = x1 + self.de_f * (x2 - x3)\n            v = np.clip(v, func.bounds.lb, func.bounds.ub)\n\n            u = np.copy(self.population[i])\n            j_rand = np.random.randint(self.dim)\n            mask = np.random.rand(self.dim) < self.de_cr\n            mask[j_rand] = True\n            u[mask] = v[mask]\n\n            f_u = func(u)\n            self.eval_count += 1\n\n            if f_u < self.fitness[i]:\n                self.population[i] = u\n                self.fitness[i] = f_u\n\n                if f_u < self.global_best_fitness:\n                    self.global_best_fitness = f_u\n                    self.global_best_position = u.copy()\n                return 1\n        return 0\n\n    def orthogonal_learning(self, func):\n        # Select a random individual instead of top ones for diversification\n        index = np.random.randint(0,self.pop_size)\n        basis = self.population[index]\n\n        # Generate orthogonal array (simplified version - random perturbations)\n        oa = np.random.uniform(-0.1, 0.1, size=self.dim) # scaled perturbations\n\n        # Create new solutions based on orthogonal array\n        new_solution = basis + oa\n        new_solution = np.clip(new_solution, func.bounds.lb, func.bounds.ub)\n\n        f_new = func(new_solution)\n        self.eval_count += 1\n\n        # Replace a random individual if the new solution is better\n        replace_index = np.random.randint(0,self.pop_size)\n        if f_new < self.fitness[replace_index]:\n            self.population[replace_index] = new_solution\n            self.fitness[replace_index] = f_new\n\n            if f_new < self.global_best_fitness:\n                self.global_best_fitness = f_new\n                self.global_best_position = new_solution.copy()\n\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.eval_count < self.budget:\n            if self.pso_success_rate > self.de_success_rate:\n                success_rate = self.pso_update(func)\n                self.pso_success_rate = 0.9 * self.pso_success_rate + 0.1 * success_rate\n                self.de_success_rate = 0.9 * self.de_success_rate  # Decay the less successful rate\n\n            else:\n                success_rate = self.de_update(func)\n                self.de_success_rate = 0.9 * self.de_success_rate + 0.1 * success_rate\n                self.pso_success_rate = 0.9 * self.pso_success_rate  # Decay the less successful rate\n\n            if self.eval_count < self.budget * 0.8:  # Reduce orthogonal learning budget\n                self.orthogonal_learning(func)\n\n        return self.global_best_fitness, self.global_best_position", "configspace": "", "generation": 3, "feedback": "The algorithm HybridPSO_DE_Adaptive scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["45aaf31d-007f-4368-89e6-dd88b28599cd"], "operator": null, "metadata": {"aucs": [0]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "80e59bcc-39d1-4ef8-b508-9ea98cdbb797", "fitness": "-inf", "name": "HybridPSODE_Gradient_Restart", "description": "A novel hybrid algorithm combining PSO, DE, and a gradient-based local search, adaptively allocating evaluations based on performance feedback and utilizing a restart mechanism to escape local optima.", "code": "import numpy as np\n\nclass HybridPSODE_Gradient_Restart:\n    def __init__(self, budget=10000, dim=10, pop_size=20, pso_weight=0.7, de_cr=0.9, de_f=0.8, local_search_rate=0.1, restart_trigger=1000):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.pso_weight = pso_weight\n        self.de_cr = de_cr\n        self.de_f = de_f\n        self.local_search_rate = local_search_rate\n        self.restart_trigger = restart_trigger\n        self.population = None\n        self.fitness = None\n        self.velocities = None\n        self.best_positions = None\n        self.global_best_fitness = np.inf\n        self.global_best_position = None\n        self.eval_count = 0\n        self.pso_success_rate = 0.5\n        self.de_success_rate = 0.5\n        self.local_search_success_rate = 0.5\n        self.last_improvement = 0 #iteration of last improvement\n        self.restart_count = 0\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.eval_count += self.pop_size\n        self.velocities = np.random.uniform(-1, 1, size=(self.pop_size, self.dim)) * 0.1\n        self.best_positions = self.population.copy()\n        self.global_best_position = self.population[np.argmin(self.fitness)].copy()\n        self.global_best_fitness = np.min(self.fitness)\n        self.last_improvement = 0\n\n\n    def pso_update(self, func):\n        cognitive_weight = 1.5\n        social_weight = 1.5\n        success_count = 0\n\n        r1 = np.random.rand(self.pop_size, self.dim)\n        r2 = np.random.rand(self.pop_size, self.dim)\n\n        cognitive_component = cognitive_weight * r1 * (self.best_positions - self.population)\n        social_component = social_weight * r2 * (self.global_best_position - self.population)\n\n        self.velocities = self.pso_weight * self.velocities + cognitive_component + social_component\n        self.population = self.population + self.velocities\n\n        self.population = np.clip(self.population, func.bounds.lb, func.bounds.ub)\n\n        new_fitness = np.array([func(x) for x in self.population])\n        self.eval_count += self.pop_size\n\n        for i in range(self.pop_size):\n            if new_fitness[i] < self.fitness[i]:\n                self.best_positions[i] = self.population[i].copy()\n                self.fitness[i] = new_fitness[i]\n                success_count += 1\n\n\n        best_index = np.argmin(self.fitness)\n        if self.fitness[best_index] < self.global_best_fitness:\n            self.global_best_fitness = self.fitness[best_index]\n            self.global_best_position = self.population[best_index].copy()\n            self.last_improvement = self.eval_count\n\n\n        return success_count/self.pop_size\n\n    def de_update(self, func):\n        success_count = 0\n        for i in range(self.pop_size):\n            idxs = np.random.choice(self.pop_size, 3, replace=False)\n            while i in idxs:\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n\n            x1, x2, x3 = self.population[idxs]\n\n            v = x1 + self.de_f * (x2 - x3)\n            v = np.clip(v, func.bounds.lb, func.bounds.ub)\n\n            u = np.copy(self.population[i])\n            j_rand = np.random.randint(self.dim)\n            for j in range(self.dim):\n                if np.random.rand() < self.de_cr or j == j_rand:\n                    u[j] = v[j]\n\n            f_u = func(u)\n            self.eval_count += 1\n\n            if f_u < self.fitness[i]:\n                self.population[i] = u\n                self.fitness[i] = f_u\n                success_count += 1\n\n                if f_u < self.global_best_fitness:\n                    self.global_best_fitness = f_u\n                    self.global_best_position = u.copy()\n                    self.last_improvement = self.eval_count\n            elif self.eval_count >= self.budget:\n                break\n        return success_count/self.pop_size\n\n    def gradient_descent(self, func, x, learning_rate=0.01, iterations=5): #Simplified gradient descent\n        x_current = x.copy()\n        f_current = func(x_current)\n        self.eval_count += 1\n        for _ in range(iterations):\n            gradient = np.zeros(self.dim)\n            for i in range(self.dim):\n                x_plus = x_current.copy()\n                x_minus = x_current.copy()\n                delta = 1e-5 # Small delta\n                x_plus[i] += delta\n                x_minus[i] -= delta\n\n                #Bound Check\n                x_plus[i] = np.clip(x_plus[i], func.bounds.lb, func.bounds.ub)\n                x_minus[i] = np.clip(x_minus[i], func.bounds.lb, func.bounds.ub)\n\n                f_plus = func(x_plus)\n                f_minus = func(x_minus)\n                self.eval_count += 2\n\n                gradient[i] = (f_plus - f_minus) / (2 * delta)\n            x_new = x_current - learning_rate * gradient\n            x_new = np.clip(x_new, func.bounds.lb, func.bounds.ub)\n            f_new = func(x_new)\n            self.eval_count += 1\n\n            if f_new < f_current:\n                x_current = x_new\n                f_current = f_new\n            else:\n                break\n            if self.eval_count >= self.budget:\n                break\n        return f_current, x_current\n\n    def local_search(self, func):\n        success_count = 0\n        for i in range(self.pop_size):\n            if np.random.rand() < self.local_search_rate:\n                f_new, x_new = self.gradient_descent(func, self.population[i])\n\n                if f_new < self.fitness[i]:\n                    self.population[i] = x_new\n                    self.fitness[i] = f_new\n                    success_count += 1\n                    if f_new < self.global_best_fitness:\n                        self.global_best_fitness = f_new\n                        self.global_best_position = x_new.copy()\n                        self.last_improvement = self.eval_count\n                if self.eval_count >= self.budget:\n                    break\n        return success_count/self.pop_size\n\n\n    def restart_population(self, func):\n        self.restart_count += 1\n        print(\"Restarting Population\")\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.eval_count += self.pop_size\n        self.velocities = np.random.uniform(-1, 1, size=(self.pop_size, self.dim)) * 0.1\n        self.best_positions = self.population.copy()\n        # Keep the best individual from the previous population\n        self.population[0] = self.global_best_position.copy()\n        self.fitness[0] = self.global_best_fitness\n        self.best_positions[0] = self.global_best_position.copy()\n        self.last_improvement = self.eval_count\n\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.eval_count < self.budget:\n            # Adaptive switching mechanism\n            if self.pso_success_rate > self.de_success_rate and self.pso_success_rate > self.local_search_success_rate:\n                success_rate = self.pso_update(func)\n                self.pso_success_rate = 0.9*self.pso_success_rate + 0.1*success_rate\n                self.de_success_rate = 0.9*self.de_success_rate\n                self.local_search_success_rate = 0.9*self.local_search_success_rate\n\n\n            elif self.de_success_rate > self.pso_success_rate and self.de_success_rate > self.local_search_success_rate:\n                success_rate = self.de_update(func)\n                self.de_success_rate = 0.9*self.de_success_rate + 0.1*success_rate\n                self.pso_success_rate = 0.9*self.pso_success_rate\n                self.local_search_success_rate = 0.9*self.local_search_success_rate\n            else:\n                success_rate = self.local_search(func)\n                self.local_search_success_rate = 0.9*self.local_search_success_rate + 0.1*success_rate\n                self.pso_success_rate = 0.9*self.pso_success_rate\n                self.de_success_rate = 0.9*self.de_success_rate\n\n            # Restart mechanism if no improvement for a while\n            if self.eval_count - self.last_improvement > self.restart_trigger:\n                if self.restart_count < 3:\n                    self.restart_population(func)\n                else:\n                    break #terminate after 3 restarts if still no improvement\n        return self.global_best_fitness, self.global_best_position", "configspace": "", "generation": 3, "feedback": "In the code, line 120, in gradient_descent, the following error occurred:\nValueError: setting an array element with a sequence.\nOn line: x_plus[i] = np.clip(x_plus[i], func.bounds.lb, func.bounds.ub)", "error": "In the code, line 120, in gradient_descent, the following error occurred:\nValueError: setting an array element with a sequence.\nOn line: x_plus[i] = np.clip(x_plus[i], func.bounds.lb, func.bounds.ub)", "parent_ids": ["45aaf31d-007f-4368-89e6-dd88b28599cd"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "7c71128d-a674-4b3a-9486-c235568f0a1c", "fitness": 0.6408368064120306, "name": "EnhancedAdaptiveDE", "description": "A robust Adaptive Differential Evolution algorithm that employs a combination of covariance matrix adaptation, orthogonal learning, a mirrored sampling strategy, and dynamic parameter control to enhance exploration, exploitation, and convergence speed, while also incorporating a diversity maintenance mechanism to prevent premature convergence.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, CR=0.9, restart_patience=500, orthogonal_learning_rate=0.1, mirror_rate=0.1, diversity_threshold=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F  # Initial mutation factor\n        self.CR = CR  # Initial crossover rate\n        self.restart_patience = restart_patience\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.orthogonal_learning_rate = orthogonal_learning_rate\n        self.covariance_matrix = np.eye(dim)  # Initialize covariance matrix\n        self.mirror_rate = mirror_rate\n        self.diversity_threshold = diversity_threshold\n\n    def __call__(self, func):\n        # Initialize population\n        pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in pop])\n        self.budget -= self.pop_size\n        \n        best_idx = np.argmin(fitness)\n        self.f_opt = fitness[best_idx]\n        self.x_opt = pop[best_idx]\n\n        iteration = 0\n        no_improvement_count = 0\n\n        while self.budget > 0:\n            # Covariance Matrix Adaptation\n            if iteration % 10 == 0:\n                 self.covariance_matrix = np.cov(pop.T) + np.eye(self.dim) * 1e-6\n\n            # Calculate population diversity\n            diversity = np.std(pop)\n\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = pop[idxs]\n                \n                # Adaptive F: perturb F with a small probability, also considering diversity\n                F_current = self.F\n                if np.random.rand() < 0.1 or diversity < self.diversity_threshold:\n                    F_current = np.random.normal(self.F, 0.1)\n                    F_current = np.clip(F_current, 0.1, 1.0) #Keep within reasonable bounds\n\n                # Sample mutation vector from a multivariate normal distribution\n                mutation_vector = np.random.multivariate_normal(np.zeros(self.dim), self.covariance_matrix)\n                v = x_r1 + F_current * mutation_vector\n                \n                # Mirrored Sampling\n                if np.random.rand() < self.mirror_rate:\n                    v = 2 * x_r1 - v #Mirrored sampling\n\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)  # Keep within bounds\n\n                # Crossover\n                j_rand = np.random.randint(self.dim)\n                u = np.copy(pop[i])\n                \n                # Adaptive CR: perturb CR with a small probability\n                CR_current = self.CR\n                if np.random.rand() < 0.1:\n                    CR_current = np.random.normal(self.CR, 0.1)\n                    CR_current = np.clip(CR_current, 0.1, 1.0) #Keep within reasonable bounds\n                \n                for j in range(self.dim):\n                    if np.random.rand() < CR_current or j == j_rand:\n                        u[j] = v[j]\n\n                # Orthogonal Learning\n                if np.random.rand() < self.orthogonal_learning_rate:\n                    direction = self.x_opt - u\n                    u = u + np.random.rand() * direction\n\n                u = np.clip(u, func.bounds.lb, func.bounds.ub)\n\n                # Selection\n                f_u = func(u)\n                self.budget -= 1\n\n                if f_u < fitness[i]:\n                    fitness[i] = f_u\n                    pop[i] = u\n\n                    if f_u < self.f_opt:\n                        self.f_opt = f_u\n                        self.x_opt = u\n                        no_improvement_count = 0 # Reset counter\n                    else:\n                        no_improvement_count += 1\n                else:\n                    no_improvement_count += 1\n\n            # Restart mechanism\n            if no_improvement_count > self.restart_patience:\n                pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                fitness = np.array([func(x) for x in pop])\n                self.budget -= self.pop_size\n                \n                best_idx = np.argmin(fitness)\n                self.f_opt = fitness[best_idx]\n                self.x_opt = pop[best_idx]\n                no_improvement_count = 0  # Reset counter\n                self.covariance_matrix = np.eye(self.dim)\n            \n            iteration += 1\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "The algorithm EnhancedAdaptiveDE scored 0.641 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["4da23f0e-6f00-43ef-a7d4-f6df57870753"], "operator": null, "metadata": {"aucs": [0.3128429181314093, 0.7284331799190339, 0.6238340184220752, 0.8956857187622986, 0.7268174143191919, 0.7561866656524567, 0.5773289177453511, 0.5666252257327726, 0.6932420476371732, 0.6500465185396064, 0.8984592978830216, 0.9913799793094644, 0.5508333877145731, 0]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "73b6cd61-1b43-4708-9fc5-5becaa320443", "fitness": "-inf", "name": "EnhancedAdaptiveDE", "description": "Enhanced Adaptive DE with a combined orthogonal learning strategy and a diversity-based population management to balance exploration and exploitation.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, CR=0.9, restart_patience=500, ol_patience=100, diversity_threshold=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.restart_patience = restart_patience\n        self.ol_patience = ol_patience\n        self.diversity_threshold = diversity_threshold\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.rng = np.random.default_rng() # Use numpy random generator\n\n    def orthogonal_learning(self, pop, fitness, func, intensify=False):\n        \"\"\"\n        Performs orthogonal learning to generate a promising candidate.\n        'intensify' focuses the search around the best solution if true.\n        \"\"\"\n        if intensify:\n            best_idx = np.argmin(fitness)\n            x_best = pop[best_idx]\n        else:\n            # Select a random member of the population\n            idx = self.rng.integers(self.pop_size)\n            x_best = pop[idx]\n        \n        # Generate orthogonal array (simplified - can be replaced with a proper OA library if needed)\n        oa = np.array([[1, 1], [1, -1], [-1, 1], [-1, -1]]) # Very simple 2-factor OA\n        \n        candidate = np.copy(x_best)\n        for i in range(min(self.dim, oa.shape[0])): # Iterate until dimension exhausted\n            # Perturb each dimension based on OA\n            perturbation_scale = 0.05 if intensify else 0.1 # Smaller perturbation if intensifying\n            perturbed_candidate = x_best + perturbation_scale * oa[i % oa.shape[0]] * (func.bounds.ub - func.bounds.lb) # scale perturbation\n            perturbed_candidate = np.clip(perturbed_candidate, func.bounds.lb, func.bounds.ub)\n            \n            f_perturbed = func(perturbed_candidate)\n            self.budget -= 1\n            if f_perturbed < self.f_opt:\n                self.f_opt = f_perturbed\n                self.x_opt = perturbed_candidate\n            \n            if f_perturbed < func(candidate):\n                candidate = perturbed_candidate # keep best\n            \n            if self.budget <= 0:\n                break\n        return candidate\n\n    def calculate_diversity(self, pop):\n        \"\"\"\n        Calculates the diversity of the population. A simple measure is the \n        average distance from the mean.\n        \"\"\"\n        mean_pop = np.mean(pop, axis=0)\n        distances = np.linalg.norm(pop - mean_pop, axis=1)\n        diversity = np.mean(distances)\n        return diversity\n        \n\n    def __call__(self, func):\n        # Initialize population\n        pop = self.rng.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in pop])\n        self.budget -= self.pop_size\n\n        best_idx = np.argmin(fitness)\n        self.f_opt = fitness[best_idx]\n        self.x_opt = pop[best_idx]\n\n        iteration = 0\n        no_improvement_count = 0\n        ol_count = 0\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = self.rng.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = pop[idxs]\n\n                # Adaptive F\n                F_current = self.F * (1 + 0.1 * self.rng.normal(0, 1))\n                F_current = np.clip(F_current, 0.1, 1.0)\n\n                v = x_r1 + F_current * (x_r2 - x_r3)\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                j_rand = self.rng.integers(self.dim)\n                u = np.copy(pop[i])\n\n                # Adaptive CR\n                CR_current = self.CR * (1 + 0.1 * self.rng.normal(0, 1))\n                CR_current = np.clip(CR_current, 0.1, 1.0)\n\n                crossover_mask = self.rng.random(self.dim) < CR_current\n                u[crossover_mask] = v[crossover_mask]\n                u[j_rand] = v[j_rand] # Guarantee at least one change\n\n\n                # Selection\n                f_u = func(u)\n                self.budget -= 1\n\n                if f_u < fitness[i]:\n                    fitness[i] = f_u\n                    pop[i] = u\n\n                    if f_u < self.f_opt:\n                        self.f_opt = f_u\n                        self.x_opt = u\n                        no_improvement_count = 0\n                        ol_count = 0\n                    else:\n                        no_improvement_count += 1\n                else:\n                    no_improvement_count += 1\n\n            # Orthogonal Learning - Combined strategy\n            if ol_count > self.ol_patience and self.budget > 0:\n              # Apply orthogonal learning - intensify around the best\n              candidate_intensify = self.orthogonal_learning(pop, fitness, func, intensify=True)\n\n              #Apply orthogonal learning - explore a random area\n              candidate_explore = self.orthogonal_learning(pop, fitness, func, intensify=False)\n              ol_count = 0\n              best_fitness = np.min(fitness)\n\n              #Check if intensified candidate is better\n              f_candidate_intensify = func(candidate_intensify)\n              self.budget -=1\n              if f_candidate_intensify < best_fitness: #Direct replacement if better\n                best_idx = np.argmax(fitness) #replace worst\n                pop[best_idx] = candidate_intensify\n                fitness[best_idx] = f_candidate_intensify\n\n              #Check if explorative candidate is better\n              f_candidate_explore = func(candidate_explore)\n              self.budget -=1\n              if f_candidate_explore < best_fitness:\n                worst_idx = np.argmax(fitness)\n                pop[worst_idx] = candidate_explore\n                fitness[worst_idx] = f_candidate_explore\n            else:\n                ol_count +=1\n\n            # Restart mechanism\n            if no_improvement_count > self.restart_patience:\n                pop = self.rng.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                fitness = np.array([func(x) for x in pop])\n                self.budget -= self.pop_size\n\n                best_idx = np.argmin(fitness)\n                self.f_opt = fitness[best_idx]\n                self.x_opt = pop[best_idx]\n                no_improvement_count = 0\n                ol_count = 0\n\n            # Diversity check\n            diversity = self.calculate_diversity(pop)\n            if diversity < self.diversity_threshold:\n                # Introduce new random solutions to increase diversity\n                num_new = int(self.pop_size * 0.2)  # Replace 20% of the population\n                new_pop = self.rng.uniform(func.bounds.lb, func.bounds.ub, size=(num_new, self.dim))\n                new_fitness = np.array([func(x) for x in new_pop])\n                self.budget -= num_new\n\n                worst_indices = np.argsort(fitness)[-num_new:]  # Indices of worst solutions\n                pop[worst_indices] = new_pop\n                fitness[worst_indices] = new_fitness\n\n                best_idx = np.argmin(fitness)\n                self.f_opt = fitness[best_idx]\n                self.x_opt = pop[best_idx]\n                no_improvement_count = 0\n                ol_count = 0\n\n\n            iteration += 1\n            if self.budget <= 0:\n                break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "Evaluation timed out after 300 seconds.", "error": "Evaluation timed out after 300 seconds.", "parent_ids": ["7d9cb530-1ec3-4f38-96b1-6dfba1e93783"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "94aeb7c9-e527-4a39-a374-9eca795c0c60", "fitness": 0.0, "name": "AdaptiveDE", "description": "Adaptive Differential Evolution with a combined strategy of orthogonal learning, covariance matrix adaptation (CMA), and a niching mechanism to promote diversity and escape local optima.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, CR=0.9, restart_patience=500, ol_patience=100, niche_radius=0.5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.restart_patience = restart_patience\n        self.ol_patience = ol_patience\n        self.niche_radius = niche_radius\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.rng = np.random.default_rng()\n        self.C = np.eye(dim)  # Covariance matrix for CMA\n\n    def orthogonal_learning(self, pop, fitness, func):\n        \"\"\"\n        Performs orthogonal learning to generate a promising candidate.\n        \"\"\"\n        best_idx = np.argmin(fitness)\n        x_best = pop[best_idx]\n        \n        # Generate orthogonal array (simplified - can be replaced with a proper OA library if needed)\n        oa = np.array([[1, 1], [1, -1], [-1, 1], [-1, -1]]) # Very simple 2-factor OA\n        \n        candidate = np.copy(x_best)\n        for i in range(min(self.dim, oa.shape[0])): # Iterate until dimension exhausted\n            # Perturb each dimension based on OA\n            perturbed_candidate = x_best + 0.05 * oa[i % oa.shape[0]] * (func.bounds.ub - func.bounds.lb) # scale perturbation\n            perturbed_candidate = np.clip(perturbed_candidate, func.bounds.lb, func.bounds.ub)\n            \n            f_perturbed = func(perturbed_candidate)\n            self.budget -= 1\n            if f_perturbed < self.f_opt:\n                self.f_opt = f_perturbed\n                self.x_opt = perturbed_candidate\n            \n            if f_perturbed < func(candidate):\n                candidate = perturbed_candidate # keep best\n            \n            if self.budget <= 0:\n                break\n        return candidate\n\n    def cma_mutation(self, pop, best_idx, func):\n        \"\"\"\n        Performs mutation using Covariance Matrix Adaptation.\n        \"\"\"\n        x_best = pop[best_idx]\n        z = self.rng.normal(0, 1, self.dim)\n        v = x_best + self.F * np.sqrt(np.diag(self.C)) @ z\n        v = np.clip(v, func.bounds.lb, func.bounds.ub)\n        return v\n\n    def niching(self, pop, fitness, x, niche_radius):\n        \"\"\"\n        Applies a niching strategy to penalize solutions that are too close to each other.\n        \"\"\"\n        for i in range(self.pop_size):\n            if np.linalg.norm(pop[i] - x) < niche_radius:\n                fitness[i] *= 1.1  # Penalize solutions within the niche\n        return fitness\n\n    def update_covariance_matrix(self, pop, mean):\n        \"\"\"\n        Updates the covariance matrix C using the population.\n        \"\"\"\n        diff = pop - mean\n        self.C = (1 - 1/self.pop_size) * self.C + (1 / self.pop_size) * (diff.T @ diff)\n        # Ensure C is positive semi-definite\n        try:\n          np.linalg.cholesky(self.C)\n        except np.linalg.LinAlgError:\n          self.C = self.C + 1e-6 * np.eye(self.dim) # Regularize\n\n\n    def __call__(self, func):\n        # Initialize population\n        pop = self.rng.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in pop])\n        self.budget -= self.pop_size\n\n        best_idx = np.argmin(fitness)\n        self.f_opt = fitness[best_idx]\n        self.x_opt = pop[best_idx]\n\n        iteration = 0\n        no_improvement_count = 0\n        ol_count = 0\n\n        while self.budget > 0:\n            mean = np.mean(pop, axis=0)\n            self.update_covariance_matrix(pop, mean)\n\n            for i in range(self.pop_size):\n                # Mutation using CMA\n                v = self.cma_mutation(pop, best_idx, func)\n\n                # Crossover\n                j_rand = self.rng.integers(self.dim)\n                u = np.copy(pop[i])\n\n                # Adaptive CR\n                CR_current = self.CR * (1 + 0.1 * self.rng.normal(0, 1))\n                CR_current = np.clip(CR_current, 0.1, 1.0)\n\n                crossover_mask = self.rng.random(self.dim) < CR_current\n                u[crossover_mask] = v[crossover_mask]\n                u[j_rand] = v[j_rand] # Guarantee at least one change\n\n                # Selection\n                f_u = func(u)\n                self.budget -= 1\n\n                # Niching\n                fitness = self.niching(pop, fitness, u, self.niche_radius)\n\n                if f_u < fitness[i]:\n                    fitness[i] = f_u\n                    pop[i] = u\n\n                    if f_u < self.f_opt:\n                        self.f_opt = f_u\n                        self.x_opt = u\n                        no_improvement_count = 0\n                        ol_count = 0\n                    else:\n                        no_improvement_count += 1\n                else:\n                    no_improvement_count += 1\n\n            # Orthogonal Learning\n            if ol_count > self.ol_patience and self.budget > 0:\n              candidate = self.orthogonal_learning(pop, fitness, func)\n              ol_count = 0\n              best_fitness = np.min(fitness)\n              f_candidate = func(candidate)\n              self.budget -=1\n              if f_candidate < best_fitness: #Direct replacement if better\n                best_idx = np.argmax(fitness) #replace worst\n                pop[best_idx] = candidate\n                fitness[best_idx] = f_candidate\n            else:\n                ol_count +=1\n\n            # Restart mechanism\n            if no_improvement_count > self.restart_patience:\n                pop = self.rng.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                fitness = np.array([func(x) for x in pop])\n                self.budget -= self.pop_size\n\n                best_idx = np.argmin(fitness)\n                self.f_opt = fitness[best_idx]\n                self.x_opt = pop[best_idx]\n                no_improvement_count = 0\n                ol_count = 0\n\n            iteration += 1\n            if self.budget <= 0:\n                break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "The algorithm AdaptiveDE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["7d9cb530-1ec3-4f38-96b1-6dfba1e93783"], "operator": null, "metadata": {"aucs": [0]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "308f7646-5e52-469e-b4b3-cba4e2d1b127", "fitness": "-inf", "name": "CMAES_ODE_DRA", "description": "Covariance matrix adaptation with orthogonal design-based sampling and dynamic resource allocation.", "code": "import numpy as np\n\nclass CMAES_ODE_DRA:\n    def __init__(self, budget=10000, dim=10, pop_size=None, initial_sigma=0.2, restart_patience=500):\n        self.budget = budget\n        self.dim = dim\n        self.initial_sigma = initial_sigma\n        self.restart_patience = restart_patience\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.iteration = 0\n        self.no_improvement_count = 0\n        if pop_size is None:\n             self.pop_size = 4 + int(3 * np.log(self.dim)) #Default CMA-ES pop size\n        else:\n            self.pop_size = pop_size\n        self.mu = self.pop_size // 2  # Number of parents\n        self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n        self.weights /= np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        self.cc = (4 + self.mueff/self.dim) / (self.dim + 4 + 2*self.mueff/self.dim)\n        self.cs = (self.mueff + 2) / (self.dim + self.mueff + 5)\n        self.c1 = 2 / ((self.dim + 1.3)**2 + self.mueff)\n        self.cmu = min(1 - self.c1, 2 * (self.mueff - 2 + 1/self.mueff) / ((self.dim + 2.3)**2 + self.mueff))\n        self.damps = 1 + 2*max(0, np.sqrt((self.mueff-1)/(self.dim+1)) - 1) + self.cs\n        self.chiN = self.dim**0.5 * (1 - 1/(4*self.dim) + 1/(21*self.dim**2))\n\n    def __call__(self, func):\n        # Initialize CMA-ES\n        mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        sigma = self.initial_sigma\n        C = np.eye(self.dim)  # Covariance matrix\n        p_s = np.zeros(self.dim)\n        p_c = np.zeros(self.dim)\n\n        while self.budget > 0:\n            # Generate orthogonal design samples\n            Z = self.orthogonal_design_sampling(self.pop_size, self.dim)\n            \n            # Sample population\n            A = np.linalg.cholesky(C)\n            samples = mean + sigma * A @ Z.T\n            samples = np.clip(samples, func.bounds.lb, func.bounds.ub).T\n\n            # Evaluate samples\n            fitness = np.array([func(x) for x in samples])\n            self.budget -= self.pop_size\n            \n            # Sort by fitness\n            idx = np.argsort(fitness)\n            fitness = fitness[idx]\n            samples = samples[idx]\n\n            # Update best solution\n            if fitness[0] < self.f_opt:\n                self.f_opt = fitness[0]\n                self.x_opt = samples[0]\n                self.no_improvement_count = 0\n            else:\n                self.no_improvement_count += 1\n\n            # Update mean\n            mean_old = mean.copy()\n            mean = np.sum(self.weights[:, None] * samples[:self.mu], axis=0)\n\n            # Update evolution path for covariance matrix\n            B = np.linalg.solve(A, (mean - mean_old))\n            p_s = (1 - self.cs) * p_s + np.sqrt(self.cs * (2 - self.cs) * self.mueff) * B\n            norm_ps = np.linalg.norm(p_s)\n            C_rate = self.c1 * (p_c[:, None] @ p_c[None, :])\n            C_rate += self.cmu * ((self.weights * B) @ B.T)\n\n            if (norm_ps/self.chiN) < (1.4+2/(self.dim+1)):\n                C = (1 - self.c1 - self.cmu) * C + C_rate\n            else:\n                C = (1 - self.c1 - self.cmu + self.c1 * self.cmu / (self.cs)**2 *(1-(1 - self.cs)**2) )* C + C_rate\n            C = np.triu(C) + np.triu(C, 1).T\n            \n            p_c = (1 - self.cc) * p_c + np.sqrt(self.cc * (2 - self.cc) * self.mueff) * (mean - mean_old) / sigma\n            # Update step size\n            sigma *= np.exp((self.cs/self.damps) * (norm_ps/self.chiN - 1))\n            \n            # Restart mechanism\n            if self.no_improvement_count > self.restart_patience:\n                mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                sigma = self.initial_sigma\n                C = np.eye(self.dim)\n                p_s = np.zeros(self.dim)\n                p_c = np.zeros(self.dim)\n                self.no_improvement_count = 0\n            self.iteration+=1\n        return self.f_opt, self.x_opt\n\n    def orthogonal_design_sampling(self, pop_size, dim):\n        # Generate an orthogonal design matrix\n        H = self.hadamard_matrix(pop_size)\n        if H is None:  # If pop_size is not a power of 2, use random sampling\n            return np.random.randn(pop_size, dim)\n        else:\n            Z = H[:, :dim]\n            # Normalize to have standard normal distribution\n            for i in range(dim):\n                Z[:, i] = (Z[:, i] - np.mean(Z[:, i])) / np.std(Z[:, i])\n            return Z\n\n    def hadamard_matrix(self, n):\n        # Generate a Hadamard matrix of size n\n        if n == 1:\n            return np.array([[1]])\n        if n % 2 != 0:\n            return None  # Hadamard matrices only exist for sizes 1, 2, or multiples of 4\n        if not np.log2(n).is_integer():\n             return None\n\n        H = np.array([[1]])\n        while H.shape[0] < n:\n            H = np.vstack((np.hstack((H, H)), np.hstack((H, -H))))\n        return H", "configspace": "", "generation": 4, "feedback": "In the code, line 42, in __call__, the following error occurred:\nValueError: operands could not be broadcast together with shapes (2,) (2,6) \nOn line: samples = mean + sigma * A @ Z.T", "error": "In the code, line 42, in __call__, the following error occurred:\nValueError: operands could not be broadcast together with shapes (2,) (2,6) \nOn line: samples = mean + sigma * A @ Z.T", "parent_ids": ["edbf797c-dcc3-460f-8549-de041651be38"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "e9c70a20-a722-4916-afab-8b26a0421ce7", "fitness": 0.3254829475130744, "name": "CauchyMutationES", "description": "Population-level mutation with a focus on exploration via Cauchy distribution sampling and exploitation via adaptive step size control.", "code": "import numpy as np\n\nclass CauchyMutationES:\n    def __init__(self, budget=10000, dim=10, pop_size=20, initial_step_size=0.1, step_size_adaptation_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.initial_step_size = initial_step_size\n        self.step_size_adaptation_rate = step_size_adaptation_rate\n        self.population = None\n        self.fitness = None\n        self.step_sizes = None\n        self.best_fitness = np.inf\n        self.best_solution = None\n        self.eval_count = 0\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.eval_count += self.pop_size\n        self.step_sizes = np.full(self.pop_size, self.initial_step_size)\n        self.best_solution = self.population[np.argmin(self.fitness)].copy()\n        self.best_fitness = np.min(self.fitness)\n\n    def mutate(self, individual, step_size, func):\n        # Cauchy mutation for heavier tails and enhanced exploration\n        mutation = step_size * np.random.standard_cauchy(size=self.dim)\n        mutated_individual = individual + mutation\n        mutated_individual = np.clip(mutated_individual, func.bounds.lb, func.bounds.ub)\n        return mutated_individual\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.eval_count < self.budget:\n            for i in range(self.pop_size):\n                # Generate mutated individual using Cauchy mutation\n                mutated_individual = self.mutate(self.population[i], self.step_sizes[i], func)\n\n                # Evaluate the mutated individual\n                mutated_fitness = func(mutated_individual)\n                self.eval_count += 1\n\n                # Selection: compare mutated individual with the original\n                if mutated_fitness < self.fitness[i]:\n                    # Update population and fitness\n                    self.population[i] = mutated_individual\n                    self.fitness[i] = mutated_fitness\n\n                    # Adjust step size adaptively (success-based)\n                    self.step_sizes[i] *= np.exp(self.step_size_adaptation_rate)\n                else:\n                    # Reduce step size if mutation was not successful\n                    self.step_sizes[i] *= np.exp(-self.step_size_adaptation_rate)\n                    \n\n                # Update global best\n                if mutated_fitness < self.best_fitness:\n                    self.best_fitness = mutated_fitness\n                    self.best_solution = mutated_individual.copy()\n                elif self.eval_count >= self.budget:\n                    break\n\n        return self.best_fitness, self.best_solution", "configspace": "", "generation": 4, "feedback": "The algorithm CauchyMutationES scored 0.325 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["45aaf31d-007f-4368-89e6-dd88b28599cd"], "operator": null, "metadata": {"aucs": [0.09414119823885903, 0.19338184350483067, 0.37001641151203035, 0.2025971195974442, 0.22483084306035794, 0.3134478888038398, 0.27326227850993023, 0.22027932547337015, 0.23396096186124482, 0.1737809788359166, 0.20229726038124163, 0.9991401054727486, 0.3138634147846445, 0.30777033147943855, 0.6749327658921647, 0.3590391684059946, 0.2864416935266021, 0.4246749770671385, 0.16702312580740197, 0.47477725804628845]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "ef2b2e12-9c4a-4138-a7f4-7527af039fcd", "fitness": 0.32156190532356854, "name": "EnhancedAdaptiveDE", "description": "An enhanced Adaptive Differential Evolution with dynamically adjusted population size, mutation strategies and an external archive to improve exploration and exploitation.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, CR=0.9, restart_patience=500):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F  # Initial mutation factor\n        self.CR = CR  # Initial crossover rate\n        self.restart_patience = restart_patience\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.archive_size = int(self.pop_size * 0.2)  # Archive size, e.g., 20% of population\n        self.archive = []\n        self.min_pop_size = 10  # Minimum population size\n        self.pop_size_reduction_factor = 0.9  # Reduce pop_size by this factor if stagnant\n\n    def __call__(self, func):\n        # Initialize population\n        pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in pop])\n        self.budget -= self.pop_size\n        \n        best_idx = np.argmin(fitness)\n        self.f_opt = fitness[best_idx]\n        self.x_opt = pop[best_idx]\n\n        iteration = 0\n        no_improvement_count = 0\n        \n        # Adaptive population size\n        current_pop_size = self.pop_size\n\n        while self.budget > 0:\n            for i in range(current_pop_size):\n                # Mutation strategy selection\n                rand = np.random.rand()\n                if rand < 0.3:\n                    # DE/rand/1\n                    idxs = np.random.choice(current_pop_size, 3, replace=False)\n                    x_r1, x_r2, x_r3 = pop[idxs]\n\n                    # Optionally include an archive member\n                    if len(self.archive) > 0 and np.random.rand() < 0.1:\n                        idx_a = np.random.randint(len(self.archive))\n                        x_r1 = self.archive[idx_a]\n\n                    v = x_r1 + self.F * (x_r2 - x_r3)\n                elif rand < 0.6:\n                    # DE/current-to-best/1\n                    idxs = np.random.choice(current_pop_size, 2, replace=False)\n                    x_r1, x_r2 = pop[idxs]\n                    v = pop[i] + self.F * (self.x_opt - pop[i]) + self.F * (x_r1 - x_r2)\n                else:\n                    # DE/best/1\n                    idxs = np.random.choice(current_pop_size, 2, replace=False)\n                    x_r1, x_r2 = pop[idxs]\n                    v = self.x_opt + self.F * (x_r1 - x_r2)\n                    \n                v = np.clip(v, func.bounds.lb, func.bounds.ub)  # Keep within bounds\n\n                # Crossover\n                j_rand = np.random.randint(self.dim)\n                u = np.copy(pop[i])\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR or j == j_rand:\n                        u[j] = v[j]\n\n                # Selection using stochastic ranking\n                f_u = func(u)\n                self.budget -= 1\n                \n                #Stochastic ranking\n                if (f_u < fitness[i]) or (np.random.rand() < 0.1): #Stochastic ranking\n                    if f_u < fitness[i]:\n                        fitness[i] = f_u\n                        pop[i] = u\n\n                        if f_u < self.f_opt:\n                            self.f_opt = f_u\n                            self.x_opt = u\n                            no_improvement_count = 0 # Reset counter\n                        else:\n                            no_improvement_count += 1\n                    else:\n                         #Accept even if worse, but update archive\n                         if len(self.archive) < self.archive_size:\n                            self.archive.append(pop[i])\n                         else:\n                            #Replace a random archive element\n                            idx_replace = np.random.randint(len(self.archive))\n                            self.archive[idx_replace] = pop[i]\n                else:\n                    no_improvement_count += 1\n\n            # Reduce population size if stagnant\n            if no_improvement_count > self.restart_patience and current_pop_size > self.min_pop_size:\n                current_pop_size = max(int(current_pop_size * self.pop_size_reduction_factor), self.min_pop_size)\n                # Resize population and fitness arrays\n                pop = pop[:current_pop_size]\n                fitness = fitness[:current_pop_size]\n                print(f\"Population size reduced to: {current_pop_size}\") # Track the reduction\n\n            # Restart mechanism\n            if no_improvement_count > self.restart_patience:\n                pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(current_pop_size, self.dim))\n                fitness = np.array([func(x) for x in pop])\n                self.budget -= current_pop_size\n                \n                best_idx = np.argmin(fitness)\n                self.f_opt = fitness[best_idx]\n                self.x_opt = pop[best_idx]\n                no_improvement_count = 0  # Reset counter\n                self.archive = [] #Clear archive\n            \n            iteration += 1\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "The algorithm EnhancedAdaptiveDE scored 0.322 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["edbf797c-dcc3-460f-8549-de041651be38"], "operator": null, "metadata": {"aucs": [0.6431238106471371, 0]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "e536995d-b1b0-4d7a-8d7d-01bf94d4e999", "fitness": 0.0, "name": "DynPopCovDE", "description": "A Differential Evolution strategy with a dynamically adjusted population size and a local search operator guided by the covariance matrix of successful mutations.", "code": "import numpy as np\n\nclass DynPopCovDE:\n    def __init__(self, budget=10000, dim=10, initial_pop_size=50, min_pop_size=10, max_pop_size=100, F=0.5, CR=0.9, success_history_size=10):\n        self.budget = budget\n        self.dim = dim\n        self.initial_pop_size = initial_pop_size\n        self.min_pop_size = min_pop_size\n        self.max_pop_size = max_pop_size\n        self.pop_size = initial_pop_size\n        self.F = F\n        self.CR = CR\n        self.success_history_size = success_history_size\n        self.success_F = np.ones(success_history_size) * F\n        self.success_CR = np.ones(success_history_size) * CR\n        self.success_count = 0\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.rng = np.random.default_rng()\n        self.C = np.eye(dim) # Initialize Covariance Matrix\n\n    def update_covariance_matrix(self, successful_mutations):\n        \"\"\"\n        Updates the covariance matrix based on successful mutations.\n        \"\"\"\n        if len(successful_mutations) > 1:\n            successful_mutations = np.array(successful_mutations)\n            self.C = np.cov(successful_mutations, rowvar=False)\n            # Regularize covariance matrix to prevent singularity\n            self.C += 1e-6 * np.eye(self.dim)\n        else:\n            # If no successful mutations, keep the identity matrix\n            self.C = np.eye(self.dim)\n\n\n    def local_search(self, x, func):\n        \"\"\"\n        Performs a local search around the given solution using the covariance matrix.\n        \"\"\"\n        try:\n            z = self.rng.multivariate_normal(mean=np.zeros(self.dim), cov=self.C)\n        except np.linalg.LinAlgError: #catch non positive definite covariance\n            z = self.rng.normal(size=self.dim)\n\n        new_x = x + 0.01 * z  # Scale the step size\n        new_x = np.clip(new_x, func.bounds.lb, func.bounds.ub)\n        return new_x\n\n    def adjust_population_size(self, improvement_rate):\n        \"\"\"\n        Dynamically adjusts the population size based on the improvement rate.\n        \"\"\"\n        if improvement_rate > 0.2:  # High improvement, increase pop size\n            self.pop_size = min(self.pop_size + 1, self.max_pop_size)\n        elif improvement_rate < 0.05:  # Low improvement, decrease pop size\n            self.pop_size = max(self.pop_size - 1, self.min_pop_size)\n\n    def __call__(self, func):\n        # Initialize population\n        pop = self.rng.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in pop])\n        self.budget -= self.pop_size\n\n        best_idx = np.argmin(fitness)\n        self.f_opt = fitness[best_idx]\n        self.x_opt = pop[best_idx]\n\n        iteration = 0\n        successful_mutations = []\n\n        while self.budget > 0:\n            improvement_count = 0\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = self.rng.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = pop[idxs]\n\n                # Sample F and CR from history\n                F_current = self.rng.choice(self.success_F)\n                CR_current = self.rng.choice(self.success_CR)\n\n                v = x_r1 + F_current * (x_r2 - x_r3)\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                j_rand = self.rng.integers(self.dim)\n                u = np.copy(pop[i])\n                crossover_mask = self.rng.random(self.dim) < CR_current\n                u[crossover_mask] = v[crossover_mask]\n                u[j_rand] = v[j_rand]  # Guarantee at least one change\n\n                # Local Search\n                u_ls = self.local_search(u, func)\n\n                # Selection\n                f_u = func(u_ls)\n                self.budget -= 1\n\n                if f_u < fitness[i]:\n                    fitness[i] = f_u\n                    pop[i] = u_ls\n                    successful_mutations.append(u_ls - pop[i])  # Store the mutation vector\n\n\n                    if f_u < self.f_opt:\n                        self.f_opt = f_u\n                        self.x_opt = u_ls\n                        improvement_count += 1\n\n                        # Update success history\n                        self.success_F[self.success_count % self.success_history_size] = F_current\n                        self.success_CR[self.success_count % self.success_history_size] = CR_current\n                        self.success_count += 1\n\n            # Update Covariance Matrix\n            if len(successful_mutations) > 0:\n                self.update_covariance_matrix(successful_mutations)\n                successful_mutations = []  # Clear the list\n\n            # Adjust population size\n            improvement_rate = improvement_count / self.pop_size\n            self.adjust_population_size(improvement_rate)\n\n            # Resample population if outside bounds (edge case handling)\n            pop = np.clip(pop, func.bounds.lb, func.bounds.ub)\n\n            iteration += 1\n            if self.budget <= 0:\n                break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "The algorithm DynPopCovDE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["7d9cb530-1ec3-4f38-96b1-6dfba1e93783"], "operator": null, "metadata": {"aucs": [0]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "837ffcec-a654-4c0e-af9c-615e1cccf5ea", "fitness": "-inf", "name": "AdaptiveDE_NM_Archive", "description": "An adaptive Differential Evolution algorithm with a diversity-enhancing mutation strategy using a combination of current-to-best and random differential evolution, coupled with a local search based on Nelder-Mead simplex method for refinement and a population archive to preserve diversity.", "code": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveDE_NM_Archive:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, CR=0.9, archive_size=10, nm_iterations=5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.archive_size = archive_size\n        self.archive = []\n        self.nm_iterations = nm_iterations\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.rng = np.random.default_rng()\n\n    def diversity_enhanced_mutation(self, pop, i, best_idx):\n        \"\"\"\n        Combines current-to-best and random DE for enhanced diversity.\n        \"\"\"\n        idxs = self.rng.choice(self.pop_size, 4, replace=False)\n        x_r1, x_r2, x_r3, x_r4 = pop[idxs]\n        x_best = pop[best_idx]\n\n        #Probabilistically choose between current-to-best and random mutation\n        if self.rng.random() < 0.5:\n            v = pop[i] + self.F * (x_best - pop[i]) + self.F * (x_r1 - x_r2)\n        else:\n            v = x_r1 + self.F * (x_r2 - x_r3)\n            \n        return v\n\n    def local_search(self, x, func, bounds):\n        \"\"\"\n        Performs Nelder-Mead local search.\n        \"\"\"\n        res = minimize(func, x, method='Nelder-Mead', bounds=bounds, options={'maxiter': self.nm_iterations})\n        return res.fun, res.x\n\n    def update_archive(self, x, fitness):\n        \"\"\"\n        Maintains an archive of diverse solutions.\n        \"\"\"\n        if len(self.archive) < self.archive_size:\n            self.archive.append((x, fitness))\n        else:\n            # Replace the worst element in the archive\n            worst_idx = np.argmax([item[1] for item in self.archive])\n            if fitness < self.archive[worst_idx][1]:\n                self.archive[worst_idx] = (x, fitness)\n\n    def __call__(self, func):\n        # Initialize population\n        pop = self.rng.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in pop])\n        self.budget -= self.pop_size\n\n        best_idx = np.argmin(fitness)\n        self.f_opt = fitness[best_idx]\n        self.x_opt = pop[best_idx]\n\n        iteration = 0\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation\n                best_idx = np.argmin(fitness)\n                v = self.diversity_enhanced_mutation(pop, i, best_idx)\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                j_rand = self.rng.integers(self.dim)\n                u = np.copy(pop[i])\n\n                crossover_mask = self.rng.random(self.dim) < self.CR\n                u[crossover_mask] = v[crossover_mask]\n                u[j_rand] = v[j_rand]\n\n                # Selection\n                f_u = func(u)\n                self.budget -= 1\n\n                #Local Search\n                f_u, u = self.local_search(u, func, bounds=[(func.bounds.lb, func.bounds.ub)] * self.dim)\n\n                if self.budget <=0:\n                  break\n\n                if f_u < fitness[i]:\n                    fitness[i] = f_u\n                    pop[i] = u\n\n                    if f_u < self.f_opt:\n                        self.f_opt = f_u\n                        self.x_opt = u\n\n                    self.update_archive(u, f_u)\n\n            iteration += 1\n            if self.budget <= 0:\n                break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "In the code, line 38, in local_search, the following error occurred:\nNameError: name 'minimize' is not defined\nOn line: res = minimize(func, x, method='Nelder-Mead', bounds=bounds, options={'maxiter': self.nm_iterations})", "error": "In the code, line 38, in local_search, the following error occurred:\nNameError: name 'minimize' is not defined\nOn line: res = minimize(func, x, method='Nelder-Mead', bounds=bounds, options={'maxiter': self.nm_iterations})", "parent_ids": ["7d9cb530-1ec3-4f38-96b1-6dfba1e93783"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "216f7ebd-6982-42c8-b4c9-4d9a5913591f", "fitness": "-inf", "name": "AdaptiveDE", "description": "Adaptive Differential Evolution with a simplified orthogonal learning strategy and a periodic local search to refine promising solutions.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, CR=0.9, local_search_interval=50):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.local_search_interval = local_search_interval\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.rng = np.random.default_rng()\n\n    def local_search(self, x, func, step_size=0.1):\n        \"\"\"\n        Performs a simple local search around a given solution.\n        \"\"\"\n        x_new = x.copy()\n        for i in range(self.dim):\n            # Perturb each dimension slightly\n            delta = self.rng.uniform(-step_size, step_size) * (func.bounds.ub - func.bounds.lb)\n            x_new[i] = x[i] + delta\n            x_new[i] = np.clip(x_new[i], func.bounds.lb, func.bounds.ub)\n        \n        f_new = func(x_new)\n        self.budget -= 1\n        if f_new < self.f_opt:\n            self.f_opt = f_new\n            self.x_opt = x_new\n        \n        return x_new, f_new\n\n\n    def orthogonal_crossover(self, x, pop, func):\n        \"\"\"\n        Performs orthogonal crossover to generate a candidate.\n        \"\"\"\n        idxs = self.rng.choice(self.pop_size, 2, replace=False)\n        x_r1, x_r2 = pop[idxs]\n\n        candidate = x.copy()\n        for i in range(self.dim):\n            candidate[i] = self.rng.choice([x[i], x_r1[i], x_r2[i]])\n        \n        candidate = np.clip(candidate, func.bounds.lb, func.bounds.ub)\n        f_candidate = func(candidate)\n        self.budget -= 1\n\n        if f_candidate < self.f_opt:\n            self.f_opt = f_candidate\n            self.x_opt = candidate\n        \n        return candidate, f_candidate\n\n    def __call__(self, func):\n        # Initialize population\n        pop = self.rng.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in pop])\n        self.budget -= self.pop_size\n\n        best_idx = np.argmin(fitness)\n        self.f_opt = fitness[best_idx]\n        self.x_opt = pop[best_idx]\n\n        iteration = 0\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = self.rng.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = pop[idxs]\n\n                # Adaptive F\n                F_current = self.F * (1 + 0.1 * self.rng.normal(0, 1))\n                F_current = np.clip(F_current, 0.1, 1.0)\n\n                v = x_r1 + F_current * (x_r2 - x_r3)\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                j_rand = self.rng.integers(self.dim)\n                u = np.copy(pop[i])\n\n                # Adaptive CR\n                CR_current = self.CR * (1 + 0.1 * self.rng.normal(0, 1))\n                CR_current = np.clip(CR_current, 0.1, 1.0)\n\n                crossover_mask = self.rng.random(self.dim) < CR_current\n                u[crossover_mask] = v[crossover_mask]\n                u[j_rand] = v[j_rand] # Guarantee at least one change\n\n                # Selection\n                f_u = func(u)\n                self.budget -= 1\n\n                if f_u < fitness[i]:\n                    fitness[i] = f_u\n                    pop[i] = u\n\n                    if f_u < self.f_opt:\n                        self.f_opt = f_u\n                        self.x_opt = u\n\n\n            # Orthogonal Crossover\n            for i in range(self.pop_size):\n              candidate, f_candidate = self.orthogonal_crossover(pop[i], pop, func)\n              if f_candidate < fitness[i]:\n                  fitness[i] = f_candidate\n                  pop[i] = candidate\n\n\n            # Local Search\n            if iteration % self.local_search_interval == 0:\n                best_idx = np.argmin(fitness)\n                pop[best_idx], fitness[best_idx] = self.local_search(pop[best_idx], func)\n                if fitness[best_idx] < self.f_opt:\n                    self.f_opt = fitness[best_idx]\n                    self.x_opt = pop[best_idx]\n\n            iteration += 1\n            if self.budget <= 0:\n                break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "In the code, line 23, in local_search, the following error occurred:\nValueError: setting an array element with a sequence.\nOn line: x_new[i] = x[i] + delta", "error": "In the code, line 23, in local_search, the following error occurred:\nValueError: setting an array element with a sequence.\nOn line: x_new[i] = x[i] + delta", "parent_ids": ["7d9cb530-1ec3-4f38-96b1-6dfba1e93783"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "948b3002-fa0d-4178-864a-40f239ebf032", "fitness": 0.26525744106745275, "name": "AdaptiveDE", "description": "Adaptive Differential Evolution with aging population, orthogonal learning, and covariance matrix adaptation for enhanced exploration and exploitation, prioritizing diversity and convergence speed.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, CR=0.9, restart_patience=500, age_limit=50):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.restart_patience = restart_patience\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.archive_size = int(self.pop_size * 0.2)\n        self.archive = []\n        self.age_limit = age_limit\n        self.ages = np.zeros(self.pop_size, dtype=int)\n        self.C = np.eye(dim)  # Covariance matrix for CMA-ES-like adaptation\n\n    def __call__(self, func):\n        pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in pop])\n        self.budget -= self.pop_size\n        \n        best_idx = np.argmin(fitness)\n        self.f_opt = fitness[best_idx]\n        self.x_opt = pop[best_idx]\n\n        iteration = 0\n        no_improvement_count = 0\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = pop[idxs]\n\n                # Optionally include an archive member\n                if len(self.archive) > 0 and np.random.rand() < 0.1:\n                    idx_a = np.random.randint(len(self.archive))\n                    x_r1 = self.archive[idx_a]\n                \n                # Adaptive F\n                if np.random.rand() < 0.1:\n                    F_current = np.random.normal(self.F, 0.1)\n                else:\n                    F_current = self.F\n                F_current = np.clip(F_current, 0.1, 1.0)\n                \n                v = x_r1 + F_current * (x_r2 - x_r3)\n\n                #CMA-ES like adaptation to sample using the Covariance matrix\n                z = np.random.multivariate_normal(np.zeros(self.dim), self.C)\n                v = x_r1 + F_current * z\n\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                j_rand = np.random.randint(self.dim)\n                u = np.copy(pop[i])\n                \n                # Adaptive CR\n                if np.random.rand() < 0.1:\n                    CR_current = np.random.normal(self.CR, 0.1)\n                else:\n                    CR_current = self.CR\n                CR_current = np.clip(CR_current, 0.1, 1.0)\n                \n                for j in range(self.dim):\n                    if np.random.rand() < CR_current or j == j_rand:\n                        u[j] = v[j]\n\n                # Selection\n                f_u = func(u)\n                self.budget -= 1\n                \n                if f_u < fitness[i]:\n                    fitness[i] = f_u\n                    pop[i] = u\n                    self.ages[i] = 0  # Reset age\n\n                    if f_u < self.f_opt:\n                        self.f_opt = f_u\n                        self.x_opt = u\n                        no_improvement_count = 0\n                        #Update Covariance matrix\n                        diff = u - x_r1\n                        self.C = (1 - 0.1) * self.C + 0.1 * np.outer(diff, diff)\n\n                    else:\n                        no_improvement_count += 1\n                else:\n                    self.ages[i] += 1\n                    no_improvement_count += 1\n                    # Archive worse solutions with probability\n                    if np.random.rand() < 0.05:\n                        if len(self.archive) < self.archive_size:\n                            self.archive.append(pop[i])\n                        else:\n                            idx_replace = np.random.randint(len(self.archive))\n                            self.archive[idx_replace] = pop[i]\n\n            # Aging and replacement\n            for i in range(self.pop_size):\n                if self.ages[i] > self.age_limit:\n                    pop[i] = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.dim))\n                    fitness[i] = func(pop[i])\n                    self.budget -= 1\n                    self.ages[i] = 0\n                    if fitness[i] < self.f_opt:\n                        self.f_opt = fitness[i]\n                        self.x_opt = pop[i]\n                        no_improvement_count = 0 # Reset if better than best\n\n            # Restart mechanism\n            if no_improvement_count > self.restart_patience:\n                pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                fitness = np.array([func(x) for x in pop])\n                self.budget -= self.pop_size\n                \n                best_idx = np.argmin(fitness)\n                self.f_opt = fitness[best_idx]\n                self.x_opt = pop[best_idx]\n                no_improvement_count = 0\n                self.archive = []\n                self.ages = np.zeros(self.pop_size, dtype=int)\n                self.C = np.eye(self.dim)\n            \n            iteration += 1\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "The algorithm AdaptiveDE scored 0.265 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["edbf797c-dcc3-460f-8549-de041651be38"], "operator": null, "metadata": {"aucs": [0.17657369618724716, 0.30464507694021814, 0.3689675619303763, 0.47610087027942205, 0]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "c15d0bc1-be7e-47f6-b62e-868af95fab3b", "fitness": "-inf", "name": "AdaptiveDELocalSearch", "description": "An adaptive Differential Evolution strategy with a dynamically adjusted population size and a local search component triggered probabilistically for intensification.", "code": "import numpy as np\n\nclass AdaptiveDELocalSearch:\n    def __init__(self, budget=10000, dim=10, initial_pop_size=50, min_pop_size=20, max_pop_size=100, F=0.5, CR=0.9, restart_patience=500, local_search_prob=0.05):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = initial_pop_size\n        self.min_pop_size = min_pop_size\n        self.max_pop_size = max_pop_size\n        self.F = F  # Initial mutation factor\n        self.CR = CR  # Initial crossover rate\n        self.restart_patience = restart_patience\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.archive_size = int(self.pop_size * 0.2)  # Archive size, e.g., 20% of population\n        self.archive = []\n        self.local_search_prob = local_search_prob\n\n    def local_search(self, x, func, step_size=0.1):\n        \"\"\"Performs a simple local search around the given solution.\"\"\"\n        x_new = x.copy()\n        for i in range(self.dim):\n            # Explore both directions for each dimension\n            delta = np.random.uniform(-step_size, step_size)\n            x_new[i] += delta\n\n            # Clip to bounds\n            x_new[i] = np.clip(x_new[i], func.bounds.lb, func.bounds.ub)\n\n        f_new = func(x_new)\n        self.budget -= 1\n        if f_new < self.f_opt:\n            self.f_opt = f_new\n            self.x_opt = x_new\n        return f_new, x_new\n\n    def __call__(self, func):\n        # Initialize population\n        pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in pop])\n        self.budget -= self.pop_size\n        \n        best_idx = np.argmin(fitness)\n        self.f_opt = fitness[best_idx]\n        self.x_opt = pop[best_idx]\n\n        iteration = 0\n        no_improvement_count = 0\n\n        while self.budget > 0:\n            # Dynamically adjust population size (simplified - can be enhanced with more sophisticated adaptation)\n            if no_improvement_count > self.restart_patience / 2 and self.pop_size > self.min_pop_size:\n                self.pop_size = max(self.min_pop_size, int(self.pop_size * 0.9))  # Reduce population size\n            elif no_improvement_count < self.restart_patience / 4 and self.pop_size < self.max_pop_size:\n                self.pop_size = min(self.max_pop_size, int(self.pop_size * 1.1))  # Increase population size\n\n\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = pop[idxs]\n\n                # Optionally include an archive member\n                if len(self.archive) > 0 and np.random.rand() < 0.1:\n                    idx_a = np.random.randint(len(self.archive))\n                    x_r1 = self.archive[idx_a]\n                \n                # Adaptive F: perturb F with a small probability, use a better distribution\n                if np.random.rand() < 0.1:\n                    F_current = np.random.normal(self.F, 0.1)\n                else:\n                    F_current = self.F\n                F_current = np.clip(F_current, 0.1, 1.0)\n                \n                v = x_r1 + F_current * (x_r2 - x_r3)\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)  # Keep within bounds\n\n                # Crossover\n                j_rand = np.random.randint(self.dim)\n                u = np.copy(pop[i])\n                \n                # Adaptive CR: perturb CR with a small probability\n                if np.random.rand() < 0.1:\n                    CR_current = np.random.normal(self.CR, 0.1)\n                else:\n                    CR_current = self.CR\n                CR_current = np.clip(CR_current, 0.1, 1.0)\n                \n                for j in range(self.dim):\n                    if np.random.rand() < CR_current or j == j_rand:\n                        u[j] = v[j]\n\n                # Selection using stochastic ranking\n                f_u = func(u)\n                self.budget -= 1\n                \n                #Stochastic ranking\n                if (f_u < fitness[i]) or (np.random.rand() < 0.1): #Stochastic ranking\n                    if f_u < fitness[i]:\n                        fitness[i] = f_u\n                        pop[i] = u\n\n                        if f_u < self.f_opt:\n                            self.f_opt = f_u\n                            self.x_opt = u\n                            no_improvement_count = 0 # Reset counter\n                        else:\n                            no_improvement_count += 1\n                    else:\n                         #Accept even if worse, but update archive\n                         if len(self.archive) < self.archive_size:\n                            self.archive.append(pop[i])\n                         else:\n                            #Replace a random archive element\n                            idx_replace = np.random.randint(len(self.archive))\n                            self.archive[idx_replace] = pop[i]\n                else:\n                    no_improvement_count += 1\n\n                # Local Search (Intensification)\n                if np.random.rand() < self.local_search_prob:\n                    f_ls, x_ls = self.local_search(pop[i], func)\n                    if f_ls < fitness[i]:\n                        fitness[i] = f_ls\n                        pop[i] = x_ls\n\n\n            # Restart mechanism\n            if no_improvement_count > self.restart_patience:\n                pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                fitness = np.array([func(x) for x in pop])\n                self.budget -= self.pop_size\n                \n                best_idx = np.argmin(fitness)\n                self.f_opt = fitness[best_idx]\n                self.x_opt = pop[best_idx]\n                no_improvement_count = 0  # Reset counter\n                self.archive = [] #Clear archive\n                self.pop_size = initial_pop_size # Reset population size\n\n            \n            iteration += 1\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "In the code, line 61, in __call__, the following error occurred:\nIndexError: index 54 is out of bounds for axis 0 with size 50\nOn line: x_r1, x_r2, x_r3 = pop[idxs]", "error": "In the code, line 61, in __call__, the following error occurred:\nIndexError: index 54 is out of bounds for axis 0 with size 50\nOn line: x_r1, x_r2, x_r3 = pop[idxs]", "parent_ids": ["edbf797c-dcc3-460f-8549-de041651be38"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "f172c59b-17b7-4a64-83c6-052a817db91b", "fitness": 0.0, "name": "AdaptiveNeighborhoodDE", "description": "Self-adaptive Differential Evolution with Neighborhood Search and dynamic parameter control, focusing on local refinement around promising solutions.", "code": "import numpy as np\n\nclass AdaptiveNeighborhoodDE:\n    def __init__(self, budget=10000, dim=10, pop_size=40, F=0.5, CR=0.9, neighborhood_size=5, adaptation_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.neighborhood_size = neighborhood_size\n        self.adaptation_rate = adaptation_rate\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        # Initialize population\n        pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in pop])\n        self.budget -= self.pop_size\n\n        best_idx = np.argmin(fitness)\n        self.f_opt = fitness[best_idx]\n        self.x_opt = pop[best_idx]\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Neighborhood Selection\n                neighbors_idx = np.random.choice(self.pop_size, self.neighborhood_size, replace=False)\n                neighbors_idx = np.append(neighbors_idx, i) # Include current individual\n\n                # Mutation\n                idxs = np.random.choice(neighbors_idx, 3, replace=False)\n                x_r1, x_r2, x_r3 = pop[idxs]\n\n                v = x_r1 + self.F * (x_r2 - x_r3)\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                j_rand = np.random.randint(self.dim)\n                u = np.copy(pop[i])\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR or j == j_rand:\n                        u[j] = v[j]\n\n                u = np.clip(u, func.bounds.lb, func.bounds.ub)\n\n                # Selection\n                f_u = func(u)\n                self.budget -= 1\n\n                if f_u < fitness[i]:\n                    fitness[i] = f_u\n                    pop[i] = u\n\n                    if f_u < self.f_opt:\n                        self.f_opt = f_u\n                        self.x_opt = u\n\n                        # Parameter Adaptation: Adapt F and CR if improvement found\n                        self.F = np.clip(self.F + self.adaptation_rate * np.random.normal(0, 1), 0.1, 1.0)\n                        self.CR = np.clip(self.CR + self.adaptation_rate * np.random.normal(0, 1), 0.1, 1.0)\n\n                # Local Refinement: Perturb best solution locally\n                if np.random.rand() < 0.05: # Small probability to perturb x_opt\n                    perturbation = np.random.normal(0, 0.01, size=self.dim)\n                    x_perturbed = np.clip(self.x_opt + perturbation, func.bounds.lb, func.bounds.ub)\n                    f_perturbed = func(x_perturbed)\n                    self.budget -= 1\n\n                    if f_perturbed < self.f_opt:\n                        self.f_opt = f_perturbed\n                        self.x_opt = x_perturbed\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "The algorithm AdaptiveNeighborhoodDE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["4da23f0e-6f00-43ef-a7d4-f6df57870753"], "operator": null, "metadata": {"aucs": [0]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "ce441a8d-45ff-4458-a93f-b1ed33b7c9d7", "fitness": 0.38757368988924806, "name": "SelfOrganizingScouts", "description": "Self-organizing scouts that dynamically adjust their search radius based on local fitness landscape to refine the search.", "code": "import numpy as np\n\nclass SelfOrganizingScouts:\n    def __init__(self, budget=10000, dim=10, num_scouts=20, initial_radius=0.5, radius_decay=0.99, stagnation_threshold=500):\n        self.budget = budget\n        self.dim = dim\n        self.num_scouts = num_scouts\n        self.initial_radius = initial_radius\n        self.radius_decay = radius_decay\n        self.stagnation_threshold = stagnation_threshold\n        self.scouts = None\n        self.radii = None\n        self.fitness = None\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.no_improvement_count = 0\n\n    def initialize_scouts(self, func):\n        self.scouts = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.num_scouts, self.dim))\n        self.radii = np.full(self.num_scouts, self.initial_radius)\n        self.fitness = np.array([func(x) for x in self.scouts])\n        self.budget -= self.num_scouts\n\n        best_idx = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_idx]\n        self.x_opt = self.scouts[best_idx]\n\n    def scout_move(self, func, scout_idx):\n        # Generate a new position within the scout's radius\n        new_pos = self.scouts[scout_idx] + np.random.uniform(-self.radii[scout_idx], self.radii[scout_idx], size=self.dim)\n        new_pos = np.clip(new_pos, func.bounds.lb, func.bounds.ub)\n\n        # Evaluate the new position\n        new_fitness = func(new_pos)\n        self.budget -= 1\n\n        # Accept the new position if it's better\n        if new_fitness < self.fitness[scout_idx]:\n            self.fitness[scout_idx] = new_fitness\n            self.scouts[scout_idx] = new_pos\n\n            # Update global best\n            if new_fitness < self.f_opt:\n                self.f_opt = new_fitness\n                self.x_opt = new_pos\n                self.no_improvement_count = 0  # Reset stagnation counter\n        else:\n            #Reduce radius if no improvement\n            self.radii[scout_idx] *= self.radius_decay\n\n    def __call__(self, func):\n        self.initialize_scouts(func)\n\n        while self.budget > 0:\n            for i in range(self.num_scouts):\n                self.scout_move(func, i)\n            \n            #Stagnation check\n            self.no_improvement_count += 1\n            if self.no_improvement_count > self.stagnation_threshold:\n                #Re-initialize scouts if stagnated\n                self.initialize_scouts(func)\n                self.no_improvement_count = 0  # Reset counter\n            \n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "The algorithm SelfOrganizingScouts scored 0.388 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["edbf797c-dcc3-460f-8549-de041651be38"], "operator": null, "metadata": {"aucs": [0.09647210992434607, 0.20168424675502383, 0.43985730365361575, 0.5985073543084087, 0.40206760363943894, 0.4421532503089437, 0.23884346248737887, 0.3745114263560412, 0.24495486538113787, 0.18388496431101853, 0.608391716439271, 0.994647584821519, 0.30814888990230793, 0.18318490430721945, 0.7376515599576459, 0.3693423609294276, 0.3640392401006384, 0.5871955665252759, 0.16583836736887403, 0.21009702030742872]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "26cd97c3-a361-40ed-97d1-edd8fe3c0199", "fitness": "-inf", "name": "HybridPSO_DE_Adaptive", "description": "Integrates PSO and DE with adaptive parameter control based on performance, uses orthogonal learning for local refinement, and introduces a restart mechanism triggered by stagnation to enhance exploration.", "code": "import numpy as np\n\nclass HybridPSO_DE_Adaptive:\n    def __init__(self, budget=10000, dim=10, pop_size=20, pso_weight=0.7, de_cr=0.9, de_f=0.8, ol_size=5, stagnation_threshold=1000):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.pso_weight = pso_weight\n        self.de_cr = de_cr\n        self.de_f = de_f\n        self.ol_size = ol_size  # Number of individuals for orthogonal learning\n        self.population = None\n        self.fitness = None\n        self.velocities = None\n        self.best_positions = None\n        self.global_best_fitness = np.inf\n        self.global_best_position = None\n        self.eval_count = 0\n        self.pso_success_rate = 0.5 #initial pso success rate\n        self.de_success_rate = 0.5 # initial de success rate\n        self.stagnation_threshold = stagnation_threshold\n        self.last_improvement = 0 # Evaluation counter of the last improvement\n        self.restart_flag = False #Flag to restart population when stagnation is detected\n        self.adaptive_pso_weight = self.pso_weight\n        self.adaptive_de_cr = self.de_cr\n        self.adaptive_de_f = self.de_f\n\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.eval_count += self.pop_size\n        self.velocities = np.random.uniform(-1, 1, size=(self.pop_size, self.dim)) * 0.1\n        self.best_positions = self.population.copy()\n        self.global_best_position = self.population[np.argmin(self.fitness)].copy()\n        self.global_best_fitness = np.min(self.fitness)\n        self.last_improvement = self.eval_count\n\n    def pso_update(self, func):\n        cognitive_weight = 1.5\n        social_weight = 1.5\n        success_count = 0\n\n        r1 = np.random.rand(self.pop_size, self.dim)\n        r2 = np.random.rand(self.pop_size, self.dim)\n\n        cognitive_component = cognitive_weight * r1 * (self.best_positions - self.population)\n        social_component = social_weight * r2 * (self.global_best_position - self.population)\n\n        self.velocities = self.adaptive_pso_weight * self.velocities + cognitive_component + social_component\n        self.population = self.population + self.velocities\n\n        self.population = np.clip(self.population, func.bounds.lb, func.bounds.ub)\n\n        new_fitness = np.array([func(x) for x in self.population])\n        self.eval_count += self.pop_size\n\n        for i in range(self.pop_size):\n            if new_fitness[i] < self.fitness[i]:\n                self.best_positions[i] = self.population[i].copy()\n                self.fitness[i] = new_fitness[i]\n                success_count +=1\n\n\n        best_index = np.argmin(self.fitness)\n        if self.fitness[best_index] < self.global_best_fitness:\n            self.global_best_fitness = self.fitness[best_index]\n            self.global_best_position = self.population[best_index].copy()\n            self.last_improvement = self.eval_count\n\n        return success_count/self.pop_size\n\n    def de_update(self, func):\n        success_count = 0\n        for i in range(self.pop_size):\n            idxs = np.random.choice(self.pop_size, 3, replace=False)\n            while i in idxs:\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n\n            x1, x2, x3 = self.population[idxs]\n\n            v = x1 + self.adaptive_de_f * (x2 - x3)\n            v = np.clip(v, func.bounds.lb, func.bounds.ub)\n\n            u = np.copy(self.population[i])\n            j_rand = np.random.randint(self.dim)\n            for j in range(self.dim):\n                if np.random.rand() < self.adaptive_de_cr or j == j_rand:\n                    u[j] = v[j]\n\n            f_u = func(u)\n            self.eval_count += 1\n\n            if f_u < self.fitness[i]:\n                self.population[i] = u\n                self.fitness[i] = f_u\n                success_count += 1\n\n                if f_u < self.global_best_fitness:\n                    self.global_best_fitness = f_u\n                    self.global_best_position = u.copy()\n                    self.last_improvement = self.eval_count\n            elif self.eval_count >= self.budget:\n                break\n        return success_count/self.pop_size\n\n    def orthogonal_learning(self, func):\n        # Select top ol_size individuals\n        top_indices = np.argsort(self.fitness)[:self.ol_size]\n        basis = self.population[top_indices]\n\n        # Generate orthogonal array (simplified version - random perturbations)\n        oa = np.random.uniform(-0.1, 0.1, size=(self.ol_size, self.dim)) # scaled perturbations\n\n        # Create new solutions based on orthogonal array\n        for i in range(self.ol_size):\n            new_solution = basis[i] + oa[i]\n            new_solution = np.clip(new_solution, func.bounds.lb, func.bounds.ub)\n\n            f_new = func(new_solution)\n            self.eval_count += 1\n\n            # Replace the worst individual if the new solution is better\n            worst_index = np.argmax(self.fitness)\n            if f_new < self.fitness[worst_index]:\n                self.population[worst_index] = new_solution\n                self.fitness[worst_index] = f_new\n\n                if f_new < self.global_best_fitness:\n                    self.global_best_fitness = f_new\n                    self.global_best_position = new_solution.copy()\n                    self.last_improvement = self.eval_count\n            elif self.eval_count >= self.budget:\n                break\n\n    def check_stagnation(self):\n        if self.eval_count - self.last_improvement > self.stagnation_threshold:\n            return True\n        return False\n\n    def restart_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.eval_count += self.pop_size\n        self.velocities = np.random.uniform(-1, 1, size=(self.pop_size, self.dim)) * 0.1\n        self.best_positions = self.population.copy()\n        \n        #Potentially keep the global best\n        self.population[0] = self.global_best_position.copy() \n        self.fitness[0] = self.global_best_fitness\n        \n        self.last_improvement = self.eval_count #reset improvement counter\n\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.eval_count < self.budget:\n            # Adaptive switching mechanism\n            if self.pso_success_rate > self.de_success_rate:\n                success_rate = self.pso_update(func)\n                self.pso_success_rate = 0.9*self.pso_success_rate + 0.1*success_rate\n                self.de_success_rate = 0.9*self.de_success_rate # Decay the less successful rate\n                self.adaptive_de_cr = min(0.99, self.adaptive_de_cr*1.01) #Increase DE parameters if PSO is doing well\n                self.adaptive_de_f = min(0.99, self.adaptive_de_f*1.01)\n                self.adaptive_pso_weight = max(0.1, self.adaptive_pso_weight*0.99) #Reduce PSO weight\n            else:\n                success_rate = self.de_update(func)\n                self.de_success_rate = 0.9*self.de_success_rate + 0.1*success_rate\n                self.pso_success_rate = 0.9*self.pso_success_rate # Decay the less successful rate\n                self.adaptive_pso_weight = min(0.99, self.adaptive_pso_weight*1.01) #Increase PSO parameters if DE is doing well\n                \n                self.adaptive_de_cr = max(0.1, self.adaptive_de_cr*0.99) #Reduce DE params\n                self.adaptive_de_f = max(0.1, self.adaptive_de_f*0.99)\n\n            if self.eval_count < self.budget * 0.8: # Reduce orthogonal learning budget\n                self.orthogonal_learning(func)\n            elif self.eval_count >= self.budget:\n                break\n\n            if self.check_stagnation():\n                self.restart_population(func)\n                \n\n        return self.global_best_fitness, self.global_best_position", "configspace": "", "generation": 5, "feedback": "Evaluation timed out after 300 seconds.", "error": "Evaluation timed out after 300 seconds.", "parent_ids": ["45aaf31d-007f-4368-89e6-dd88b28599cd"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "5c2fb01d-7558-443f-b6d2-3ddb8ba794b9", "fitness": "-inf", "name": "BioInspiredAdaptiveDE", "description": "Bio-inspired Adaptive Differential Evolution with Lvy flight mutation and a self-organizing selection mechanism for enhanced exploration and exploitation.", "code": "import numpy as np\n\nclass BioInspiredAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, CR=0.9, levy_exponent=1.5, restart_patience=500, selection_pressure=2.0):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.levy_exponent = levy_exponent\n        self.restart_patience = restart_patience\n        self.selection_pressure = selection_pressure  # Parameter for self-organizing selection\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.rng = np.random.default_rng()\n\n    def levy_flight(self, size):\n        \"\"\"\n        Generates Lvy flight steps.\n        \"\"\"\n        u = self.rng.normal(0, 1, size=size)\n        v = self.rng.normal(0, 1, size=size)\n        sigma = (np.math.gamma(1 + self.levy_exponent) * np.sin(np.pi * self.levy_exponent / 2) /\n                 (np.math.gamma((1 + self.levy_exponent) / 2) * self.levy_exponent * (2 ** ((self.levy_exponent - 1) / 2)))) ** (1 / self.levy_exponent)\n        step = sigma * u / abs(v) ** (1 / self.levy_exponent)\n        return step\n\n    def self_organizing_selection(self, pop, fitness):\n        \"\"\"\n        Selects individuals based on fitness ranking using a self-organizing probability.\n        \"\"\"\n        ranked_indices = np.argsort(fitness)\n        probabilities = (np.arange(1, self.pop_size + 1) / np.sum(np.arange(1, self.pop_size + 1))) ** self.selection_pressure # Skewed probabilities\n        probabilities /= np.sum(probabilities) # Normalize\n        selected_indices = self.rng.choice(ranked_indices, size=self.pop_size, replace=True, p=probabilities)\n        return pop[selected_indices]\n\n    def __call__(self, func):\n        # Initialize population\n        pop = self.rng.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in pop])\n        self.budget -= self.pop_size\n\n        best_idx = np.argmin(fitness)\n        self.f_opt = fitness[best_idx]\n        self.x_opt = pop[best_idx]\n\n        iteration = 0\n        no_improvement_count = 0\n\n        while self.budget > 0:\n            new_pop = np.copy(pop)\n            for i in range(self.pop_size):\n                # Mutation with Lvy flight\n                idxs = self.rng.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = pop[idxs]\n\n                # Adaptive F\n                F_current = self.F * (1 + 0.1 * self.rng.normal(0, 1))\n                F_current = np.clip(F_current, 0.1, 1.0)\n\n                levy_steps = self.levy_flight(self.dim)\n                v = x_r1 + F_current * (x_r2 - x_r3) + 0.01 * levy_steps * (func.bounds.ub - func.bounds.lb)\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                j_rand = self.rng.integers(self.dim)\n                u = np.copy(pop[i])\n\n                # Adaptive CR\n                CR_current = self.CR * (1 + 0.1 * self.rng.normal(0, 1))\n                CR_current = np.clip(CR_current, 0.1, 1.0)\n\n                crossover_mask = self.rng.random(self.dim) < CR_current\n                u[crossover_mask] = v[crossover_mask]\n                u[j_rand] = v[j_rand] # Guarantee at least one change\n\n\n                # Selection\n                f_u = func(u)\n                self.budget -= 1\n\n                if f_u < fitness[i]:\n                    fitness[i] = f_u\n                    new_pop[i] = u\n\n                    if f_u < self.f_opt:\n                        self.f_opt = f_u\n                        self.x_opt = u\n                        no_improvement_count = 0\n                    else:\n                        no_improvement_count += 1\n                else:\n                    no_improvement_count += 1\n            \n            # Self-Organizing Selection\n            pop = self.self_organizing_selection(new_pop, fitness)\n            \n            # Update fitness values\n            fitness = np.array([func(x) if i >= self.pop_size or np.any(x != pop[i]) else fitness[i] for i, x in enumerate(pop)]) #re-eval only changed individuals\n            self.budget -= np.sum(i >= self.pop_size or np.any(x != pop[i]) for i, x in enumerate(pop))\n\n            best_idx = np.argmin(fitness)\n            if fitness[best_idx] < self.f_opt:\n                  self.f_opt = fitness[best_idx]\n                  self.x_opt = pop[best_idx]\n                  no_improvement_count = 0\n            \n            # Restart mechanism\n            if no_improvement_count > self.restart_patience:\n                pop = self.rng.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                fitness = np.array([func(x) for x in pop])\n                self.budget -= self.pop_size\n\n                best_idx = np.argmin(fitness)\n                self.f_opt = fitness[best_idx]\n                self.x_opt = pop[best_idx]\n                no_improvement_count = 0\n\n            iteration += 1\n            if self.budget <= 0:\n                break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "In the code, line 805, in __getattr__, the following error occurred:\nAttributeError: module 'numpy' has no attribute 'math'. Did you mean: 'emath'?", "error": "In the code, line 805, in __getattr__, the following error occurred:\nAttributeError: module 'numpy' has no attribute 'math'. Did you mean: 'emath'?", "parent_ids": ["7d9cb530-1ec3-4f38-96b1-6dfba1e93783"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "c3748345-26fc-49bc-990c-ffb0408ef3b3", "fitness": 0.16297799769802232, "name": "PACMA_ES", "description": "Population-based Adaptive Covariance Matrix Adaptation Evolution Strategy (PACMA-ES) uses CMA-ES within an evolving population framework, leveraging inter-population communication and restart strategies to enhance exploration and exploitation.", "code": "import numpy as np\n\nclass PACMA_ES:\n    def __init__(self, budget=10000, dim=10, pop_size=50, num_populations=5,\n                 sigma0=0.1, restart_patience=500, communication_interval=100):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.num_populations = num_populations\n        self.sigma0 = sigma0\n        self.restart_patience = restart_patience\n        self.communication_interval = communication_interval\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.rng = np.random.default_rng()\n\n        # Initialize populations\n        self.populations = []\n        for _ in range(self.num_populations):\n            self.populations.append({\n                'mean': self.rng.uniform(-2.5, 2.5, size=self.dim),\n                'C': np.eye(self.dim),  # Covariance matrix\n                'sigma': self.sigma0,  # Step size\n                'fitness': np.inf * np.ones(self.pop_size),\n                'individuals': np.zeros((self.pop_size, self.dim)),\n                'no_improvement_count': 0\n            })\n\n    def sample_population(self, pop, func):\n        \"\"\"Samples new individuals for a population using CMA-ES.\"\"\"\n        z = self.rng.normal(0, 1, size=(self.pop_size, self.dim))\n        pop['individuals'] = pop['mean'] + pop['sigma'] * np.dot(z, np.linalg.cholesky(pop['C']).T)\n        pop['individuals'] = np.clip(pop['individuals'], func.bounds.lb, func.bounds.ub)\n        \n        pop['fitness'] = np.array([func(x) for x in pop['individuals']])\n        self.budget -= self.pop_size\n\n        return pop\n\n    def update_cma(self, pop, mu=None):\n        \"\"\"Updates CMA parameters based on the sampled individuals.\"\"\"\n\n        if mu is None:\n            mu = self.pop_size // 4  # Select top mu individuals\n        \n        idx = np.argsort(pop['fitness'])\n        \n        weights = np.log(mu+1/2) - np.log(np.arange(1,mu+1))\n        weights = weights / np.sum(weights)\n        \n        mean_old = pop['mean'].copy()\n        pop['mean'] = np.sum(weights[:,None] * pop['individuals'][idx[:mu]], axis=0)\n        \n        # Cumulation for covariance matrix adaptation\n        c_sigma = 0.3 #damping\n        c_c = 0.4\n        \n        # Update evolution path\n        pop['p_sigma'] = (1-c_sigma) * getattr(pop, 'p_sigma', np.zeros(self.dim)) + np.sqrt(c_sigma * (2-c_sigma)) * (pop['mean'] - mean_old) / pop['sigma']\n        pop['p_c'] = (1-c_c) * getattr(pop, 'p_c', np.zeros(self.dim)) + np.sqrt(c_c * (2-c_c)) * ((pop['mean'] - mean_old) / pop['sigma']) / np.sqrt(np.sum(((pop['mean'] - mean_old) / pop['sigma'])**2)) \n        \n        # Update covariance matrix\n        c_mu = mu / self.pop_size\n        c_1 = 1.5/ (self.dim+2)\n        \n        pop['C'] = (1-c_1 - c_mu) * pop['C'] + c_1 * np.outer(pop['p_c'],pop['p_c']) + c_mu * np.mean([np.outer(pop['individuals'][i] - mean_old, pop['individuals'][i] - mean_old) for i in idx[:mu]], axis=0)\n\n        # Update step size\n        pop['sigma'] *= np.exp((c_sigma / 0.841) * (np.linalg.norm(pop['p_sigma'])/np.sqrt(self.dim) - 1))\n\n    def communicate(self):\n        \"\"\"Shares information between populations.\"\"\"\n        best_fitnesses = [np.min(pop['fitness']) for pop in self.populations]\n        best_pop_idx = np.argmin(best_fitnesses)\n        best_pop = self.populations[best_pop_idx]\n\n        for i, pop in enumerate(self.populations):\n            if i != best_pop_idx:\n                # Replace worst individuals in other pops with best from best pop\n                worst_idx = np.argmax(pop['fitness'])\n                best_idx = np.argmin(best_pop['fitness'])\n                pop['individuals'][worst_idx] = best_pop['individuals'][best_idx].copy()\n                pop['fitness'][worst_idx] = best_pop['fitness'][best_idx]\n                pop['mean'] = best_pop['mean'].copy()  #Steal best pop mean\n\n    def restart_population(self, pop, func):\n        \"\"\"Restarts a population if it stagnates.\"\"\"\n        pop['mean'] = self.rng.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        pop['C'] = np.eye(self.dim)\n        pop['sigma'] = self.sigma0\n        pop['fitness'] = np.inf * np.ones(self.pop_size)\n        pop['individuals'] = np.zeros((self.pop_size, self.dim))\n        pop['no_improvement_count'] = 0\n        return pop\n\n    def __call__(self, func):\n        iteration = 0\n\n        while self.budget > 0:\n            for i in range(self.num_populations):\n                # Sample new individuals\n                self.populations[i] = self.sample_population(self.populations[i], func)\n\n                # Update best solution found so far\n                current_best_idx = np.argmin(self.populations[i]['fitness'])\n                if self.populations[i]['fitness'][current_best_idx] < self.f_opt:\n                    self.f_opt = self.populations[i]['fitness'][current_best_idx]\n                    self.x_opt = self.populations[i]['individuals'][current_best_idx]\n                    for pop in self.populations:\n                        pop['no_improvement_count'] = 0 #reset patience for all pops\n                else:\n                    self.populations[i]['no_improvement_count'] += 1\n\n                # Update CMA-ES parameters\n                self.update_cma(self.populations[i])\n\n                # Restart if no improvement\n                if self.populations[i]['no_improvement_count'] > self.restart_patience:\n                    self.populations[i] = self.restart_population(self.populations[i], func)\n\n            # Communication between populations\n            if iteration % self.communication_interval == 0:\n                self.communicate()\n\n            iteration += 1\n            if self.budget <= 0:\n                break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "The algorithm PACMA_ES scored 0.163 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["7d9cb530-1ec3-4f38-96b1-6dfba1e93783"], "operator": null, "metadata": {"aucs": [0.11408455019117847, 0.08102199903871443, 0.25904225046206497, 0.10321746814419352, 0.1565325503709939, 0.15619685554417806, 0.1759435964469832, 0.1795406857625056, 0.15432840545615634, 0.12294936104390708, 0.1363089013165787, 0.2182821446012183, 0.25139901294175504, 0.15902846382025126, 0.1257369764099875, 0.2787796618001157, 0.12370903061113514, 0.14640044600943924, 0.15709529034034497, 0.15996230364874442]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "575ef623-8129-4cee-9d7c-a8b7d06b7047", "fitness": 0.6620472898213803, "name": "EnhancedAdaptiveDE", "description": "An enhanced adaptive differential evolution algorithm with dynamic parameter control, covariance matrix adaptation, orthogonal learning, and a novel aging mechanism to promote population diversity and escape local optima.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, CR=0.9, restart_patience=500, orthogonal_learning_rate=0.1, age_threshold=50):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F  # Initial mutation factor\n        self.CR = CR  # Initial crossover rate\n        self.restart_patience = restart_patience\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.orthogonal_learning_rate = orthogonal_learning_rate\n        self.covariance_matrix = np.eye(dim)  # Initialize covariance matrix\n        self.age_threshold = age_threshold  # Threshold for aging mechanism\n        self.ages = np.zeros(pop_size)  # Initialize ages for each individual\n\n    def __call__(self, func):\n        # Initialize population\n        pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in pop])\n        self.budget -= self.pop_size\n        \n        best_idx = np.argmin(fitness)\n        self.f_opt = fitness[best_idx]\n        self.x_opt = pop[best_idx]\n\n        iteration = 0\n        no_improvement_count = 0\n\n        while self.budget > 0:\n            # Covariance Matrix Adaptation\n            if iteration % 10 == 0:\n                 self.covariance_matrix = np.cov(pop.T) + np.eye(self.dim) * 1e-6\n\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = pop[idxs]\n                \n                # Adaptive F: perturb F with a small probability\n                F_current = self.F + np.random.normal(0, 0.1)\n                F_current = np.clip(F_current, 0.1, 1.0)\n\n                # Sample mutation vector from a multivariate normal distribution\n                mutation_vector = np.random.multivariate_normal(np.zeros(self.dim), self.covariance_matrix)\n                v = x_r1 + F_current * mutation_vector\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)  # Keep within bounds\n\n                # Crossover\n                j_rand = np.random.randint(self.dim)\n                u = np.copy(pop[i])\n                \n                # Adaptive CR: perturb CR with a small probability\n                CR_current = self.CR + np.random.normal(0, 0.1)\n                CR_current = np.clip(CR_current, 0.1, 1.0)\n                \n                for j in range(self.dim):\n                    if np.random.rand() < CR_current or j == j_rand:\n                        u[j] = v[j]\n\n                # Orthogonal Learning\n                if np.random.rand() < self.orthogonal_learning_rate:\n                    direction = self.x_opt - u\n                    u = u + np.random.rand() * direction\n                    u = np.clip(u, func.bounds.lb, func.bounds.ub)\n\n                # Selection\n                f_u = func(u)\n                self.budget -= 1\n\n                if f_u < fitness[i]:\n                    fitness[i] = f_u\n                    pop[i] = u\n                    self.ages[i] = 0  # Reset age if improved\n\n                    if f_u < self.f_opt:\n                        self.f_opt = f_u\n                        self.x_opt = u\n                        no_improvement_count = 0 # Reset counter\n                    else:\n                        no_improvement_count += 1\n                else:\n                    no_improvement_count += 1\n                    self.ages[i] += 1  # Increment age if not improved\n            \n            # Aging Mechanism: Replace old individuals\n            for i in range(self.pop_size):\n                if self.ages[i] > self.age_threshold:\n                    pop[i] = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                    fitness[i] = func(pop[i])\n                    self.budget -= 1\n                    self.ages[i] = 0  # Reset age\n                    if fitness[i] < self.f_opt:\n                        self.f_opt = fitness[i]\n                        self.x_opt = pop[i]\n\n            # Restart mechanism\n            if no_improvement_count > self.restart_patience:\n                pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                fitness = np.array([func(x) for x in pop])\n                self.budget -= self.pop_size\n                \n                best_idx = np.argmin(fitness)\n                self.f_opt = fitness[best_idx]\n                self.x_opt = pop[best_idx]\n                no_improvement_count = 0  # Reset counter\n                self.covariance_matrix = np.eye(self.dim)\n                self.ages = np.zeros(self.pop_size)\n            \n            iteration += 1\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "The algorithm EnhancedAdaptiveDE scored 0.662 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["4da23f0e-6f00-43ef-a7d4-f6df57870753"], "operator": null, "metadata": {"aucs": [0.36061984301991135, 0.7268488090658605, 0.6203569834515155, 0.8834145476968123, 0.6225474898948928, 0.7435644218117021, 0.5561523905560248, 0.5611674618559631, 0.6830203206123222, 0.614245645873228, 0.8760729200342912, 0.996953636750086, 0.5288097805115121, 0.5903378769441214, 0.9281340108677489, 0.7188607482737001, 0.55864379624387, 0.8659459768585939, 0.29969767926153623, 0.5055514568439108]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "edd1f49e-a1a3-4b37-beee-557483c28f0b", "fitness": 0.0, "name": "HybridPSO_DE_Adaptive", "description": "A simplified hybrid algorithm combining PSO and DE with adaptive parameter control based on exponential moving averages of success rates and a simplified orthogonal learning strategy focusing on diversity injection.", "code": "import numpy as np\n\nclass HybridPSO_DE_Adaptive:\n    def __init__(self, budget=10000, dim=10, pop_size=20, pso_weight=0.7, de_cr=0.9, de_f=0.8, ol_size=5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.pso_weight = pso_weight\n        self.de_cr = de_cr\n        self.de_f = de_f\n        self.ol_size = ol_size\n        self.population = None\n        self.fitness = None\n        self.velocities = None\n        self.global_best_fitness = np.inf\n        self.global_best_position = None\n        self.eval_count = 0\n        self.pso_success_rate = 0.5\n        self.de_success_rate = 0.5\n        self.ema_alpha = 0.2 # Exponential moving average alpha\n\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.eval_count += self.pop_size\n        self.velocities = np.random.uniform(-1, 1, size=(self.pop_size, self.dim)) * 0.1\n        self.global_best_position = self.population[np.argmin(self.fitness)].copy()\n        self.global_best_fitness = np.min(self.fitness)\n\n    def pso_update(self, func):\n        c1 = 1.5\n        c2 = 1.5\n        success_count = 0\n\n        r1 = np.random.rand(self.pop_size, self.dim)\n        r2 = np.random.rand(self.pop_size, self.dim)\n\n        self.velocities = self.pso_weight * self.velocities + \\\n                           c1 * r1 * (self.population - self.global_best_position) + \\\n                           c2 * r2 * (self.population - self.global_best_position)  #Simplified towards global best\n\n        new_population = self.population + self.velocities\n        new_population = np.clip(new_population, func.bounds.lb, func.bounds.ub)\n        new_fitness = np.array([func(x) for x in new_population])\n        self.eval_count += self.pop_size\n\n        for i in range(self.pop_size):\n            if new_fitness[i] < self.fitness[i]:\n                if new_fitness[i] < self.global_best_fitness:\n                    self.global_best_fitness = new_fitness[i]\n                    self.global_best_position = new_population[i].copy()\n                self.population[i] = new_population[i].copy()\n                self.fitness[i] = new_fitness[i]\n                success_count += 1\n\n        return success_count / self.pop_size\n\n    def de_update(self, func):\n        success_count = 0\n        for i in range(self.pop_size):\n            idxs = np.random.choice(self.pop_size, 3, replace=False)\n            x1, x2, x3 = self.population[idxs]\n\n            v = x1 + self.de_f * (x2 - x3)\n            v = np.clip(v, func.bounds.lb, func.bounds.ub)\n\n            if np.random.rand() < self.de_cr:\n                u = v\n            else:\n                u = self.population[i].copy() # Simplified crossover\n            \n\n            f_u = func(u)\n            self.eval_count += 1\n\n            if f_u < self.fitness[i]:\n                if f_u < self.global_best_fitness:\n                    self.global_best_fitness = f_u\n                    self.global_best_position = u.copy()\n                self.population[i] = u\n                self.fitness[i] = f_u\n                success_count += 1\n\n        return success_count / self.pop_size\n\n    def orthogonal_learning(self, func):\n        # Diversity injection - perturb random individuals\n        num_individuals = min(self.ol_size, self.pop_size)\n        indices = np.random.choice(self.pop_size, num_individuals, replace=False)\n\n        for i in indices:\n            perturbation = np.random.uniform(-0.5, 0.5, self.dim)  # Larger perturbation\n            new_solution = self.population[i] + perturbation\n            new_solution = np.clip(new_solution, func.bounds.lb, func.bounds.ub)\n\n            f_new = func(new_solution)\n            self.eval_count += 1\n\n            if f_new < self.fitness[i]:\n                if f_new < self.global_best_fitness:\n                    self.global_best_fitness = f_new\n                    self.global_best_position = new_solution.copy()\n                self.population[i] = new_solution\n                self.fitness[i] = f_new\n\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.eval_count < self.budget:\n            if self.pso_success_rate > self.de_success_rate:\n                success_rate = self.pso_update(func)\n                self.pso_success_rate = (1 - self.ema_alpha) * self.pso_success_rate + self.ema_alpha * success_rate  # EMA update\n                self.de_success_rate = (1 - self.ema_alpha) * self.de_success_rate # Decay less successful\n\n            else:\n                success_rate = self.de_update(func)\n                self.de_success_rate = (1 - self.ema_alpha) * self.de_success_rate + self.ema_alpha * success_rate  # EMA update\n                self.pso_success_rate = (1 - self.ema_alpha) * self.pso_success_rate  # Decay less successful\n\n            if self.eval_count < self.budget * 0.8:\n                self.orthogonal_learning(func)\n\n        return self.global_best_fitness, self.global_best_position", "configspace": "", "generation": 5, "feedback": "The algorithm HybridPSO_DE_Adaptive scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["45aaf31d-007f-4368-89e6-dd88b28599cd"], "operator": null, "metadata": {"aucs": [0]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "72e2598c-b1a6-4d4a-bd23-a0da066bad13", "fitness": 0.5536994714969395, "name": "HybridPSO_DE_Adaptive", "description": "A simplified hybrid algorithm combining PSO and DE with adaptive parameter control based on performance and a mutation factor adaptation mechanism for faster convergence.", "code": "import numpy as np\n\nclass HybridPSO_DE_Adaptive:\n    def __init__(self, budget=10000, dim=10, pop_size=20, pso_weight=0.7, de_cr=0.9, de_f=0.8):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.pso_weight = pso_weight\n        self.de_cr = de_cr\n        self.de_f = de_f\n        self.population = None\n        self.fitness = None\n        self.velocities = None\n        self.best_positions = None\n        self.global_best_fitness = np.inf\n        self.global_best_position = None\n        self.eval_count = 0\n        self.mutation_factor = self.de_f\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.eval_count += self.pop_size\n        self.velocities = np.random.uniform(-1, 1, size=(self.pop_size, self.dim)) * 0.1\n        self.best_positions = self.population.copy()\n        self.global_best_position = self.population[np.argmin(self.fitness)].copy()\n        self.global_best_fitness = np.min(self.fitness)\n\n    def pso_update(self, func):\n        cognitive_weight = 1.5\n        social_weight = 1.5\n\n        r1 = np.random.rand(self.pop_size, self.dim)\n        r2 = np.random.rand(self.pop_size, self.dim)\n\n        cognitive_component = cognitive_weight * r1 * (self.best_positions - self.population)\n        social_component = social_weight * r2 * (self.global_best_position - self.population)\n\n        self.velocities = self.pso_weight * self.velocities + cognitive_component + social_component\n        self.population = self.population + self.velocities\n\n        self.population = np.clip(self.population, func.bounds.lb, func.bounds.ub)\n\n        new_fitness = np.array([func(x) for x in self.population])\n        self.eval_count += self.pop_size\n\n        for i in range(self.pop_size):\n            if new_fitness[i] < self.fitness[i]:\n                self.best_positions[i] = self.population[i].copy()\n                self.fitness[i] = new_fitness[i]\n\n        best_index = np.argmin(self.fitness)\n        if self.fitness[best_index] < self.global_best_fitness:\n            self.global_best_fitness = self.fitness[best_index]\n            self.global_best_position = self.population[best_index].copy()\n\n    def de_update(self, func):\n        for i in range(self.pop_size):\n            idxs = np.random.choice(self.pop_size, 3, replace=False)\n            while i in idxs:\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n\n            x1, x2, x3 = self.population[idxs]\n\n            v = x1 + self.mutation_factor * (x2 - x3)\n            v = np.clip(v, func.bounds.lb, func.bounds.ub)\n\n            u = np.copy(self.population[i])\n            j_rand = np.random.randint(self.dim)\n            for j in range(self.dim):\n                if np.random.rand() < self.de_cr or j == j_rand:\n                    u[j] = v[j]\n\n            f_u = func(u)\n            self.eval_count += 1\n\n            if f_u < self.fitness[i]:\n                self.population[i] = u\n                self.fitness[i] = f_u\n\n                if f_u < self.global_best_fitness:\n                    self.global_best_fitness = f_u\n                    self.global_best_position = u.copy()\n        \n        # Adapt mutation factor\n        if np.random.rand() < 0.1:\n            self.mutation_factor = np.random.uniform(0.5, 1.0) # Randomize to escape local optima\n        else:\n            self.mutation_factor *= 0.99 # Gradual reduction for convergence\n\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.eval_count < self.budget:\n            if np.random.rand() < 0.5:\n                self.pso_update(func)\n            else:\n                self.de_update(func)\n\n            if self.eval_count >= self.budget:\n                break\n\n        return self.global_best_fitness, self.global_best_position", "configspace": "", "generation": 6, "feedback": "The algorithm HybridPSO_DE_Adaptive scored 0.554 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["45aaf31d-007f-4368-89e6-dd88b28599cd"], "operator": null, "metadata": {"aucs": [0.20497516775747382, 0.41524647250264946, 0.6340705472020596, 0.9009098839661505, 0.4883190582156246, 0.7926514127126187, 0.5735322993649989, 0.5908490627781171, 0.7003496575047818, 0.3093552616551426, 0.7596255117147682, 0.9951773044807428, 0.31505793715506825, 0.2934632132001673, 0.6685285371259236, 0.7089356022487732, 0.40263726779499576, 0.36414158324706924, 0.4615903430894779, 0.49457330622218987]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "2d288c5f-1fec-4c00-8e87-692f0cd9537c", "fitness": 0.5733672390286013, "name": "EnhancedAdaptiveDE", "description": "Enhanced Adaptive Differential Evolution with a mirrored sampling strategy for mutation and an aging archive to promote diversity and prevent premature convergence.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, CR=0.9, restart_patience=500, orthogonal_learning_rate=0.1, archive_size=20):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F  # Initial mutation factor\n        self.CR = CR  # Initial crossover rate\n        self.restart_patience = restart_patience\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.orthogonal_learning_rate = orthogonal_learning_rate\n        self.archive_size = archive_size\n        self.archive = []\n        self.covariance_matrix = np.eye(dim)  # Initialize covariance matrix\n        self.age = np.zeros(pop_size) # Age of each individual\n\n    def __call__(self, func):\n        # Initialize population\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        pop = np.random.uniform(lb, ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in pop])\n        self.budget -= self.pop_size\n        \n        best_idx = np.argmin(fitness)\n        self.f_opt = fitness[best_idx]\n        self.x_opt = pop[best_idx]\n\n        iteration = 0\n        no_improvement_count = 0\n\n        while self.budget > 0:\n            # Covariance Matrix Adaptation\n            if iteration % 10 == 0:\n                 self.covariance_matrix = np.cov(pop.T) + np.eye(self.dim) * 1e-6\n\n            for i in range(self.pop_size):\n                # Mutation using mirrored sampling\n                idxs = np.random.choice(self.pop_size, 2, replace=False)\n                x_r1, x_r2 = pop[idxs]\n\n                # Add archive individuals to the selection pool\n                if len(self.archive) > 0:\n                    arch_idx = np.random.randint(len(self.archive))\n                    x_r3 = self.archive[arch_idx]\n                else:\n                    x_r3 = pop[np.random.choice(self.pop_size, 1)[0]] #Fall back to random if archive is empty\n\n                # Adaptive F: perturb F with a small probability\n                F_current = self.F\n                if np.random.rand() < 0.1:\n                    F_current = np.random.normal(self.F, 0.1)\n                    F_current = np.clip(F_current, 0.1, 1.0) #Keep within reasonable bounds\n\n                # Original mutation vector\n                v = pop[i] + F_current * (x_r1 - x_r2)\n                \n                # Mirrored sampling: creates a mirrored vector with respect to the population mean\n                pop_mean = np.mean(pop, axis=0)\n                v_mirrored = 2 * pop_mean - v\n\n                #Clip both vectors to bounds\n                v = np.clip(v, lb, ub)\n                v_mirrored = np.clip(v_mirrored, lb, ub)\n\n                # Select either original or mirrored vector randomly\n                if np.random.rand() < 0.5:\n                    v_mutation = v\n                else:\n                    v_mutation = v_mirrored\n                \n                # Crossover\n                j_rand = np.random.randint(self.dim)\n                u = np.copy(pop[i])\n                \n                # Adaptive CR: perturb CR with a small probability\n                CR_current = self.CR\n                if np.random.rand() < 0.1:\n                    CR_current = np.random.normal(self.CR, 0.1)\n                    CR_current = np.clip(CR_current, 0.1, 1.0) #Keep within reasonable bounds\n                \n                for j in range(self.dim):\n                    if np.random.rand() < CR_current or j == j_rand:\n                        u[j] = v_mutation[j]\n\n                # Orthogonal Learning\n                if np.random.rand() < self.orthogonal_learning_rate:\n                    direction = self.x_opt - u\n                    u = u + np.random.rand() * direction\n\n                u = np.clip(u, lb, ub)\n\n                # Selection\n                f_u = func(u)\n                self.budget -= 1\n\n                if f_u < fitness[i]:\n                    # Update fitness and population\n                    fitness[i] = f_u\n                    pop[i] = u\n                    self.age[i] = 0 #Reset age\n\n                    if f_u < self.f_opt:\n                        self.f_opt = f_u\n                        self.x_opt = u\n                        no_improvement_count = 0 # Reset counter\n                    else:\n                        no_improvement_count += 1\n                else:\n                    no_improvement_count += 1\n                    self.age[i] += 1 #Increment age if not improved\n\n                    # Add replaced individual to the archive\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(pop[i].copy())\n                    else:\n                        # Replace a random element in the archive\n                        replace_idx = np.random.randint(self.archive_size)\n                        self.archive[replace_idx] = pop[i].copy()\n\n\n            # Restart mechanism\n            if no_improvement_count > self.restart_patience:\n                pop = np.random.uniform(lb, ub, size=(self.pop_size, self.dim))\n                fitness = np.array([func(x) for x in pop])\n                self.budget -= self.pop_size\n                \n                best_idx = np.argmin(fitness)\n                self.f_opt = fitness[best_idx]\n                self.x_opt = pop[best_idx]\n                no_improvement_count = 0  # Reset counter\n                self.covariance_matrix = np.eye(self.dim)\n                self.archive = [] #Clear archive\n                self.age = np.zeros(self.pop_size) #Reset ages\n\n            # Aging mechanism: replace old individuals with new random ones\n            for i in range(self.pop_size):\n                if self.age[i] > self.restart_patience / 2:\n                    pop[i] = np.random.uniform(lb, ub, size=self.dim)\n                    fitness[i] = func(pop[i])\n                    self.budget -= 1\n                    self.age[i] = 0 #reset age\n                    if fitness[i] < self.f_opt:\n                        self.f_opt = fitness[i]\n                        self.x_opt = pop[i]\n                        \n            \n            iteration += 1\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "The algorithm EnhancedAdaptiveDE scored 0.573 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["4da23f0e-6f00-43ef-a7d4-f6df57870753"], "operator": null, "metadata": {"aucs": [0.16742872199208692, 0.4658283762545561, 0.4735304864044322, 0.901333938323732, 0.6225491666281335, 0.6382360508801771, 0.3218105180311399, 0.5545717527787246, 0.6111057738115632, 0.29852798897531585, 0.8954371971771322, 0.9908414840039512, 0.4334114700798284, 0.6179472671900458, 0.8865069675577445, 0.6589357374619446, 0.4540533092730973, 0.7722095712304405, 0.2082261698816108, 0.49485283263636626]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "e62bc77c-9678-4d0e-a771-9c627672505d", "fitness": 0.0, "name": "EnhancedAdaptiveDE", "description": "Enhanced Adaptive Differential Evolution with self-adaptive covariance matrix adaptation and orthogonal learning with a dynamic restart mechanism, focusing on exploration and exploitation balance.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, CR=0.9, restart_patience=500, orthogonal_learning_rate=0.1, cma_learning_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F  # Initial mutation factor\n        self.CR = CR  # Initial crossover rate\n        self.restart_patience = restart_patience\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.orthogonal_learning_rate = orthogonal_learning_rate\n        self.covariance_matrix = np.eye(dim)  # Initialize covariance matrix\n        self.cma_learning_rate = cma_learning_rate\n\n    def __call__(self, func):\n        # Initialize population\n        pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in pop])\n        self.budget -= self.pop_size\n        \n        best_idx = np.argmin(fitness)\n        self.f_opt = fitness[best_idx]\n        self.x_opt = pop[best_idx]\n\n        iteration = 0\n        no_improvement_count = 0\n\n        while self.budget > 0:\n            # Covariance Matrix Adaptation\n            if iteration % 10 == 0:\n                # CMA-ES style update: adapt covariance matrix based on successful steps\n                diffs = pop - np.mean(pop, axis=0)\n                self.covariance_matrix = (1 - self.cma_learning_rate) * self.covariance_matrix + \\\n                                          self.cma_learning_rate * np.cov(pop.T) + np.eye(self.dim) * 1e-6\n\n\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = pop[idxs]\n                \n                # Adaptive F: perturb F with a small probability\n                F_current = self.F\n                if np.random.rand() < 0.1:\n                    F_current = np.random.normal(self.F, 0.1)\n                    F_current = np.clip(F_current, 0.1, 1.0) #Keep within reasonable bounds\n\n                # Sample mutation vector from a multivariate normal distribution\n                mutation_vector = np.random.multivariate_normal(np.zeros(self.dim), self.covariance_matrix)\n                v = x_r1 + F_current * mutation_vector\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)  # Keep within bounds\n\n                # Crossover\n                j_rand = np.random.randint(self.dim)\n                u = np.copy(pop[i])\n                \n                # Adaptive CR: perturb CR with a small probability\n                CR_current = self.CR\n                if np.random.rand() < 0.1:\n                    CR_current = np.random.normal(self.CR, 0.1)\n                    CR_current = np.clip(CR_current, 0.1, 1.0) #Keep within reasonable bounds\n                \n                for j in range(self.dim):\n                    if np.random.rand() < CR_current or j == j_rand:\n                        u[j] = v[j]\n\n                # Orthogonal Learning: adaptive step size\n                if np.random.rand() < self.orthogonal_learning_rate:\n                    direction = self.x_opt - u\n                    step_size = np.random.rand() * (np.linalg.norm(u - self.x_opt) / (func.bounds.ub - func.bounds.lb)) # Adaptive step size\n                    u = u + step_size * direction\n\n                u = np.clip(u, func.bounds.lb, func.bounds.ub)\n\n                # Selection\n                f_u = func(u)\n                self.budget -= 1\n\n                if f_u < fitness[i]:\n                    fitness[i] = f_u\n                    pop[i] = u\n\n                    if f_u < self.f_opt:\n                        self.f_opt = f_u\n                        self.x_opt = u\n                        no_improvement_count = 0 # Reset counter\n                    else:\n                        no_improvement_count += 1\n                else:\n                    no_improvement_count += 1\n\n            # Dynamic Restart Mechanism\n            if no_improvement_count > self.restart_patience:\n                # Adaptive restart: restart only a fraction of the population\n                num_to_restart = int(self.pop_size * 0.5)  # Restart half of the population\n                indices_to_restart = np.random.choice(self.pop_size, num_to_restart, replace=False)\n                \n                pop[indices_to_restart] = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(num_to_restart, self.dim))\n                fitness[indices_to_restart] = np.array([func(x) for x in pop[indices_to_restart]])\n                self.budget -= num_to_restart\n                \n                best_idx = np.argmin(fitness)\n                self.f_opt = fitness[best_idx]\n                self.x_opt = pop[best_idx]\n                no_improvement_count = 0  # Reset counter\n                self.covariance_matrix = np.eye(self.dim)\n            \n            iteration += 1\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "The algorithm EnhancedAdaptiveDE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["4da23f0e-6f00-43ef-a7d4-f6df57870753"], "operator": null, "metadata": {"aucs": [0]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "295eb1e5-2413-4ba4-9820-d4c6c6b1de4f", "fitness": 0.692567790209307, "name": "AdaptiveDE", "description": "Adaptive Differential Evolution with orthogonal learning, covariance matrix adaptation, stochastic ranking, aging, and a mirrored sampling technique to enhance exploration near boundaries.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, CR=0.9, restart_patience=500, archive_size_factor=0.2):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.restart_patience = restart_patience\n        self.archive_size = int(self.pop_size * archive_size_factor)\n        self.archive = []\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.age = np.zeros(self.pop_size)  # Initialize age for each individual\n\n    def __call__(self, func):\n        # Initialize population\n        bounds_lb = func.bounds.lb\n        bounds_ub = func.bounds.ub\n        pop = np.random.uniform(bounds_lb, bounds_ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in pop])\n        self.budget -= self.pop_size\n\n        best_idx = np.argmin(fitness)\n        self.f_opt = fitness[best_idx]\n        self.x_opt = pop[best_idx]\n\n        no_improvement_count = 0\n        iteration = 0\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = pop[idxs]\n\n                # Use archive member with probability\n                if len(self.archive) > 0 and np.random.rand() < 0.1:\n                    idx_a = np.random.randint(len(self.archive))\n                    x_r1 = self.archive[idx_a]\n\n                # Adaptive F\n                if np.random.rand() < 0.1:\n                    F_current = np.random.normal(self.F, 0.1)\n                else:\n                    F_current = self.F\n                F_current = np.clip(F_current, 0.1, 1.0)\n\n                v = x_r1 + F_current * (x_r2 - x_r3)\n\n                # Mirrored sampling to handle boundary constraints\n                for j in range(self.dim):\n                    if v[j] < bounds_lb[j]:\n                        v[j] = bounds_lb[j] + (bounds_lb[j] - v[j])  # Mirror\n                    elif v[j] > bounds_ub[j]:\n                        v[j] = bounds_ub[j] - (v[j] - bounds_ub[j])  # Mirror\n                v = np.clip(v, bounds_lb, bounds_ub)\n\n                # Crossover\n                j_rand = np.random.randint(self.dim)\n                u = np.copy(pop[i])\n\n                # Adaptive CR\n                if np.random.rand() < 0.1:\n                    CR_current = np.random.normal(self.CR, 0.1)\n                else:\n                    CR_current = self.CR\n                CR_current = np.clip(CR_current, 0.1, 1.0)\n\n                for j in range(self.dim):\n                    if np.random.rand() < CR_current or j == j_rand:\n                        u[j] = v[j]\n\n                # Evaluate trial vector\n                f_u = func(u)\n                self.budget -= 1\n\n                # Stochastic ranking\n                if (f_u < fitness[i]) or (np.random.rand() < 0.1):\n                    if f_u < fitness[i]:\n                        fitness[i] = f_u\n                        pop[i] = u\n                        self.age[i] = 0  # Reset age\n\n                        if f_u < self.f_opt:\n                            self.f_opt = f_u\n                            self.x_opt = u\n                            no_improvement_count = 0\n                        else:\n                            no_improvement_count += 1\n                    else:\n                        # Stochastic acceptance: replace if worse, update archive\n                        if len(self.archive) < self.archive_size:\n                            self.archive.append(pop[i])\n                        else:\n                            idx_replace = np.random.randint(len(self.archive))\n                            self.archive[idx_replace] = pop[i]\n                else:\n                    no_improvement_count += 1\n                    self.age[i] += 1  # Increase age\n\n            # Aging mechanism: replace old individuals\n            for i in range(self.pop_size):\n                if self.age[i] > 0.5 * self.restart_patience:  # Age threshold\n                    pop[i] = np.random.uniform(bounds_lb, bounds_ub)\n                    fitness[i] = func(pop[i])\n                    self.budget -= 1\n                    self.age[i] = 0\n                    if fitness[i] < self.f_opt:\n                        self.f_opt = fitness[i]\n                        self.x_opt = pop[i]\n                        no_improvement_count = 0\n\n            # Restart mechanism\n            if no_improvement_count > self.restart_patience:\n                pop = np.random.uniform(bounds_lb, bounds_ub, size=(self.pop_size, self.dim))\n                fitness = np.array([func(x) for x in pop])\n                self.budget -= self.pop_size\n                best_idx = np.argmin(fitness)\n                self.f_opt = fitness[best_idx]\n                self.x_opt = pop[best_idx]\n                no_improvement_count = 0\n                self.archive = []\n                self.age = np.zeros(self.pop_size)\n\n            iteration += 1\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "The algorithm AdaptiveDE scored 0.693 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["edbf797c-dcc3-460f-8549-de041651be38"], "operator": null, "metadata": {"aucs": [0.24783374136702718, 0.6584418813239672, 0.7157591527477112, 0.8419667120678888, 0.7525061545106112, 0.8125718527425914, 0.6284425262384903, 0.6554890816926391, 0.761847274547413, 0.6816514528793411, 0.8714893739802649, 0.9929459105256434, 0.6397580322803965, 0.7117445278390631, 0.9179409244135702, 0.7806396944199254, 0.6598043037104593, 0.8362693531811058, 0.18472707645676567, 0.4995267772612657]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "c9bd2903-3dd2-4919-bf9e-672ead43060b", "fitness": 0.7249750949180572, "name": "EnsembleAdaptiveDE", "description": "An enhanced adaptive differential evolution algorithm using a dynamically adjusted ensemble of mutation strategies and orthogonal learning to improve exploration and exploitation.", "code": "import numpy as np\n\nclass EnsembleAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, CR=0.9, restart_patience=500, ensemble_size=3):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F  # Initial mutation factor\n        self.CR = CR  # Initial crossover rate\n        self.restart_patience = restart_patience\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.ensemble_size = ensemble_size\n        self.archive_size = int(self.pop_size * 0.2)\n        self.archive = []\n\n        # Mutation strategy pool\n        self.mutation_strategies = [\n            self.mutation_DE_rand1,\n            self.mutation_DE_best1,\n            self.mutation_DE_current_to_best1\n        ]\n        self.strategy_weights = np.ones(self.ensemble_size) / self.ensemble_size  # Initially uniform weights\n        self.success_rates = np.zeros(self.ensemble_size)  # Track success rates of each strategy\n        self.strategy_counts = np.zeros(self.ensemble_size)\n\n    def mutation_DE_rand1(self, pop, i):\n        idxs = np.random.choice(self.pop_size, 3, replace=False)\n        x_r1, x_r2, x_r3 = pop[idxs]\n        v = x_r1 + self.F * (x_r2 - x_r3)\n        return v\n    \n    def mutation_DE_best1(self, pop, best_idx, i):\n        idxs = np.random.choice(self.pop_size, 2, replace=False)\n        x_r1, x_r2 = pop[idxs]\n        v = pop[best_idx] + self.F * (x_r1 - x_r2)\n        return v\n\n    def mutation_DE_current_to_best1(self, pop, best_idx, i):\n         idxs = np.random.choice(self.pop_size, 2, replace=False)\n         x_r1, x_r2 = pop[idxs]\n         v = pop[i] + self.F * (pop[best_idx] - pop[i]) + self.F * (x_r1 - x_r2)\n         return v\n    \n\n    def orthogonal_learning(self, u, bounds):\n        # Simple orthogonal learning: generate a random orthogonal vector\n        rand_vec = np.random.rand(self.dim)\n        rand_vec /= np.linalg.norm(rand_vec)  # Normalize\n        step_size = np.random.uniform(-0.1, 0.1) #Adaptive step size\n        \n        u_new = u + step_size * rand_vec\n        u_new = np.clip(u_new, bounds.lb, bounds.ub)\n        return u_new\n\n    def __call__(self, func):\n        # Initialize population\n        pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in pop])\n        self.budget -= self.pop_size\n        \n        best_idx = np.argmin(fitness)\n        self.f_opt = fitness[best_idx]\n        self.x_opt = pop[best_idx]\n\n        iteration = 0\n        no_improvement_count = 0\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Ensemble of mutation strategies: select a strategy based on weights\n                strategy_idx = np.random.choice(self.ensemble_size, p=self.strategy_weights)\n                self.strategy_counts[strategy_idx] += 1\n                mutation_strategy = self.mutation_strategies[strategy_idx]\n                \n                if mutation_strategy == self.mutation_DE_rand1:\n                    v = mutation_strategy(pop, i)\n                else:\n                    v = mutation_strategy(pop, best_idx, i)\n\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                j_rand = np.random.randint(self.dim)\n                u = np.copy(pop[i])\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR or j == j_rand:\n                        u[j] = v[j]\n\n                # Apply orthogonal learning\n                if np.random.rand() < 0.1:\n                    u = self.orthogonal_learning(u, func.bounds)\n\n                # Selection\n                f_u = func(u)\n                self.budget -= 1\n\n                if f_u < fitness[i]:\n                    # Strategy was successful\n                    self.success_rates[strategy_idx] += 1\n                    \n                    fitness[i] = f_u\n                    pop[i] = u\n\n                    if f_u < self.f_opt:\n                        self.f_opt = f_u\n                        self.x_opt = u\n                        no_improvement_count = 0\n                    else:\n                        no_improvement_count += 1\n                else:\n                    no_improvement_count += 1\n                \n                #Update archive\n                if len(self.archive) < self.archive_size:\n                    self.archive.append(pop[i].copy())\n                elif np.random.rand() < 0.1: #Small chance to replace\n                    idx_replace = np.random.randint(len(self.archive))\n                    self.archive[idx_replace] = pop[i].copy()\n\n            # Update strategy weights based on success rates\n            self.strategy_weights = self.success_rates / self.strategy_counts\n            self.strategy_weights /= np.sum(self.strategy_weights)\n            self.strategy_weights = np.nan_to_num(self.strategy_weights)  # Handle potential NaN values\n            if np.sum(self.strategy_weights) == 0:\n                self.strategy_weights = np.ones(self.ensemble_size) / self.ensemble_size\n\n            # Restart mechanism\n            if no_improvement_count > self.restart_patience:\n                pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                fitness = np.array([func(x) for x in pop])\n                self.budget -= self.pop_size\n                \n                best_idx = np.argmin(fitness)\n                self.f_opt = fitness[best_idx]\n                self.x_opt = pop[best_idx]\n                no_improvement_count = 0\n                self.archive = []\n                self.strategy_weights = np.ones(self.ensemble_size) / self.ensemble_size  # Reset weights\n                self.success_rates = np.zeros(self.ensemble_size)\n                self.strategy_counts = np.zeros(self.ensemble_size)\n\n            iteration += 1\n\n            best_idx = np.argmin(fitness)\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "The algorithm EnsembleAdaptiveDE scored 0.725 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["edbf797c-dcc3-460f-8549-de041651be38"], "operator": null, "metadata": {"aucs": [0.35666996998431644, 0.8142118607865952, 0.8419483402169583, 0.9380042026060744, 0.8938023787414371, 0.910832155889376, 0.37401825682289325, 0.8538426656706173, 0.8946062053331103, 0.4651589627885069, 0.9413197986904631, 0.9932531942430366, 0.8305240945799123, 0.3834651030253713, 0.924155842297414, 0.9022583378258384, 0.4128466666800522, 0.9314726259049101, 0.32915459543840353, 0.5079566408358585]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "5ac2e865-088a-4323-a192-07813fe304be", "fitness": "-inf", "name": "EnhancedAdaptiveDE", "description": "Adaptive Differential Evolution with orthogonal array-based crossover and a simplified aging mechanism for improved exploration and exploitation.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, CR=0.9, restart_patience=500, age_threshold=50):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F  # Initial mutation factor\n        self.CR = CR  # Initial crossover rate\n        self.restart_patience = restart_patience\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.age_threshold = age_threshold  # Threshold for aging mechanism\n        self.ages = np.zeros(pop_size)  # Initialize ages for each individual\n\n    def __call__(self, func):\n        # Initialize population\n        pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in pop])\n        self.budget -= self.pop_size\n        \n        best_idx = np.argmin(fitness)\n        self.f_opt = fitness[best_idx]\n        self.x_opt = pop[best_idx]\n\n        iteration = 0\n        no_improvement_count = 0\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = pop[idxs]\n                v = x_r1 + self.F * (x_r2 - x_r3)\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)  # Keep within bounds\n\n                # Orthogonal Array-Based Crossover\n                u = np.copy(pop[i])\n                oa = self.create_orthogonal_array(self.dim)\n                for j in range(self.dim):\n                    if oa[np.random.randint(len(oa))] < self.CR:\n                        u[j] = v[j]\n\n                # Selection\n                f_u = func(u)\n                self.budget -= 1\n\n                if f_u < fitness[i]:\n                    fitness[i] = f_u\n                    pop[i] = u\n                    self.ages[i] = 0  # Reset age if improved\n\n                    if f_u < self.f_opt:\n                        self.f_opt = f_u\n                        self.x_opt = u\n                        no_improvement_count = 0 # Reset counter\n                    else:\n                        no_improvement_count += 1\n                else:\n                    no_improvement_count += 1\n                    self.ages[i] += 1  # Increment age if not improved\n            \n            # Simplified Aging Mechanism: Replace only the worst individual if old\n            if np.any(self.ages > self.age_threshold):\n                worst_aged_idx = np.argmax(self.ages)\n                pop[worst_aged_idx] = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                fitness[worst_aged_idx] = func(pop[worst_aged_idx])\n                self.budget -= 1\n                self.ages[worst_aged_idx] = 0  # Reset age\n                if fitness[worst_aged_idx] < self.f_opt:\n                    self.f_opt = fitness[worst_aged_idx]\n                    self.x_opt = pop[worst_aged_idx]\n\n            # Restart mechanism\n            if no_improvement_count > self.restart_patience:\n                pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                fitness = np.array([func(x) for x in pop])\n                self.budget -= self.pop_size\n                \n                best_idx = np.argmin(fitness)\n                self.f_opt = fitness[best_idx]\n                self.x_opt = pop[best_idx]\n                no_improvement_count = 0  # Reset counter\n                self.ages = np.zeros(self.pop_size)\n            \n            iteration += 1\n        return self.f_opt, self.x_opt\n\n    def create_orthogonal_array(self, n):\n        # A simple orthogonal array generator for 2-level factors\n        oa = []\n        for i in range(n):\n            row = [0] * (2**i) + [1] * (2**i)\n            oa.append(row * (2**(n-i-1)))\n        return np.array(oa).T", "configspace": "", "generation": 6, "feedback": "In the code, line 41, in __call__, the following error occurred:\nValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\nOn line: if oa[np.random.randint(len(oa))] < self.CR:", "error": "In the code, line 41, in __call__, the following error occurred:\nValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\nOn line: if oa[np.random.randint(len(oa))] < self.CR:", "parent_ids": ["575ef623-8129-4cee-9d7c-a8b7d06b7047"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
