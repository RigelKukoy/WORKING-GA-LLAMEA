{"id": "e3f8e28c-8ae7-45c3-bd7e-660daf87dbdc", "fitness": 0.3997373646507569, "name": "AdaptiveDifferentialEvolution", "description": "An adaptive differential evolution algorithm with self-adaptation of crossover rate and scaling factor based on successful and unsuccessful search experiences.", "code": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget=10000, dim=10, pop_size=50):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = 0.5  # Initial scaling factor\n        self.CR = 0.5  # Initial crossover rate\n        self.F_memory = []  # Memory for successful scaling factors\n        self.CR_memory = []  # Memory for successful crossover rates\n        self.memory_size = 10 # Memory size for F and CR\n        self.p = 0.1 # Probability to select from the memory\n        self.lb = -5.0\n        self.ub = 5.0\n        \n\n    def __call__(self, func):\n        # Initialization\n        population = np.random.uniform(self.lb, self.ub, (self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index]\n\n        generation = 0\n        while self.budget > 0:\n            generation += 1\n            for i in range(self.pop_size):\n                # Mutation\n                indices = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = population[indices]\n\n                # Adaptive F\n                if len(self.F_memory) > 0 and np.random.rand() < self.p:\n                    F = np.random.choice(self.F_memory)\n                else:\n                    F = np.clip(np.random.normal(self.F, 0.1), 0.1, 1.0)\n                \n                v = population[i] + F * (x_r1 - x_r2)\n\n                # Crossover\n                j_rand = np.random.randint(self.dim)\n\n                # Adaptive CR\n                if len(self.CR_memory) > 0 and np.random.rand() < self.p:\n                    CR = np.random.choice(self.CR_memory)\n                else:\n                    CR = np.clip(np.random.normal(self.CR, 0.1), 0.0, 1.0)\n                    \n                u = np.array([v[j] if np.random.rand() < CR or j == j_rand else population[i][j] for j in range(self.dim)])\n\n                # Boundary handling\n                u = np.clip(u, self.lb, self.ub)\n\n                # Selection\n                f_u = func(u)\n                self.budget -= 1\n                \n                if f_u < fitness[i]:\n                    # Update successful memory\n                    self.F_memory.append(F)\n                    self.CR_memory.append(CR)\n                    \n                    if len(self.F_memory) > self.memory_size:\n                        self.F_memory.pop(0)\n                        self.CR_memory.pop(0)\n                    \n                    fitness[i] = f_u\n                    population[i] = u\n                    \n                    # Update F and CR\n                    self.F = 0.9 * self.F + 0.1 * F\n                    self.CR = 0.9 * self.CR + 0.1 * CR\n\n                    if f_u < self.f_opt:\n                        self.f_opt = f_u\n                        self.x_opt = u\n\n            best_index = np.argmin(fitness)\n            if fitness[best_index] < self.f_opt:\n                self.f_opt = fitness[best_index]\n                self.x_opt = population[best_index]\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveDifferentialEvolution scored 0.400 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": "init", "metadata": {"llm_response": "# Description: An adaptive differential evolution algorithm with self-adaptation of crossover rate and scaling factor based on successful and unsuccessful search experiences.\n# Code: \n```python\nimport numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget=10000, dim=10, pop_size=50):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = 0.5  # Initial scaling factor\n        self.CR = 0.5  # Initial crossover rate\n        self.F_memory = []  # Memory for successful scaling factors\n        self.CR_memory = []  # Memory for successful crossover rates\n        self.memory_size = 10 # Memory size for F and CR\n        self.p = 0.1 # Probability to select from the memory\n        self.lb = -5.0\n        self.ub = 5.0\n        \n\n    def __call__(self, func):\n        # Initialization\n        population = np.random.uniform(self.lb, self.ub, (self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index]\n\n        generation = 0\n        while self.budget > 0:\n            generation += 1\n            for i in range(self.pop_size):\n                # Mutation\n                indices = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = population[indices]\n\n                # Adaptive F\n                if len(self.F_memory) > 0 and np.random.rand() < self.p:\n                    F = np.random.choice(self.F_memory)\n                else:\n                    F = np.clip(np.random.normal(self.F, 0.1), 0.1, 1.0)\n                \n                v = population[i] + F * (x_r1 - x_r2)\n\n                # Crossover\n                j_rand = np.random.randint(self.dim)\n\n                # Adaptive CR\n                if len(self.CR_memory) > 0 and np.random.rand() < self.p:\n                    CR = np.random.choice(self.CR_memory)\n                else:\n                    CR = np.clip(np.random.normal(self.CR, 0.1), 0.0, 1.0)\n                    \n                u = np.array([v[j] if np.random.rand() < CR or j == j_rand else population[i][j] for j in range(self.dim)])\n\n                # Boundary handling\n                u = np.clip(u, self.lb, self.ub)\n\n                # Selection\n                f_u = func(u)\n                self.budget -= 1\n                \n                if f_u < fitness[i]:\n                    # Update successful memory\n                    self.F_memory.append(F)\n                    self.CR_memory.append(CR)\n                    \n                    if len(self.F_memory) > self.memory_size:\n                        self.F_memory.pop(0)\n                        self.CR_memory.pop(0)\n                    \n                    fitness[i] = f_u\n                    population[i] = u\n                    \n                    # Update F and CR\n                    self.F = 0.9 * self.F + 0.1 * F\n                    self.CR = 0.9 * self.CR + 0.1 * CR\n\n                    if f_u < self.f_opt:\n                        self.f_opt = f_u\n                        self.x_opt = u\n\n            best_index = np.argmin(fitness)\n            if fitness[best_index] < self.f_opt:\n                self.f_opt = fitness[best_index]\n                self.x_opt = population[best_index]\n\n        return self.f_opt, self.x_opt\n```", "generation": 0, "operator": "init", "aucs": [0.14321034873955374, 0.31530590375005985, 0.4048027392738187, 0.34440427442406174, 0.2755483837014009, 0.40911884278325084, 0.2937014078515249, 0.32836318204353354, 0.3665889411268828, 0.18907534567285378, 0.5549589131665469, 0.9818592009116338, 0.44169682206419714, 0.2849244360949741, 0.7356418015618208, 0.4287454093733639, 0.3182930587692534, 0.4726028993475623, 0.2164840353748958, 0.48942134698394935]}, "task_prompt": ""}
{"id": "766720ab-b003-4396-a78b-9f26c9fe9ae6", "fitness": 0.41845092969145326, "name": "AdaptiveDifferentialEvolution", "description": "Adaptive Differential Evolution with archive and restart mechanism to improve exploration and exploitation.", "code": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, Cr=0.9, archive_size=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F  # Mutation factor\n        self.Cr = Cr  # Crossover rate\n        self.archive_size = archive_size\n        self.archive = []\n        self.lb = -5.0\n        self.ub = 5.0\n        self.restart_trigger = 0.1 # Fraction of budget remaining to trigger restart\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget_used = self.pop_size\n        \n        \n        while self.budget_used < self.budget:\n            \n            # Adaptive F and Cr (optional, but can improve performance)\n            F = np.random.normal(self.F, 0.1, self.pop_size)\n            F = np.clip(F, 0.1, 1.0)\n            \n            Cr = np.random.normal(self.Cr, 0.1, self.pop_size)\n            Cr = np.clip(Cr, 0.1, 1.0)\n            \n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x1, x2, x3 = self.population[idxs]\n                mutant = self.population[i] + F[i] * (x2 - x3)\n                mutant = np.clip(mutant, self.lb, self.ub)\n\n                # Crossover\n                trial = np.copy(self.population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < Cr[i] or j == j_rand:\n                        trial[j] = mutant[j]\n\n                # Selection\n                f_trial = func(trial)\n                self.budget_used += 1\n\n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n                    \n                    # Update archive\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(trial)\n                    else:\n                        # Replace a random element in archive\n                        idx_to_replace = np.random.randint(self.archive_size)\n                        self.archive[idx_to_replace] = trial\n\n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = trial\n                    \n            # Restart mechanism\n            if self.budget_used > self.budget * (1 - self.restart_trigger):\n                 # Reset population around the best solution found so far\n                 self.population = np.random.normal(self.x_opt, 0.5, size=(self.pop_size, self.dim))\n                 self.population = np.clip(self.population, self.lb, self.ub)\n                 self.fitness = np.array([func(x) for x in self.population])\n                 self.budget_used += self.pop_size\n                 self.restart_trigger = 0 # Only restart once near the end\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveDifferentialEvolution scored 0.418 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": "init", "metadata": {"llm_response": "# Description: Adaptive Differential Evolution with archive and restart mechanism to improve exploration and exploitation.\n# Code:\n```python\nimport numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, Cr=0.9, archive_size=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F  # Mutation factor\n        self.Cr = Cr  # Crossover rate\n        self.archive_size = archive_size\n        self.archive = []\n        self.lb = -5.0\n        self.ub = 5.0\n        self.restart_trigger = 0.1 # Fraction of budget remaining to trigger restart\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget_used = self.pop_size\n        \n        \n        while self.budget_used < self.budget:\n            \n            # Adaptive F and Cr (optional, but can improve performance)\n            F = np.random.normal(self.F, 0.1, self.pop_size)\n            F = np.clip(F, 0.1, 1.0)\n            \n            Cr = np.random.normal(self.Cr, 0.1, self.pop_size)\n            Cr = np.clip(Cr, 0.1, 1.0)\n            \n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x1, x2, x3 = self.population[idxs]\n                mutant = self.population[i] + F[i] * (x2 - x3)\n                mutant = np.clip(mutant, self.lb, self.ub)\n\n                # Crossover\n                trial = np.copy(self.population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < Cr[i] or j == j_rand:\n                        trial[j] = mutant[j]\n\n                # Selection\n                f_trial = func(trial)\n                self.budget_used += 1\n\n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n                    \n                    # Update archive\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(trial)\n                    else:\n                        # Replace a random element in archive\n                        idx_to_replace = np.random.randint(self.archive_size)\n                        self.archive[idx_to_replace] = trial\n\n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = trial\n                    \n            # Restart mechanism\n            if self.budget_used > self.budget * (1 - self.restart_trigger):\n                 # Reset population around the best solution found so far\n                 self.population = np.random.normal(self.x_opt, 0.5, size=(self.pop_size, self.dim))\n                 self.population = np.clip(self.population, self.lb, self.ub)\n                 self.fitness = np.array([func(x) for x in self.population])\n                 self.budget_used += self.pop_size\n                 self.restart_trigger = 0 # Only restart once near the end\n\n        return self.f_opt, self.x_opt\n```", "generation": 0, "operator": "init", "aucs": [0.14899279947130928, 0.2803993549089372, 0.37646059268376064, 0.5256751348817129, 0.32283800676959806, 0.4530494695189714, 0.30557624268989914, 0.34248328147584417, 0.31838728077041345, 0.20551780110692786, 0.6469845243994197, 0.9982086996919072, 0.3938644252700957, 0.3095892571732235, 0.7637665369191345, 0.4276232433513294, 0.3454363801355722, 0.5284896677255013, 0.18105558753872264, 0.4946203073467844]}, "task_prompt": ""}
{"id": "960b6f98-0894-478d-87ea-1a05282bdd13", "fitness": "-inf", "name": "AntColonyOptimization", "description": "A population-based algorithm inspired by the movement of ants, using pheromone trails and evaporation to guide the search towards promising regions, while incorporating a local search to fine-tune solutions.", "code": "import numpy as np\n\nclass AntColonyOptimization:\n    def __init__(self, budget=10000, dim=10, colony_size=20, evaporation_rate=0.1, alpha=1, beta=2, local_search_iterations=5):\n        self.budget = budget\n        self.dim = dim\n        self.colony_size = colony_size\n        self.evaporation_rate = evaporation_rate\n        self.alpha = alpha # Pheromone influence\n        self.beta = beta # Heuristic influence (fitness)\n        self.local_search_iterations = local_search_iterations\n\n        self.pheromone = None\n        self.ants = None\n        self.fitness = None\n        self.best_ant_index = None\n\n    def initialize_ants(self, func):\n        self.ants = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.colony_size, self.dim))\n        self.fitness = np.array([func(ant) for ant in self.ants])\n        self.best_ant_index = np.argmin(self.fitness)\n        \n        self.pheromone = np.ones(self.dim) # Initial pheromone trails for each dimension\n\n    def local_search(self, ant, func):\n        best_ant = ant.copy()\n        best_fitness = func(ant)\n        \n        for _ in range(self.local_search_iterations):\n            # Generate a neighbor by slightly perturbing the current ant\n            neighbor = ant + np.random.normal(0, 0.1, size=self.dim)  # Adjust standard deviation as needed\n            neighbor = np.clip(neighbor, func.bounds.lb, func.bounds.ub) # Keep within bounds\n            \n            neighbor_fitness = func(neighbor)\n            \n            if neighbor_fitness < best_fitness:\n                best_fitness = neighbor_fitness\n                best_ant = neighbor\n        \n        return best_ant, best_fitness\n\n    def choose_dimension(self, pheromone_values, fitness_values, func):\n         # Calculate probabilities for each dimension based on pheromone and fitness\n        probabilities = (pheromone_values**self.alpha) * ((1 / (fitness_values + 1e-9))**self.beta) # add small value to avoid divide by zero\n        probabilities /= np.sum(probabilities)  # Normalize probabilities\n        \n        # Choose a dimension based on the probabilities\n        chosen_dimension = np.random.choice(len(pheromone_values), p=probabilities)\n        return chosen_dimension\n\n    def update_pheromone(self, func):\n        # Evaporate pheromone\n        self.pheromone *= (1 - self.evaporation_rate)\n\n        # Deposit pheromone based on ant quality (best ant deposits more)\n        best_ant = self.ants[self.best_ant_index]\n        self.pheromone += (1.0 / (self.fitness[self.best_ant_index] + 1e-9)) * np.abs(best_ant)\n        \n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        eval_count = 0\n\n        self.initialize_ants(func)\n        eval_count += self.colony_size\n        \n        while eval_count < self.budget:\n            new_ants = np.zeros_like(self.ants)\n            new_fitness = np.zeros_like(self.fitness)\n            \n            for i in range(self.colony_size):\n                # Construct a new solution (ant)\n                ant = self.ants[i].copy()\n\n                # Iteratively improve each dimension\n                for _ in range(self.dim):\n                    chosen_dimension = self.choose_dimension(self.pheromone, self.fitness, func) # Choose which dimension to change based on pheromones and fitness\n                    \n                    # Explore the chosen dimension using a Gaussian distribution centered around current value\n                    ant[chosen_dimension] = np.random.normal(ant[chosen_dimension], 0.2) # adjust the std-dev\n                    ant[chosen_dimension] = np.clip(ant[chosen_dimension], func.bounds.lb[chosen_dimension], func.bounds.ub[chosen_dimension])\n\n                # Local Search\n                ant, fitness = self.local_search(ant, func)\n                eval_count += self.local_search_iterations\n                \n                new_ants[i] = ant\n                new_fitness[i] = fitness\n\n                if fitness < self.f_opt:\n                    self.f_opt = fitness\n                    self.x_opt = ant.copy()\n            \n            self.ants = new_ants\n            self.fitness = new_fitness\n            self.best_ant_index = np.argmin(self.fitness)\n            \n            self.update_pheromone(func)\n            \n            if eval_count > self.budget:\n                break\n        \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "In the code, line 44, in choose_dimension, the following error occurred:\nValueError: operands could not be broadcast together with shapes (2,) (20,) \nOn line: probabilities = (pheromone_values**self.alpha) * ((1 / (fitness_values + 1e-9))**self.beta) # add small value to avoid divide by zero", "error": "In the code, line 44, in choose_dimension, the following error occurred:\nValueError: operands could not be broadcast together with shapes (2,) (20,) \nOn line: probabilities = (pheromone_values**self.alpha) * ((1 / (fitness_values + 1e-9))**self.beta) # add small value to avoid divide by zero", "parent_ids": [], "operator": "init", "metadata": {"llm_response": "# Description: A population-based algorithm inspired by the movement of ants, using pheromone trails and evaporation to guide the search towards promising regions, while incorporating a local search to fine-tune solutions.\n# Code: \n```python\nimport numpy as np\n\nclass AntColonyOptimization:\n    def __init__(self, budget=10000, dim=10, colony_size=20, evaporation_rate=0.1, alpha=1, beta=2, local_search_iterations=5):\n        self.budget = budget\n        self.dim = dim\n        self.colony_size = colony_size\n        self.evaporation_rate = evaporation_rate\n        self.alpha = alpha # Pheromone influence\n        self.beta = beta # Heuristic influence (fitness)\n        self.local_search_iterations = local_search_iterations\n\n        self.pheromone = None\n        self.ants = None\n        self.fitness = None\n        self.best_ant_index = None\n\n    def initialize_ants(self, func):\n        self.ants = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.colony_size, self.dim))\n        self.fitness = np.array([func(ant) for ant in self.ants])\n        self.best_ant_index = np.argmin(self.fitness)\n        \n        self.pheromone = np.ones(self.dim) # Initial pheromone trails for each dimension\n\n    def local_search(self, ant, func):\n        best_ant = ant.copy()\n        best_fitness = func(ant)\n        \n        for _ in range(self.local_search_iterations):\n            # Generate a neighbor by slightly perturbing the current ant\n            neighbor = ant + np.random.normal(0, 0.1, size=self.dim)  # Adjust standard deviation as needed\n            neighbor = np.clip(neighbor, func.bounds.lb, func.bounds.ub) # Keep within bounds\n            \n            neighbor_fitness = func(neighbor)\n            \n            if neighbor_fitness < best_fitness:\n                best_fitness = neighbor_fitness\n                best_ant = neighbor\n        \n        return best_ant, best_fitness\n\n    def choose_dimension(self, pheromone_values, fitness_values, func):\n         # Calculate probabilities for each dimension based on pheromone and fitness\n        probabilities = (pheromone_values**self.alpha) * ((1 / (fitness_values + 1e-9))**self.beta) # add small value to avoid divide by zero\n        probabilities /= np.sum(probabilities)  # Normalize probabilities\n        \n        # Choose a dimension based on the probabilities\n        chosen_dimension = np.random.choice(len(pheromone_values), p=probabilities)\n        return chosen_dimension\n\n    def update_pheromone(self, func):\n        # Evaporate pheromone\n        self.pheromone *= (1 - self.evaporation_rate)\n\n        # Deposit pheromone based on ant quality (best ant deposits more)\n        best_ant = self.ants[self.best_ant_index]\n        self.pheromone += (1.0 / (self.fitness[self.best_ant_index] + 1e-9)) * np.abs(best_ant)\n        \n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        eval_count = 0\n\n        self.initialize_ants(func)\n        eval_count += self.colony_size\n        \n        while eval_count < self.budget:\n            new_ants = np.zeros_like(self.ants)\n            new_fitness = np.zeros_like(self.fitness)\n            \n            for i in range(self.colony_size):\n                # Construct a new solution (ant)\n                ant = self.ants[i].copy()\n\n                # Iteratively improve each dimension\n                for _ in range(self.dim):\n                    chosen_dimension = self.choose_dimension(self.pheromone, self.fitness, func) # Choose which dimension to change based on pheromones and fitness\n                    \n                    # Explore the chosen dimension using a Gaussian distribution centered around current value\n                    ant[chosen_dimension] = np.random.normal(ant[chosen_dimension], 0.2) # adjust the std-dev\n                    ant[chosen_dimension] = np.clip(ant[chosen_dimension], func.bounds.lb[chosen_dimension], func.bounds.ub[chosen_dimension])\n\n                # Local Search\n                ant, fitness = self.local_search(ant, func)\n                eval_count += self.local_search_iterations\n                \n                new_ants[i] = ant\n                new_fitness[i] = fitness\n\n                if fitness < self.f_opt:\n                    self.f_opt = fitness\n                    self.x_opt = ant.copy()\n            \n            self.ants = new_ants\n            self.fitness = new_fitness\n            self.best_ant_index = np.argmin(self.fitness)\n            \n            self.update_pheromone(func)\n            \n            if eval_count > self.budget:\n                break\n        \n        return self.f_opt, self.x_opt\n```", "generation": 0, "operator": "init"}, "task_prompt": ""}
{"id": "848302de-9973-4dba-8916-b3898be42c35", "fitness": 0.4286531442927918, "name": "SocialForagingOptimization", "description": "Population-based algorithm inspired by social foraging behavior, with adaptive step size and local search enhancement.", "code": "import numpy as np\n\nclass SocialForagingOptimization:\n    def __init__(self, budget=10000, dim=10, population_size=20, alpha=0.1, beta=1.0, gamma=0.5, local_search_iterations=5):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = population_size\n        self.alpha = alpha  # Exploration weight\n        self.beta = beta  # Social influence weight\n        self.gamma = gamma # Local search weight\n        self.local_search_iterations = local_search_iterations\n\n    def __call__(self, func):\n        # Initialize population within the bounds.\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.population_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.population_size\n        \n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index]\n\n        iteration = 0\n        while self.budget > 0:\n            iteration += 1\n            for i in range(self.population_size):\n                # Social Foraging Movement\n                best_index = np.argmin(fitness)\n                \n                # Adaptive step size\n                step_size = (func.bounds.ub - func.bounds.lb) / (iteration + 1)\n                \n                # Exploration: Random walk\n                exploration = step_size * np.random.uniform(-1, 1, size=self.dim)\n\n                # Social Influence: Move towards the best solution\n                social_influence = self.beta * (population[best_index] - population[i])\n\n                # Combined movement\n                new_position = population[i] + self.alpha * exploration + social_influence\n                \n                # Boundary Handling (clip to stay within bounds)\n                new_position = np.clip(new_position, func.bounds.lb, func.bounds.ub)\n\n                # Local Search around the new position\n                best_local_f = np.inf\n                best_local_x = new_position\n                for _ in range(self.local_search_iterations):\n                     if self.budget <= 0:\n                         break\n                     local_step = step_size * np.random.uniform(-0.1, 0.1, size=self.dim)\n                     local_x = new_position + local_step\n                     local_x = np.clip(local_x, func.bounds.lb, func.bounds.ub)\n                     local_f = func(local_x)\n                     self.budget -= 1\n                     if local_f < best_local_f:\n                         best_local_f = local_f\n                         best_local_x = local_x\n                \n                \n                #Evaluate new position with local search result\n                new_fitness = func(best_local_x) if self.budget > 0 else np.inf\n                if self.budget > 0:\n                    self.budget -=1\n                else:\n                    new_fitness = np.inf\n                    \n                \n                if new_fitness < fitness[i]:\n                    fitness[i] = new_fitness\n                    population[i] = best_local_x\n                    \n                    if new_fitness < self.f_opt:\n                        self.f_opt = new_fitness\n                        self.x_opt = best_local_x\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm SocialForagingOptimization scored 0.429 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": "init", "metadata": {"llm_response": "# Description: Population-based algorithm inspired by social foraging behavior, with adaptive step size and local search enhancement.\n# Code: \n```python\nimport numpy as np\n\nclass SocialForagingOptimization:\n    def __init__(self, budget=10000, dim=10, population_size=20, alpha=0.1, beta=1.0, gamma=0.5, local_search_iterations=5):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = population_size\n        self.alpha = alpha  # Exploration weight\n        self.beta = beta  # Social influence weight\n        self.gamma = gamma # Local search weight\n        self.local_search_iterations = local_search_iterations\n\n    def __call__(self, func):\n        # Initialize population within the bounds.\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.population_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.population_size\n        \n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index]\n\n        iteration = 0\n        while self.budget > 0:\n            iteration += 1\n            for i in range(self.population_size):\n                # Social Foraging Movement\n                best_index = np.argmin(fitness)\n                \n                # Adaptive step size\n                step_size = (func.bounds.ub - func.bounds.lb) / (iteration + 1)\n                \n                # Exploration: Random walk\n                exploration = step_size * np.random.uniform(-1, 1, size=self.dim)\n\n                # Social Influence: Move towards the best solution\n                social_influence = self.beta * (population[best_index] - population[i])\n\n                # Combined movement\n                new_position = population[i] + self.alpha * exploration + social_influence\n                \n                # Boundary Handling (clip to stay within bounds)\n                new_position = np.clip(new_position, func.bounds.lb, func.bounds.ub)\n\n                # Local Search around the new position\n                best_local_f = np.inf\n                best_local_x = new_position\n                for _ in range(self.local_search_iterations):\n                     if self.budget <= 0:\n                         break\n                     local_step = step_size * np.random.uniform(-0.1, 0.1, size=self.dim)\n                     local_x = new_position + local_step\n                     local_x = np.clip(local_x, func.bounds.lb, func.bounds.ub)\n                     local_f = func(local_x)\n                     self.budget -= 1\n                     if local_f < best_local_f:\n                         best_local_f = local_f\n                         best_local_x = local_x\n                \n                \n                #Evaluate new position with local search result\n                new_fitness = func(best_local_x) if self.budget > 0 else np.inf\n                if self.budget > 0:\n                    self.budget -=1\n                else:\n                    new_fitness = np.inf\n                    \n                \n                if new_fitness < fitness[i]:\n                    fitness[i] = new_fitness\n                    population[i] = best_local_x\n                    \n                    if new_fitness < self.f_opt:\n                        self.f_opt = new_fitness\n                        self.x_opt = best_local_x\n\n        return self.f_opt, self.x_opt\n```", "generation": 0, "operator": "init", "aucs": [0.10192947741334502, 0.1835129804754434, 0.49934333752706994, 0.2099883693512038, 0.4880718882074857, 0.5547393976666632, 0.2616779600351856, 0.4345668114025001, 0.17885130702401875, 0.17912105656163313, 0.9242614926861822, 0.997315679906982, 0.24730197287902767, 0.524595674551722, 0.9399045476118574, 0.33473664292667893, 0.46152816480417735, 0.6847193294870833, 0.14960837376238634, 0.21728842157518902]}, "task_prompt": ""}
{"id": "16f1e2fa-91d7-49da-8f8e-b5e9584c981d", "fitness": 0.34829361690223, "name": "DiffusionOptimization", "description": "A population-based algorithm inspired by the diffusion of particles, where particles randomly move and are attracted to the best location found so far. The algorithm uses a dynamic inertia weight to balance exploration and exploitation, and a constriction factor to control the particle velocity.", "code": "import numpy as np\n\nclass DiffusionOptimization:\n    def __init__(self, budget=10000, dim=10, pop_size=20, inertia_max=0.9, inertia_min=0.4, constriction_factor=0.7):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.inertia_max = inertia_max\n        self.inertia_min = inertia_min\n        self.constriction_factor = constriction_factor  # Constriction factor to prevent divergence\n\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        \n        # Initialize population\n        population = np.random.uniform(lb, ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size # update budget\n\n        # Initialize best solution\n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index].copy()  # Use copy to avoid modification of original\n\n        # Initialize velocities\n        velocities = np.random.uniform(-abs(ub - lb), abs(ub - lb), size=(self.pop_size, self.dim))\n\n        # Optimization loop\n        while self.budget > 0:\n            # Calculate inertia weight (linearly decreasing)\n            inertia = self.inertia_max - (self.inertia_max - self.inertia_min) * (1 - self.budget / self.budget)\n\n            for i in range(self.pop_size):\n                # Update velocity\n                r1 = np.random.rand(self.dim)\n                velocities[i] = self.constriction_factor * (inertia * velocities[i] + r1 * (self.x_opt - population[i]))\n\n                # Update position\n                population[i] = population[i] + velocities[i]\n                \n                # Boundary handling (clip values)\n                population[i] = np.clip(population[i], lb, ub)\n\n                # Evaluate fitness\n                f = func(population[i])\n                self.budget -= 1 # update budget\n                if f < fitness[i]:\n                    fitness[i] = f\n\n                # Update best solution\n                if f < self.f_opt:\n                    self.f_opt = f\n                    self.x_opt = population[i].copy()\n                \n                if self.budget <= 0:\n                    break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "The algorithm DiffusionOptimization scored 0.348 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": "random_new", "metadata": {"llm_response": "# Description: A population-based algorithm inspired by the diffusion of particles, where particles randomly move and are attracted to the best location found so far. The algorithm uses a dynamic inertia weight to balance exploration and exploitation, and a constriction factor to control the particle velocity.\n\n# Code:\n```python\nimport numpy as np\n\nclass DiffusionOptimization:\n    def __init__(self, budget=10000, dim=10, pop_size=20, inertia_max=0.9, inertia_min=0.4, constriction_factor=0.7):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.inertia_max = inertia_max\n        self.inertia_min = inertia_min\n        self.constriction_factor = constriction_factor  # Constriction factor to prevent divergence\n\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        \n        # Initialize population\n        population = np.random.uniform(lb, ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size # update budget\n\n        # Initialize best solution\n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index].copy()  # Use copy to avoid modification of original\n\n        # Initialize velocities\n        velocities = np.random.uniform(-abs(ub - lb), abs(ub - lb), size=(self.pop_size, self.dim))\n\n        # Optimization loop\n        while self.budget > 0:\n            # Calculate inertia weight (linearly decreasing)\n            inertia = self.inertia_max - (self.inertia_max - self.inertia_min) * (1 - self.budget / self.budget)\n\n            for i in range(self.pop_size):\n                # Update velocity\n                r1 = np.random.rand(self.dim)\n                velocities[i] = self.constriction_factor * (inertia * velocities[i] + r1 * (self.x_opt - population[i]))\n\n                # Update position\n                population[i] = population[i] + velocities[i]\n                \n                # Boundary handling (clip values)\n                population[i] = np.clip(population[i], lb, ub)\n\n                # Evaluate fitness\n                f = func(population[i])\n                self.budget -= 1 # update budget\n                if f < fitness[i]:\n                    fitness[i] = f\n\n                # Update best solution\n                if f < self.f_opt:\n                    self.f_opt = f\n                    self.x_opt = population[i].copy()\n                \n                if self.budget <= 0:\n                    break\n\n        return self.f_opt, self.x_opt\n```", "operator": "random_new", "theta_sampled": 0.43440346009180963, "generation": 1, "aucs": [0.17882335471800814, 0.18108219163496864, 0.9170468167895759, 0.22702765702955763, 0.24885553066596544, 0.28452571306414165, 0.22152631226335284, 0.2767562351158154, 0.3319369155206171, 0.18596166016923643, 0.2223576046337682, 0.9979560129278363, 0.23823079215397724, 0.2721951024864133, 0.6485915689265869, 0.33516656029694114, 0.22088456651769917, 0.37882600792705157, 0.15348183950329186, 0.44463989569979523]}, "task_prompt": ""}
{"id": "646f31c7-6642-473c-a744-159688d7691a", "fitness": "-inf", "name": "AdaptiveHybridOptimization", "description": "An adaptive search algorithm that uses a combination of global exploration with differential evolution and local exploitation using a Nelder-Mead simplex.", "code": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveHybridOptimization:\n    def __init__(self, budget=10000, dim=10, de_pop_size=20, nm_iters=5):\n        self.budget = budget\n        self.dim = dim\n        self.de_pop_size = de_pop_size\n        self.nm_iters = nm_iters\n        self.F = 0.5  # Differential evolution parameter\n        self.CR = 0.7 # Crossover probability\n\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        \n        # Initialize population for differential evolution\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.de_pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.de_pop_size\n\n        # Initial best\n        best_index = np.argmin(fitness)\n        if fitness[best_index] < self.f_opt:\n            self.f_opt = fitness[best_index]\n            self.x_opt = population[best_index]\n\n        # Main loop\n        while self.budget > 0:\n            for i in range(self.de_pop_size):\n                # Differential Evolution\n                idxs = [idx for idx in range(self.de_pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n\n                mutant = a + self.F * (b - c)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                trial = np.copy(population[i])\n                cross_points = np.random.rand(self.dim) < self.CR\n                trial[cross_points] = mutant[cross_points]\n                \n                # Evaluate trial vector\n                f_trial = func(trial)\n                self.budget -= 1\n                \n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    population[i] = trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n            # Local search using Nelder-Mead around the best solution\n            if self.budget > 0:\n                nm_budget = min(self.budget, self.nm_iters* (self.dim + 1)) #Limit the budget used by NM.\n\n                result = minimize(func, self.x_opt, method='Nelder-Mead',\n                                  bounds=np.array([func.bounds.lb, func.bounds.ub]).T,\n                                  options={'maxiter': self.nm_iters, 'maxfev': nm_budget})\n                \n                self.budget -= result.nfev\n                \n                if result.fun < self.f_opt:\n                    self.f_opt = result.fun\n                    self.x_opt = result.x\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "", "error": "Validation failed: Forbidden import: 'scipy.optimize' is not available in the evaluation environment", "parent_ids": [], "operator": "random_new", "metadata": {"llm_response": "# Description: An adaptive search algorithm that uses a combination of global exploration with differential evolution and local exploitation using a Nelder-Mead simplex.\n# Code: \n```python\nimport numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveHybridOptimization:\n    def __init__(self, budget=10000, dim=10, de_pop_size=20, nm_iters=5):\n        self.budget = budget\n        self.dim = dim\n        self.de_pop_size = de_pop_size\n        self.nm_iters = nm_iters\n        self.F = 0.5  # Differential evolution parameter\n        self.CR = 0.7 # Crossover probability\n\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        \n        # Initialize population for differential evolution\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.de_pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.de_pop_size\n\n        # Initial best\n        best_index = np.argmin(fitness)\n        if fitness[best_index] < self.f_opt:\n            self.f_opt = fitness[best_index]\n            self.x_opt = population[best_index]\n\n        # Main loop\n        while self.budget > 0:\n            for i in range(self.de_pop_size):\n                # Differential Evolution\n                idxs = [idx for idx in range(self.de_pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n\n                mutant = a + self.F * (b - c)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                trial = np.copy(population[i])\n                cross_points = np.random.rand(self.dim) < self.CR\n                trial[cross_points] = mutant[cross_points]\n                \n                # Evaluate trial vector\n                f_trial = func(trial)\n                self.budget -= 1\n                \n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    population[i] = trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n            # Local search using Nelder-Mead around the best solution\n            if self.budget > 0:\n                nm_budget = min(self.budget, self.nm_iters* (self.dim + 1)) #Limit the budget used by NM.\n\n                result = minimize(func, self.x_opt, method='Nelder-Mead',\n                                  bounds=np.array([func.bounds.lb, func.bounds.ub]).T,\n                                  options={'maxiter': self.nm_iters, 'maxfev': nm_budget})\n                \n                self.budget -= result.nfev\n                \n                if result.fun < self.f_opt:\n                    self.f_opt = result.fun\n                    self.x_opt = result.x\n\n        return self.f_opt, self.x_opt\n```"}, "task_prompt": ""}
{"id": "09f2df5d-bd76-4716-9159-ce14092a2f1b", "fitness": "-inf", "name": "SocialForagingOptimizationWithArchive", "description": "Combines Social Foraging with an archive for enhanced exploration and exploitation.", "code": "import numpy as np\n\nclass SocialForagingOptimizationWithArchive:\n    def __init__(self, budget=10000, dim=10, population_size=20, alpha=0.1, beta=1.0, gamma=0.5, local_search_iterations=5, archive_size=10):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = population_size\n        self.alpha = alpha  # Exploration weight\n        self.beta = beta  # Social influence weight\n        self.gamma = gamma # Local search weight\n        self.local_search_iterations = local_search_iterations\n        self.archive_size = archive_size\n        self.archive = []\n\n    def __call__(self, func):\n        # Initialize population within the bounds.\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.population_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.population_size\n        \n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index]\n\n        iteration = 0\n        while self.budget > 0:\n            iteration += 1\n            for i in range(self.population_size):\n                # Social Foraging Movement\n                best_index = np.argmin(fitness)\n                \n                # Adaptive step size\n                step_size = (func.bounds.ub - func.bounds.lb) / (iteration + 1)\n                \n                # Exploration: Random walk\n                exploration = step_size * np.random.uniform(-1, 1, size=self.dim)\n\n                # Social Influence: Move towards the best solution\n                social_influence = self.beta * (population[best_index] - population[i])\n\n                # Archive Influence: Randomly select an archive member\n                if self.archive:\n                    archive_member = self.archive[np.random.randint(len(self.archive))]\n                    archive_influence = self.gamma * (archive_member - population[i])\n                else:\n                    archive_influence = 0  # No influence if archive is empty\n\n                # Combined movement\n                new_position = population[i] + self.alpha * exploration + social_influence + archive_influence\n                \n                # Boundary Handling (clip to stay within bounds)\n                new_position = np.clip(new_position, func.bounds.lb, func.bounds.ub)\n\n                # Local Search around the new position\n                best_local_f = np.inf\n                best_local_x = new_position\n                for _ in range(self.local_search_iterations):\n                     if self.budget <= 0:\n                         break\n                     local_step = step_size * np.random.uniform(-0.1, 0.1, size=self.dim)\n                     local_x = new_position + local_step\n                     local_x = np.clip(local_x, func.bounds.lb, func.bounds.ub)\n                     local_f = func(local_x)\n                     self.budget -= 1\n                     if local_f < best_local_f:\n                         best_local_f = local_f\n                         best_local_x = local_x\n                \n                \n                #Evaluate new position with local search result\n                new_fitness = func(best_local_x) if self.budget > 0 else np.inf\n                if self.budget > 0:\n                    self.budget -=1\n                else:\n                    new_fitness = np.inf\n                    \n                \n                if new_fitness < fitness[i]:\n                    fitness[i] = new_fitness\n                    population[i] = best_local_x\n\n                    # Update archive\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(best_local_x)\n                    else:\n                        # Replace a random element in archive\n                        idx_to_replace = np.random.randint(self.archive_size)\n                        self.archive[idx_to_replace] = best_local_x\n                    \n                    if new_fitness < self.f_opt:\n                        self.f_opt = new_fitness\n                        self.x_opt = best_local_x\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "Evaluation timed out after 300 seconds.", "error": "Evaluation timed out after 300 seconds.", "parent_ids": ["848302de-9973-4dba-8916-b3898be42c35", "766720ab-b003-4396-a78b-9f26c9fe9ae6"], "operator": "crossover", "metadata": {"llm_response": "# Description: Combines Social Foraging with an archive for enhanced exploration and exploitation.\n# Code: \n```python\nimport numpy as np\n\nclass SocialForagingOptimizationWithArchive:\n    def __init__(self, budget=10000, dim=10, population_size=20, alpha=0.1, beta=1.0, gamma=0.5, local_search_iterations=5, archive_size=10):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = population_size\n        self.alpha = alpha  # Exploration weight\n        self.beta = beta  # Social influence weight\n        self.gamma = gamma # Local search weight\n        self.local_search_iterations = local_search_iterations\n        self.archive_size = archive_size\n        self.archive = []\n\n    def __call__(self, func):\n        # Initialize population within the bounds.\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.population_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.population_size\n        \n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index]\n\n        iteration = 0\n        while self.budget > 0:\n            iteration += 1\n            for i in range(self.population_size):\n                # Social Foraging Movement\n                best_index = np.argmin(fitness)\n                \n                # Adaptive step size\n                step_size = (func.bounds.ub - func.bounds.lb) / (iteration + 1)\n                \n                # Exploration: Random walk\n                exploration = step_size * np.random.uniform(-1, 1, size=self.dim)\n\n                # Social Influence: Move towards the best solution\n                social_influence = self.beta * (population[best_index] - population[i])\n\n                # Archive Influence: Randomly select an archive member\n                if self.archive:\n                    archive_member = self.archive[np.random.randint(len(self.archive))]\n                    archive_influence = self.gamma * (archive_member - population[i])\n                else:\n                    archive_influence = 0  # No influence if archive is empty\n\n                # Combined movement\n                new_position = population[i] + self.alpha * exploration + social_influence + archive_influence\n                \n                # Boundary Handling (clip to stay within bounds)\n                new_position = np.clip(new_position, func.bounds.lb, func.bounds.ub)\n\n                # Local Search around the new position\n                best_local_f = np.inf\n                best_local_x = new_position\n                for _ in range(self.local_search_iterations):\n                     if self.budget <= 0:\n                         break\n                     local_step = step_size * np.random.uniform(-0.1, 0.1, size=self.dim)\n                     local_x = new_position + local_step\n                     local_x = np.clip(local_x, func.bounds.lb, func.bounds.ub)\n                     local_f = func(local_x)\n                     self.budget -= 1\n                     if local_f < best_local_f:\n                         best_local_f = local_f\n                         best_local_x = local_x\n                \n                \n                #Evaluate new position with local search result\n                new_fitness = func(best_local_x) if self.budget > 0 else np.inf\n                if self.budget > 0:\n                    self.budget -=1\n                else:\n                    new_fitness = np.inf\n                    \n                \n                if new_fitness < fitness[i]:\n                    fitness[i] = new_fitness\n                    population[i] = best_local_x\n\n                    # Update archive\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(best_local_x)\n                    else:\n                        # Replace a random element in archive\n                        idx_to_replace = np.random.randint(self.archive_size)\n                        self.archive[idx_to_replace] = best_local_x\n                    \n                    if new_fitness < self.f_opt:\n                        self.f_opt = new_fitness\n                        self.x_opt = best_local_x\n\n        return self.f_opt, self.x_opt\n```", "operator": "crossover", "theta_sampled": 0.01795601729856725, "generation": 1}, "task_prompt": ""}
{"id": "ff00e08d-642a-46cd-a5c5-cbbf02e71d29", "fitness": 0.0, "name": "AdaptiveDifferentialEvolution", "description": "An adaptive differential evolution algorithm with dynamically adjusted parameters and a simplified restart mechanism for improved exploration and exploitation.", "code": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget=10000, dim=10, pop_size=40, F=0.5, Cr=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F  # Mutation factor\n        self.Cr = Cr  # Crossover rate\n        self.lb = -5.0\n        self.ub = 5.0\n        self.best_fitness = np.inf\n        self.best_solution = None\n        self.population = None\n        self.fitness = None\n        self.budget_used = 0\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        \n        while self.budget_used < self.budget:\n            self.iterate(func)\n\n        return self.best_fitness, self.best_solution\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget_used += self.pop_size\n        \n        best_index = np.argmin(self.fitness)\n        self.best_fitness = self.fitness[best_index]\n        self.best_solution = self.population[best_index]\n\n    def iterate(self, func):\n        for i in range(self.pop_size):\n            # Adaptive F and Cr\n            F = np.clip(np.random.normal(self.F, 0.1), 0.1, 1.0)\n            Cr = np.clip(np.random.normal(self.Cr, 0.1), 0.1, 1.0)\n\n            # Mutation\n            idxs = np.random.choice(self.pop_size, 3, replace=False)\n            x1, x2, x3 = self.population[idxs]\n            mutant = self.population[i] + F * (x2 - x3)\n            mutant = np.clip(mutant, self.lb, self.ub)\n\n            # Crossover\n            trial = np.copy(self.population[i])\n            crossover_points = np.random.rand(self.dim) < Cr\n            if not np.any(crossover_points):\n                crossover_points[np.random.randint(0, self.dim)] = True\n            trial[crossover_points] = mutant[crossover_points]\n\n            # Selection\n            f_trial = func(trial)\n            self.budget_used += 1\n\n            if f_trial < self.fitness[i]:\n                self.fitness[i] = f_trial\n                self.population[i] = trial\n\n                if f_trial < self.best_fitness:\n                    self.best_fitness = f_trial\n                    self.best_solution = trial\n\n            # Restart if stagnation detected (simplified)\n            if self.budget_used > self.budget * 0.9 and np.random.rand() < 0.05:  # Restart occasionally near the end\n                self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n                self.fitness = np.array([func(x) for x in self.population])\n                self.budget_used += self.pop_size\n                best_index = np.argmin(self.fitness)\n                if self.fitness[best_index] < self.best_fitness:\n                    self.best_fitness = self.fitness[best_index]\n                    self.best_solution = self.population[best_index]", "configspace": "", "generation": 1, "feedback": "The algorithm AdaptiveDifferentialEvolution scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["766720ab-b003-4396-a78b-9f26c9fe9ae6"], "operator": "simplify", "metadata": {"llm_response": "# Description: An adaptive differential evolution algorithm with dynamically adjusted parameters and a simplified restart mechanism for improved exploration and exploitation.\n# Code: \n```python\nimport numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget=10000, dim=10, pop_size=40, F=0.5, Cr=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F  # Mutation factor\n        self.Cr = Cr  # Crossover rate\n        self.lb = -5.0\n        self.ub = 5.0\n        self.best_fitness = np.inf\n        self.best_solution = None\n        self.population = None\n        self.fitness = None\n        self.budget_used = 0\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        \n        while self.budget_used < self.budget:\n            self.iterate(func)\n\n        return self.best_fitness, self.best_solution\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget_used += self.pop_size\n        \n        best_index = np.argmin(self.fitness)\n        self.best_fitness = self.fitness[best_index]\n        self.best_solution = self.population[best_index]\n\n    def iterate(self, func):\n        for i in range(self.pop_size):\n            # Adaptive F and Cr\n            F = np.clip(np.random.normal(self.F, 0.1), 0.1, 1.0)\n            Cr = np.clip(np.random.normal(self.Cr, 0.1), 0.1, 1.0)\n\n            # Mutation\n            idxs = np.random.choice(self.pop_size, 3, replace=False)\n            x1, x2, x3 = self.population[idxs]\n            mutant = self.population[i] + F * (x2 - x3)\n            mutant = np.clip(mutant, self.lb, self.ub)\n\n            # Crossover\n            trial = np.copy(self.population[i])\n            crossover_points = np.random.rand(self.dim) < Cr\n            if not np.any(crossover_points):\n                crossover_points[np.random.randint(0, self.dim)] = True\n            trial[crossover_points] = mutant[crossover_points]\n\n            # Selection\n            f_trial = func(trial)\n            self.budget_used += 1\n\n            if f_trial < self.fitness[i]:\n                self.fitness[i] = f_trial\n                self.population[i] = trial\n\n                if f_trial < self.best_fitness:\n                    self.best_fitness = f_trial\n                    self.best_solution = trial\n\n            # Restart if stagnation detected (simplified)\n            if self.budget_used > self.budget * 0.9 and np.random.rand() < 0.05:  # Restart occasionally near the end\n                self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n                self.fitness = np.array([func(x) for x in self.population])\n                self.budget_used += self.pop_size\n                best_index = np.argmin(self.fitness)\n                if self.fitness[best_index] < self.best_fitness:\n                    self.best_fitness = self.fitness[best_index]\n                    self.best_solution = self.population[best_index]\n```", "operator": "simplify", "theta_sampled": 0.375025720345183, "generation": 1, "aucs": [0]}, "task_prompt": ""}
{"id": "f9cf0cc3-f705-489d-a85a-f8870e5e6ae0", "fitness": "-inf", "name": "SocialForagingOptimizationPlus", "description": "Social Foraging with Archive-Based Mutation and Late-Restart Mechanism", "code": "import numpy as np\n\nclass SocialForagingOptimizationPlus:\n    def __init__(self, budget=10000, dim=10, population_size=20, alpha=0.1, beta=1.0, archive_size=10, local_search_iterations=5, restart_trigger=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = population_size\n        self.alpha = alpha  # Exploration weight\n        self.beta = beta  # Social influence weight\n        self.archive_size = archive_size\n        self.archive = []\n        self.local_search_iterations = local_search_iterations\n        self.restart_trigger = restart_trigger\n\n    def __call__(self, func):\n        # Initialize population within the bounds.\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        population = np.random.uniform(lb, ub, size=(self.population_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.population_size\n        \n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index]\n\n        iteration = 0\n        while self.budget > 0:\n            iteration += 1\n            for i in range(self.population_size):\n                # Social Foraging Movement\n                best_index = np.argmin(fitness)\n                \n                # Adaptive step size\n                step_size = (ub - lb) / (iteration + 1)\n                \n                # Exploration: Random walk\n                exploration = step_size * np.random.uniform(-1, 1, size=self.dim)\n\n                # Social Influence: Move towards the best solution\n                social_influence = self.beta * (population[best_index] - population[i])\n\n                # Archive-based mutation (Inspired by Differential Evolution)\n                if len(self.archive) > 1 and np.random.rand() < 0.3:  # Apply with a probability\n                    archive_idx = np.random.randint(len(self.archive))\n                    archive_vector = self.archive[archive_idx]\n                    mutation = 0.5 * (archive_vector - population[i]) # Simple mutation\n                else:\n                    mutation = 0\n\n                # Combined movement\n                new_position = population[i] + self.alpha * exploration + social_influence + mutation\n                \n                # Boundary Handling (clip to stay within bounds)\n                new_position = np.clip(new_position, lb, ub)\n\n                # Local Search around the new position\n                best_local_f = np.inf\n                best_local_x = new_position\n                for _ in range(self.local_search_iterations):\n                     if self.budget <= 0:\n                         break\n                     local_step = step_size * np.random.uniform(-0.1, 0.1, size=self.dim)\n                     local_x = new_position + local_step\n                     local_x = np.clip(local_x, lb, ub)\n                     local_f = func(local_x)\n                     self.budget -= 1\n                     if local_f < best_local_f:\n                         best_local_f = local_f\n                         best_local_x = local_x\n                \n                \n                #Evaluate new position with local search result\n                new_fitness = func(best_local_x) if self.budget > 0 else np.inf\n                if self.budget > 0:\n                    self.budget -=1\n                else:\n                    new_fitness = np.inf\n                    \n                \n                if new_fitness < fitness[i]:\n                    fitness[i] = new_fitness\n                    population[i] = best_local_x\n\n                    # Update archive\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(best_local_x)\n                    else:\n                        # Replace a random element in archive\n                        idx_to_replace = np.random.randint(self.archive_size)\n                        self.archive[idx_to_replace] = best_local_x\n                    \n                    if new_fitness < self.f_opt:\n                        self.f_opt = new_fitness\n                        self.x_opt = best_local_x\n            \n            # Restart mechanism\n            if self.budget <= self.budget * self.restart_trigger and self.restart_trigger > 0:\n                 # Reset population around the best solution found so far\n                 population = np.random.normal(self.x_opt, 0.5, size=(self.population_size, self.dim))\n                 population = np.clip(population, lb, ub)\n                 fitness = np.array([func(x) for x in population])\n                 self.budget -= self.population_size\n                 self.restart_trigger = 0 # Only restart once near the end\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "Evaluation timed out after 300 seconds.", "error": "Evaluation timed out after 300 seconds.", "parent_ids": ["848302de-9973-4dba-8916-b3898be42c35", "766720ab-b003-4396-a78b-9f26c9fe9ae6"], "operator": "crossover", "metadata": {"llm_response": "# Description: Social Foraging with Archive-Based Mutation and Late-Restart Mechanism\n# Code: \n```python\nimport numpy as np\n\nclass SocialForagingOptimizationPlus:\n    def __init__(self, budget=10000, dim=10, population_size=20, alpha=0.1, beta=1.0, archive_size=10, local_search_iterations=5, restart_trigger=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = population_size\n        self.alpha = alpha  # Exploration weight\n        self.beta = beta  # Social influence weight\n        self.archive_size = archive_size\n        self.archive = []\n        self.local_search_iterations = local_search_iterations\n        self.restart_trigger = restart_trigger\n\n    def __call__(self, func):\n        # Initialize population within the bounds.\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        population = np.random.uniform(lb, ub, size=(self.population_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.population_size\n        \n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index]\n\n        iteration = 0\n        while self.budget > 0:\n            iteration += 1\n            for i in range(self.population_size):\n                # Social Foraging Movement\n                best_index = np.argmin(fitness)\n                \n                # Adaptive step size\n                step_size = (ub - lb) / (iteration + 1)\n                \n                # Exploration: Random walk\n                exploration = step_size * np.random.uniform(-1, 1, size=self.dim)\n\n                # Social Influence: Move towards the best solution\n                social_influence = self.beta * (population[best_index] - population[i])\n\n                # Archive-based mutation (Inspired by Differential Evolution)\n                if len(self.archive) > 1 and np.random.rand() < 0.3:  # Apply with a probability\n                    archive_idx = np.random.randint(len(self.archive))\n                    archive_vector = self.archive[archive_idx]\n                    mutation = 0.5 * (archive_vector - population[i]) # Simple mutation\n                else:\n                    mutation = 0\n\n                # Combined movement\n                new_position = population[i] + self.alpha * exploration + social_influence + mutation\n                \n                # Boundary Handling (clip to stay within bounds)\n                new_position = np.clip(new_position, lb, ub)\n\n                # Local Search around the new position\n                best_local_f = np.inf\n                best_local_x = new_position\n                for _ in range(self.local_search_iterations):\n                     if self.budget <= 0:\n                         break\n                     local_step = step_size * np.random.uniform(-0.1, 0.1, size=self.dim)\n                     local_x = new_position + local_step\n                     local_x = np.clip(local_x, lb, ub)\n                     local_f = func(local_x)\n                     self.budget -= 1\n                     if local_f < best_local_f:\n                         best_local_f = local_f\n                         best_local_x = local_x\n                \n                \n                #Evaluate new position with local search result\n                new_fitness = func(best_local_x) if self.budget > 0 else np.inf\n                if self.budget > 0:\n                    self.budget -=1\n                else:\n                    new_fitness = np.inf\n                    \n                \n                if new_fitness < fitness[i]:\n                    fitness[i] = new_fitness\n                    population[i] = best_local_x\n\n                    # Update archive\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(best_local_x)\n                    else:\n                        # Replace a random element in archive\n                        idx_to_replace = np.random.randint(self.archive_size)\n                        self.archive[idx_to_replace] = best_local_x\n                    \n                    if new_fitness < self.f_opt:\n                        self.f_opt = new_fitness\n                        self.x_opt = best_local_x\n            \n            # Restart mechanism\n            if self.budget <= self.budget * self.restart_trigger and self.restart_trigger > 0:\n                 # Reset population around the best solution found so far\n                 population = np.random.normal(self.x_opt, 0.5, size=(self.population_size, self.dim))\n                 population = np.clip(population, lb, ub)\n                 fitness = np.array([func(x) for x in population])\n                 self.budget -= self.population_size\n                 self.restart_trigger = 0 # Only restart once near the end\n\n        return self.f_opt, self.x_opt\n```", "operator": "crossover", "theta_sampled": 0.017598028176734688, "generation": 1}, "task_prompt": ""}
{"id": "aa65eabf-7fc9-4313-8ea1-9688da9a6999", "fitness": 0.0, "name": "AdaptiveSearch", "description": "An adaptive search algorithm that balances exploration and exploitation using a combination of a global random search and a local gradient-based search, with adaptive probabilities for each search strategy.", "code": "import numpy as np\n\nclass AdaptiveSearch:\n    def __init__(self, budget=10000, dim=10, learning_rate=0.1, exploration_prob=0.5):\n        self.budget = budget\n        self.dim = dim\n        self.learning_rate = learning_rate\n        self.exploration_prob = exploration_prob\n        self.current_budget = 0\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        x = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        f = func(x)\n        self.current_budget += 1\n\n        if f < self.f_opt:\n            self.f_opt = f\n            self.x_opt = x\n\n        while self.current_budget < self.budget:\n            if np.random.rand() < self.exploration_prob:\n                # Global random search\n                x_new = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                f_new = func(x_new)\n                self.current_budget += 1\n\n                if f_new < self.f_opt:\n                    self.f_opt = f_new\n                    self.x_opt = x_new\n            else:\n                # Local gradient-based search (finite difference approximation)\n                gradient = np.zeros(self.dim)\n                delta = 1e-5\n                for i in range(self.dim):\n                    x_plus = x.copy()\n                    x_plus[i] += delta\n                    if x_plus[i] > func.bounds.ub[i % len(func.bounds.ub)]:\n                         x_plus[i] = func.bounds.ub[i % len(func.bounds.ub)]\n\n                    f_plus = func(x_plus)\n                    self.current_budget += 1\n                    if self.current_budget >= self.budget:\n                        break\n                    \n                    x_minus = x.copy()\n                    x_minus[i] -= delta\n                    if x_minus[i] < func.bounds.lb[i % len(func.bounds.lb)]:\n                         x_minus[i] = func.bounds.lb[i % len(func.bounds.lb)]\n                    \n                    f_minus = func(x_minus)\n                    self.current_budget += 1\n                    if self.current_budget >= self.budget:\n                        break\n\n                    gradient[i] = (f_plus - f_minus) / (2 * delta)\n\n\n                x_new = x - self.learning_rate * gradient\n                \n                # Clip the new x to respect bounds\n                x_new = np.clip(x_new, func.bounds.lb, func.bounds.ub)\n\n                f_new = func(x_new)\n                self.current_budget += 1\n\n                if f_new < self.f_opt:\n                    self.f_opt = f_new\n                    self.x_opt = x_new\n                    x = x_new # Move to the new best position after local search\n                \n\n            # Adapt exploration probability\n            if self.current_budget < self.budget:\n                 self.exploration_prob = 0.5 * (1 + np.cos(np.pi * self.current_budget / self.budget))\n            \n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "The algorithm AdaptiveSearch scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": "random_new", "metadata": {"llm_response": "# Description: An adaptive search algorithm that balances exploration and exploitation using a combination of a global random search and a local gradient-based search, with adaptive probabilities for each search strategy.\n# Code: \n```python\nimport numpy as np\n\nclass AdaptiveSearch:\n    def __init__(self, budget=10000, dim=10, learning_rate=0.1, exploration_prob=0.5):\n        self.budget = budget\n        self.dim = dim\n        self.learning_rate = learning_rate\n        self.exploration_prob = exploration_prob\n        self.current_budget = 0\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        x = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        f = func(x)\n        self.current_budget += 1\n\n        if f < self.f_opt:\n            self.f_opt = f\n            self.x_opt = x\n\n        while self.current_budget < self.budget:\n            if np.random.rand() < self.exploration_prob:\n                # Global random search\n                x_new = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                f_new = func(x_new)\n                self.current_budget += 1\n\n                if f_new < self.f_opt:\n                    self.f_opt = f_new\n                    self.x_opt = x_new\n            else:\n                # Local gradient-based search (finite difference approximation)\n                gradient = np.zeros(self.dim)\n                delta = 1e-5\n                for i in range(self.dim):\n                    x_plus = x.copy()\n                    x_plus[i] += delta\n                    if x_plus[i] > func.bounds.ub[i % len(func.bounds.ub)]:\n                         x_plus[i] = func.bounds.ub[i % len(func.bounds.ub)]\n\n                    f_plus = func(x_plus)\n                    self.current_budget += 1\n                    if self.current_budget >= self.budget:\n                        break\n                    \n                    x_minus = x.copy()\n                    x_minus[i] -= delta\n                    if x_minus[i] < func.bounds.lb[i % len(func.bounds.lb)]:\n                         x_minus[i] = func.bounds.lb[i % len(func.bounds.lb)]\n                    \n                    f_minus = func(x_minus)\n                    self.current_budget += 1\n                    if self.current_budget >= self.budget:\n                        break\n\n                    gradient[i] = (f_plus - f_minus) / (2 * delta)\n\n\n                x_new = x - self.learning_rate * gradient\n                \n                # Clip the new x to respect bounds\n                x_new = np.clip(x_new, func.bounds.lb, func.bounds.ub)\n\n                f_new = func(x_new)\n                self.current_budget += 1\n\n                if f_new < self.f_opt:\n                    self.f_opt = f_new\n                    self.x_opt = x_new\n                    x = x_new # Move to the new best position after local search\n                \n\n            # Adapt exploration probability\n            if self.current_budget < self.budget:\n                 self.exploration_prob = 0.5 * (1 + np.cos(np.pi * self.current_budget / self.budget))\n            \n\n        return self.f_opt, self.x_opt\n```", "operator": "random_new", "theta_sampled": 0.5792128901025653, "generation": 1, "aucs": [0]}, "task_prompt": ""}
