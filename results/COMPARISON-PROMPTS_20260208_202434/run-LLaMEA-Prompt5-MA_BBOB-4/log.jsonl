{"id": "94768647-f252-46d5-98c5-9ea8189ba67b", "fitness": 0.6995689417264203, "name": "SelfAdaptiveDE", "description": "A population-based algorithm that uses a combination of differential evolution mutation, crossover, and a self-adaptive learning rate for global exploration and local exploitation.", "code": "import numpy as np\n\nclass SelfAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, Cr=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F  # Mutation factor\n        self.Cr = Cr  # Crossover rate\n        self.learning_rate = 0.1  # Initial learning rate for parameter adaptation\n\n    def __call__(self, func):\n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size # Deduct initial population evaluations from budget\n\n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)]\n\n        generation = 0\n        while self.budget > 0:\n            generation += 1\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + self.F * (b - c), func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.Cr\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    population[i] = trial\n\n                    # Update optimal solution\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                        # Increase learning rate if improvement is found\n                        self.learning_rate = min(1.0, self.learning_rate * 1.1)\n                else:\n                    # Reduce learning rate if no improvement\n                    self.learning_rate = max(0.01, self.learning_rate * 0.9)\n\n            # Adapt parameters based on learning rate\n            self.F = np.clip(self.F + self.learning_rate * np.random.normal(0, 0.1), 0.1, 1.0)\n            self.Cr = np.clip(self.Cr + self.learning_rate * np.random.normal(0, 0.1), 0.1, 1.0)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm SelfAdaptiveDE scored 0.700 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.27667912590903176, 0.6615370270344121, 0.6770468513937116, 0.8922619903826364, 0.7417662119794398, 0.7903457799319364, 0.6519662313922344, 0.6609620278770837, 0.7472636036455731, 0.6782774156700625, 0.8619263440985384, 0.9979916552090675, 0.6542081120775955, 0.720508993136799, 0.9143521419479829, 0.7804410626719768, 0.6183104945439517, 0.8431765243177838, 0.30000015511524103, 0.5223570861933464]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "cd640a9c-3f3e-4c4b-a0a0-8a7f23a8c648", "fitness": 0.2973511696164294, "name": "AntColonyOptimization", "description": "A population-based algorithm inspired by the foraging behavior of ants, where solutions are probabilistically attracted to the best solution found so far and to promising regions based on evaluation history.", "code": "import numpy as np\n\nclass AntColonyOptimization:\n    def __init__(self, budget=10000, dim=10, colony_size=20, evaporation_rate=0.1, alpha=1, beta=2, initial_pheromone=1.0):\n        self.budget = budget\n        self.dim = dim\n        self.colony_size = colony_size\n        self.evaporation_rate = evaporation_rate\n        self.alpha = alpha  # Influence of pheromone\n        self.beta = beta    # Influence of heuristic information (fitness)\n        self.initial_pheromone = initial_pheromone\n        self.pheromone = None\n\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.func = func # Store the function for within the class to avoid passing it repeatedly\n\n        # Initialize ants randomly\n        ants = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.colony_size, self.dim))\n        fitness = np.array([func(ant) for ant in ants])\n\n        self.budget -= self.colony_size\n\n        best_index = np.argmin(fitness)\n        best_ant = ants[best_index]\n        best_fitness = fitness[best_index]\n        \n        if best_fitness < self.f_opt:\n            self.f_opt = best_fitness\n            self.x_opt = best_ant\n\n        # Initialize pheromone matrix\n        self.pheromone = np.full((self.colony_size, self.dim), self.initial_pheromone)\n        \n        while self.budget > 0:\n            # Calculate probabilities for each ant to move towards the best ant\n            probabilities = self.calculate_probabilities(ants, fitness)\n\n            # Move ants probabilistically, considering pheromone and heuristic information\n            new_ants = self.move_ants(ants, best_ant, probabilities)\n\n            # Evaluate new ant positions\n            new_fitness = np.array([func(ant) for ant in new_ants])\n            self.budget -= self.colony_size\n\n            # Update best solution\n            for i in range(self.colony_size):\n                if new_fitness[i] < best_fitness:\n                    best_fitness = new_fitness[i]\n                    best_ant = new_ants[i]\n\n                    if best_fitness < self.f_opt:\n                        self.f_opt = best_fitness\n                        self.x_opt = best_ant\n                        \n\n            # Update pheromone levels (evaporation and deposition)\n            self.update_pheromone(ants, fitness)\n\n            ants = new_ants\n            fitness = new_fitness\n\n        return self.f_opt, self.x_opt\n\n    def calculate_probabilities(self, ants, fitness):\n        \"\"\"Calculates the probability for each ant to move towards the best ant or explore\"\"\"\n        probabilities = np.zeros(self.colony_size)\n        for i in range(self.colony_size):\n            # Heuristic information: inverse of fitness (better fitness = higher heuristic value)\n            heuristic = 1 / (fitness[i] + 1e-8)  # Avoid division by zero\n            probabilities[i] = (self.pheromone[i, :].mean() ** self.alpha) * (heuristic ** self.beta)\n        probabilities /= probabilities.sum() # Normalize\n\n        return probabilities\n\n    def move_ants(self, ants, best_ant, probabilities):\n      \"\"\"Moves the ants stochastically towards the best ant, with some random exploration.\"\"\"\n      new_ants = np.zeros_like(ants)\n      for i in range(self.colony_size):\n          # Move towards the best ant with probability related to pheromone and heuristic\n          move_prob = probabilities[i]\n\n          # Exploration: With some probability, explore randomly\n          if np.random.rand() < move_prob:\n              # Move towards best_ant with some noise\n              direction = best_ant - ants[i]\n              step_size = np.random.uniform(0, 0.1)  # Adjust step size as needed\n              new_ants[i] = ants[i] + step_size * direction + np.random.normal(0, 0.01, self.dim)\n          else:\n              # Explore randomly\n              new_ants[i] = np.random.uniform(self.func.bounds.lb, self.func.bounds.ub, self.dim)\n          \n          # Ensure boundaries\n          new_ants[i] = np.clip(new_ants[i], self.func.bounds.lb, self.func.bounds.ub)\n\n      return new_ants\n      \n    def update_pheromone(self, ants, fitness):\n        \"\"\"Updates pheromone levels based on the fitness of the ants.\"\"\"\n        # Evaporation\n        self.pheromone *= (1 - self.evaporation_rate)\n\n        # Pheromone deposition: Deposit pheromone based on the quality of the solution\n        for i in range(self.colony_size):\n            delta_pheromone = 1 / (fitness[i] + 1e-8)  # Amount of pheromone deposited\n            self.pheromone[i, :] += delta_pheromone", "configspace": "", "generation": 0, "feedback": "The algorithm AntColonyOptimization scored 0.297 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.12869284448894447, 0.21096310378203376, 0.2720589171270166, 0.2177840517657036, 0.21380603626409178, 0.2699565113216118, 0.2453317472525277, 0.2235381778886254, 0.20799347493096154, 0.1623697316514051, 0.23523035897600342, 0.9998279957639723, 0.26061082670561153, 0.23310103640023827, 0.6050676264063222, 0.2930398372284173, 0.23597208097554156, 0.2959122723691505, 0.17103378862574825, 0.4647329724046608]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "b35ab084-3b8a-45aa-8934-17d8e4bc7196", "fitness": "-inf", "name": "OrthogonalCMAES", "description": "Population-based algorithm utilizing a covariance matrix adaptation strategy enhanced with orthogonal learning and a restart mechanism.", "code": "import numpy as np\n\nclass OrthogonalCMAES:\n    def __init__(self, budget=10000, dim=10, popsize=None, cs=0.3, damps=1, ccov1=0.05, ccovmu=0.05, cma_decay = 0.99):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = popsize if popsize is not None else 4 + int(3 * np.log(dim))\n        self.cs = cs\n        self.damps = damps\n        self.ccov1 = ccov1\n        self.ccovmu = ccovmu\n        self.cma_decay = cma_decay\n        self.mu = self.popsize // 2  # Number of parents/elite individuals\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.eval_count = 0\n\n        # Initialize CMA-ES parameters\n        mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        sigma = 0.5  # Step size\n        C = np.eye(self.dim)  # Covariance matrix\n\n        # Evolution path parameters\n        pc = np.zeros(self.dim)\n        ps = np.zeros(self.dim)\n        chiN = np.sqrt(self.dim) * (1 - (1 / (4 * self.dim)) + (1 / (21 * self.dim**2)))\n\n        # Weights for recombination\n        weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n        weights = weights / np.sum(weights)\n        mueff = np.sum(weights)**2 / np.sum(weights**2)\n\n        # Adaptation parameters\n        cc = (4 + mueff/self.dim) / (self.dim + 4 + 2*mueff/self.dim)\n        cs = (mueff+2) / (self.dim+mueff+5)\n        damps = 1 + 2*max(0, np.sqrt((mueff-1)/(self.dim+1))-1) + cs\n        ccov1 = self.ccov1 / ((self.dim+1.3)**2+mueff)\n        ccovmu = min(1-ccov1, self.ccovmu * mueff / ((self.dim+2)**2+mueff))\n\n        restart_count = 0\n        while self.eval_count < self.budget:\n            # Generate and evaluate offspring\n            Z = np.random.randn(self.dim, self.popsize)\n            x = mean[:, np.newaxis] + sigma * np.dot(np.linalg.cholesky(C), Z)  # Sample from multivariate normal\n\n            f = np.array([func(xi) for xi in x.T if self.eval_count < self.budget])\n            self.eval_count += len(f)\n\n            if len(f) == 0:\n                break\n\n            # Update best solution\n            idx_best = np.argmin(f)\n            if f[idx_best] < self.f_opt:\n                self.f_opt = f[idx_best]\n                self.x_opt = x[:, idx_best]\n\n            # Select parents\n            idx_sort = np.argsort(f)\n            x_sorted = x[:, idx_sort[:self.mu]]\n\n            # Recombination: new mean\n            mean_new = np.dot(x_sorted, weights)\n\n            # Update evolution paths\n            ps = (1 - cs) * ps + np.sqrt(cs * (2 - cs) * mueff) * np.dot(np.linalg.inv(np.linalg.cholesky(C)), (mean_new - mean)) / sigma\n            hsig = np.linalg.norm(ps) / np.sqrt(1 - (1 - cs)**(2 * self.eval_count / self.popsize)) / chiN < 1.4 + 2 / (self.dim + 1)\n            pc = (1 - cc) * pc + hsig * np.sqrt(cc * (2 - cc) * mueff) * (mean_new - mean) / sigma\n\n            # Update covariance matrix\n            artmp = (1 / sigma) * (x_sorted - mean)\n            C = (1 - ccov1 - ccovmu + ccov1 * cc * (2 - cc)) * C + ccov1 * np.outer(pc, pc) + ccovmu * np.dot(artmp, np.diag(weights).dot(artmp.T))\n\n            # Ensure positive definiteness\n            C = np.triu(C) + np.triu(C, 1).T\n            C = np.real((C + C.T) / 2)\n\n            # Update step size\n            sigma = sigma * np.exp((cs / self.damps) * (np.linalg.norm(ps) / chiN - 1))\n            mean = mean_new\n\n            sigma *= self.cma_decay\n            # Restart mechanism (optional)\n            if np.linalg.cond(C) > 1e14 or np.isnan(C).any():\n                restart_count +=1\n                mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                sigma = 0.5\n                C = np.eye(self.dim)\n                pc = np.zeros(self.dim)\n                ps = np.zeros(self.dim)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "In the code, line 73, in __call__, the following error occurred:\nValueError: operands could not be broadcast together with shapes (2,3) (2,) \nOn line: artmp = (1 / sigma) * (x_sorted - mean)", "error": "In the code, line 73, in __call__, the following error occurred:\nValueError: operands could not be broadcast together with shapes (2,3) (2,) \nOn line: artmp = (1 / sigma) * (x_sorted - mean)", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "76eeee7e-2f52-45b1-9d9e-c4eef0373db8", "fitness": "-inf", "name": "AdaptiveSlimeMoldAlgorithm", "description": "A population-based algorithm inspired by the foraging behavior of slime mold, incorporating adaptive parameter control and a local search component to balance exploration and exploitation.", "code": "import numpy as np\n\nclass AdaptiveSlimeMoldAlgorithm:\n    def __init__(self, budget=10000, dim=10, pop_size=25):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lb = -5.0\n        self.ub = 5.0\n        self.positions = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.full(self.pop_size, np.inf)\n        self.best_fitness = np.inf\n        self.best_position = None\n        self.eval_count = 0\n\n        self.w = 0.8 # inertia weight\n\n    def __call__(self, func):\n        while self.eval_count < self.budget:\n            # Evaluate fitness\n            for i in range(self.pop_size):\n                if self.eval_count < self.budget:\n                    if self.fitness[i] == np.inf:\n                        self.fitness[i] = func(self.positions[i])\n                        self.eval_count += 1\n                    \n                    if self.fitness[i] < self.best_fitness:\n                        self.best_fitness = self.fitness[i]\n                        self.best_position = self.positions[i].copy()\n            \n            # Update weights adaptively\n            if self.eval_count > self.budget * 0.75: # start exploitation\n                self.w = 0.4 + (0.8 - 0.4) * (self.budget - self.eval_count) / (self.budget * 0.25)\n            else:\n                self.w = 0.8\n\n            # Calculate selection probability (smell)\n            fit_norm = np.exp(-np.abs(self.fitness - self.best_fitness))  # Normalizing fitness to [0, 1]\n            S = fit_norm / np.sum(fit_norm)\n            \n            # Update positions\n            for i in range(self.pop_size):\n                a = np.random.choice(self.pop_size, p=S)\n                b = np.random.choice(self.pop_size, p=S)\n\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                A = np.linalg.norm(self.positions[a] - self.positions[b])\n                C = np.random.uniform(-1, 1, self.dim) #oscillation range\n                \n                if np.random.rand() < 0.5:\n                    self.positions[i] = self.w * (self.best_position + A * (np.abs(2 * r1 * self.positions[a] - self.positions[b])))\n                else:\n                    self.positions[i] = self.w * (self.best_position - A * (np.abs(2 * r1 * self.positions[a] - self.positions[b])))\n\n                # Boundary handling\n                self.positions[i] = np.clip(self.positions[i], self.lb, self.ub)\n                \n            # Local search: Random perturbation around the best solution\n            if self.eval_count < self.budget:\n                perturbation = np.random.normal(0, 0.05, self.dim)  # small perturbation\n                new_position = self.best_position + perturbation\n                new_position = np.clip(new_position, self.lb, self.ub)\n\n                new_fitness = func(new_position)\n                self.eval_count += 1\n\n                if new_fitness < self.best_fitness:\n                    self.best_fitness = new_fitness\n                    self.best_position = new_position.copy()\n\n        return self.best_fitness, self.best_position", "configspace": "", "generation": 0, "feedback": "In the code, line 995, in numpy.random.mtrand.RandomState.choice, the following error occurred:\nValueError: probabilities contain NaN", "error": "In the code, line 995, in numpy.random.mtrand.RandomState.choice, the following error occurred:\nValueError: probabilities contain NaN", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "3e1d384b-bd64-4e40-863e-d06b94ab7771", "fitness": "-inf", "name": "AdaptiveMomentumGradientDescent", "description": "A gradient-based method with adaptive step size and momentum, incorporating a population of solutions for exploration and a restart mechanism.", "code": "import numpy as np\n\nclass AdaptiveMomentumGradientDescent:\n    def __init__(self, budget=10000, dim=10, popsize=5, learning_rate=0.1, momentum=0.9, adaptive_lr_decay=0.99, restart_prob=0.05):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = popsize\n        self.learning_rate = learning_rate\n        self.momentum = momentum\n        self.adaptive_lr_decay = adaptive_lr_decay\n        self.restart_prob = restart_prob\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.eval_count = 0\n\n        # Initialize a population of solutions\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.popsize, self.dim))\n        velocities = np.zeros_like(population)  # Initialize velocities for momentum\n\n        while self.eval_count < self.budget:\n            # Evaluate the population\n            fitness = np.array([func(x) for x in population if self.eval_count < self.budget])\n            self.eval_count += len(fitness)\n\n            if len(fitness) == 0:\n                break\n\n            # Update best solution\n            best_idx = np.argmin(fitness)\n            if fitness[best_idx] < self.f_opt:\n                self.f_opt = fitness[best_idx]\n                self.x_opt = population[best_idx]\n\n            # Calculate gradients for each individual in the population (Approximation)\n            gradients = np.zeros_like(population)\n            for i in range(self.popsize):\n                if i < len(fitness):  # handle cases where the budget is almost exhausted\n                    epsilon = 1e-5  # Small perturbation for gradient approximation\n                    for j in range(self.dim):\n                        population_plus = population[i].copy()\n                        population_minus = population[i].copy()\n                        population_plus[j] += epsilon\n                        population_minus[j] -= epsilon\n\n                        # Ensure bounds are respected\n                        population_plus[j] = np.clip(population_plus[j], func.bounds.lb, func.bounds.ub)\n                        population_minus[j] = np.clip(population_minus[j], func.bounds.lb, func.bounds.ub)\n\n                        fitness_plus = func(population_plus) if self.eval_count < self.budget else np.inf\n                        self.eval_count += 1 if self.eval_count < self.budget else 0\n                        fitness_minus = func(population_minus) if self.eval_count < self.budget else np.inf\n                        self.eval_count += 1 if self.eval_count < self.budget else 0\n\n                        gradients[i, j] = (fitness_plus - fitness_minus) / (2 * epsilon)\n\n            # Update population using momentum gradient descent\n            for i in range(self.popsize):\n                if i < len(fitness):  #handle cases where the budget is almost exhausted\n                    velocities[i] = self.momentum * velocities[i] - self.learning_rate * gradients[i]\n                    population[i] += velocities[i]\n\n                    # Clip to bounds\n                    population[i] = np.clip(population[i], func.bounds.lb, func.bounds.ub)\n\n            # Adaptive learning rate decay\n            self.learning_rate *= self.adaptive_lr_decay\n\n            # Restart mechanism\n            for i in range(self.popsize):\n                 if np.random.rand() < self.restart_prob:\n                     population[i] = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                     velocities[i] = 0  # Reset velocity upon restart\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "In the code, line 48, in __call__, the following error occurred:\nValueError: setting an array element with a sequence.\nOn line: population_plus[j] = np.clip(population_plus[j], func.bounds.lb, func.bounds.ub)", "error": "In the code, line 48, in __call__, the following error occurred:\nValueError: setting an array element with a sequence.\nOn line: population_plus[j] = np.clip(population_plus[j], func.bounds.lb, func.bounds.ub)", "parent_ids": ["b35ab084-3b8a-45aa-8934-17d8e4bc7196"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "640d5617-bca9-4812-987e-6d6b4664faec", "fitness": 0.6480635157797097, "name": "OrthogonalCMAES", "description": "A population-based optimization algorithm combining Covariance Matrix Adaptation (CMA-ES) with orthogonal sampling and a self-adjusting restart strategy based on stagnation detection to enhance exploration and exploitation.", "code": "import numpy as np\n\nclass OrthogonalCMAES:\n    def __init__(self, budget=10000, dim=10, popsize=None, cs=0.3, damps=1, ccov1=0.05, ccovmu=0.05, cma_decay = 0.99, stagnation_threshold=100):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = popsize if popsize is not None else 4 + int(3 * np.log(dim))\n        self.cs = cs\n        self.damps = damps\n        self.ccov1 = ccov1\n        self.ccovmu = ccovmu\n        self.cma_decay = cma_decay\n        self.mu = self.popsize // 2  # Number of parents/elite individuals\n        self.stagnation_threshold = stagnation_threshold\n        self.best_fitness_history = []\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.eval_count = 0\n\n        # Initialize CMA-ES parameters\n        mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        sigma = 0.5  # Step size\n        C = np.eye(self.dim)  # Covariance matrix\n\n        # Evolution path parameters\n        pc = np.zeros(self.dim)\n        ps = np.zeros(self.dim)\n        chiN = np.sqrt(self.dim) * (1 - (1 / (4 * self.dim)) + (1 / (21 * self.dim**2)))\n\n        # Weights for recombination\n        weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n        weights = weights / np.sum(weights)\n        mueff = np.sum(weights)**2 / np.sum(weights**2)\n\n        # Adaptation parameters\n        cc = (4 + mueff/self.dim) / (self.dim + 4 + 2*mueff/self.dim)\n        cs = (mueff+2) / (self.dim+mueff+5)\n        damps = 1 + 2*max(0, np.sqrt((mueff-1)/(self.dim+1))-1) + cs\n        ccov1 = self.ccov1 / ((self.dim+1.3)**2+mueff)\n        ccovmu = min(1-ccov1, self.ccovmu * mueff / ((self.dim+2)**2+mueff))\n\n        restart_count = 0\n        stagnation_counter = 0\n\n        while self.eval_count < self.budget:\n            # Generate and evaluate offspring\n            Z = np.random.randn(self.dim, self.popsize)\n            x = mean[:, np.newaxis] + sigma * np.dot(np.linalg.cholesky(C), Z)  # Sample from multivariate normal\n\n            f = np.array([func(xi) for xi in x.T if self.eval_count < self.budget])\n            self.eval_count += len(f)\n\n            if len(f) == 0:\n                break\n\n            # Update best solution\n            idx_best = np.argmin(f)\n            if f[idx_best] < self.f_opt:\n                self.f_opt = f[idx_best]\n                self.x_opt = x[:, idx_best]\n                stagnation_counter = 0 # Reset stagnation counter if improvement found\n            else:\n                stagnation_counter += 1\n\n            # Select parents\n            idx_sort = np.argsort(f)\n            x_sorted = x[:, idx_sort[:self.mu]]\n\n            # Recombination: new mean\n            mean_new = np.dot(x_sorted, weights)\n\n            # Update evolution paths\n            ps = (1 - cs) * ps + np.sqrt(cs * (2 - cs) * mueff) * np.dot(np.linalg.inv(np.linalg.cholesky(C)), (mean_new - mean)) / sigma\n            hsig = np.linalg.norm(ps) / np.sqrt(1 - (1 - cs)**(2 * self.eval_count / self.popsize)) / chiN < 1.4 + 2 / (self.dim + 1)\n            pc = (1 - cc) * pc + hsig * np.sqrt(cc * (2 - cc) * mueff) * (mean_new - mean) / sigma\n\n            # Update covariance matrix\n            artmp = (1 / sigma) * (x_sorted - mean[:, np.newaxis])\n            C = (1 - ccov1 - ccovmu + ccov1 * cc * (2 - cc)) * C + ccov1 * np.outer(pc, pc) + ccovmu * np.dot(artmp, np.diag(weights).dot(artmp.T))\n\n            # Ensure positive definiteness\n            C = np.triu(C) + np.triu(C, 1).T\n            C = np.real((C + C.T) / 2)\n\n            # Update step size\n            sigma = sigma * np.exp((cs / self.damps) * (np.linalg.norm(ps) / chiN - 1))\n            mean = mean_new\n\n            sigma *= self.cma_decay\n\n            # Stagnation Detection and Restart Mechanism\n            if stagnation_counter > self.stagnation_threshold:\n                restart_count += 1\n                mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                sigma = 0.5\n                C = np.eye(self.dim)\n                pc = np.zeros(self.dim)\n                ps = np.zeros(self.dim)\n                stagnation_counter = 0  # Reset stagnation counter\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "The algorithm OrthogonalCMAES scored 0.648 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["b35ab084-3b8a-45aa-8934-17d8e4bc7196"], "operator": null, "metadata": {"aucs": [0.19992408574986087, 0.8891081623292058, 0.939726151800892, 0.49856711552300936, 0.9427151263638835, 0.9715722127421258, 0.3249392962176155, 0.9345456260480994, 0.9498005330215645, 0.16122477290173043, 0.8931893276329074, 0.9933604207583269, 0.2861586502561181, 0.41933771605340964, 0.7897920891742906, 0.3308264240282599, 0.7867698845066702, 0.9649726460189259, 0.18050482719695027, 0.5042352472703484]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "80b0c4fa-28fd-42e3-88a8-0a45c391a47f", "fitness": 0.31945445919110826, "name": "AdaptiveSlimeMoldAlgorithm", "description": "A simplified Slime Mold Algorithm with adaptive weight and local search enhancements to balance exploration and exploitation effectively.", "code": "import numpy as np\n\nclass AdaptiveSlimeMoldAlgorithm:\n    def __init__(self, budget=10000, dim=10, pop_size=25):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lb = -5.0\n        self.ub = 5.0\n        self.positions = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.full(self.pop_size, np.inf)\n        self.best_fitness = np.inf\n        self.best_position = None\n        self.eval_count = 0\n        self.w = 0.8\n\n    def __call__(self, func):\n        while self.eval_count < self.budget:\n            # Evaluate fitness\n            for i in range(self.pop_size):\n                if self.eval_count < self.budget:\n                    if self.fitness[i] == np.inf:\n                        self.fitness[i] = func(self.positions[i])\n                        self.eval_count += 1\n\n                    if self.fitness[i] < self.best_fitness:\n                        self.best_fitness = self.fitness[i]\n                        self.best_position = self.positions[i].copy()\n\n            # Adaptive weight update\n            if self.eval_count > self.budget * 0.75:\n                self.w = 0.4 + (0.8 - 0.4) * (self.budget - self.eval_count) / (self.budget * 0.25)\n            else:\n                self.w = 0.8\n\n            # Calculate selection probability\n            fit_norm = np.abs(self.fitness - self.best_fitness)\n            fit_norm = np.exp(-fit_norm / (np.mean(fit_norm) + 1e-8)) # Avoid division by zero\n            S = fit_norm / np.sum(fit_norm)\n\n            # Check for NaN values in S and correct them.\n            if np.any(np.isnan(S)):\n                S = np.nan_to_num(S, nan=1.0 / self.pop_size)\n                S /= np.sum(S)\n\n            # Update positions\n            for i in range(self.pop_size):\n                a = np.random.choice(self.pop_size, p=S)\n                b = np.random.choice(self.pop_size, p=S)\n\n                r1 = np.random.rand(self.dim)\n                A = np.linalg.norm(self.positions[a] - self.positions[b])\n                C = np.random.uniform(-1, 1, self.dim)\n\n                if np.random.rand() < 0.5:\n                    self.positions[i] = self.w * (self.best_position + A * (np.abs(2 * r1 * self.positions[a] - self.positions[b])))\n                else:\n                    self.positions[i] = self.w * (self.best_position - A * (np.abs(2 * r1 * self.positions[a] - self.positions[b])))\n\n                # Boundary handling\n                self.positions[i] = np.clip(self.positions[i], self.lb, self.ub)\n\n            # Local search: Perturb the best solution\n            if self.eval_count < self.budget:\n                perturbation = np.random.normal(0, 0.05, self.dim)\n                new_position = self.best_position + perturbation\n                new_position = np.clip(new_position, self.lb, self.ub)\n\n                new_fitness = func(new_position)\n                self.eval_count += 1\n\n                if new_fitness < self.best_fitness:\n                    self.best_fitness = new_fitness\n                    self.best_position = new_position.copy()\n\n        return self.best_fitness, self.best_position", "configspace": "", "generation": 1, "feedback": "The algorithm AdaptiveSlimeMoldAlgorithm scored 0.319 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["76eeee7e-2f52-45b1-9d9e-c4eef0373db8"], "operator": null, "metadata": {"aucs": [0.0911596155454758, 0.201262430825214, 0.28440662978520137, 0.9672766805532351, 0.13677717579980142, 0.17781822005525139, 0.22380244234205804, 0.16041561619389777, 0.18016157403406996, 0.15314607806138558, 0.9612444351599643, 0.22183656719008416, 0.23436118520210614, 0.17265669344881063, 0.9079379027737069, 0.30933165880736835, 0.20275585745631108, 0.22502351537809495, 0.15939509243688676, 0.4183198127732414]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "9449cc64-1123-49f1-9eab-57556b662920", "fitness": 0.39761381760965464, "name": "ChargedParticleOptimization", "description": "A population-based algorithm inspired by charged particles interacting under electrostatic forces, combined with velocity updates based on the best solutions found so far.", "code": "import numpy as np\n\nclass ChargedParticleOptimization:\n    def __init__(self, budget=10000, dim=10, population_size=20, charge_decay_rate=0.95, \n                 velocity_damping=0.8, social_influence=0.5):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = population_size\n        self.charge_decay_rate = charge_decay_rate\n        self.velocity_damping = velocity_damping\n        self.social_influence = social_influence\n\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.func = func\n\n        # Initialize particles randomly\n        particles = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.population_size, self.dim))\n        fitness = np.array([func(particle) for particle in particles])\n        self.budget -= self.population_size\n\n        best_index = np.argmin(fitness)\n        best_particle = particles[best_index]\n        best_fitness = fitness[best_index]\n        \n        if best_fitness < self.f_opt:\n            self.f_opt = best_fitness\n            self.x_opt = best_particle\n\n        # Initialize velocities\n        velocities = np.zeros((self.population_size, self.dim))\n\n        # Initialize charges (proportional to fitness, but scaled)\n        charges = self.calculate_charges(fitness)\n\n        while self.budget > 0:\n            # Calculate electrostatic forces between particles\n            forces = self.calculate_forces(particles, charges)\n\n            # Update velocities based on forces and best solution\n            velocities = self.update_velocities(velocities, forces, particles, best_particle)\n\n            # Move particles\n            new_particles = self.move_particles(particles, velocities)\n\n            # Evaluate new particle positions\n            new_fitness = np.array([func(particle) for particle in new_particles])\n            self.budget -= self.population_size\n\n            # Update best solution\n            for i in range(self.population_size):\n                if new_fitness[i] < best_fitness:\n                    best_fitness = new_fitness[i]\n                    best_particle = new_particles[i]\n                    if best_fitness < self.f_opt:\n                        self.f_opt = best_fitness\n                        self.x_opt = best_particle\n\n            # Update charges based on new fitness\n            charges = self.calculate_charges(new_fitness)\n\n            # Decay charges\n            charges *= self.charge_decay_rate\n\n            particles = new_particles\n            fitness = new_fitness\n\n        return self.f_opt, self.x_opt\n\n    def calculate_charges(self, fitness):\n        \"\"\"Calculates charges based on fitness values.\"\"\"\n        # Scale fitness to [0, 1] range\n        best_fitness = np.min(fitness)\n        worst_fitness = np.max(fitness)\n        scaled_fitness = (fitness - best_fitness) / (worst_fitness - best_fitness + 1e-8)\n        \n        # Invert and normalize to get charges (better fitness = higher charge)\n        charges = 1 - scaled_fitness\n        charges /= np.sum(charges) # Normalize\n        return charges\n\n    def calculate_forces(self, particles, charges):\n        \"\"\"Calculates electrostatic forces between particles.\"\"\"\n        forces = np.zeros((self.population_size, self.dim))\n        for i in range(self.population_size):\n            for j in range(self.population_size):\n                if i != j:\n                    # Calculate distance between particles\n                    distance = np.linalg.norm(particles[j] - particles[i]) + 1e-8  # Avoid division by zero\n                    # Calculate force magnitude (Coulomb's law)\n                    force_magnitude = (charges[i] * charges[j]) / (distance**2)\n                    # Calculate force direction\n                    force_direction = (particles[j] - particles[i]) / distance\n                    # Accumulate force\n                    forces[i] += force_magnitude * force_direction\n        return forces\n\n    def update_velocities(self, velocities, forces, particles, best_particle):\n        \"\"\"Updates particle velocities based on forces and attraction to the best solution.\"\"\"\n        new_velocities = self.velocity_damping * velocities + forces + \\\n                         self.social_influence * np.random.rand() * (best_particle - particles)\n        return new_velocities\n\n    def move_particles(self, particles, velocities):\n        \"\"\"Moves particles based on their velocities, ensuring they stay within bounds.\"\"\"\n        new_particles = particles + velocities\n        new_particles = np.clip(new_particles, self.func.bounds.lb, self.func.bounds.ub)\n        return new_particles", "configspace": "", "generation": 1, "feedback": "The algorithm ChargedParticleOptimization scored 0.398 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["cd640a9c-3f3e-4c4b-a0a0-8a7f23a8c648"], "operator": null, "metadata": {"aucs": [0.15912849878338908, 0.19943055410897825, 0.42615297571646293, 0.19829456169592918, 0.33795465536843383, 0.2428894428623365, 0.29928425438968786, 0.3783973524026374, 0.40094329264021755, 0.1930109097933811, 0.8855207821391897, 0.9949004928151683, 0.25128375628844746, 0.2183491969389122, 0.7182744526639994, 0.33168595264465406, 0.3960537656714057, 0.5665853358625326, 0.2593498668830103, 0.4947862525243204]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "ffc1a63a-6473-462c-8681-21c5254f3693", "fitness": 0.25431903793034516, "name": "AntColonyOptimization", "description": "Ant colony optimization with adaptive pheromone update and dynamic step size, focusing on balanced exploration and exploitation around the best solution.", "code": "import numpy as np\n\nclass AntColonyOptimization:\n    def __init__(self, budget=10000, dim=10, colony_size=20, evaporation_rate=0.1, alpha=1, beta=2, initial_pheromone=1.0, q=1.0):\n        self.budget = budget\n        self.dim = dim\n        self.colony_size = colony_size\n        self.evaporation_rate = evaporation_rate\n        self.alpha = alpha  # Influence of pheromone\n        self.beta = beta    # Influence of heuristic information (fitness)\n        self.initial_pheromone = initial_pheromone\n        self.pheromone = None\n        self.q = q #Pheromone intensity constant\n\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.func = func # Store the function for within the class to avoid passing it repeatedly\n\n        # Initialize ants randomly\n        ants = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.colony_size, self.dim))\n        fitness = np.array([func(ant) for ant in ants])\n\n        self.budget -= self.colony_size\n\n        best_index = np.argmin(fitness)\n        best_ant = ants[best_index]\n        best_fitness = fitness[best_index]\n        \n        if best_fitness < self.f_opt:\n            self.f_opt = best_fitness\n            self.x_opt = best_ant\n\n        # Initialize pheromone matrix\n        self.pheromone = np.full((self.colony_size, self.dim), self.initial_pheromone)\n        \n        iteration = 0\n        while self.budget > 0:\n            iteration += 1\n            # Calculate probabilities for each ant to move towards the best ant\n            probabilities = self.calculate_probabilities(ants, fitness)\n\n            # Move ants probabilistically, considering pheromone and heuristic information\n            new_ants = self.move_ants(ants, best_ant, probabilities, iteration)\n\n            # Evaluate new ant positions\n            new_fitness = np.array([func(ant) for ant in new_ants])\n            self.budget -= self.colony_size\n\n            # Update best solution\n            for i in range(self.colony_size):\n                if new_fitness[i] < best_fitness:\n                    best_fitness = new_fitness[i]\n                    best_ant = new_ants[i]\n\n                    if best_fitness < self.f_opt:\n                        self.f_opt = best_fitness\n                        self.x_opt = best_ant\n                        \n\n            # Update pheromone levels (evaporation and deposition)\n            self.update_pheromone(ants, fitness, best_fitness)\n\n            ants = new_ants\n            fitness = new_fitness\n\n        return self.f_opt, self.x_opt\n\n    def calculate_probabilities(self, ants, fitness):\n        \"\"\"Calculates the probability for each ant to move towards the best ant or explore\"\"\"\n        probabilities = np.zeros(self.colony_size)\n        for i in range(self.colony_size):\n            # Heuristic information: inverse of fitness (better fitness = higher heuristic value)\n            heuristic = (1 / (fitness[i] - self.f_opt + 1e-8))  # Avoid division by zero and consider distance to best\n            probabilities[i] = (np.prod(self.pheromone[i, :] ** self.alpha)) * (heuristic ** self.beta)\n\n        probabilities /= probabilities.sum()  # Normalize\n\n        return probabilities\n\n    def move_ants(self, ants, best_ant, probabilities, iteration):\n      \"\"\"Moves the ants stochastically towards the best ant, with some random exploration.\"\"\"\n      new_ants = np.zeros_like(ants)\n      step_size = 0.5 * np.exp(-iteration/100) # Dynamic Step Size\n      for i in range(self.colony_size):\n          # Move towards the best ant with probability related to pheromone and heuristic\n          move_prob = probabilities[i]\n\n          # Exploration: With some probability, explore randomly\n          if np.random.rand() < move_prob:\n              # Move towards best_ant with some noise\n              direction = best_ant - ants[i]\n              new_ants[i] = ants[i] + step_size * direction + np.random.normal(0, step_size/10, self.dim) # smaller noise\n          else:\n              # Explore randomly\n              new_ants[i] = ants[i] + np.random.uniform(-step_size, step_size, self.dim)\n          \n          # Ensure boundaries\n          new_ants[i] = np.clip(new_ants[i], self.func.bounds.lb, self.func.bounds.ub)\n\n      return new_ants\n      \n    def update_pheromone(self, ants, fitness, best_fitness):\n        \"\"\"Updates pheromone levels based on the fitness of the ants.\"\"\"\n        # Evaporation\n        self.pheromone *= (1 - self.evaporation_rate)\n\n        # Pheromone deposition: Deposit pheromone based on the quality of the solution\n        for i in range(self.colony_size):\n            delta_pheromone = self.q / (abs(fitness[i] - best_fitness) + 1e-8)  # Amount of pheromone deposited, related to best_fitness\n            self.pheromone[i, :] += delta_pheromone", "configspace": "", "generation": 1, "feedback": "The algorithm AntColonyOptimization scored 0.254 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["cd640a9c-3f3e-4c4b-a0a0-8a7f23a8c648"], "operator": null, "metadata": {"aucs": [0.11421252004358262, 0.2284563540273501, 0.2685238175986626, 0.16696223742810068, 0.14566595593169218, 0.1841785272274068, 0.17484261351135533, 0.19952269984173276, 0.16349447234394865, 0.145906578596534, 0.37574756455997527, 0.9451062318961194, 0.2858214732895271, 0.21730014280552035, 0.1636797381534736, 0.2697900454136526, 0.22087014806964622, 0.19422004664239279, 0.18376762132076352, 0.43831196990546706]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "e99c4a60-f2a4-46df-a894-045376522ec2", "fitness": 0.0, "name": "AdaptiveMutationDE", "description": "A differential evolution variant that dynamically adjusts its mutation strategy based on the success rate of different mutation operators, enhancing exploration and exploitation.", "code": "import numpy as np\n\nclass AdaptiveMutationDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, Cr=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F  # Mutation factor\n        self.Cr = Cr  # Crossover rate\n        self.mutation_strategies = [\"rand1\", \"best1\", \"current_to_rand1\", \"current_to_best1\"]\n        self.success_counts = {strategy: 1 for strategy in self.mutation_strategies}  # Initialize success counts to 1 to avoid division by zero\n        self.strategy_weights = {strategy: 1 / len(self.mutation_strategies) for strategy in self.mutation_strategies} # Initialize weights equally\n\n    def mutate(self, population, i, func, strategy):\n        idxs = [idx for idx in range(self.pop_size) if idx != i]\n        a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n        \n        if strategy == \"rand1\":\n            mutant = np.clip(a + self.F * (b - c), func.bounds.lb, func.bounds.ub)\n        elif strategy == \"best1\":\n            best_idx = np.argmin([func(x) for x in population]) # Find best using evaluations\n            best = population[best_idx]\n            mutant = np.clip(best + self.F * (b - c), func.bounds.lb, func.bounds.ub)\n        elif strategy == \"current_to_rand1\":\n            mutant = np.clip(population[i] + self.F * (a - population[i]) + self.F * (b - c), func.bounds.lb, func.bounds.ub)\n        elif strategy == \"current_to_best1\":\n            best_idx = np.argmin([func(x) for x in population]) # Find best using evaluations\n            best = population[best_idx]\n            mutant = np.clip(population[i] + self.F * (best - population[i]) + self.F * (b - c), func.bounds.lb, func.bounds.ub)\n        else:\n            raise ValueError(\"Invalid mutation strategy\")\n        return mutant\n\n    def __call__(self, func):\n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size  # Deduct initial population evaluations from budget\n\n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)]\n\n        generation = 0\n        while self.budget > 0:\n            generation += 1\n            for i in range(self.pop_size):\n                # Select mutation strategy based on weights\n                strategy = np.random.choice(self.mutation_strategies, p=list(self.strategy_weights.values()))\n\n                # Mutation\n                mutant = self.mutate(population, i, func, strategy)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.Cr\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    population[i] = trial\n                    self.success_counts[strategy] += 1\n\n                    # Update optimal solution\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                \n            # Update strategy weights based on success counts\n            total_success = sum(self.success_counts.values())\n            for strategy in self.mutation_strategies:\n                self.strategy_weights[strategy] = self.success_counts[strategy] / total_success\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "The algorithm AdaptiveMutationDE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["94768647-f252-46d5-98c5-9ea8189ba67b"], "operator": null, "metadata": {"aucs": [0]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "28a0cd49-3dce-44ff-8b67-87123e8b0760", "fitness": "-inf", "name": "HybridPSO", "description": "A hybrid metaheuristic combining particle swarm optimization with a simplex method for enhanced exploration and exploitation.", "code": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridPSO:\n    def __init__(self, budget=10000, dim=10, pop_size=25, inertia=0.7, c1=1.5, c2=1.5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.inertia = inertia\n        self.c1 = c1\n        self.c2 = c2\n        self.lb = -5.0\n        self.ub = 5.0\n        self.positions = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, size=(self.pop_size, self.dim))\n        self.fitness = np.full(self.pop_size, np.inf)\n        self.personal_best_positions = self.positions.copy()\n        self.personal_best_fitness = np.full(self.pop_size, np.inf)\n        self.global_best_fitness = np.inf\n        self.global_best_position = None\n        self.eval_count = 0\n\n    def __call__(self, func):\n        while self.eval_count < self.budget:\n            # Evaluate fitness\n            for i in range(self.pop_size):\n                if self.eval_count < self.budget:\n                    if self.fitness[i] == np.inf:\n                        self.fitness[i] = func(self.positions[i])\n                        self.eval_count += 1\n\n                    if self.fitness[i] < self.personal_best_fitness[i]:\n                        self.personal_best_fitness[i] = self.fitness[i]\n                        self.personal_best_positions[i] = self.positions[i].copy()\n\n                    if self.fitness[i] < self.global_best_fitness:\n                        self.global_best_fitness = self.fitness[i]\n                        self.global_best_position = self.positions[i].copy()\n\n            # Update velocities and positions\n            for i in range(self.pop_size):\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n\n                self.velocities[i] = (self.inertia * self.velocities[i] +\n                                     self.c1 * r1 * (self.personal_best_positions[i] - self.positions[i]) +\n                                     self.c2 * r2 * (self.global_best_position - self.positions[i]))\n\n                self.positions[i] = self.positions[i] + self.velocities[i]\n\n                # Boundary handling\n                self.positions[i] = np.clip(self.positions[i], self.lb, self.ub)\n            \n            # Apply simplex method to the best particle after a certain number of evaluations\n            if self.eval_count > self.budget * 0.5 and self.eval_count < self.budget:\n                \n                # Define the local optimization function with bounds\n                def local_optimization_function(x):\n                    if self.eval_count < self.budget:\n                        fitness = func(x)\n                        self.eval_count += 1\n                        return fitness\n                    else:\n                        return np.inf\n\n                bounds = [(self.lb, self.ub)] * self.dim\n                \n                # Run the simplex method with the best particle as the initial guess\n                result = minimize(local_optimization_function, self.global_best_position, method='Nelder-Mead', bounds=bounds, options={'maxfev': max(1, int((self.budget - self.eval_count)/5))})  # Reduce maxfev\n\n                if result.success:\n                    if result.fun < self.global_best_fitness:\n                        self.global_best_fitness = result.fun\n                        self.global_best_position = result.x.copy()\n\n        return self.global_best_fitness, self.global_best_position", "configspace": "", "generation": 1, "feedback": "Evaluation timed out after 300 seconds.", "error": "Evaluation timed out after 300 seconds.", "parent_ids": ["76eeee7e-2f52-45b1-9d9e-c4eef0373db8"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "72b0459e-08f4-40f7-9003-4e7c1e060452", "fitness": 0.4424322549449796, "name": "EnhancedSelfAdaptiveDE", "description": "An enhanced self-adaptive differential evolution algorithm with a restart mechanism and orthogonal array-based crossover to improve exploration and exploitation.", "code": "import numpy as np\n\nclass EnhancedSelfAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, Cr=0.9, restart_prob=0.05):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F  # Mutation factor\n        self.Cr = Cr  # Crossover rate\n        self.learning_rate = 0.1  # Initial learning rate for parameter adaptation\n        self.restart_prob = restart_prob # Probability of restarting the population\n        self.oa = self._create_orthogonal_array() # Orthogonal Array for Crossover\n\n    def _create_orthogonal_array(self, strength=2):\n        \"\"\"Creates an orthogonal array for crossover.\n        For simplicity, using a predefined OA. A more general approach would dynamically generate it based on dim.\n        This OA is a L9 array (3^4), suitable for dimensions with groups of 3.\n        \"\"\"\n        return np.array([\n            [0, 0, 0, 0],\n            [0, 1, 1, 1],\n            [0, 2, 2, 2],\n            [1, 0, 1, 2],\n            [1, 1, 2, 0],\n            [1, 2, 0, 1],\n            [2, 0, 2, 1],\n            [2, 1, 0, 2],\n            [2, 2, 1, 0]\n        ])\n    \n    def _orthogonal_crossover(self, parent1, parent2, parent3):\n        \"\"\"Performs orthogonal array-based crossover.\"\"\"\n        \n        if self.dim < 4:\n             return parent1 # Fallback to standard DE if dimension is too small for OA\n            \n        group_size = min(self.dim // 4, 3)  # Ensure we don't exceed dimension limits and use groups of 3\n        num_groups = self.dim // group_size\n\n        child = parent1.copy()\n        for g in range(num_groups):\n            start_idx = g * group_size\n            end_idx = start_idx + group_size\n            \n            oa_row_idx = np.random.randint(0, len(self.oa))\n            oa_row = self.oa[oa_row_idx]\n\n            for i in range(group_size):\n                idx = start_idx + i\n                if oa_row[i] == 0:\n                    child[idx] = parent1[idx]\n                elif oa_row[i] == 1:\n                    child[idx] = parent2[idx]\n                else:\n                    child[idx] = parent3[idx]\n        \n        # Handle any remaining dimensions with standard crossover\n        for i in range(num_groups * group_size, self.dim):\n            if np.random.rand() < self.Cr:\n                child[i] = parent2[i]\n            else:\n                child[i] = parent1[i]\n        return child\n\n\n    def __call__(self, func):\n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size # Deduct initial population evaluations from budget\n\n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)]\n\n        generation = 0\n        while self.budget > 0:\n            generation += 1\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + self.F * (b - c), func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                # cross_points = np.random.rand(self.dim) < self.Cr\n                # if not np.any(cross_points):\n                #     cross_points[np.random.randint(0, self.dim)] = True\n                # trial = np.where(cross_points, mutant, population[i])\n                trial = self._orthogonal_crossover(population[i], mutant, a) # Using OA Crossover\n                \n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    population[i] = trial\n\n                    # Update optimal solution\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                        # Increase learning rate if improvement is found\n                        self.learning_rate = min(1.0, self.learning_rate * 1.1)\n                else:\n                    # Reduce learning rate if no improvement\n                    self.learning_rate = max(0.01, self.learning_rate * 0.9)\n\n            # Adapt parameters based on learning rate\n            self.F = np.clip(self.F + self.learning_rate * np.random.normal(0, 0.1), 0.1, 1.0)\n            self.Cr = np.clip(self.Cr + self.learning_rate * np.random.normal(0, 0.1), 0.1, 1.0)\n            \n            # Restart mechanism\n            if np.random.rand() < self.restart_prob:\n                population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                fitness = np.array([func(x) for x in population])\n                self.budget -= self.pop_size # Deduct initial population evaluations from budget\n                \n                if np.min(fitness) < self.f_opt:\n                  self.f_opt = np.min(fitness)\n                  self.x_opt = population[np.argmin(fitness)]\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "The algorithm EnhancedSelfAdaptiveDE scored 0.442 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["94768647-f252-46d5-98c5-9ea8189ba67b"], "operator": null, "metadata": {"aucs": [0.15139691899582308, 0.26617992844403415, 0.3557524003273955, 0.8617672732963458, 0.4296932361386232, 0.6431001207821319, 0.3643823605391534, 0.4236337057177074, 0.3934642762462176, 0.3061688283830236, 0.5545282628797821, 0.9958760164274408, 0.2600511979885155, 0.3364876001404825, 0.7364339528129975, 0]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "6778f218-4a8a-473c-bc92-73a5e497e550", "fitness": 0.0885719802058092, "name": "EnhancedSelfAdaptiveDE", "description": "A simplified self-adaptive differential evolution algorithm with orthogonal array crossover, momentum-based adaptation, and stochastic ranking.", "code": "import numpy as np\n\nclass EnhancedSelfAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=40, F=0.5, Cr=0.9, restart_prob=0.05, momentum=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F  # Mutation factor\n        self.Cr = Cr  # Crossover rate\n        self.momentum = momentum # Momentum for parameter updates\n        self.restart_prob = restart_prob # Probability of restarting the population\n        self.oa = self._create_orthogonal_array() # Orthogonal Array for Crossover\n        self.F_history = [F]\n        self.Cr_history = [Cr]\n\n    def _create_orthogonal_array(self, strength=2):\n        \"\"\"Creates an orthogonal array for crossover. Using a predefined OA.\"\"\"\n        return np.array([\n            [0, 0, 0, 0],\n            [0, 1, 1, 1],\n            [0, 2, 2, 2],\n            [1, 0, 1, 2],\n            [1, 1, 2, 0],\n            [1, 2, 0, 1],\n            [2, 0, 2, 1],\n            [2, 1, 0, 2],\n            [2, 2, 1, 0]\n        ])\n    \n    def _orthogonal_crossover(self, parent1, parent2, parent3):\n        \"\"\"Performs orthogonal array-based crossover.\"\"\"\n        if self.dim < 4:\n             return parent1 # Fallback to standard DE if dimension is too small for OA\n            \n        group_size = min(self.dim // 4, 3)\n        num_groups = self.dim // group_size\n\n        child = parent1.copy()\n        for g in range(num_groups):\n            start_idx = g * group_size\n            end_idx = start_idx + group_size\n            \n            oa_row_idx = np.random.randint(0, len(self.oa))\n            oa_row = self.oa[oa_row_idx]\n\n            for i in range(group_size):\n                idx = start_idx + i\n                if oa_row[i] == 0:\n                    child[idx] = parent1[idx]\n                elif oa_row[i] == 1:\n                    child[idx] = parent2[idx]\n                else:\n                    child[idx] = parent3[idx]\n        \n        # Handle any remaining dimensions with standard crossover\n        for i in range(num_groups * group_size, self.dim):\n            if np.random.rand() < self.Cr:\n                child[i] = parent2[i]\n            else:\n                child[i] = parent1[i]\n        return child\n\n\n    def __call__(self, func):\n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)]\n\n        F_momentum = 0\n        Cr_momentum = 0\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + self.F * (b - c), func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                trial = self._orthogonal_crossover(population[i], mutant, a)\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n\n                # Stochastic Ranking\n                if (f_trial < fitness[i]) or (np.random.rand() < 0.1): # Accept even if worse with a small probability\n                    fitness[i] = f_trial\n                    population[i] = trial\n\n                    # Update optimal solution\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n            \n            # Adapt parameters with momentum\n            best_idx = np.argmin(fitness)\n            success_F = self.F\n            success_Cr = self.Cr\n\n            self.F = np.clip(self.F + self.momentum * F_momentum + 0.1 * np.random.normal(0, 0.1), 0.1, 1.0)\n            self.Cr = np.clip(self.Cr + self.momentum * Cr_momentum + 0.1 * np.random.normal(0, 0.1), 0.1, 1.0)\n            \n            F_momentum = self.F - success_F\n            Cr_momentum = self.Cr - success_Cr\n            \n            self.F_history.append(self.F)\n            self.Cr_history.append(self.Cr)\n\n            # Restart mechanism\n            if np.random.rand() < self.restart_prob:\n                population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                fitness = np.array([func(x) for x in population])\n                self.budget -= self.pop_size\n                \n                if np.min(fitness) < self.f_opt:\n                  self.f_opt = np.min(fitness)\n                  self.x_opt = population[np.argmin(fitness)]\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "The algorithm EnhancedSelfAdaptiveDE scored 0.089 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["72b0459e-08f4-40f7-9003-4e7c1e060452"], "operator": null, "metadata": {"aucs": [0.1771439604116184, 0]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "967ad241-f17e-4532-b415-a39da57ca34c", "fitness": "-inf", "name": "HybridPSO_CMAES", "description": "A novel hybrid algorithm combining the strengths of Particle Swarm Optimization (PSO) with Covariance Matrix Adaptation Evolution Strategy (CMA-ES) to balance exploration and exploitation effectively.", "code": "import numpy as np\n\nclass HybridPSO_CMAES:\n    def __init__(self, budget=10000, dim=10, popsize=None, pso_weight=0.7, cma_decay = 0.99):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = popsize if popsize is not None else 4 + int(3 * np.log(dim))\n        self.pso_weight = pso_weight\n        self.cma_decay = cma_decay\n        self.cma_popsize = self.popsize // 2 # Population size for CMA-ES part\n        self.pso_popsize = self.popsize - self.cma_popsize # Population size for PSO part\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.eval_count = 0\n\n        # Initialize PSO particles\n        pso_particles = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pso_popsize, self.dim))\n        pso_velocities = np.random.uniform(-1, 1, size=(self.pso_popsize, self.dim)) * 0.1  # Small initial velocities\n        pso_personal_best_positions = pso_particles.copy()\n        pso_personal_best_fitnesses = np.array([func(x) for x in pso_particles if self.eval_count < self.budget])\n        self.eval_count += len(pso_personal_best_fitnesses)\n        if len(pso_personal_best_fitnesses) == 0:\n            return self.f_opt, self.x_opt\n        pso_global_best_index = np.argmin(pso_personal_best_fitnesses)\n        pso_global_best_position = pso_personal_best_positions[pso_global_best_index].copy()\n        pso_global_best_fitness = pso_personal_best_fitnesses[pso_global_best_index]\n\n        # Initialize CMA-ES parameters\n        cma_mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        cma_sigma = 0.5  # Step size\n        cma_C = np.eye(self.dim)  # Covariance matrix\n\n        # Evolution path parameters for CMA-ES\n        cma_pc = np.zeros(self.dim)\n        cma_ps = np.zeros(self.dim)\n        cma_chiN = np.sqrt(self.dim) * (1 - (1 / (4 * self.dim)) + (1 / (21 * self.dim**2)))\n\n        # Weights for recombination in CMA-ES (using mu = popsize // 2)\n        cma_mu = self.cma_popsize // 2\n        cma_weights = np.log(cma_mu + 0.5) - np.log(np.arange(1, cma_mu + 1))\n        cma_weights = cma_weights / np.sum(cma_weights)\n        cma_mueff = np.sum(cma_weights)**2 / np.sum(cma_weights**2)\n\n        # Adaptation parameters for CMA-ES\n        cma_cc = (4 + cma_mueff/self.dim) / (self.dim + 4 + 2*cma_mueff/self.dim)\n        cma_cs = (cma_mueff+2) / (self.dim+cma_mueff+5)\n        cma_damps = 1 + 2*max(0, np.sqrt((cma_mueff-1)/(self.dim+1))-1) + cma_cs\n        cma_ccov1 = 0.05 / ((self.dim+1.3)**2+cma_mueff)\n        cma_ccovmu = min(1-cma_ccov1, 0.05 * cma_mueff / ((self.dim+2)**2+cma_mueff))\n\n        while self.eval_count < self.budget:\n            # PSO update\n            r1 = np.random.rand(self.pso_popsize, self.dim)\n            r2 = np.random.rand(self.pso_popsize, self.dim)\n            pso_velocities = (self.pso_weight * pso_velocities +\n                              r1 * (pso_personal_best_positions - pso_particles) +\n                              r2 * (pso_global_best_position - pso_particles))\n            pso_particles = pso_particles + pso_velocities\n\n            # Clip particles to bounds\n            pso_particles = np.clip(pso_particles, func.bounds.lb, func.bounds.ub)\n\n            # Evaluate PSO particles\n            pso_fitnesses = np.array([func(x) for x in pso_particles if self.eval_count < self.budget])\n            self.eval_count += len(pso_fitnesses)\n            if len(pso_fitnesses) == 0:\n                break\n\n            # Update PSO personal bests\n            for i in range(len(pso_fitnesses)):\n                if pso_fitnesses[i] < pso_personal_best_fitnesses[i]:\n                    pso_personal_best_fitnesses[i] = pso_fitnesses[i]\n                    pso_personal_best_positions[i] = pso_particles[i].copy()\n\n            # Update PSO global best\n            current_best_index = np.argmin(pso_personal_best_fitnesses)\n            if pso_personal_best_fitnesses[current_best_index] < pso_global_best_fitness:\n                pso_global_best_fitness = pso_personal_best_fitnesses[current_best_index]\n                pso_global_best_position = pso_personal_best_positions[current_best_index].copy()\n\n            # CMA-ES update\n            cma_Z = np.random.randn(self.dim, self.cma_popsize)\n            cma_x = cma_mean[:, np.newaxis] + cma_sigma * np.dot(np.linalg.cholesky(cma_C), cma_Z)  # Sample from multivariate normal\n\n            cma_f = np.array([func(xi) for xi in cma_x.T if self.eval_count < self.budget])\n            self.eval_count += len(cma_f)\n            if len(cma_f) == 0:\n                break\n\n            # Select parents\n            cma_idx_sort = np.argsort(cma_f)\n            cma_x_sorted = cma_x[:, cma_idx_sort[:cma_mu]]\n\n            # Recombination: new mean\n            cma_mean_new = np.dot(cma_x_sorted, cma_weights)\n\n            # Update evolution paths\n            cma_ps = (1 - cma_cs) * cma_ps + np.sqrt(cma_cs * (2 - cma_cs) * cma_mueff) * np.dot(np.linalg.inv(np.linalg.cholesky(cma_C)), (cma_mean_new - cma_mean)) / cma_sigma\n            cma_hsig = np.linalg.norm(cma_ps) / np.sqrt(1 - (1 - cma_cs)**(2 * self.eval_count / self.cma_popsize)) / cma_chiN < 1.4 + 2 / (self.dim + 1)\n            cma_pc = (1 - cma_cc) * cma_pc + cma_hsig * np.sqrt(cma_cc * (2 - cma_cc) * cma_mueff) * (cma_mean_new - cma_mean) / cma_sigma\n\n            # Update covariance matrix\n            cma_artmp = (1 / cma_sigma) * (cma_x_sorted - cma_mean[:, np.newaxis])\n            cma_C = (1 - cma_ccov1 - cma_ccovmu + cma_ccov1 * cma_cc * (2 - cma_cc)) * cma_C + cma_ccov1 * np.outer(cma_pc, cma_pc) + cma_ccovmu * np.dot(cma_artmp, np.diag(cma_weights).dot(cma_artmp.T))\n\n            # Ensure positive definiteness\n            cma_C = np.triu(cma_C) + np.triu(cma_C, 1).T\n            cma_C = np.real((cma_C + cma_C.T) / 2)\n\n            # Update step size\n            cma_sigma = cma_sigma * np.exp((cma_cs / cma_damps) * (np.linalg.norm(cma_ps) / cma_chiN - 1))\n            cma_mean = cma_mean_new\n\n            cma_sigma *= self.cma_decay\n\n            # Global best update\n            cma_idx_best = np.argmin(cma_f)\n            if cma_f[cma_idx_best] < pso_global_best_fitness:\n                pso_global_best_fitness = cma_f[cma_idx_best]\n                pso_global_best_position = cma_x[:, cma_idx_best].copy()\n\n\n            if pso_global_best_fitness < self.f_opt:\n                self.f_opt = pso_global_best_fitness\n                self.x_opt = pso_global_best_position\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "Evaluation timed out after 300 seconds.", "error": "Evaluation timed out after 300 seconds.", "parent_ids": ["640d5617-bca9-4812-987e-6d6b4664faec"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "156f2448-7e04-43e5-ab25-f25769aec3d5", "fitness": "-inf", "name": "MirroredSelfAdaptiveDE", "description": "A Differential Evolution variant that uses a mirrored sampling strategy to enhance boundary exploration, orthogonal array crossover for efficient parameter mixing, and dynamic population sizing to balance exploration and exploitation.", "code": "import numpy as np\n\nclass MirroredSelfAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size_init=50, F=0.5, Cr=0.9, restart_prob=0.05, pop_size_adapt=True):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_init = pop_size_init\n        self.F = F  # Mutation factor\n        self.Cr = Cr  # Crossover rate\n        self.learning_rate = 0.1  # Initial learning rate for parameter adaptation\n        self.restart_prob = restart_prob # Probability of restarting the population\n        self.oa = self._create_orthogonal_array() # Orthogonal Array for Crossover\n        self.pop_size_adapt = pop_size_adapt\n        self.pop_sizes = [pop_init]\n\n    def _create_orthogonal_array(self, strength=2):\n        \"\"\"Creates an orthogonal array for crossover.\n        For simplicity, using a predefined OA. A more general approach would dynamically generate it based on dim.\n        This OA is a L9 array (3^4), suitable for dimensions with groups of 3.\n        \"\"\"\n        return np.array([\n            [0, 0, 0, 0],\n            [0, 1, 1, 1],\n            [0, 2, 2, 2],\n            [1, 0, 1, 2],\n            [1, 1, 2, 0],\n            [1, 2, 0, 1],\n            [2, 0, 2, 1],\n            [2, 1, 0, 2],\n            [2, 2, 1, 0]\n        ])\n    \n    def _orthogonal_crossover(self, parent1, parent2, parent3):\n        \"\"\"Performs orthogonal array-based crossover.\"\"\"\n        \n        if self.dim < 4:\n             return parent1 # Fallback to standard DE if dimension is too small for OA\n            \n        group_size = min(self.dim // 4, 3)  # Ensure we don't exceed dimension limits and use groups of 3\n        num_groups = self.dim // group_size\n\n        child = parent1.copy()\n        for g in range(num_groups):\n            start_idx = g * group_size\n            end_idx = start_idx + group_size\n            \n            oa_row_idx = np.random.randint(0, len(self.oa))\n            oa_row = self.oa[oa_row_idx]\n\n            for i in range(group_size):\n                idx = start_idx + i\n                if oa_row[i] == 0:\n                    child[idx] = parent1[idx]\n                elif oa_row[i] == 1:\n                    child[idx] = parent2[idx]\n                else:\n                    child[idx] = parent3[idx]\n        \n        # Handle any remaining dimensions with standard crossover\n        for i in range(num_groups * group_size, self.dim):\n            if np.random.rand() < self.Cr:\n                child[i] = parent2[i]\n            else:\n                child[i] = parent1[i]\n        return child\n    \n    def _mirrored_mutation(self, a, b, c, bounds):\n        \"\"\"Performs mirrored mutation to handle boundary constraints.\"\"\"\n        mutant = a + self.F * (b - c)\n        \n        # Mirroring strategy\n        for i in range(self.dim):\n            if mutant[i] < bounds.lb:\n                mutant[i] = bounds.lb + abs(mutant[i] - bounds.lb)\n            elif mutant[i] > bounds.ub:\n                mutant[i] = bounds.ub - abs(mutant[i] - bounds.ub)\n        \n        return mutant\n\n\n    def __call__(self, func):\n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size # Deduct initial population evaluations from budget\n\n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)]\n\n        generation = 0\n        while self.budget > 0:\n            generation += 1\n            \n            # Population size adaptation\n            if self.pop_size_adapt:\n                if generation % 10 == 0:\n                  if np.std(fitness) < 1e-3:\n                      self.pop_size = min(2*self.pop_size, 200) # Increased population size if converged\n                  else:\n                      self.pop_size = max(self.pop_size_init, self.pop_size // 2) # Reduce if not converged\n                  self.pop_sizes.append(self.pop_size)\n\n            # Main loop\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = self._mirrored_mutation(a, b, c, func.bounds)\n                # Crossover\n                trial = self._orthogonal_crossover(population[i], mutant, a) # Using OA Crossover\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    population[i] = trial\n\n                    # Update optimal solution\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                        # Increase learning rate if improvement is found\n                        self.learning_rate = min(1.0, self.learning_rate * 1.1)\n                else:\n                    # Reduce learning rate if no improvement\n                    self.learning_rate = max(0.01, self.learning_rate * 0.9)\n\n            # Adapt parameters based on learning rate\n            self.F = np.clip(self.F + self.learning_rate * np.random.normal(0, 0.1), 0.1, 1.0)\n            self.Cr = np.clip(self.Cr + self.learning_rate * np.random.normal(0, 0.1), 0.1, 1.0)\n            \n            # Restart mechanism\n            if np.random.rand() < self.restart_prob:\n                population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                fitness = np.array([func(x) for x in population])\n                self.budget -= self.pop_size # Deduct initial population evaluations from budget\n                \n                if np.min(fitness) < self.f_opt:\n                  self.f_opt = np.min(fitness)\n                  self.x_opt = population[np.argmin(fitness)]\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "In the code, line 73, in _mirrored_mutation, the following error occurred:\nValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\nOn line: if mutant[i] < bounds.lb:", "error": "In the code, line 73, in _mirrored_mutation, the following error occurred:\nValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\nOn line: if mutant[i] < bounds.lb:", "parent_ids": ["72b0459e-08f4-40f7-9003-4e7c1e060452"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "15644e17-f8e9-4366-82a4-6b560c7c9e4b", "fitness": "-inf", "name": "EnhancedSelfAdaptiveDE", "description": "A self-adaptive differential evolution algorithm with enhanced parameter control, orthogonal crossover, and a stagnation-based restart strategy for improved exploration and exploitation.", "code": "import numpy as np\n\nclass EnhancedSelfAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, Cr=0.9, restart_trigger=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F  # Mutation factor\n        self.Cr = Cr  # Crossover rate\n        self.learning_rate = 0.1  # Initial learning rate for parameter adaptation\n        self.restart_trigger = restart_trigger  # Threshold for restarting the population\n        self.stagnation_counter = 0\n        self.max_stagnation = 50 # Number of generations to wait before restarting\n        self.best_fitness_history = []\n\n    def orthogonal_crossover(self, population, i, mutant, num_samples=5):\n        \"\"\"Performs orthogonal crossover between the individual and mutant.\"\"\"\n        offspring = np.copy(population[i])\n        # Select random indices for crossover\n        indices = np.random.choice(self.dim, num_samples, replace=False)\n        offspring[indices] = mutant[indices]\n        return offspring\n\n    def __call__(self, func):\n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size # Deduct initial population evaluations from budget\n\n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)]\n        self.best_fitness_history.append(self.f_opt)\n\n        generation = 0\n        while self.budget > 0:\n            generation += 1\n            stagnation = False\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + self.F * (b - c), func.bounds.lb, func.bounds.ub)\n\n                # Crossover (Orthogonal Crossover)\n                trial = self.orthogonal_crossover(population, i, mutant)\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    population[i] = trial\n\n                    # Update optimal solution\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                        # Increase learning rate if improvement is found\n                        self.learning_rate = min(1.0, self.learning_rate * 1.1)\n                        self.stagnation_counter = 0\n                    else:\n                        # Reduce learning rate if no improvement\n                        self.learning_rate = max(0.01, self.learning_rate * 0.9)\n                        self.stagnation_counter += 1\n                else:\n                    # Reduce learning rate if no improvement\n                    self.learning_rate = max(0.01, self.learning_rate * 0.9)\n                    self.stagnation_counter += 1\n            # Adapt parameters based on learning rate\n            self.F = np.clip(self.F + self.learning_rate * np.random.normal(0, 0.1), 0.1, 1.0)\n            self.Cr = np.clip(self.Cr + self.learning_rate * np.random.normal(0, 0.1), 0.1, 1.0)\n\n            self.best_fitness_history.append(self.f_opt)\n\n            # Restart mechanism based on stagnation\n            if self.stagnation_counter > self.max_stagnation or (len(self.best_fitness_history) > 10 and np.std(self.best_fitness_history[-10:]) < self.restart_trigger):\n                # Reset population\n                population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                fitness = np.array([func(x) for x in population])\n                self.budget -= self.pop_size  # Deduct evaluations from budget\n                self.f_opt = np.min(fitness)\n                self.x_opt = population[np.argmin(fitness)]\n                self.learning_rate = 0.1  # Reset learning rate\n                self.stagnation_counter = 0\n                self.best_fitness_history.append(self.f_opt)\n                \n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "In the code, line 1025, in numpy.random.mtrand.RandomState.choice, the following error occurred:\nValueError: Cannot take a larger sample than population when 'replace=False'", "error": "In the code, line 1025, in numpy.random.mtrand.RandomState.choice, the following error occurred:\nValueError: Cannot take a larger sample than population when 'replace=False'", "parent_ids": ["94768647-f252-46d5-98c5-9ea8189ba67b"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "9892e2b3-d169-4b69-a60c-21548936dcae", "fitness": "-inf", "name": "AdaptiveChargedParticleOptimization", "description": "A charged particle optimization algorithm with adaptive charge scaling, dynamic velocity adjustments based on population diversity, and a local search phase to refine promising solutions.", "code": "import numpy as np\n\nclass AdaptiveChargedParticleOptimization:\n    def __init__(self, budget=10000, dim=10, population_size=20, charge_decay_rate=0.95,\n                 velocity_damping=0.8, social_influence=0.5, local_search_probability=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = population_size\n        self.charge_decay_rate = charge_decay_rate\n        self.velocity_damping = velocity_damping\n        self.social_influence = social_influence\n        self.local_search_probability = local_search_probability\n\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.func = func\n\n        # Initialize particles randomly\n        particles = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.population_size, self.dim))\n        fitness = np.array([func(particle) for particle in particles])\n        self.budget -= self.population_size\n\n        best_index = np.argmin(fitness)\n        best_particle = particles[best_index]\n        best_fitness = fitness[best_index]\n\n        if best_fitness < self.f_opt:\n            self.f_opt = best_fitness\n            self.x_opt = best_particle\n\n        # Initialize velocities\n        velocities = np.zeros((self.population_size, self.dim))\n\n        # Initialize charges (proportional to fitness, but scaled)\n        charges = self.calculate_charges(fitness)\n\n        while self.budget > 0:\n            # Calculate electrostatic forces between particles\n            forces = self.calculate_forces(particles, charges)\n\n            # Update velocities based on forces and best solution, adaptively\n            velocities = self.update_velocities(velocities, forces, particles, best_particle, fitness)\n\n            # Move particles\n            new_particles = self.move_particles(particles, velocities)\n\n            # Evaluate new particle positions\n            new_fitness = np.array([func(particle) for particle in new_particles])\n            self.budget -= self.population_size\n\n            # Local search: Apply a local search operator to the best particle with a certain probability\n            for i in range(self.population_size):\n                if np.random.rand() < self.local_search_probability:\n                    new_particles[i] = self.local_search(new_particles[i], func.bounds.lb, func.bounds.ub)\n                    new_fitness[i] = func(new_particles[i])\n                    self.budget -= 1  # Account for additional function evaluation\n                    \n\n            # Update best solution\n            for i in range(self.population_size):\n                if new_fitness[i] < best_fitness:\n                    best_fitness = new_fitness[i]\n                    best_particle = new_particles[i]\n                    if best_fitness < self.f_opt:\n                        self.f_opt = best_fitness\n                        self.x_opt = best_particle\n\n            # Update charges based on new fitness\n            charges = self.calculate_charges(new_fitness)\n\n            # Decay charges\n            charges *= self.charge_decay_rate\n\n            particles = new_particles\n            fitness = new_fitness\n\n        return self.f_opt, self.x_opt\n\n    def calculate_charges(self, fitness):\n        \"\"\"Calculates charges based on fitness values, adaptively scaled.\"\"\"\n        # Adaptive scaling of charges based on population diversity\n        std_fitness = np.std(fitness)\n        if std_fitness == 0:\n            # If population is uniform, assign equal charges\n            charges = np.ones(self.population_size) / self.population_size\n            return charges\n\n        # Scale fitness based on standard deviation\n        scaled_fitness = (fitness - np.min(fitness)) / (std_fitness + 1e-8)\n        charges = np.exp(-scaled_fitness)  # Exponential scaling\n\n        # Normalize charges\n        charges /= np.sum(charges)\n        return charges\n\n\n    def calculate_forces(self, particles, charges):\n        \"\"\"Calculates electrostatic forces between particles.\"\"\"\n        forces = np.zeros((self.population_size, self.dim))\n        for i in range(self.population_size):\n            for j in range(self.population_size):\n                if i != j:\n                    # Calculate distance between particles\n                    distance = np.linalg.norm(particles[j] - particles[i]) + 1e-8  # Avoid division by zero\n                    # Calculate force magnitude (Coulomb's law)\n                    force_magnitude = (charges[i] * charges[j]) / (distance**2)\n                    # Calculate force direction\n                    force_direction = (particles[j] - particles[i]) / distance\n                    # Accumulate force\n                    forces[i] += force_magnitude * force_direction\n        return forces\n\n    def update_velocities(self, velocities, forces, particles, best_particle, fitness):\n        \"\"\"Updates particle velocities based on forces and attraction to the best solution, adaptively.\"\"\"\n        # Adaptive velocity damping based on population diversity\n        std_fitness = np.std(fitness)\n        adaptive_damping = self.velocity_damping * (1 - np.exp(-std_fitness))  # Reduce damping if diversity is high\n        \n        # Adaptive social influence\n        adaptive_social_influence = self.social_influence * (1 + np.exp(-std_fitness))\n\n        new_velocities = adaptive_damping * velocities + forces + \\\n                         adaptive_social_influence * np.random.rand() * (best_particle - particles)\n        return new_velocities\n\n    def move_particles(self, particles, velocities):\n        \"\"\"Moves particles based on their velocities, ensuring they stay within bounds.\"\"\"\n        new_particles = particles + velocities\n        new_particles = np.clip(new_particles, self.func.bounds.lb, self.func.bounds.ub)\n        return new_particles\n\n    def local_search(self, particle, lower_bound, upper_bound, step_size=0.1):\n        \"\"\"Performs a local search around a particle.\"\"\"\n        new_particle = particle.copy()\n        for i in range(self.dim):\n            # Perturb each dimension with a small random step\n            new_particle[i] += np.random.uniform(-step_size, step_size) * (upper_bound - lower_bound)\n            new_particle[i] = np.clip(new_particle[i], lower_bound, upper_bound)  # Keep within bounds\n        return new_particle", "configspace": "", "generation": 2, "feedback": "In the code, line 139, in local_search, the following error occurred:\nValueError: setting an array element with a sequence.\nOn line: new_particle[i] += np.random.uniform(-step_size, step_size) * (upper_bound - lower_bound)", "error": "In the code, line 139, in local_search, the following error occurred:\nValueError: setting an array element with a sequence.\nOn line: new_particle[i] += np.random.uniform(-step_size, step_size) * (upper_bound - lower_bound)", "parent_ids": ["9449cc64-1123-49f1-9eab-57556b662920"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "3bd2ec25-ec5f-407e-adbd-93ddf8de2180", "fitness": 0.2596221611026238, "name": "SimplifiedCMAES", "description": "A simplified CMA-ES variant with adaptive step size control and a basic restart strategy to balance exploration and exploitation.", "code": "import numpy as np\n\nclass SimplifiedCMAES:\n    def __init__(self, budget=10000, dim=10, popsize=None, initial_step_size=0.5, stagnation_threshold=50):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = popsize if popsize is not None else 4 + int(3 * np.log(dim))\n        self.initial_step_size = initial_step_size\n        self.stagnation_threshold = stagnation_threshold\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.eval_count = 0\n        mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        sigma = self.initial_step_size\n        C = np.eye(self.dim)\n        stagnation_counter = 0\n        best_fitness_history = []\n\n        while self.eval_count < self.budget:\n            z = np.random.randn(self.dim, self.popsize)\n            x = mean[:, np.newaxis] + sigma * np.dot(np.linalg.cholesky(C), z)\n\n            f = np.array([func(xi) for xi in x.T if self.eval_count < self.budget])\n            self.eval_count += len(f)\n\n            if len(f) == 0:\n                break\n\n            idx_best = np.argmin(f)\n            if f[idx_best] < self.f_opt:\n                self.f_opt = f[idx_best]\n                self.x_opt = x[:, idx_best]\n                stagnation_counter = 0\n            else:\n                stagnation_counter += 1\n\n            idx_sort = np.argsort(f)\n            x_sorted = x[:, idx_sort[:self.popsize // 2]]\n\n            mean = np.mean(x_sorted, axis=1)\n\n            sigma *= np.exp(0.1 * (np.std(f) / np.mean(f) - 1))\n\n            if stagnation_counter > self.stagnation_threshold:\n                mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                sigma = self.initial_step_size\n                C = np.eye(self.dim)\n                stagnation_counter = 0\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "The algorithm SimplifiedCMAES scored 0.260 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["640d5617-bca9-4812-987e-6d6b4664faec"], "operator": null, "metadata": {"aucs": [0.11053738998553742, 0.14211342911807479, 0.5592457318533466, 0.12122028669428087, 0.18764398115305703, 0.7847672349799313, 0.21182258896766393, 0.17013233879947276, 0.1568062733940162, 0.15335382659035146, 0.15911094890541222, 0.49713517241032845, 0.2627053444185402, 0.1734008637795721, 0.23655975516452488, 0.3151531259112065, 0.2555399901522588, 0.13958190575329077, 0.23343736012215555, 0.32217567389945545]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "c51ccb82-0f18-45ea-8ef6-8ded5e04778f", "fitness": "-inf", "name": "EnhancedSelfAdaptiveDE", "description": "A simplified self-adaptive differential evolution algorithm using a fixed orthogonal array and a dynamic population size adjustment strategy for better exploration and exploitation balance.", "code": "import numpy as np\n\nclass EnhancedSelfAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, Cr=0.9, restart_prob=0.05):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F  # Mutation factor\n        self.Cr = Cr  # Crossover rate\n        self.restart_prob = restart_prob # Probability of restarting the population\n        self.oa = self._create_orthogonal_array() # Orthogonal Array for Crossover\n        self.min_pop_size = 10\n        self.max_pop_size = 100\n\n\n    def _create_orthogonal_array(self, strength=2):\n        \"\"\"Creates an orthogonal array for crossover.\"\"\"\n        return np.array([\n            [0, 0, 0, 0],\n            [0, 1, 1, 1],\n            [0, 2, 2, 2],\n            [1, 0, 1, 2],\n            [1, 1, 2, 0],\n            [1, 2, 0, 1],\n            [2, 0, 2, 1],\n            [2, 1, 0, 2],\n            [2, 2, 1, 0]\n        ])\n    \n    def _orthogonal_crossover(self, parent1, parent2, parent3):\n        \"\"\"Performs orthogonal array-based crossover.\"\"\"\n        group_size = min(self.dim // 4, 3)\n        num_groups = self.dim // group_size\n\n        child = parent1.copy()\n        for g in range(num_groups):\n            start_idx = g * group_size\n            end_idx = start_idx + group_size\n            \n            oa_row_idx = np.random.randint(0, len(self.oa))\n            oa_row = self.oa[oa_row_idx]\n\n            for i in range(group_size):\n                idx = start_idx + i\n                if oa_row[i] == 0:\n                    child[idx] = parent1[idx]\n                elif oa_row[i] == 1:\n                    child[idx] = parent2[idx]\n                else:\n                    child[idx] = parent3[idx]\n        \n        for i in range(num_groups * group_size, self.dim):\n            if np.random.rand() < self.Cr:\n                child[i] = parent2[i]\n            else:\n                child[i] = parent1[i]\n        return child\n\n\n    def __call__(self, func):\n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)]\n\n        generation = 0\n        stagnation_counter = 0\n        while self.budget > 0:\n            generation += 1\n            best_fitness_before = self.f_opt\n\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + self.F * (b - c), func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                trial = self._orthogonal_crossover(population[i], mutant, a)\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    population[i] = trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n            #Dynamic population size adjustment\n            if self.f_opt < best_fitness_before:\n                stagnation_counter = 0\n                self.pop_size = min(self.pop_size + 1, self.max_pop_size)  # Increase population if improving\n            else:\n                stagnation_counter += 1\n                if stagnation_counter > 20:\n                    self.pop_size = max(self.pop_size - 2, self.min_pop_size)  # Decrease population if stagnating\n                    stagnation_counter = 0\n            self.pop_size = int(self.pop_size)\n\n            #Adapt F and Cr (Simplified - no learning rate)\n            self.F = np.clip(np.random.normal(0.5, 0.1), 0.1, 1.0)\n            self.Cr = np.clip(np.random.normal(0.9, 0.1), 0.1, 1.0)\n\n\n            # Restart mechanism\n            if np.random.rand() < self.restart_prob:\n                population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                fitness = np.array([func(x) for x in population])\n                self.budget -= self.pop_size\n                \n                if np.min(fitness) < self.f_opt:\n                  self.f_opt = np.min(fitness)\n                  self.x_opt = population[np.argmin(fitness)]\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "In the code, line 33, in _orthogonal_crossover, the following error occurred:\nZeroDivisionError: integer division or modulo by zero\nOn line: num_groups = self.dim // group_size", "error": "In the code, line 33, in _orthogonal_crossover, the following error occurred:\nZeroDivisionError: integer division or modulo by zero\nOn line: num_groups = self.dim // group_size", "parent_ids": ["72b0459e-08f4-40f7-9003-4e7c1e060452"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "a83a17cc-b04d-4773-9303-a0c1f8ef7a9f", "fitness": 0.41357758621668045, "name": "ChargedParticleOptimization", "description": "A charged particle optimization algorithm with adaptive charge calculation based on fitness rank and a dynamic social influence factor.", "code": "import numpy as np\n\nclass ChargedParticleOptimization:\n    def __init__(self, budget=10000, dim=10, population_size=20, charge_decay_rate=0.95,\n                 velocity_damping=0.8, social_influence_initial=0.5, social_influence_decay=0.99):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = population_size\n        self.charge_decay_rate = charge_decay_rate\n        self.velocity_damping = velocity_damping\n        self.social_influence_initial = social_influence_initial\n        self.social_influence = social_influence_initial\n        self.social_influence_decay = social_influence_decay\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.func = func\n\n        # Initialize particles randomly\n        particles = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.population_size, self.dim))\n        fitness = np.array([func(particle) for particle in particles])\n        self.budget -= self.population_size\n\n        best_index = np.argmin(fitness)\n        best_particle = particles[best_index]\n        best_fitness = fitness[best_index]\n\n        if best_fitness < self.f_opt:\n            self.f_opt = best_fitness\n            self.x_opt = best_particle\n\n        # Initialize velocities\n        velocities = np.zeros((self.population_size, self.dim))\n\n        # Initialize charges (based on fitness rank)\n        charges = self.calculate_charges(fitness)\n\n        while self.budget > 0:\n            # Calculate electrostatic forces between particles\n            forces = self.calculate_forces(particles, charges)\n\n            # Update velocities based on forces and best solution\n            velocities = self.update_velocities(velocities, forces, particles, best_particle)\n\n            # Move particles\n            new_particles = self.move_particles(particles, velocities)\n\n            # Evaluate new particle positions\n            new_fitness = np.array([func(particle) for particle in new_particles])\n            self.budget -= self.population_size\n\n            # Update best solution\n            for i in range(self.population_size):\n                if new_fitness[i] < best_fitness:\n                    best_fitness = new_fitness[i]\n                    best_particle = new_particles[i]\n                    if best_fitness < self.f_opt:\n                        self.f_opt = best_fitness\n                        self.x_opt = best_particle\n\n            # Update charges based on new fitness\n            charges = self.calculate_charges(new_fitness)\n\n            # Decay charges\n            charges *= self.charge_decay_rate\n\n            particles = new_particles\n            fitness = new_fitness\n\n            # Decay social influence\n            self.social_influence *= self.social_influence_decay\n\n        return self.f_opt, self.x_opt\n\n    def calculate_charges(self, fitness):\n        \"\"\"Calculates charges based on fitness rank.\"\"\"\n        ranked_fitness = np.argsort(fitness)\n        charges = np.zeros(self.population_size)\n        for i in range(self.population_size):\n            charges[ranked_fitness[i]] = (self.population_size - i)  # Higher rank = higher charge\n\n        charges /= np.sum(charges)  # Normalize\n        return charges\n\n\n    def calculate_forces(self, particles, charges):\n        \"\"\"Calculates electrostatic forces between particles.\"\"\"\n        forces = np.zeros((self.population_size, self.dim))\n        for i in range(self.population_size):\n            for j in range(self.population_size):\n                if i != j:\n                    # Calculate distance between particles\n                    distance = np.linalg.norm(particles[j] - particles[i]) + 1e-8  # Avoid division by zero\n                    # Calculate force magnitude (Coulomb's law)\n                    force_magnitude = (charges[i] * charges[j]) / (distance**2)\n                    # Calculate force direction\n                    force_direction = (particles[j] - particles[i]) / distance\n                    # Accumulate force\n                    forces[i] += force_magnitude * force_direction\n        return forces\n\n    def update_velocities(self, velocities, forces, particles, best_particle):\n        \"\"\"Updates particle velocities based on forces and attraction to the best solution.\"\"\"\n        new_velocities = self.velocity_damping * velocities + forces + \\\n                         self.social_influence * np.random.rand(self.dim) * (best_particle - particles)\n        return new_velocities\n\n    def move_particles(self, particles, velocities):\n        \"\"\"Moves particles based on their velocities, ensuring they stay within bounds.\"\"\"\n        new_particles = particles + velocities\n        new_particles = np.clip(new_particles, self.func.bounds.lb, self.func.bounds.ub)\n        return new_particles", "configspace": "", "generation": 2, "feedback": "The algorithm ChargedParticleOptimization scored 0.414 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["9449cc64-1123-49f1-9eab-57556b662920"], "operator": null, "metadata": {"aucs": [0.20109294017027823, 0.41032709123994615, 0.42837574301224945, 0.9013794177485869, 0.2519674898048061, 0.2525776161789529, 0.309912558308449, 0.37552677320484795, 0.36147557136731434, 0.14844959595340845, 0.5262490213569024, 0.984199920667546, 0.2332703881073448, 0.2585690532710794, 0.6499872357102484, 0.4292211421373773, 0.34901760675625737, 0.5124760034260436, 0.17801500814501647, 0.5094615477669542]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "a8f61623-05c8-4948-9ec8-361f5c9fe932", "fitness": 0.35813427635828765, "name": "ChargedParticleOptimization", "description": "A simplified Charged Particle Optimization with adaptive velocity clamping and a focus on exploration by reducing social influence.", "code": "import numpy as np\n\nclass ChargedParticleOptimization:\n    def __init__(self, budget=10000, dim=10, population_size=20, charge_decay_rate=0.95,\n                 velocity_damping=0.7, social_influence_initial=0.3, social_influence_decay=0.99,\n                 velocity_clamp_factor=0.5):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = population_size\n        self.charge_decay_rate = charge_decay_rate\n        self.velocity_damping = velocity_damping\n        self.social_influence_initial = social_influence_initial\n        self.social_influence = social_influence_initial\n        self.social_influence_decay = social_influence_decay\n        self.velocity_clamp_factor = velocity_clamp_factor  # New parameter for velocity clamping\n\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.func = func\n\n        # Initialize particles randomly\n        particles = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.population_size, self.dim))\n        fitness = np.array([func(particle) for particle in particles])\n        self.budget -= self.population_size\n\n        best_index = np.argmin(fitness)\n        best_particle = particles[best_index]\n        best_fitness = fitness[best_index]\n\n        if best_fitness < self.f_opt:\n            self.f_opt = best_fitness\n            self.x_opt = best_particle\n\n        # Initialize velocities\n        velocities = np.zeros((self.population_size, self.dim))\n\n        # Initialize charges (based on fitness rank)\n        charges = self.calculate_charges(fitness)\n\n        while self.budget > 0:\n            # Calculate electrostatic forces between particles\n            forces = self.calculate_forces(particles, charges)\n\n            # Update velocities based on forces and best solution\n            velocities = self.update_velocities(velocities, forces, particles, best_particle, func.bounds.ub - func.bounds.lb)  # Pass bound range for adaptive clamping\n\n            # Move particles\n            new_particles = self.move_particles(particles, velocities)\n\n            # Evaluate new particle positions\n            new_fitness = np.array([func(particle) for particle in new_particles])\n            self.budget -= self.population_size\n\n            # Update best solution\n            for i in range(self.population_size):\n                if new_fitness[i] < best_fitness:\n                    best_fitness = new_fitness[i]\n                    best_particle = new_particles[i]\n                    if best_fitness < self.f_opt:\n                        self.f_opt = best_fitness\n                        self.x_opt = best_particle\n\n            # Update charges based on new fitness\n            charges = self.calculate_charges(new_fitness)\n\n            # Decay charges\n            charges *= self.charge_decay_rate\n\n            particles = new_particles\n            fitness = new_fitness\n\n            # Decay social influence\n            self.social_influence *= self.social_influence_decay\n\n        return self.f_opt, self.x_opt\n\n    def calculate_charges(self, fitness):\n        \"\"\"Calculates charges based on fitness rank.\"\"\"\n        ranked_fitness = np.argsort(fitness)\n        charges = np.zeros(self.population_size)\n        for i in range(self.population_size):\n            charges[ranked_fitness[i]] = (self.population_size - i)  # Higher rank = higher charge\n\n        charges /= np.sum(charges)  # Normalize\n        return charges\n\n\n    def calculate_forces(self, particles, charges):\n        \"\"\"Calculates electrostatic forces between particles.\"\"\"\n        forces = np.zeros((self.population_size, self.dim))\n        for i in range(self.population_size):\n            for j in range(self.population_size):\n                if i != j:\n                    # Calculate distance between particles\n                    distance = np.linalg.norm(particles[j] - particles[i]) + 1e-8  # Avoid division by zero\n                    # Calculate force magnitude (Coulomb's law)\n                    force_magnitude = (charges[i] * charges[j]) / (distance**2)\n                    # Calculate force direction\n                    force_direction = (particles[j] - particles[i]) / distance\n                    # Accumulate force\n                    forces[i] += force_magnitude * force_direction\n        return forces\n\n    def update_velocities(self, velocities, forces, particles, best_particle, bound_range):\n        \"\"\"Updates particle velocities based on forces and attraction to the best solution, with adaptive clamping.\"\"\"\n        new_velocities = self.velocity_damping * velocities + forces + \\\n                         self.social_influence * np.random.rand(self.dim) * (best_particle - particles)\n\n        # Adaptive velocity clamping\n        max_velocity = self.velocity_clamp_factor * bound_range\n        new_velocities = np.clip(new_velocities, -max_velocity, max_velocity)\n\n\n        return new_velocities\n\n    def move_particles(self, particles, velocities):\n        \"\"\"Moves particles based on their velocities, ensuring they stay within bounds.\"\"\"\n        new_particles = particles + velocities\n        new_particles = np.clip(new_particles, self.func.bounds.lb, self.func.bounds.ub)\n        return new_particles", "configspace": "", "generation": 3, "feedback": "The algorithm ChargedParticleOptimization scored 0.358 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["a83a17cc-b04d-4773-9303-a0c1f8ef7a9f"], "operator": null, "metadata": {"aucs": [0.11454636087055592, 0.18583504984174193, 0.4444596336212674, 0.19656596811773774, 0.2895273979284555, 0.4592073173174006, 0.29423586572212024, 0.3640139816561002, 0.3468724767920357, 0.304310842491162, 0.39194719897285235, 0.9968376400042194, 0.2786030456367604, 0.26306161325293, 0.552649789155196, 0.3204388499673628, 0.27246605547097447, 0.3763438123055316, 0.2100021505792421, 0.5007604774621075]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "c4a74b79-c481-429e-8a51-ae77d6b3cf82", "fitness": "-inf", "name": "DynamicOrthogonalCMAES", "description": "An enhanced CMA-ES variant that incorporates a dynamic population size adjustment based on stagnation and diversity, alongside orthogonal sampling to boost exploration and exploitation.", "code": "import numpy as np\n\nclass DynamicOrthogonalCMAES:\n    def __init__(self, budget=10000, dim=10, popsize_init=None, cs=0.3, damps=1, ccov1=0.05, ccovmu=0.05, cma_decay = 0.99, stagnation_threshold=100, diversity_threshold=0.1, popsize_increase_factor=1.5, popsize_decrease_factor=0.5):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = popsize_init if popsize_init is not None else 4 + int(3 * np.log(dim))\n        self.popsize_init = self.popsize  # Store initial popsize\n        self.cs = cs\n        self.damps = damps\n        self.ccov1 = ccov1\n        self.ccovmu = ccovmu\n        self.cma_decay = cma_decay\n        self.mu = self.popsize // 2  # Number of parents/elite individuals\n        self.stagnation_threshold = stagnation_threshold\n        self.diversity_threshold = diversity_threshold\n        self.popsize_increase_factor = popsize_increase_factor\n        self.popsize_decrease_factor = popsize_decrease_factor\n        self.best_fitness_history = []\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.eval_count = 0\n\n        # Initialize CMA-ES parameters\n        mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        sigma = 0.5  # Step size\n        C = np.eye(self.dim)  # Covariance matrix\n\n        # Evolution path parameters\n        pc = np.zeros(self.dim)\n        ps = np.zeros(self.dim)\n        chiN = np.sqrt(self.dim) * (1 - (1 / (4 * self.dim)) + (1 / (21 * self.dim**2)))\n\n        # Weights for recombination\n        weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n        weights = weights / np.sum(weights)\n        mueff = np.sum(weights)**2 / np.sum(weights**2)\n\n        # Adaptation parameters\n        cc = (4 + mueff/self.dim) / (self.dim + 4 + 2*mueff/self.dim)\n        cs = (mueff+2) / (self.dim+mueff+5)\n        damps = 1 + 2*max(0, np.sqrt((mueff-1)/(self.dim+1))-1) + cs\n        ccov1 = self.ccov1 / ((self.dim+1.3)**2+mueff)\n        ccovmu = min(1-ccov1, self.ccovmu * mueff / ((self.dim+2)**2+mueff))\n\n        restart_count = 0\n        stagnation_counter = 0\n\n        while self.eval_count < self.budget:\n            # Generate and evaluate offspring\n            Z = np.random.randn(self.dim, self.popsize)\n            x = mean[:, np.newaxis] + sigma * np.dot(np.linalg.cholesky(C), Z)  # Sample from multivariate normal\n\n            # Orthogonal sampling enhancement (Latin Hypercube Sampling could also be used)\n            for i in range(self.popsize):\n                for j in range(self.dim):\n                    x[j, i] = np.clip(x[j, i], func.bounds.lb, func.bounds.ub)\n\n            f = np.array([func(xi) for xi in x.T if self.eval_count < self.budget])\n            self.eval_count += len(f)\n\n            if len(f) == 0:\n                break\n\n            # Update best solution\n            idx_best = np.argmin(f)\n            if f[idx_best] < self.f_opt:\n                self.f_opt = f[idx_best]\n                self.x_opt = x[:, idx_best]\n                stagnation_counter = 0 # Reset stagnation counter if improvement found\n            else:\n                stagnation_counter += 1\n\n            # Select parents\n            idx_sort = np.argsort(f)\n            x_sorted = x[:, idx_sort[:self.mu]]\n\n            # Recombination: new mean\n            mean_new = np.dot(x_sorted, weights)\n\n            # Update evolution paths\n            ps = (1 - cs) * ps + np.sqrt(cs * (2 - cs) * mueff) * np.dot(np.linalg.inv(np.linalg.cholesky(C)), (mean_new - mean)) / sigma\n            hsig = np.linalg.norm(ps) / np.sqrt(1 - (1 - cs)**(2 * self.eval_count / self.popsize)) / chiN < 1.4 + 2 / (self.dim + 1)\n            pc = (1 - cc) * pc + hsig * np.sqrt(cc * (2 - cc) * mueff) * (mean_new - mean) / sigma\n\n            # Update covariance matrix\n            artmp = (1 / sigma) * (x_sorted - mean[:, np.newaxis])\n            C = (1 - ccov1 - ccovmu + ccov1 * cc * (2 - cc)) * C + ccov1 * np.outer(pc, pc) + ccovmu * np.dot(artmp, np.diag(weights).dot(artmp.T))\n\n            # Ensure positive definiteness\n            C = np.triu(C) + np.triu(C, 1).T\n            C = np.real((C + C.T) / 2)\n\n            # Update step size\n            sigma = sigma * np.exp((cs / self.damps) * (np.linalg.norm(ps) / chiN - 1))\n            mean = mean_new\n\n            sigma *= self.cma_decay\n\n            # Stagnation Detection and Restart Mechanism\n            if stagnation_counter > self.stagnation_threshold:\n                restart_count += 1\n                mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                sigma = 0.5\n                C = np.eye(self.dim)\n                pc = np.zeros(self.dim)\n                ps = np.zeros(self.dim)\n                stagnation_counter = 0  # Reset stagnation counter\n                self.popsize = self.popsize_init # Reset popsize to initial value\n\n            # Diversity Check and Population Size Adjustment\n            diversity = np.std(f)  # Simple diversity measure (fitness std)\n            if diversity < self.diversity_threshold:\n                self.popsize = int(min(self.popsize * self.popsize_increase_factor, self.budget / 10)) # Ensure popsize doesn't exceed budget limitations\n                self.mu = self.popsize // 2\n                print(f\"Low diversity detected. Increasing popsize to {self.popsize}\")\n\n            elif self.eval_count > self.budget/2 and diversity > self.diversity_threshold * 5 and self.popsize > 4 + int(3 * np.log(self.dim)): # Reduce popsize after initial exploration\n                  self.popsize = int(max(self.popsize * self.popsize_decrease_factor, 4 + int(3 * np.log(self.dim))))\n                  self.mu = self.popsize // 2\n                  print(f\"High diversity detected. Decreasing popsize to {self.popsize}\")\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "In the code, line 59, in __call__, the following error occurred:\nValueError: setting an array element with a sequence.\nOn line: x[j, i] = np.clip(x[j, i], func.bounds.lb, func.bounds.ub)", "error": "In the code, line 59, in __call__, the following error occurred:\nValueError: setting an array element with a sequence.\nOn line: x[j, i] = np.clip(x[j, i], func.bounds.lb, func.bounds.ub)", "parent_ids": ["640d5617-bca9-4812-987e-6d6b4664faec"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "9da1a0c6-e65f-43b9-8203-3b424ee12951", "fitness": "-inf", "name": "", "description": "Self-adaptive Differential Evolution with Ensemble of Mutation Strategies, dynamically adjusting strategy probabilities based on their success rate.", "code": "raise Exception(\"Could not extract generated code. The code should be encapsulated with ``` in your response.\")", "configspace": "", "generation": 3, "feedback": "Traceback (most recent call last):\n  File \"C:\\Users\\Kukoy\\AppData\\Local\\Temp\\blade_env_vttbffww\\run_eval.py\", line 10, in <module>\n    result=problem.evaluate(solution)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Kukoy\\OneDrive\\Documents\\BLADE-WORKING\\BLADE\\iohblade\\problems\\mabbob.py\", line 143, in evaluate\n    exec(code, safe_globals, local_env)\n  File \"<string>\", line 1, in <module>\nException: Could not extract generated code. The code should be encapsulated with ``` in your response.", "error": "Traceback (most recent call last):\n  File \"C:\\Users\\Kukoy\\AppData\\Local\\Temp\\blade_env_vttbffww\\run_eval.py\", line 10, in <module>\n    result=problem.evaluate(solution)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Kukoy\\OneDrive\\Documents\\BLADE-WORKING\\BLADE\\iohblade\\problems\\mabbob.py\", line 143, in evaluate\n    exec(code, safe_globals, local_env)\n  File \"<string>\", line 1, in <module>\nException: Could not extract generated code. The code should be encapsulated with ``` in your response.", "parent_ids": ["94768647-f252-46d5-98c5-9ea8189ba67b"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "7f5458b5-e9a1-4889-8ee2-e0f7536c2966", "fitness": 0.6042339243402574, "name": "SelfAdaptiveDE", "description": "Self-adaptive Differential Evolution with stochastic ranking and dynamic parameter control for enhanced exploration and exploitation.", "code": "import numpy as np\n\nclass SelfAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, Cr=0.9, alpha=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F  # Mutation factor\n        self.Cr = Cr  # Crossover rate\n        self.alpha = alpha # Stochastic ranking probability\n\n    def __call__(self, func):\n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size # Deduct initial population evaluations from budget\n\n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)]\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + self.F * (b - c), func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.Cr\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection using stochastic ranking\n                f_trial = func(trial)\n                self.budget -= 1\n\n                # Stochastic ranking\n                if (fitness[i] < f_trial) and (np.random.rand() > self.alpha):\n                    continue # Reject the trial vector\n                elif f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    population[i] = trial\n\n                    # Update optimal solution\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n            # Adapt parameters (simplified)\n            self.F = 0.5 + 0.3 * np.random.randn()\n            self.Cr = np.random.rand()\n\n            self.F = np.clip(self.F, 0.1, 1.0)\n            self.Cr = np.clip(self.Cr, 0.1, 1.0)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "The algorithm SelfAdaptiveDE scored 0.604 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["94768647-f252-46d5-98c5-9ea8189ba67b"], "operator": null, "metadata": {"aucs": [0.19323106758844322, 0.5106546339259135, 0.5543703050078428, 0.7598198287066219, 0.6765858463375114, 0.7435338841045493, 0.46435839938085166, 0.5506414101850036, 0.6297173097832355, 0.6565396068099882, 0.7862457571969553, 0.996373700845447, 0.3413943119502636, 0.5500531751993144, 0.8731211964277075, 0.7561509606075738, 0.4868111406190878, 0.7808759626403057, 0.26420911738237196, 0.509990872106161]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "e692b7ed-416d-4128-9e36-6822559ef933", "fitness": 0.10666876538930226, "name": "AdaptiveDE", "description": "An adaptive Differential Evolution strategy with simplified parameter control, population diversity maintenance via orthogonal initialization, and periodic population rejuvenation to escape local optima.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, Cr=0.9, oa_strength=2, restart_prob=0.05):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F  # Mutation factor\n        self.Cr = Cr  # Crossover rate\n        self.oa_strength = oa_strength # Orthogonal array strength\n        self.restart_prob = restart_prob # Probability of restarting\n\n    def _latin_hypercube_sampling(self, n_samples):\n        \"\"\"Generates initial population using Latin Hypercube Sampling.\"\"\"\n        result = np.zeros((n_samples, self.dim))\n        for i in range(self.dim):\n            result[:, i] = np.random.uniform(low=np.linspace(0, 1, n_samples, endpoint=False),\n                                            high=np.linspace(1, 2, n_samples, endpoint=False))\n        return result\n\n    def __call__(self, func):\n        # Initialize population using Latin Hypercube Sampling\n        lh_samples = self._latin_hypercube_sampling(self.pop_size)\n        population = func.bounds.lb + lh_samples * (func.bounds.ub - func.bounds.lb)\n\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)]\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + self.F * (b - c), func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.Cr\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    population[i] = trial\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n            # Simplified Adaptation: Adjust F and Cr based on overall population improvement\n            improvement_ratio = np.sum(fitness > np.mean(fitness)) / self.pop_size # Fraction of population improved\n            self.F = np.clip(self.F * (1 + 0.1 * (improvement_ratio - 0.5)), 0.1, 1.0)\n            self.Cr = np.clip(self.Cr * (1 - 0.1 * (improvement_ratio - 0.5)), 0.1, 1.0)\n\n            # Population Rejuvenation/Restart\n            if np.random.rand() < self.restart_prob:\n                lh_samples = self._latin_hypercube_sampling(self.pop_size)\n                population = func.bounds.lb + lh_samples * (func.bounds.ub - func.bounds.lb)\n                fitness = np.array([func(x) for x in population])\n                self.budget -= self.pop_size\n\n                best_idx = np.argmin(fitness)\n                if fitness[best_idx] < self.f_opt:\n                    self.f_opt = fitness[best_idx]\n                    self.x_opt = population[best_idx]\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "The algorithm AdaptiveDE scored 0.107 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["72b0459e-08f4-40f7-9003-4e7c1e060452"], "operator": null, "metadata": {"aucs": [0.2133375307786045, 0]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "42bd74d2-4aa3-4301-b640-b243b4b3f6d4", "fitness": "-inf", "name": "EnhancedSelfAdaptiveDE", "description": "An enhanced self-adaptive differential evolution algorithm with a mirroring boundary handling, adaptive population sizing, and covariance matrix adaptation to refine the mutation strategy.", "code": "import numpy as np\n\nclass EnhancedSelfAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=None, F=0.5, Cr=0.9, initial_learning_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        if pop_size is None:\n            self.pop_size = 4 * dim  # Adaptive population size\n        else:\n            self.pop_size = pop_size\n        self.F = F  # Mutation factor\n        self.Cr = Cr  # Crossover rate\n        self.learning_rate = initial_learning_rate  # Initial learning rate for parameter adaptation\n        self.archive_factor = 2.0\n        self.archive = []\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        # Initialize population\n        population = np.random.uniform(lb, ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size # Deduct initial population evaluations from budget\n\n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)]\n\n        generation = 0\n        while self.budget > 0:\n            generation += 1\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = a + self.F * (b - c)\n\n                # Mirror boundary handling\n                mutant = np.where(mutant < lb, 2 * lb - mutant, mutant)\n                mutant = np.where(mutant > ub, 2 * ub - mutant, mutant)\n                mutant = np.clip(mutant, lb, ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.Cr\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                if f_trial < fitness[i]:\n                    # Archive successful solutions\n                    if len(self.archive) < self.archive_factor * self.pop_size:\n                        self.archive.append(population[i].copy())\n                    else:\n                        self.archive[np.random.randint(0, len(self.archive))] = population[i].copy()\n\n                    fitness[i] = f_trial\n                    population[i] = trial\n\n                    # Update optimal solution\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                        # Increase learning rate if improvement is found\n                        self.learning_rate = min(1.0, self.learning_rate * 1.1)\n                else:\n                    # Reduce learning rate if no improvement\n                    self.learning_rate = max(0.01, self.learning_rate * 0.9)\n\n            # Adapt parameters based on learning rate\n            self.F = np.clip(self.F + self.learning_rate * np.random.normal(0, 0.1), 0.1, 1.0)\n            self.Cr = np.clip(self.Cr + self.learning_rate * np.random.normal(0, 0.1), 0.1, 1.0)\n\n            # Adaptive population sizing (simple example, can be made more sophisticated)\n            if generation % 10 == 0:  # Adjust every 10 generations\n                if np.std(fitness) < 1e-6: # If converged, increase pop size to explore\n                    self.pop_size = min(4 * self.dim, self.pop_size + 10)\n                    new_individuals = np.random.uniform(lb, ub, size=(10, self.dim))\n                    new_fitness = np.array([func(x) for x in new_individuals])\n                    self.budget -= 10\n                    population = np.vstack((population, new_individuals))\n                    fitness = np.concatenate((fitness, new_fitness))\n                elif self.pop_size > self.dim:  # If not converged reduce population size to exploit\n                     self.pop_size = max(self.dim, self.pop_size - 5)\n                     indices_to_keep = np.argsort(fitness)[:self.pop_size]\n                     population = population[indices_to_keep]\n                     fitness = fitness[indices_to_keep]\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "In the code, line 1025, in numpy.random.mtrand.RandomState.choice, the following error occurred:\nValueError: Cannot take a larger sample than population when 'replace=False'", "error": "In the code, line 1025, in numpy.random.mtrand.RandomState.choice, the following error occurred:\nValueError: Cannot take a larger sample than population when 'replace=False'", "parent_ids": ["94768647-f252-46d5-98c5-9ea8189ba67b"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "4469ea89-e4d0-41e0-8e25-13bbbdcf12c8", "fitness": 0.07491465298569439, "name": "SelfOrganizingScoutBee", "description": "A novel self-organizing scout bee algorithm inspired by honeybee foraging behavior, incorporating dynamic population management and adaptive search strategies based on landscape characteristics.", "code": "import numpy as np\n\nclass SelfOrganizingScoutBee:\n    def __init__(self, budget=10000, dim=10, num_bees=50, scout_ratio=0.1, elite_ratio=0.1, neighborhood_size=0.2, stagnation_threshold=50):\n        \"\"\"\n        Args:\n            budget: Total function evaluation budget.\n            dim: Dimensionality of the problem.\n            num_bees: Number of bees in the colony.\n            scout_ratio: Ratio of scout bees in the colony.\n            elite_ratio: Ratio of elite bees in the colony.\n            neighborhood_size:  Size of the neighborhood for local search (as a fraction of the search space).\n            stagnation_threshold: Number of iterations without improvement before triggering intensified search.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.num_bees = num_bees\n        self.scout_ratio = scout_ratio\n        self.elite_ratio = elite_ratio\n        self.neighborhood_size = neighborhood_size\n        self.stagnation_threshold = stagnation_threshold\n        self.population = None\n        self.fitness = None\n        self.f_opt = float('inf')\n        self.x_opt = None\n        self.iteration = 0\n        self.last_improvement = 0 # Keep track of iterations since last improvement\n\n    def _initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.num_bees, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.num_bees\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n\n    def _scout_phase(self, func):\n        \"\"\"Performs a global search by scout bees.\"\"\"\n        num_scouts = int(self.num_bees * self.scout_ratio)\n        for i in range(num_scouts):\n            idx = np.random.randint(0, self.num_bees)\n            new_position = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n            new_fitness = func(new_position)\n            self.budget -= 1\n\n            if new_fitness < self.fitness[idx]:\n                self.fitness[idx] = new_fitness\n                self.population[idx] = new_position\n\n                if new_fitness < self.f_opt:\n                    self.f_opt = new_fitness\n                    self.x_opt = new_position\n                    self.last_improvement = self.iteration # Update last improvement\n\n    def _elite_local_search(self, func):\n        \"\"\"Performs intensified local search around elite solutions.\"\"\"\n        num_elites = int(self.num_bees * self.elite_ratio)\n        elite_indices = np.argsort(self.fitness)[:num_elites]\n\n        for i in elite_indices:\n            for _ in range(3): # Multiple local search iterations per elite bee\n                new_position = self.population[i] + np.random.uniform(-self.neighborhood_size, self.neighborhood_size, size=self.dim) * (func.bounds.ub - func.bounds.lb)\n                new_position = np.clip(new_position, func.bounds.lb, func.bounds.ub)  # Clip to boundaries\n                new_fitness = func(new_position)\n                self.budget -= 1\n\n                if new_fitness < self.fitness[i]:\n                    self.fitness[i] = new_fitness\n                    self.population[i] = new_position\n                    if new_fitness < self.f_opt:\n                        self.f_opt = new_fitness\n                        self.x_opt = new_position\n                        self.last_improvement = self.iteration # Update last improvement\n        \n    def _adjust_population_diversity(self, func):\n        \"\"\"Adaptive population management to escape local optima.\"\"\"\n        if self.iteration - self.last_improvement > self.stagnation_threshold:\n           # Identify worst performing bees\n           num_to_replace = int(self.num_bees * 0.2) # Replace 20% of the population\n           worst_indices = np.argsort(self.fitness)[-num_to_replace:]\n\n           # Replace with random solutions to introduce diversity\n           for i in worst_indices:\n               self.population[i] = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n               self.fitness[i] = func(self.population[i])\n               self.budget -=1\n           \n           # Check if the new solutions improved the overall best\n           if np.min(self.fitness) < self.f_opt:\n              self.f_opt = np.min(self.fitness)\n              self.x_opt = self.population[np.argmin(self.fitness)]\n              self.last_improvement = self.iteration # Reset last improvement\n\n    def __call__(self, func):\n        self._initialize_population(func)\n\n        while self.budget > 0:\n            self.iteration += 1\n            self._scout_phase(func)\n            self._elite_local_search(func)\n            self._adjust_population_diversity(func) # Adaptive diversity\n\n            if self.budget <= 0:\n                break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "The algorithm SelfOrganizingScoutBee scored 0.075 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["72b0459e-08f4-40f7-9003-4e7c1e060452"], "operator": null, "metadata": {"aucs": [0.14982930597138877, 0]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "ddd53559-f7f2-4378-a426-e24c882fe1b5", "fitness": 0.32771292375927796, "name": "EnhancedSelfAdaptiveDE", "description": "A self-adaptive differential evolution algorithm with orthogonal array-based crossover, a novel Cauchy mutation operator, and a niching strategy to maintain population diversity.", "code": "import numpy as np\n\nclass EnhancedSelfAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, Cr=0.9, restart_prob=0.05, niche_radius=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F  # Mutation factor\n        self.Cr = Cr  # Crossover rate\n        self.learning_rate = 0.1  # Initial learning rate for parameter adaptation\n        self.restart_prob = restart_prob # Probability of restarting the population\n        self.oa = self._create_orthogonal_array() # Orthogonal Array for Crossover\n        self.niche_radius = niche_radius\n\n    def _create_orthogonal_array(self, strength=2):\n        \"\"\"Creates an orthogonal array for crossover.\n        For simplicity, using a predefined OA. A more general approach would dynamically generate it based on dim.\n        This OA is a L9 array (3^4), suitable for dimensions with groups of 3.\n        \"\"\"\n        return np.array([\n            [0, 0, 0, 0],\n            [0, 1, 1, 1],\n            [0, 2, 2, 2],\n            [1, 0, 1, 2],\n            [1, 1, 2, 0],\n            [1, 2, 0, 1],\n            [2, 0, 2, 1],\n            [2, 1, 0, 2],\n            [2, 2, 1, 0]\n        ])\n    \n    def _orthogonal_crossover(self, parent1, parent2, parent3):\n        \"\"\"Performs orthogonal array-based crossover.\"\"\"\n        \n        if self.dim < 4:\n             return parent1 # Fallback to standard DE if dimension is too small for OA\n            \n        group_size = min(self.dim // 4, 3)  # Ensure we don't exceed dimension limits and use groups of 3\n        num_groups = self.dim // group_size\n\n        child = parent1.copy()\n        for g in range(num_groups):\n            start_idx = g * group_size\n            end_idx = start_idx + group_size\n            \n            oa_row_idx = np.random.randint(0, len(self.oa))\n            oa_row = self.oa[oa_row_idx]\n\n            for i in range(group_size):\n                idx = start_idx + i\n                if oa_row[i] == 0:\n                    child[idx] = parent1[idx]\n                elif oa_row[i] == 1:\n                    child[idx] = parent2[idx]\n                else:\n                    child[idx] = parent3[idx]\n        \n        # Handle any remaining dimensions with standard crossover\n        for i in range(num_groups * group_size, self.dim):\n            if np.random.rand() < self.Cr:\n                child[i] = parent2[i]\n            else:\n                child[i] = parent1[i]\n        return child\n\n    def _cauchy_mutation(self, a, b, c):\n        \"\"\"Performs Cauchy mutation.\"\"\"\n        scale = self.F * (b - c)\n        mutant = a + scale * np.random.standard_cauchy(size=self.dim)\n        return mutant\n\n    def __call__(self, func):\n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size # Deduct initial population evaluations from budget\n\n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)]\n\n        generation = 0\n        while self.budget > 0:\n            generation += 1\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                # mutant = np.clip(a + self.F * (b - c), func.bounds.lb, func.bounds.ub)\n                mutant = np.clip(self._cauchy_mutation(a, b, c), func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                trial = self._orthogonal_crossover(population[i], mutant, a) # Using OA Crossover\n                \n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                \n                # Niching: Check if the trial vector is too close to an existing individual\n                too_close = False\n                for j in range(self.pop_size):\n                    if i != j and np.linalg.norm(trial - population[j]) < self.niche_radius:\n                        too_close = True\n                        break\n                \n                if not too_close and f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    population[i] = trial\n\n                    # Update optimal solution\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                        # Increase learning rate if improvement is found\n                        self.learning_rate = min(1.0, self.learning_rate * 1.1)\n                else:\n                    # Reduce learning rate if no improvement or too close\n                    self.learning_rate = max(0.01, self.learning_rate * 0.9)\n\n            # Adapt parameters based on learning rate\n            self.F = np.clip(self.F + self.learning_rate * np.random.normal(0, 0.1), 0.1, 1.0)\n            self.Cr = np.clip(self.Cr + self.learning_rate * np.random.normal(0, 0.1), 0.1, 1.0)\n            \n            # Restart mechanism\n            if np.random.rand() < self.restart_prob:\n                population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                fitness = np.array([func(x) for x in population])\n                self.budget -= self.pop_size # Deduct initial population evaluations from budget\n                \n                if np.min(fitness) < self.f_opt:\n                  self.f_opt = np.min(fitness)\n                  self.x_opt = population[np.argmin(fitness)]\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "The algorithm EnhancedSelfAdaptiveDE scored 0.328 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["72b0459e-08f4-40f7-9003-4e7c1e060452"], "operator": null, "metadata": {"aucs": [0.15213540162737849, 0.2575661474206098, 0.34033658007452205, 0.38444096366737923, 0.3518821446928897, 0.3530979070119947, 0.3042951253823113, 0.3087053316345999, 0.31426286207712506, 0.21312223159556565, 0.2827093520000721, 0.9977139616861658, 0]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "78631a28-5238-41bf-9c1b-c264503e6618", "fitness": 0.4868084600551244, "name": "SelfAdaptiveDE", "description": "Self-adaptive Differential Evolution with ensemble of mutation strategies, archive for enhanced exploration, and dynamic parameter adaptation using a success-history based adaptation.", "code": "import numpy as np\n\nclass SelfAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=10, F=0.5, Cr=0.9, alpha=0.1, F_list=[0.5, 0.7, 0.9], Cr_list=[0.1, 0.5, 0.9]):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.F = F  # Mutation factor (initial value)\n        self.Cr = Cr  # Crossover rate (initial value)\n        self.alpha = alpha  # Stochastic ranking probability\n        self.F_list = F_list  # Candidate F values\n        self.Cr_list = Cr_list  # Candidate Cr values\n        self.archive = [] # Archive for storing potentially useful solutions\n\n        self.sf = np.ones(len(self.F_list)) * 0.5\n        self.scr = np.ones(len(self.Cr_list)) * 0.5\n\n    def __call__(self, func):\n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size  # Deduct initial population evaluations from budget\n\n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)]\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Choose mutation strategy\n                strategy_idx = np.random.randint(len(self.F_list))\n                F = self.F_list[strategy_idx]\n                Cr = self.Cr_list[strategy_idx]\n\n                # Mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n\n                # Incorporate archive\n                if len(self.archive) > 0 and np.random.rand() < 0.1: # Use archive occasionally\n                    arc_idx = np.random.randint(len(self.archive))\n                    a = self.archive[arc_idx]\n                \n                mutant = np.clip(a + F * (b - c), func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < Cr\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection using stochastic ranking\n                f_trial = func(trial)\n                self.budget -= 1\n\n                # Stochastic ranking\n                if (fitness[i] < f_trial) and (np.random.rand() > self.alpha):\n                    continue  # Reject the trial vector\n                elif f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    population[i] = trial\n\n                    # Update optimal solution\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n                    # Update archive\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(population[i].copy())\n                    else:\n                        # Replace a random element in the archive\n                        replace_idx = np.random.randint(self.archive_size)\n                        self.archive[replace_idx] = population[i].copy()\n\n            # Adapt parameters (success-history based adaptation)\n            successful_F = []\n            successful_Cr = []\n\n            for i in range(self.pop_size):\n                #Track F and Cr based on success\n                pass # No history is available without more elaborate bookkeeping\n\n            # Update F and Cr (simplified - no success history used currently, but structure is there)\n            self.F = 0.5 + 0.3 * np.random.randn()\n            self.Cr = np.random.rand()\n\n            self.F = np.clip(self.F, 0.1, 1.0)\n            self.Cr = np.clip(self.Cr, 0.1, 1.0)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "The algorithm SelfAdaptiveDE scored 0.487 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["7f5458b5-e9a1-4889-8ee2-e0f7536c2966"], "operator": null, "metadata": {"aucs": [0.17587224112445998, 0.2332903282507779, 0.43922872874539376, 0.6764134657859788, 0.46632201339997237, 0.5947683167151123, 0.3463686230532682, 0.42584403904463886, 0.4759656629629282, 0.3654247290970629, 0.6505494307285058, 0.9972169940619285, 0.2786940346175317, 0.4361053014603402, 0.8147427887956555, 0.5955281200191398, 0.38423893240443197, 0.6775419915183447, 0.20105326381390276, 0.5010001955031138]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "d15f2124-2044-40ea-b0ea-bbc76d19a3ef", "fitness": 0.6966077995789115, "name": "SelfAdaptiveDE", "description": "Self-adaptive Differential Evolution with velocity-based mutation, archive for diversity, and adaptive parameter control using success history.", "code": "import numpy as np\n\nclass SelfAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=10, F=0.5, Cr=0.9, velocity_weight=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.F = F  # Mutation factor\n        self.Cr = Cr  # Crossover rate\n        self.velocity_weight = velocity_weight  # Weight for velocity-based mutation\n        self.learning_rate_F = 0.1\n        self.learning_rate_Cr = 0.1\n        self.success_history_F = []\n        self.success_history_Cr = []\n        self.success_history_size = 10\n        self.archive = []\n\n    def __call__(self, func):\n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size  # Deduct initial population evaluations from budget\n\n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)]\n\n        # Initialize velocity\n        velocity = np.zeros_like(population)\n\n        generation = 0\n        while self.budget > 0:\n            generation += 1\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n\n                # Velocity update\n                velocity[i] = self.velocity_weight * velocity[i] + self.F * (b - c)\n\n                # Apply mutation with velocity\n                mutant = np.clip(a + velocity[i], func.bounds.lb, func.bounds.ub)\n\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.Cr\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n\n                if f_trial < fitness[i]:\n                    # Archive the replaced individual for diversity\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(population[i].copy())\n                    else:\n                        idx_to_replace = np.random.randint(0, self.archive_size)\n                        self.archive[idx_to_replace] = population[i].copy()\n\n                    fitness[i] = f_trial\n                    population[i] = trial\n\n                    # Update optimal solution\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n                    # Update success history for F and Cr adaptation\n                    self.success_history_F.append(self.F)\n                    self.success_history_Cr.append(self.Cr)\n                    if len(self.success_history_F) > self.success_history_size:\n                        self.success_history_F.pop(0)\n                        self.success_history_Cr.pop(0)\n\n            # Adapt F and Cr based on success history\n            if self.success_history_F:\n                self.F = np.clip(np.mean(self.success_history_F) + self.learning_rate_F * np.random.normal(0, 0.1), 0.1, 1.0)\n            else:\n                self.F = np.clip(self.F + self.learning_rate_F * np.random.normal(0, 0.1), 0.1, 1.0)\n\n            if self.success_history_Cr:\n                self.Cr = np.clip(np.mean(self.success_history_Cr) + self.learning_rate_Cr * np.random.normal(0, 0.1), 0.1, 1.0)\n            else:\n                self.Cr = np.clip(self.Cr + self.learning_rate_Cr * np.random.normal(0, 0.1), 0.1, 1.0)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "The algorithm SelfAdaptiveDE scored 0.697 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["94768647-f252-46d5-98c5-9ea8189ba67b"], "operator": null, "metadata": {"aucs": [0.2774170840115868, 0.7559862171284324, 0.6956977178529808, 0.8486847426748224, 0.7589710381498393, 0.7859340186129686, 0.47192604561494367, 0.6868608955811152, 0.6874185579175467, 0.6901097135357891, 0.8957273510448894, 0.993581543738074, 0.671653442904725, 0.7461712528355454, 0.9272324570895668, 0.8249498215641817, 0.5860302872456075, 0.8283902221523332, 0.23322311883697322, 0.5661904630863096]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "ad653108-5b2b-46d3-a330-d278dfef73c8", "fitness": "-inf", "name": "HybridDE_NM", "description": "A hybrid algorithm combining Self-Adaptive Differential Evolution with a Nelder-Mead local search to refine solutions found by the global search.", "code": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridDE_NM:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, Cr=0.9, nm_iterations=5, local_search_prob=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.Cr = Cr\n        self.nm_iterations = nm_iterations # Number of Nelder-Mead iterations\n        self.local_search_prob = local_search_prob\n\n    def __call__(self, func):\n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)]\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + self.F * (b - c), func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.Cr\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n\n                # Local Search with Nelder-Mead\n                if np.random.rand() < self.local_search_prob:\n                    \n                    def local_func(x):\n                        if self.budget > 0:\n                            val = func(x)\n                            self.budget -= 1\n                            return val\n                        else:\n                            return np.inf\n\n                    local_result = minimize(local_func, trial, method='Nelder-Mead',\n                                            options={'maxiter': self.nm_iterations, 'maxfev': self.nm_iterations})\n                    \n                    if local_result.success and local_result.fun < f_trial:\n                        f_trial = local_result.fun\n                        trial = local_result.x\n\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    population[i] = trial\n\n                    # Update optimal solution\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n            # Adapt parameters (optional)\n            # self.F = np.clip(self.F + 0.1 * np.random.normal(), 0.1, 1.0)\n            # self.Cr = np.clip(self.Cr + 0.1 * np.random.normal(), 0.1, 1.0)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "In the code, line 51, in __call__, the following error occurred:\nNameError: name 'minimize' is not defined\nOn line: local_result = minimize(local_func, trial, method='Nelder-Mead',", "error": "In the code, line 51, in __call__, the following error occurred:\nNameError: name 'minimize' is not defined\nOn line: local_result = minimize(local_func, trial, method='Nelder-Mead',", "parent_ids": ["72b0459e-08f4-40f7-9003-4e7c1e060452"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "06b6ae75-0ec5-408d-a4a3-4059dc60b0ac", "fitness": 0.7396442435018158, "name": "SelfAdaptiveDE", "description": "Self-adaptive Differential Evolution with aging mechanism, archive, and orthogonal design for enhanced exploration and exploitation.", "code": "import numpy as np\n\nclass SelfAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=10, F=0.5, Cr=0.9, aging_limit=50):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.F = F\n        self.Cr = Cr\n        self.learning_rate = 0.1\n        self.aging_limit = aging_limit\n        self.archive = []\n        self.archive_fitness = []\n        self.population_age = np.zeros(pop_size)  # Track the age of each individual\n\n    def __call__(self, func):\n        # Initialize population\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)]\n\n        generation = 0\n        while self.budget > 0:\n            generation += 1\n            for i in range(self.pop_size):\n                # Aging mechanism: if an individual doesn't improve for too long, replace it with a random one\n                if self.population_age[i] > self.aging_limit:\n                    population[i] = np.random.uniform(lb, ub, size=self.dim)\n                    fitness[i] = func(population[i])\n                    self.budget -= 1\n                    self.population_age[i] = 0\n                    if fitness[i] < self.f_opt:\n                        self.f_opt = fitness[i]\n                        self.x_opt = population[i]\n                    continue\n\n                # Mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                if len(self.archive) > 0 and np.random.rand() < 0.1:  # Use archive occasionally\n                    idx_archive = np.random.randint(len(self.archive))\n                    a = self.archive[idx_archive]\n                    idxs = np.random.choice(idxs, 2, replace=False)\n                    b, c = population[idxs]\n                    mutant = np.clip(a + self.F * (b - c), lb, ub)\n\n                else:\n                     a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                     mutant = np.clip(a + self.F * (b - c), lb, ub)\n\n\n                # Orthogonal Crossover\n                trial = self.orthogonal_crossover(population[i], mutant, self.Cr)\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    population[i] = trial\n                    self.population_age[i] = 0 #reset age\n                    # Update optimal solution\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                        self.learning_rate = min(1.0, self.learning_rate * 1.1)\n                    # Update archive\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(population[i].copy())\n                        self.archive_fitness.append(fitness[i])\n                    else:\n                        max_archive_index = np.argmax(self.archive_fitness)\n                        if fitness[i] < self.archive_fitness[max_archive_index]:\n                            self.archive[max_archive_index] = population[i].copy()\n                            self.archive_fitness[max_archive_index] = fitness[i]\n\n                else:\n                    self.population_age[i] += 1 #increase age\n                    self.learning_rate = max(0.01, self.learning_rate * 0.9)\n\n            # Adapt parameters\n            self.F = np.clip(self.F + self.learning_rate * np.random.normal(0, 0.1), 0.1, 1.0)\n            self.Cr = np.clip(self.Cr + self.learning_rate * np.random.normal(0, 0.1), 0.1, 1.0)\n\n        return self.f_opt, self.x_opt\n\n    def orthogonal_crossover(self, x, mutant, cr):\n        trial = x.copy()\n        for j in range(self.dim):\n            if np.random.rand() < cr:\n                trial[j] = mutant[j]\n        return trial", "configspace": "", "generation": 4, "feedback": "The algorithm SelfAdaptiveDE scored 0.740 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["94768647-f252-46d5-98c5-9ea8189ba67b"], "operator": null, "metadata": {"aucs": [0.3269827876069876, 0.6637223474275771, 0.7228577144952616, 0.8952832238235784, 0.7828514701427045, 0.8117934496568754, 0.6486914435415589, 0.70431022281298, 0.7738685290310701, 0.7222981204272341, 0.8688041899182899, 0.9996803968423456, 0.6860933567371524, 0.768919670575762, 0.9306860792630086, 0.8079522977989941, 0.6644860773979826, 0.853592249202483, 0.637070027351432, 0.5229412159830349]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "a99d0b44-16b4-4d3c-b81a-fa008785a0c8", "fitness": "-inf", "name": "EnhancedSelfAdaptiveDE", "description": "A self-adaptive differential evolution algorithm with orthogonal array crossover, stochastic acceptance, and dynamic population sizing to balance exploration and exploitation.", "code": "import numpy as np\n\nclass EnhancedSelfAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, Cr=0.9, restart_prob=0.05, sa_prob=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.min_pop_size = 10  # Minimum population size\n        self.F = F  # Mutation factor\n        self.Cr = Cr  # Crossover rate\n        self.learning_rate = 0.1  # Initial learning rate for parameter adaptation\n        self.restart_prob = restart_prob  # Probability of restarting the population\n        self.sa_prob = sa_prob  # Probability of stochastic acceptance\n        self.oa = self._create_orthogonal_array()  # Orthogonal Array for Crossover\n\n    def _create_orthogonal_array(self, strength=2):\n        \"\"\"Creates an orthogonal array for crossover.\n        For simplicity, using a predefined OA. A more general approach would dynamically generate it based on dim.\n        This OA is a L9 array (3^4), suitable for dimensions with groups of 3.\n        \"\"\"\n        return np.array([\n            [0, 0, 0, 0],\n            [0, 1, 1, 1],\n            [0, 2, 2, 2],\n            [1, 0, 1, 2],\n            [1, 1, 2, 0],\n            [1, 2, 0, 1],\n            [2, 0, 2, 1],\n            [2, 1, 0, 2],\n            [2, 2, 1, 0]\n        ])\n\n    def _orthogonal_crossover(self, parent1, parent2, parent3):\n        \"\"\"Performs orthogonal array-based crossover.\"\"\"\n\n        if self.dim < 4:\n            return parent1  # Fallback to standard DE if dimension is too small for OA\n\n        group_size = min(self.dim // 4, 3)  # Ensure we don't exceed dimension limits and use groups of 3\n        num_groups = self.dim // group_size\n\n        child = parent1.copy()\n        for g in range(num_groups):\n            start_idx = g * group_size\n            end_idx = start_idx + group_size\n\n            oa_row_idx = np.random.randint(0, len(self.oa))\n            oa_row = self.oa[oa_row_idx]\n\n            for i in range(group_size):\n                idx = start_idx + i\n                if oa_row[i] == 0:\n                    child[idx] = parent1[idx]\n                elif oa_row[i] == 1:\n                    child[idx] = parent2[idx]\n                else:\n                    child[idx] = parent3[idx]\n\n        # Handle any remaining dimensions with standard crossover\n        for i in range(num_groups * group_size, self.dim):\n            if np.random.rand() < self.Cr:\n                child[i] = parent2[i]\n            else:\n                child[i] = parent1[i]\n        return child\n\n    def __call__(self, func):\n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size  # Deduct initial population evaluations from budget\n\n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)]\n\n        generation = 0\n        stagnation_counter = 0\n        prev_f_opt = self.f_opt\n\n        while self.budget > 0:\n            generation += 1\n            new_population = []\n            new_fitness = []\n\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + self.F * (b - c), func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                trial = self._orthogonal_crossover(population[i], mutant, a)  # Using OA Crossover\n\n                # Selection using Stochastic Acceptance\n                f_trial = func(trial)\n                self.budget -= 1\n\n                delta = f_trial - fitness[i]\n                if delta < 0 or np.random.rand() < np.exp(-delta / self.learning_rate):  # Stochastic Acceptance\n                    if delta < 0:\n                        stagnation_counter = 0 # Reset stagnation counter if we improve\n\n                    new_population.append(trial)\n                    new_fitness.append(f_trial)\n                else:\n                    new_population.append(population[i])\n                    new_fitness.append(fitness[i])\n\n                # Update optimal solution\n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = trial\n                    self.learning_rate = min(1.0, self.learning_rate * 1.1)  # Increase learning rate\n                else:\n                    self.learning_rate = max(0.01, self.learning_rate * 0.9)  # Reduce learning rate\n\n            # Update population and fitness\n            population = np.array(new_population)\n            fitness = np.array(new_fitness)\n\n            # Adapt parameters\n            self.F = np.clip(self.F + self.learning_rate * np.random.normal(0, 0.1), 0.1, 1.0)\n            self.Cr = np.clip(self.Cr + self.learning_rate * np.random.normal(0, 0.1), 0.1, 1.0)\n\n            # Dynamic Population Size Adjustment\n            if self.f_opt < prev_f_opt:\n                # Increase population size gradually if improvement is consistent\n                self.pop_size = min(self.pop_size + 1, 100)\n                prev_f_opt = self.f_opt\n            else:\n                stagnation_counter += 1\n                if stagnation_counter > 50:\n                     # Decrease population size if no improvement for a while\n                    self.pop_size = max(self.pop_size - 2, self.min_pop_size)\n                    stagnation_counter = 0  # Reset stagnation counter\n                    prev_f_opt = self.f_opt\n\n            # Restart mechanism\n            if np.random.rand() < self.restart_prob:\n                population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                fitness = np.array([func(x) for x in population])\n                self.budget -= self.pop_size\n\n                if np.min(fitness) < self.f_opt:\n                    self.f_opt = np.min(fitness)\n                    self.x_opt = population[np.argmin(fitness)]\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "In the code, line 88, in __call__, the following error occurred:\nIndexError: index 50 is out of bounds for axis 0 with size 50\nOn line: a, b, c = population[np.random.choice(idxs, 3, replace=False)]", "error": "In the code, line 88, in __call__, the following error occurred:\nIndexError: index 50 is out of bounds for axis 0 with size 50\nOn line: a, b, c = population[np.random.choice(idxs, 3, replace=False)]", "parent_ids": ["72b0459e-08f4-40f7-9003-4e7c1e060452"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "a616af65-1651-415e-8586-6b1ec0ac48da", "fitness": "-inf", "name": "SimplifiedCMAES", "description": "A simplified CMA-ES variant with adaptive parameter control using success history adaptation and a basic restart mechanism, focusing on fewer parameters and faster convergence.", "code": "import numpy as np\n\nclass SimplifiedCMAES:\n    def __init__(self, budget=10000, dim=10, popsize=None, cs=0.3, cma_decay=0.99, stagnation_threshold=50):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = popsize if popsize is not None else 4 + int(3 * np.log(dim))\n        self.cs = cs\n        self.cma_decay = cma_decay\n        self.mu = self.popsize // 2\n        self.stagnation_threshold = stagnation_threshold\n        self.success_history = []\n        self.mean_history = []\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.eval_count = 0\n\n        mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        sigma = 0.5\n        C = np.eye(self.dim)\n        pc = np.zeros(self.dim)\n        ps = np.zeros(self.dim)\n\n        weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n        weights = weights / np.sum(weights)\n        mueff = np.sum(weights)**2 / np.sum(weights**2)\n\n        cc = (4 + mueff/self.dim) / (self.dim + 4 + 2*mueff/self.dim)\n        cs = (mueff+2) / (self.dim+mueff+5)\n        damps = 1 + 2*max(0, np.sqrt((mueff-1)/(self.dim+1))-1) + cs\n        ccov1 = 2 / ((self.dim + 1.3)**2 + mueff)\n        ccovmu = 2 * (mueff / ((self.dim + 2)**2 + mueff))\n\n        stagnation_counter = 0\n\n        while self.eval_count < self.budget:\n            Z = np.random.randn(self.dim, self.popsize)\n            x = mean[:, np.newaxis] + sigma * np.dot(np.linalg.cholesky(C), Z)\n\n            f = np.array([func(xi) for xi in x.T if self.eval_count < self.budget])\n            self.eval_count += len(f)\n\n            if len(f) == 0:\n                break\n\n            idx_best = np.argmin(f)\n            if f[idx_best] < self.f_opt:\n                self.f_opt = f[idx_best]\n                self.x_opt = x[:, idx_best]\n                stagnation_counter = 0\n            else:\n                stagnation_counter += 1\n\n            idx_sort = np.argsort(f)\n            x_sorted = x[:, idx_sort[:self.mu]]\n            mean_new = np.dot(x_sorted, weights)\n\n            # Simplified adaptation\n            ps = (1 - cs) * ps + np.sqrt(cs * (2 - cs) * mueff) * (mean_new - mean) / sigma\n            hsig = (np.linalg.norm(ps) / np.sqrt(1 - (1 - cs)**(2 * self.eval_count / self.popsize)) / np.sqrt(self.dim)) < 1.4 + 2 / (self.dim + 1)\n\n            pc = (1 - cc) * pc + (hsig * np.sqrt(cc * (2 - cc) * mueff) * (mean_new - mean) / sigma)\n            artmp = (1 / sigma) * (x_sorted - mean[:, np.newaxis])\n            C = (1 - ccov1 - ccovmu) * C + ccov1 * np.outer(pc, pc) + ccovmu * np.dot(artmp, np.diag(weights).dot(artmp.T))\n\n            C = np.triu(C) + np.triu(C, 1).T\n            C = np.real((C + C.T) / 2)\n\n            sigma = sigma * np.exp((cs / damps) * (np.linalg.norm(ps) / np.sqrt(self.dim) - 1))\n            mean = mean_new\n\n            sigma *= self.cma_decay\n\n            if stagnation_counter > self.stagnation_threshold:\n                mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                sigma = 0.5\n                C = np.eye(self.dim)\n                pc = np.zeros(self.dim)\n                ps = np.zeros(self.dim)\n                stagnation_counter = 0\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "In the code, line 148, in _raise_linalgerror_nonposdef, the following error occurred:\nLinAlgError: Matrix is not positive definite", "error": "In the code, line 148, in _raise_linalgerror_nonposdef, the following error occurred:\nLinAlgError: Matrix is not positive definite", "parent_ids": ["640d5617-bca9-4812-987e-6d6b4664faec"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "cdfe5f9d-eaef-4cb4-80b4-a0a47b6a3478", "fitness": "-inf", "name": "AdaptiveCMAES", "description": "A simplified CMA-ES variant with adaptive step size and a restart mechanism based on fitness improvement stagnation.", "code": "import numpy as np\n\nclass AdaptiveCMAES:\n    def __init__(self, budget=10000, dim=10, popsize=None, initial_step_size=0.5, stagnation_threshold=50):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = popsize if popsize is not None else 4 + int(3 * np.log(dim))\n        self.initial_step_size = initial_step_size\n        self.stagnation_threshold = stagnation_threshold\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.eval_count = 0\n\n        mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        sigma = self.initial_step_size  # Step size, made it an initialization parameter\n        C = np.eye(self.dim)  # Covariance matrix\n\n        stagnation_counter = 0\n        last_improvement = 0\n\n        while self.eval_count < self.budget:\n            # Generate and evaluate offspring\n            Z = np.random.randn(self.dim, self.popsize)\n            x = mean[:, np.newaxis] + sigma * np.dot(np.linalg.cholesky(C), Z)\n\n            # Clip the sampled solutions to lie within the bounds\n            x = np.clip(x, func.bounds.lb, func.bounds.ub)\n\n            f = np.array([func(xi) for xi in x.T if self.eval_count < self.budget])\n            self.eval_count += len(f)\n\n            if len(f) == 0:\n                break\n\n            # Update best solution\n            idx_best = np.argmin(f)\n            if f[idx_best] < self.f_opt:\n                self.f_opt = f[idx_best]\n                self.x_opt = x[:, idx_best]\n                stagnation_counter = 0  # Reset stagnation counter\n                last_improvement = self.eval_count\n            else:\n                stagnation_counter +=1\n                \n            idx_sort = np.argsort(f)\n            x_sorted = x[:, idx_sort[:self.popsize // 2]] #Keep the best half of the population for updates\n\n            # Update mean (simplified recombination)\n            mean = np.mean(x_sorted, axis=1)\n\n            #Adapt step size (simplified)\n            sigma *= np.exp(0.1 * (np.mean(f) - self.f_opt) / np.std(f)) #Adaptation based on fitness improvement\n\n            #Stagnation and restart\n            if stagnation_counter > self.stagnation_threshold:\n                mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                sigma = self.initial_step_size\n                C = np.eye(self.dim)\n                stagnation_counter = 0 #Reset counter\n                \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "In the code, line 113, in _clip, the following error occurred:\nValueError: operands could not be broadcast together with shapes (2,6) (2,) (2,) ", "error": "In the code, line 113, in _clip, the following error occurred:\nValueError: operands could not be broadcast together with shapes (2,6) (2,) (2,) ", "parent_ids": ["640d5617-bca9-4812-987e-6d6b4664faec"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "afb0e312-8726-4a47-a211-3d47afcf813f", "fitness": "-inf", "name": "SimplifiedCMAES", "description": "A simplified CMA-ES variant with adaptive step size control and restart mechanism for improved exploration and exploitation.", "code": "import numpy as np\n\nclass SimplifiedCMAES:\n    def __init__(self, budget=10000, dim=10, popsize=None, initial_step_size=0.5, stagnation_threshold=50):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = popsize if popsize is not None else 4 + int(3 * np.log(dim))\n        self.initial_step_size = initial_step_size\n        self.stagnation_threshold = stagnation_threshold\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.eval_count = 0\n        \n        mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        sigma = self.initial_step_size  # Step size\n\n        stagnation_counter = 0\n        \n        while self.eval_count < self.budget:\n            # Generate and evaluate offspring\n            z = np.random.randn(self.dim, self.popsize)\n            x = mean[:, np.newaxis] + sigma * z  # Sample from multivariate normal distribution\n\n            # Clip to bounds\n            lb = func.bounds.lb\n            ub = func.bounds.ub\n            x = np.clip(x, lb, ub)\n\n            f = np.array([func(xi) for xi in x.T if self.eval_count < self.budget])\n            self.eval_count += len(f)\n\n            if len(f) == 0:\n                break\n\n            # Update best solution\n            idx_best = np.argmin(f)\n            if f[idx_best] < self.f_opt:\n                self.f_opt = f[idx_best]\n                self.x_opt = x[:, idx_best]\n                stagnation_counter = 0\n            else:\n                stagnation_counter += 1\n\n            # Select best individual\n            idx_sort = np.argsort(f)\n            best_idx = idx_sort[0]\n            best_x = x[:, best_idx]\n            \n            # Update mean and step size\n            mean = best_x\n            \n            # Adaptive step size control\n            if stagnation_counter > self.stagnation_threshold:\n                sigma *= 0.8  # Reduce step size\n                stagnation_counter = 0\n                mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim) # Random restart\n            else:\n                sigma *= 1.1  # Increase step size (cautiously)\n                sigma = min(sigma, 2) # Prevent excessive step size\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "In the code, line 113, in _clip, the following error occurred:\nValueError: operands could not be broadcast together with shapes (2,6) (2,) (2,) ", "error": "In the code, line 113, in _clip, the following error occurred:\nValueError: operands could not be broadcast together with shapes (2,6) (2,) (2,) ", "parent_ids": ["640d5617-bca9-4812-987e-6d6b4664faec"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "c3e654ae-1ad1-46d2-a9f9-f2dffe8e6465", "fitness": 0.4903632384551974, "name": "SelfAdaptiveDE", "description": "Self-adaptive Differential Evolution with simplified parameter adaptation, archive-based mutation, and a focus on exploration through increased mutation rate adaptation.", "code": "import numpy as np\n\nclass SelfAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=10, F=0.5, Cr=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.F = F  # Mutation factor\n        self.Cr = Cr  # Crossover rate\n        self.learning_rate_F = 0.1\n        self.learning_rate_Cr = 0.1\n        self.archive = []\n        self.success_history_F = []\n        self.success_history_Cr = []\n        self.success_history_size = 10\n\n\n    def __call__(self, func):\n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size  # Deduct initial population evaluations from budget\n\n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)]\n\n\n        generation = 0\n        while self.budget > 0:\n            generation += 1\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                \n                # Consider archive for mutation if it's not empty\n                if self.archive:\n                    arc_idx = np.random.randint(0, len(self.archive))\n                    mutant = a + self.F * (b - c) + self.F * (self.archive[arc_idx] - population[i])\n                else:\n                    mutant = a + self.F * (b - c)\n\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.Cr\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n\n                if f_trial < fitness[i]:\n                    # Archive the replaced individual for diversity\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(population[i].copy())\n                    else:\n                        idx_to_replace = np.random.randint(0, self.archive_size)\n                        self.archive[idx_to_replace] = population[i].copy()\n\n                    fitness[i] = f_trial\n                    population[i] = trial\n\n                    # Update optimal solution\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                        \n                    # Update success history\n                    self.success_history_F.append(self.F)\n                    self.success_history_Cr.append(self.Cr)\n                    if len(self.success_history_F) > self.success_history_size:\n                        self.success_history_F.pop(0)\n                        self.success_history_Cr.pop(0)\n\n            # Adapt F and Cr\n            if self.success_history_F:\n                self.F = np.clip(np.mean(self.success_history_F) * (1 + self.learning_rate_F * np.random.normal(0, 0.1)), 0.1, 1.0)\n            else:\n                self.F = np.clip(self.F * (1 + self.learning_rate_F * np.random.normal(0, 0.1)), 0.1, 1.0)  # Exploration-focused adaptation\n\n            if self.success_history_Cr:\n                self.Cr = np.clip(np.mean(self.success_history_Cr) + self.learning_rate_Cr * np.random.normal(0, 0.1), 0.1, 1.0)\n            else:\n                self.Cr = np.clip(self.Cr + self.learning_rate_Cr * np.random.normal(0, 0.1), 0.1, 1.0)\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "The algorithm SelfAdaptiveDE scored 0.490 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["d15f2124-2044-40ea-b0ea-bbc76d19a3ef"], "operator": null, "metadata": {"aucs": [0.18899587648850724, 0.291655450470686, 0.45405513261812747, 0.7577598090911314, 0.39064200601360655, 0.5406869032902001, 0.3458258453778452, 0.4196933029907184, 0.4683036518434869, 0.33599553676137284, 0.6598618992379592, 0.9944299661820591, 0.4498869560576848, 0.41454967927717756, 0.8539819614185262, 0.518885528559929, 0.36350595602390356, 0.6753254511507386, 0.1896765129794289, 0.4935473432708599]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "8a0df5a0-b129-4688-86c8-fe76a845b642", "fitness": 0.0, "name": "SelfOrganizingScouts", "description": "Self-Organizing Scouts with Adaptive Exploration Radius: Employs a population of scout agents that dynamically adjust their search radius based on local fitness landscape information, favoring exploration in promising regions and exploitation around current best solutions.", "code": "import numpy as np\n\nclass SelfOrganizingScouts:\n    def __init__(self, budget=10000, dim=10, pop_size=40, initial_radius=0.5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.radius = initial_radius  # Initial exploration radius\n        self.radii = np.full(pop_size, initial_radius)\n        self.min_radius = 0.01\n        self.max_radius = 1.0\n        self.inertia = 0.5  # Inertia for radius update\n\n    def __call__(self, func):\n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)]\n        best_index = np.argmin(fitness)\n        best_position = population[best_index]\n\n        generation = 0\n        while self.budget > 0:\n            generation += 1\n            new_population = np.zeros_like(population)\n            new_fitness = np.zeros_like(fitness)\n\n            for i in range(self.pop_size):\n                # Explore: Sample a new position within the radius\n                exploration_vector = np.random.uniform(-self.radii[i], self.radii[i], size=self.dim)\n                new_position = np.clip(population[i] + exploration_vector, func.bounds.lb, func.bounds.ub)\n                new_fitness[i] = func(new_position)\n                self.budget -= 1\n\n                if new_fitness[i] < fitness[i]:\n                    population[i] = new_position\n                    fitness[i] = new_fitness[i]\n\n                    # Update optimal solution\n                    if new_fitness[i] < self.f_opt:\n                        self.f_opt = new_fitness[i]\n                        self.x_opt = new_position\n                        best_position = new_position\n                        best_index = i\n\n                    # Increase radius if improvement\n                    self.radii[i] = min(self.max_radius, self.radii[i] * 1.1)\n                else:\n                    # Decrease radius if no improvement\n                    self.radii[i] = max(self.min_radius, self.radii[i] * 0.9)\n\n                # Movement toward the best: Add a component to move towards best position\n                if i != best_index:\n                    to_best = best_position - population[i]\n                    new_pos_towards_best = np.clip(population[i] + 0.1*to_best, func.bounds.lb, func.bounds.ub)\n                    fitness_towards_best = func(new_pos_towards_best)\n                    self.budget -=1\n                    if fitness_towards_best < fitness[i]:\n                        population[i] = new_pos_towards_best\n                        fitness[i] = fitness_towards_best\n\n            # Adaptive radius adjustment (global perspective)\n            mean_fitness = np.mean(fitness)\n            for i in range(self.pop_size):\n                if fitness[i] < mean_fitness:\n                    self.radii[i] = min(self.max_radius, self.radii[i] * 1.05)  # Increase radius if better than average\n                else:\n                    self.radii[i] = max(self.min_radius, self.radii[i] * 0.95)  # Decrease radius if worse than average\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "The algorithm SelfOrganizingScouts scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["94768647-f252-46d5-98c5-9ea8189ba67b"], "operator": null, "metadata": {"aucs": [0]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "35e472e9-818e-45d5-af23-635df80f61ba", "fitness": 0.0, "name": "SelfAdaptiveDE", "description": "Self-adaptive Differential Evolution with simplified parameter adaptation, adaptive population size reduction, and combined mutation strategies for enhanced exploration and exploitation.", "code": "import numpy as np\n\nclass SelfAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_rate=0.2, F=0.5, Cr=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = int(pop_size * archive_rate)\n        self.F = F\n        self.Cr = Cr\n        self.population = None\n        self.fitness = None\n        self.f_opt = float('inf')\n        self.x_opt = None\n        self.archive = []\n        self.success_F = []\n        self.success_Cr = []\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n\n    def mutate(self, i, func):\n        idxs = [idx for idx in range(self.pop_size) if idx != i]\n        a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n\n        # Combine mutation strategies: DE/rand/1 and DE/current-to-rand/1\n        if np.random.rand() < 0.5:\n            mutant = a + self.F * (b - c)  # DE/rand/1\n        else:\n            mutant = self.population[i] + self.F * (np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim) - self.population[i]) + self.F * (b - c) #DE/current-to-rand/1\n\n        mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n        return mutant\n\n    def crossover(self, mutant, i):\n        cross_points = np.random.rand(self.dim) < self.Cr\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        trial = np.where(cross_points, mutant, self.population[i])\n        return trial\n\n    def select(self, trial, i, func):\n        f_trial = func(trial)\n        self.budget -= 1\n\n        if f_trial < self.fitness[i]:\n            # Update Archive\n            if len(self.archive) < self.archive_size:\n                self.archive.append(self.population[i].copy())\n            else:\n                idx_to_replace = np.random.randint(0, len(self.archive))\n                self.archive[idx_to_replace] = self.population[i].copy()\n\n            self.success_F.append(self.F)\n            self.success_Cr.append(self.Cr)\n\n            self.fitness[i] = f_trial\n            self.population[i] = trial\n\n            if f_trial < self.f_opt:\n                self.f_opt = f_trial\n                self.x_opt = trial\n            return True\n        return False\n\n    def adapt_parameters(self):\n        if self.success_F:\n            self.F = np.mean(self.success_F)\n        self.F = np.clip(self.F + 0.1 * np.random.normal(0, 0.1), 0.1, 1.0)\n\n        if self.success_Cr:\n            self.Cr = np.mean(self.success_Cr)\n        self.Cr = np.clip(self.Cr + 0.1 * np.random.normal(0, 0.1), 0.1, 1.0)\n\n    def reduce_population(self):\n        if self.pop_size > 10 and self.budget < self.pop_size * 2:\n            num_to_reduce = max(1, self.pop_size // 5)\n            indices_to_remove = np.argsort(self.fitness)[-num_to_reduce:]\n            self.population = np.delete(self.population, indices_to_remove, axis=0)\n            self.fitness = np.delete(self.fitness, indices_to_remove)\n            self.pop_size = self.population.shape[0]\n            self.archive_size = int(self.pop_size * 0.2) # Update archive size as well\n            print(f\"Population reduced to {self.pop_size}\") # For debugging\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        generation = 0\n        while self.budget > 0:\n            generation += 1\n            self.success_F = []\n            self.success_Cr = []\n\n            for i in range(self.pop_size):\n                mutant = self.mutate(i, func)\n                trial = self.crossover(mutant, i)\n                self.select(trial, i, func)\n\n            self.adapt_parameters()\n            self.reduce_population()\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "The algorithm SelfAdaptiveDE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["d15f2124-2044-40ea-b0ea-bbc76d19a3ef"], "operator": null, "metadata": {"aucs": [0]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "c644790d-f6f7-49f4-87d6-80d5786151e8", "fitness": 0.6962951258181829, "name": "AdaptiveMutationDE", "description": "Population-based optimization using a combination of Gaussian mutation, Cauchy mutation for diversity, and adaptive parameter control with a success-history based memory.", "code": "import numpy as np\n\nclass AdaptiveMutationDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=10, F=0.5, Cr=0.9, lr_F=0.1, lr_Cr=0.1, p_cauchy=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.F = F\n        self.Cr = Cr\n        self.lr_F = lr_F\n        self.lr_Cr = lr_Cr\n        self.p_cauchy = p_cauchy  # Probability of using Cauchy mutation\n        self.archive = []\n        self.archive_fitness = []\n        self.memory_F = []\n        self.memory_Cr = []\n        self.memory_size = 10\n        self.success_F = []\n        self.success_Cr = []\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)]\n\n        while self.budget > 0:\n            success_F = []\n            success_Cr = []\n            for i in range(self.pop_size):\n                # Mutation: Adaptive choice between Gaussian and Cauchy mutation\n                if np.random.rand() < self.p_cauchy:\n                    # Cauchy mutation for exploration\n                    idxs = [idx for idx in range(self.pop_size) if idx != i]\n                    a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                    mutant = population[i] + self.F * np.random.standard_cauchy(size=self.dim)\n                    mutant = np.clip(mutant, lb, ub)\n\n\n                else:\n                    # Gaussian mutation for exploitation\n                    idxs = [idx for idx in range(self.pop_size) if idx != i]\n                    if len(self.archive) > 0 and np.random.rand() < 0.1:\n                        idx_archive = np.random.randint(len(self.archive))\n                        a = self.archive[idx_archive]\n                        idxs = np.random.choice(idxs, 2, replace=False)\n                        b, c = population[idxs]\n                        mutant = np.clip(a + self.F * (b - c), lb, ub)\n                    else:\n                        a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                        mutant = np.clip(a + self.F * (b - c), lb, ub)\n\n                # Crossover\n                trial = self.crossover(population[i], mutant, self.Cr)\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                if f_trial < fitness[i]:\n                    success_F.append(self.F)\n                    success_Cr.append(self.Cr)\n                    fitness[i] = f_trial\n                    population[i] = trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n                    # Update archive\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(population[i].copy())\n                        self.archive_fitness.append(fitness[i])\n                    else:\n                        max_archive_index = np.argmax(self.archive_fitness)\n                        if fitness[i] < self.archive_fitness[max_archive_index]:\n                            self.archive[max_archive_index] = population[i].copy()\n                            self.archive_fitness[max_archive_index] = fitness[i]\n\n            # Adaptive F and Cr using success history\n            if success_F:\n                self.F = np.mean(success_F)\n            if success_Cr:\n                self.Cr = np.mean(success_Cr)\n\n            # Parameter adaptation with learning rates\n            self.F = np.clip(self.F + self.lr_F * np.random.normal(0, 0.1), 0.1, 1.0)\n            self.Cr = np.clip(self.Cr + self.lr_Cr * np.random.normal(0, 0.1), 0.1, 1.0)\n\n        return self.f_opt, self.x_opt\n\n    def crossover(self, x, mutant, cr):\n        trial = x.copy()\n        for j in range(self.dim):\n            if np.random.rand() < cr:\n                trial[j] = mutant[j]\n        return trial", "configspace": "", "generation": 5, "feedback": "The algorithm AdaptiveMutationDE scored 0.696 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["06b6ae75-0ec5-408d-a4a3-4059dc60b0ac"], "operator": null, "metadata": {"aucs": [0.3619732623542615, 0.7307331985937326, 0.7336762565585668, 0.8749025435644711, 0.7494883302240489, 0.7910339721835018, 0.5138121105365474, 0.6777954888001271, 0.7873736450155226, 0.7048194215352481, 0.8577943293025139, 0.9940578792555198, 0.4876597208418495, 0.7663919485868316, 0.9254259342169314, 0.8023568906149439, 0.5722186649967531, 0.8491281815821111, 0.2251677631276744, 0.520092974472501]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "35493b0f-5eff-424d-a38d-a2f466881e37", "fitness": 0.0, "name": "AdaptiveDE_PSO_LocalSearch", "description": "An adaptive Differential Evolution with a novel mutation strategy inspired by particle swarm optimization and a local search operator triggered based on stagnation detection.", "code": "import numpy as np\n\nclass AdaptiveDE_PSO_LocalSearch:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=10, F=0.5, Cr=0.9, w=0.7, c1=1.5, c2=1.5, stagnation_threshold=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.F = F  # Mutation factor\n        self.Cr = Cr  # Crossover rate\n        self.w = w # Inertia weight for PSO\n        self.c1 = c1 # Cognitive coefficient for PSO\n        self.c2 = c2 # Social coefficient for PSO\n        self.learning_rate_F = 0.1\n        self.learning_rate_Cr = 0.1\n        self.success_history_F = []\n        self.success_history_Cr = []\n        self.success_history_size = 10\n        self.archive = []\n        self.stagnation_counter = 0\n        self.stagnation_threshold = stagnation_threshold\n\n\n    def local_search(self, individual, func, radius=0.1):\n        \"\"\"Performs a simple local search around the individual.\"\"\"\n        best_individual = individual.copy()\n        best_fitness = func(individual)\n        self.budget -=1\n\n        for _ in range(10): #Limited local search evaluations\n            neighbor = individual + np.random.uniform(-radius, radius, size=self.dim)\n            neighbor = np.clip(neighbor, func.bounds.lb, func.bounds.ub)\n            fitness = func(neighbor)\n            self.budget -= 1\n            if fitness < best_fitness:\n                best_fitness = fitness\n                best_individual = neighbor.copy()\n        return best_individual, best_fitness\n\n\n\n    def __call__(self, func):\n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size  # Deduct initial population evaluations from budget\n\n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)]\n\n        # Initialize velocity\n        velocity = np.zeros_like(population)\n\n        generation = 0\n        while self.budget > 0:\n            generation += 1\n            best_idx = np.argmin(fitness)\n            global_best = population[best_idx].copy()\n\n            for i in range(self.pop_size):\n                # Mutation (PSO inspired)\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                velocity[i] = self.w * velocity[i] + \\\n                              self.c1 * r1 * (population[i] - population[i]) + \\\n                              self.c2 * r2 * (global_best - population[i])\n\n                mutant = np.clip(population[i] + velocity[i], func.bounds.lb, func.bounds.ub)\n\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.Cr\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n\n                if f_trial < fitness[i]:\n                    # Archive the replaced individual for diversity\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(population[i].copy())\n                    else:\n                        idx_to_replace = np.random.randint(0, self.archive_size)\n                        self.archive[idx_to_replace] = population[i].copy()\n\n                    fitness[i] = f_trial\n                    population[i] = trial\n\n                    # Update optimal solution\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                        self.stagnation_counter = 0 # Reset stagnation counter\n\n                    # Update success history for F and Cr adaptation\n                    self.success_history_F.append(self.F)\n                    self.success_history_Cr.append(self.Cr)\n                    if len(self.success_history_F) > self.success_history_size:\n                        self.success_history_F.pop(0)\n                        self.success_history_Cr.pop(0)\n                else:\n                    self.stagnation_counter += 1 # Increment stagnation counter\n\n\n            # Adapt F and Cr based on success history\n            if self.success_history_F:\n                self.F = np.clip(np.mean(self.success_history_F) + self.learning_rate_F * np.random.normal(0, 0.1), 0.1, 1.0)\n            else:\n                self.F = np.clip(self.F + self.learning_rate_F * np.random.normal(0, 0.1), 0.1, 1.0)\n\n            if self.success_history_Cr:\n                self.Cr = np.clip(np.mean(self.success_history_Cr) + self.learning_rate_Cr * np.random.normal(0, 0.1), 0.1, 1.0)\n            else:\n                self.Cr = np.clip(self.Cr + self.learning_rate_Cr * np.random.normal(0, 0.1), 0.1, 1.0)\n\n\n            # Stagnation Detection and Local Search\n            if self.stagnation_counter > self.stagnation_threshold:\n                # Apply local search to the best individual\n                best_idx = np.argmin(fitness)\n                population[best_idx], fitness[best_idx] = self.local_search(population[best_idx], func)\n                if fitness[best_idx] < self.f_opt:\n                    self.f_opt = fitness[best_idx]\n                    self.x_opt = population[best_idx]\n\n\n                self.stagnation_counter = 0 # Reset stagnation counter\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "The algorithm AdaptiveDE_PSO_LocalSearch scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["d15f2124-2044-40ea-b0ea-bbc76d19a3ef"], "operator": null, "metadata": {"aucs": [0]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "72fa2964-53d3-4bf6-8488-69b67b1a94f1", "fitness": 0.49179816403479554, "name": "SelfAdaptiveDE", "description": "Self-adaptive Differential Evolution with simplified parameter adaptation, archive-based mutation and a reduced parameter set.", "code": "import numpy as np\n\nclass SelfAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=10, F=0.5, Cr=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.F = F  # Mutation factor\n        self.Cr = Cr  # Crossover rate\n        self.archive = []\n        self.success_history_F = []\n        self.success_history_Cr = []\n        self.success_history_size = 5\n\n    def __call__(self, func):\n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)]\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n\n                # Archive-based mutation\n                if self.archive:\n                    arch_idx = np.random.randint(0, len(self.archive))\n                    d = self.archive[arch_idx]\n                    mutant = np.clip(a + self.F * (b - c) + self.F * (d - population[i]), func.bounds.lb, func.bounds.ub)\n                else:\n                    mutant = np.clip(a + self.F * (b - c), func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.Cr\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n\n                if f_trial < fitness[i]:\n                    # Archive the replaced individual\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(population[i].copy())\n                    else:\n                        idx_to_replace = np.random.randint(0, self.archive_size)\n                        self.archive[idx_to_replace] = population[i].copy()\n\n                    # Update success history\n                    self.success_history_F.append(self.F)\n                    self.success_history_Cr.append(self.Cr)\n                    if len(self.success_history_F) > self.success_history_size:\n                         self.success_history_F.pop(0)\n                         self.success_history_Cr.pop(0)\n\n                    fitness[i] = f_trial\n                    population[i] = trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n            #Adapt F and Cr\n            if self.success_history_F:\n                 self.F = np.clip(np.mean(self.success_history_F), 0.1, 0.9)\n            if self.success_history_Cr:\n                self.Cr = np.clip(np.mean(self.success_history_Cr), 0.1, 0.9)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "The algorithm SelfAdaptiveDE scored 0.492 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["d15f2124-2044-40ea-b0ea-bbc76d19a3ef"], "operator": null, "metadata": {"aucs": [0.17706349389281117, 0.3019120893751167, 0.44783155609250425, 0.7476191616679285, 0.44549581899226376, 0.5887477765771811, 0.31195597888211535, 0.415687762620247, 0.482630553277163, 0.31080768702295114, 0.692919540338915, 0.9994975201101499, 0.40370686488090946, 0.3879712785852345, 0.8268776154663199, 0.5246020300855041, 0.3769356669260374, 0.7035760947413854, 0.20112561088476255, 0.48899918027640954]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "14ff7a3d-cdc5-4eb5-a207-70c9eb43e6b7", "fitness": "-inf", "name": "HybridCMAESDE", "description": "A population-based algorithm employing a self-adaptive covariance matrix adaptation evolution strategy (CMA-ES) combined with a differential evolution (DE) operator for mutation to balance exploration and exploitation, and uses a success-history based adaptation of CMA-ES parameters.", "code": "import numpy as np\n\nclass HybridCMAESDE:\n    def __init__(self, budget=10000, dim=10, pop_size=None, mu=None, cs=0.3, damp=None, c_cov=None):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size if pop_size is not None else 4 + int(3 * np.log(self.dim))\n        self.mu = mu if mu is not None else self.pop_size // 2\n        self.mu = max(1, self.mu)  # Ensure mu is at least 1\n        self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n        self.weights /= np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        self.cs = cs\n        self.damp = damp if damp is not None else 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1) + self.cs\n        self.c_cov = c_cov if c_cov is not None else (1 / 3) * (self.mueff / (self.dim + 1.3) + (1 - self.mueff / (self.dim + 1.3)) * self.cs**2)\n        self.c_cov_mu = (1 / self.c_cov) * (self.mueff / (self.dim + 1.3))\n\n        self.mean = None\n        self.sigma = 0.5\n        self.pc = np.zeros(self.dim)\n        self.C = np.eye(self.dim)\n        self.B = None\n        self.D = None\n        self.invsqrtC = None\n\n        self.success_rate = 0.5\n        self.success_history = []  # Store recent success rates for adaptation\n        self.history_length = 10  # Length of success history\n\n    def _update_decomposition(self):\n        self.C = np.triu(self.C) + np.triu(self.C, 1).T  # Ensure symmetry\n        self.D, self.B = np.linalg.eigh(self.C)\n        self.D = np.sqrt(self.D)\n        self.invsqrtC = self.B @ np.diag(1 / self.D) @ self.B.T\n        \n\n    def __call__(self, func):\n        # Initialization\n        self.mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        self._update_decomposition()\n\n        fitness_history = []\n        x_opt = None\n        f_opt = float('inf')\n\n        while self.budget > 0:\n            # Sampling\n            z = np.random.randn(self.pop_size, self.dim)\n            x = self.mean + self.sigma * (self.B @ (self.D * z.T)).T\n            x = np.clip(x, func.bounds.lb, func.bounds.ub)\n\n            # Evaluation\n            fitness = np.array([func(xi) for xi in x])\n            self.budget -= self.pop_size\n\n            # Selection and Recombination\n            idx = np.argsort(fitness)\n            fitness = fitness[idx]\n            x = x[idx]\n\n            if fitness[0] < f_opt:\n                f_opt = fitness[0]\n                x_opt = x[0]\n\n            # Update CMA-ES parameters\n            x_mean = np.sum(x[:self.mu].T * self.weights, axis=1)\n            y = (x[:self.mu] - self.mean) / self.sigma\n            y_mean = np.sum(y.T * self.weights, axis=1)\n\n            self.pc = (1 - self.cs) * self.pc + np.sqrt(self.cs * (2 - self.cs) * self.mueff) * y_mean\n            self.C = (1 - self.c_cov * (1 - self.c_cov_mu)) * self.C + self.c_cov * self.c_cov_mu * (y_mean[:, None] @ y_mean[None, :])\n            for i in range(self.mu):\n                self.C += self.c_cov * (1 - self.c_cov_mu) * self.weights[i] * (y[i][:, None] @ y[i][None, :])\n            \n            self.mean = x_mean\n            self._update_decomposition()\n\n            # Differential Evolution Mutation as additional Exploration\n            idxs = np.random.choice(self.pop_size, size=(self.pop_size, 3), replace=False)\n            for i in range(self.pop_size):\n              mutant = x[idxs[i, 0]] + 0.5 * (x[idxs[i, 1]] - x[idxs[i, 2]])\n              mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n              # Crossover (Binomial)\n              cross_points = np.random.rand(self.dim) < 0.9\n              if not np.any(cross_points):\n                  cross_points[np.random.randint(0, self.dim)] = True\n              trial = np.where(cross_points, mutant, x[i])\n              \n              f_trial = func(trial)\n              self.budget -= 1\n              if f_trial < fitness[i]:\n                  fitness[i] = f_trial\n                  x[i] = trial\n                  if f_trial < f_opt:\n                      f_opt = f_trial\n                      x_opt = trial\n            \n            # Success rate adaptation (for sigma)\n            success_rate = np.mean(fitness < fitness_history[-self.pop_size:] if len(fitness_history) >= self.pop_size else [False]) if len(fitness_history) > 0 else 0.5 # Initialize with 0.5 at the first iteration.\n            self.success_history.append(success_rate)\n            if len(self.success_history) > self.history_length:\n                self.success_history.pop(0)\n\n            self.success_rate = np.mean(self.success_history)  # Average recent success rates\n            self.sigma *= np.exp(0.1 * (self.success_rate - 0.2))  # Adjust sigma based on success rate\n            fitness_history.extend(list(fitness))\n\n\n        return f_opt, x_opt", "configspace": "", "generation": 5, "feedback": "In the code, line 49, in __call__, the following error occurred:\nValueError: operands could not be broadcast together with shapes (2,) (2,6) \nOn line: x = self.mean + self.sigma * (self.B @ (self.D * z.T)).T", "error": "In the code, line 49, in __call__, the following error occurred:\nValueError: operands could not be broadcast together with shapes (2,) (2,6) \nOn line: x = self.mean + self.sigma * (self.B @ (self.D * z.T)).T", "parent_ids": ["94768647-f252-46d5-98c5-9ea8189ba67b"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "ce573ca2-496b-4aa9-ab7a-98009db51c9f", "fitness": 0.2702555173007115, "name": "SelfAdaptiveDE", "description": "Self-adaptive Differential Evolution with momentum-based velocity update, improved parameter adaptation using weighted success history, and a ring topology for enhanced exploration.", "code": "import numpy as np\n\nclass SelfAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=10, F=0.5, Cr=0.9, velocity_weight=0.1, momentum=0.5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.F = F  # Mutation factor\n        self.Cr = Cr  # Crossover rate\n        self.velocity_weight = velocity_weight  # Weight for velocity-based mutation\n        self.momentum = momentum # Momentum for velocity update\n        self.learning_rate_F = 0.1\n        self.learning_rate_Cr = 0.1\n        self.success_history_F = []\n        self.success_history_Cr = []\n        self.success_history_size = 10\n        self.archive = []\n\n    def __call__(self, func):\n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size  # Deduct initial population evaluations from budget\n\n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)]\n\n        # Initialize velocity\n        velocity = np.zeros_like(population)\n\n        generation = 0\n        while self.budget > 0:\n            generation += 1\n\n            # Ring Topology DE\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                # Ring topology selection: use neighbors in the population\n                l = (i - 1) % self.pop_size\n                r = (i + 1) % self.pop_size\n                a = population[i]\n                b = population[l]\n                c = population[r]\n\n                # Momentum-based Velocity update\n                velocity[i] = self.momentum * velocity[i] + self.velocity_weight * velocity[i] + self.F * (b - c)\n\n                # Apply mutation with velocity\n                mutant = np.clip(a + velocity[i], func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.Cr\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n\n                if f_trial < fitness[i]:\n                    # Archive the replaced individual for diversity\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(population[i].copy())\n                    else:\n                        idx_to_replace = np.random.randint(0, self.archive_size)\n                        self.archive[idx_to_replace] = population[i].copy()\n\n                    fitness[i] = f_trial\n                    population[i] = trial\n\n                    # Update optimal solution\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n                    # Update success history for F and Cr adaptation\n                    self.success_history_F.append(self.F)\n                    self.success_history_Cr.append(self.Cr)\n                    if len(self.success_history_F) > self.success_history_size:\n                        self.success_history_F.pop(0)\n                        self.success_history_Cr.pop(0)\n\n            # Adapt F and Cr based on weighted success history\n            if self.success_history_F:\n                weights = np.linspace(0.1, 1.0, len(self.success_history_F))  # Linear weights\n                weighted_avg_F = np.average(self.success_history_F, weights=weights)\n                self.F = np.clip(weighted_avg_F + self.learning_rate_F * np.random.normal(0, 0.1), 0.1, 1.0)\n            else:\n                self.F = np.clip(self.F + self.learning_rate_F * np.random.normal(0, 0.1), 0.1, 1.0)\n\n            if self.success_history_Cr:\n                weights = np.linspace(0.1, 1.0, len(self.success_history_Cr))\n                weighted_avg_Cr = np.average(self.success_history_Cr, weights=weights)\n                self.Cr = np.clip(weighted_avg_Cr + self.learning_rate_Cr * np.random.normal(0, 0.1), 0.1, 1.0)\n            else:\n                self.Cr = np.clip(self.Cr + self.learning_rate_Cr * np.random.normal(0, 0.1), 0.1, 1.0)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "The algorithm SelfAdaptiveDE scored 0.270 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["d15f2124-2044-40ea-b0ea-bbc76d19a3ef"], "operator": null, "metadata": {"aucs": [0.10993736939224308, 0.15545343255534083, 0.28237956212246096, 0.19632587567120696, 0.19850392970050634, 0.19296422495404886, 0.22215943038000552, 0.18648401755264798, 0.17480008922918844, 0.15229004478224473, 0.2111763474031162, 0.9984460058631481, 0.23108483988879502, 0.21900892129485494, 0.5771825616135198, 0.2493697643295042, 0.20426719320082865, 0.25546204807603834, 0.14879918625328215, 0.4390155017512496]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "b7a9c6ea-54c7-4c22-ad7f-abec8622713b", "fitness": 0.10060530567535525, "name": "SelfAdaptiveDE", "description": "Self-adaptive Differential Evolution with a success-rate based mutation adaptation, a diversity-promoting archive and orthogonal design, and a restart mechanism for escaping local optima.", "code": "import numpy as np\n\nclass SelfAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=10, F=0.5, Cr=0.9, aging_limit=50, restart_prob=0.05):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.F = F\n        self.Cr = Cr\n        self.learning_rate = 0.1\n        self.aging_limit = aging_limit\n        self.restart_prob = restart_prob\n        self.archive = []\n        self.archive_fitness = []\n        self.population_age = np.zeros(pop_size)\n        self.success_F = []\n        self.success_Cr = []\n        self.success_history_size = 10\n        self.sf_memory = []\n        self.scr_memory = []\n        self.sr_memory = []\n\n\n    def __call__(self, func):\n        # Initialize population\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)]\n\n        generation = 0\n        while self.budget > 0:\n            generation += 1\n            for i in range(self.pop_size):\n                # Aging mechanism: if an individual doesn't improve for too long, replace it with a random one\n                if self.population_age[i] > self.aging_limit:\n                    population[i] = np.random.uniform(lb, ub, size=self.dim)\n                    fitness[i] = func(population[i])\n                    self.budget -= 1\n                    self.population_age[i] = 0\n                    if fitness[i] < self.f_opt:\n                        self.f_opt = fitness[i]\n                        self.x_opt = population[i]\n                    continue\n\n                # Mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                if len(self.archive) > 0 and np.random.rand() < 0.1:  # Use archive occasionally\n                    idx_archive = np.random.randint(len(self.archive))\n                    a = self.archive[idx_archive]\n                    idxs = np.random.choice(idxs, 2, replace=False)\n                    b, c = population[idxs]\n                    mutant = np.clip(a + self.F * (b - c), lb, ub)\n\n                else:\n                     a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                     mutant = np.clip(a + self.F * (b - c), lb, ub)\n\n\n                # Orthogonal Crossover\n                trial = self.orthogonal_crossover(population[i], mutant, self.Cr)\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                if f_trial < fitness[i]:\n                    delta = fitness[i] - f_trial\n                    fitness[i] = f_trial\n                    population[i] = trial\n                    self.population_age[i] = 0 #reset age\n\n                    # Update optimal solution\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                        self.learning_rate = min(1.0, self.learning_rate * 1.1)\n\n                    # Update archive\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(population[i].copy())\n                        self.archive_fitness.append(fitness[i])\n                    else:\n                        max_archive_index = np.argmax(self.archive_fitness)\n                        if fitness[i] < self.archive_fitness[max_archive_index]:\n                            self.archive[max_archive_index] = population[i].copy()\n                            self.archive_fitness[max_archive_index] = fitness[i]\n\n                    # Success history based adaptation\n                    self.sf_memory.append(self.F)\n                    self.scr_memory.append(self.Cr)\n                    self.sr_memory.append(delta)\n                    if len(self.sf_memory) > self.success_history_size:\n                         self.sf_memory.pop(0)\n                         self.scr_memory.pop(0)\n                         self.sr_memory.pop(0)\n\n\n                else:\n                    self.population_age[i] += 1 #increase age\n                    self.learning_rate = max(0.01, self.learning_rate * 0.9)\n\n            # Adapt parameters using success history\n            if len(self.sf_memory) > 0:\n                weights = np.array(self.sr_memory) / np.sum(self.sr_memory)\n                self.F = np.clip(np.sum(np.array(self.sf_memory) * weights), 0.1, 1.0)\n                self.Cr = np.clip(np.sum(np.array(self.scr_memory) * weights), 0.1, 1.0)\n            else:\n                 self.F = np.clip(self.F + self.learning_rate * np.random.normal(0, 0.1), 0.1, 1.0)\n                 self.Cr = np.clip(self.Cr + self.learning_rate * np.random.normal(0, 0.1), 0.1, 1.0)\n                 \n            # Restart mechanism\n            if np.random.rand() < self.restart_prob:\n                population = np.random.uniform(lb, ub, size=(self.pop_size, self.dim))\n                fitness = np.array([func(x) for x in population])\n                self.budget -= self.pop_size\n                best_index = np.argmin(fitness)\n                if fitness[best_index] < self.f_opt:\n                    self.f_opt = fitness[best_index]\n                    self.x_opt = population[best_index]\n                self.population_age = np.zeros(self.pop_size)\n\n\n        return self.f_opt, self.x_opt\n\n    def orthogonal_crossover(self, x, mutant, cr):\n        trial = x.copy()\n        for j in range(self.dim):\n            if np.random.rand() < cr:\n                trial[j] = mutant[j]\n        return trial", "configspace": "", "generation": 6, "feedback": "The algorithm SelfAdaptiveDE scored 0.101 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["06b6ae75-0ec5-408d-a4a3-4059dc60b0ac"], "operator": null, "metadata": {"aucs": [0.2012106113507105, 0]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "9b4122d3-00d2-4a44-b590-fede3e3fd8fd", "fitness": 0.0, "name": "DynamicPopulationDE", "description": "A differential evolution algorithm with a dynamic population size, a mutation strategy that blends current-to-best and random differential evolution, and adaptive parameter control using a sigmoid function based on success history.", "code": "import numpy as np\n\nclass DynamicPopulationDE:\n    def __init__(self, budget=10000, dim=10, initial_pop_size=50, min_pop_size=10, max_pop_size=100, F=0.5, Cr=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.initial_pop_size = initial_pop_size\n        self.min_pop_size = min_pop_size\n        self.max_pop_size = max_pop_size\n        self.pop_size = initial_pop_size\n        self.F = F  # Mutation factor\n        self.Cr = Cr  # Crossover rate\n        self.success_history_F = []\n        self.success_history_Cr = []\n        self.memory_size = 10\n        self.C = 0.1\n        self.population = None\n        self.fitness = None\n        self.f_opt = float('inf')\n        self.x_opt = None\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n\n    def update_parameters(self):\n        if self.success_history_F:\n            # Sigmoid-based adaptation of F and Cr\n            mean_F = np.mean(self.success_history_F)\n            mean_Cr = np.mean(self.success_history_Cr)\n\n            # Sigmoid function for adaptive scaling\n            scale_F = 1 / (1 + np.exp(-self.C * (mean_F - 0.5)))\n            scale_Cr = 1 / (1 + np.exp(self.C * (mean_Cr - 0.5)))\n\n            self.F = np.clip(np.random.normal(mean_F, 0.1 * scale_F), 0.1, 1.0)\n            self.Cr = np.clip(np.random.normal(mean_Cr, 0.1 * scale_Cr), 0.1, 1.0)\n\n            if len(self.success_history_F) > self.memory_size:\n                self.success_history_F.pop(0)\n                self.success_history_Cr.pop(0)\n\n    def adjust_population_size(self):\n        # Dynamic population size adjustment based on improvement rate\n        improvement_threshold = 0.01  # Define a threshold for improvement\n        improvement_rate = (self.f_opt - np.min(self.fitness)) / self.f_opt if self.f_opt != 0 else 0\n        \n        if improvement_rate > improvement_threshold:\n            # Increase population size if significant improvement\n            self.pop_size = min(self.pop_size + 5, self.max_pop_size)\n        else:\n            # Decrease population size if no significant improvement\n            self.pop_size = max(self.pop_size - 5, self.min_pop_size)\n        \n        # Ensure population is re-initialized if the size changes significantly.\n        if self.pop_size != len(self.population):\n            return True\n        return False\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        generation = 0\n\n        while self.budget > 0:\n            generation += 1\n            best_index = np.argmin(self.fitness)\n\n            for i in range(self.pop_size):\n                # Mutation: Current-to-best with random differential\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(self.population[i] + self.F * (self.population[best_index] - self.population[i]) + self.F * (b - c), func.bounds.lb, func.bounds.ub)\n\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.Cr\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, self.population[i])\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n\n                if f_trial < self.fitness[i]:\n                    self.success_history_F.append(self.F)\n                    self.success_history_Cr.append(self.Cr)\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n                    \n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                \n            self.update_parameters()\n            if self.adjust_population_size():\n                #Reinitialize population with new size and re-evaluate fitness\n                new_population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                new_fitness = np.array([func(x) for x in new_population])\n                self.budget -= self.pop_size - len(self.population) # Update budget based on change in pop size\n\n                # Carry over individuals from the old population to the new one if possible\n                min_len = min(len(new_population), len(self.population))\n                new_population[:min_len] = self.population[:min_len]\n                new_fitness[:min_len] = self.fitness[:min_len]\n\n                self.population = new_population\n                self.fitness = new_fitness\n                \n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "The algorithm DynamicPopulationDE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["94768647-f252-46d5-98c5-9ea8189ba67b"], "operator": null, "metadata": {"aucs": [0]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "0b1944af-46dc-49cf-8ecf-fe103c91d46e", "fitness": 0.6849490624829735, "name": "SelfAdaptiveDE", "description": "Self-adaptive Differential Evolution with simplified parameter adaptation, archive, and a probabilistic velocity update for balanced exploration and exploitation.", "code": "import numpy as np\n\nclass SelfAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=10, F=0.5, Cr=0.9, velocity_weight=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.F = F\n        self.Cr = Cr\n        self.velocity_weight = velocity_weight\n        self.archive = []\n\n    def __call__(self, func):\n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)]\n\n        # Initialize velocity\n        velocity = np.zeros_like(population)\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n\n                # Probabilistic Velocity update\n                if np.random.rand() < 0.5:\n                    velocity[i] = self.velocity_weight * velocity[i] + self.F * (b - c)\n                else:\n                    velocity[i] = self.F * (b - c)\n                mutant = np.clip(a + velocity[i], func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.Cr\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n\n                if f_trial < fitness[i]:\n                    # Archive\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(population[i].copy())\n                    else:\n                        idx_to_replace = np.random.randint(0, self.archive_size)\n                        self.archive[idx_to_replace] = population[i].copy()\n\n                    fitness[i] = f_trial\n                    population[i] = trial\n\n                    # Update optimal solution\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n            # Simplified Adaptation of F and Cr\n            self.F = np.clip(self.F + 0.1 * np.random.normal(0, 0.1), 0.1, 1.0)\n            self.Cr = np.clip(self.Cr + 0.1 * np.random.normal(0, 0.1), 0.1, 1.0)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "The algorithm SelfAdaptiveDE scored 0.685 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["d15f2124-2044-40ea-b0ea-bbc76d19a3ef"], "operator": null, "metadata": {"aucs": [0.43564598309832414, 0.6017594138304283, 0.6523603951968908, 0.8702535331837071, 0.7685116140540498, 0.7594120338521697, 0.6624866125065341, 0.6269484605776476, 0.7818006057556426, 0.6594622079214187, 0.857188584460029, 0.9852457753441041, 0.4036723608901329, 0.7255491738372811, 0.9153059091249554, 0.794325446345958, 0.5957928773312574, 0.8363980181972737, 0.25302832953772003, 0.5138339146139448]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "13276003-5f68-44a4-b7f5-5d6155380b47", "fitness": 0.5242933811616032, "name": "AdaptiveRestartDE", "description": "Adaptive Differential Evolution with a restart mechanism triggered by stagnation detection and a Cauchy mutation operator for enhanced exploration.", "code": "import numpy as np\n\nclass AdaptiveRestartDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=10, F=0.5, Cr=0.9, stagnation_threshold=1e-6, stagnation_generations=50):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.F = F  # Mutation factor\n        self.Cr = Cr  # Crossover rate\n        self.stagnation_threshold = stagnation_threshold\n        self.stagnation_generations = stagnation_generations\n        self.best_fitness_history = []\n        self.learning_rate_F = 0.1\n        self.learning_rate_Cr = 0.1\n        self.success_history_F = []\n        self.success_history_Cr = []\n        self.success_history_size = 10\n        self.archive = []\n\n    def __call__(self, func):\n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size  # Deduct initial population evaluations from budget\n\n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)]\n        self.best_fitness_history.append(self.f_opt)\n\n        generation = 0\n        while self.budget > 0:\n            generation += 1\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n\n                # Cauchy mutation with adaptive F\n                mutant = a + self.F * (b - c) + 0.01 * np.random.standard_cauchy(size=self.dim) # Adding Cauchy noise\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.Cr\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n\n                if f_trial < fitness[i]:\n                    # Archive the replaced individual for diversity\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(population[i].copy())\n                    else:\n                        idx_to_replace = np.random.randint(0, self.archive_size)\n                        self.archive[idx_to_replace] = population[i].copy()\n\n                    fitness[i] = f_trial\n                    population[i] = trial\n\n                    # Update optimal solution\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n                    # Update success history for F and Cr adaptation\n                    self.success_history_F.append(self.F)\n                    self.success_history_Cr.append(self.Cr)\n                    if len(self.success_history_F) > self.success_history_size:\n                        self.success_history_F.pop(0)\n                        self.success_history_Cr.pop(0)\n\n            # Adapt F and Cr based on success history\n            if self.success_history_F:\n                self.F = np.clip(np.mean(self.success_history_F) + self.learning_rate_F * np.random.normal(0, 0.1), 0.1, 1.0)\n            else:\n                self.F = np.clip(self.F + self.learning_rate_F * np.random.normal(0, 0.1), 0.1, 1.0)\n\n            if self.success_history_Cr:\n                self.Cr = np.clip(np.mean(self.success_history_Cr) + self.learning_rate_Cr * np.random.normal(0, 0.1), 0.1, 1.0)\n            else:\n                self.Cr = np.clip(self.Cr + self.learning_rate_Cr * np.random.normal(0, 0.1), 0.1, 1.0)\n\n            # Stagnation detection and restart mechanism\n            self.best_fitness_history.append(self.f_opt)\n            if len(self.best_fitness_history) > self.stagnation_generations:\n                self.best_fitness_history.pop(0)\n                if np.abs(self.best_fitness_history[-1] - self.best_fitness_history[0]) < self.stagnation_threshold:\n                    # Restart population\n                    population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                    fitness = np.array([func(x) for x in population])\n                    self.budget -= self.pop_size  # Deduct initial population evaluations from budget\n\n                    self.f_opt = np.min(fitness)\n                    self.x_opt = population[np.argmin(fitness)]\n                    self.best_fitness_history = [self.f_opt]  # Reset fitness history\n                    self.success_history_F = [] # Reset success history\n                    self.success_history_Cr = []\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "The algorithm AdaptiveRestartDE scored 0.524 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["d15f2124-2044-40ea-b0ea-bbc76d19a3ef"], "operator": null, "metadata": {"aucs": [0.24056861880415215, 0.4381982572258225, 0.47074676305570406, 0.8546554805017919, 0.45044472344668285, 0.5627422075188071, 0.33819080979893146, 0.42029733888224563, 0.46406501012248436, 0.4107364697519056, 0.85699245593068, 0.9996553188466347, 0.3470603069113156, 0.44605757338603313, 0.839110643023405, 0.5648454725196999, 0.4050159547839812, 0.6457211516123498, 0.2095024541739191, 0.5212606129355206]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "9c8c3692-acde-4980-9553-7c0cd05ad218", "fitness": 0.5937945609277455, "name": "EnhancedAdaptiveMutationDE", "description": "Enhanced Adaptive Mutation DE with a periodic restart mechanism, orthogonal crossover, and a combined archive update strategy for improved exploration, exploitation, and diversity.", "code": "import numpy as np\n\nclass EnhancedAdaptiveMutationDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=10, F=0.5, Cr=0.9, lr_F=0.1, lr_Cr=0.1, p_cauchy=0.1, restart_interval=500):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.F = F\n        self.Cr = Cr\n        self.lr_F = lr_F\n        self.lr_Cr = lr_Cr\n        self.p_cauchy = p_cauchy  # Probability of using Cauchy mutation\n        self.archive = []\n        self.archive_fitness = []\n        self.memory_F = []\n        self.memory_Cr = []\n        self.memory_size = 10\n        self.success_F = []\n        self.success_Cr = []\n        self.restart_interval = restart_interval\n        self.generation = 0\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)]\n\n        while self.budget > 0:\n            success_F = []\n            success_Cr = []\n            for i in range(self.pop_size):\n                # Mutation: Adaptive choice between Gaussian and Cauchy mutation\n                if np.random.rand() < self.p_cauchy:\n                    # Cauchy mutation for exploration\n                    idxs = [idx for idx in range(self.pop_size) if idx != i]\n                    a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                    mutant = population[i] + self.F * np.random.standard_cauchy(size=self.dim)\n                    mutant = np.clip(mutant, lb, ub)\n\n\n                else:\n                    # Gaussian mutation for exploitation\n                    idxs = [idx for idx in range(self.pop_size) if idx != i]\n                    if len(self.archive) > 0 and np.random.rand() < 0.1:\n                        idx_archive = np.random.randint(len(self.archive))\n                        a = self.archive[idx_archive]\n                        idxs = np.random.choice(idxs, 2, replace=False)\n                        b, c = population[idxs]\n                        mutant = np.clip(a + self.F * (b - c), lb, ub)\n                    else:\n                        a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                        mutant = np.clip(a + self.F * (b - c), lb, ub)\n\n                # Crossover: Orthogonal Crossover\n                trial = self.orthogonal_crossover(population[i], mutant)\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                if f_trial < fitness[i]:\n                    success_F.append(self.F)\n                    success_Cr.append(self.Cr)\n                    fitness[i] = f_trial\n                    population[i] = trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n                    # Combined Archive Update Strategy\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(population[i].copy())\n                        self.archive_fitness.append(fitness[i])\n                    else:\n                        # Replace worst in archive with probability 0.5, otherwise replace random\n                        if np.random.rand() < 0.5:\n                            max_archive_index = np.argmax(self.archive_fitness)\n                            if fitness[i] < self.archive_fitness[max_archive_index]:\n                                self.archive[max_archive_index] = population[i].copy()\n                                self.archive_fitness[max_archive_index] = fitness[i]\n                        else:\n                            random_archive_index = np.random.randint(self.archive_size)\n                            self.archive[random_archive_index] = population[i].copy()\n                            self.archive_fitness[random_archive_index] = fitness[i]\n\n            # Adaptive F and Cr using success history\n            if success_F:\n                self.F = np.mean(success_F)\n            if success_Cr:\n                self.Cr = np.mean(success_Cr)\n\n            # Parameter adaptation with learning rates\n            self.F = np.clip(self.F + self.lr_F * np.random.normal(0, 0.1), 0.1, 1.0)\n            self.Cr = np.clip(self.Cr + self.lr_Cr * np.random.normal(0, 0.1), 0.1, 1.0)\n            \n            self.generation += 1\n            if self.generation % self.restart_interval == 0:\n                # Periodic Restart\n                population = np.random.uniform(lb, ub, size=(self.pop_size, self.dim))\n                fitness = np.array([func(x) for x in population])\n                self.budget -= self.pop_size\n                best_index = np.argmin(fitness)\n                if fitness[best_index] < self.f_opt:\n                    self.f_opt = fitness[best_index]\n                    self.x_opt = population[best_index]\n\n\n        return self.f_opt, self.x_opt\n\n    def orthogonal_crossover(self, x, mutant):\n        trial = x.copy()\n        num_levels = 3  # Example: Low, Medium, High\n        design_matrix = np.array([[1, 1], [2, 3], [3, 2]])  # Orthogonal array L9(3^4)\n        \n        for j in range(self.dim):\n            level = np.random.randint(1, num_levels + 1) # Generate a random level for each dimension\n            \n            if level == 1:\n                value = x[j] # Copy from parent x\n            elif level == 2:\n                value = mutant[j]  # Copy from mutant\n            else:\n                value = 0.5 * (x[j] + mutant[j])  # Average of parent and mutant\n\n            trial[j] = value\n        return trial", "configspace": "", "generation": 6, "feedback": "The algorithm EnhancedAdaptiveMutationDE scored 0.594 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["c644790d-f6f7-49f4-87d6-80d5786151e8"], "operator": null, "metadata": {"aucs": [0.2486536128288539, 0.40688207337914484, 0.5393069715126303, 0.8423000669625316, 0.7103291005429967, 0.8026228183123549, 0.623899223496502, 0.5698873242119364, 0.7358154399658678, 0.31264461306325253, 0.6404880035127521, 0.9987421004754423, 0.27245171674115676, 0.7423976658721567, 0.7834307885642502, 0.7506632396776312, 0.503096809964392, 0.6909901877185641, 0.20368052888456867, 0.49760893286792596]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "fb494a4f-09ff-4c40-a8ba-acd6b9fc2dcc", "fitness": 0.5425991024157513, "name": "SelfAdaptiveDE", "description": "Self-adaptive Differential Evolution with orthogonal design, success-history adaptation of F and CR, and a simplified aging mechanism for enhanced exploration and exploitation.", "code": "import numpy as np\n\nclass SelfAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=5, F=0.5, Cr=0.9, aging_limit=30, sf_lr=0.1, scr_lr=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.F = F\n        self.Cr = Cr\n        self.sf_lr = sf_lr  # Learning rate for F\n        self.scr_lr = scr_lr # Learning rate for CR\n        self.aging_limit = aging_limit\n        self.archive = []\n        self.archive_fitness = []\n        self.population_age = np.zeros(pop_size)  # Track the age of each individual\n        self.sf_memory = []\n        self.scr_memory = []\n\n\n    def __call__(self, func):\n        # Initialize population\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)]\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Aging mechanism: if an individual doesn't improve for too long, replace it with a random one\n                if self.population_age[i] > self.aging_limit:\n                    population[i] = np.random.uniform(lb, ub, size=self.dim)\n                    fitness[i] = func(population[i])\n                    self.budget -= 1\n                    self.population_age[i] = 0\n                    if fitness[i] < self.f_opt:\n                        self.f_opt = fitness[i]\n                        self.x_opt = population[i]\n                    continue\n\n                # Mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + self.F * (b - c), lb, ub)\n\n                # Orthogonal Crossover\n                trial = self.orthogonal_crossover(population[i], mutant, self.Cr)\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                if f_trial < fitness[i]:\n                    delta = fitness[i] - f_trial\n                    fitness[i] = f_trial\n                    population[i] = trial\n                    self.population_age[i] = 0 #reset age\n                    # Update optimal solution\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n                    # Update archive\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(population[i].copy())\n                        self.archive_fitness.append(fitness[i])\n                    else:\n                        max_archive_index = np.argmax(self.archive_fitness)\n                        if fitness[i] < self.archive_fitness[max_archive_index]:\n                            self.archive[max_archive_index] = population[i].copy()\n                            self.archive_fitness[max_archive_index] = fitness[i]\n\n                    # Success history adaptation\n                    self.sf_memory.append(self.F)\n                    self.scr_memory.append(self.Cr)\n\n                    self.F = np.clip(self.F + self.sf_lr * np.random.normal(0, 0.1), 0.1, 1.0)\n                    self.Cr = np.clip(self.Cr + self.scr_lr * np.random.normal(0, 0.1), 0.1, 1.0)\n\n                else:\n                    self.population_age[i] += 1 #increase age\n\n\n\n        return self.f_opt, self.x_opt\n\n    def orthogonal_crossover(self, x, mutant, cr):\n        trial = x.copy()\n        for j in range(self.dim):\n            if np.random.rand() < cr:\n                trial[j] = mutant[j]\n        return trial", "configspace": "", "generation": 6, "feedback": "The algorithm SelfAdaptiveDE scored 0.543 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["06b6ae75-0ec5-408d-a4a3-4059dc60b0ac"], "operator": null, "metadata": {"aucs": [0.21253721911198553, 0.2711405096112558, 0.45130996922396494, 0.45789107633810266, 0.3743730497510832, 0.6750652460371889, 0.3682247766764615, 0.4974497812729828, 0.4688575743302996, 0.7952904842865482, 0.5647979588921292, 0.9967121402773674, 0.5333946579670927, 0.5821250303252069, 0.8755931381458495, 0.7175503879879095, 0.4137024235703235, 0.7605155973500469, 0.32362205326375415, 0.5118289738954698]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "c84edff1-084c-4a6a-a670-fa790c623576", "fitness": "-inf", "name": "FuzzyAdaptiveDE", "description": "An enhanced Self-Adaptive Differential Evolution with a dual population, local search fine-tuning, and adaptive exploration-exploitation balance through fuzzy logic control.", "code": "import numpy as np\nimport skfuzzy as fuzz\nfrom skfuzzy import control as ctrl\n\nclass FuzzyAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=10, local_search_probability=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.local_search_probability = local_search_probability\n        self.archive = []\n        self.archive_fitness = []\n        self.F = 0.5\n        self.Cr = 0.9\n        self.elite_ratio = 0.1  # Ratio of population size for the elite population\n\n        # Fuzzy Logic Controller for adaptive parameter adjustment\n        self.improvement_rate = ctrl.Antecedent(np.linspace(0, 1, 100), 'improvement_rate')\n        self.diversity = ctrl.Antecedent(np.linspace(0, 1, 100), 'diversity')\n        self.explore_prob = ctrl.Consequent(np.linspace(0, 1, 100), 'explore_prob')  # Probability of exploration-focused mutation\n\n        # Define fuzzy membership functions (example: trapezoidal for simplicity)\n        self.improvement_rate['low'] = fuzz.trapmf(self.improvement_rate.universe, 0, 0, 0.3, 0.5)\n        self.improvement_rate['medium'] = fuzz.trimf(self.improvement_rate.universe, 0.3, 0.5, 0.7)\n        self.improvement_rate['high'] = fuzz.trapmf(self.improvement_rate.universe, 0.5, 0.7, 1, 1)\n\n        self.diversity['low'] = fuzz.trapmf(self.diversity.universe, 0, 0, 0.3, 0.5)\n        self.diversity['medium'] = fuzz.trimf(self.diversity.universe, 0.3, 0.5, 0.7)\n        self.diversity['high'] = fuzz.trapmf(self.diversity.universe, 0.5, 0.7, 1, 1)\n\n        self.explore_prob['low'] = fuzz.trapmf(self.explore_prob.universe, 0, 0, 0.3, 0.5)\n        self.explore_prob['medium'] = fuzz.trimf(self.explore_prob.universe, 0.3, 0.5, 0.7)\n        self.explore_prob['high'] = fuzz.trapmf(self.explore_prob.universe, 0.5, 0.7, 1, 1)\n\n        # Define fuzzy rules (example)\n        rule1 = ctrl.Rule(self.improvement_rate['low'] & self.diversity['low'], self.explore_prob['high'])\n        rule2 = ctrl.Rule(self.improvement_rate['high'] & self.diversity['high'], self.explore_prob['low'])\n        rule3 = ctrl.Rule(self.improvement_rate['medium'] | self.diversity['medium'], self.explore_prob['medium'])\n\n        self.fuzzy_ctrl = ctrl.ControlSystem([rule1, rule2, rule3])\n        self.fuzzy_sim = ctrl.ControlSystemSimulation(self.fuzzy_ctrl)\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        # Initialize two populations\n        main_pop_size = self.pop_size\n        elite_pop_size = int(self.pop_size * self.elite_ratio)\n\n        main_population = np.random.uniform(lb, ub, size=(main_pop_size, self.dim))\n        elite_population = np.random.uniform(lb, ub, size=(elite_pop_size, self.dim))  # Smaller elite population\n\n        main_fitness = np.array([func(x) for x in main_population])\n        elite_fitness = np.array([func(x) for x in elite_population])\n\n        self.budget -= (main_pop_size + elite_pop_size)\n        self.f_opt = np.min(np.concatenate([main_fitness, elite_fitness]))\n        self.x_opt = main_population[np.argmin(main_fitness)] if np.min(main_fitness) < np.min(elite_fitness) else elite_population[np.argmin(elite_fitness)]\n\n        generation = 0\n        while self.budget > 0:\n            generation += 1\n\n            # --- Main Population DE ---\n            for i in range(main_pop_size):\n                if self.budget <= 0: break\n\n                idxs = [idx for idx in range(main_pop_size) if idx != i]\n                a, b, c = main_population[np.random.choice(idxs, 3, replace=False)]\n\n                # Adaptive Exploration/Exploitation using Fuzzy Logic\n                improvement_rate = (np.mean(main_fitness) - np.min(main_fitness)) / (np.max(main_fitness) - np.min(main_fitness) + 1e-8)\n                diversity_values = np.std(main_population, axis=0)\n                diversity = np.mean(diversity_values) / (ub - lb + 1e-8)  # Normalize diversity\n                \n                self.fuzzy_sim.input['improvement_rate'] = improvement_rate\n                self.fuzzy_sim.input['diversity'] = diversity\n                self.fuzzy_sim.compute()\n                explore_probability = self.fuzzy_sim.output['explore_prob']\n\n                if np.random.rand() < explore_probability:\n                    # Exploration: More aggressive mutation\n                    mutant = np.clip(a + (np.random.rand() * 2 - 1) * (b - c), lb, ub)\n                else:\n                    # Exploitation: Standard DE mutation\n                    mutant = np.clip(a + self.F * (b - c), lb, ub)\n\n                trial = self.crossover(main_population[i], mutant, self.Cr)\n                f_trial = func(trial)\n                self.budget -= 1\n\n                if f_trial < main_fitness[i]:\n                    main_fitness[i] = f_trial\n                    main_population[i] = trial\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                \n                    #Archive update\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(main_population[i].copy())\n                        self.archive_fitness.append(main_fitness[i])\n                    else:\n                        max_archive_index = np.argmax(self.archive_fitness)\n                        if main_fitness[i] < self.archive_fitness[max_archive_index]:\n                            self.archive[max_archive_index] = main_population[i].copy()\n                            self.archive_fitness[max_archive_index] = main_fitness[i]\n\n            # --- Elite Population Local Search ---\n            for i in range(elite_pop_size):\n                if self.budget <= 0: break\n                if np.random.rand() < self.local_search_probability:\n                    # Perform local search around the elite individual\n                    current_pos = elite_population[i].copy()\n                    step_size = 0.01 * (ub - lb)  # Small step size for local search\n                    new_pos = current_pos + np.random.uniform(-step_size, step_size, size=self.dim)\n                    new_pos = np.clip(new_pos, lb, ub)\n                    f_new = func(new_pos)\n                    self.budget -= 1\n                    if f_new < elite_fitness[i]:\n                        elite_fitness[i] = f_new\n                        elite_population[i] = new_pos\n                        if f_new < self.f_opt:\n                            self.f_opt = f_new\n                            self.x_opt = new_pos\n\n            # --- Population Update: Elitism ---\n            combined_population = np.concatenate((main_population, elite_population))\n            combined_fitness = np.concatenate((main_fitness, elite_fitness))\n            sorted_indices = np.argsort(combined_fitness)\n            main_population = combined_population[sorted_indices[:main_pop_size]]\n            main_fitness = combined_fitness[sorted_indices[:main_pop_size]]\n\n            # Adapt F and Cr (simple adaptation)\n            self.F = np.clip(self.F + np.random.normal(0, 0.05), 0.1, 1.0)\n            self.Cr = np.clip(self.Cr + np.random.normal(0, 0.05), 0.1, 1.0)\n\n        return self.f_opt, self.x_opt\n\n    def crossover(self, x, mutant, cr):\n        trial = x.copy()\n        for j in range(self.dim):\n            if np.random.rand() < cr:\n                trial[j] = mutant[j]\n        return trial", "configspace": "", "generation": 6, "feedback": "In the code, line 2, in <module>, the following error occurred:\nModuleNotFoundError: No module named 'skfuzzy'\nOn line: import skfuzzy as fuzz", "error": "In the code, line 2, in <module>, the following error occurred:\nModuleNotFoundError: No module named 'skfuzzy'\nOn line: import skfuzzy as fuzz", "parent_ids": ["06b6ae75-0ec5-408d-a4a3-4059dc60b0ac"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "f6959d33-90f3-4c8e-ae79-758a3bf085a7", "fitness": "-inf", "name": "SHADE_OD", "description": "Self-adaptive Differential Evolution with a combined mutation strategy (DE/rand/1 and DE/current-to-best/1), success-history based parameter adaptation with a forgetting mechanism, and orthogonal design-based initialization.", "code": "import numpy as np\n\nclass SHADE_OD:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=50, memory_size=10, F=0.5, Cr=0.5, p_best=0.1, p_rand=0.5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.memory_size = memory_size\n        self.F = F\n        self.Cr = Cr\n        self.p_best = p_best\n        self.p_rand = p_rand\n        self.archive = []\n        self.memory_F = np.full(self.memory_size, 0.5)\n        self.memory_Cr = np.full(self.memory_size, 0.5)\n        self.success_F = []\n        self.success_Cr = []\n        self.memory_idx = 0\n        self.orthogonal_matrix = self.generate_orthogonal_array(self.pop_size, self.dim)\n\n    def generate_orthogonal_array(self, n, k):\n        if n <= 0 or k <= 0:\n            return np.array([])\n\n        if n == 1:\n            return np.zeros((1, k))\n        \n        if n == 2:\n            return np.random.choice([0, 1], size=(n, k))\n\n        # Simple placeholder if a more robust orthogonal array generation isn't feasible\n        # Ensure orthogonality to some extent: each column pair has roughly equal co-occurrences of all possible pairs of values\n        arr = np.random.choice([0, 1], size=(n, k))\n        return arr\n    \n    def initialize_population(self, lb, ub):\n        population = np.zeros((self.pop_size, self.dim))\n        for i in range(self.pop_size):\n            for j in range(self.dim):\n                population[i, j] = lb + (ub - lb) * self.orthogonal_matrix[i % self.orthogonal_matrix.shape[0], j]\n\n        return population\n\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = self.initialize_population(lb, ub)  # Orthogonal initialization\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)]\n\n        while self.budget > 0:\n            self.success_F = []\n            self.success_Cr = []\n            for i in range(self.pop_size):\n                # Mutation Strategy Selection\n                if np.random.rand() < self.p_rand:\n                    # DE/rand/1\n                    idxs = [idx for idx in range(self.pop_size) if idx != i]\n                    a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                    mutant = a + self.F * (b - c)\n                else:\n                    # DE/current-to-best/1\n                    best_idx = np.argmin(fitness)\n                    idxs = [idx for idx in range(self.pop_size) if idx != i]\n                    a, b = population[np.random.choice(idxs, 2, replace=False)]\n                    mutant = population[i] + self.F * (population[best_idx] - population[i]) + self.F * (a - b)\n                mutant = np.clip(mutant, lb, ub)\n\n\n                # Crossover\n                Cr = self.memory_Cr[self.memory_idx]\n                trial = self.crossover(population[i], mutant, Cr)\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                if f_trial < fitness[i]:\n                    self.success_F.append(self.F)\n                    self.success_Cr.append(Cr)\n                    fitness[i] = f_trial\n                    population[i] = trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                    \n                    # Update archive\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(population[i].copy())\n                    else:\n                        idx_to_replace = np.random.randint(self.archive_size)\n                        self.archive[idx_to_replace] = population[i].copy()\n\n\n            # Memory update (SHADE parameter adaptation)\n            if self.success_F:\n                self.memory_F[self.memory_idx] = np.mean(self.success_F)\n            if self.success_Cr:\n                self.memory_Cr[self.memory_idx] = np.mean(self.success_Cr)\n                \n            self.F = self.memory_F[self.memory_idx]\n            self.Cr = self.memory_Cr[self.memory_idx]\n\n            self.memory_idx = (self.memory_idx + 1) % self.memory_size\n            \n            # Forgetting mechanism: Add a small random perturbation to the historical values\n            self.memory_F = np.clip(self.memory_F + 0.05 * np.random.normal(0, 0.1, self.memory_size), 0.1, 1.0)\n            self.memory_Cr = np.clip(self.memory_Cr + 0.05 * np.random.normal(0, 0.1, self.memory_size), 0.1, 1.0)\n\n        return self.f_opt, self.x_opt\n\n    def crossover(self, x, mutant, cr):\n        trial = x.copy()\n        j_rand = np.random.randint(self.dim)\n        for j in range(self.dim):\n            if np.random.rand() < cr or j == j_rand:\n                trial[j] = mutant[j]\n        return trial", "configspace": "", "generation": 6, "feedback": "In the code, line 41, in initialize_population, the following error occurred:\nValueError: setting an array element with a sequence.\nOn line: population[i, j] = lb + (ub - lb) * self.orthogonal_matrix[i % self.orthogonal_matrix.shape[0], j]", "error": "In the code, line 41, in initialize_population, the following error occurred:\nValueError: setting an array element with a sequence.\nOn line: population[i, j] = lb + (ub - lb) * self.orthogonal_matrix[i % self.orthogonal_matrix.shape[0], j]", "parent_ids": ["c644790d-f6f7-49f4-87d6-80d5786151e8"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "983c0415-7dbc-4bb8-9c8b-41db96888c33", "fitness": "-inf", "name": "RestartCMAES", "description": "Covariance Matrix Adaptation Evolution Strategy (CMA-ES) with a decaying learning rate and a population restart mechanism for escaping local optima.", "code": "import numpy as np\n\nclass RestartCMAES:\n    def __init__(self, budget=10000, dim=10, pop_size=None, sigma0=0.3, mu_factor=0.25, restart_trigger=1e-12):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size if pop_size is not None else 4 + int(3 * np.log(dim))\n        self.sigma0 = sigma0\n        self.mu = int(self.pop_size * mu_factor)  # Number of parents\n        self.weights = np.log(self.pop_size + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights /= np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        self.restart_trigger = restart_trigger\n\n        # Strategy parameter setting: Adaptation\n        self.cs = (self.mueff + 2) / (self.dim + self.mueff + 5)\n        self.damps = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1) + self.cs\n        self.ccov = (1 / self.mueff) * ((self.mueff + self.dim) / (self.dim + 2)**2) + (1 - 1 / self.mueff) * min(1, (2 * self.mueff - 1) / ((self.dim + 2)**2 + self.mueff))\n        self.cc = (4 + self.mueff / self.dim) / (self.dim + 4 + 2 * self.mueff / self.dim)\n        self.chiN = self.dim**0.5 * (1 - 1/(4*self.dim) + 1/(21*self.dim**2))\n        self.decay_rate = 0.999  # Decay rate for sigma\n\n        self.mean = None\n        self.sigma = sigma0\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.C = np.eye(self.dim)  # Covariance matrix\n        self.B = None\n        self.D = None\n        self.restart_count = 0\n\n    def sample_population(self):\n        z = np.random.randn(self.dim, self.pop_size)\n        y = self.B @ (self.D * z)\n        x = self.mean[:, np.newaxis] + self.sigma * y\n        return x.T\n\n    def update_distribution(self, x, fitness):\n        # Selection and Recombination\n        idx = np.argsort(fitness)\n        x_mu = x[idx[:self.mu]]\n        self.mean = np.sum(x_mu * self.weights[:, np.newaxis], axis=0)\n\n        # Cumulation: Update evolution paths\n        y_mu = (x_mu - self.mean) / self.sigma\n        z_mu = np.linalg.solve(self.B, y_mu.T) / self.D\n        self.ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs) * self.mueff) * (self.B @ np.sum(self.weights[:, np.newaxis] * z_mu, axis=1))\n        hsig = np.linalg.norm(self.ps) / np.sqrt(1 - (1 - self.cs)**(2 * self.budget / self.pop_size)) / self.chiN < 1.4 + 2 / (self.dim + 1)\n        self.pc = (1 - self.cc) * self.pc + hsig * np.sqrt(self.cc * (2 - self.cc) * self.mueff) * np.mean(y_mu, axis=0)\n\n        # Covariance matrix adaptation\n        artmp = (1/self.sigma) * (x_mu - self.mean).T\n        self.C = (1 - self.ccov) * self.C + self.ccov * (1/self.mueff) * (artmp @ np.diag(self.weights) @ artmp.T)  + self.ccov * (1 - hsig) * self.cc * (2 - self.cc) * self.C\n\n        self.sigma *= np.exp((self.cs / self.damps) * (np.linalg.norm(self.ps) / self.chiN - 1))\n        self.sigma *= self.decay_rate # Decay sigma over time\n\n\n    def __call__(self, func):\n        self.mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        self.C = np.eye(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.pc = np.zeros(self.dim)\n\n        self.B, self.D = np.linalg.eigh(self.C)  # Eigen decomposition\n        self.D = np.sqrt(np.diag(self.D))\n        self.B = self.B  # eigenvectors\n\n        fitness_values = []\n\n        while self.budget > 0:\n            x = self.sample_population()\n            fitness = np.array([func(xi) for xi in x])\n            self.budget -= self.pop_size\n            fitness_values.extend(fitness.tolist())\n            self.update_distribution(x, fitness)\n\n            self.B, self.D = np.linalg.eigh(self.C)  # Eigen decomposition\n            self.D = np.sqrt(np.diag(self.D))\n            self.B = self.B  # eigenvectors\n\n            # Restart mechanism\n            if np.std(fitness) < self.restart_trigger:\n                self.restart_count += 1\n                self.mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                self.sigma = self.sigma0\n                self.C = np.eye(self.dim)\n                self.ps = np.zeros(self.dim)\n                self.pc = np.zeros(self.dim)\n                self.B, self.D = np.linalg.eigh(self.C)  # Eigen decomposition\n                self.D = np.sqrt(np.diag(self.D))\n                self.B = self.B  # eigenvectors\n\n        best_index = np.argmin(fitness_values)\n        return min(fitness_values), None", "configspace": "", "generation": 7, "feedback": "In the code, line 34, in sample_population, the following error occurred:\nValueError: operands could not be broadcast together with shapes (2,) (2,6) \nOn line: y = self.B @ (self.D * z)", "error": "In the code, line 34, in sample_population, the following error occurred:\nValueError: operands could not be broadcast together with shapes (2,) (2,6) \nOn line: y = self.B @ (self.D * z)", "parent_ids": ["d15f2124-2044-40ea-b0ea-bbc76d19a3ef"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "eb50b47c-9470-4b3a-acb6-58a8a2023b31", "fitness": 0.43605325223422575, "name": "SelfAdaptiveDE", "description": "Self-adaptive Differential Evolution with momentum-based parameter adaptation and a simplified parameter tuning mechanism, combined with a perturbation operator for enhanced exploration.", "code": "import numpy as np\n\nclass SelfAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, Cr=0.9, momentum=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F  # Mutation factor\n        self.Cr = Cr  # Crossover rate\n        self.momentum = momentum # Momentum for parameter adaptation\n        self.F_velocity = 0.0\n        self.Cr_velocity = 0.0\n\n    def __call__(self, func):\n        # Initialize population\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        population = np.random.uniform(lb, ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size # Deduct initial population evaluations from budget\n\n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)]\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + self.F * (b - c), lb, ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.Cr\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Perturbation (exploration) - Add small random noise\n                trial = np.clip(trial + np.random.normal(0, 0.01, self.dim), lb, ub)\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    population[i] = trial\n\n                    # Update optimal solution\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n                        # Simplified parameter tuning: if improvement, slightly increase F and Cr\n                        self.F = np.clip(self.F * 1.05, 0.1, 1.0)\n                        self.Cr = np.clip(self.Cr * 1.05, 0.1, 1.0)\n                else:\n                    # If no improvement, slightly decrease F and Cr\n                    self.F = np.clip(self.F * 0.95, 0.1, 1.0)\n                    self.Cr = np.clip(self.Cr * 0.95, 0.1, 1.0)\n\n            # Momentum-based parameter adaptation\n            self.F_velocity = self.momentum * self.F_velocity + (self.F - 0.5) * (1 - self.momentum)\n            self.Cr_velocity = self.momentum * self.Cr_velocity + (self.Cr - 0.9) * (1 - self.momentum)\n\n            self.F = np.clip(self.F + self.F_velocity, 0.1, 1.0)\n            self.Cr = np.clip(self.Cr + self.Cr_velocity, 0.1, 1.0)\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "The algorithm SelfAdaptiveDE scored 0.436 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["94768647-f252-46d5-98c5-9ea8189ba67b"], "operator": null, "metadata": {"aucs": [0.19038345299574122, 0.21678533252638765, 0.45123211668605256, 0.5361765675377994, 0.3995393463357364, 0.5481824590674733, 0.33362898863352297, 0.4083271871011871, 0.39353987985942707, 0.20704676015511836, 0.3621339469434218, 0.986094298068402, 0.30295715857323113, 0.3210015216873756, 0.8262442865044303, 0.5364562778407388, 0.38074687614722735, 0.588404441439365, 0.21201209817627686, 0.5201720484055992]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "f506774e-0cbc-4571-bd1a-a7a1612489cb", "fitness": 0.09225150782321295, "name": "SelfAdaptiveDE", "description": "Enhanced Self-Adaptive Differential Evolution with a decay-based learning rate and population diversity maintenance.", "code": "import numpy as np\n\nclass SelfAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, Cr=0.9, diversity_threshold=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F  # Mutation factor\n        self.Cr = Cr  # Crossover rate\n        self.learning_rate = 0.1  # Initial learning rate for parameter adaptation\n        self.diversity_threshold = diversity_threshold  # Threshold for population diversity\n        self.decay_rate = 0.995 # Decay rate for the learning rate\n\n    def __call__(self, func):\n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size # Deduct initial population evaluations from budget\n\n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)]\n\n        generation = 0\n        while self.budget > 0:\n            generation += 1\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + self.F * (b - c), func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.Cr\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    population[i] = trial\n\n                    # Update optimal solution\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                        # Increase learning rate if improvement is found\n                        self.learning_rate = min(1.0, self.learning_rate * 1.1)\n                else:\n                    # No immediate improvement, slightly perturb F and Cr\n                    self.F = np.clip(self.F + 0.01 * np.random.normal(0, 0.1), 0.1, 1.0)\n                    self.Cr = np.clip(self.Cr + 0.01 * np.random.normal(0, 0.1), 0.1, 1.0)\n\n            # Adapt parameters based on learning rate\n            self.F = np.clip(self.F + self.learning_rate * np.random.normal(0, 0.1), 0.1, 1.0)\n            self.Cr = np.clip(self.Cr + self.learning_rate * np.random.normal(0, 0.1), 0.1, 1.0)\n\n            # Decay the learning rate\n            self.learning_rate *= self.decay_rate\n\n            # Population Diversity Check and Adjustment\n            if self.population_diversity(population) < self.diversity_threshold:\n                # Reset a portion of the population to random values\n                num_to_reset = int(0.1 * self.pop_size)\n                indices_to_reset = np.random.choice(self.pop_size, num_to_reset, replace=False)\n                population[indices_to_reset] = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(num_to_reset, self.dim))\n                fitness[indices_to_reset] = [func(x) for x in population[indices_to_reset]]\n                self.budget -= num_to_reset  # Update budget\n\n        return self.f_opt, self.x_opt\n\n    def population_diversity(self, population):\n        \"\"\"Calculates the average pairwise distance between individuals in the population.\"\"\"\n        distances = []\n        for i in range(self.pop_size):\n            for j in range(i + 1, self.pop_size):\n                distances.append(np.linalg.norm(population[i] - population[j]))\n        return np.mean(distances) if distances else 1.0", "configspace": "", "generation": 7, "feedback": "The algorithm SelfAdaptiveDE scored 0.092 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["94768647-f252-46d5-98c5-9ea8189ba67b"], "operator": null, "metadata": {"aucs": [0.1845030156464259, 0]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "49246577-85cb-42a5-b533-099478958489", "fitness": "-inf", "name": "SelfAdaptiveDE", "description": "Self-adaptive Differential Evolution with simplified parameter adaptation, adaptive population size, and probabilistic archive usage for enhanced exploration and exploitation.", "code": "import numpy as np\n\nclass SelfAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=10, F=0.5, Cr=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.F = F\n        self.Cr = Cr\n        self.archive = []\n        self.archive_fitness = []\n        self.min_pop_size = 10\n        self.max_pop_size = 100\n        self.success_F = []\n        self.success_Cr = []\n        self.memory_size = 10\n\n    def __call__(self, func):\n        # Initialize population\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)]\n\n        while self.budget > 0:\n\n            # Adjust population size based on success (simplified)\n            if len(self.success_F) > self.memory_size // 2 and np.mean(self.success_F) > self.F:\n                self.pop_size = min(self.pop_size + 1, self.max_pop_size)\n            elif len(self.success_F) > self.memory_size // 2 and np.mean(self.success_F) < self.F:\n                 self.pop_size = max(self.pop_size - 1, self.min_pop_size)\n\n\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                if len(self.archive) > 0 and np.random.rand() < 0.2:  #Probabilistic archive usage\n                    arch_idx = np.random.randint(len(self.archive))\n                    a = self.archive[arch_idx]\n                mutant = np.clip(a + self.F * (b - c), lb, ub)\n\n                # Crossover\n                trial = self.crossover(population[i], mutant, self.Cr)\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                if f_trial < fitness[i]:\n                    # Success\n                    self.success_F.append(self.F)\n                    self.success_Cr.append(self.Cr)\n                    if len(self.success_F) > self.memory_size:\n                        self.success_F.pop(0)\n                        self.success_Cr.pop(0)\n\n                    fitness[i] = f_trial\n                    population[i] = trial\n                    # Update optimal solution\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n                    # Update archive\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(population[i].copy())\n                        self.archive_fitness.append(fitness[i])\n                    else:\n                        max_archive_index = np.argmax(self.archive_fitness)\n                        if fitness[i] < self.archive_fitness[max_archive_index]:\n                            self.archive[max_archive_index] = population[i].copy()\n                            self.archive_fitness[max_archive_index] = fitness[i]\n\n                #Adapt F and Cr (simplified - no learning rate)\n                self.F = np.clip(np.random.normal(np.mean(self.success_F) if self.success_F else 0.5, 0.1), 0.1, 1.0)\n                self.Cr = np.clip(np.random.normal(np.mean(self.success_Cr) if self.success_Cr else 0.9, 0.1), 0.1, 1.0)\n\n\n        return self.f_opt, self.x_opt\n\n    def crossover(self, x, mutant, cr):\n        trial = x.copy()\n        for j in range(self.dim):\n            if np.random.rand() < cr:\n                trial[j] = mutant[j]\n        return trial", "configspace": "", "generation": 7, "feedback": "In the code, line 41, in __call__, the following error occurred:\nIndexError: index 50 is out of bounds for axis 0 with size 50\nOn line: a, b, c = population[np.random.choice(idxs, 3, replace=False)]", "error": "In the code, line 41, in __call__, the following error occurred:\nIndexError: index 50 is out of bounds for axis 0 with size 50\nOn line: a, b, c = population[np.random.choice(idxs, 3, replace=False)]", "parent_ids": ["06b6ae75-0ec5-408d-a4a3-4059dc60b0ac"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "38a357a2-c730-4249-b293-d91964d445f7", "fitness": 0.0, "name": "SelfAdaptiveDE", "description": "Self-adaptive Differential Evolution with simplified parameter adaptation, stochastic ranking for constraint handling, and periodic population rejuvenation to enhance exploration.", "code": "import numpy as np\n\nclass SelfAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=5, F=0.5, Cr=0.9, rejuvenation_rate=0.05):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.F = F  # Mutation factor\n        self.Cr = Cr  # Crossover rate\n        self.rejuvenation_rate = rejuvenation_rate\n        self.archive = []\n\n    def __call__(self, func):\n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size  # Deduct initial population evaluations from budget\n\n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)]\n\n        generation = 0\n        while self.budget > 0:\n            generation += 1\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n\n                # Mutant Generation\n                mutant = np.clip(a + self.F * (b - c), func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.Cr\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n\n                # Stochastic Ranking\n                if f_trial < fitness[i]:\n                    # Archive the replaced individual for diversity\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(population[i].copy())\n                    else:\n                        idx_to_replace = np.random.randint(0, self.archive_size)\n                        self.archive[idx_to_replace] = population[i].copy()\n                    fitness[i] = f_trial\n                    population[i] = trial\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n            # Adapt F and Cr (simplified)\n            self.F = np.clip(self.F + 0.1 * np.random.normal(0, 0.1), 0.1, 1.0)\n            self.Cr = np.clip(self.Cr + 0.1 * np.random.normal(0, 0.1), 0.1, 1.0)\n\n            # Population Rejuvenation\n            if np.random.rand() < self.rejuvenation_rate:\n                worst_index = np.argmax(fitness)\n                population[worst_index] = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                fitness[worst_index] = func(population[worst_index])\n                self.budget -= 1\n                if fitness[worst_index] < self.f_opt:\n                    self.f_opt = fitness[worst_index]\n                    self.x_opt = population[worst_index]\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "The algorithm SelfAdaptiveDE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["d15f2124-2044-40ea-b0ea-bbc76d19a3ef"], "operator": null, "metadata": {"aucs": [0]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "aa44511f-8e44-439a-8c9a-af3e2adbf6b4", "fitness": "-inf", "name": "SelfAdaptiveCMAESDE", "description": "A population-based algorithm with self-adaptive covariance matrix adaptation, differential mutation, and a restart mechanism for enhanced exploration and exploitation in challenging landscapes.", "code": "import numpy as np\n\nclass SelfAdaptiveCMAESDE:\n    def __init__(self, budget=10000, dim=10, pop_size=None, initial_sigma=0.5, c_sigma=0.3, d_sigma=1.0, c_c=0.1, c_1=0.1, c_mu=0.1, mu_ratio=0.25, restart_trigger=1e-12):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size if pop_size is not None else 4 + int(3 * np.log(dim))\n        self.initial_sigma = initial_sigma\n        self.c_sigma = c_sigma\n        self.d_sigma = d_sigma\n        self.c_c = c_c\n        self.c_1 = c_1\n        self.c_mu = c_mu\n        self.mu_ratio = mu_ratio\n        self.restart_trigger = restart_trigger\n\n        self.mu = int(self.pop_size * self.mu_ratio)\n        self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n        self.weights /= np.sum(self.weights)\n\n    def __call__(self, func):\n        # Initialization\n        mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        sigma = self.initial_sigma\n        C = np.eye(self.dim)\n        p_sigma = np.zeros(self.dim)\n        p_c = np.zeros(self.dim)\n        eigen_decomposition = np.linalg.eigh(C)  # Perform eigendecomposition once\n        B = eigen_decomposition[1]\n        D = np.sqrt(eigen_decomposition[0])\n\n        f_opt = float('inf')\n        x_opt = None\n        evals = 0\n\n        generation = 0\n        while evals < self.budget:\n            generation += 1\n            # Generate samples\n            z = np.random.randn(self.dim, self.pop_size)\n            x = mean[:, np.newaxis] + sigma * (B @ (D * z))\n            x = np.clip(x, func.bounds.lb, func.bounds.ub)\n\n            # Evaluate samples\n            fitness = np.array([func(xi) for xi in x.T])\n            evals += self.pop_size\n            if evals > self.budget:\n                fitness = fitness[:self.pop_size - (evals - self.budget)]\n                x = x[:, :self.pop_size - (evals - self.budget)]\n                \n            # Update optimal solution\n            if np.min(fitness) < f_opt:\n                f_opt = np.min(fitness)\n                x_opt = x[:, np.argmin(fitness)]\n\n            # Selection and Recombination\n            fitness_sorted_idx = np.argsort(fitness)\n            x_mu = x[:, fitness_sorted_idx[:self.mu]]\n            mean_old = mean.copy()\n            mean = np.sum(x_mu * self.weights[np.newaxis, :], axis=1)\n\n            # Covariance Matrix Adaptation\n            p_sigma = (1 - self.c_sigma) * p_sigma + np.sqrt(self.c_sigma * (2 - self.c_sigma)) * (B @ ((mean - mean_old) / sigma / D))\n            C_sigma = np.sum(p_sigma**2)\n            sigma *= np.exp((self.c_sigma / self.d_sigma) * (C_sigma - self.dim) / 2)\n\n            p_c = (1 - self.c_c) * p_c + np.sqrt(self.c_c * (2 - self.c_c)) * ((mean - mean_old) / sigma)\n            C = (1 - self.c_1 - self.c_mu) * C + self.c_1 * (p_c[:, np.newaxis] @ p_c[np.newaxis, :])\n            for i in range(self.mu):\n                C += self.c_mu * self.weights[i] * ((x_mu[:, i] - mean_old)[:, np.newaxis] @ (x_mu[:, i] - mean_old)[np.newaxis, :] / sigma**2)\n\n            # Ensure positive definiteness\n            C = np.triu(C) + np.triu(C, 1).T\n\n            # Eigendecomposition update only if necessary\n            if (generation % (1 + int(10*self.dim/self.pop_size))) == 0:\n                try:\n                    eigen_decomposition = np.linalg.eigh(C)\n                    B = eigen_decomposition[1]\n                    D = np.sqrt(np.maximum(eigen_decomposition[0], 0))  # Ensure positive eigenvalues\n                except np.linalg.LinAlgError:\n                    # Reset if decomposition fails\n                    C = np.eye(self.dim)\n                    p_sigma = np.zeros(self.dim)\n                    p_c = np.zeros(self.dim)\n                    eigen_decomposition = np.linalg.eigh(C)\n                    B = eigen_decomposition[1]\n                    D = np.sqrt(eigen_decomposition[0])\n                    sigma = self.initial_sigma\n\n            # Restart mechanism\n            if np.max(D) / np.min(D) > 1e7 or sigma < self.restart_trigger:\n                mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                sigma = self.initial_sigma\n                C = np.eye(self.dim)\n                p_sigma = np.zeros(self.dim)\n                p_c = np.zeros(self.dim)\n                eigen_decomposition = np.linalg.eigh(C)\n                B = eigen_decomposition[1]\n                D = np.sqrt(eigen_decomposition[0])\n\n        return f_opt, x_opt", "configspace": "", "generation": 7, "feedback": "In the code, line 41, in __call__, the following error occurred:\nValueError: operands could not be broadcast together with shapes (2,) (2,6) \nOn line: x = mean[:, np.newaxis] + sigma * (B @ (D * z))", "error": "In the code, line 41, in __call__, the following error occurred:\nValueError: operands could not be broadcast together with shapes (2,) (2,6) \nOn line: x = mean[:, np.newaxis] + sigma * (B @ (D * z))", "parent_ids": ["94768647-f252-46d5-98c5-9ea8189ba67b"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "822114ec-47d3-4173-85d4-b27d4bbede57", "fitness": 0.5657608503551507, "name": "AdaptiveDE", "description": "Adaptive Differential Evolution with momentum-based population update and orthogonal design for exploration.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, Cr=0.9, momentum=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.Cr = Cr\n        self.momentum = momentum\n        self.population_velocity = np.zeros((pop_size, dim))\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)]\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + self.F * (b - c), lb, ub)\n\n                # Orthogonal Crossover\n                trial = self.orthogonal_crossover(population[i], mutant, self.Cr)\n\n                f_trial = func(trial)\n                self.budget -= 1\n\n                if f_trial < fitness[i]:\n                    # Momentum update for population\n                    velocity = self.momentum * self.population_velocity[i] + (1 - self.momentum) * (trial - population[i])\n                    population[i] = trial\n                    self.population_velocity[i] = velocity\n                    fitness[i] = f_trial\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n            # Adaptive F and Cr (simplified)\n            success_indices = fitness < np.mean(fitness) #Indices better than mean\n            if np.any(success_indices):\n                self.F = np.clip(np.mean(self.F + 0.1 * np.random.randn(1)), 0.1, 1.0)\n                self.Cr = np.clip(np.mean(self.Cr + 0.1 * np.random.randn(1)), 0.1, 1.0)\n\n\n        return self.f_opt, self.x_opt\n\n    def orthogonal_crossover(self, x, mutant, cr):\n        trial = x.copy()\n        for j in range(self.dim):\n            if np.random.rand() < cr:\n                trial[j] = mutant[j]\n        return trial", "configspace": "", "generation": 7, "feedback": "The algorithm AdaptiveDE scored 0.566 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["06b6ae75-0ec5-408d-a4a3-4059dc60b0ac"], "operator": null, "metadata": {"aucs": [0.2709127699077867, 0.3015011367491789, 0.5443201247626166, 0.8848716416084741, 0.5267165415405226, 0.7598690937215302, 0.4049181524531269, 0.4246201463598125, 0.560319078944707, 0.30171331073643015, 0.6786696372558487, 0.9964533999036922, 0.5996252860031709, 0.4996333117848971, 0.8342627829339649, 0.7255299700309005, 0.4860444453305246, 0.6155624329715277, 0.3933220314662853, 0.5063517126380168]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "22fadf31-7554-4aa1-9fbb-6cb307d30f85", "fitness": 0.0, "name": "AdaptiveDE", "description": "Simplified Adaptive Differential Evolution with orthogonal crossover and a dynamic archive.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_rate=0.1, F=0.5, Cr=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = int(pop_size * archive_rate)  # Dynamic archive size\n        self.F = F\n        self.Cr = Cr\n        self.archive = []\n        self.lb = None\n        self.ub = None\n\n    def __call__(self, func):\n        self.lb, self.ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index]\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + self.F * (b - c), self.lb, self.ub)\n\n                # Orthogonal Crossover\n                trial = self.orthogonal_crossover(population[i], mutant)\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    population[i] = trial\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n            # Archive update (simplified)\n            if len(self.archive) < self.archive_size:\n                self.archive = population[np.argsort(fitness)[:self.archive_size]].tolist()\n            else:\n                worst_archive_index = np.argmax([func(x) for x in self.archive])\n                best_pop_index = np.argmin(fitness)\n                if fitness[best_pop_index] < func(self.archive[worst_archive_index]):\n                    self.archive[worst_archive_index] = population[best_pop_index].copy()\n\n            # Adaptive F and Cr (simplified) - No memory\n            self.F = np.clip(self.F + 0.01 * np.random.normal(0, 0.1), 0.1, 1.0)\n            self.Cr = np.clip(self.Cr + 0.01 * np.random.normal(0, 0.1), 0.1, 1.0)\n\n        return self.f_opt, self.x_opt\n\n    def orthogonal_crossover(self, x, mutant):\n        trial = x.copy()\n        num_change = int(self.Cr * self.dim)\n        change_indices = np.random.choice(range(self.dim), num_change, replace=False)\n        for j in change_indices:\n            trial[j] = mutant[j]\n        return trial", "configspace": "", "generation": 7, "feedback": "The algorithm AdaptiveDE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["c644790d-f6f7-49f4-87d6-80d5786151e8"], "operator": null, "metadata": {"aucs": [0]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "74b2ad58-453e-4714-84dd-30abf5c60c71", "fitness": 0.6842623461371835, "name": "SelfAdaptiveDE", "description": "Self-adaptive Differential Evolution with orthogonal crossover, velocity-based mutation with probabilistic selection, archive, and success-history adaptation of F and Cr.", "code": "import numpy as np\n\nclass SelfAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=10, F=0.5, Cr=0.9, velocity_weight=0.1, ortho_group_size=5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.F = F  # Mutation factor\n        self.Cr = Cr  # Crossover rate\n        self.velocity_weight = velocity_weight  # Weight for velocity-based mutation\n        self.learning_rate_F = 0.1\n        self.learning_rate_Cr = 0.1\n        self.success_history_F = []\n        self.success_history_Cr = []\n        self.success_history_size = 10\n        self.archive = []\n        self.ortho_group_size = ortho_group_size # Size of groups for orthogonal crossover\n\n\n    def __call__(self, func):\n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size  # Deduct initial population evaluations from budget\n\n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)]\n\n        # Initialize velocity\n        velocity = np.zeros_like(population)\n\n        generation = 0\n        while self.budget > 0:\n            generation += 1\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n\n                # Velocity update\n                velocity[i] = self.velocity_weight * velocity[i] + self.F * (b - c)\n\n                # Apply mutation with probabilistic velocity selection\n                if np.random.rand() < 0.5:  # Apply velocity with 50% probability\n                    mutant = np.clip(a + velocity[i], func.bounds.lb, func.bounds.ub)\n                else:\n                    mutant = np.clip(a + self.F * (b - c), func.bounds.lb, func.bounds.ub)  # Original DE mutation\n\n\n                # Orthogonal Crossover\n                trial = population[i].copy()\n                num_groups = self.dim // self.ortho_group_size\n                for g in range(num_groups):\n                    start_idx = g * self.ortho_group_size\n                    end_idx = (g + 1) * self.ortho_group_size\n                    if np.random.rand() < self.Cr: # apply crossover to each group with Cr probability\n                        for j in range(start_idx, end_idx):\n                            trial[j] = mutant[j]\n\n                # handle remaining dimensions\n                remaining_dims = self.dim % self.ortho_group_size\n                if remaining_dims > 0 and np.random.rand() < self.Cr:\n                    start_idx = self.dim - remaining_dims\n                    for j in range(start_idx, self.dim):\n                         trial[j] = mutant[j]\n\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n\n                if f_trial < fitness[i]:\n                    # Archive the replaced individual for diversity\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(population[i].copy())\n                    else:\n                        idx_to_replace = np.random.randint(0, self.archive_size)\n                        self.archive[idx_to_replace] = population[i].copy()\n\n                    fitness[i] = f_trial\n                    population[i] = trial\n\n                    # Update optimal solution\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n                    # Update success history for F and Cr adaptation\n                    self.success_history_F.append(self.F)\n                    self.success_history_Cr.append(self.Cr)\n                    if len(self.success_history_F) > self.success_history_size:\n                        self.success_history_F.pop(0)\n                        self.success_history_Cr.pop(0)\n\n            # Adapt F and Cr based on success history\n            if self.success_history_F:\n                self.F = np.clip(np.mean(self.success_history_F) + self.learning_rate_F * np.random.normal(0, 0.1), 0.1, 1.0)\n            else:\n                self.F = np.clip(self.F + self.learning_rate_F * np.random.normal(0, 0.1), 0.1, 1.0)\n\n            if self.success_history_Cr:\n                self.Cr = np.clip(np.mean(self.success_history_Cr) + self.learning_rate_Cr * np.random.normal(0, 0.1), 0.1, 1.0)\n            else:\n                self.Cr = np.clip(self.Cr + self.learning_rate_Cr * np.random.normal(0, 0.1), 0.1, 1.0)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "The algorithm SelfAdaptiveDE scored 0.684 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["d15f2124-2044-40ea-b0ea-bbc76d19a3ef"], "operator": null, "metadata": {"aucs": [0.3537404412168368, 0.6213546724284391, 0.6871237074396475, 0.8712744207000515, 0.7397089469219782, 0.7367524096903579, 0.5975652844139849, 0.6979884836366057, 0.726451732544448, 0.4695319241359225, 0.8582957944417018, 0.997409893733556, 0.7205405818124633, 0.7112763503817024, 0.932072109712311, 0.7392104610237054, 0.492685810925792, 0.8136021359759436, 0.42627377137366307, 0.4923879902345597]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "9da0d10c-9c30-4e21-8f77-95b36b46b5e5", "fitness": "-inf", "name": "ImprovedAdaptiveMutationDE", "description": "Improved Adaptive Mutation DE with combined Cauchy and Gaussian mutation, distance-based parameter adaptation, and periodic population refreshment.", "code": "import numpy as np\n\nclass ImprovedAdaptiveMutationDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=10, F=0.5, Cr=0.9, lr_F=0.1, lr_Cr=0.1, p_cauchy=0.1, refresh_rate=0.2):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.F = F\n        self.Cr = Cr\n        self.lr_F = lr_F\n        self.lr_Cr = lr_Cr\n        self.p_cauchy = p_cauchy  # Probability of using Cauchy mutation\n        self.refresh_rate = refresh_rate # Rate to refresh population\n        self.archive = []\n        self.archive_fitness = []\n        self.memory_F = []\n        self.memory_Cr = []\n        self.memory_size = 10\n        self.success_F = []\n        self.success_Cr = []\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)]\n        iteration = 0\n\n        while self.budget > 0:\n            iteration += 1\n            success_F = []\n            success_Cr = []\n            for i in range(self.pop_size):\n                # Mutation: Adaptive choice between Gaussian and Cauchy mutation\n                if np.random.rand() < self.p_cauchy:\n                    # Cauchy mutation for exploration\n                    idxs = [idx for idx in range(self.pop_size) if idx != i]\n                    a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                    mutant = population[i] + self.F * np.random.standard_cauchy(size=self.dim)\n                    mutant = np.clip(mutant, lb, ub)\n\n\n                else:\n                    # Gaussian mutation for exploitation\n                    idxs = [idx for idx in range(self.pop_size) if idx != i]\n                    if len(self.archive) > 0 and np.random.rand() < 0.1:\n                        idx_archive = np.random.randint(len(self.archive))\n                        a = self.archive[idx_archive]\n                        idxs = np.random.choice(idxs, 2, replace=False)\n                        b, c = population[idxs]\n                        mutant = np.clip(a + self.F * (b - c), lb, ub)\n                    else:\n                        a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                        mutant = np.clip(a + self.F * (b - c), lb, ub)\n\n                # Crossover\n                trial = self.crossover(population[i], mutant, self.Cr)\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                if f_trial < fitness[i]:\n                    success_F.append(self.F)\n                    success_Cr.append(self.Cr)\n                    fitness[i] = f_trial\n                    population[i] = trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n                    # Update archive\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(population[i].copy())\n                        self.archive_fitness.append(fitness[i])\n                    else:\n                        max_archive_index = np.argmax(self.archive_fitness)\n                        if fitness[i] < self.archive_fitness[max_archive_index]:\n                            self.archive[max_archive_index] = population[i].copy()\n                            self.archive_fitness[max_archive_index] = fitness[i]\n\n            # Adaptive F and Cr using success history\n            if success_F:\n                self.F = np.mean(success_F)\n            if success_Cr:\n                self.Cr = np.mean(success_Cr)\n\n            # Distance based parameter adaptation\n            distances = np.linalg.norm(population - self.x_opt, axis=1)\n            self.F = np.clip(self.F + self.lr_F * (np.mean(distances) - np.median(distances)), 0.1, 1.0)\n            self.Cr = np.clip(self.Cr + self.lr_Cr * (np.mean(distances) - np.median(distances)), 0.1, 1.0)\n\n\n            # Periodic population refreshment\n            if iteration % int(self.budget * self.refresh_rate) == 0 and self.budget > self.pop_size:\n                worst_indices = np.argsort(fitness)[-int(self.pop_size * self.refresh_rate):]\n                population[worst_indices] = np.random.uniform(lb, ub, size=(int(self.pop_size * self.refresh_rate), self.dim))\n                new_fitness = np.array([func(x) for x in population[worst_indices]])\n                self.budget -= int(self.pop_size * self.refresh_rate)\n                fitness[worst_indices] = new_fitness\n                if np.min(fitness) < self.f_opt:\n                    self.f_opt = np.min(fitness)\n                    self.x_opt = population[np.argmin(fitness)]\n\n\n        return self.f_opt, self.x_opt\n\n    def crossover(self, x, mutant, cr):\n        trial = x.copy()\n        for j in range(self.dim):\n            if np.random.rand() < cr:\n                trial[j] = mutant[j]\n        return trial", "configspace": "", "generation": 8, "feedback": "In the code, line 98, in __call__, the following error occurred:\nZeroDivisionError: integer modulo by zero\nOn line: if iteration % int(self.budget * self.refresh_rate) == 0 and self.budget > self.pop_size:", "error": "In the code, line 98, in __call__, the following error occurred:\nZeroDivisionError: integer modulo by zero\nOn line: if iteration % int(self.budget * self.refresh_rate) == 0 and self.budget > self.pop_size:", "parent_ids": ["c644790d-f6f7-49f4-87d6-80d5786151e8"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "44072340-79e4-4094-bb29-5e9f84d76c3e", "fitness": 0.6902459916830356, "name": "EnhancedAdaptiveMutationDE", "description": "Enhanced Adaptive Mutation DE with orthogonal crossover, velocity-based mutation, and a weighted archive for diversity.", "code": "import numpy as np\n\nclass EnhancedAdaptiveMutationDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=10, F=0.5, Cr=0.9, lr_F=0.1, lr_Cr=0.1, p_cauchy=0.1, velocity_weight=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.F = F\n        self.Cr = Cr\n        self.lr_F = lr_F\n        self.lr_Cr = lr_Cr\n        self.p_cauchy = p_cauchy  # Probability of using Cauchy mutation\n        self.velocity_weight = velocity_weight\n        self.archive = []\n        self.archive_fitness = []\n        self.memory_F = []\n        self.memory_Cr = []\n        self.memory_size = 10\n        self.success_F = []\n        self.success_Cr = []\n        self.velocities = np.zeros((pop_size, dim)) # Initialize velocities for velocity-based mutation\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)]\n\n        while self.budget > 0:\n            success_F = []\n            success_Cr = []\n            for i in range(self.pop_size):\n                # Mutation: Adaptive choice between Gaussian and Cauchy mutation\n                if np.random.rand() < self.p_cauchy:\n                    # Cauchy mutation for exploration\n                    idxs = [idx for idx in range(self.pop_size) if idx != i]\n                    a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                    mutant = population[i] + self.F * np.random.standard_cauchy(size=self.dim) + self.velocity_weight * self.velocities[i]\n                    mutant = np.clip(mutant, lb, ub)\n\n\n                else:\n                    # Gaussian mutation for exploitation\n                    idxs = [idx for idx in range(self.pop_size) if idx != i]\n                    if len(self.archive) > 0 and np.random.rand() < 0.1:\n                        # Weighted archive selection: favor better archive members\n                        archive_probs = np.exp(-np.array(self.archive_fitness) / self.f_opt)  # Boltzmann selection\n                        archive_probs /= np.sum(archive_probs)\n                        idx_archive = np.random.choice(len(self.archive), p=archive_probs)\n                        a = self.archive[idx_archive]\n                        idxs = np.random.choice(idxs, 2, replace=False)\n                        b, c = population[idxs]\n                        mutant = np.clip(a + self.F * (b - c) + self.velocity_weight * self.velocities[i], lb, ub)\n                    else:\n                        a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                        mutant = np.clip(a + self.F * (b - c) + self.velocity_weight * self.velocities[i], lb, ub)\n\n                # Crossover (Orthogonal Crossover)\n                trial = self.orthogonal_crossover(population[i], mutant, self.Cr)\n                # trial = self.crossover(population[i], mutant, self.Cr)\n\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                if f_trial < fitness[i]:\n                    success_F.append(self.F)\n                    success_Cr.append(self.Cr)\n                    # Update velocity\n                    self.velocities[i] = trial - population[i]\n                    fitness[i] = f_trial\n                    population[i] = trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n                    # Update archive\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(population[i].copy())\n                        self.archive_fitness.append(fitness[i])\n                    else:\n                        max_archive_index = np.argmax(self.archive_fitness)\n                        if fitness[i] < self.archive_fitness[max_archive_index]:\n                            self.archive[max_archive_index] = population[i].copy()\n                            self.archive_fitness[max_archive_index] = fitness[i]\n\n            # Adaptive F and Cr using success history\n            if success_F:\n                self.F = np.mean(success_F)\n            if success_Cr:\n                self.Cr = np.mean(success_Cr)\n\n            # Parameter adaptation with learning rates\n            self.F = np.clip(self.F + self.lr_F * np.random.normal(0, 0.1), 0.1, 1.0)\n            self.Cr = np.clip(self.Cr + self.lr_Cr * np.random.normal(0, 0.1), 0.1, 1.0)\n\n        return self.f_opt, self.x_opt\n\n    def crossover(self, x, mutant, cr):\n        trial = x.copy()\n        for j in range(self.dim):\n            if np.random.rand() < cr:\n                trial[j] = mutant[j]\n        return trial\n\n    def orthogonal_crossover(self, x, mutant, cr):\n        # Orthogonal Crossover\n        trial = x.copy()\n        num_changed = 0\n        for j in range(self.dim):\n            if np.random.rand() < cr:\n                trial[j] = mutant[j]\n                num_changed += 1\n\n        # Ensure at least one dimension is changed\n        if num_changed == 0:\n            j = np.random.randint(self.dim)\n            trial[j] = mutant[j]\n        return trial", "configspace": "", "generation": 8, "feedback": "The algorithm EnhancedAdaptiveMutationDE scored 0.690 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["c644790d-f6f7-49f4-87d6-80d5786151e8"], "operator": null, "metadata": {"aucs": [0.289158407352068, 0.6629036664212711, 0.6978555523038514, 0.8590143914030313, 0.7471526071092782, 0.8100452655358372, 0.5712250331616845, 0.6648024360571853, 0.6179400021141526, 0.6530440121077961, 0.853229644786706, 0.9944505046030774, 0.6742903755718839, 0.7676801510354927, 0.9219366457529101, 0.7850620100891454, 0.5827948845539777, 0.8587993947387403, 0.28707873211929347, 0.5064561168433284]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "a86bde1c-4ecb-47af-9100-0e6f3e7992f5", "fitness": 0.0, "name": "CooperativeNeighborhoodDE", "description": "Cooperative Differential Evolution with Neighborhood-based Mutation and adaptive population sizing to balance exploration and exploitation.", "code": "import numpy as np\n\nclass CooperativeNeighborhoodDE:\n    def __init__(self, budget=10000, dim=10, pop_size_min=20, pop_size_max=100, neighborhood_size=5, F=0.5, Cr=0.9, pop_size_adapt_freq=50):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size_min = pop_size_min\n        self.pop_size_max = pop_size_max\n        self.pop_size = pop_size_min  # Start with smaller population\n        self.neighborhood_size = neighborhood_size\n        self.F = F  # Mutation factor\n        self.Cr = Cr  # Crossover rate\n        self.pop_size_adapt_freq = pop_size_adapt_freq\n        self.success_history_F = []\n        self.success_history_Cr = []\n        self.success_history_size = 10\n        self.learning_rate_F = 0.1\n        self.learning_rate_Cr = 0.1\n        self.pop_size_adapt_rate = 0.1\n\n    def __call__(self, func):\n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size  # Deduct initial population evaluations from budget\n\n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)]\n\n        generation = 0\n        while self.budget > 0:\n            generation += 1\n\n            # Adaptive Population Size\n            if generation % self.pop_size_adapt_freq == 0:\n                if np.std(fitness) < 1e-3: # If population converges, increase pop size\n                    self.pop_size = min(self.pop_size_max, int(self.pop_size * (1 + self.pop_size_adapt_rate)))\n                else: # otherwise decrease it (but not under pop_size_min)\n                     self.pop_size = max(self.pop_size_min, int(self.pop_size * (1 - self.pop_size_adapt_rate)))\n                \n                # Resize population\n                if self.pop_size != population.shape[0]:\n                    old_pop_size = population.shape[0]\n                    population = np.resize(population, (self.pop_size, self.dim))\n                    fitness = np.resize(fitness, self.pop_size)\n                    if self.pop_size > old_pop_size:\n                        population[old_pop_size:] = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size - old_pop_size, self.dim))\n                        fitness[old_pop_size:] = np.array([func(x) for x in population[old_pop_size:]])\n                        self.budget -= (self.pop_size - old_pop_size)\n                    self.f_opt = np.min(fitness)\n                    self.x_opt = population[np.argmin(fitness)]\n            \n            for i in range(self.pop_size):\n                # Neighborhood-based Mutation\n                neighbor_indices = np.random.choice(self.pop_size, self.neighborhood_size, replace=False)\n                \n                # Ensure 'i' is not in the neighborhood indices\n                neighbor_indices = neighbor_indices[neighbor_indices != i]\n                \n                if len(neighbor_indices) < 3:\n                    # Handle edge case if neighborhood size is too small or i was selected\n                    remaining_needed = 3 - len(neighbor_indices)\n                    extra_indices = np.random.choice(self.pop_size, remaining_needed, replace=False)\n                    neighbor_indices = np.concatenate([neighbor_indices, extra_indices])\n                \n                # Still need to remove i if it's present\n                neighbor_indices = neighbor_indices[neighbor_indices != i]\n                if len(neighbor_indices) > 3:\n                     neighbor_indices = neighbor_indices[:3]\n                elif len(neighbor_indices) < 3:\n                   remaining_needed = 3 - len(neighbor_indices)\n                   extra_indices = np.random.choice(self.pop_size, remaining_needed, replace=False)\n                   neighbor_indices = np.concatenate([neighbor_indices, extra_indices])\n\n                a, b, c = population[neighbor_indices[0]], population[neighbor_indices[1]], population[neighbor_indices[2]]\n\n                # Mutation\n                mutant = np.clip(a + self.F * (b - c), func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.Cr\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    population[i] = trial\n\n                    # Update optimal solution\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                    \n                    self.success_history_F.append(self.F)\n                    self.success_history_Cr.append(self.Cr)\n                    if len(self.success_history_F) > self.success_history_size:\n                        self.success_history_F.pop(0)\n                        self.success_history_Cr.pop(0)\n                        \n\n            # Adapt F and Cr based on success history\n            if self.success_history_F:\n                self.F = np.clip(np.mean(self.success_history_F) + self.learning_rate_F * np.random.normal(0, 0.1), 0.1, 1.0)\n            else:\n                self.F = np.clip(self.F + self.learning_rate_F * np.random.normal(0, 0.1), 0.1, 1.0)\n\n            if self.success_history_Cr:\n                self.Cr = np.clip(np.mean(self.success_history_Cr) + self.learning_rate_Cr * np.random.normal(0, 0.1), 0.1, 1.0)\n            else:\n                self.Cr = np.clip(self.Cr + self.learning_rate_Cr * np.random.normal(0, 0.1), 0.1, 1.0)\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "The algorithm CooperativeNeighborhoodDE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["d15f2124-2044-40ea-b0ea-bbc76d19a3ef"], "operator": null, "metadata": {"aucs": [0]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "2c84c754-6535-451d-a412-1ccc94c6b205", "fitness": "-inf", "name": "OrthogonalAdaptiveDE", "description": "Population-based optimization using a combination of JADE-inspired mutation with current-to-pbest, adaptive parameter control with a success-history based memory, and orthogonal learning.", "code": "import numpy as np\n\nclass OrthogonalAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=10, pbest_rate=0.1, lr_F=0.1, lr_Cr=0.1, memory_size=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.pbest_rate = pbest_rate\n        self.lr_F = lr_F\n        self.lr_Cr = lr_Cr\n        self.memory_size = memory_size\n        self.memory_F = np.full(self.memory_size, 0.5)\n        self.memory_Cr = np.full(self.memory_size, 0.9)\n        self.archive = []\n        self.F = 0.5\n        self.Cr = 0.9\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)]\n\n        while self.budget > 0:\n            # Sort population by fitness\n            sorted_indices = np.argsort(fitness)\n            population = population[sorted_indices]\n            fitness = fitness[sorted_indices]\n            \n            # Determine pbest index\n            pbest_index = max(1, int(self.pop_size * self.pbest_rate))\n            \n            successful_cr = []\n            successful_f = []\n\n            for i in range(self.pop_size):\n                # Mutation: JADE-inspired current-to-pbest/1\n                pbest = population[np.random.randint(0, pbest_index)]\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                x_r1, x_r2 = population[np.random.choice(idxs, 2, replace=False)]\n                \n                mutant = population[i] + self.F * (pbest - population[i]) + self.F * (x_r1 - x_r2)\n                mutant = np.clip(mutant, lb, ub)\n                \n\n                # Crossover\n                trial = self.crossover(population[i], mutant, self.Cr)\n                \n                # Evaluate trial vector\n                f_trial = func(trial)\n                self.budget -= 1\n\n                # Selection\n                if f_trial < fitness[i]:\n                    successful_cr.append(self.Cr)\n                    successful_f.append(self.F)\n                    fitness[i] = f_trial\n                    population[i] = trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                        \n                        \n                    # Update archive\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(population[i].copy())\n                    else:\n                        j = np.random.randint(0, self.archive_size)\n                        self.archive[j] = population[i].copy()\n                        \n            # Adaptive F and CR (Leaning from success history)\n            if successful_cr:\n                self.Cr = np.mean(successful_cr)\n            if successful_f:\n                self.F = np.mean(successful_f)\n\n            # Parameter adaptation with learning rates\n            self.F = np.clip(self.F + self.lr_F * np.random.normal(0, 0.1), 0.1, 1.0)\n            self.Cr = np.clip(self.Cr + self.lr_Cr * np.random.normal(0, 0.1), 0.1, 1.0)\n\n            # Orthogonal learning (optional) - improves exploration\n            if self.budget > self.pop_size:  # Ensure enough budget for orthogonal design\n                best_indices = np.argsort(fitness)[:5]  # Select top 5 individuals\n                best_solutions = population[best_indices]\n\n                # Generate orthogonal array (example: L9(3^4) - requires pyDOE)\n                design_matrix = self.generate_orthogonal_array(9, 4) #dummy OA\n                \n                for k in range(len(best_solutions)):\n                    for row in design_matrix:\n                        trial_vector = best_solutions[k].copy()\n                        for j in range(self.dim):  # Map factors to dimensions\n                                # Map OA levels (0, 1, 2) to parameter range of the dimension\n                                trial_vector[j] = lb + (ub - lb) * (row[(j%4)] / 2)\n                            \n                        f_trial = func(trial_vector)\n                        self.budget -= 1  # Account for orthogonal design evaluations\n\n                        if f_trial < fitness[best_indices[k]]:\n                            fitness[best_indices[k]] = f_trial\n                            population[best_indices[k]] = trial_vector\n                            if f_trial < self.f_opt:\n                                self.f_opt = f_trial\n                                self.x_opt = trial_vector\n\n        return self.f_opt, self.x_opt\n\n    def crossover(self, x, mutant, cr):\n        trial = x.copy()\n        j_rand = np.random.randint(self.dim)\n        for j in range(self.dim):\n            if np.random.rand() < cr or j == j_rand:\n                trial[j] = mutant[j]\n        return trial\n    \n    def generate_orthogonal_array(self, n_rows, n_factors):\n        #Dummy orthogonal array generator since pyDOE is not allowed\n        #Replace with a proper OA generator if pyDOE is available\n\n        # Example orthogonal array L9(3^4) - replace with actual generation\n        oa = np.array([[0, 0, 0, 0],\n                       [0, 1, 1, 1],\n                       [0, 2, 2, 2],\n                       [1, 0, 1, 2],\n                       [1, 1, 2, 0],\n                       [1, 2, 0, 1],\n                       [2, 0, 2, 1],\n                       [2, 1, 0, 2],\n                       [2, 2, 1, 0]])\n        \n        if n_rows == 9 and n_factors == 4:\n            return oa\n        else:\n            return np.random.randint(0, 3, size=(n_rows, n_factors)) # Fallback random array", "configspace": "", "generation": 8, "feedback": "In the code, line 98, in __call__, the following error occurred:\nValueError: setting an array element with a sequence.\nOn line: trial_vector[j] = lb + (ub - lb) * (row[(j%4)] / 2)", "error": "In the code, line 98, in __call__, the following error occurred:\nValueError: setting an array element with a sequence.\nOn line: trial_vector[j] = lb + (ub - lb) * (row[(j%4)] / 2)", "parent_ids": ["c644790d-f6f7-49f4-87d6-80d5786151e8"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "8de6ff07-348c-4011-87d4-e1e9bb2352ce", "fitness": 0.0, "name": "DynamicPopulationNeighborhoodDE", "description": "A differential evolution variant that incorporates a dynamically adjusted population size and a neighborhood-based mutation strategy for improved exploration and exploitation.", "code": "import numpy as np\n\nclass DynamicPopulationNeighborhoodDE:\n    def __init__(self, budget=10000, dim=10, initial_pop_size=50, F=0.5, Cr=0.9, neighborhood_size=5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = initial_pop_size\n        self.initial_pop_size = initial_pop_size\n        self.F = F  # Mutation factor\n        self.Cr = Cr  # Crossover rate\n        self.neighborhood_size = neighborhood_size # Size of the neighborhood for mutation\n        self.population = None\n        self.fitness = None\n        self.f_opt = float('inf')\n        self.x_opt = None\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        generation = 0\n        while self.budget > 0:\n            generation += 1\n            \n            # Adjust population size dynamically based on progress (example strategy)\n            if generation % 10 == 0:\n                if np.std(self.fitness) < 1e-3: # Convergence detected\n                    if self.pop_size > 10:\n                        self.pop_size = max(10, int(self.pop_size * 0.9)) # Reduce population\n                else:\n                    if self.pop_size < self.initial_pop_size * 2:\n                        self.pop_size = min(self.initial_pop_size * 2, int(self.pop_size * 1.1)) # Increase population\n                \n                if self.pop_size != self.population.shape[0]:\n                    # Resize population\n                    if self.pop_size > self.population.shape[0]:\n                        # Add new random individuals\n                        num_new = self.pop_size - self.population.shape[0]\n                        new_population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(num_new, self.dim))\n                        new_fitness = np.array([func(x) for x in new_population])\n                        self.budget -= num_new\n                        self.population = np.vstack((self.population, new_population))\n                        self.fitness = np.concatenate((self.fitness, new_fitness))\n\n                        # Update optimal solution\n                        for i in range(num_new):\n                            if new_fitness[i] < self.f_opt:\n                                self.f_opt = new_fitness[i]\n                                self.x_opt = new_population[i]\n                    else:\n                         #Remove worst individuals if decreasing the population\n                        num_remove = self.population.shape[0] - self.pop_size\n                        idx_to_remove = np.argsort(self.fitness)[-num_remove:]\n                        self.population = np.delete(self.population, idx_to_remove, axis=0)\n                        self.fitness = np.delete(self.fitness, idx_to_remove)\n            \n            for i in range(self.population.shape[0]):\n                # Neighborhood-based mutation\n                neighbors_idx = np.random.choice(self.population.shape[0], self.neighborhood_size, replace=False)\n                a, b, c = self.population[neighbors_idx[0]], self.population[neighbors_idx[1]], self.population[neighbors_idx[2]]\n                mutant = np.clip(a + self.F * (b - c), func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.Cr\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, self.population[i])\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n\n                    # Update optimal solution\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "The algorithm DynamicPopulationNeighborhoodDE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["94768647-f252-46d5-98c5-9ea8189ba67b"], "operator": null, "metadata": {"aucs": [0]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "7d8d6a85-f634-49f9-827c-eeef2a6979a4", "fitness": "-inf", "name": "EnhancedAdaptiveMutationDE", "description": "Enhanced Adaptive Mutation DE with simplified parameter adaptation, orthogonal crossover, and a local search component.", "code": "import numpy as np\nfrom scipy.stats import cauchy\n\nclass EnhancedAdaptiveMutationDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=10, F=0.5, Cr=0.9, lr=0.1, p_cauchy=0.1, local_search_prob = 0.05):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.F = F\n        self.Cr = Cr\n        self.lr = lr  # Single learning rate for both F and Cr\n        self.p_cauchy = p_cauchy\n        self.local_search_prob = local_search_prob\n        self.archive = []\n        self.archive_fitness = []\n        self.memory_size = 10\n        self.success_F = []\n        self.success_Cr = []\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)]\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation\n                if np.random.rand() < self.p_cauchy:\n                    # Cauchy mutation\n                    idxs = [idx for idx in range(self.pop_size) if idx != i]\n                    a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                    mutant = population[i] + self.F * cauchy.rvs(loc=0, scale=1, size=self.dim)\n                    mutant = np.clip(mutant, lb, ub)\n                else:\n                    # Gaussian mutation\n                    idxs = [idx for idx in range(self.pop_size) if idx != i]\n                    a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                    mutant = np.clip(a + self.F * (b - c), lb, ub)\n\n                # Crossover (Orthogonal Crossover)\n                trial = self.orthogonal_crossover(population[i], mutant)\n                trial = np.clip(trial, lb, ub)\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                if f_trial < fitness[i]:\n                    self.success_F.append(self.F)\n                    self.success_Cr.append(self.Cr)\n                    fitness[i] = f_trial\n                    population[i] = trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n                    # Update archive\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(population[i].copy())\n                        self.archive_fitness.append(fitness[i])\n                    else:\n                        max_archive_index = np.argmax(self.archive_fitness)\n                        if fitness[i] < self.archive_fitness[max_archive_index]:\n                            self.archive[max_archive_index] = population[i].copy()\n                            self.archive_fitness[max_archive_index] = fitness[i]\n                \n                # Local Search\n                if np.random.rand() < self.local_search_prob:\n                    trial_ls = self.local_search(population[i], func, lb, ub)\n                    f_trial_ls = func(trial_ls)\n                    self.budget -= 1\n                    if f_trial_ls < fitness[i]:\n                        fitness[i] = f_trial_ls\n                        population[i] = trial_ls\n                        if f_trial_ls < self.f_opt:\n                            self.f_opt = f_trial_ls\n                            self.x_opt = trial_ls\n            # Adaptive F and Cr\n            if self.success_F:\n                self.F = np.mean(self.success_F)\n                self.Cr = np.mean(self.success_Cr)\n\n            # Parameter adaptation\n            self.F = np.clip(self.F + self.lr * np.random.normal(0, 0.1), 0.1, 1.0)\n            self.Cr = np.clip(self.Cr + self.lr * np.random.normal(0, 0.1), 0.1, 1.0)\n            self.success_F = []\n            self.success_Cr = []\n\n        return self.f_opt, self.x_opt\n    \n    def orthogonal_crossover(self, x, mutant):\n        trial = x.copy()\n        num_changes = np.random.randint(1, self.dim + 1)  # Number of dimensions to change\n        indices = np.random.choice(self.dim, num_changes, replace=False)  # Select dimensions randomly\n        trial[indices] = mutant[indices]  # Apply crossover on selected dimensions\n        return trial\n    \n    def local_search(self, x, func, lb, ub, step_size=0.1):\n        \"\"\"Performs a simple local search around x.\"\"\"\n        x_new = x.copy()\n        for i in range(self.dim):\n            # Explore both directions for each dimension\n            for direction in [-1, 1]:\n                x_perturbed = x.copy()\n                x_perturbed[i] += direction * step_size\n                x_perturbed = np.clip(x_perturbed, lb, ub)\n                \n                f_perturbed = func(x_perturbed)\n                self.budget -= 1\n                if f_perturbed < func(x_new):\n                    x_new = x_perturbed.copy()\n        return x_new", "configspace": "", "generation": 8, "feedback": "In the code, line 36, in __call__, the following error occurred:\nNameError: name 'cauchy' is not defined\nOn line: mutant = population[i] + self.F * cauchy.rvs(loc=0, scale=1, size=self.dim)", "error": "In the code, line 36, in __call__, the following error occurred:\nNameError: name 'cauchy' is not defined\nOn line: mutant = population[i] + self.F * cauchy.rvs(loc=0, scale=1, size=self.dim)", "parent_ids": ["c644790d-f6f7-49f4-87d6-80d5786151e8"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "3d1fc530-3455-4b51-9ee7-7d8b78ee1009", "fitness": 0.0, "name": "SelfAdaptiveDE", "description": "A simplified self-adaptive differential evolution with a focus on parameter adaptation using a success-history archive and a reduced set of control parameters for efficiency.", "code": "import numpy as np\n\nclass SelfAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=40, archive_size=5, F=0.5, Cr=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.F = F  # Mutation factor\n        self.Cr = Cr  # Crossover rate\n        self.learning_rate_F = 0.1\n        self.learning_rate_Cr = 0.1\n        self.success_history_F = []\n        self.success_history_Cr = []\n        self.success_history_size = 5\n        self.archive = []\n        self.archive_fitness = [] # Archive fitness\n\n    def __call__(self, func):\n        # Initialize population\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        population = np.random.uniform(lb, ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size  # Deduct initial population evaluations from budget\n\n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)]\n\n        generation = 0\n        while self.budget > 0:\n            generation += 1\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n\n                # Mutation with F\n                mutant = np.clip(a + self.F * (b - c), lb, ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.Cr\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n\n                if f_trial < fitness[i]:\n                    # Archive the replaced individual for diversity\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(population[i].copy())\n                        self.archive_fitness.append(fitness[i])\n                    else:\n                        idx_to_replace = np.random.randint(0, self.archive_size)\n                        self.archive[idx_to_replace] = population[i].copy()\n                        self.archive_fitness[idx_to_replace] = fitness[i]\n\n                    fitness[i] = f_trial\n                    population[i] = trial\n\n                    # Update optimal solution\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n                    # Update success history for F and Cr adaptation\n                    self.success_history_F.append(self.F)\n                    self.success_history_Cr.append(self.Cr)\n                    if len(self.success_history_F) > self.success_history_size:\n                        self.success_history_F.pop(0)\n                        self.success_history_Cr.pop(0)\n\n            # Adapt F and Cr based on success history\n            if self.success_history_F:\n                self.F = np.clip(np.mean(self.success_history_F) + self.learning_rate_F * np.random.normal(0, 0.1), 0.1, 1.0)\n            else:\n                self.F = np.clip(self.F + self.learning_rate_F * np.random.normal(0, 0.1), 0.1, 1.0)\n\n            if self.success_history_Cr:\n                self.Cr = np.clip(np.mean(self.success_history_Cr) + self.learning_rate_Cr * np.random.normal(0, 0.1), 0.1, 1.0)\n            else:\n                self.Cr = np.clip(self.Cr + self.learning_rate_Cr * np.random.normal(0, 0.1), 0.1, 1.0)\n\n            # Archive-based perturbation (occasional)\n            if generation % 5 == 0 and self.archive: # Every 5 generations\n                i = np.random.randint(0, self.pop_size)\n                arch_idx = np.random.randint(0, len(self.archive))\n                population[i] = np.clip(0.5 * population[i] + 0.5 * self.archive[arch_idx], lb, ub) #Perturb towards archive\n                fitness[i] = func(population[i])\n                self.budget -= 1\n\n                if fitness[i] < self.f_opt:\n                    self.f_opt = fitness[i]\n                    self.x_opt = population[i]\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "The algorithm SelfAdaptiveDE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["d15f2124-2044-40ea-b0ea-bbc76d19a3ef"], "operator": null, "metadata": {"aucs": [0]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "6a68a373-0998-42dd-937d-71865774063f", "fitness": 0.5185046425962636, "name": "SelfAdaptiveDE", "description": "Self-adaptive Differential Evolution with a simplified parameter adaptation scheme based on success rate of mutation and crossover.", "code": "import numpy as np\n\nclass SelfAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, Cr=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F  # Mutation factor\n        self.Cr = Cr  # Crossover rate\n        self.success_F = []\n        self.success_Cr = []\n        self.success_history_size = 10\n        self.epsilon = 1e-6 # Avoid division by zero\n\n    def __call__(self, func):\n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size # Deduct initial population evaluations from budget\n\n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)]\n\n        generation = 0\n        while self.budget > 0:\n            generation += 1\n            successful_mutations = 0\n            successful_crossovers = 0\n\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + self.F * (b - c), func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.Cr\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    population[i] = trial\n\n                    # Update optimal solution\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                    \n                    successful_mutations += 1\n                    successful_crossovers += 1\n\n            # Adapt parameters based on success rate\n            success_rate_mutations = successful_mutations / self.pop_size\n            success_rate_crossovers = successful_crossovers / self.pop_size\n            \n            self.F = np.clip(self.F * (1 + 0.2 * (success_rate_mutations - 0.5)), 0.1, 1.0)\n            self.Cr = np.clip(self.Cr * (1 + 0.2 * (success_rate_crossovers - 0.5)), 0.1, 1.0)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 9, "feedback": "The algorithm SelfAdaptiveDE scored 0.519 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["94768647-f252-46d5-98c5-9ea8189ba67b"], "operator": null, "metadata": {"aucs": [0.1640204386962314, 0.30584881152641885, 0.4377425130855688, 0.7925206292531055, 0.35466135464306137, 0.779934128357467, 0.45697005560058446, 0.5176415094628724, 0.5565197508355838, 0.301048884384286, 0.43348990924071307, 0.9987973204624612, 0.2666031673421577, 0.47122338404118014, 0.7474967758218899, 0.7695499944270778, 0.41660525110197755, 0.8054448521238714, 0.282243093169367, 0.5117310283493979]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "21b08327-4d65-4429-a7dd-355ee12b098f", "fitness": 0.0, "name": "SelfAdaptiveDE", "description": "Self-adaptive Differential Evolution with a novel aging mechanism based on fitness rank, adaptive mutation strategies, and orthogonal design for exploration and exploitation.", "code": "import numpy as np\n\nclass SelfAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=10, F=0.5, Cr=0.9, aging_threshold=0.2):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.F = F\n        self.Cr = Cr\n        self.learning_rate = 0.1\n        self.aging_threshold = aging_threshold  # Proportion of worst individuals to replace\n        self.archive = []\n        self.archive_fitness = []\n\n    def __call__(self, func):\n        # Initialize population\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)]\n\n        generation = 0\n        while self.budget > 0:\n            generation += 1\n\n            # Aging mechanism based on fitness rank: replace a fraction of the worst individuals\n            num_to_replace = int(self.pop_size * self.aging_threshold)\n            if num_to_replace > 0:\n                worst_indices = np.argsort(fitness)[-num_to_replace:]  # Indices of worst individuals\n                for i in worst_indices:\n                    population[i] = np.random.uniform(lb, ub, size=self.dim)\n                    fitness[i] = func(population[i])\n                    self.budget -= 1\n                    if fitness[i] < self.f_opt:\n                        self.f_opt = fitness[i]\n                        self.x_opt = population[i]\n\n\n            for i in range(self.pop_size):\n\n                # Adaptive mutation strategy\n                if np.random.rand() < 0.5:  # Differential Evolution mutation\n                    idxs = [idx for idx in range(self.pop_size) if idx != i]\n                    if len(self.archive) > 0 and np.random.rand() < 0.1:  # Use archive occasionally\n                        idx_archive = np.random.randint(len(self.archive))\n                        a = self.archive[idx_archive]\n                        idxs = np.random.choice(idxs, 2, replace=False)\n                        b, c = population[idxs]\n                        mutant = np.clip(a + self.F * (b - c), lb, ub)\n\n                    else:\n                        a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                        mutant = np.clip(a + self.F * (b - c), lb, ub)\n\n                else:  # Current-to-best mutation\n                    best_idx = np.argmin(fitness)\n                    idxs = [idx for idx in range(self.pop_size) if idx != i and idx != best_idx]\n                    if len(idxs) > 0:\n                         r1 = population[np.random.choice(idxs)]\n                         mutant = np.clip(population[i] + self.F * (population[best_idx] - population[i]) + self.F * (r1 - population[i]), lb, ub)\n                    else:\n                        mutant = population[i].copy()\n\n                # Orthogonal Crossover\n                trial = self.orthogonal_crossover(population[i], mutant, self.Cr)\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    population[i] = trial\n\n                    # Update optimal solution\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                        self.learning_rate = min(1.0, self.learning_rate * 1.1)\n                    # Update archive\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(population[i].copy())\n                        self.archive_fitness.append(fitness[i])\n                    else:\n                        max_archive_index = np.argmax(self.archive_fitness)\n                        if fitness[i] < self.archive_fitness[max_archive_index]:\n                            self.archive[max_archive_index] = population[i].copy()\n                            self.archive_fitness[max_archive_index] = fitness[i]\n\n\n                else:\n                    self.learning_rate = max(0.01, self.learning_rate * 0.9)\n\n            # Adapt parameters\n            self.F = np.clip(self.F + self.learning_rate * np.random.normal(0, 0.1), 0.1, 1.0)\n            self.Cr = np.clip(self.Cr + self.learning_rate * np.random.normal(0, 0.1), 0.1, 1.0)\n\n\n        return self.f_opt, self.x_opt\n\n    def orthogonal_crossover(self, x, mutant, cr):\n        trial = x.copy()\n        for j in range(self.dim):\n            if np.random.rand() < cr:\n                trial[j] = mutant[j]\n        return trial", "configspace": "", "generation": 9, "feedback": "The algorithm SelfAdaptiveDE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["06b6ae75-0ec5-408d-a4a3-4059dc60b0ac"], "operator": null, "metadata": {"aucs": [0]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "e0385451-20e5-4254-b9d8-f9afa1908b56", "fitness": 0.6159710688185573, "name": "SelfAdaptiveDE", "description": "Self-adaptive Differential Evolution with a simplified parameter adaptation scheme and a focus on exploration through increased mutation factor variation.", "code": "import numpy as np\n\nclass SelfAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, Cr=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F  # Mutation factor\n        self.Cr = Cr  # Crossover rate\n\n    def __call__(self, func):\n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size # Deduct initial population evaluations from budget\n\n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)]\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n\n                # Increased mutation factor variation for exploration\n                mutant = np.clip(a + (self.F + np.random.normal(0, 0.1)) * (b - c), func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.Cr\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    population[i] = trial\n\n                    # Update optimal solution\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n            # Adapt parameters - Simplified adaptation (only F)\n            self.F = np.clip(self.F + np.random.normal(0, 0.05), 0.1, 1.0) # Simplified F adaptation.  Less aggressive.\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 9, "feedback": "The algorithm SelfAdaptiveDE scored 0.616 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["94768647-f252-46d5-98c5-9ea8189ba67b"], "operator": null, "metadata": {"aucs": [0.3522163569272446, 0.28473160160899147, 0.5816476394435062, 0.8965982275663786, 0.7030605375882304, 0.7129786820091027, 0.581403291601188, 0.7329245568195555, 0.6624822139506683, 0.4959892967631655, 0.8160983441994546, 0.9974115195773783, 0.5156186056744886, 0.6757883158312235, 0.7389695057702993, 0.49247974126674343, 0.5669681592385556, 0.8152192919208467, 0.1926804639628389, 0.5041550246512855]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "7a2cd73d-4cfa-4aa5-a3dd-16c76b58dcae", "fitness": 0.6939322814164408, "name": "EnhancedSelfAdaptiveDE", "description": "Enhanced Self-Adaptive Differential Evolution with Orthogonal Crossover and a population archive for diversity maintenance.", "code": "import numpy as np\n\nclass EnhancedSelfAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.archive = []\n        self.archive_fitness = []\n        self.F = 0.5  # Initial mutation factor\n        self.Cr = 0.9  # Initial crossover rate\n        self.F_history = []\n        self.Cr_history = []\n        self.success_rate = 0.0\n\n    def __call__(self, func):\n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)]\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Parameter Adaptation (simplified success-history)\n                if self.F_history:\n                    self.F = np.mean(self.F_history)\n                if self.Cr_history:\n                    self.Cr = np.mean(self.Cr_history)\n                self.F = np.clip(np.random.normal(self.F, 0.1), 0.1, 1.0)\n                self.Cr = np.clip(np.random.normal(self.Cr, 0.1), 0.1, 1.0)\n\n                # Mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                if self.archive:\n                     idxs.append(np.argmin(self.archive_fitness) + self.pop_size) #pull from archive\n                     candidates = np.vstack((population, np.array(self.archive)))\n                else:\n                    candidates = population\n                a, b, c = candidates[np.random.choice(idxs, 3, replace=False)]\n\n                mutant = np.clip(a + self.F * (b - c), func.bounds.lb, func.bounds.ub)\n\n                # Orthogonal Crossover\n                trial = population[i].copy()\n                num_changed_params = 0\n                for j in range(self.dim):\n                    if np.random.rand() < self.Cr:\n                        trial[j] = mutant[j]\n                        num_changed_params += 1\n                if num_changed_params == 0:\n                    trial[np.random.randint(0, self.dim)] = mutant[np.random.randint(0, self.dim)]  # Ensure at least one change\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n\n                if f_trial < fitness[i]:\n                    #Success\n                    self.F_history.append(self.F)\n                    self.Cr_history.append(self.Cr)\n\n                    fitness[i] = f_trial\n                    population[i] = trial\n\n                    # Update optimal solution\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n                    # Archive Update\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(population[i].copy())\n                        self.archive_fitness.append(f_trial)\n                    else:\n                        max_archive_fitness_index = np.argmax(self.archive_fitness)\n                        if f_trial < self.archive_fitness[max_archive_fitness_index]:\n                            self.archive[max_archive_fitness_index] = population[i].copy()\n                            self.archive_fitness[max_archive_fitness_index] = f_trial\n\n\n                self.F_history = self.F_history[-10:] # keep last 10 values\n                self.Cr_history =  self.Cr_history[-10:]\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 9, "feedback": "The algorithm EnhancedSelfAdaptiveDE scored 0.694 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["94768647-f252-46d5-98c5-9ea8189ba67b"], "operator": null, "metadata": {"aucs": [0.3681797091333717, 0.25188707965871715, 0.5848315882127835, 0.8989130105422636, 0.7850909853425441, 0.7771562137305135, 0.7403091723521957, 0.5488775027956101, 0.8041785079782826, 0.7511617854465714, 0.8417442541498125, 0.9900682119048821, 0.48115148472884706, 0.8139680946376235, 0.8602149428348711, 0.8215346467019233, 0.553475898845597, 0.8319032432328864, 0.6787438570406383, 0.4952554390588825]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "2a53cb19-5f1d-43ce-8e77-193acbe18342", "fitness": 0.0, "name": "SelfAdaptiveDE", "description": "Adaptive Differential Evolution with a simplified self-adaptation mechanism and a focus on population diversity.", "code": "import numpy as np\n\nclass SelfAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, Cr=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F  # Mutation factor\n        self.Cr = Cr  # Crossover rate\n\n    def __call__(self, func):\n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size # Deduct initial population evaluations from budget\n\n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)]\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + self.F * (b - c), func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.Cr\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    population[i] = trial\n\n                    # Update optimal solution\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n            # Adapt parameters (simplified adaptation)\n            self.F = np.clip(self.F + 0.1 * np.random.normal(0, 0.1), 0.1, 1.0)\n            self.Cr = np.clip(self.Cr + 0.1 * np.random.normal(0, 0.1), 0.1, 1.0)\n\n            # Introducing random immigrants to maintain diversity\n            if np.random.rand() < 0.05:  # 5% chance of introducing immigrants\n                immigrant_idx = np.random.randint(0, self.pop_size)\n                population[immigrant_idx] = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                fitness[immigrant_idx] = func(population[immigrant_idx])\n                self.budget -= 1\n                if fitness[immigrant_idx] < self.f_opt:\n                    self.f_opt = fitness[immigrant_idx]\n                    self.x_opt = population[immigrant_idx]\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 9, "feedback": "The algorithm SelfAdaptiveDE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["94768647-f252-46d5-98c5-9ea8189ba67b"], "operator": null, "metadata": {"aucs": [0]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "b22f9e37-453d-421c-bf60-50fa3aef06e4", "fitness": 0.0, "name": "AdaptiveMutationDE", "description": "Self-adaptive Differential Evolution with a combined mutation strategy (current-to-best/rand/pbest) and an aging population for enhanced exploration.", "code": "import numpy as np\n\nclass AdaptiveMutationDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, Cr=0.9, p_best=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.Cr = Cr\n        self.p_best = p_best\n        self.population = None\n        self.fitness = None\n        self.f_opt = float('inf')\n        self.x_opt = None\n        self.age = np.zeros(pop_size)\n\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        self.population = np.random.uniform(lb, ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n        self.age = np.zeros(self.pop_size)\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation Strategy\n                if np.random.rand() < self.p_best:\n                    # Current-to-pbest mutation\n                    p_best_idx = np.argsort(self.fitness)[:max(1, int(self.p_best * self.pop_size))]\n                    best_idx = np.random.choice(p_best_idx)\n                    idxs = [idx for idx in range(self.pop_size) if idx != i]\n                    a, b = self.population[np.random.choice(idxs, 2, replace=False)]\n                    mutant = self.population[i] + self.F * (self.population[best_idx] - self.population[i]) + self.F * (a - b)\n                else:\n                    # Rand mutation\n                    idxs = [idx for idx in range(self.pop_size) if idx != i]\n                    a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n                    mutant = a + self.F * (b - c)\n\n                mutant = np.clip(mutant, lb, ub)\n\n                # Crossover\n                trial = self.crossover(self.population[i], mutant, self.Cr)\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n                    self.age[i] = 0  # Reset age\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                else:\n                    self.age[i] += 1\n\n            # Aging mechanism: replace old individuals with random ones\n            for i in range(self.pop_size):\n                if self.age[i] > self.dim * 5:  # Adjust threshold as needed\n                    self.population[i] = np.random.uniform(lb, ub, size=self.dim)\n                    self.fitness[i] = func(self.population[i])\n                    self.budget -=1\n                    self.age[i] = 0\n                    if self.fitness[i] < self.f_opt:\n                        self.f_opt = self.fitness[i]\n                        self.x_opt = self.population[i]\n\n        return self.f_opt, self.x_opt\n\n    def crossover(self, x, mutant, cr):\n        trial = x.copy()\n        for j in range(self.dim):\n            if np.random.rand() < cr:\n                trial[j] = mutant[j]\n        return trial", "configspace": "", "generation": 9, "feedback": "The algorithm AdaptiveMutationDE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["c644790d-f6f7-49f4-87d6-80d5786151e8"], "operator": null, "metadata": {"aucs": [0]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "c06f1489-162e-4878-acbc-e1dc1730fbbb", "fitness": 0.660284382315256, "name": "SelfAdaptiveDE", "description": "Self-adaptive Differential Evolution with a novel probabilistic mutation strategy based on fitness ranking and a dynamic archive updating mechanism, enhancing both exploration and exploitation.", "code": "import numpy as np\n\nclass SelfAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=10, F=0.5, Cr=0.9, aging_limit=50):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.F = F\n        self.Cr = Cr\n        self.learning_rate = 0.1\n        self.aging_limit = aging_limit\n        self.archive = []\n        self.archive_fitness = []\n        self.population_age = np.zeros(pop_size)  # Track the age of each individual\n\n    def __call__(self, func):\n        # Initialize population\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)]\n\n        generation = 0\n        while self.budget > 0:\n            generation += 1\n            # Sort population based on fitness\n            sorted_indices = np.argsort(fitness)\n            population = population[sorted_indices]\n            fitness = fitness[sorted_indices]\n\n            for i in range(self.pop_size):\n                # Aging mechanism: if an individual doesn't improve for too long, replace it with a random one\n                if self.population_age[i] > self.aging_limit:\n                    population[i] = np.random.uniform(lb, ub, size=self.dim)\n                    fitness[i] = func(population[i])\n                    self.budget -= 1\n                    self.population_age[i] = 0\n                    if fitness[i] < self.f_opt:\n                        self.f_opt = fitness[i]\n                        self.x_opt = population[i]\n                    continue\n\n                # Probabilistic Mutation based on rank\n                rank = i / self.pop_size\n                mutation_prob = 0.5 + 0.5 * rank  # Higher rank (worse fitness) -> higher mutation probability\n\n                if np.random.rand() < mutation_prob:\n                    idxs = [idx for idx in range(self.pop_size) if idx != i]\n                    if len(self.archive) > 0 and np.random.rand() < 0.1:  # Use archive occasionally\n                        idx_archive = np.random.randint(len(self.archive))\n                        a = self.archive[idx_archive]\n                        idxs = np.random.choice(idxs, 2, replace=False)\n                        b, c = population[idxs]\n                        mutant = np.clip(a + self.F * (b - c), lb, ub)\n\n                    else:\n                        a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                        mutant = np.clip(a + self.F * (b - c), lb, ub)\n\n\n                else:\n                    mutant = population[i].copy()  # No mutation\n\n\n                # Orthogonal Crossover\n                trial = self.orthogonal_crossover(population[i], mutant, self.Cr)\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    population[i] = trial\n                    self.population_age[i] = 0 #reset age\n                    # Update optimal solution\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                        self.learning_rate = min(1.0, self.learning_rate * 1.1)\n                    # Dynamic Archive Update (replace worst)\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(population[i].copy())\n                        self.archive_fitness.append(fitness[i])\n                    else:\n                        max_archive_index = np.argmax(self.archive_fitness)\n                        if fitness[i] < np.max(self.archive_fitness):\n                            worst_archive_index = np.argmax(self.archive_fitness)\n                            self.archive[worst_archive_index] = population[i].copy()\n                            self.archive_fitness[worst_archive_index] = fitness[i]\n                else:\n                    self.population_age[i] += 1 #increase age\n                    self.learning_rate = max(0.01, self.learning_rate * 0.9)\n\n            # Adapt parameters\n            self.F = np.clip(self.F + self.learning_rate * np.random.normal(0, 0.1), 0.1, 1.0)\n            self.Cr = np.clip(self.Cr + self.learning_rate * np.random.normal(0, 0.1), 0.1, 1.0)\n\n        return self.f_opt, self.x_opt\n\n    def orthogonal_crossover(self, x, mutant, cr):\n        trial = x.copy()\n        for j in range(self.dim):\n            if np.random.rand() < cr:\n                trial[j] = mutant[j]\n        return trial", "configspace": "", "generation": 9, "feedback": "The algorithm SelfAdaptiveDE scored 0.660 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["06b6ae75-0ec5-408d-a4a3-4059dc60b0ac"], "operator": null, "metadata": {"aucs": [0.29858519270257544, 0.589009380418059, 0.5921769326786239, 0.8744606491207697, 0.7017290293120837, 0.7531682241681006, 0.5644738238056233, 0.5818162228236107, 0.6806281939749383, 0.5931265937912138, 0.8675973466009406, 0.999909756382664, 0.5709131855003664, 0.7000750028546958, 0.8974232830128916, 0.7178591237743857, 0.552395752660264, 0.8345120111452885, 0.3175496970182887, 0.5182782445597369]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "68a1af65-5ef1-464a-bb70-12e3a9b1ffa0", "fitness": 0.3005857177199066, "name": "AdaptiveCovarianceDE", "description": "Adaptive Differential Evolution with a success-history based parameter adaptation, covariance matrix adaptation, and a restart strategy.", "code": "import numpy as np\n\nclass AdaptiveCovarianceDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=10, F=0.5, Cr=0.9, restart_trigger=100):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.F = F\n        self.Cr = Cr\n        self.memory_size = 10\n        self.sf_memory = np.ones(self.memory_size) * 0.5\n        self.scr_memory = np.ones(self.memory_size) * 0.5\n        self.memory_idx = 0\n        self.archive = []\n        self.archive_fitness = []\n        self.restart_trigger = restart_trigger\n        self.no_improvement_count = 0\n        self.best_fitness_history = []\n\n        self.C = np.eye(dim)  # Covariance matrix for CMA-ES-like mutation\n        self.mu = np.zeros(dim) # Mean for CMA-ES-like mutation\n\n    def __call__(self, func):\n        # Initialize population\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)]\n        self.best_fitness_history.append(self.f_opt)\n\n\n        while self.budget > 0:\n            \n            if len(self.best_fitness_history) > self.restart_trigger:\n                if self.best_fitness_history[-1] == self.best_fitness_history[-self.restart_trigger]:\n                    # Trigger restart if no improvement for restart_trigger generations\n                    population = np.random.uniform(lb, ub, size=(self.pop_size, self.dim))\n                    fitness = np.array([func(x) for x in population])\n                    self.budget -= self.pop_size\n                    self.f_opt = np.min(fitness)\n                    self.x_opt = population[np.argmin(fitness)]\n                    self.C = np.eye(self.dim)  # Reset covariance matrix\n                    self.mu = np.zeros(self.dim) #reset mean\n                    self.sf_memory = np.ones(self.memory_size) * 0.5\n                    self.scr_memory = np.ones(self.memory_size) * 0.5\n                    self.memory_idx = 0\n                    self.archive = []\n                    self.archive_fitness = []\n                    self.best_fitness_history = [self.f_opt]\n                    continue\n            \n            \n            successful_mutations_F = []\n            successful_mutations_Cr = []\n\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n\n                # CMA-ES like mutation with covariance matrix adaptation\n                z = np.random.multivariate_normal(self.mu, self.C)\n                mutant = np.clip(population[i] + self.F * z, lb, ub)\n\n                # Crossover\n                trial = self.crossover(population[i], mutant, self.Cr)\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n\n                if f_trial < fitness[i]:\n                    successful_mutations_F.append(self.F)\n                    successful_mutations_Cr.append(self.Cr)\n\n                    delta = fitness[i] - f_trial\n                    fitness[i] = f_trial\n                    population[i] = trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                        self.best_fitness_history.append(self.f_opt)\n\n                    # Update archive\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(population[i].copy())\n                        self.archive_fitness.append(fitness[i])\n                    else:\n                        max_archive_index = np.argmax(self.archive_fitness)\n                        if fitness[i] < self.archive_fitness[max_archive_index]:\n                            self.archive[max_archive_index] = population[i].copy()\n                            self.archive_fitness[max_archive_index] = fitness[i]\n                else:\n                    self.best_fitness_history.append(self.f_opt) #record best fitness even if not improved\n\n            # Adapt parameters using success history\n            if successful_mutations_F:\n                self.sf_memory[self.memory_idx] = np.mean(successful_mutations_F)\n                self.scr_memory[self.memory_idx] = np.mean(successful_mutations_Cr)\n\n                self.F = np.random.choice(self.sf_memory)\n                self.Cr = np.random.choice(self.scr_memory)\n\n                self.memory_idx = (self.memory_idx + 1) % self.memory_size\n\n            # Adapt covariance matrix (simplified CMA-ES)\n            weights = np.zeros(self.pop_size)\n            for i in range(self.pop_size):\n                weights[i] = (fitness.max() - fitness[i]) / np.sum(fitness.max() - fitness)\n            \n            self.mu = np.average(population, axis=0, weights=weights)\n            \n            # Simple rank-one update\n            diff = population - self.mu\n            self.C = (1 - 0.1) * self.C + 0.1 * np.mean([np.outer(diff[i], diff[i]) for i in range(self.pop_size)], axis=0)\n\n\n        return self.f_opt, self.x_opt\n\n    def crossover(self, x, mutant, cr):\n        trial = x.copy()\n        for j in range(self.dim):\n            if np.random.rand() < cr:\n                trial[j] = mutant[j]\n        return trial", "configspace": "", "generation": 9, "feedback": "The algorithm AdaptiveCovarianceDE scored 0.301 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["06b6ae75-0ec5-408d-a4a3-4059dc60b0ac"], "operator": null, "metadata": {"aucs": [0.11670595454473387, 0.22118638202365815, 0.2834838209074334, 0.23595846486465566, 0.25016725901705616, 0.2398154671781002, 0.24471672902330122, 0.23150778734321498, 0.22348357360516125, 0.16763777870058616, 0.2530432210557879, 0.9991430305693378, 0.25820815596988445, 0.2322447943086834, 0.6201411970348002, 0.28053805282521216, 0.25070147472801607, 0.280936442159724, 0.1643586843954965, 0.45773608414328626]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "bdc6ffcc-7182-4440-a627-a76cb7369919", "fitness": 0.0, "name": "CooperativeAdaptiveDE", "description": "Cooperative Adaptive Differential Evolution with a dynamically adjusted population size and a pool of mutation strategies.", "code": "import numpy as np\n\nclass CooperativeAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size_initial=50, pop_size_min=10, pop_size_max=100, archive_size=10, lr_F=0.1, lr_Cr=0.1, pop_adapt_freq=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_initial = pop_size_initial\n        self.pop_size_min = pop_size_min\n        self.pop_size_max = pop_size_max\n        self.archive_size = archive_size\n        self.lr_F = lr_F\n        self.lr_Cr = lr_Cr\n        self.archive = []\n        self.archive_fitness = []\n        self.memory_F = []\n        self.memory_Cr = []\n        self.memory_size = 10\n        self.success_F = []\n        self.success_Cr = []\n        self.mutation_strategies = [self.mutation_rand1, self.mutation_current_to_best_1, self.mutation_rand2, self.mutation_current_to_rand_1]\n        self.pop_adapt_freq = pop_adapt_freq\n        self.generation = 0\n        self.F = 0.5\n        self.Cr = 0.9\n\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)]\n\n        while self.budget > 0:\n            self.generation += 1\n            success_F = []\n            success_Cr = []\n            for i in range(self.pop_size):\n                # Mutation: Select a mutation strategy randomly\n                mutation_strategy = np.random.choice(self.mutation_strategies)\n                mutant = mutation_strategy(population, i)\n                mutant = np.clip(mutant, lb, ub)\n\n                # Crossover\n                trial = self.crossover(population[i], mutant, self.Cr)\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                if f_trial < fitness[i]:\n                    success_F.append(self.F)\n                    success_Cr.append(self.Cr)\n                    fitness[i] = f_trial\n                    population[i] = trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n                    # Update archive\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(population[i].copy())\n                        self.archive_fitness.append(fitness[i])\n                    else:\n                        max_archive_index = np.argmax(self.archive_fitness)\n                        if fitness[i] < self.archive_fitness[max_archive_index]:\n                            self.archive[max_archive_index] = population[i].copy()\n                            self.archive_fitness[max_archive_index] = fitness[i]\n\n            # Adaptive F and Cr using success history\n            if success_F:\n                self.F = np.mean(success_F)\n            if success_Cr:\n                self.Cr = np.mean(success_Cr)\n\n            # Parameter adaptation with learning rates\n            self.F = np.clip(self.F + self.lr_F * np.random.normal(0, 0.1), 0.1, 1.0)\n            self.Cr = np.clip(self.Cr + self.lr_Cr * np.random.normal(0, 0.1), 0.1, 1.0)\n\n            # Dynamically adjust population size\n            if self.generation % self.pop_adapt_freq == 0:\n                if np.random.rand() < 0.5:  # Probabilistically increase or decrease\n                    self.pop_size = min(self.pop_size + 1, self.pop_size_max)\n                else:\n                    self.pop_size = max(self.pop_size - 1, self.pop_size_min)\n                \n                # Resize population if necessary, keep the best individuals\n                if self.pop_size != len(population):\n                    best_indices = np.argsort(fitness)[:self.pop_size]\n                    population = population[best_indices]\n                    fitness = fitness[best_indices]\n                    while len(population) < self.pop_size:\n                        new_individual = np.random.uniform(lb, ub, size=(self.dim))\n                        population = np.vstack((population, new_individual))\n                        fitness = np.append(fitness, func(new_individual))\n                        self.budget -= 1\n\n\n        return self.f_opt, self.x_opt\n\n    def crossover(self, x, mutant, cr):\n        trial = x.copy()\n        for j in range(self.dim):\n            if np.random.rand() < cr:\n                trial[j] = mutant[j]\n        return trial\n    \n    def mutation_rand1(self, population, i):\n        idxs = [idx for idx in range(self.pop_size) if idx != i]\n        a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n        return population[i] + self.F * (b - c)\n\n    def mutation_current_to_best_1(self, population, i):\n         best = population[np.argmin(np.array([np.sum(x**2) for x in population]))] #Find best with fitness function, replace with self.f_opt later?\n         idxs = [idx for idx in range(self.pop_size) if idx != i]\n         a, b = population[np.random.choice(idxs, 2, replace=False)]\n         return population[i] + self.F * (best - population[i]) + self.F * (a - b)\n    \n    def mutation_rand2(self, population, i):\n        idxs = [idx for idx in range(self.pop_size) if idx != i]\n        a, b, c, d, e = population[np.random.choice(idxs, 5, replace=False)]\n        return a + self.F * (b - c) + self.F * (d-e)\n\n    def mutation_current_to_rand_1(self, population, i):\n        idxs = [idx for idx in range(self.pop_size) if idx != i]\n        r1, r2 = population[np.random.choice(idxs, 2, replace=False)]\n        return population[i] + np.random.rand() * (r1 - population[i]) + self.F * (population[i] - r2)", "configspace": "", "generation": 10, "feedback": "The algorithm CooperativeAdaptiveDE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["c644790d-f6f7-49f4-87d6-80d5786151e8"], "operator": null, "metadata": {"aucs": [0]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "696f0984-bb54-4fcb-b99b-119dcae57906", "fitness": 0.5614004788107971, "name": "SelfAdaptiveDE", "description": "Self-adaptive Differential Evolution with simplified parameter adaptation and a combined mutation strategy using both current-to-best and random differential vectors, enhanced by a Cauchy perturbation.", "code": "import numpy as np\n\nclass SelfAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=10, F=0.5, Cr=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.F = F  # Mutation factor\n        self.Cr = Cr  # Crossover rate\n        self.archive = []\n        self.success_history_F = []\n        self.success_history_Cr = []\n        self.success_history_size = 5\n\n    def __call__(self, func):\n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size  # Deduct initial population evaluations from budget\n\n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)]\n\n        generation = 0\n        while self.budget > 0:\n            generation += 1\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                best_idx = np.argmin(fitness)\n                best = population[best_idx]\n\n                # Combined mutation: current-to-best and random diff + Cauchy\n                mutant = population[i] + self.F * (best - population[i]) + self.F * (a - b) + 0.01 * np.random.standard_cauchy(size=self.dim)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.Cr\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n\n                if f_trial < fitness[i]:\n                    # Archive the replaced individual for diversity\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(population[i].copy())\n                    else:\n                        idx_to_replace = np.random.randint(0, self.archive_size)\n                        self.archive[idx_to_replace] = population[i].copy()\n\n                    fitness[i] = f_trial\n                    population[i] = trial\n\n                    # Update optimal solution\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                    \n                    self.success_history_F.append(self.F)\n                    self.success_history_Cr.append(self.Cr)\n                    if len(self.success_history_F) > self.success_history_size:\n                        self.success_history_F.pop(0)\n                        self.success_history_Cr.pop(0)\n\n            #Adapt F and Cr\n            if self.success_history_F:\n                self.F = np.clip(np.median(self.success_history_F), 0.1, 1.0) + 0.01*np.random.normal(0,1)\n            else:\n                self.F = 0.5 + 0.01*np.random.normal(0,1)\n            self.F = np.clip(self.F, 0.1, 1.0)\n                \n            if self.success_history_Cr:\n                self.Cr = np.clip(np.median(self.success_history_Cr), 0.1, 1.0) + 0.01*np.random.normal(0,1)\n            else:\n                self.Cr = 0.9 + 0.01*np.random.normal(0,1)\n            self.Cr = np.clip(self.Cr, 0.1, 1.0)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 10, "feedback": "The algorithm SelfAdaptiveDE scored 0.561 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["d15f2124-2044-40ea-b0ea-bbc76d19a3ef"], "operator": null, "metadata": {"aucs": [0.24559524019100498, 0.5526647906486715, 0.5146780040658541, 0.939701590754509, 0.5207668146148816, 0.5995472323725379, 0.37434848065475346, 0.47263778117361055, 0.5216176334543131, 0.220299854436818, 0.9177472794770539, 0.9952932168005572, 0.32219448663109684, 0.5697801159168081, 0.9467451651344195, 0.5942948799342951, 0.44443966066974394, 0.7542449673304762, 0.21754283816245323, 0.5038695437920824]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "b9113d16-a143-426f-82c5-d63c68f6af64", "fitness": 0.31114561141216485, "name": "CooperativeSwarm", "description": "Cooperative Swarm Optimization with Dynamic Subgrouping and Adaptive Exploration-Exploitation Balance.", "code": "import numpy as np\n\nclass CooperativeSwarm:\n    def __init__(self, budget=10000, dim=10, pop_size=50, num_subgroups=5, inertia_weight=0.7, cognitive_coeff=1.5, social_coeff=1.5, exploration_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.num_subgroups = num_subgroups\n        self.inertia_weight = inertia_weight\n        self.cognitive_coeff = cognitive_coeff\n        self.social_coeff = social_coeff\n        self.exploration_rate = exploration_rate\n        self.learning_rate = 0.1\n\n    def __call__(self, func):\n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)]\n\n        # Initialize velocities\n        velocities = np.zeros_like(population)\n\n        # Initialize subgroups\n        subgroups = [list(range(i * (self.pop_size // self.num_subgroups), (i + 1) * (self.pop_size // self.num_subgroups))) for i in range(self.num_subgroups)]\n        remaining = self.pop_size % self.num_subgroups\n        for i in range(remaining):\n            subgroups[i].append(self.pop_size - 1 - i)\n\n        generation = 0\n        while self.budget > 0:\n            generation += 1\n            # Dynamic subgroup reassignment (every few generations)\n            if generation % 10 == 0:\n                all_indices = list(range(self.pop_size))\n                np.random.shuffle(all_indices)\n                subgroups = [all_indices[i * (self.pop_size // self.num_subgroups):(i + 1) * (self.pop_size // self.num_subgroups)] for i in range(self.num_subgroups)]\n                remaining = self.pop_size % self.num_subgroups\n                for i in range(remaining):\n                    subgroups[i].append(all_indices[self.pop_size - 1 - i])\n\n            for i in range(self.pop_size):\n                # Find best particle within the subgroup\n                for j in range(self.num_subgroups):\n                    if i in subgroups[j]:\n                         current_subgroup = subgroups[j]\n                         break\n                \n                best_in_subgroup_index = current_subgroup[np.argmin(fitness[current_subgroup])]\n                \n                # Velocity update\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                velocities[i] = (self.inertia_weight * velocities[i]\n                                  + self.cognitive_coeff * r1 * (self.x_opt - population[i])\n                                  + self.social_coeff * r2 * (population[best_in_subgroup_index] - population[i]))\n\n                # Exploration: Random jump with probability exploration_rate\n                if np.random.rand() < self.exploration_rate:\n                    velocities[i] += np.random.uniform(-1, 1, self.dim)\n\n                # Update position\n                new_position = population[i] + velocities[i]\n                new_position = np.clip(new_position, func.bounds.lb, func.bounds.ub)\n\n                # Evaluate new position\n                new_fitness = func(new_position)\n                self.budget -= 1\n\n                # Update if better\n                if new_fitness < fitness[i]:\n                    fitness[i] = new_fitness\n                    population[i] = new_position\n\n                    # Update global best\n                    if new_fitness < self.f_opt:\n                        self.f_opt = new_fitness\n                        self.x_opt = new_position\n\n            #Adapt Exploration Rate\n            self.exploration_rate = np.clip(self.exploration_rate + self.learning_rate * np.random.normal(0,0.01), 0.05, 0.5)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 10, "feedback": "The algorithm CooperativeSwarm scored 0.311 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["d15f2124-2044-40ea-b0ea-bbc76d19a3ef"], "operator": null, "metadata": {"aucs": [0.13952759026095884, 0.16677575950603873, 0.33369030126318056, 0.16436061224451803, 0.2346780489390764, 0.2998626323623551, 0.24754368731951182, 0.2506286097350945, 0.20640120075758273, 0.2048360332395871, 0.25670282386944276, 0.9984430760889785, 0.2163080071641763, 0.275540622688748, 0.6394071408932097, 0.3473830702967178, 0.23155879767089405, 0.37902938381067786, 0.15068160916545859, 0.4795532209670894]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "4971d467-c3ef-44b2-a5ac-f17258710fa4", "fitness": 0.691691235419229, "name": "SelfOrganizingScoutsDE", "description": "Self-organizing scouts enhance exploration and exploitation in DE through a dual-population approach, adaptive parameter control and memory-based learning.", "code": "import numpy as np\n\nclass SelfOrganizingScoutsDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, scout_ratio=0.2, archive_size=10, F=0.5, Cr=0.9, learning_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.scout_size = int(pop_size * scout_ratio)\n        self.main_size = pop_size - self.scout_size\n        self.archive_size = archive_size\n        self.F = F  # Mutation factor\n        self.Cr = Cr  # Crossover rate\n        self.learning_rate = learning_rate\n        self.success_history_F = []\n        self.success_history_Cr = []\n        self.success_history_size = 10\n        self.archive = []\n\n    def __call__(self, func):\n        # Initialize main population\n        main_population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.main_size, self.dim))\n        main_fitness = np.array([func(x) for x in main_population])\n        self.budget -= self.main_size\n\n        # Initialize scout population\n        scout_population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.scout_size, self.dim))\n        scout_fitness = np.array([func(x) for x in scout_population])\n        self.budget -= self.scout_size\n\n        population = np.concatenate((main_population, scout_population))\n        fitness = np.concatenate((main_fitness, scout_fitness))\n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)]\n        \n        generation = 0\n        while self.budget > 0:\n            generation += 1\n\n            # Main population DE\n            for i in range(self.main_size):\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n\n                mutant = np.clip(a + self.F * (b - c), func.bounds.lb, func.bounds.ub)\n\n                cross_points = np.random.rand(self.dim) < self.Cr\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, main_population[i])\n\n                f_trial = func(trial)\n                self.budget -= 1\n                \n                if f_trial < main_fitness[i]:\n                    # Archive the replaced individual for diversity\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(main_population[i].copy())\n                    else:\n                        idx_to_replace = np.random.randint(0, self.archive_size)\n                        self.archive[idx_to_replace] = main_population[i].copy()\n\n                    main_fitness[i] = f_trial\n                    main_population[i] = trial\n                    population[i] = trial\n                    fitness[i] = f_trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n                    self.success_history_F.append(self.F)\n                    self.success_history_Cr.append(self.Cr)\n                    if len(self.success_history_F) > self.success_history_size:\n                        self.success_history_F.pop(0)\n                        self.success_history_Cr.pop(0)\n\n            # Scout population behavior (random jumps and memory-based learning)\n            for i in range(self.scout_size):\n                idx = self.main_size + i\n                if np.random.rand() < 0.5:  # Random jump\n                    trial = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                else:  # Memory-based learning (exploit archive)\n                    if self.archive:\n                        memory = self.archive[np.random.randint(0, len(self.archive))]\n                        trial = np.clip(scout_population[i] + self.F * (memory - scout_population[i]), func.bounds.lb, func.bounds.ub)\n                    else:\n                         trial = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n\n                f_trial = func(trial)\n                self.budget -= 1\n                \n                if f_trial < scout_fitness[i]:\n                    # Archive the replaced individual for diversity\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(scout_population[i].copy())\n                    else:\n                        idx_to_replace = np.random.randint(0, self.archive_size)\n                        self.archive[idx_to_replace] = scout_population[i].copy()\n                    scout_fitness[i] = f_trial\n                    scout_population[i] = trial\n                    population[self.main_size + i] = trial\n                    fitness[self.main_size + i] = f_trial\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n            \n            # Adapt F and Cr based on success history\n            if self.success_history_F:\n                self.F = np.clip(np.mean(self.success_history_F) + self.learning_rate * np.random.normal(0, 0.1), 0.1, 1.0)\n            else:\n                self.F = np.clip(self.F + self.learning_rate * np.random.normal(0, 0.1), 0.1, 1.0)\n\n            if self.success_history_Cr:\n                self.Cr = np.clip(np.mean(self.success_history_Cr) + self.learning_rate * np.random.normal(0, 0.1), 0.1, 1.0)\n            else:\n                self.Cr = np.clip(self.Cr + self.learning_rate * np.random.normal(0, 0.1), 0.1, 1.0)\n            \n            # Re-evaluate scout fitness for possible update of f_opt and x_opt\n            fitness = np.concatenate((main_fitness, scout_fitness))\n            if np.min(fitness) < self.f_opt:\n                self.f_opt = np.min(fitness)\n                self.x_opt = population[np.argmin(fitness)]\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 10, "feedback": "The algorithm SelfOrganizingScoutsDE scored 0.692 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["d15f2124-2044-40ea-b0ea-bbc76d19a3ef"], "operator": null, "metadata": {"aucs": [0.3688153382316346, 0.7056331013399901, 0.5698838633700332, 0.8657106932595328, 0.7395771064913002, 0.7897841353532723, 0.627863313973309, 0.6144343922019853, 0.7581841302477346, 0.6449794125366042, 0.8775388358357593, 0.996773157310622, 0.6615020096406897, 0.6515469779754506, 0.9128260556563728, 0.7703644533118026, 0.6340739377809503, 0.8395364679267528, 0.24118236262174952, 0.5636149633190322]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "ed977d08-d1d0-4f7a-a58e-247292d0f63b", "fitness": "-inf", "name": "AdaptiveMutationDEDynamicPop", "description": "Self-adaptive Differential Evolution with a combination of multiple mutation strategies, dynamic population size, and a restart mechanism to enhance exploration and exploitation.", "code": "import numpy as np\n\nclass AdaptiveMutationDEDynamicPop:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=10, F=0.5, Cr=0.9, lr_F=0.1, lr_Cr=0.1, p_cauchy=0.1, restart_freq=2000, min_pop_size=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.max_pop_size = pop_size\n        self.min_pop_size = min_pop_size\n        self.archive_size = archive_size\n        self.F = F\n        self.Cr = Cr\n        self.lr_F = lr_F\n        self.lr_Cr = lr_Cr\n        self.p_cauchy = p_cauchy  # Probability of using Cauchy mutation\n        self.archive = []\n        self.archive_fitness = []\n        self.memory_F = []\n        self.memory_Cr = []\n        self.memory_size = 10\n        self.success_F = []\n        self.success_Cr = []\n        self.restart_freq = restart_freq\n        self.eval_count = 0\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n        self.eval_count += self.pop_size\n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)]\n\n        while self.budget > 0:\n            success_F = []\n            success_Cr = []\n            for i in range(self.pop_size):\n                # Mutation: Adaptive choice between multiple mutation strategies\n                rand = np.random.rand()\n                if rand < self.p_cauchy:\n                    # Cauchy mutation for exploration\n                    idxs = [idx for idx in range(self.pop_size) if idx != i]\n                    a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                    mutant = population[i] + self.F * np.random.standard_cauchy(size=self.dim)\n                    mutant = np.clip(mutant, lb, ub)\n                elif rand < 2 * self.p_cauchy and len(self.archive) > 0:\n                    # Mutation using archive\n                    idx_archive = np.random.randint(len(self.archive))\n                    a = self.archive[idx_archive]\n                    idxs = [idx for idx in range(self.pop_size) if idx != i]\n                    b, c = population[np.random.choice(idxs, 2, replace=False)]\n                    mutant = np.clip(a + self.F * (b - c), lb, ub)\n                else:\n                    # Standard DE mutation\n                    idxs = [idx for idx in range(self.pop_size) if idx != i]\n                    a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                    mutant = np.clip(a + self.F * (b - c), lb, ub)\n\n                # Crossover\n                trial = self.crossover(population[i], mutant, self.Cr)\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                self.eval_count += 1\n                if f_trial < fitness[i]:\n                    success_F.append(self.F)\n                    success_Cr.append(self.Cr)\n                    fitness[i] = f_trial\n                    population[i] = trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n                    # Update archive\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(population[i].copy())\n                        self.archive_fitness.append(fitness[i])\n                    else:\n                        max_archive_index = np.argmax(self.archive_fitness)\n                        if fitness[i] < self.archive_fitness[max_archive_index]:\n                            self.archive[max_archive_index] = population[i].copy()\n                            self.archive_fitness[max_archive_index] = fitness[i]\n\n            # Adaptive F and Cr using success history\n            if success_F:\n                self.F = np.mean(success_F)\n            if success_Cr:\n                self.Cr = np.mean(success_Cr)\n\n            # Parameter adaptation with learning rates\n            self.F = np.clip(self.F + self.lr_F * np.random.normal(0, 0.1), 0.1, 1.0)\n            self.Cr = np.clip(self.Cr + self.lr_Cr * np.random.normal(0, 0.1), 0.1, 1.0)\n            \n            # Dynamic population size adjustment\n            if len(success_F) > 0.2 * self.pop_size and self.pop_size < self.max_pop_size:\n                self.pop_size = min(self.pop_size + 1, self.max_pop_size)  # Increase if successful\n            elif len(success_F) < 0.05 * self.pop_size and self.pop_size > self.min_pop_size:\n                 self.pop_size = max(self.pop_size -1, self.min_pop_size) # Decrease if not successful\n\n            # Restart mechanism\n            if self.eval_count >= self.restart_freq:\n                population = np.random.uniform(lb, ub, size=(self.pop_size, self.dim))\n                fitness = np.array([func(x) for x in population])\n                self.budget -= self.pop_size\n                self.eval_count = self.pop_size\n                best_index = np.argmin(fitness)\n                if fitness[best_index] < self.f_opt:\n                    self.f_opt = fitness[best_index]\n                    self.x_opt = population[best_index]\n                self.eval_count = 0\n\n\n        return self.f_opt, self.x_opt\n\n    def crossover(self, x, mutant, cr):\n        trial = x.copy()\n        for j in range(self.dim):\n            if np.random.rand() < cr:\n                trial[j] = mutant[j]\n        return trial", "configspace": "", "generation": 10, "feedback": "In the code, line 44, in __call__, the following error occurred:\nIndexError: index 48 is out of bounds for axis 0 with size 48\nOn line: a, b, c = population[np.random.choice(idxs, 3, replace=False)]", "error": "In the code, line 44, in __call__, the following error occurred:\nIndexError: index 48 is out of bounds for axis 0 with size 48\nOn line: a, b, c = population[np.random.choice(idxs, 3, replace=False)]", "parent_ids": ["c644790d-f6f7-49f4-87d6-80d5786151e8"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "663f4ac5-4c97-439e-a0b0-d95561f044c3", "fitness": 0.0, "name": "SelfAdaptiveDE", "description": "Self-adaptive Differential Evolution with a combined mutation strategy (current-to-best/rand/pbest), success-history based parameter adaptation, and L-SHADE inspired population reduction.", "code": "import numpy as np\n\nclass SelfAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=100, archive_size=50, F=0.5, Cr=0.9, p_best=0.1, reduction_factor=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.F = F  # Mutation factor\n        self.Cr = Cr  # Crossover rate\n        self.p_best = p_best # Percentage of best individuals to select from\n        self.reduction_factor = reduction_factor # Population reduction factor\n        self.learning_rate_F = 0.1\n        self.learning_rate_Cr = 0.1\n        self.success_history_F = []\n        self.success_history_Cr = []\n        self.success_history_size = 10\n        self.archive = []\n\n    def __call__(self, func):\n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size  # Deduct initial population evaluations from budget\n\n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)]\n\n        generation = 0\n        while self.budget > self.pop_size:\n            generation += 1\n            sorted_indices = np.argsort(fitness)\n            best_indices = sorted_indices[:int(self.p_best * self.pop_size)]\n\n            for i in range(self.pop_size):\n                # Mutation Strategy: Combination of current-to-best, rand, and pbest\n\n                #current-to-pbest\n                pbest_index = np.random.choice(best_indices)\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                x_r1, x_r2 = population[np.random.choice(idxs, 2, replace=False)]\n                mutant = population[i] + self.F * (population[pbest_index] - population[i]) + self.F * (x_r1 - x_r2)\n\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.Cr\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n\n                if f_trial < fitness[i]:\n                    # Archive the replaced individual for diversity\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(population[i].copy())\n                    else:\n                        idx_to_replace = np.random.randint(0, self.archive_size)\n                        self.archive[idx_to_replace] = population[i].copy()\n\n                    fitness[i] = f_trial\n                    population[i] = trial\n\n                    # Update optimal solution\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n                    # Update success history for F and Cr adaptation\n                    self.success_history_F.append(self.F)\n                    self.success_history_Cr.append(self.Cr)\n                    if len(self.success_history_F) > self.success_history_size:\n                        self.success_history_F.pop(0)\n                        self.success_history_Cr.pop(0)\n\n            # Adapt F and Cr based on success history\n            if self.success_history_F:\n                self.F = np.clip(np.mean(self.success_history_F) + self.learning_rate_F * np.random.normal(0, 0.1), 0.1, 1.0)\n            else:\n                self.F = np.clip(self.F + self.learning_rate_F * np.random.normal(0, 0.1), 0.1, 1.0)\n\n            if self.success_history_Cr:\n                self.Cr = np.clip(np.mean(self.success_history_Cr) + self.learning_rate_Cr * np.random.normal(0, 0.1), 0.1, 1.0)\n            else:\n                self.Cr = np.clip(self.Cr + self.learning_rate_Cr * np.random.normal(0, 0.1), 0.1, 1.0)\n\n            # L-SHADE inspired population reduction\n            new_pop_size = int(self.pop_size * self.reduction_factor)\n            if new_pop_size < 10:\n                new_pop_size = 10\n            if new_pop_size < self.pop_size:\n                 sorted_indices = np.argsort(fitness)\n                 population = population[sorted_indices[:new_pop_size]]\n                 fitness = fitness[sorted_indices[:new_pop_size]]\n                 self.pop_size = new_pop_size\n\n        # Final refinement with remaining budget\n        while self.budget > 0:\n           \n            for i in range(self.pop_size):\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + self.F * (b - c), func.bounds.lb, func.bounds.ub)\n\n                cross_points = np.random.rand(self.dim) < self.Cr\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                f_trial = func(trial)\n                self.budget -= 1\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    population[i] = trial\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 10, "feedback": "The algorithm SelfAdaptiveDE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["d15f2124-2044-40ea-b0ea-bbc76d19a3ef"], "operator": null, "metadata": {"aucs": [0]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "5c558f17-b6fc-4a9f-8942-68f5aeff6055", "fitness": 0.0, "name": "SelfAdaptiveDE", "description": "Self-adaptive Differential Evolution with orthogonal learning, success-history adaptation of F and CR, and a toroidal boundary handling mechanism for enhanced exploration and exploitation.", "code": "import numpy as np\n\nclass SelfAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=10, F=0.5, Cr=0.9, velocity_weight=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.F = F  # Mutation factor\n        self.Cr = Cr  # Crossover rate\n        self.velocity_weight = velocity_weight  # Weight for velocity-based mutation\n        self.learning_rate_F = 0.1\n        self.learning_rate_Cr = 0.1\n        self.success_history_F = []\n        self.success_history_Cr = []\n        self.success_history_size = 10\n        self.archive = []\n        self.orthogonal_design_size = min(pop_size, dim * 2) # Reduced orthogonal design size\n        self.lb = -5.0\n        self.ub = 5.0\n\n\n    def __call__(self, func):\n        # Initialize population\n        population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size  # Deduct initial population evaluations from budget\n\n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)]\n\n        # Initialize velocity\n        velocity = np.zeros_like(population)\n\n        generation = 0\n        while self.budget > 0:\n            generation += 1\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n\n                # Velocity update\n                velocity[i] = self.velocity_weight * velocity[i] + self.F * (b - c)\n\n                # Apply mutation with velocity\n                mutant = a + velocity[i]\n                # Toroidal boundary handling\n                mutant = (mutant - self.lb) % (self.ub - self.lb) + self.lb\n                mutant = np.clip(mutant, self.lb, self.ub)\n\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.Cr\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n\n                if f_trial < fitness[i]:\n                    # Archive the replaced individual for diversity\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(population[i].copy())\n                    else:\n                        idx_to_replace = np.random.randint(0, self.archive_size)\n                        self.archive[idx_to_replace] = population[i].copy()\n\n                    fitness[i] = f_trial\n                    population[i] = trial\n\n                    # Update optimal solution\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n                    # Update success history for F and Cr adaptation\n                    self.success_history_F.append(self.F)\n                    self.success_history_Cr.append(self.Cr)\n                    if len(self.success_history_F) > self.success_history_size:\n                        self.success_history_F.pop(0)\n                        self.success_history_Cr.pop(0)\n\n            # Adapt F and Cr based on success history\n            if self.success_history_F:\n                self.F = np.clip(np.mean(self.success_history_F) + self.learning_rate_F * np.random.normal(0, 0.1), 0.1, 1.0)\n            else:\n                self.F = np.clip(self.F + self.learning_rate_F * np.random.normal(0, 0.1), 0.1, 1.0)\n\n            if self.success_history_Cr:\n                self.Cr = np.clip(np.mean(self.success_history_Cr) + self.learning_rate_Cr * np.random.normal(0, 0.1), 0.1, 1.0)\n            else:\n                self.Cr = np.clip(self.Cr + self.learning_rate_Cr * np.random.normal(0, 0.1), 0.1, 1.0)\n\n            # Orthogonal Learning\n            if generation % 5 == 0 and self.budget > self.orthogonal_design_size:\n                best_idx = np.argmin(fitness)\n                center = population[best_idx].copy()\n                orthogonal_array = self.generate_orthogonal_array(self.orthogonal_design_size, self.dim)\n                for j in range(self.orthogonal_design_size):\n                    trial = center + 0.1 * (orthogonal_array[j] - 0.5) * (self.ub - self.lb)\n                    trial = np.clip(trial, self.lb, self.ub)\n                    f_trial = func(trial)\n                    self.budget -= 1\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n                    if f_trial < fitness[best_idx]:\n                      fitness[best_idx] = f_trial\n                      population[best_idx] = trial\n\n        return self.f_opt, self.x_opt\n\n    def generate_orthogonal_array(self, num_points, num_variables):\n        # A simple implementation of orthogonal array generation for demonstration.\n        # In practice, you'd use a library or a more robust method.\n        array = np.random.rand(num_points, num_variables)\n        return array", "configspace": "", "generation": 10, "feedback": "The algorithm SelfAdaptiveDE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["d15f2124-2044-40ea-b0ea-bbc76d19a3ef"], "operator": null, "metadata": {"aucs": [0]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "baf81fef-da08-4bf1-a2b9-0d32ed61131d", "fitness": 0.7630542438722347, "name": "CooperativeAdaptiveDE", "description": "Cooperative Differential Evolution with a population split into explorers and exploiters, guided by a shared archive and self-adaptive parameter control.", "code": "import numpy as np\n\nclass CooperativeAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, explorer_ratio=0.5, archive_size=10, F=0.5, Cr=0.9, lr_F=0.1, lr_Cr=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.explorer_ratio = explorer_ratio  # Ratio of population dedicated to exploration\n        self.archive_size = archive_size\n        self.F = F\n        self.Cr = Cr\n        self.lr_F = lr_F\n        self.lr_Cr = lr_Cr\n        self.archive = []\n        self.archive_fitness = []\n        self.explorer_size = int(self.pop_size * self.explorer_ratio)\n        self.exploiter_size = self.pop_size - self.explorer_size\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)]\n\n        while self.budget > 0:\n            # Split population into explorers and exploiters\n            explorer_indices = np.argsort(fitness)[:self.explorer_size]  # Worst individuals become explorers\n            exploiter_indices = np.argsort(fitness)[self.explorer_size:] # Best individuals become exploiters\n            explorers = population[explorer_indices]\n            exploiters = population[exploiter_indices]\n            explorer_fitness = fitness[explorer_indices]\n            exploiter_fitness = fitness[exploiter_indices]\n            \n            # Explorer update\n            for i in range(self.explorer_size):\n                idxs = [idx for idx in range(self.pop_size) if idx != explorer_indices[i]]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = explorers[i] + self.F * (b - c) + np.random.normal(0, 0.1, size=self.dim) # Add noise for exploration\n                mutant = np.clip(mutant, lb, ub)\n                trial = self.crossover(explorers[i], mutant, self.Cr)\n                f_trial = func(trial)\n                self.budget -= 1\n                if f_trial < explorer_fitness[i]:\n                    explorer_fitness[i] = f_trial\n                    explorers[i] = trial\n                    population[explorer_indices[i]] = trial\n                    fitness[explorer_indices[i]] = f_trial\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                    self.update_archive(trial, f_trial)\n\n            # Exploiter update\n            for i in range(self.exploiter_size):\n                idxs = [idx for idx in range(self.pop_size) if idx != exploiter_indices[i]]\n                #Exploitation around best solution using archive\n                if len(self.archive) > 0:\n                    archive_idx = np.random.randint(len(self.archive))\n                    a = self.archive[archive_idx]\n                    b, c = population[np.random.choice(idxs, 2, replace=False)]\n                    mutant = np.clip(a + self.F * (b - c), lb, ub)\n                else:\n                    a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                    mutant = np.clip(a + self.F * (b - c), lb, ub)\n                \n                trial = self.crossover(exploiters[i], mutant, self.Cr)\n                f_trial = func(trial)\n                self.budget -= 1\n\n                if f_trial < exploiter_fitness[i]:\n                    exploiter_fitness[i] = f_trial\n                    exploiters[i] = trial\n                    population[exploiter_indices[i]] = trial\n                    fitness[exploiter_indices[i]] = f_trial\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                    self.update_archive(trial, f_trial)\n\n            # Adaptive F and Cr\n            self.F = np.clip(self.F + self.lr_F * np.random.normal(0, 0.1), 0.1, 1.0)\n            self.Cr = np.clip(self.Cr + self.lr_Cr * np.random.normal(0, 0.1), 0.1, 1.0)\n\n        return self.f_opt, self.x_opt\n\n    def crossover(self, x, mutant, cr):\n        trial = x.copy()\n        for j in range(self.dim):\n            if np.random.rand() < cr:\n                trial[j] = mutant[j]\n        return trial\n\n    def update_archive(self, x, fitness):\n        if len(self.archive) < self.archive_size:\n            self.archive.append(x.copy())\n            self.archive_fitness.append(fitness)\n        else:\n            max_archive_index = np.argmax(self.archive_fitness)\n            if fitness < self.archive_fitness[max_archive_index]:\n                self.archive[max_archive_index] = x.copy()\n                self.archive_fitness[max_archive_index] = fitness", "configspace": "", "generation": 10, "feedback": "The algorithm CooperativeAdaptiveDE scored 0.763 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["c644790d-f6f7-49f4-87d6-80d5786151e8"], "operator": null, "metadata": {"aucs": [0.47264145982465133, 0.8029778061066286, 0.8057528939251792, 0.9287438032465801, 0.8033285097426462, 0.833695522512675, 0.8093090569421292, 0.7321596310517002, 0.8127436990707841, 0.7688537322846603, 0.9194590042627068, 0.9920219215073159, 0.671616481770777, 0.8144466679454256, 0.9487135705450869, 0.8590532230015604, 0.6537551414792473, 0.8720829101757699, 0.2209675340544458, 0.5387623079947235]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "d783d3ae-44f7-46e0-a11a-73f7f9ad953e", "fitness": "-inf", "name": "EnhancedSelfAdaptiveDE", "description": "Enhanced Self-Adaptive Differential Evolution with orthogonal learning, covariance matrix adaptation, and a restart mechanism for improved exploration and exploitation.", "code": "import numpy as np\nfrom scipy.linalg import orth\n\nclass EnhancedSelfAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=10, F=0.5, Cr=0.9, velocity_weight=0.1, orthogonal_learning_rate=0.1, restart_threshold=1e-6):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.F = F  # Mutation factor\n        self.Cr = Cr  # Crossover rate\n        self.velocity_weight = velocity_weight  # Weight for velocity-based mutation\n        self.learning_rate_F = 0.1\n        self.learning_rate_Cr = 0.1\n        self.success_history_F = []\n        self.success_history_Cr = []\n        self.success_history_size = 10\n        self.archive = []\n        self.orthogonal_learning_rate = orthogonal_learning_rate\n        self.restart_threshold = restart_threshold\n        self.covariance_matrix = np.eye(dim)\n        self.mean = np.zeros(dim)\n        self.restart_iterations = 100\n\n    def orthogonal_design(self, population, num_points):\n        \"\"\"Generates an orthogonal design around the best solution.\"\"\"\n        best_idx = np.argmin([f for f in self.fitness_history[-self.pop_size:]])\n        best_solution = self.population_history[-1][best_idx]\n\n        # Generate an orthogonal matrix\n        if self.dim > 1:\n            orthogonal_matrix = orth(np.random.randn(self.dim, self.dim))[:num_points]\n        else:\n            orthogonal_matrix = np.random.rand(num_points, self.dim)\n\n        # Scale the orthogonal matrix to the search space range\n        scaled_matrix = orthogonal_matrix * self.orthogonal_learning_rate\n\n        # Generate new points around the best solution\n        new_points = best_solution + scaled_matrix\n        new_points = np.clip(new_points, self.func_bounds.lb, self.func_bounds.ub)  # Clip values\n\n        return new_points\n\n    def __call__(self, func):\n        self.func_bounds = func.bounds\n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size  # Deduct initial population evaluations from budget\n\n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)]\n\n        # Initialize velocity\n        velocity = np.zeros_like(population)\n        self.population_history = [population.copy()]\n        self.fitness_history = list(fitness.copy())\n\n        generation = 0\n        last_improvement = 0\n\n        while self.budget > 0:\n            generation += 1\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n\n                # Velocity update\n                velocity[i] = self.velocity_weight * velocity[i] + self.F * (b - c)\n\n                # Apply mutation with velocity\n                mutant = np.clip(a + velocity[i] + np.random.multivariate_normal(np.zeros(self.dim), self.covariance_matrix), func.bounds.lb, func.bounds.ub)\n\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.Cr\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                self.fitness_history.append(f_trial)\n\n                if f_trial < fitness[i]:\n                    # Archive the replaced individual for diversity\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(population[i].copy())\n                    else:\n                        idx_to_replace = np.random.randint(0, self.archive_size)\n                        self.archive[idx_to_replace] = population[i].copy()\n\n                    fitness[i] = f_trial\n                    population[i] = trial\n\n                    # Update optimal solution\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                        last_improvement = generation\n\n                    # Update success history for F and Cr adaptation\n                    self.success_history_F.append(self.F)\n                    self.success_history_Cr.append(self.Cr)\n                    if len(self.success_history_F) > self.success_history_size:\n                        self.success_history_F.pop(0)\n                        self.success_history_Cr.pop(0)\n            self.population_history.append(population.copy())\n\n            # Adapt F and Cr based on success history\n            if self.success_history_F:\n                self.F = np.clip(np.mean(self.success_history_F) + self.learning_rate_F * np.random.normal(0, 0.1), 0.1, 1.0)\n            else:\n                self.F = np.clip(self.F + self.learning_rate_F * np.random.normal(0, 0.1), 0.1, 1.0)\n\n            if self.success_history_Cr:\n                self.Cr = np.clip(np.mean(self.success_history_Cr) + self.learning_rate_Cr * np.random.normal(0, 0.1), 0.1, 1.0)\n            else:\n                self.Cr = np.clip(self.Cr + self.learning_rate_Cr * np.random.normal(0, 0.1), 0.1, 1.0)\n\n            # Update covariance matrix using CMA-ES-like update\n            weights = np.maximum(0, np.log(self.pop_size / 2 + 1) - np.log(np.arange(1, self.pop_size + 1)))\n            weights /= np.sum(weights)\n            diff = population - self.mean\n            self.covariance_matrix = np.cov(diff.T, aweights=weights, ddof=0)\n            self.mean = np.average(population, axis=0, weights=weights)\n\n\n            # Orthogonal Learning\n            num_orthogonal_points = int(self.pop_size * self.orthogonal_learning_rate)\n            orthogonal_points = self.orthogonal_design(population, num_orthogonal_points)\n\n            orthogonal_fitness = np.array([func(x) for x in orthogonal_points])\n            self.budget -= num_orthogonal_points\n            self.fitness_history.extend(list(orthogonal_fitness))\n\n            # Replace worst individuals with orthogonal points\n            worst_indices = np.argsort(fitness)[-num_orthogonal_points:]\n            for idx, orth_idx in zip(worst_indices, range(num_orthogonal_points)):\n                if orthogonal_fitness[orth_idx] < fitness[idx]:\n                    fitness[idx] = orthogonal_fitness[orth_idx]\n                    population[idx] = orthogonal_points[orth_idx]\n                    if orthogonal_fitness[orth_idx] < self.f_opt:\n                        self.f_opt = orthogonal_fitness[orth_idx]\n                        self.x_opt = orthogonal_points[orth_idx]\n                        last_improvement = generation\n\n\n            # Restart mechanism\n            if generation - last_improvement > self.restart_iterations:\n                population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                fitness = np.array([func(x) for x in population])\n                self.budget -= self.pop_size\n                self.covariance_matrix = np.eye(self.dim)  # Reset covariance\n                self.mean = np.mean(population, axis=0)\n                last_improvement = generation\n                self.success_history_F = []\n                self.success_history_Cr = []\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 11, "feedback": "In the code, line 32, in orthogonal_design, the following error occurred:\nNameError: name 'orth' is not defined\nOn line: orthogonal_matrix = orth(np.random.randn(self.dim, self.dim))[:num_points]", "error": "In the code, line 32, in orthogonal_design, the following error occurred:\nNameError: name 'orth' is not defined\nOn line: orthogonal_matrix = orth(np.random.randn(self.dim, self.dim))[:num_points]", "parent_ids": ["d15f2124-2044-40ea-b0ea-bbc76d19a3ef"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "ec85b5e8-587f-44e3-bb70-ae422cf0bad3", "fitness": "-inf", "name": "CooperativeAdaptiveDE", "description": "Cooperative Differential Evolution with dynamic population splitting based on fitness landscape analysis, using spectral clustering to identify promising regions and allocate more individuals.", "code": "import numpy as np\nfrom sklearn.cluster import SpectralClustering\nfrom sklearn.metrics import silhouette_score\n\nclass CooperativeAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, explorer_ratio=0.5, archive_size=10, F=0.5, Cr=0.9, lr_F=0.1, lr_Cr=0.1, cluster_update_interval=500):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.explorer_ratio = explorer_ratio  # Initial ratio\n        self.archive_size = archive_size\n        self.F = F\n        self.Cr = Cr\n        self.lr_F = lr_F\n        self.lr_Cr = lr_Cr\n        self.archive = []\n        self.archive_fitness = []\n        self.explorer_size = int(self.pop_size * self.explorer_ratio)\n        self.exploiter_size = self.pop_size - self.explorer_size\n        self.cluster_update_interval = cluster_update_interval\n        self.eval_count = 0\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n        self.eval_count += self.pop_size\n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)]\n\n        while self.budget > 0:\n            if self.eval_count % self.cluster_update_interval == 0 and self.eval_count > 0:\n                # Dynamic population splitting based on spectral clustering\n                num_clusters = min(4, self.pop_size // 4)  # Adjust number of clusters\n                if num_clusters > 1:\n                    try:\n                        clustering = SpectralClustering(n_clusters=num_clusters, assign_labels='discretize', random_state=0, affinity='nearest_neighbors').fit(population)\n                        labels = clustering.labels_\n                        \n                        # Adjust population sizes based on cluster fitness\n                        cluster_fitness = [np.mean(fitness[labels == i]) for i in range(num_clusters)]\n                        best_cluster = np.argmin(cluster_fitness)\n                        \n                        # Increase exploiter size for the best cluster\n                        cluster_size = np.sum(labels == best_cluster)\n                        self.explorer_ratio = max(0.1, 1 - (cluster_size / self.pop_size) * 1.5)  # Ensure at least 10% explorers\n                        self.explorer_size = int(self.pop_size * self.explorer_ratio)\n                        self.exploiter_size = self.pop_size - self.explorer_size\n                    except:\n                        pass  # Handle clustering errors\n            \n            # Split population into explorers and exploiters\n            explorer_indices = np.argsort(fitness)[:self.explorer_size]  # Worst individuals become explorers\n            exploiter_indices = np.argsort(fitness)[self.explorer_size:] # Best individuals become exploiters\n            explorers = population[explorer_indices]\n            exploiters = population[exploiter_indices]\n            explorer_fitness = fitness[explorer_indices]\n            exploiter_fitness = fitness[exploiter_indices]\n            \n            # Explorer update\n            for i in range(self.explorer_size):\n                idxs = [idx for idx in range(self.pop_size) if idx != explorer_indices[i]]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = explorers[i] + self.F * (b - c) + np.random.normal(0, 0.1, size=self.dim) # Add noise for exploration\n                mutant = np.clip(mutant, lb, ub)\n                trial = self.crossover(explorers[i], mutant, self.Cr)\n                f_trial = func(trial)\n                self.budget -= 1\n                self.eval_count += 1\n                if f_trial < explorer_fitness[i]:\n                    explorer_fitness[i] = f_trial\n                    explorers[i] = trial\n                    population[explorer_indices[i]] = trial\n                    fitness[explorer_indices[i]] = f_trial\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                    self.update_archive(trial, f_trial)\n\n            # Exploiter update\n            for i in range(self.exploiter_size):\n                idxs = [idx for idx in range(self.pop_size) if idx != exploiter_indices[i]]\n                #Exploitation around best solution using archive\n                if len(self.archive) > 0:\n                    archive_idx = np.random.randint(len(self.archive))\n                    a = self.archive[archive_idx]\n                    b, c = population[np.random.choice(idxs, 2, replace=False)]\n                    mutant = np.clip(a + self.F * (b - c), lb, ub)\n                else:\n                    a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                    mutant = np.clip(a + self.F * (b - c), lb, ub)\n                \n                trial = self.crossover(exploiters[i], mutant, self.Cr)\n                f_trial = func(trial)\n                self.budget -= 1\n                self.eval_count += 1\n\n                if f_trial < exploiter_fitness[i]:\n                    exploiter_fitness[i] = f_trial\n                    exploiters[i] = trial\n                    population[exploiter_indices[i]] = trial\n                    fitness[exploiter_indices[i]] = f_trial\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                    self.update_archive(trial, f_trial)\n\n            # Adaptive F and Cr\n            self.F = np.clip(self.F + self.lr_F * np.random.normal(0, 0.1), 0.1, 1.0)\n            self.Cr = np.clip(self.Cr + self.lr_Cr * np.random.normal(0, 0.1), 0.1, 1.0)\n\n        return self.f_opt, self.x_opt\n\n    def crossover(self, x, mutant, cr):\n        trial = x.copy()\n        for j in range(self.dim):\n            if np.random.rand() < cr:\n                trial[j] = mutant[j]\n        return trial\n\n    def update_archive(self, x, fitness):\n        if len(self.archive) < self.archive_size:\n            self.archive.append(x.copy())\n            self.archive_fitness.append(fitness)\n        else:\n            max_archive_index = np.argmax(self.archive_fitness)\n            if fitness < self.archive_fitness[max_archive_index]:\n                self.archive[max_archive_index] = x.copy()\n                self.archive_fitness[max_archive_index] = fitness", "configspace": "", "generation": 11, "feedback": "Evaluation timed out after 300 seconds.", "error": "Evaluation timed out after 300 seconds.", "parent_ids": ["baf81fef-da08-4bf1-a2b9-0d32ed61131d"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "f301c1c5-0957-4ba0-bb3b-52dc297e0038", "fitness": 0.5931189167587558, "name": "AdaptiveDE", "description": "Adaptive Differential Evolution with orthogonal crossover and a simplified self-adaptation mechanism for improved exploration and exploitation.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, Cr=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F  # Mutation factor\n        self.Cr = Cr  # Crossover rate\n        self.lr_F = 0.1 #learning rate for F\n        self.lr_Cr = 0.1 #learning rate for Cr\n        self.archive_size = int(self.pop_size*0.2)\n        self.archive = []\n\n    def __call__(self, func):\n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)]\n        \n        mse_F = 0.1\n        mse_Cr = 0.1\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + self.F * (b - c), func.bounds.lb, func.bounds.ub)\n\n                # Orthogonal Crossover\n                trial = population[i].copy()\n                num_changes = max(1, int(self.Cr * self.dim)) # Ensure at least one change\n                change_indices = np.random.choice(self.dim, num_changes, replace=False)\n                trial[change_indices] = mutant[change_indices]\n                \n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n\n                if f_trial < fitness[i]:\n                    # Update archive\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(population[i].copy())\n                    else:\n                        self.archive[np.random.randint(0, self.archive_size)] = population[i].copy()\n                        \n                    fitness[i] = f_trial\n                    population[i] = trial\n\n                    # Update optimal solution\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n                    # Adapt parameters (simplified)\n                    self.F = np.clip(self.F + self.lr_F * np.random.normal(0, mse_F), 0.1, 1.0)\n                    self.Cr = np.clip(self.Cr + self.lr_Cr * np.random.normal(0, mse_Cr), 0.1, 1.0)\n                else:\n                    pass\n                    # No change in parameters\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 11, "feedback": "The algorithm AdaptiveDE scored 0.593 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["94768647-f252-46d5-98c5-9ea8189ba67b"], "operator": null, "metadata": {"aucs": [0.17197430905359978, 0.3857467974176447, 0.5501406791173552, 0.8493042257738006, 0.5944083262960838, 0.6676109728617852, 0.6481326469729236, 0.5616991937864706, 0.7670296855405956, 0.47675695691768183, 0.8385649269481981, 0.9944405108570262, 0.30190881781668355, 0.6140751006151703, 0.759241963393615, 0.6156946078728267, 0.5404446129344662, 0.833437425518313, 0.19546470860160547, 0.4963018668792738]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "b6ecade9-8938-4f3f-9015-47a8e7878dd2", "fitness": "-inf", "name": "EnhancedSelfAdaptiveDE", "description": "Enhanced Self-Adaptive Differential Evolution with dynamic population size adjustment and orthogonal crossover for improved exploration and exploitation.", "code": "import numpy as np\n\nclass EnhancedSelfAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, Cr=0.9, min_pop_size=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.min_pop_size = min_pop_size\n        self.F = F  # Mutation factor\n        self.Cr = Cr  # Crossover rate\n        self.learning_rate = 0.1  # Initial learning rate for parameter adaptation\n        self.archive_size = int(self.pop_size * 0.2) # Archive size as a fraction of pop_size\n        self.archive = []\n\n    def __call__(self, func):\n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size # Deduct initial population evaluations from budget\n\n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)]\n\n        generation = 0\n        while self.budget > 0:\n            generation += 1\n\n            # Population size adjustment\n            if generation % 10 == 0:\n                improvement_ratio = np.sum(fitness < np.mean(fitness)) / self.pop_size\n                if improvement_ratio > 0.3 and self.pop_size < 100:\n                    self.pop_size = min(100, int(self.pop_size * 1.1))  # Increase population if good progress\n                elif improvement_ratio < 0.1 and self.pop_size > self.min_pop_size:\n                    self.pop_size = max(self.min_pop_size, int(self.pop_size * 0.9))  # Decrease population if stagnant\n                \n                # Resize population and fitness array, keeping best individuals\n                sorted_indices = np.argsort(fitness)\n                population = population[sorted_indices[:self.pop_size]]\n                fitness = fitness[sorted_indices[:self.pop_size]]\n                \n                # Re-evaluate fitness for new individuals, if population increased\n                if population.shape[0] < self.pop_size:\n                    new_individuals = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size - population.shape[0], self.dim))\n                    population = np.concatenate((population, new_individuals))\n                    new_fitness = np.array([func(x) for x in new_individuals])\n                    fitness = np.concatenate((fitness,new_fitness))\n                    self.budget -= new_individuals.shape[0]\n\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n\n                # Incorporate archive if it's not empty\n                if self.archive:\n                    arch_idx = np.random.randint(0, len(self.archive))\n                    arch_member = self.archive[arch_idx]\n                    mutant = np.clip(a + self.F * (b - c) + self.F * (self.x_opt - population[i]), func.bounds.lb, func.bounds.ub)\n                else:\n                    mutant = np.clip(a + self.F * (b - c), func.bounds.lb, func.bounds.ub)\n                \n                # Orthogonal Crossover\n                trial = self.orthogonal_crossover(population[i], mutant)\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub) # Clip again after crossover\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n\n                if f_trial < fitness[i]:\n                    # Update Archive\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(population[i].copy())\n                    else:\n                        #Replace random element in archive with the replaced particle\n                        replace_idx = np.random.randint(0, len(self.archive))\n                        self.archive[replace_idx] = population[i].copy()\n\n                    fitness[i] = f_trial\n                    population[i] = trial\n\n                    # Update optimal solution\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                        # Increase learning rate if improvement is found\n                        self.learning_rate = min(1.0, self.learning_rate * 1.1)\n                else:\n                    # Reduce learning rate if no improvement\n                    self.learning_rate = max(0.01, self.learning_rate * 0.9)\n\n            # Adapt parameters based on learning rate\n            self.F = np.clip(self.F + self.learning_rate * np.random.normal(0, 0.1), 0.1, 1.0)\n            self.Cr = np.clip(self.Cr + self.learning_rate * np.random.normal(0, 0.1), 0.1, 1.0)\n\n        return self.f_opt, self.x_opt\n\n    def orthogonal_crossover(self, x, mutant, num_samples=5):\n        \"\"\"\n        Performs orthogonal crossover between two vectors.\n        \"\"\"\n        lb = -1.0 * np.ones(self.dim)\n        ub = np.ones(self.dim)\n        levels = [np.linspace(lb[i], ub[i], num_samples) for i in range(self.dim)]\n        \n        dm = np.zeros((self.dim, num_samples))\n        for i in range(self.dim):\n            dm[i, :] = levels[i]\n        \n        cm = 0.5 * (x + mutant)\n        dm = cm + 0.5 * (mutant - x) * dm\n        \n        fitness_values = np.zeros(num_samples)\n        \n        for k in range(num_samples):\n            trial_vector = dm[:, k]\n            fitness_values[k] = np.sum((trial_vector - cm) ** 2) # Dummy fitness\n        \n        best_level_index = np.argmin(fitness_values)\n        return dm[:, best_level_index]", "configspace": "", "generation": 11, "feedback": "In the code, line 111, in orthogonal_crossover, the following error occurred:\nValueError: operands could not be broadcast together with shapes (2,) (2,5) \nOn line: dm = cm + 0.5 * (mutant - x) * dm", "error": "In the code, line 111, in orthogonal_crossover, the following error occurred:\nValueError: operands could not be broadcast together with shapes (2,) (2,5) \nOn line: dm = cm + 0.5 * (mutant - x) * dm", "parent_ids": ["94768647-f252-46d5-98c5-9ea8189ba67b"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "c68eb5dc-2394-4b2d-9bb1-48bd05e8bb46", "fitness": 0.0, "name": "SelfAdaptiveDE", "description": "Self-adaptive Differential Evolution with a dynamically adjusted population size, an enhanced mutation strategy incorporating both velocity and archive information, and adaptive parameter control based on a weighted success history.", "code": "import numpy as np\n\nclass SelfAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=10, F=0.5, Cr=0.9, velocity_weight=0.1, pop_size_adapt=True):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.F = F  # Mutation factor\n        self.Cr = Cr  # Crossover rate\n        self.velocity_weight = velocity_weight  # Weight for velocity-based mutation\n        self.learning_rate_F = 0.1\n        self.learning_rate_Cr = 0.1\n        self.success_history_F = []\n        self.success_history_Cr = []\n        self.success_history_size = 10\n        self.archive = []\n        self.archive_prob = 0.2  # Probability of using an archive vector in mutation\n        self.pop_size_adapt = pop_size_adapt\n        self.pop_size_factor = 1.0 #initial value\n        self.pop_size_learning_rate = 0.1\n\n    def __call__(self, func):\n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size  # Deduct initial population evaluations from budget\n\n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)]\n\n        # Initialize velocity\n        velocity = np.zeros_like(population)\n\n        generation = 0\n        while self.budget > 0:\n            generation += 1\n            # Potentially adapt the population size\n            if self.pop_size_adapt:\n                if len(self.success_history_F) > self.success_history_size // 2: #make sure we have sufficient data\n                    success_rate = len(self.success_history_F) / generation\n                    self.pop_size_factor += self.pop_size_learning_rate * (success_rate - 0.5)\n                    self.pop_size_factor = np.clip(self.pop_size_factor, 0.5, 2.0)\n                    new_pop_size = int(50 * self.pop_size_factor)  # Adjust based on a scaling factor\n                    if new_pop_size != self.pop_size:\n                        self.pop_size = new_pop_size #update if different\n                        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                        fitness = np.array([func(x) for x in population])\n                        velocity = np.zeros_like(population) #reinitialize velocity as well\n                        #Re-evaluate all fitness values if pop size changed! Critical. This ensures consistency\n                        fitness = np.array([func(x) for x in population])\n                        self.budget -= self.pop_size #account for the full re-evaluation\n                        self.f_opt = np.min(fitness) #update best\n                        self.x_opt = population[np.argmin(fitness)]\n\n\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n\n                # Velocity update\n                velocity[i] = self.velocity_weight * velocity[i] + self.F * (b - c)\n\n                # Archive utilization\n                if self.archive and np.random.rand() < self.archive_prob:\n                    arc_idx = np.random.randint(0, len(self.archive))\n                    velocity[i] += self.F * (self.archive[arc_idx] - population[i]) # Incorporate archive\n\n                # Apply mutation with velocity\n                mutant = np.clip(a + velocity[i], func.bounds.lb, func.bounds.ub)\n\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.Cr\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n\n                if f_trial < fitness[i]:\n                    # Archive the replaced individual for diversity\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(population[i].copy())\n                    else:\n                        idx_to_replace = np.random.randint(0, self.archive_size)\n                        self.archive[idx_to_replace] = population[i].copy()\n\n                    fitness[i] = f_trial\n                    population[i] = trial\n\n                    # Update optimal solution\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n                    # Update success history for F and Cr adaptation\n                    self.success_history_F.append(self.F)\n                    self.success_history_Cr.append(self.Cr)\n                    if len(self.success_history_F) > self.success_history_size:\n                        self.success_history_F.pop(0)\n                        self.success_history_Cr.pop(0)\n\n            # Adapt F and Cr based on success history\n            if self.success_history_F:\n                self.F = np.clip(np.mean(self.success_history_F) + self.learning_rate_F * np.random.normal(0, 0.1), 0.1, 1.0)\n            else:\n                self.F = np.clip(self.F + self.learning_rate_F * np.random.normal(0, 0.1), 0.1, 1.0)\n\n            if self.success_history_Cr:\n                self.Cr = np.clip(np.mean(self.success_history_Cr) + self.learning_rate_Cr * np.random.normal(0, 0.1), 0.1, 1.0)\n            else:\n                self.Cr = np.clip(self.Cr + self.learning_rate_Cr * np.random.normal(0, 0.1), 0.1, 1.0)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 11, "feedback": "The algorithm SelfAdaptiveDE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["d15f2124-2044-40ea-b0ea-bbc76d19a3ef"], "operator": null, "metadata": {"aucs": [0]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "a03335c1-0971-4142-a7ef-f64205a3a7f4", "fitness": 0.293164833976576, "name": "AdaptivePopulationDE", "description": "Adaptive Population Resizing Differential Evolution with stochastic ranking and a local search phase triggered by stagnation.", "code": "import numpy as np\n\nclass AdaptivePopulationDE:\n    def __init__(self, budget=10000, dim=10, initial_pop_size=50, min_pop_size=10, max_pop_size=100, F=0.5, Cr=0.9, stagnation_limit=50, local_search_iterations=5):\n        self.budget = budget\n        self.dim = dim\n        self.initial_pop_size = initial_pop_size\n        self.min_pop_size = min_pop_size\n        self.max_pop_size = max_pop_size\n        self.pop_size = initial_pop_size\n        self.F = F\n        self.Cr = Cr\n        self.stagnation_limit = stagnation_limit\n        self.local_search_iterations = local_search_iterations\n        self.stagnation_counter = 0\n        self.learning_rate = 0.1\n        self.best_fitness_history = []\n\n    def __call__(self, func):\n        # Initialize population\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)]\n        self.best_fitness_history.append(self.f_opt)\n\n        generation = 0\n        while self.budget > 0:\n            generation += 1\n\n            # Adaptive Population Resizing\n            if len(self.best_fitness_history) > self.stagnation_limit:\n                if np.std(self.best_fitness_history[-self.stagnation_limit:]) < 1e-6:  # Stagnation detected\n                    self.stagnation_counter += 1\n                    if self.stagnation_counter > self.stagnation_limit:  # Longer stagnation\n                        # Increase population size (Exploration)\n                        self.pop_size = min(self.pop_size + 10, self.max_pop_size)\n                        self.stagnation_counter = 0\n                        # Add new random individuals\n                        new_individuals = np.random.uniform(lb, ub, size=(10, self.dim))\n                        new_fitness = np.array([func(x) for x in new_individuals])\n                        self.budget -= 10\n                        population = np.concatenate((population, new_individuals))\n                        fitness = np.concatenate((fitness, new_fitness))\n\n                        if np.any(new_fitness < self.f_opt):\n                             self.f_opt = np.min(fitness)\n                             self.x_opt = population[np.argmin(fitness)]\n\n                else:\n                    self.stagnation_counter = 0\n            else:\n                self.stagnation_counter = 0\n\n            if self.budget <= 0:\n                break\n\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + self.F * (b - c), lb, ub)\n\n                # Crossover\n                trial = self.crossover(population[i], mutant, self.Cr)\n\n                # Selection using Stochastic Ranking\n                f_trial = func(trial)\n                self.budget -= 1\n\n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = trial\n\n                if (fitness[i] < 0 and f_trial < 0) or (fitness[i] >= 0 and f_trial >= 0):\n                    if f_trial < fitness[i]:\n                        fitness[i] = f_trial\n                        population[i] = trial\n                else:\n                    p_threshold = fitness[i] / (fitness[i] - f_trial) if (fitness[i] - f_trial) != 0 else 0.5\n                    if np.random.rand() < p_threshold:\n                        fitness[i] = f_trial\n                        population[i] = trial\n\n            # Local Search triggered by stagnation\n            if self.stagnation_counter > self.stagnation_limit // 2:\n                best_index = np.argmin(fitness)\n                for _ in range(self.local_search_iterations):\n                    neighbor = np.clip(population[best_index] + np.random.normal(0, 0.01, self.dim), lb, ub)\n                    f_neighbor = func(neighbor)\n                    self.budget -= 1\n                    if f_neighbor < fitness[best_index]:\n                        fitness[best_index] = f_neighbor\n                        population[best_index] = neighbor\n                        if f_neighbor < self.f_opt:\n                            self.f_opt = f_neighbor\n                            self.x_opt = neighbor\n\n\n            self.best_fitness_history.append(self.f_opt)\n\n            # Adaptive F and Cr\n            self.F = np.clip(self.F + self.learning_rate * np.random.normal(0, 0.1), 0.1, 1.0)\n            self.Cr = np.clip(self.Cr + self.learning_rate * np.random.normal(0, 0.1), 0.1, 1.0)\n\n            # Population reduction if needed.  Favor removing solutions with poor fitness\n            if self.pop_size > self.initial_pop_size and generation % 10 == 0:\n                num_to_remove = self.pop_size - self.initial_pop_size\n\n                removal_indices = np.argsort(fitness)[-num_to_remove:] # Remove worst.\n                population = np.delete(population, removal_indices, axis=0)\n                fitness = np.delete(fitness, removal_indices)\n                self.pop_size = population.shape[0]\n\n\n        return self.f_opt, self.x_opt\n\n    def crossover(self, x, mutant, cr):\n        trial = x.copy()\n        for j in range(self.dim):\n            if np.random.rand() < cr:\n                trial[j] = mutant[j]\n        return trial", "configspace": "", "generation": 11, "feedback": "The algorithm AdaptivePopulationDE scored 0.293 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["06b6ae75-0ec5-408d-a4a3-4059dc60b0ac"], "operator": null, "metadata": {"aucs": [0.586329667953152, 0]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "761a5742-33da-4476-8e69-d654f843cae4", "fitness": "-inf", "name": "SelfAdaptiveDE", "description": "A self-adaptive Differential Evolution with dynamically adjusted population size and a success-history based mutation factor adaptation mechanism.", "code": "import numpy as np\n\nclass SelfAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size_init=50, F=0.5, Cr=0.9, archive_size=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size_init = pop_size_init\n        self.pop_size = pop_size_init  # Start with initial population size\n        self.F = F  # Mutation factor\n        self.Cr = Cr  # Crossover rate\n        self.archive = []\n        self.archive_size = archive_size\n        self.memory_F = np.ones(archive_size) * 0.5\n        self.memory_Cr = np.ones(archive_size) * 0.9\n        self.learning_rate = 0.1\n        self.success_F = []\n        self.success_Cr = []\n\n\n    def __call__(self, func):\n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size  # Deduct initial population evaluations from budget\n\n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)]\n\n        generation = 0\n        while self.budget > 0:\n            generation += 1\n\n            # Dynamically adjust population size based on remaining budget\n            self.pop_size = min(self.pop_size_init + generation // 10, self.budget // 2)  # Example dynamic pop size\n\n            for i in range(self.pop_size):\n                # Parameter adaptation\n                memory_idx = np.random.randint(self.archive_size)\n                self.F = self.memory_F[memory_idx]\n                self.Cr = self.memory_Cr[memory_idx]\n\n                # Mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + self.F * (b - c), func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.Cr\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                if f_trial < fitness[i]:\n                    # Store successful F and Cr values\n                    self.success_F.append(self.F)\n                    self.success_Cr.append(self.Cr)\n\n                    delta = fitness[i] - f_trial\n                    fitness[i] = f_trial\n                    population[i] = trial\n\n                    # Update optimal solution\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n\n                # Success history based parameter adaptation\n                if self.success_F:\n                    self.memory_F[memory_idx] = np.mean(self.success_F)\n                    self.memory_Cr[memory_idx] = np.mean(self.success_Cr)\n\n                self.success_F = [] # Reset success history at the end of each generation\n                self.success_Cr = []\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 11, "feedback": "In the code, line 1025, in numpy.random.mtrand.RandomState.choice, the following error occurred:\nValueError: Cannot take a larger sample than population when 'replace=False'", "error": "In the code, line 1025, in numpy.random.mtrand.RandomState.choice, the following error occurred:\nValueError: Cannot take a larger sample than population when 'replace=False'", "parent_ids": ["94768647-f252-46d5-98c5-9ea8189ba67b"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "1a1ad7e7-16c3-4f47-8d34-bb3539894f20", "fitness": "-inf", "name": "SelfAdaptiveDEDualPopLocalSearch", "description": "Self-adaptive Differential Evolution with a dual-population structure, dynamically adjusting mutation strategies, and incorporating a local search component based on gradient estimation to enhance exploitation.", "code": "import numpy as np\n\nclass SelfAdaptiveDEDualPopLocalSearch:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=10, F=0.5, Cr=0.9,\n                 velocity_weight=0.1, local_search_prob=0.1, local_search_radius=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.F = F  # Mutation factor\n        self.Cr = Cr  # Crossover rate\n        self.velocity_weight = velocity_weight  # Weight for velocity-based mutation\n        self.learning_rate_F = 0.1\n        self.learning_rate_Cr = 0.1\n        self.success_history_F = []\n        self.success_history_Cr = []\n        self.success_history_size = 10\n        self.archive = []\n        self.local_search_prob = local_search_prob\n        self.local_search_radius = local_search_radius\n        self.explorer_ratio = 0.5  # Ratio of explorers in the population\n\n    def __call__(self, func):\n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size  # Deduct initial population evaluations from budget\n\n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)]\n\n        # Initialize velocity\n        velocity = np.zeros_like(population)\n\n        # Split population into explorers and exploiters\n        num_explorers = int(self.pop_size * self.explorer_ratio)\n        explorers = population[:num_explorers]\n        exploiters = population[num_explorers:]\n        fitness_explorers = fitness[:num_explorers]\n        fitness_exploiters = fitness[num_explorers:]\n\n        generation = 0\n        while self.budget > 0:\n            generation += 1\n\n            # Explorer update\n            for i in range(num_explorers):\n                # Mutation (DE/rand/1) - Encourages exploration\n                idxs = np.random.choice(num_explorers, 5, replace=False)\n                a, b, c, d, e = explorers[idxs]  # Select 5 distinct explorers\n                mutant = np.clip(a + self.F * (b - c) + self.F * (d-e), func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.Cr\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, explorers[i])\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n\n                if f_trial < fitness_explorers[i]:\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(explorers[i].copy())\n                    else:\n                        idx_to_replace = np.random.randint(0, self.archive_size)\n                        self.archive[idx_to_replace] = explorers[i].copy()\n                    fitness_explorers[i] = f_trial\n                    explorers[i] = trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n                    self.success_history_F.append(self.F)\n                    self.success_history_Cr.append(self.Cr)\n                    if len(self.success_history_F) > self.success_history_size:\n                        self.success_history_F.pop(0)\n                        self.success_history_Cr.pop(0)\n            \n            # Exploiter update with local search\n            for i in range(self.pop_size - num_explorers):\n                # Mutation (DE/best/1) - Encourages exploitation\n                best_index = np.argmin(fitness_exploiters) # index of best among exploiters\n                idxs = [idx for idx in range(self.pop_size - num_explorers) if idx != i]\n                b, c = exploiters[np.random.choice(idxs, 2, replace=False)]\n                mutant = np.clip(exploiters[best_index] + self.F * (b - c), func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.Cr\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, exploiters[i])\n\n                # Local search with probability\n                if np.random.rand() < self.local_search_prob:\n                    # Estimate gradient (simple finite difference)\n                    gradient = np.zeros(self.dim)\n                    delta = self.local_search_radius\n                    for d in range(self.dim):\n                        x_plus = trial.copy()\n                        x_minus = trial.copy()\n                        x_plus[d] += delta\n                        x_minus[d] -= delta\n                        x_plus = np.clip(x_plus, func.bounds.lb, func.bounds.ub)\n                        x_minus = np.clip(x_minus, func.bounds.lb, func.bounds.ub)\n                        \n                        f_plus = func(x_plus) if self.budget > 0 else fitness_exploiters[i]\n                        self.budget -= 1 if self.budget > 0 else 0\n                        f_minus = func(x_minus) if self.budget > 0 else fitness_exploiters[i]\n                        self.budget -= 1 if self.budget > 0 else 0\n\n                        gradient[d] = (f_plus - f_minus) / (2 * delta)\n                    \n                    # Move against the gradient\n                    trial = np.clip(trial - self.local_search_radius * gradient, func.bounds.lb, func.bounds.ub)\n\n                # Selection\n                f_trial = func(trial) if self.budget > 0 else fitness_exploiters[i]\n                self.budget -= 1 if self.budget > 0 else 0\n\n                if f_trial < fitness_exploiters[i]:\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(exploiters[i].copy())\n                    else:\n                        idx_to_replace = np.random.randint(0, self.archive_size)\n                        self.archive[idx_to_replace] = exploiters[i].copy()\n                    fitness_exploiters[i] = f_trial\n                    exploiters[i] = trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n                    self.success_history_F.append(self.F)\n                    self.success_history_Cr.append(self.Cr)\n                    if len(self.success_history_F) > self.success_history_size:\n                        self.success_history_F.pop(0)\n                        self.success_history_Cr.pop(0)\n\n            # Adapt F and Cr based on success history\n            if self.success_history_F:\n                self.F = np.clip(np.mean(self.success_history_F) + self.learning_rate_F * np.random.normal(0, 0.1), 0.1, 1.0)\n            else:\n                self.F = np.clip(self.F + self.learning_rate_F * np.random.normal(0, 0.1), 0.1, 1.0)\n\n            if self.success_history_Cr:\n                self.Cr = np.clip(np.mean(self.success_history_Cr) + self.learning_rate_Cr * np.random.normal(0, 0.1), 0.1, 1.0)\n            else:\n                self.Cr = np.clip(self.Cr + self.learning_rate_Cr * np.random.normal(0, 0.1), 0.1, 1.0)\n\n            # Combine populations\n            population = np.concatenate((explorers, exploiters), axis=0)\n            fitness = np.concatenate((fitness_explorers, fitness_exploiters))\n            \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 11, "feedback": "Evaluation timed out after 300 seconds.", "error": "Evaluation timed out after 300 seconds.", "parent_ids": ["d15f2124-2044-40ea-b0ea-bbc76d19a3ef"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "65784b54-994f-409c-9873-898faa67d3a8", "fitness": "-inf", "name": "DynamicPopulationDE", "description": "A Differential Evolution variant with a dynamically adjusted population size, orthogonal learning, and a restart mechanism based on population diversity to balance exploration and exploitation.", "code": "import numpy as np\n\nclass DynamicPopulationDE:\n    def __init__(self, budget=10000, dim=10, initial_pop_size=50, min_pop_size=10, max_pop_size=100, F=0.5, Cr=0.9, diversity_threshold=0.1, restart_frequency=50):\n        self.budget = budget\n        self.dim = dim\n        self.initial_pop_size = initial_pop_size\n        self.min_pop_size = min_pop_size\n        self.max_pop_size = max_pop_size\n        self.pop_size = initial_pop_size\n        self.F = F\n        self.Cr = Cr\n        self.diversity_threshold = diversity_threshold\n        self.restart_frequency = restart_frequency\n        self.generation = 0\n        self.lb = None\n        self.ub = None\n\n    def __call__(self, func):\n        self.lb, self.ub = func.bounds.lb, func.bounds.ub\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n\n        while self.budget > 0:\n            self.generation += 1\n            self.evolve(func)\n            self.adjust_population_size()\n\n            if self.generation % self.restart_frequency == 0:\n                if self.population_diversity() < self.diversity_threshold:\n                    self.restart_population(func)\n\n        return self.f_opt, self.x_opt\n\n    def evolve(self, func):\n        for i in range(self.pop_size):\n            if self.budget <= 0:\n                break\n\n            # Mutation\n            idxs = [idx for idx in range(self.pop_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            mutant = np.clip(a + self.F * (b - c), self.lb, self.ub)\n\n            # Orthogonal Crossover with one-point crossover\n            trial = self.orthogonal_crossover(self.population[i], mutant, self.Cr)\n\n            # Selection\n            f_trial = func(trial)\n            self.budget -= 1\n\n            if f_trial < self.fitness[i]:\n                self.fitness[i] = f_trial\n                self.population[i] = trial\n\n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = trial\n\n    def orthogonal_crossover(self, x, mutant, cr):\n        trial = x.copy()\n        crossover_point = np.random.randint(self.dim) # One-point crossover\n        for j in range(self.dim):\n            if np.random.rand() < cr or j == crossover_point: #Ensure at least one gene is exchanged\n                trial[j] = mutant[j]\n        return trial\n\n    def adjust_population_size(self):\n        # Dynamically adjust population size based on performance\n        if np.random.rand() < 0.1:  # Adjust population size probabilistically\n            if np.std(self.fitness) > 0.01:  # If fitness variance is high, increase population\n                self.pop_size = min(self.pop_size + 5, self.max_pop_size)\n            else:  # If fitness is converging, decrease population\n                self.pop_size = max(self.pop_size - 5, self.min_pop_size)\n\n            if self.pop_size != len(self.population):\n                self.resize_population()\n\n    def resize_population(self):\n        # Resize population array\n        new_population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        new_fitness = np.array([np.inf] * self.pop_size)\n\n        # Copy over the best individuals\n        num_to_copy = min(self.pop_size, len(self.population))\n        best_indices = np.argsort(self.fitness)[:num_to_copy]\n        new_population[:num_to_copy] = self.population[best_indices]\n        new_fitness[:num_to_copy] = self.fitness[best_indices]\n        self.population = new_population\n        self.fitness = new_fitness\n\n    def population_diversity(self):\n        # Calculate population diversity based on the average distance between individuals\n        distances = []\n        for i in range(self.pop_size):\n            for j in range(i + 1, self.pop_size):\n                distances.append(np.linalg.norm(self.population[i] - self.population[j]))\n        if distances:\n            return np.mean(distances) / (self.ub - self.lb) # Normalized diversity\n        else:\n            return 0.0\n\n    def restart_population(self, func):\n        # Restart the population with new random individuals\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]", "configspace": "", "generation": 12, "feedback": "In the code, line 34, in __call__, the following error occurred:\nValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\nOn line: if self.population_diversity() < self.diversity_threshold:", "error": "In the code, line 34, in __call__, the following error occurred:\nValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\nOn line: if self.population_diversity() < self.diversity_threshold:", "parent_ids": ["06b6ae75-0ec5-408d-a4a3-4059dc60b0ac"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "17c45eed-b097-4999-b128-d4f7ec5735f0", "fitness": "-inf", "name": "EnsembleAdaptiveDE", "description": "Differential Evolution with Ensemble of Mutation Strategies and Adaptive Population Size, adjusting the population size based on the success rate of the current generation to balance exploration and exploitation.", "code": "import numpy as np\n\nclass EnsembleAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size_init=50, F=0.5, Cr=0.9, pop_size_adapt_freq=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_init = pop_size_init\n        self.F = F  # Mutation factor\n        self.Cr = Cr  # Crossover rate\n        self.pop_size_adapt_freq = pop_size_adapt_freq\n        self.mutation_strategies = [\n            lambda p, a, b, c, F: p + F * (b - c),  # DE/rand/1\n            lambda p, a, b, c, F: a + F * (b - c),  # DE/best/1\n            lambda p, a, b, c, F, best: p + F * (b - c) + F * (best - p), #DE/current-to-best/1\n        ]\n        self.success_rates = [] # to store success rates of generations\n\n    def __call__(self, func):\n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)]\n\n        generation = 0\n        successes = 0  # Track successful mutations in current generation\n        while self.budget > 0:\n            generation += 1\n            \n            new_population = np.copy(population)\n            new_fitness = np.copy(fitness)\n\n            for i in range(self.pop_size):\n                # Mutation - Ensemble of strategies\n                strategy = np.random.choice(self.mutation_strategies)\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                \n                if strategy.__name__ == '<lambda_2>':  #current-to-best mutation\n                    best_idx = np.argmin(fitness)\n                    mutant = np.clip(strategy(population[i], a, b, c, self.F, population[best_idx]), func.bounds.lb, func.bounds.ub)\n                else:\n                    mutant = np.clip(strategy(population[i], a, b, c, self.F), func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.Cr\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                if f_trial < fitness[i]:\n                    successes += 1\n                    new_fitness[i] = f_trial\n                    new_population[i] = trial\n\n                    # Update optimal solution\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n            \n            population = new_population\n            fitness = new_fitness\n\n            # Population size adaptation\n            if generation % self.pop_size_adapt_freq == 0:\n                success_rate = successes / self.pop_size\n                self.success_rates.append(success_rate)\n                successes = 0  # Reset success counter\n                \n                if len(self.success_rates) > 5:\n                    # adjust pop size based on the average of the last 5 success rates\n                    avg_success_rate = np.mean(self.success_rates[-5:])\n                    \n                    if avg_success_rate > 0.4:  # High success rate, reduce population to exploit\n                        self.pop_size = max(10, int(self.pop_size * 0.9))\n                        \n                    elif avg_success_rate < 0.1:  # Low success rate, increase population to explore\n                        self.pop_size = min(200, int(self.pop_size * 1.1))\n                \n                # resize population\n                old_pop_size = population.shape[0]\n                if old_pop_size != self.pop_size:\n                    if self.pop_size > old_pop_size:\n                        # Add individuals (random initialization)\n                        new_individuals = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size - old_pop_size, self.dim))\n                        population = np.vstack((population, new_individuals))\n                        fitness = np.concatenate((fitness, np.array([func(x) for x in new_individuals])))\n                        self.budget -= (self.pop_size - old_pop_size)\n                        \n                    elif self.pop_size < old_pop_size:\n                        # Remove the worst individuals\n                        indices_to_keep = np.argsort(fitness)[:self.pop_size]  # Keep the best individuals\n                        population = population[indices_to_keep]\n                        fitness = fitness[indices_to_keep]\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 12, "feedback": "In the code, line 45, in __call__, the following error occurred:\nTypeError: EnsembleAdaptiveDE.__init__.<locals>.<lambda>() missing 1 required positional argument: 'best'\nOn line: mutant = np.clip(strategy(population[i], a, b, c, self.F), func.bounds.lb, func.bounds.ub)", "error": "In the code, line 45, in __call__, the following error occurred:\nTypeError: EnsembleAdaptiveDE.__init__.<locals>.<lambda>() missing 1 required positional argument: 'best'\nOn line: mutant = np.clip(strategy(population[i], a, b, c, self.F), func.bounds.lb, func.bounds.ub)", "parent_ids": ["94768647-f252-46d5-98c5-9ea8189ba67b"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "65e3b37f-d755-48de-94fa-6af014aa8d60", "fitness": "-inf", "name": "SelfAdaptiveDE", "description": "Self-adaptive Differential Evolution with a dual archive (success and diversity), adaptive exploration rate, and orthogonal crossover for enhanced search space coverage.", "code": "import numpy as np\n\nclass SelfAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=10, F=0.5, Cr=0.9, velocity_weight=0.1, exploration_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.F = F  # Mutation factor\n        self.Cr = Cr  # Crossover rate\n        self.velocity_weight = velocity_weight  # Weight for velocity-based mutation\n        self.exploration_rate = exploration_rate # Probability of exploration\n        self.learning_rate_F = 0.1\n        self.learning_rate_Cr = 0.1\n        self.success_history_F = []\n        self.success_history_Cr = []\n        self.success_history_size = 10\n        self.success_archive = [] # Archive for successful solutions\n        self.diversity_archive = [] # Archive for diverse solutions\n\n    def __call__(self, func):\n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size  # Deduct initial population evaluations from budget\n\n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)]\n\n        # Initialize velocity\n        velocity = np.zeros_like(population)\n\n        generation = 0\n        while self.budget > 0:\n            generation += 1\n            for i in range(self.pop_size):\n                # Exploration with probability exploration_rate\n                if np.random.rand() < self.exploration_rate:\n                    trial = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.dim))\n                else:\n                    # Mutation\n                    idxs = [idx for idx in range(self.pop_size) if idx != i]\n                    a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n\n                    # Velocity update\n                    velocity[i] = self.velocity_weight * velocity[i] + self.F * (b - c)\n\n                    # Apply mutation with velocity\n                    mutant = np.clip(a + velocity[i], func.bounds.lb, func.bounds.ub)\n\n                    # Orthogonal Crossover\n                    trial = self.orthogonal_crossover(population[i], mutant)\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n\n                if f_trial < fitness[i]:\n                    # Archive the replaced individual\n                    if len(self.success_archive) < self.archive_size:\n                        self.success_archive.append((population[i].copy(), fitness[i]))\n                    else:\n                        # Replace the worst solution in the success archive\n                        worst_idx = np.argmax([item[1] for item in self.success_archive])\n                        if fitness[i] < self.success_archive[worst_idx][1]:\n                            self.success_archive[worst_idx] = (population[i].copy(), fitness[i])\n\n                    fitness[i] = f_trial\n                    population[i] = trial\n\n                    # Update optimal solution\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n                    # Update success history for F and Cr adaptation\n                    self.success_history_F.append(self.F)\n                    self.success_history_Cr.append(self.Cr)\n                    if len(self.success_history_F) > self.success_history_size:\n                        self.success_history_F.pop(0)\n                        self.success_history_Cr.pop(0)\n                else:\n                    # Add to diversity archive if not improving (helps exploration)\n                    if len(self.diversity_archive) < self.archive_size:\n                        self.diversity_archive.append((trial.copy(), f_trial))\n                    else:\n                        # Replace a random element in the archive\n                        idx_to_replace = np.random.randint(0, self.archive_size)\n                        self.diversity_archive[idx_to_replace] = (trial.copy(), f_trial)\n\n            # Adapt F and Cr based on success history\n            if self.success_history_F:\n                self.F = np.clip(np.mean(self.success_history_F) + self.learning_rate_F * np.random.normal(0, 0.1), 0.1, 1.0)\n            else:\n                self.F = np.clip(self.F + self.learning_rate_F * np.random.normal(0, 0.1), 0.1, 1.0)\n\n            if self.success_history_Cr:\n                self.Cr = np.clip(np.mean(self.success_history_Cr) + self.learning_rate_Cr * np.random.normal(0, 0.1), 0.1, 1.0)\n            else:\n                self.Cr = np.clip(self.Cr + self.learning_rate_Cr * np.random.normal(0, 0.1), 0.1, 1.0)\n\n            # Adapt exploration rate\n            if generation % 10 == 0:\n               self.exploration_rate = np.clip(self.exploration_rate + np.random.normal(0, 0.02), 0.05, 0.5)\n\n        return self.f_opt, self.x_opt\n\n    def orthogonal_crossover(self, x, mutant):\n        # Orthogonal design crossover\n        levels = 2\n        design = np.array([[0, 1], [1, 0]])  # Example orthogonal array for 2 factors, 2 levels\n\n        trial = x.copy()\n        for j in range(self.dim // 2): # Iterate pairs of dimensions\n            # Select two dimensions for orthogonal crossover\n            d1 = 2 * j\n            d2 = 2 * j + 1\n\n            # Create two new candidate solutions\n            candidate1 = x.copy()\n            candidate1[d1] = x[d1] if design[0, 0] == 0 else mutant[d1]\n            candidate1[d2] = x[d2] if design[0, 1] == 0 else mutant[d2]\n\n            candidate2 = x.copy()\n            candidate2[d1] = x[d1] if design[1, 0] == 0 else mutant[d1]\n            candidate2[d2] = x[d2] if design[1, 1] == 0 else mutant[d2]\n\n            # Choose the best candidate\n            f_candidate1 = np.inf # Placeholder, since evaluation is performed outside\n            f_candidate2 = np.inf # Placeholder\n            \n            # In this version, we just return the trial solution directly instead of evaluation.\n            # This prevents going over budget for evaluation.\n            trial[d1] = candidate1[d1] # Just return one candidate\n            trial[d2] = candidate1[d2]\n\n            # The performance is not determined here.\n            \n        return trial", "configspace": "", "generation": 12, "feedback": "Evaluation timed out after 300 seconds.", "error": "Evaluation timed out after 300 seconds.", "parent_ids": ["d15f2124-2044-40ea-b0ea-bbc76d19a3ef"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "8169ad26-78a2-4c98-98a3-84dee26e292f", "fitness": "-inf", "name": "AdaptiveDELocalSearch", "description": "Differential Evolution with Local Search, adaptive restart, and orthogonal learning to balance exploration and exploitation.", "code": "import numpy as np\n\nclass AdaptiveDELocalSearch:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, Cr=0.9, local_search_probability=0.1, ls_radius=0.1, restart_trigger=0.05, orthogonal_components=5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.Cr = Cr\n        self.local_search_probability = local_search_probability\n        self.ls_radius = ls_radius\n        self.restart_trigger = restart_trigger # Percentage of budget to trigger a restart\n        self.orthogonal_components = orthogonal_components\n        self.best_fitness_history = []\n\n    def __call__(self, func):\n        # Initialize population\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)]\n        self.best_fitness_history.append(self.f_opt)\n\n        generation = 0\n        initial_budget = self.budget \n\n        while self.budget > 0:\n            generation += 1\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + self.F * (b - c), lb, ub)\n\n                # Crossover\n                trial = self.crossover(population[i], mutant, self.Cr)\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    population[i] = trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                        self.best_fitness_history.append(self.f_opt)\n                else:\n                    self.best_fitness_history.append(self.f_opt) # track even when no improvement\n            \n            # Local search: with a small probability, improve around the best solution\n            if np.random.rand() < self.local_search_probability and self.budget > self.dim:\n                best_index = np.argmin(fitness)\n                x_current = population[best_index].copy()\n                for j in range(self.dim):\n                    delta = np.random.uniform(-self.ls_radius, self.ls_radius)\n                    x_new = np.clip(x_current.copy(), lb, ub)\n                    x_new[j] += delta\n                    f_new = func(x_new)\n                    self.budget -=1\n                    if f_new < fitness[best_index]:\n                        fitness[best_index] = f_new\n                        population[best_index] = x_new\n                        if f_new < self.f_opt:\n                            self.f_opt = f_new\n                            self.x_opt = x_new\n                            self.best_fitness_history.append(self.f_opt)\n\n            # Orthogonal Learning: Explore the space around the best\n            if self.budget > self.dim * self.orthogonal_components:\n                best_index = np.argmin(fitness)\n                center = population[best_index]\n                design = self.orthogonal_design(center, self.orthogonal_components, lb, ub)\n                for x in design:\n                    f_x = func(x)\n                    self.budget -= 1\n                    if f_x < self.f_opt:\n                        self.f_opt = f_x\n                        self.x_opt = x\n                        fitness[best_index] = f_x\n                        population[best_index] = x\n                        self.best_fitness_history.append(self.f_opt)\n\n            # Adaptive Restart: If no significant improvement, restart the population\n            if len(self.best_fitness_history) > 50:\n                improvement = self.best_fitness_history[-50] - self.f_opt\n                if improvement / abs(self.best_fitness_history[-50]) < 1e-6 and self.budget < (1-self.restart_trigger) * initial_budget: #No significant improvement recently\n                    population = np.random.uniform(lb, ub, size=(self.pop_size, self.dim))\n                    fitness = np.array([func(x) for x in population])\n                    self.budget -= self.pop_size\n                    best_index = np.argmin(fitness)\n                    if fitness[best_index] < self.f_opt:\n                        self.f_opt = fitness[best_index]\n                        self.x_opt = population[best_index]\n                    self.best_fitness_history.append(self.f_opt)\n\n        return self.f_opt, self.x_opt\n\n    def crossover(self, x, mutant, cr):\n        trial = x.copy()\n        for j in range(self.dim):\n            if np.random.rand() < cr:\n                trial[j] = mutant[j]\n        return trial\n    \n    def orthogonal_design(self, center, num_points, lb, ub):\n        design = []\n        for i in range(num_points):\n            point = center.copy()\n            for j in range(self.dim):\n                # Sample around the center in each dimension\n                point[j] = center[j] + np.random.uniform(-0.5, 0.5) * (ub - lb)\n                point[j] = np.clip(point[j], lb, ub)\n            design.append(point)\n        return design", "configspace": "", "generation": 12, "feedback": "In the code, line 117, in orthogonal_design, the following error occurred:\nValueError: setting an array element with a sequence.\nOn line: point[j] = center[j] + np.random.uniform(-0.5, 0.5) * (ub - lb)", "error": "In the code, line 117, in orthogonal_design, the following error occurred:\nValueError: setting an array element with a sequence.\nOn line: point[j] = center[j] + np.random.uniform(-0.5, 0.5) * (ub - lb)", "parent_ids": ["06b6ae75-0ec5-408d-a4a3-4059dc60b0ac"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "7ff9c1e4-8732-4d05-b382-295573d3d006", "fitness": 0.5567311514545169, "name": "EnhancedSelfAdaptiveDE", "description": "Enhanced Self-Adaptive Differential Evolution with simplified parameter adaptation, combined mutation strategies, and a dynamically updated archive.", "code": "import numpy as np\n\nclass EnhancedSelfAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=10, F=0.5, Cr=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.F = F\n        self.Cr = Cr\n        self.archive = []\n        self.learning_rate = 0.1\n        self.success_F = []\n        self.success_Cr = []\n        self.success_size = 5\n\n    def __call__(self, func):\n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        best_index = np.argmin(fitness)\n        self.best_fitness = fitness[best_index]\n        self.best_solution = population[best_index].copy()\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation Strategy: DE/rand/1 and DE/current-to-rand/1 with probability 0.5 each\n                if np.random.rand() < 0.5:\n                    # DE/rand/1\n                    idxs = np.random.choice(self.pop_size, 5, replace=False)\n                    a, b, c, d, e = population[idxs]\n                    mutant = a + self.F * (b - c) + self.F * (d - e)\n                else:\n                    # DE/current-to-rand/1\n                    idxs = np.random.choice(self.pop_size, 3, replace=False)\n                    a, b, c = population[idxs]\n                    mutant = population[i] + self.F * (a - population[i]) + self.F * (b - c)\n\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.Cr\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n\n                if f_trial < fitness[i]:\n                    # Update Archive\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(population[i].copy())\n                    else:\n                        idx_to_replace = np.random.randint(0, self.archive_size)\n                        self.archive[idx_to_replace] = population[i].copy()\n\n                    # Update population and fitness\n                    fitness[i] = f_trial\n                    population[i] = trial\n\n                    # Update best solution\n                    if f_trial < self.best_fitness:\n                        self.best_fitness = f_trial\n                        self.best_solution = trial.copy()\n                    \n                    # Store successful F and Cr\n                    self.success_F.append(self.F)\n                    self.success_Cr.append(self.Cr)\n                    if len(self.success_F) > self.success_size:\n                        self.success_F.pop(0)\n                        self.success_Cr.pop(0)\n\n            # Adapt F and Cr\n            if self.success_F:\n                self.F = np.clip(np.mean(self.success_F) * (1 + self.learning_rate * np.random.normal(0, 1)), 0.1, 1.0)\n            else:\n                self.F = np.clip(self.F * (1 + self.learning_rate * np.random.normal(0, 1)), 0.1, 1.0)\n            \n            if self.success_Cr:\n                self.Cr = np.clip(np.mean(self.success_Cr) * (1 + self.learning_rate * np.random.normal(0, 1)), 0.1, 1.0)\n            else:\n                self.Cr = np.clip(self.Cr * (1 + self.learning_rate * np.random.normal(0, 1)), 0.1, 1.0)\n\n\n        return self.best_fitness, self.best_solution", "configspace": "", "generation": 12, "feedback": "The algorithm EnhancedSelfAdaptiveDE scored 0.557 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["d15f2124-2044-40ea-b0ea-bbc76d19a3ef"], "operator": null, "metadata": {"aucs": [0.21679000074099453, 0.30384219579898286, 0.45787527852468957, 0.7918786958128718, 0.5055966015712712, 0.7372015208994878, 0.6764073130570628, 0.4725236006181921, 0.6564157459072679, 0.2478842446229519, 0.7562503815841183, 0.9975199691777843, 0.4408941830559838, 0.7526687819131812, 0.7805630348614746, 0.3312283718547333, 0.5442183149816857, 0.7598453494500607, 0.20799502368067035, 0.4970244209768725]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "24ff2ddb-cfd6-4bab-b72c-d09323bfaa93", "fitness": 0.5367941264403582, "name": "CooperativeAdaptiveDE", "description": "Cooperative Differential Evolution with dual populations (explorers and exploiters), adaptive exploration range, and a combined archive/elite strategy for enhanced information sharing.", "code": "import numpy as np\n\nclass CooperativeAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, explorer_ratio=0.5, archive_size=10, F=0.5, Cr=0.9, lr_F=0.1, lr_Cr=0.1, explore_range=1.0):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.explorer_ratio = explorer_ratio  # Ratio of population dedicated to exploration\n        self.archive_size = archive_size\n        self.F = F\n        self.Cr = Cr\n        self.lr_F = lr_F\n        self.lr_Cr = lr_Cr\n        self.archive = []\n        self.archive_fitness = []\n        self.explorer_size = int(self.pop_size * self.explorer_ratio)\n        self.exploiter_size = self.pop_size - self.explorer_size\n        self.explore_range = explore_range  # Initial exploration range, adaptively changed\n        self.elite_size = max(1, int(0.1 * self.pop_size))  # Keep top 10% as elites\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)]\n        elites_indices = np.argsort(fitness)[:self.elite_size]\n        elites = population[elites_indices].copy()\n        elites_fitness = fitness[elites_indices].copy()\n\n        while self.budget > 0:\n            # Split population into explorers and exploiters\n            explorer_indices = np.argsort(fitness)[-self.explorer_size:]  # Worst individuals become explorers\n            exploiter_indices = np.argsort(fitness)[:self.exploiter_size] # Best individuals become exploiters\n            explorers = population[explorer_indices]\n            exploiters = population[exploiter_indices]\n            explorer_fitness = fitness[explorer_indices]\n            exploiter_fitness = fitness[exploiter_indices]\n\n            # Explorer update\n            for i in range(self.explorer_size):\n                idxs = [idx for idx in range(self.pop_size) if idx != explorer_indices[i]]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = explorers[i] + self.F * (b - c) + np.random.uniform(-self.explore_range, self.explore_range, size=self.dim) # Adaptive range\n                mutant = np.clip(mutant, lb, ub)\n                trial = self.crossover(explorers[i], mutant, self.Cr)\n                f_trial = func(trial)\n                self.budget -= 1\n                if f_trial < explorer_fitness[i]:\n                    explorer_fitness[i] = f_trial\n                    explorers[i] = trial\n                    population[explorer_indices[i]] = trial\n                    fitness[explorer_indices[i]] = f_trial\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                    self.update_archive(trial, f_trial)\n\n            # Exploiter update\n            for i in range(self.exploiter_size):\n                idxs = [idx for idx in range(self.pop_size) if idx != exploiter_indices[i]]\n\n                # Choose a reference point: either archive or an elite member\n                if len(self.archive) > 0 and np.random.rand() < 0.7:  # 70% chance to use archive\n                    archive_idx = np.random.randint(len(self.archive))\n                    a = self.archive[archive_idx]\n                elif len(elites) > 0:\n                    elite_idx = np.random.randint(len(elites))\n                    a = elites[elite_idx]\n                else:\n                    a = population[np.random.choice(idxs, 1)[0]]  # Fallback if both are empty\n\n                b, c = population[np.random.choice(idxs, 2, replace=False)]\n                mutant = np.clip(a + self.F * (b - c), lb, ub)\n                trial = self.crossover(exploiters[i], mutant, self.Cr)\n                f_trial = func(trial)\n                self.budget -= 1\n\n                if f_trial < exploiter_fitness[i]:\n                    exploiter_fitness[i] = f_trial\n                    exploiters[i] = trial\n                    population[exploiter_indices[i]] = trial\n                    fitness[exploiter_indices[i]] = f_trial\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                    self.update_archive(trial, f_trial)\n\n            # Adaptive F and Cr\n            self.F = np.clip(self.F + self.lr_F * np.random.normal(0, 0.1), 0.1, 1.0)\n            self.Cr = np.clip(self.Cr + self.lr_Cr * np.random.normal(0, 0.1), 0.1, 1.0)\n\n            # Adaptive Explore Range\n            if np.random.rand() < 0.1:\n                if np.std(fitness) < 0.01:\n                    self.explore_range *= 1.2  # Increase exploration\n                else:\n                    self.explore_range *= 0.8  # Decrease exploration\n\n            self.explore_range = np.clip(self.explore_range, 0.01, 2.0)\n\n            # Update Elites\n            combined_population = np.concatenate([population, elites], axis=0)\n            combined_fitness = np.concatenate([fitness, elites_fitness])\n            elites_indices = np.argsort(combined_fitness)[:self.elite_size]\n            elites = combined_population[elites_indices].copy()\n            elites_fitness = combined_fitness[elites_indices].copy()\n\n\n        return self.f_opt, self.x_opt\n\n    def crossover(self, x, mutant, cr):\n        trial = x.copy()\n        for j in range(self.dim):\n            if np.random.rand() < cr:\n                trial[j] = mutant[j]\n        return trial\n\n    def update_archive(self, x, fitness):\n        if len(self.archive) < self.archive_size:\n            self.archive.append(x.copy())\n            self.archive_fitness.append(fitness)\n        else:\n            max_archive_index = np.argmax(self.archive_fitness)\n            if fitness < self.archive_fitness[max_archive_index]:\n                self.archive[max_archive_index] = x.copy()\n                self.archive_fitness[max_archive_index] = fitness", "configspace": "", "generation": 12, "feedback": "The algorithm CooperativeAdaptiveDE scored 0.537 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["baf81fef-da08-4bf1-a2b9-0d32ed61131d"], "operator": null, "metadata": {"aucs": [0.19659151946997577, 0.32533421779201877, 0.5536116000173745, 0.7818316962043246, 0.5439563467651213, 0.5345267108439653, 0.3638842077241238, 0.45815114222804776, 0.593115034700042, 0.5814474299396859, 0.6848210211484381, 0.9885072082699147, 0.4569748128512586, 0.40870349317384846, 0.8369689308563578, 0.5712917622564283, 0.4679869480726714, 0.6201258491827559, 0.2534714381618437, 0.5145811591489666]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "3ded8022-9d10-46d9-9e87-278d5bda8bf4", "fitness": 0.0, "name": "EnsembleAdaptiveDE", "description": "A multi-strategy ensemble DE with dynamic population sizing, orthogonal learning, and a success-history-based parameter adaptation mechanism.", "code": "import numpy as np\n\nclass EnsembleAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, initial_pop_size=50, min_pop_size=10, archive_size=10, F=0.5, Cr=0.9, lr=0.1, strategy_probs=[0.3, 0.3, 0.4]):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = initial_pop_size\n        self.min_pop_size = min_pop_size\n        self.archive_size = archive_size\n        self.F = F\n        self.Cr = Cr\n        self.lr = lr\n        self.archive = []\n        self.archive_fitness = []\n        self.strategy_probs = strategy_probs  # Probabilities for each mutation strategy\n        self.success_F = []\n        self.success_Cr = []\n        self.memory_size = 10\n        self.strategy_selection_probs = np.ones(len(strategy_probs)) / len(strategy_probs) #Initialize selection probs\n        self.mutation_strategies = [self.mutation_strategy_1, self.mutation_strategy_2, self.mutation_3]\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)]\n\n        while self.budget > 0:\n            # Dynamic population sizing: reduce population if stagnation is detected\n            if np.std(fitness) < 1e-6 and self.pop_size > self.min_pop_size:\n                self.pop_size = max(self.min_pop_size, int(self.pop_size * 0.9))\n                population = population[:self.pop_size]\n                fitness = fitness[:self.pop_size]\n                print(f\"Reducing population size to {self.pop_size}\")\n\n            for i in range(self.pop_size):\n                # Strategy selection\n                strategy_index = np.random.choice(len(self.mutation_strategies), p=self.strategy_selection_probs)\n                mutation_strategy = self.mutation_strategies[strategy_index]\n                \n                # Mutation using selected strategy\n                mutant = mutation_strategy(population, i, lb, ub)\n                \n                # Orthogonal Crossover\n                trial = self.orthogonal_crossover(population[i], mutant, self.Cr)\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n\n                if f_trial < fitness[i]:\n                    # Success\n                    delta_f = abs(fitness[i] - f_trial)\n                    fitness[i] = f_trial\n                    population[i] = trial\n\n                    # Update optimal solution\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n                    # Update archive\n                    self.update_archive(population[i], fitness[i])\n                    \n                    # Memory update for successful parameters\n                    self.success_F.append(self.F)\n                    self.success_Cr.append(self.Cr)\n                    if len(self.success_F) > self.memory_size:\n                        self.success_F.pop(0)\n                        self.success_Cr.pop(0)\n\n                    # Increase probability for the selected strategy\n                    self.strategy_selection_probs[strategy_index] *= 1.1\n                else:\n                    # Decrease probability for the selected strategy\n                    self.strategy_selection_probs[strategy_index] *= 0.9\n                \n                # Normalize strategy selection probabilities\n                self.strategy_selection_probs /= np.sum(self.strategy_selection_probs)\n\n            # Adapt parameters using success history\n            if self.success_F:\n                self.F = np.mean(self.success_F)\n                self.Cr = np.mean(self.success_Cr)\n            \n            # Random parameter adaptation for exploration\n            self.F = np.clip(self.F + self.lr * np.random.normal(0, 0.1), 0.1, 1.0)\n            self.Cr = np.clip(self.Cr + self.lr * np.random.normal(0, 0.1), 0.1, 1.0)\n            \n            self.success_F = []\n            self.success_Cr = []\n\n        return self.f_opt, self.x_opt\n\n    def mutation_strategy_1(self, population, i, lb, ub):\n        # DE/rand/1\n        idxs = [idx for idx in range(self.pop_size) if idx != i]\n        a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n        mutant = np.clip(a + self.F * (b - c), lb, ub)\n        return mutant\n    \n    def mutation_strategy_2(self, population, i, lb, ub):\n        # DE/current-to-rand/1\n        idxs = [idx for idx in range(self.pop_size) if idx != i]\n        a, b = population[np.random.choice(idxs, 2, replace=False)]\n        mutant = np.clip(population[i] + self.F * (a - population[i]) + self.F * (b - population[np.random.randint(len(idxs))]), lb, ub)\n        return mutant\n\n    def mutation_3(self, population, i, lb, ub):\n         #DE/current-to-best/1\n        idxs = [idx for idx in range(self.pop_size) if idx != i]\n        a, b = population[np.random.choice(idxs, 2, replace=False)]\n        mutant = np.clip(population[i] + self.F * (self.x_opt - population[i]) + self.F * (a - b), lb, ub)\n        return mutant\n\n    def orthogonal_crossover(self, x, mutant, cr):\n        trial = x.copy()\n        for j in range(self.dim):\n            if np.random.rand() < cr:\n                trial[j] = mutant[j]\n        return trial\n\n    def update_archive(self, individual, fitness):\n        if len(self.archive) < self.archive_size:\n            self.archive.append(individual.copy())\n            self.archive_fitness.append(fitness)\n        else:\n            max_archive_index = np.argmax(self.archive_fitness)\n            if fitness < self.archive_fitness[max_archive_index]:\n                self.archive[max_archive_index] = individual.copy()\n                self.archive_fitness[max_archive_index] = fitness", "configspace": "", "generation": 12, "feedback": "The algorithm EnsembleAdaptiveDE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["06b6ae75-0ec5-408d-a4a3-4059dc60b0ac"], "operator": null, "metadata": {"aucs": [0]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "30f0491d-cc5b-4bd9-bade-f492e5b01e56", "fitness": "-inf", "name": "SelfAdaptiveDE", "description": "A differential evolution variant that incorporates a success-history based parameter adaptation with multiple mutation strategies and an external archive to enhance exploration and exploitation.", "code": "import numpy as np\n\nclass SelfAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=10, mutation_strategies=['DE/rand/1', 'DE/best/1', 'DE/current-to-rand/1']):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.mutation_strategies = mutation_strategies\n        self.F = 0.5 * np.ones(self.pop_size)  # Mutation factor for each individual\n        self.Cr = 0.9 * np.ones(self.pop_size)  # Crossover rate for each individual\n        self.archive = []\n        self.archive_fitness = []\n        self.sf = 0.5 * np.ones(len(mutation_strategies))\n        self.scr = 0.9 * np.ones(len(mutation_strategies))\n\n    def __call__(self, func):\n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)]\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Strategy Selection\n                strategy_index = np.random.randint(len(self.mutation_strategies))\n                strategy = self.mutation_strategies[strategy_index]\n\n                # Parameter Adaptation\n                Fi = np.random.choice(np.clip(np.random.normal(self.sf[strategy_index], 0.1), 0.1, 1.0),1)[0]\n                Cri = np.random.choice(np.clip(np.random.normal(self.scr[strategy_index], 0.1), 0.1, 1.0),1)[0]\n\n                # Mutation\n                if strategy == 'DE/rand/1':\n                    idxs = np.random.choice(self.pop_size + len(self.archive), 3, replace=False)\n                    xa, xb, xc = self._get_individuals(idxs,population)\n                    mutant = np.clip(population[i] + Fi * (xb - xc), func.bounds.lb, func.bounds.ub)\n                elif strategy == 'DE/best/1':\n                    best_idx = np.argmin(fitness)\n                    idxs = np.random.choice(self.pop_size + len(self.archive), 2, replace=False)\n                    xa, xb = self._get_individuals(idxs,population)\n                    mutant = np.clip(population[best_idx] + Fi * (xa - xb), func.bounds.lb, func.bounds.ub)\n                elif strategy == 'DE/current-to-rand/1':\n                    idxs = np.random.choice(self.pop_size + len(self.archive), 2, replace=False)\n                    xa, xb = self._get_individuals(idxs,population)\n                    mutant = np.clip(population[i] + Fi * (population[i] - xa) + Fi*(xb-population[i]), func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < Cri\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n\n                if f_trial < fitness[i]:\n                    # Success\n                    self.sf[strategy_index] = Fi\n                    self.scr[strategy_index] = Cri\n                    \n                    #Archive\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(population[i].copy())\n                        self.archive_fitness.append(fitness[i])\n                    else:\n                        idx_replace = np.random.randint(0, len(self.archive))\n                        self.archive[idx_replace] = population[i].copy()\n                        self.archive_fitness[idx_replace] = fitness[i]\n\n                    fitness[i] = f_trial\n                    population[i] = trial\n\n                    # Update optimal solution\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n        return self.f_opt, self.x_opt\n    \n    def _get_individuals(self,idxs, population):\n        individuals = []\n        for idx in idxs:\n            if idx < self.pop_size:\n                individuals.append(population[idx])\n            else:\n                individuals.append(self.archive[idx - self.pop_size])\n        return individuals", "configspace": "", "generation": 12, "feedback": "In the code, line 967, in numpy.random.mtrand.RandomState.choice, the following error occurred:\nValueError: a must be 1-dimensional or an integer", "error": "In the code, line 967, in numpy.random.mtrand.RandomState.choice, the following error occurred:\nValueError: a must be 1-dimensional or an integer", "parent_ids": ["94768647-f252-46d5-98c5-9ea8189ba67b"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
