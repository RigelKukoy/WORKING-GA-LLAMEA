{"id": "e3c296ee-f2e8-4001-9fb1-ebf269e1f612", "fitness": "-inf", "name": "AdaptiveDEARestart", "description": "Adaptive Differential Evolution with Archive and Restart, using a population-based approach with differential evolution operators, an archive to store promising solutions, and a restart mechanism to escape local optima.", "code": "import numpy as np\n\nclass AdaptiveDEARestart:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=100, F=0.5, Cr=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.F = F  # Differential weight\n        self.Cr = Cr # Crossover rate\n        self.archive = []\n        self.archive_fitness = []\n        self.restart_trigger = 0.1 * budget\n        self.restart_count = 0\n        self.best_fitness_history = []\n\n    def initialize_population(self, func):\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        return population, fitness\n\n    def mutate(self, population, archive):\n        mutated_population = np.zeros_like(population)\n        for i in range(self.pop_size):\n            # Choose three distinct indices\n            indices = np.random.choice(self.pop_size + len(archive), 3, replace=False)\n            \n            if indices[0] < self.pop_size:\n                x_r1 = population[indices[0]]\n            else:\n                x_r1 = archive[indices[0] - self.pop_size]\n            \n            if indices[1] < self.pop_size:\n                x_r2 = population[indices[1]]\n            else:\n                x_r2 = archive[indices[1] - self.pop_size]\n\n            if indices[2] < self.pop_size:\n                x_r3 = population[indices[2]]\n            else:\n                x_r3 = archive[indices[2] - self.pop_size]\n                \n            mutated_population[i] = population[i] + self.F * (x_r1 - x_r2) # Using current individual\n            \n            # Clip to bounds. More robust than if statements.\n            mutated_population[i] = np.clip(mutated_population[i], func.bounds.lb, func.bounds.ub)\n\n        return mutated_population\n\n    def crossover(self, population, mutated_population):\n        crossed_population = np.zeros_like(population)\n        for i in range(self.pop_size):\n            for j in range(self.dim):\n                if np.random.rand() < self.Cr:\n                    crossed_population[i, j] = mutated_population[i, j]\n                else:\n                    crossed_population[i, j] = population[i, j]\n        return crossed_population\n\n    def update_archive(self, population, fitness):\n         for i in range(self.pop_size):\n            if len(self.archive) < self.archive_size:\n                self.archive.append(population[i].copy())\n                self.archive_fitness.append(fitness[i])\n            else:\n                # Replace worst individual in archive\n                max_index = np.argmax(self.archive_fitness)\n                if fitness[i] < self.archive_fitness[max_index]:\n                    self.archive[max_index] = population[i].copy()\n                    self.archive_fitness[max_index] = fitness[i]\n                    \n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        eval_count = 0\n\n        population, fitness = self.initialize_population(func)\n        eval_count += self.pop_size\n        \n        best_index = np.argmin(fitness)\n        if fitness[best_index] < self.f_opt:\n            self.f_opt = fitness[best_index]\n            self.x_opt = population[best_index].copy()\n            self.best_fitness_history.append(self.f_opt)\n                \n        self.archive = []\n        self.archive_fitness = []\n        self.update_archive(population, fitness)\n\n        while eval_count < self.budget:\n            mutated_population = self.mutate(population, self.archive)\n            crossed_population = self.crossover(population, mutated_population)\n            \n            new_fitness = np.array([func(x) if eval_count + k < self.budget else np.inf for k, x in enumerate(crossed_population)]) # Check budget before evaluation.\n            eval_count += self.pop_size\n            \n            # Selection\n            for i in range(self.pop_size):\n                if new_fitness[i] < fitness[i]:\n                    population[i] = crossed_population[i].copy()\n                    fitness[i] = new_fitness[i]\n            \n            best_index = np.argmin(fitness)\n            if fitness[best_index] < self.f_opt:\n                self.f_opt = fitness[best_index]\n                self.x_opt = population[best_index].copy()\n                self.best_fitness_history.append(self.f_opt)\n\n            self.update_archive(population, fitness)\n\n            #Restart mechanism\n            if eval_count > (self.restart_count+1) * self.restart_trigger:\n                if len(self.best_fitness_history) > 1 and self.best_fitness_history[-1] >= self.best_fitness_history[-2]:\n                    population, fitness = self.initialize_population(func)\n                    eval_count += self.pop_size\n                    self.archive = []\n                    self.archive_fitness = []\n                    self.update_archive(population, fitness)\n\n                    best_index = np.argmin(fitness)\n                    if fitness[best_index] < self.f_opt:\n                        self.f_opt = fitness[best_index]\n                        self.x_opt = population[best_index].copy()\n                        self.best_fitness_history.append(self.f_opt)\n                self.restart_count += 1\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "In the code, line 46, in mutate, the following error occurred:\nNameError: name 'func' is not defined\nOn line: mutated_population[i] = np.clip(mutated_population[i], func.bounds.lb, func.bounds.ub)", "error": "In the code, line 46, in mutate, the following error occurred:\nNameError: name 'func' is not defined\nOn line: mutated_population[i] = np.clip(mutated_population[i], func.bounds.lb, func.bounds.ub)", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "19246a1a-e4d3-4144-b736-0e41dd855553", "fitness": "-inf", "name": "HybridOptimizer", "description": "A population-based algorithm employing a velocity-based update rule inspired by Particle Swarm Optimization, combined with a differential evolution-like mutation and crossover to enhance exploration, and a local search component based on gradient estimation to improve exploitation.", "code": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget=10000, dim=10, pop_size=50, inertia=0.7, cognitive_coeff=1.4, social_coeff=1.4, mutation_rate=0.1, local_search_iterations=5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.inertia = inertia\n        self.cognitive_coeff = cognitive_coeff\n        self.social_coeff = social_coeff\n        self.mutation_rate = mutation_rate\n        self.local_search_iterations = local_search_iterations\n\n        self.X = None\n        self.V = None  # Velocities\n        self.fitness = None\n        self.personal_best_X = None\n        self.personal_best_fitness = None\n        self.global_best_X = None\n        self.global_best_fitness = np.inf\n        self.eval_count = 0\n\n\n    def initialize_population(self, func):\n        self.X = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.V = np.zeros_like(self.X)  # Initialize velocities to zero\n        self.fitness = np.array([func(x) for x in self.X])\n        self.eval_count += self.pop_size\n        self.personal_best_X = self.X.copy()\n        self.personal_best_fitness = self.fitness.copy()\n\n        self.global_best_X = self.X[np.argmin(self.fitness)].copy()\n        self.global_best_fitness = np.min(self.fitness)\n\n    def update_velocity(self):\n        r1 = np.random.rand(self.pop_size, self.dim)\n        r2 = np.random.rand(self.pop_size, self.dim)\n        cognitive_component = self.cognitive_coeff * r1 * (self.personal_best_X - self.X)\n        social_component = self.social_coeff * r2 * (self.global_best_X - self.X)\n        self.V = self.inertia * self.V + cognitive_component + social_component\n\n    def differential_mutation(self, func):\n         for i in range(self.pop_size):\n            idxs = np.random.choice(self.pop_size, 3, replace=False)\n            x_r1, x_r2, x_r3 = self.X[idxs]\n            mutant = x_r1 + self.mutation_rate * (x_r2 - x_r3)\n            mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n            # Crossover\n            j_rand = np.random.randint(self.dim)\n            for j in range(self.dim):\n                if np.random.rand() > 0.9 and j != j_rand:\n                    mutant[j] = self.X[i, j]\n\n            f_mutant = func(mutant)\n            self.eval_count += 1\n            if f_mutant < self.fitness[i]:\n                self.X[i] = mutant\n                self.fitness[i] = f_mutant\n            \n    def local_search(self, func):\n        for i in range(self.pop_size):\n            x = self.X[i].copy()\n            f = self.fitness[i]\n            for _ in range(self.local_search_iterations):\n                # Estimate gradient (simplified finite difference)\n                gradient = np.zeros(self.dim)\n                delta = 1e-4\n                for j in range(self.dim):\n                    x_plus = x.copy()\n                    x_minus = x.copy()\n                    x_plus[j] += delta\n                    x_minus[j] -= delta\n                    x_plus[j] = np.clip(x_plus[j], func.bounds.lb, func.bounds.ub)\n                    x_minus[j] = np.clip(x_minus[j], func.bounds.lb, func.bounds.ub)\n\n                    f_plus = func(x_plus)\n                    f_minus = func(x_minus)\n                    self.eval_count += 2\n                    gradient[j] = (f_plus - f_minus) / (2 * delta)\n\n                # Update position based on gradient\n                x_new = x - 0.01 * gradient  # Learning rate 0.01\n                x_new = np.clip(x_new, func.bounds.lb, func.bounds.ub)\n                f_new = func(x_new)\n                self.eval_count += 1\n\n                if f_new < f:\n                    x = x_new\n                    f = f_new\n                    self.X[i] = x\n                    self.fitness[i] = f\n\n    def update_best(self):\n        for i in range(self.pop_size):\n            if self.fitness[i] < self.personal_best_fitness[i]:\n                self.personal_best_fitness[i] = self.fitness[i]\n                self.personal_best_X[i] = self.X[i].copy()\n\n            if self.fitness[i] < self.global_best_fitness:\n                self.global_best_fitness = self.fitness[i]\n                self.global_best_X = self.X[i].copy()\n\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.eval_count < self.budget:\n            self.update_velocity()\n            self.X = np.clip(self.X + self.V, func.bounds.lb, func.bounds.ub) #Update positions\n\n            #Evaluate new positions\n            for i in range(self.pop_size):\n              if self.eval_count < self.budget:\n                f = func(self.X[i])\n                self.fitness[i] = f\n                self.eval_count += 1\n              else:\n                break\n\n            self.differential_mutation(func)\n\n            if self.eval_count + self.pop_size * self.local_search_iterations * 3 <= self.budget:  #Check budget before local search\n                self.local_search(func)\n\n            self.update_best()\n\n        return self.global_best_fitness, self.global_best_X", "configspace": "", "generation": 0, "feedback": "In the code, line 74, in local_search, the following error occurred:\nValueError: setting an array element with a sequence.\nOn line: x_plus[j] = np.clip(x_plus[j], func.bounds.lb, func.bounds.ub)", "error": "In the code, line 74, in local_search, the following error occurred:\nValueError: setting an array element with a sequence.\nOn line: x_plus[j] = np.clip(x_plus[j], func.bounds.lb, func.bounds.ub)", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "59f780a6-0dd6-452f-998e-3a4daa71db8d", "fitness": 0.0, "name": "SelfAdaptiveDE", "description": "A self-adaptive Differential Evolution strategy with a population-based archive for diversity and a local search step to enhance exploitation.", "code": "import numpy as np\n\nclass SelfAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=25, f=0.7, cr=0.9, local_search_iterations=5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.F = f  # Differential weight\n        self.CR = cr # Crossover rate\n        self.local_search_iterations = local_search_iterations\n        self.population = None\n        self.fitness = None\n        self.archive = None\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        self.archive = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.archive_size, self.dim))\n        self.archive_fitness = np.full(self.archive_size, np.inf) #init archive fitness\n\n    def mutate(self, func, i):\n        # Select three distinct individuals from the population and archive\n        indices = np.random.choice(self.pop_size + self.archive_size, 3, replace=False)\n        \n        if indices[0] < self.pop_size:\n            x_r1 = self.population[indices[0]]\n        else:\n            x_r1 = self.archive[indices[0] - self.pop_size]\n            \n        if indices[1] < self.pop_size:\n            x_r2 = self.population[indices[1]]\n        else:\n            x_r2 = self.archive[indices[1] - self.pop_size]\n\n        if indices[2] < self.pop_size:\n            x_r3 = self.population[indices[2]]\n        else:\n            x_r3 = self.archive[indices[2] - self.pop_size]\n\n        x_mutated = self.population[i] + self.F * (x_r2 - x_r3) # Added x_r1\n\n        # Repair the mutated vector if it exceeds the bounds\n        x_mutated = np.clip(x_mutated, func.bounds.lb, func.bounds.ub)\n        return x_mutated\n\n    def crossover(self, x_mutated, i):\n        x_trial = np.copy(self.population[i])\n        j_rand = np.random.randint(self.dim)\n        for j in range(self.dim):\n            if np.random.rand() < self.CR or j == j_rand:\n                x_trial[j] = x_mutated[j]\n        return x_trial\n\n    def local_search(self, func, x):\n        # Simple local search around the given solution\n        best_x = x.copy()\n        best_f = func(x)\n        self.budget -= 1\n        \n        if self.budget <= 0:\n          return best_f, best_x\n          \n        for _ in range(self.local_search_iterations):\n            x_neighbor = x + np.random.normal(0, 0.1, self.dim)  # Small perturbation\n            x_neighbor = np.clip(x_neighbor, func.bounds.lb, func.bounds.ub)\n            f_neighbor = func(x_neighbor)\n            self.budget -= 1\n            \n            if self.budget <= 0:\n              return best_f, best_x\n\n            if f_neighbor < best_f:\n                best_f = f_neighbor\n                best_x = x_neighbor.copy()\n        return best_f, best_x\n\n    def update_archive(self):\n        # Update the archive with the worst individuals from the population\n        indices = np.argsort(self.fitness)[-self.archive_size:]  # Indices of worst individuals\n        for i, idx in enumerate(indices):\n            if self.fitness[idx] < np.max(self.archive_fitness):\n                worst_archive_index = np.argmax(self.archive_fitness)\n                self.archive[worst_archive_index] = self.population[idx].copy()\n                self.archive_fitness[worst_archive_index] = self.fitness[idx]\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)].copy()\n        \n        while self.budget > 0:\n            for i in range(self.pop_size):\n                x_mutated = self.mutate(func, i)\n                x_trial = self.crossover(x_mutated, i)\n\n                f_trial = func(x_trial)\n                self.budget -= 1\n                \n                if self.budget <= 0:\n                  break # Exit inner loop\n\n                #Local search\n                f_ls, x_ls = self.local_search(func, x_trial)\n\n                if f_ls < self.fitness[i]:\n                    self.population[i] = x_ls.copy()\n                    self.fitness[i] = f_ls\n\n                    if f_ls < self.f_opt:\n                        self.f_opt = f_ls\n                        self.x_opt = x_ls.copy()\n            if self.budget <= 0:\n                break # Exit outer loop\n\n            self.update_archive()\n            \n            best_index = np.argmin(self.fitness)\n            if self.fitness[best_index] < self.f_opt:\n              self.f_opt = self.fitness[best_index]\n              self.x_opt = self.population[best_index].copy()\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm SelfAdaptiveDE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "06376a10-7b13-4b4b-9705-685d36ecf581", "fitness": 0.0, "name": "HybridPSO_DE", "description": "A population-based algorithm that combines particle swarm optimization principles with differential evolution mutation for enhanced exploration and exploitation.", "code": "import numpy as np\n\nclass HybridPSO_DE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, w=0.7, c1=1.5, c2=1.5, cr=0.7, f=0.8):\n        \"\"\"\n        Initializes the Hybrid PSO-DE optimizer.\n\n        Args:\n            budget (int): The maximum number of function evaluations.\n            dim (int): The dimensionality of the problem.\n            pop_size (int): The size of the population.\n            w (float): Inertia weight for PSO.\n            c1 (float): Cognitive coefficient for PSO.\n            c2 (float): Social coefficient for PSO.\n            cr (float): Crossover rate for DE.\n            f (float): Mutation factor for DE.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.w = w\n        self.c1 = c1\n        self.c2 = c2\n        self.cr = cr\n        self.f = f\n\n        self.particles = None\n        self.velocities = None\n        self.fitness = None\n        self.personal_best_positions = None\n        self.personal_best_fitness = None\n        self.global_best_position = None\n        self.global_best_fitness = np.inf\n        self.func_evals = 0\n\n    def initialize_population(self, func):\n        \"\"\"\n        Initializes the population randomly within the bounds.\n\n        Args:\n            func: The black-box optimization function.\n        \"\"\"\n        self.particles = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, size=(self.pop_size, self.dim))  # Initialize velocities\n        self.fitness = np.array([func(x) for x in self.particles])\n        self.func_evals += self.pop_size  #update function evaluations\n        self.personal_best_positions = self.particles.copy()\n        self.personal_best_fitness = self.fitness.copy()\n\n        self.global_best_position = self.particles[np.argmin(self.fitness)].copy()\n        self.global_best_fitness = np.min(self.fitness)\n\n\n    def update_global_best(self):\n        \"\"\"\n        Updates the global best position and fitness.\n        \"\"\"\n        best_index = np.argmin(self.fitness)\n        if self.fitness[best_index] < self.global_best_fitness:\n            self.global_best_fitness = self.fitness[best_index]\n            self.global_best_position = self.particles[best_index].copy()\n            \n\n    def pso_update(self, func):\n        \"\"\"\n        Updates the particles' positions and velocities using PSO rules.\n\n        Args:\n            func: The black-box optimization function.\n        \"\"\"\n        for i in range(self.pop_size):\n            r1 = np.random.rand(self.dim)\n            r2 = np.random.rand(self.dim)\n\n            self.velocities[i] = (self.w * self.velocities[i]\n                                  + self.c1 * r1 * (self.personal_best_positions[i] - self.particles[i])\n                                  + self.c2 * r2 * (self.global_best_position - self.particles[i]))\n\n            self.particles[i] = self.particles[i] + self.velocities[i]\n\n            # Boundary handling: clip particles to the bounds\n            self.particles[i] = np.clip(self.particles[i], func.bounds.lb, func.bounds.ub)\n\n            f = func(self.particles[i])\n            self.func_evals += 1\n            self.fitness[i] = f\n\n            if f < self.personal_best_fitness[i]:\n                self.personal_best_fitness[i] = f\n                self.personal_best_positions[i] = self.particles[i].copy()\n\n    def de_mutation(self, func):\n         \"\"\"\n         Applies differential evolution mutation to the population.\n\n         Args:\n            func: The black-box optimization function.\n         \"\"\"\n         for i in range(self.pop_size):\n            # Choose three random indices, distinct from each other and i\n            idxs = np.random.choice(self.pop_size, 3, replace=False)\n            while i in idxs:\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n           \n            x1, x2, x3 = self.particles[idxs[0]], self.particles[idxs[1]], self.particles[idxs[2]]\n           \n            # Mutation\n            v = x1 + self.f * (x2 - x3)\n            v = np.clip(v, func.bounds.lb, func.bounds.ub)\n           \n            # Crossover\n            jrand = np.random.randint(self.dim)  # Randomly choose one index to always crossover\n            u = np.array([v[j] if (np.random.rand() < self.cr or j == jrand) else self.particles[i][j] for j in range(self.dim)])\n           \n            f = func(u)\n            self.func_evals += 1\n            \n            if f < self.fitness[i]:\n                self.fitness[i] = f\n                self.particles[i] = u.copy()\n                if f < self.personal_best_fitness[i]:\n                    self.personal_best_fitness[i] = f\n                    self.personal_best_positions[i] = self.particles[i].copy()\n    def __call__(self, func):\n        \"\"\"\n        Optimizes the given black-box function using the Hybrid PSO-DE algorithm.\n\n        Args:\n            func: The black-box optimization function.\n\n        Returns:\n            tuple: The best function value found and the corresponding solution.\n        \"\"\"\n        self.initialize_population(func)\n        self.update_global_best()\n\n        while self.func_evals < self.budget:\n            self.pso_update(func)\n            self.de_mutation(func)\n            self.update_global_best()\n            \n            # Early stopping condition (optional):\n            if self.global_best_fitness == -np.inf or self.global_best_fitness == np.inf or np.isnan(self.global_best_fitness):\n                break\n\n        return self.global_best_fitness, self.global_best_position", "configspace": "", "generation": 0, "feedback": "The algorithm HybridPSO_DE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "9ff996e5-12fb-45c0-bee6-d4a2b24def7c", "fitness": "-inf", "name": "OrthogonalLearningSA", "description": "A population-based algorithm employing orthogonal learning to construct promising candidates and simulated annealing for acceptance to balance exploration and exploitation.", "code": "import numpy as np\n\nclass OrthogonalLearningSA:\n    def __init__(self, budget=10000, dim=10, pop_size=50, T_initial=1.0, T_min=0.0001, alpha=0.99):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.T = T_initial  # Initial temperature\n        self.T_min = T_min  # Minimum temperature\n        self.alpha = alpha  # Cooling rate\n\n    def initialize_population(self, func):\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        return population, fitness\n\n    def orthogonal_design(self, population, best_individual, func):\n        \"\"\"\n        Generates a new candidate solution based on orthogonal design principles around the best individual.\n        \"\"\"\n        n_factors = min(self.dim, self.pop_size)  # Number of factors (dimensions to vary)\n        levels = 3  # Number of levels for each factor\n        \n        # Select n_factors dimensions randomly\n        selected_dims = np.random.choice(self.dim, n_factors, replace=False)\n        \n        # Create an orthogonal array (simplified, only works for levels=3 and specific n_factors)\n        if n_factors == 3:\n            orthogonal_array = np.array([\n                [-1, -1, -1],\n                [-1, 0, 1],\n                [-1, 1, 0],\n                [0, -1, 1],\n                [0, 0, -1],\n                [0, 1, -1],\n                [1, -1, 0],\n                [1, 0, -1],\n                [1, 1, 1]\n            ])\n            n_combinations = 9\n        elif n_factors == 4:\n             orthogonal_array = np.array([\n                [-1, -1, -1, -1],\n                [-1, -1, 1, 1],\n                [-1, 1, -1, 1],\n                [-1, 1, 1, -1],\n                [1, -1, -1, 1],\n                [1, -1, 1, -1],\n                [1, 1, -1, -1],\n                [1, 1, 1, 1],\n            ])\n             n_combinations = 8 # for hadamard matrix of order 8\n        else:  # simplified for other dimensions\n            orthogonal_array = np.random.choice([-1, 0, 1], size=(self.pop_size, n_factors))\n            n_combinations = self.pop_size\n\n        new_candidates = np.zeros((n_combinations, self.dim))\n        \n        # Map -1, 0, 1 to lower bound, mean, upper bound\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        \n        for i in range(n_combinations):\n            new_solution = best_individual.copy()\n            for j, dim_index in enumerate(selected_dims):\n                if orthogonal_array[i, j] == -1:\n                    new_solution[dim_index] = lb\n                elif orthogonal_array[i, j] == 0:\n                    new_solution[dim_index] = (lb + ub) / 2.0\n                else:\n                    new_solution[dim_index] = ub\n\n            new_solution = np.clip(new_solution, lb, ub) # ensure feasibility\n            new_candidates[i, :] = new_solution\n            \n        return new_candidates\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        eval_count = 0\n\n        population, fitness = self.initialize_population(func)\n        eval_count += self.pop_size\n\n        best_index = np.argmin(fitness)\n        if fitness[best_index] < self.f_opt:\n            self.f_opt = fitness[best_index]\n            self.x_opt = population[best_index].copy()\n            \n        while eval_count < self.budget and self.T > self.T_min:\n            # Generate new candidates using orthogonal learning around the best individual\n            candidates = self.orthogonal_design(population, population[best_index], func)\n            \n            # Evaluate the new candidates\n            candidate_fitness = np.array([func(x) if eval_count + k < self.budget else np.inf for k, x in enumerate(candidates)]) # Budget check\n            eval_count += len(candidates)\n            \n            # Simulated Annealing selection\n            for i in range(len(candidates)):\n                delta_E = candidate_fitness[i] - fitness[best_index]\n                if delta_E < 0:\n                    population[best_index] = candidates[i].copy()\n                    fitness[best_index] = candidate_fitness[i]\n                    \n                    if fitness[best_index] < self.f_opt:\n                        self.f_opt = fitness[best_index]\n                        self.x_opt = population[best_index].copy()\n                else:\n                    # Acceptance probability based on temperature\n                    acceptance_probability = np.exp(-delta_E / self.T)\n                    if np.random.rand() < acceptance_probability:\n                        population[best_index] = candidates[i].copy()\n                        fitness[best_index] = candidate_fitness[i]\n            \n            # Find the new best individual\n            best_index = np.argmin(fitness)\n            \n            # Cool down the temperature\n            self.T *= self.alpha\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "In the code, line 69, in orthogonal_design, the following error occurred:\nValueError: setting an array element with a sequence.\nOn line: new_solution[dim_index] = (lb + ub) / 2.0", "error": "In the code, line 69, in orthogonal_design, the following error occurred:\nValueError: setting an array element with a sequence.\nOn line: new_solution[dim_index] = (lb + ub) / 2.0", "parent_ids": ["e3c296ee-f2e8-4001-9fb1-ebf269e1f612"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "00eae025-1129-4952-9fd3-ee30aa1ba18b", "fitness": 0.0, "name": "HybridOptimizer", "description": "A simplified hybrid algorithm that blends Particle Swarm Optimization's velocity updates with Differential Evolution's mutation strategy within a unified loop, emphasizing parameter adaptation for enhanced exploration and exploitation.", "code": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget=10000, dim=10, pop_size=40, w=0.7, c1=1.4, c2=1.4, cr=0.8, f=0.7):\n        \"\"\"\n        Initializes the Hybrid Optimizer.\n\n        Args:\n            budget (int): The maximum number of function evaluations.\n            dim (int): The dimensionality of the problem.\n            pop_size (int): The size of the population.\n            w (float): Inertia weight for PSO.\n            c1 (float): Cognitive coefficient for PSO.\n            c2 (float): Social coefficient for PSO.\n            cr (float): Crossover rate for DE.\n            f (float): Mutation factor for DE.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.w = w\n        self.c1 = c1\n        self.c2 = c2\n        self.cr = cr\n        self.f = f\n\n        self.particles = None\n        self.velocities = None\n        self.fitness = None\n        self.personal_best_positions = None\n        self.personal_best_fitness = None\n        self.global_best_position = None\n        self.global_best_fitness = np.inf\n        self.func_evals = 0\n\n    def initialize_population(self, func):\n        \"\"\"\n        Initializes the population randomly within the bounds.\n\n        Args:\n            func: The black-box optimization function.\n        \"\"\"\n        self.particles = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, size=(self.pop_size, self.dim))  # Initialize velocities\n        self.fitness = np.array([func(x) for x in self.particles])\n        self.func_evals += self.pop_size  #update function evaluations\n        self.personal_best_positions = self.particles.copy()\n        self.personal_best_fitness = self.fitness.copy()\n\n        self.global_best_position = self.particles[np.argmin(self.fitness)].copy()\n        self.global_best_fitness = np.min(self.fitness)\n\n\n    def update_global_best(self):\n        \"\"\"\n        Updates the global best position and fitness.\n        \"\"\"\n        best_index = np.argmin(self.fitness)\n        if self.fitness[best_index] < self.global_best_fitness:\n            self.global_best_fitness = self.fitness[best_index]\n            self.global_best_position = self.particles[best_index].copy()\n\n    def update_particle(self, i, func):\n        \"\"\"\n        Updates a single particle's position and velocity using a blend of PSO and DE.\n\n        Args:\n            i (int): Index of the particle to update.\n            func: The black-box optimization function.\n        \"\"\"\n        # PSO velocity update\n        r1 = np.random.rand(self.dim)\n        r2 = np.random.rand(self.dim)\n        self.velocities[i] = (self.w * self.velocities[i]\n                              + self.c1 * r1 * (self.personal_best_positions[i] - self.particles[i])\n                              + self.c2 * r2 * (self.global_best_position - self.particles[i]))\n        # DE mutation\n        idxs = np.random.choice(self.pop_size, 3, replace=False)\n        while i in idxs:\n            idxs = np.random.choice(self.pop_size, 3, replace=False)\n\n        x1, x2, x3 = self.particles[idxs[0]], self.particles[idxs[1]], self.particles[idxs[2]]\n        v = x1 + self.f * (x2 - x3)\n        v = np.clip(v, func.bounds.lb, func.bounds.ub)\n\n        # Crossover\n        jrand = np.random.randint(self.dim)\n        u = np.array([v[j] if (np.random.rand() < self.cr or j == jrand) else self.particles[i][j] for j in range(self.dim)])\n        \n        # Update position\n        new_position = self.particles[i] + self.velocities[i]\n        new_position = np.clip(new_position, func.bounds.lb, func.bounds.ub)\n\n        # Evaluate new position (using DE's mutated vector as the other alternative)\n        f_pso = func(new_position)\n        f_de = func(u)\n        self.func_evals += 2\n\n        #Greedy selection\n        if f_de < f_pso:\n             new_f = f_de\n             new_pos = u\n        else:\n             new_f = f_pso\n             new_pos = new_position\n        \n        if new_f < self.fitness[i]:\n            self.fitness[i] = new_f\n            self.particles[i] = new_pos.copy()\n\n            if new_f < self.personal_best_fitness[i]:\n                self.personal_best_fitness[i] = new_f\n                self.personal_best_positions[i] = self.particles[i].copy()\n\n\n    def __call__(self, func):\n        \"\"\"\n        Optimizes the given black-box function using the Hybrid Optimizer algorithm.\n\n        Args:\n            func: The black-box optimization function.\n\n        Returns:\n            tuple: The best function value found and the corresponding solution.\n        \"\"\"\n        self.initialize_population(func)\n        self.update_global_best()\n\n        while self.func_evals < self.budget:\n            for i in range(self.pop_size):\n                self.update_particle(i, func)\n            self.update_global_best()\n\n\n        return self.global_best_fitness, self.global_best_position", "configspace": "", "generation": 1, "feedback": "The algorithm HybridOptimizer scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["06376a10-7b13-4b4b-9705-685d36ecf581"], "operator": null, "metadata": {"aucs": [0]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "16675356-73a3-40af-9412-e1c02bf95132", "fitness": 0.0, "name": "HybridOptimizer", "description": "Combines Particle Swarm Optimization with Differential Evolution and a Gaussian mutation-based local search for enhanced exploration and exploitation.", "code": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget=10000, dim=10, pop_size=50, inertia=0.7, cognitive_coeff=1.4, social_coeff=1.4, de_mutation_rate=0.5, local_search_prob=0.2):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.inertia = inertia\n        self.cognitive_coeff = cognitive_coeff\n        self.social_coeff = social_coeff\n        self.de_mutation_rate = de_mutation_rate\n        self.local_search_prob = local_search_prob\n\n        self.X = None\n        self.V = None\n        self.fitness = None\n        self.personal_best_X = None\n        self.personal_best_fitness = None\n        self.global_best_X = None\n        self.global_best_fitness = np.inf\n        self.eval_count = 0\n\n    def initialize_population(self, func):\n        self.X = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.V = np.zeros_like(self.X)\n        self.fitness = np.array([func(x) for x in self.X])\n        self.eval_count += self.pop_size\n        self.personal_best_X = self.X.copy()\n        self.personal_best_fitness = self.fitness.copy()\n\n        self.global_best_X = self.X[np.argmin(self.fitness)].copy()\n        self.global_best_fitness = np.min(self.fitness)\n\n    def update_velocity(self):\n        r1 = np.random.rand(self.pop_size, self.dim)\n        r2 = np.random.rand(self.pop_size, self.dim)\n        cognitive_component = self.cognitive_coeff * r1 * (self.personal_best_X - self.X)\n        social_component = self.social_coeff * r2 * (self.global_best_X - self.X)\n        self.V = self.inertia * self.V + cognitive_component + social_component\n\n    def differential_evolution(self, func):\n        for i in range(self.pop_size):\n            idxs = np.random.choice(self.pop_size, 3, replace=False)\n            x_r1, x_r2, x_r3 = self.X[idxs]\n            mutant = x_r1 + self.de_mutation_rate * (x_r2 - x_r3)\n            mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n            if np.random.rand() < 0.9:  # Simplified crossover\n                j_rand = np.random.randint(self.dim)\n                mutant[j_rand] = self.X[i, j_rand]\n\n            f_mutant = func(mutant)\n            self.eval_count += 1\n            if f_mutant < self.fitness[i]:\n                self.X[i] = mutant\n                self.fitness[i] = f_mutant\n\n    def local_search(self, func):\n        for i in range(self.pop_size):\n            if np.random.rand() < self.local_search_prob:\n                x = self.X[i].copy()\n                sigma = 0.1 * (func.bounds.ub - func.bounds.lb)  # Adaptive step size\n                x_new = x + np.random.normal(0, sigma, self.dim)\n                x_new = np.clip(x_new, func.bounds.lb, func.bounds.ub)\n                f_new = func(x_new)\n                self.eval_count += 1\n                if f_new < self.fitness[i]:\n                    self.X[i] = x_new\n                    self.fitness[i] = f_new\n\n    def update_best(self):\n        for i in range(self.pop_size):\n            if self.fitness[i] < self.personal_best_fitness[i]:\n                self.personal_best_fitness[i] = self.fitness[i]\n                self.personal_best_X[i] = self.X[i].copy()\n\n            if self.fitness[i] < self.global_best_fitness:\n                self.global_best_fitness = self.fitness[i]\n                self.global_best_X = self.X[i].copy()\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.eval_count < self.budget:\n            self.update_velocity()\n            self.X = np.clip(self.X + self.V, func.bounds.lb, func.bounds.ub)\n\n            # Evaluate new positions\n            for i in range(self.pop_size):\n                if self.eval_count < self.budget:\n                    f = func(self.X[i])\n                    self.fitness[i] = f\n                    self.eval_count += 1\n                else:\n                    break\n\n            self.differential_evolution(func)\n            self.local_search(func)\n            self.update_best()\n\n        return self.global_best_fitness, self.global_best_X", "configspace": "", "generation": 1, "feedback": "The algorithm HybridOptimizer scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["19246a1a-e4d3-4144-b736-0e41dd855553"], "operator": null, "metadata": {"aucs": [0]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "50031b2e-a7fe-41fe-95de-300812f230ab", "fitness": "-inf", "name": "OrthogonalDE", "description": "Population-based optimization with orthogonal learning and adaptive mutation, focusing on exploration and exploitation via orthogonal experimental design and dynamically adjusted mutation parameters.", "code": "import numpy as np\n\nclass OrthogonalDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, Cr=0.9, orthogonal_levels=3):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F  # Differential weight\n        self.Cr = Cr # Crossover rate\n        self.orthogonal_levels = orthogonal_levels\n        self.best_fitness_history = []\n\n    def initialize_population(self, func):\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        return population, fitness\n\n    def generate_orthogonal_array(self, n_factors, levels):\n        \"\"\"Generates an orthogonal array using Plackett-Burman design.\"\"\"\n        # This is a simplified example.  For robustness, use a dedicated OA library.\n        # This approach only works for levels = 2.  A more general OA constructor\n        # would be needed for higher levels.\n        if levels != 2:\n           raise ValueError(\"Orthogonal array generation currently only supports 2 levels.\")\n\n        # Example Plackett-Burman matrix for 11 factors (12 runs)\n        Hadamard_matrix = np.array([\n            [+1, +1, +1, +1, +1, +1, +1, +1, +1, +1, +1, +1],\n            [-1, +1, -1, +1, -1, +1, +1, -1, -1, -1, +1, -1],\n            [-1, -1, +1, -1, +1, +1, +1, -1, -1, +1, -1, -1],\n            [-1, -1, -1, +1, +1, +1, -1, -1, +1, -1, +1, -1],\n            [-1, -1, -1, -1, +1, -1, -1, +1, -1, +1, +1, +1],\n            [-1, -1, -1, -1, -1, -1, +1, -1, +1, +1, +1, +1],\n            [-1, -1, -1, +1, -1, -1, -1, +1, +1, +1, -1, +1],\n            [-1, +1, -1, -1, -1, -1, -1, -1, +1, +1, +1, +1],\n            [-1, +1, +1, -1, -1, -1, +1, -1, -1, +1, +1, -1],\n            [-1, +1, +1, +1, -1, +1, -1, +1, -1, -1, +1, -1],\n            [-1, +1, +1, +1, +1, -1, +1, -1, +1, -1, -1, -1],\n            [-1, -1, +1, +1, +1, +1, -1, +1, -1, +1, -1, -1]\n        ])\n\n        orthogonal_array = Hadamard_matrix[:levels*levels, :n_factors]\n\n        return orthogonal_array\n\n    def orthogonal_crossover(self, population, func):\n        \"\"\"Performs crossover based on orthogonal experimental design.\"\"\"\n        n_factors = self.dim  # Each dimension is a factor\n        levels = self.orthogonal_levels\n        \n        # Generate orthogonal array\n        try:\n            orthogonal_array = self.generate_orthogonal_array(n_factors, levels)\n        except ValueError:\n            return population  # If levels > 2, return original population\n\n        n_runs = orthogonal_array.shape[0]\n        \n        new_population = np.zeros_like(population)\n        \n        for i in range(self.pop_size):\n            # Select two parents randomly\n            parent1_idx, parent2_idx = np.random.choice(self.pop_size, 2, replace=False)\n            parent1 = population[parent1_idx]\n            parent2 = population[parent2_idx]\n\n            # Create a temporary population based on the orthogonal array\n            temp_population = np.zeros((n_runs, self.dim))\n            for j in range(n_runs):\n                for k in range(self.dim):\n                  if orthogonal_array[j,k] > 0:\n                    temp_population[j, k] = parent1[k]\n                  else:\n                    temp_population[j, k] = parent2[k]\n            \n            # Evaluate the temporary population (only evaluate if within budget)\n            temp_fitness = [func(x) for x in temp_population[:min(n_runs, self.budget - func.num_evaluations)]]\n            \n            # Select the best individual from the temporary population\n            best_idx = np.argmin(temp_fitness)\n            new_population[i] = temp_population[best_idx]\n        \n        return new_population\n\n    def mutate(self, population, func):\n        mutated_population = np.zeros_like(population)\n        for i in range(self.pop_size):\n            # Adaptive F: linearly decrease from 0.9 to 0.1\n            adaptive_F = 0.9 - (0.8 * func.num_evaluations / self.budget)\n            \n            # Choose three distinct indices\n            indices = np.random.choice(self.pop_size, 3, replace=False)\n            x_r1 = population[indices[0]]\n            x_r2 = population[indices[1]]\n            x_r3 = population[indices[2]]\n\n            mutated_population[i] = population[i] + adaptive_F * (x_r1 - x_r2)\n            mutated_population[i] = np.clip(mutated_population[i], func.bounds.lb, func.bounds.ub)\n\n        return mutated_population\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        eval_count = 0\n\n        population, fitness = self.initialize_population(func)\n        eval_count += self.pop_size\n        \n        best_index = np.argmin(fitness)\n        if fitness[best_index] < self.f_opt:\n            self.f_opt = fitness[best_index]\n            self.x_opt = population[best_index].copy()\n            self.best_fitness_history.append(self.f_opt)\n\n        while eval_count < self.budget:\n            mutated_population = self.mutate(population, func)\n            crossed_population = self.orthogonal_crossover(mutated_population, func) # Use orthogonal crossover\n\n            new_fitness = np.array([func(x) if eval_count + k < self.budget else np.inf for k, x in enumerate(crossed_population)])\n            eval_count += self.pop_size\n\n            # Selection\n            for i in range(self.pop_size):\n                if new_fitness[i] < fitness[i]:\n                    population[i] = crossed_population[i].copy()\n                    fitness[i] = new_fitness[i]\n\n            best_index = np.argmin(fitness)\n            if fitness[best_index] < self.f_opt:\n                self.f_opt = fitness[best_index]\n                self.x_opt = population[best_index].copy()\n                self.best_fitness_history.append(self.f_opt)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "In the code, line 89, in mutate, the following error occurred:\nAttributeError: 'ioh.iohcpp.problem.Discus' object has no attribute 'num_evaluations'\nOn line: adaptive_F = 0.9 - (0.8 * func.num_evaluations / self.budget)", "error": "In the code, line 89, in mutate, the following error occurred:\nAttributeError: 'ioh.iohcpp.problem.Discus' object has no attribute 'num_evaluations'\nOn line: adaptive_F = 0.9 - (0.8 * func.num_evaluations / self.budget)", "parent_ids": ["e3c296ee-f2e8-4001-9fb1-ebf269e1f612"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "f5213b8c-75c6-412c-9b73-002acd6f7bed", "fitness": "-inf", "name": "SelfAdaptiveDE", "description": "A self-adaptive Differential Evolution with a dynamically adjusted population size, a more robust mutation strategy incorporating both population and archive information, and a refined local search employing a decaying step size.", "code": "import numpy as np\n\nclass SelfAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=25, f=0.7, cr=0.9, local_search_iterations=5, initial_local_search_step=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.F = f  # Differential weight\n        self.CR = cr # Crossover rate\n        self.local_search_iterations = local_search_iterations\n        self.initial_local_search_step = initial_local_search_step\n        self.population = None\n        self.fitness = None\n        self.archive = None\n        self.archive_fitness = None\n        self.best_fitness_history = [] # Keep track of best fitness over time\n        self.dynamic_pop_size = self.pop_size # Dynamically adjusted population size\n\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.dynamic_pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.dynamic_pop_size\n        self.archive = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.archive_size, self.dim))\n        self.archive_fitness = np.full(self.archive_size, np.inf) #init archive fitness\n\n    def mutate(self, func, i):\n        # Robust mutation strategy using both population and archive\n        indices = np.random.choice(self.dynamic_pop_size + self.archive_size, 4, replace=False)\n\n        if indices[0] < self.dynamic_pop_size:\n            x_r0 = self.population[indices[0]]\n        else:\n            x_r0 = self.archive[indices[0] - self.dynamic_pop_size]\n\n        if indices[1] < self.dynamic_pop_size:\n            x_r1 = self.population[indices[1]]\n        else:\n            x_r1 = self.archive[indices[1] - self.dynamic_pop_size]\n            \n        if indices[2] < self.dynamic_pop_size:\n            x_r2 = self.population[indices[2]]\n        else:\n            x_r2 = self.archive[indices[2] - self.dynamic_pop_size]\n\n        if indices[3] < self.dynamic_pop_size:\n            x_r3 = self.population[indices[3]]\n        else:\n            x_r3 = self.archive[indices[3] - self.dynamic_pop_size]\n        \n        # More exploitative mutation\n        x_mutated = x_r0 + self.F * (x_r1 - x_r2) + 0.01 * (x_r3 - self.population[i]) # added a small corrective component\n        # Repair the mutated vector if it exceeds the bounds\n        x_mutated = np.clip(x_mutated, func.bounds.lb, func.bounds.ub)\n        return x_mutated\n\n    def crossover(self, x_mutated, i):\n        x_trial = np.copy(self.population[i])\n        j_rand = np.random.randint(self.dim)\n        for j in range(self.dim):\n            if np.random.rand() < self.CR or j == j_rand:\n                x_trial[j] = x_mutated[j]\n        return x_trial\n\n    def local_search(self, func, x):\n        # Refined local search with decaying step size\n        best_x = x.copy()\n        best_f = func(x)\n        self.budget -= 1\n        \n        if self.budget <= 0:\n          return best_f, best_x\n        \n        step_size = self.initial_local_search_step\n        for _ in range(self.local_search_iterations):\n            x_neighbor = x + np.random.normal(0, step_size, self.dim)  # Decaying perturbation\n            x_neighbor = np.clip(x_neighbor, func.bounds.lb, func.bounds.ub)\n            f_neighbor = func(x_neighbor)\n            self.budget -= 1\n            \n            if self.budget <= 0:\n              return best_f, best_x\n\n            if f_neighbor < best_f:\n                best_f = f_neighbor\n                best_x = x_neighbor.copy()\n            step_size *= 0.9  # Decay step size\n        return best_f, best_x\n\n    def update_archive(self):\n        # Update the archive with diverse individuals\n        combined_population = np.vstack((self.population, self.archive))\n        combined_fitness = np.concatenate((self.fitness, self.archive_fitness))\n        \n        # Sort by fitness\n        sorted_indices = np.argsort(combined_fitness)[::-1]\n        \n        # Select the best individuals for replacement, avoiding duplicates\n        selected_count = 0\n        for idx in sorted_indices:\n            if idx < self.dynamic_pop_size:\n                individual = self.population[idx]\n                fitness_value = self.fitness[idx]\n            else:\n                individual = self.archive[idx - self.dynamic_pop_size]\n                fitness_value = self.archive_fitness[idx - self.dynamic_pop_size]\n                \n            # Check if the individual is already in the archive\n            is_duplicate = False\n            for archive_ind in range(self.archive_size):\n                if np.allclose(individual, self.archive[archive_ind]):\n                    is_duplicate = True\n                    break\n            \n            if not is_duplicate:\n                worst_archive_index = np.argmax(self.archive_fitness)\n                if fitness_value < self.archive_fitness[worst_archive_index]:  # Only replace if better\n                    self.archive[worst_archive_index] = individual.copy()\n                    self.archive_fitness[worst_archive_index] = fitness_value\n                    selected_count += 1\n            if selected_count >= self.archive_size:\n              break  # Stop after filling the archive\n\n    def adjust_population_size(self):\n        # Dynamically adjust population size based on stagnation\n        if len(self.best_fitness_history) > 10:\n            if np.std(self.best_fitness_history[-10:]) < 1e-5: #Stagnation detected\n              self.dynamic_pop_size = max(10, int(self.dynamic_pop_size * 0.9))  # Reduce population size\n            else:\n              self.dynamic_pop_size = min(self.pop_size, int(self.dynamic_pop_size * 1.1)) # Increase population size, with max at initial pop_size\n\n            # Re-initialize population if size changes\n            if self.dynamic_pop_size != len(self.population):\n                self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.dynamic_pop_size, self.dim))\n                self.fitness = np.array([func(x) for x in self.population])\n                self.budget -= self.dynamic_pop_size\n                \n    def __call__(self, func):\n        self.initialize_population(func)\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)].copy()\n        self.best_fitness_history = [self.f_opt]  # Initialize history\n\n        while self.budget > 0:\n            self.adjust_population_size()\n            for i in range(self.dynamic_pop_size):\n                x_mutated = self.mutate(func, i)\n                x_trial = self.crossover(x_mutated, i)\n\n                f_trial = func(x_trial)\n                self.budget -= 1\n                \n                if self.budget <= 0:\n                  break # Exit inner loop\n\n                #Local search\n                f_ls, x_ls = self.local_search(func, x_trial)\n\n                if f_ls < self.fitness[i]:\n                    self.population[i] = x_ls.copy()\n                    self.fitness[i] = f_ls\n\n                    if f_ls < self.f_opt:\n                        self.f_opt = f_ls\n                        self.x_opt = x_ls.copy()\n            if self.budget <= 0:\n                break # Exit outer loop\n\n            self.update_archive()\n            \n            best_index = np.argmin(self.fitness)\n            if self.fitness[best_index] < self.f_opt:\n              self.f_opt = self.fitness[best_index]\n              self.x_opt = self.population[best_index].copy()\n\n            self.best_fitness_history.append(self.f_opt)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "In the code, line 135, in adjust_population_size, the following error occurred:\nNameError: name 'func' is not defined\nOn line: self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.dynamic_pop_size, self.dim))", "error": "In the code, line 135, in adjust_population_size, the following error occurred:\nNameError: name 'func' is not defined\nOn line: self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.dynamic_pop_size, self.dim))", "parent_ids": ["59f780a6-0dd6-452f-998e-3a4daa71db8d"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "8ec88c42-cd90-489a-85f1-7025b46081ae", "fitness": "-inf", "name": "AdaptiveHybridPSO_DE", "description": "A hybrid algorithm combining PSO and DE with adaptive parameter control and a local search phase to balance exploration and exploitation.", "code": "import numpy as np\n\nclass AdaptiveHybridPSO_DE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, w_init=0.9, w_final=0.4, c1=2.0, c2=2.0, cr_init=0.5, cr_final=0.9, f_init=0.5, f_final=0.9, local_search_prob=0.1):\n        \"\"\"\n        Initializes the Adaptive Hybrid PSO-DE optimizer.\n\n        Args:\n            budget (int): The maximum number of function evaluations.\n            dim (int): The dimensionality of the problem.\n            pop_size (int): The size of the population.\n            w_init (float): Initial inertia weight for PSO.\n            w_final (float): Final inertia weight for PSO.\n            c1 (float): Cognitive coefficient for PSO.\n            c2 (float): Social coefficient for PSO.\n            cr_init (float): Initial crossover rate for DE.\n            cr_final (float): Final crossover rate for DE.\n            f_init (float): Initial mutation factor for DE.\n            f_final (float): Final mutation factor for DE.\n            local_search_prob (float): Probability of performing local search.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.w_init = w_init\n        self.w_final = w_final\n        self.c1 = c1\n        self.c2 = c2\n        self.cr_init = cr_init\n        self.cr_final = cr_final\n        self.f_init = f_init\n        self.f_final = f_final\n        self.local_search_prob = local_search_prob\n\n        self.particles = None\n        self.velocities = None\n        self.fitness = None\n        self.personal_best_positions = None\n        self.personal_best_fitness = None\n        self.global_best_position = None\n        self.global_best_fitness = np.inf\n        self.func_evals = 0\n        self.iteration = 0\n\n    def initialize_population(self, func):\n        \"\"\"\n        Initializes the population randomly within the bounds.\n\n        Args:\n            func: The black-box optimization function.\n        \"\"\"\n        self.particles = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, size=(self.pop_size, self.dim))  # Initialize velocities\n        self.fitness = np.array([func(x) for x in self.particles])\n        self.func_evals += self.pop_size  #update function evaluations\n        self.personal_best_positions = self.particles.copy()\n        self.personal_best_fitness = self.fitness.copy()\n\n        self.global_best_position = self.particles[np.argmin(self.fitness)].copy()\n        self.global_best_fitness = np.min(self.fitness)\n\n\n    def update_global_best(self):\n        \"\"\"\n        Updates the global best position and fitness.\n        \"\"\"\n        best_index = np.argmin(self.fitness)\n        if self.fitness[best_index] < self.global_best_fitness:\n            self.global_best_fitness = self.fitness[best_index]\n            self.global_best_position = self.particles[best_index].copy()\n\n    def update_parameters(self):\n        \"\"\"\n        Updates PSO and DE parameters adaptively.\n        \"\"\"\n        # Linear decreasing inertia weight\n        self.w = self.w_init - (self.w_init - self.w_final) * (self.func_evals / self.budget)\n        \n        # Linear increasing Crossover Rate\n        self.cr = self.cr_init + (self.cr_final - self.cr_init) * (self.func_evals / self.budget)\n        \n        # Linear increasing Mutation Factor\n        self.f = self.f_init + (self.f_final - self.f_init) * (self.func_evals / self.budget)\n\n\n    def pso_update(self, func):\n        \"\"\"\n        Updates the particles' positions and velocities using PSO rules.\n\n        Args:\n            func: The black-box optimization function.\n        \"\"\"\n        for i in range(self.pop_size):\n            r1 = np.random.rand(self.dim)\n            r2 = np.random.rand(self.dim)\n\n            self.velocities[i] = (self.w * self.velocities[i]\n                                  + self.c1 * r1 * (self.personal_best_positions[i] - self.particles[i])\n                                  + self.c2 * r2 * (self.global_best_position - self.particles[i]))\n\n            self.particles[i] = self.particles[i] + self.velocities[i]\n\n            # Boundary handling: clip particles to the bounds\n            self.particles[i] = np.clip(self.particles[i], func.bounds.lb, func.bounds.ub)\n\n            f = func(self.particles[i])\n            self.func_evals += 1\n            self.fitness[i] = f\n\n            if f < self.personal_best_fitness[i]:\n                self.personal_best_fitness[i] = f\n                self.personal_best_positions[i] = self.particles[i].copy()\n\n    def de_mutation(self, func):\n         \"\"\"\n         Applies differential evolution mutation to the population.\n\n         Args:\n            func: The black-box optimization function.\n         \"\"\"\n         for i in range(self.pop_size):\n            # Choose three random indices, distinct from each other and i\n            idxs = np.random.choice(self.pop_size, 3, replace=False)\n            while i in idxs:\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n           \n            x1, x2, x3 = self.particles[idxs[0]], self.particles[idxs[1]], self.particles[idxs[2]]\n           \n            # Mutation\n            v = x1 + self.f * (x2 - x3)\n            v = np.clip(v, func.bounds.lb, func.bounds.ub)\n           \n            # Crossover\n            jrand = np.random.randint(self.dim)  # Randomly choose one index to always crossover\n            u = np.array([v[j] if (np.random.rand() < self.cr or j == jrand) else self.particles[i][j] for j in range(self.dim)])\n           \n            f = func(u)\n            self.func_evals += 1\n            \n            if f < self.fitness[i]:\n                self.fitness[i] = f\n                self.particles[i] = u.copy()\n                if f < self.personal_best_fitness[i]:\n                    self.personal_best_fitness[i] = f\n                    self.personal_best_positions[i] = self.particles[i].copy()\n\n    def local_search(self, func):\n        \"\"\"\n        Performs a local search around the global best solution.\n\n        Args:\n            func: The black-box optimization function.\n        \"\"\"\n        for i in range(self.dim):\n            # Generate a small perturbation\n            perturbation = np.random.normal(0, 0.05)  # Adjust stddev as needed\n            new_position = self.global_best_position.copy()\n            new_position[i] += perturbation\n\n            # Boundary handling\n            new_position = np.clip(new_position, func.bounds.lb, func.bounds.ub)\n\n            f = func(new_position)\n            self.func_evals += 1\n\n            if f < self.global_best_fitness:\n                self.global_best_fitness = f\n                self.global_best_position = new_position.copy()\n\n    def __call__(self, func):\n        \"\"\"\n        Optimizes the given black-box function using the Adaptive Hybrid PSO-DE algorithm.\n\n        Args:\n            func: The black-box optimization function.\n\n        Returns:\n            tuple: The best function value found and the corresponding solution.\n        \"\"\"\n        self.initialize_population(func)\n        self.update_global_best()\n\n        while self.func_evals < self.budget:\n            self.update_parameters()\n            self.pso_update(func)\n            self.de_mutation(func)\n            self.update_global_best()\n\n            if np.random.rand() < self.local_search_prob:\n                self.local_search(func)\n                self.update_global_best()\n                \n            # Early stopping condition (optional):\n            if self.global_best_fitness == -np.inf or self.global_best_fitness == np.inf or np.isnan(self.global_best_fitness):\n                break\n\n            self.iteration += 1\n\n        return self.global_best_fitness, self.global_best_position", "configspace": "", "generation": 1, "feedback": "Evaluation timed out after 300 seconds.", "error": "Evaluation timed out after 300 seconds.", "parent_ids": ["06376a10-7b13-4b4b-9705-685d36ecf581"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "b3123acc-b2e5-4059-b300-43148eb89d36", "fitness": 0.2857385773444451, "name": "SelfAdaptiveDE", "description": "A simplified self-adaptive Differential Evolution with archive and adaptive mutation factor, prioritizing exploration and efficient function evaluation within the budget.", "code": "import numpy as np\n\nclass SelfAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=40, archive_size=20, initial_f=0.5, cr=0.9, local_search_prob=0.05):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.F = initial_f  # Initial differential weight, adaptive later\n        self.CR = cr # Crossover rate\n        self.local_search_prob = local_search_prob\n        self.population = None\n        self.fitness = None\n        self.archive = None\n        self.archive_fitness = None\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        self.archive = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.archive_size, self.dim))\n        self.archive_fitness = np.full(self.archive_size, np.inf)\n\n    def mutate(self, i):\n        # Select three distinct individuals from the population and archive\n        indices = np.random.choice(self.pop_size + self.archive_size, 3, replace=False)\n        \n        if indices[0] < self.pop_size:\n            x_r1 = self.population[indices[0]]\n        else:\n            x_r1 = self.archive[indices[0] - self.pop_size]\n            \n        if indices[1] < self.pop_size:\n            x_r2 = self.population[indices[1]]\n        else:\n            x_r2 = self.archive[indices[1] - self.pop_size]\n\n        if indices[2] < self.pop_size:\n            x_r3 = self.population[indices[2]]\n        else:\n            x_r3 = self.archive[indices[2] - self.pop_size]\n\n        # Adaptive F: adjust based on success\n        self.F = np.clip(np.random.normal(self.F, 0.1), 0.1, 0.9)\n\n        x_mutated = self.population[i] + self.F * (x_r2 - x_r3)\n\n        return x_mutated\n\n    def crossover(self, x_mutated, i):\n        x_trial = np.copy(self.population[i])\n        j_rand = np.random.randint(self.dim)\n        for j in range(self.dim):\n            if np.random.rand() < self.CR or j == j_rand:\n                x_trial[j] = x_mutated[j]\n        return x_trial\n\n    def local_search(self, func, x):\n        x_neighbor = x + np.random.normal(0, 0.05, self.dim)  # Smaller perturbation\n        x_neighbor = np.clip(x_neighbor, func.bounds.lb, func.bounds.ub)\n        f_neighbor = func(x_neighbor)\n        self.budget -= 1\n        return f_neighbor, x_neighbor\n\n    def update_archive(self):\n        # Update the archive with diverse individuals from the population\n        # Replace a random element of the archive with a random element of the population\n        if self.archive_size > 0 and self.pop_size > 0: #Prevent errors when size is 0\n            archive_index = np.random.randint(self.archive_size)\n            population_index = np.random.randint(self.pop_size)\n            \n            if self.fitness[population_index] < self.archive_fitness[archive_index]:\n                self.archive[archive_index] = self.population[population_index].copy()\n                self.archive_fitness[archive_index] = self.fitness[population_index]\n\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)].copy()\n        \n        while self.budget > 0:\n            for i in range(self.pop_size):\n                x_mutated = self.mutate(i)\n                x_trial = self.crossover(x_mutated, i)\n\n                x_trial = np.clip(x_trial, func.bounds.lb, func.bounds.ub) #ensure bounds are not violated after crossover\n                f_trial = func(x_trial)\n                self.budget -= 1\n                \n                if self.budget <= 0:\n                  break # Exit inner loop\n\n                if np.random.rand() < self.local_search_prob: #Apply local search with a given probability\n                    f_ls, x_ls = self.local_search(func, x_trial)\n                    if f_ls < f_trial:\n                        f_trial = f_ls\n                        x_trial = x_ls.copy()\n\n\n                if f_trial < self.fitness[i]:\n                    self.population[i] = x_trial.copy()\n                    self.fitness[i] = f_trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = x_trial.copy()\n            if self.budget <= 0:\n                break # Exit outer loop\n\n            self.update_archive()\n\n            best_index = np.argmin(self.fitness)\n            if self.fitness[best_index] < self.f_opt:\n                self.f_opt = self.fitness[best_index]\n                self.x_opt = self.population[best_index].copy()\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "The algorithm SelfAdaptiveDE scored 0.286 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["59f780a6-0dd6-452f-998e-3a4daa71db8d"], "operator": null, "metadata": {"aucs": [0.17293125283576305, 0.348250313169686, 0.3824909765973269, 0.5250203441194493, 0]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "4b7f533f-c974-465e-b5e2-434130e93bb3", "fitness": "-inf", "name": "RingTopologyPSO_DE", "description": "A population-based algorithm that uses a ring topology to share information amongst particles, incorporating velocity clamping and adaptive parameter control for enhanced exploration and exploitation.", "code": "import numpy as np\n\nclass RingTopologyPSO_DE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, w_start=0.9, w_end=0.4, c1=2.0, c2=2.0, cr=0.7, f=0.8, v_max_ratio=0.2):\n        \"\"\"\n        Initializes the Ring Topology PSO-DE optimizer.\n\n        Args:\n            budget (int): The maximum number of function evaluations.\n            dim (int): The dimensionality of the problem.\n            pop_size (int): The size of the population.\n            w_start (float): Initial inertia weight for PSO.\n            w_end (float): Final inertia weight for PSO.\n            c1 (float): Cognitive coefficient for PSO.\n            c2 (float): Social coefficient for PSO.\n            cr (float): Crossover rate for DE.\n            f (float): Mutation factor for DE.\n            v_max_ratio (float): Maximum velocity as a ratio of the search space.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.w_start = w_start\n        self.w_end = w_end\n        self.c1 = c1\n        self.c2 = c2\n        self.cr = cr\n        self.f = f\n        self.v_max_ratio = v_max_ratio\n\n        self.particles = None\n        self.velocities = None\n        self.fitness = None\n        self.personal_best_positions = None\n        self.personal_best_fitness = None\n        self.neighbor_best_positions = None # Ring topology neighbor best\n        self.neighbor_best_fitness = None\n        self.func_evals = 0\n\n    def initialize_population(self, func):\n        \"\"\"\n        Initializes the population randomly within the bounds.\n\n        Args:\n            func: The black-box optimization function.\n        \"\"\"\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        self.particles = np.random.uniform(lb, ub, size=(self.pop_size, self.dim))\n        self.velocities = np.random.uniform(-self.v_max_ratio * (ub - lb), self.v_max_ratio * (ub - lb), size=(self.pop_size, self.dim))  # Initialize velocities\n\n        self.fitness = np.array([func(x) for x in self.particles])\n        self.func_evals += self.pop_size\n        self.personal_best_positions = self.particles.copy()\n        self.personal_best_fitness = self.fitness.copy()\n\n        self.neighbor_best_positions = np.zeros((self.pop_size, self.dim))\n        self.neighbor_best_fitness = np.full(self.pop_size, np.inf)\n        self.update_neighbor_best() # Initialize neighbor best\n\n    def update_neighbor_best(self):\n        \"\"\"\n        Updates the neighbor best position and fitness based on ring topology.\n        Each particle's neighbor is the particle to its left and right.\n        \"\"\"\n        for i in range(self.pop_size):\n            left_neighbor = (i - 1) % self.pop_size\n            right_neighbor = (i + 1) % self.pop_size\n\n            neighbors = [i, left_neighbor, right_neighbor]\n            best_neighbor = neighbors[np.argmin(self.fitness[neighbors])]\n\n            self.neighbor_best_positions[i] = self.particles[best_neighbor].copy()\n            self.neighbor_best_fitness[i] = self.fitness[best_neighbor]\n\n    def pso_update(self, func, iteration, total_iterations):\n        \"\"\"\n        Updates the particles' positions and velocities using PSO rules with ring topology.\n\n        Args:\n            func: The black-box optimization function.\n            iteration (int): Current iteration number.\n            total_iterations (int): Total number of iterations.\n        \"\"\"\n\n        # Adaptive inertia weight\n        w = self.w_start - (self.w_start - self.w_end) * (iteration / total_iterations)\n\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        v_max = self.v_max_ratio * (ub - lb)\n        for i in range(self.pop_size):\n            r1 = np.random.rand(self.dim)\n            r2 = np.random.rand(self.dim)\n\n            self.velocities[i] = (w * self.velocities[i]\n                                  + self.c1 * r1 * (self.personal_best_positions[i] - self.particles[i])\n                                  + self.c2 * r2 * (self.neighbor_best_positions[i] - self.particles[i]))\n\n            # Velocity clamping\n            self.velocities[i] = np.clip(self.velocities[i], -v_max, v_max)\n\n            self.particles[i] = self.particles[i] + self.velocities[i]\n\n            # Boundary handling: clip particles to the bounds\n            self.particles[i] = np.clip(self.particles[i], lb, ub)\n\n            f = func(self.particles[i])\n            self.func_evals += 1\n            self.fitness[i] = f\n\n            if f < self.personal_best_fitness[i]:\n                self.personal_best_fitness[i] = f\n                self.personal_best_positions[i] = self.particles[i].copy()\n\n\n    def de_mutation(self, func):\n        \"\"\"\n        Applies differential evolution mutation to the population.\n\n        Args:\n            func: The black-box optimization function.\n        \"\"\"\n        for i in range(self.pop_size):\n            # Choose three random indices, distinct from each other and i\n            idxs = np.random.choice(self.pop_size, 3, replace=False)\n            while i in idxs:\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n\n            x1, x2, x3 = self.particles[idxs[0]], self.particles[idxs[1]], self.particles[idxs[2]]\n\n            # Mutation\n            v = x1 + self.f * (x2 - x3)\n            v = np.clip(v, func.bounds.lb, func.bounds.ub)\n\n            # Crossover\n            jrand = np.random.randint(self.dim)  # Randomly choose one index to always crossover\n            u = np.array([v[j] if (np.random.rand() < self.cr or j == jrand) else self.particles[i][j] for j in range(self.dim)])\n\n            f = func(u)\n            self.func_evals += 1\n\n            if f < self.fitness[i]:\n                self.fitness[i] = f\n                self.particles[i] = u.copy()\n                if f < self.personal_best_fitness[i]:\n                    self.personal_best_fitness[i] = f\n                    self.personal_best_positions[i] = self.particles[i].copy()\n\n    def __call__(self, func):\n        \"\"\"\n        Optimizes the given black-box function using the Ring Topology PSO-DE algorithm.\n\n        Args:\n            func: The black-box optimization function.\n\n        Returns:\n            tuple: The best function value found and the corresponding solution.\n        \"\"\"\n        self.initialize_population(func)\n        total_iterations = self.budget // (2 * self.pop_size) # rough estimate, assuming PSO and DE both take pop_size evaluations per iter\n        iteration = 0\n\n        while self.func_evals < self.budget:\n            self.pso_update(func, iteration, total_iterations)\n            self.de_mutation(func)\n            self.update_neighbor_best() # Update neighbor best after each iteration\n            iteration += 1\n\n\n        best_index = np.argmin(self.personal_best_fitness)\n        return self.personal_best_fitness[best_index], self.personal_best_positions[best_index]", "configspace": "", "generation": 1, "feedback": "Evaluation timed out after 300 seconds.", "error": "Evaluation timed out after 300 seconds.", "parent_ids": ["06376a10-7b13-4b4b-9705-685d36ecf581"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "7bd6e0b3-d47a-4fc1-a7fa-623c84b452b7", "fitness": 0.3772076897633626, "name": "AdaptiveDE", "description": "Simplified Adaptive Differential Evolution with reduced parameter set and enhanced exploitation using a focused local search around the best candidate.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=30, initial_f=0.5, cr=0.9, local_search_prob=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = initial_f  # Initial differential weight, adaptive later\n        self.CR = cr # Crossover rate\n        self.local_search_prob = local_search_prob\n        self.population = None\n        self.fitness = None\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n\n    def mutate(self, i):\n        # Select three distinct individuals from the population\n        indices = np.random.choice(self.pop_size, 3, replace=False)\n        x_r1 = self.population[indices[0]]\n        x_r2 = self.population[indices[1]]\n        x_r3 = self.population[indices[2]]\n\n        # Adaptive F: adjust based on success\n        self.F = np.clip(np.random.normal(self.F, 0.1), 0.1, 0.9)\n\n        x_mutated = self.population[i] + self.F * (x_r2 - x_r3)\n\n        return x_mutated\n\n    def crossover(self, x_mutated, i):\n        x_trial = np.copy(self.population[i])\n        j_rand = np.random.randint(self.dim)\n        for j in range(self.dim):\n            if np.random.rand() < self.CR or j == j_rand:\n                x_trial[j] = x_mutated[j]\n        return x_trial\n\n    def local_search(self, func, x):\n        # Focused local search around x with smaller steps\n        x_neighbor = x + np.random.normal(0, 0.01, self.dim)  # Smaller perturbation\n        x_neighbor = np.clip(x_neighbor, func.bounds.lb, func.bounds.ub)\n        f_neighbor = func(x_neighbor)\n        self.budget -= 1\n        return f_neighbor, x_neighbor\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)].copy()\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                x_mutated = self.mutate(i)\n                x_trial = self.crossover(x_mutated, i)\n\n                x_trial = np.clip(x_trial, func.bounds.lb, func.bounds.ub) #ensure bounds are not violated after crossover\n                f_trial = func(x_trial)\n                self.budget -= 1\n                \n                if self.budget <= 0:\n                  break # Exit inner loop\n\n                if np.random.rand() < self.local_search_prob: #Apply local search with a given probability\n                    f_ls, x_ls = self.local_search(func, x_trial)\n                    if f_ls < f_trial:\n                        f_trial = f_ls\n                        x_trial = x_ls.copy()\n\n\n                if f_trial < self.fitness[i]:\n                    self.population[i] = x_trial.copy()\n                    self.fitness[i] = f_trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = x_trial.copy()\n            if self.budget <= 0:\n                break # Exit outer loop\n\n            best_index = np.argmin(self.fitness)\n            if self.fitness[best_index] < self.f_opt:\n                self.f_opt = self.fitness[best_index]\n                self.x_opt = self.population[best_index].copy()\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "The algorithm AdaptiveDE scored 0.377 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["b3123acc-b2e5-4059-b300-43148eb89d36"], "operator": null, "metadata": {"aucs": [0.16322105655194796, 0.36745297395875987, 0.4492864842935834, 0.7651659850769095, 0.39437841520771466, 0.55002617328335, 0.3281304297346359, 0]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "61d81577-e2b2-41dc-a411-381492005100", "fitness": 0.3442344426655522, "name": "HybridOptimizer", "description": "Adaptive Hybrid Optimization combining PSO and DE with a shrinking population for exploration-exploitation balance.", "code": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget=10000, dim=10, pop_size=40, w=0.7, c1=1.4, c2=1.4, cr=0.8, f=0.7, shrink_factor=0.95):\n        \"\"\"\n        Initializes the Hybrid Optimizer.\n\n        Args:\n            budget (int): The maximum number of function evaluations.\n            dim (int): The dimensionality of the problem.\n            pop_size (int): The initial size of the population.\n            w (float): Inertia weight for PSO.\n            c1 (float): Cognitive coefficient for PSO.\n            c2 (float): Social coefficient for PSO.\n            cr (float): Crossover rate for DE.\n            f (float): Mutation factor for DE.\n            shrink_factor (float): Factor by which population size shrinks.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.w = w\n        self.c1 = c1\n        self.c2 = c2\n        self.cr = cr\n        self.f = f\n        self.shrink_factor = shrink_factor\n\n        self.particles = None\n        self.velocities = None\n        self.fitness = None\n        self.personal_best_positions = None\n        self.personal_best_fitness = None\n        self.global_best_position = None\n        self.global_best_fitness = np.inf\n        self.func_evals = 0\n\n    def initialize_population(self, func):\n        \"\"\"\n        Initializes the population randomly within the bounds.\n\n        Args:\n            func: The black-box optimization function.\n        \"\"\"\n        self.particles = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, size=(self.pop_size, self.dim))  # Initialize velocities\n        self.fitness = np.array([func(x) for x in self.particles])\n        self.func_evals += self.pop_size  #update function evaluations\n        self.personal_best_positions = self.particles.copy()\n        self.personal_best_fitness = self.fitness.copy()\n\n        self.global_best_position = self.particles[np.argmin(self.fitness)].copy()\n        self.global_best_fitness = np.min(self.fitness)\n\n\n    def update_global_best(self):\n        \"\"\"\n        Updates the global best position and fitness.\n        \"\"\"\n        best_index = np.argmin(self.fitness)\n        if self.fitness[best_index] < self.global_best_fitness:\n            self.global_best_fitness = self.fitness[best_index]\n            self.global_best_position = self.particles[best_index].copy()\n\n    def update_particle(self, i, func):\n        \"\"\"\n        Updates a single particle's position and velocity using a blend of PSO and DE.\n\n        Args:\n            i (int): Index of the particle to update.\n            func: The black-box optimization function.\n        \"\"\"\n        # PSO velocity update\n        r1 = np.random.rand(self.dim)\n        r2 = np.random.rand(self.dim)\n        self.velocities[i] = (self.w * self.velocities[i]\n                              + self.c1 * r1 * (self.personal_best_positions[i] - self.particles[i])\n                              + self.c2 * r2 * (self.global_best_position - self.particles[i]))\n        \n        new_position = self.particles[i] + self.velocities[i]\n        new_position = np.clip(new_position, func.bounds.lb, func.bounds.ub)\n\n        new_fitness = func(new_position)\n        self.func_evals += 1\n        \n        if new_fitness < self.fitness[i]:\n            self.fitness[i] = new_fitness\n            self.particles[i] = new_position.copy()\n\n            if new_fitness < self.personal_best_fitness[i]:\n                self.personal_best_fitness[i] = new_fitness\n                self.personal_best_positions[i] = self.particles[i].copy()\n    \n    def shrink_population(self):\n        \"\"\"Shrinks the population size.\"\"\"\n        new_pop_size = int(self.pop_size * self.shrink_factor)\n        if new_pop_size < 10:  # Minimum population size\n            return\n\n        # Select the best individuals to keep\n        sorted_indices = np.argsort(self.fitness)\n        best_indices = sorted_indices[:new_pop_size]\n\n        self.particles = self.particles[best_indices].copy()\n        self.velocities = self.velocities[best_indices].copy()\n        self.fitness = self.fitness[best_indices].copy()\n        self.personal_best_positions = self.personal_best_positions[best_indices].copy()\n        self.personal_best_fitness = self.personal_best_fitness[best_indices].copy()\n        self.pop_size = new_pop_size\n\n\n    def __call__(self, func):\n        \"\"\"\n        Optimizes the given black-box function using the Hybrid Optimizer algorithm.\n\n        Args:\n            func: The black-box optimization function.\n\n        Returns:\n            tuple: The best function value found and the corresponding solution.\n        \"\"\"\n        self.initialize_population(func)\n        self.update_global_best()\n\n        generation = 0\n        while self.func_evals < self.budget:\n            for i in range(self.pop_size):\n                self.update_particle(i, func)\n            self.update_global_best()\n\n            if generation % 10 == 0:\n                self.shrink_population()\n\n            generation += 1\n        return self.global_best_fitness, self.global_best_position", "configspace": "", "generation": 2, "feedback": "The algorithm HybridOptimizer scored 0.344 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["00eae025-1129-4952-9fd3-ee30aa1ba18b"], "operator": null, "metadata": {"aucs": [0.13336712052956223, 0.19944171295246227, 0.3220893240389491, 0.2436135112869341, 0.22928250702004727, 0.2436299148741038, 0.27761182564427067, 0.3542467031146014, 0.2634255782606182, 0.2444917533065809, 0.5246313213471705, 0.9955143272751588, 0.2710737472997943, 0.2739287628421675, 0.647932234734954, 0.35258053532454725, 0.2616446136463335, 0.3773363772858763, 0.21213875076903466, 0.4567082317578772]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "913e8daf-61e9-4eb3-8abb-bece999c9dee", "fitness": "-inf", "name": "SOM_PSO_DE", "description": "A hybrid algorithm employing a Self-Organizing Map (SOM) to adaptively adjust PSO parameters based on population diversity, combined with a DE mutation strategy for enhanced exploration.", "code": "import numpy as np\n\nclass SOM_PSO_DE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, som_grid_size=10, w_init=0.9, c1_init=1.5, c2_init=1.5, cr=0.7, f=0.8):\n        \"\"\"\n        Initializes the SOM-PSO-DE optimizer.\n\n        Args:\n            budget (int): The maximum number of function evaluations.\n            dim (int): The dimensionality of the problem.\n            pop_size (int): The size of the population.\n            som_grid_size (int): The size of the SOM grid (som_grid_size x som_grid_size).\n            w_init (float): Initial inertia weight for PSO.\n            c1_init (float): Initial cognitive coefficient for PSO.\n            c2_init (float): Initial social coefficient for PSO.\n            cr (float): Crossover rate for DE.\n            f (float): Mutation factor for DE.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.som_grid_size = som_grid_size\n        self.w = w_init\n        self.c1 = c1_init\n        self.c2 = c2_init\n        self.cr = cr\n        self.f = f\n\n        self.particles = None\n        self.velocities = None\n        self.fitness = None\n        self.personal_best_positions = None\n        self.personal_best_fitness = None\n        self.global_best_position = None\n        self.global_best_fitness = np.inf\n        self.func_evals = 0\n\n        # Initialize SOM\n        self.som = np.random.rand(som_grid_size, som_grid_size, 3)  # 3 features: w, c1, c2\n        self.learning_rate_som = 0.1\n        self.sigma_som = som_grid_size / 2\n\n    def initialize_population(self, func):\n        \"\"\"\n        Initializes the population randomly within the bounds.\n\n        Args:\n            func: The black-box optimization function.\n        \"\"\"\n        self.particles = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, size=(self.pop_size, self.dim))  # Initialize velocities\n        self.fitness = np.array([func(x) for x in self.particles])\n        self.func_evals += self.pop_size  #update function evaluations\n        self.personal_best_positions = self.particles.copy()\n        self.personal_best_fitness = self.fitness.copy()\n\n        self.global_best_position = self.particles[np.argmin(self.fitness)].copy()\n        self.global_best_fitness = np.min(self.fitness)\n\n    def update_global_best(self):\n        \"\"\"\n        Updates the global best position and fitness.\n        \"\"\"\n        best_index = np.argmin(self.fitness)\n        if self.fitness[best_index] < self.global_best_fitness:\n            self.global_best_fitness = self.fitness[best_index]\n            self.global_best_position = self.particles[best_index].copy()\n\n    def pso_update(self, func):\n        \"\"\"\n        Updates the particles' positions and velocities using PSO rules.\n\n        Args:\n            func: The black-box optimization function.\n        \"\"\"\n        for i in range(self.pop_size):\n            # Find best matching unit (BMU) in SOM based on particle diversity\n            diversity = np.std(self.particles, axis=1)\n            bmu = self.find_bmu(diversity[i])  # Diversity as input feature\n            w, c1, c2 = self.som[bmu[0], bmu[1]]\n\n            r1 = np.random.rand(self.dim)\n            r2 = np.random.rand(self.dim)\n\n            self.velocities[i] = (w * self.velocities[i]\n                                  + c1 * r1 * (self.personal_best_positions[i] - self.particles[i])\n                                  + c2 * r2 * (self.global_best_position - self.particles[i]))\n\n            self.particles[i] = self.particles[i] + self.velocities[i]\n\n            # Boundary handling: clip particles to the bounds\n            self.particles[i] = np.clip(self.particles[i], func.bounds.lb, func.bounds.ub)\n\n            f = func(self.particles[i])\n            self.func_evals += 1\n            self.fitness[i] = f\n\n            if f < self.personal_best_fitness[i]:\n                self.personal_best_fitness[i] = f\n                self.personal_best_positions[i] = self.particles[i].copy()\n\n    def de_mutation(self, func):\n         \"\"\"\n         Applies differential evolution mutation to the population.\n\n         Args:\n            func: The black-box optimization function.\n         \"\"\"\n         for i in range(self.pop_size):\n            # Choose three random indices, distinct from each other and i\n            idxs = np.random.choice(self.pop_size, 3, replace=False)\n            while i in idxs:\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n           \n            x1, x2, x3 = self.particles[idxs[0]], self.particles[idxs[1]], self.particles[idxs[2]]\n           \n            # Mutation\n            v = x1 + self.f * (x2 - x3)\n            v = np.clip(v, func.bounds.lb, func.bounds.ub)\n           \n            # Crossover\n            jrand = np.random.randint(self.dim)  # Randomly choose one index to always crossover\n            u = np.array([v[j] if (np.random.rand() < self.cr or j == jrand) else self.particles[i][j] for j in range(self.dim)])\n           \n            f = func(u)\n            self.func_evals += 1\n            \n            if f < self.fitness[i]:\n                self.fitness[i] = f\n                self.particles[i] = u.copy()\n                if f < self.personal_best_fitness[i]:\n                    self.personal_best_fitness[i] = f\n                    self.personal_best_positions[i] = self.particles[i].copy()\n\n    def find_bmu(self, diversity):\n        \"\"\"\n        Finds the best matching unit (BMU) in the SOM for a given diversity value.\n\n        Args:\n            diversity (float): The diversity value of a particle.\n\n        Returns:\n            tuple: The coordinates (row, col) of the BMU in the SOM grid.\n        \"\"\"\n        min_dist = np.inf\n        bmu = (0, 0)\n        for i in range(self.som_grid_size):\n            for j in range(self.som_grid_size):\n                # Calculate Euclidean distance between diversity and SOM node\n                dist = np.linalg.norm(np.array([self.w, self.c1, self.c2]) - self.som[i, j]) #Use global pso parameters here as feature vector\n                if dist < min_dist:\n                    min_dist = dist\n                    bmu = (i, j)\n        return bmu\n\n    def update_som(self):\n        \"\"\"\n        Updates the SOM based on the current population's diversity.\n        \"\"\"\n        #Calculate mean values for PSO parameters\n        w_mean = np.mean([self.som[i,j,0] for i in range(self.som_grid_size) for j in range(self.som_grid_size)])\n        c1_mean = np.mean([self.som[i,j,1] for i in range(self.som_grid_size) for j in range(self.som_grid_size)])\n        c2_mean = np.mean([self.som[i,j,2] for i in range(self.som_grid_size) for j in range(self.som_grid_size)])\n      \n        for i in range(self.som_grid_size):\n            for j in range(self.som_grid_size):\n                #Calculate distance from BMU\n                dist = np.sqrt((i - self.som_grid_size // 2)**2 + (j - self.som_grid_size // 2)**2)\n\n                #Calculate neighborhood function\n                neighborhood = np.exp(-dist**2 / (2 * self.sigma_som**2))\n\n                #Update SOM weights\n                self.som[i,j,0] += self.learning_rate_som * neighborhood * (self.w - self.som[i,j,0])\n                self.som[i,j,1] += self.learning_rate_som * neighborhood * (self.c1 - self.som[i,j,1])\n                self.som[i,j,2] += self.learning_rate_som * neighborhood * (self.c2 - self.som[i,j,2])\n                \n    def __call__(self, func):\n        \"\"\"\n        Optimizes the given black-box function using the SOM-PSO-DE algorithm.\n\n        Args:\n            func: The black-box optimization function.\n\n        Returns:\n            tuple: The best function value found and the corresponding solution.\n        \"\"\"\n        self.initialize_population(func)\n        self.update_global_best()\n\n        while self.func_evals < self.budget:\n            self.pso_update(func)\n            self.de_mutation(func)\n            self.update_global_best()\n            self.update_som()\n\n            # Early stopping condition (optional):\n            if self.global_best_fitness == -np.inf or self.global_best_fitness == np.inf or np.isnan(self.global_best_fitness):\n                break\n\n        return self.global_best_fitness, self.global_best_position", "configspace": "", "generation": 2, "feedback": "Evaluation timed out after 300 seconds.", "error": "Evaluation timed out after 300 seconds.", "parent_ids": ["06376a10-7b13-4b4b-9705-685d36ecf581"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "88a94d8e-71cf-4b05-bce1-3703f430c9e5", "fitness": 0.4155072030187341, "name": "SelfAdaptiveDE", "description": "A simplified self-adaptive Differential Evolution with adaptive CR and F, a smaller archive, and no local search for efficiency.", "code": "import numpy as np\n\nclass SelfAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=40, archive_size=10, f_min=0.1, f_max=0.9, cr_min=0.1, cr_max=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.f_min = f_min\n        self.f_max = f_max\n        self.cr_min = cr_min\n        self.cr_max = cr_max\n        self.population = None\n        self.fitness = None\n        self.archive = None\n        self.archive_fitness = None\n        self.F = np.random.uniform(f_min, f_max, size=pop_size)\n        self.CR = np.random.uniform(cr_min, cr_max, size=pop_size)\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        self.archive = np.copy(self.population[:self.archive_size]) # Initialize archive with good solutions\n        self.archive_fitness = np.copy(self.fitness[:self.archive_size])\n        \n    def mutate(self, i):\n        indices = np.random.choice(self.pop_size + self.archive_size, 3, replace=False)\n        \n        if indices[0] < self.pop_size:\n            x_r1 = self.population[indices[0]]\n        else:\n            x_r1 = self.archive[indices[0] - self.pop_size]\n            \n        if indices[1] < self.pop_size:\n            x_r2 = self.population[indices[1]]\n        else:\n            x_r2 = self.archive[indices[1] - self.pop_size]\n\n        if indices[2] < self.pop_size:\n            x_r3 = self.population[indices[2]]\n        else:\n            x_r3 = self.archive[indices[2] - self.pop_size]\n\n        x_mutated = self.population[i] + self.F[i] * (x_r2 - x_r3)\n\n        x_mutated = np.clip(x_mutated, -5.0, 5.0)\n        return x_mutated\n\n    def crossover(self, x_mutated, i):\n        x_trial = np.copy(self.population[i])\n        j_rand = np.random.randint(self.dim)\n        for j in range(self.dim):\n            if np.random.rand() < self.CR[i] or j == j_rand:\n                x_trial[j] = x_mutated[j]\n        return x_trial\n\n    def update_archive(self):\n        combined_population = np.concatenate((self.population, self.archive), axis=0)\n        combined_fitness = np.concatenate((self.fitness, self.archive_fitness))\n        \n        sorted_indices = np.argsort(combined_fitness)[:self.archive_size]\n        \n        self.archive = combined_population[sorted_indices].copy()\n        self.archive_fitness = combined_fitness[sorted_indices].copy()\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)].copy()\n        \n        generation = 0\n        while self.budget > 0:\n            generation += 1\n            for i in range(self.pop_size):\n                x_mutated = self.mutate(i)\n                x_trial = self.crossover(x_mutated, i)\n                \n                f_trial = func(x_trial)\n                self.budget -= 1\n\n                if f_trial < self.fitness[i]:\n                    self.population[i] = x_trial.copy()\n                    self.fitness[i] = f_trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = x_trial.copy()\n                        \n                #Adapt F and CR\n                if np.random.rand() < 0.1:\n                  self.F[i] = np.random.uniform(self.f_min, self.f_max)\n                if np.random.rand() < 0.1:\n                  self.CR[i] = np.random.uniform(self.cr_min, self.cr_max)\n                \n                if self.budget <= 0:\n                    break\n            \n            self.update_archive()\n            \n            if self.budget <= 0:\n                break\n        \n        best_index = np.argmin(self.fitness)\n        if self.fitness[best_index] < self.f_opt:\n            self.f_opt = self.fitness[best_index]\n            self.x_opt = self.population[best_index].copy()\n        \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "The algorithm SelfAdaptiveDE scored 0.416 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["59f780a6-0dd6-452f-998e-3a4daa71db8d"], "operator": null, "metadata": {"aucs": [0.1559006438292876, 0.2810411469856998, 0.38799201123376525, 0.5078412018925647, 0.344291185237913, 0.4526102772144388, 0.30016984985436734, 0.35825467284533197, 0.30583740249758984, 0.19490870434214835, 0.6008874554751932, 0.9993483444125177, 0.2818071440257759, 0.312431479775083, 0.75179427272405, 0.44983985324216447, 0.3525412338751026, 0.5756301103188286, 0.21139577816927735, 0.4856212924235831]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "dd26ffb2-881e-4ded-a882-489aeb2abc1f", "fitness": "-inf", "name": "AdaptiveHybridOptimizer", "description": "A hybrid optimizer that combines PSO and DE with adaptive parameter control and a selection mechanism based on success history to balance exploration and exploitation.", "code": "import numpy as np\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget=10000, dim=10, pop_size=40, w_init=0.9, w_final=0.4, c1=2.0, c2=2.0, cr_init=0.5, cr_final=0.9, f_init=0.5, f_final=0.9, p_adaptive=0.1):\n        \"\"\"\n        Initializes the Adaptive Hybrid Optimizer.\n\n        Args:\n            budget (int): The maximum number of function evaluations.\n            dim (int): The dimensionality of the problem.\n            pop_size (int): The size of the population.\n            w_init (float): Initial inertia weight for PSO.\n            w_final (float): Final inertia weight for PSO.\n            c1 (float): Cognitive coefficient for PSO.\n            c2 (float): Social coefficient for PSO.\n            cr_init (float): Initial crossover rate for DE.\n            cr_final (float): Final crossover rate for DE.\n            f_init (float): Initial mutation factor for DE.\n            f_final (float): Final mutation factor for DE.\n            p_adaptive (float): Probability of updating parameters adaptively.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.w_init = w_init\n        self.w_final = w_final\n        self.c1 = c1\n        self.c2 = c2\n        self.cr_init = cr_init\n        self.cr_final = cr_final\n        self.f_init = f_init\n        self.f_final = f_final\n        self.p_adaptive = p_adaptive\n\n        self.particles = None\n        self.velocities = None\n        self.fitness = None\n        self.personal_best_positions = None\n        self.personal_best_fitness = None\n        self.global_best_position = None\n        self.global_best_fitness = np.inf\n        self.func_evals = 0\n        self.success_history_cr = []\n        self.success_history_f = []\n\n        self.memory_cr = np.full(self.pop_size, cr_init)  #CR memory for each particle\n        self.memory_f = np.full(self.pop_size, f_init) # F memory for each particle\n        self.archive = [] # Archive for DE\n\n    def initialize_population(self, func):\n        \"\"\"\n        Initializes the population randomly within the bounds.\n\n        Args:\n            func: The black-box optimization function.\n        \"\"\"\n        self.particles = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, size=(self.pop_size, self.dim))  # Initialize velocities\n        self.fitness = np.array([func(x) for x in self.particles])\n        self.func_evals += self.pop_size  #update function evaluations\n        self.personal_best_positions = self.particles.copy()\n        self.personal_best_fitness = self.fitness.copy()\n\n        self.global_best_position = self.particles[np.argmin(self.fitness)].copy()\n        self.global_best_fitness = np.min(self.fitness)\n\n\n    def update_global_best(self):\n        \"\"\"\n        Updates the global best position and fitness.\n        \"\"\"\n        best_index = np.argmin(self.fitness)\n        if self.fitness[best_index] < self.global_best_fitness:\n            self.global_best_fitness = self.fitness[best_index]\n            self.global_best_position = self.particles[best_index].copy()\n\n    def update_particle(self, i, func, gen):\n        \"\"\"\n        Updates a single particle's position and velocity using a blend of PSO and DE.\n\n        Args:\n            i (int): Index of the particle to update.\n            func: The black-box optimization function.\n            gen: Current generation number.\n        \"\"\"\n        w = self.w_init - (self.w_init - self.w_final) * (gen / self.budget)\n        cr = self.memory_cr[i]\n        f = self.memory_f[i]\n\n\n        # PSO velocity update\n        r1 = np.random.rand(self.dim)\n        r2 = np.random.rand(self.dim)\n        self.velocities[i] = (w * self.velocities[i]\n                              + self.c1 * r1 * (self.personal_best_positions[i] - self.particles[i])\n                              + self.c2 * r2 * (self.global_best_position - self.particles[i]))\n        # DE mutation\n        idxs = np.random.choice(self.pop_size, 3, replace=False)\n        while i in idxs:\n            idxs = np.random.choice(self.pop_size, 3, replace=False)\n\n        x1, x2, x3 = self.particles[idxs[0]], self.particles[idxs[1]], self.particles[idxs[2]]\n        v = x1 + f * (x2 - x3)\n        v = np.clip(v, func.bounds.lb, func.bounds.ub)\n\n        # Crossover\n        jrand = np.random.randint(self.dim)\n        u = np.array([v[j] if (np.random.rand() < cr or j == jrand) else self.particles[i][j] for j in range(self.dim)])\n        \n        # Update position\n        new_position = self.particles[i] + self.velocities[i]\n        new_position = np.clip(new_position, func.bounds.lb, func.bounds.ub)\n\n        # Evaluate new position (using DE's mutated vector as the other alternative)\n        f_pso = func(new_position)\n        f_de = func(u)\n        self.func_evals += 2\n\n        #Greedy selection\n        if f_de < f_pso:\n             new_f = f_de\n             new_pos = u\n        else:\n             new_f = f_pso\n             new_pos = new_position\n        \n        delta_fitness = abs(self.fitness[i] - new_f)\n        \n        if new_f < self.fitness[i]:\n            self.success_history_cr.append(cr)\n            self.success_history_f.append(f)\n            self.archive.append(self.particles[i].copy()) # Add to archive\n            self.fitness[i] = new_f\n            self.particles[i] = new_pos.copy()\n\n            if new_f < self.personal_best_fitness[i]:\n                self.personal_best_fitness[i] = new_f\n                self.personal_best_positions[i] = self.particles[i].copy()\n\n\n        #Adapt CR and F\n\n        if np.random.rand() < self.p_adaptive: #update parameters\n\n          if self.success_history_cr:\n            cr = np.clip(np.mean(self.success_history_cr), 0.1, 0.9)\n          else:\n            cr = np.random.uniform(self.cr_init, self.cr_final) #If no success, use a random value\n\n          if self.success_history_f:\n            f = np.clip(np.mean(self.success_history_f), 0.1, 0.9)\n          else:\n            f = np.random.uniform(self.f_init, self.f_final)\n\n        self.memory_cr[i] = cr\n        self.memory_f[i] = f\n\n    def __call__(self, func):\n        \"\"\"\n        Optimizes the given black-box function using the Hybrid Optimizer algorithm.\n\n        Args:\n            func: The black-box optimization function.\n\n        Returns:\n            tuple: The best function value found and the corresponding solution.\n        \"\"\"\n        self.initialize_population(func)\n        self.update_global_best()\n        gen = 0\n        while self.func_evals < self.budget:\n            for i in range(self.pop_size):\n                self.update_particle(i, func, gen)\n            self.update_global_best()\n            gen +=1\n            self.success_history_cr = []\n            self.success_history_f = []\n\n\n        return self.global_best_fitness, self.global_best_position", "configspace": "", "generation": 2, "feedback": "Evaluation timed out after 300 seconds.", "error": "Evaluation timed out after 300 seconds.", "parent_ids": ["00eae025-1129-4952-9fd3-ee30aa1ba18b"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "b6f91b58-d5e8-43ff-8cd5-00ae102597fe", "fitness": "-inf", "name": "AdaptiveHybridOptimizer", "description": "Adaptive Hybrid Optimization with orthogonal learning, dynamically balancing PSO, DE, and orthogonal design for efficient exploration and exploitation.", "code": "import numpy as np\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget=10000, dim=10, pop_size=40, w_init=0.9, w_end=0.4, c1=1.5, c2=1.5, cr=0.9, f=0.6, ortho_ratio = 0.1):\n        \"\"\"\n        Initializes the Adaptive Hybrid Optimizer with orthogonal learning.\n\n        Args:\n            budget (int): The maximum number of function evaluations.\n            dim (int): The dimensionality of the problem.\n            pop_size (int): The size of the population.\n            w_init (float): Initial inertia weight for PSO.\n            w_end (float): Final inertia weight for PSO.\n            c1 (float): Cognitive coefficient for PSO.\n            c2 (float): Social coefficient for PSO.\n            cr (float): Crossover rate for DE.\n            f (float): Mutation factor for DE.\n            ortho_ratio(float): Ratio of orthogonal design based learning\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.w_init = w_init\n        self.w_end = w_end\n        self.c1 = c1\n        self.c2 = c2\n        self.cr = cr\n        self.f = f\n        self.ortho_ratio = ortho_ratio\n\n        self.particles = None\n        self.velocities = None\n        self.fitness = None\n        self.personal_best_positions = None\n        self.personal_best_fitness = None\n        self.global_best_position = None\n        self.global_best_fitness = np.inf\n        self.func_evals = 0\n\n    def initialize_population(self, func):\n        \"\"\"\n        Initializes the population randomly within the bounds.\n\n        Args:\n            func: The black-box optimization function.\n        \"\"\"\n        self.particles = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, size=(self.pop_size, self.dim))  # Initialize velocities\n        self.fitness = np.array([func(x) for x in self.particles])\n        self.func_evals += self.pop_size  #update function evaluations\n        self.personal_best_positions = self.particles.copy()\n        self.personal_best_fitness = self.fitness.copy()\n\n        self.global_best_position = self.particles[np.argmin(self.fitness)].copy()\n        self.global_best_fitness = np.min(self.fitness)\n\n\n    def update_global_best(self):\n        \"\"\"\n        Updates the global best position and fitness.\n        \"\"\"\n        best_index = np.argmin(self.fitness)\n        if self.fitness[best_index] < self.global_best_fitness:\n            self.global_best_fitness = self.fitness[best_index]\n            self.global_best_position = self.particles[best_index].copy()\n\n    def update_particle(self, i, func, iter, max_iter):\n        \"\"\"\n        Updates a single particle's position using a blend of PSO, DE, and Orthogonal Learning.\n\n        Args:\n            i (int): Index of the particle to update.\n            func: The black-box optimization function.\n            iter(int): current iteration\n            max_iter(int): max iteration\n        \"\"\"\n        #Adaptive Inertia Weight\n        w = self.w_init - (self.w_init - self.w_end) * (iter / max_iter)\n        \n        # PSO velocity update\n        r1 = np.random.rand(self.dim)\n        r2 = np.random.rand(self.dim)\n        self.velocities[i] = (w * self.velocities[i]\n                              + self.c1 * r1 * (self.personal_best_positions[i] - self.particles[i])\n                              + self.c2 * r2 * (self.global_best_position - self.particles[i]))\n        \n        new_position = self.particles[i] + self.velocities[i]\n        new_position = np.clip(new_position, func.bounds.lb, func.bounds.ub)\n        \n        # DE mutation\n        idxs = np.random.choice(self.pop_size, 3, replace=False)\n        while i in idxs:\n            idxs = np.random.choice(self.pop_size, 3, replace=False)\n\n        x1, x2, x3 = self.particles[idxs[0]], self.particles[idxs[1]], self.particles[idxs[2]]\n        v = x1 + self.f * (x2 - x3)\n        v = np.clip(v, func.bounds.lb, func.bounds.ub)\n\n        # Crossover\n        jrand = np.random.randint(self.dim)\n        u = np.array([v[j] if (np.random.rand() < self.cr or j == jrand) else self.particles[i][j] for j in range(self.dim)])\n        \n\n        #Orthogonal Learning\n        if np.random.rand() < self.ortho_ratio:\n           L = self.generate_orthogonal_array(self.dim)\n           levels = [self.particles[i][j] for j in range(self.dim)] \n           positions = []\n           fitnesses = []\n           for row in L:\n             pos = np.zeros(self.dim)\n             for j in range(self.dim):\n                pos[j] = levels[j] * row[j] #simplified Orthogonal Design\n             pos = np.clip(pos, func.bounds.lb, func.bounds.ub)    \n             positions.append(pos)\n             fitnesses.append(func(pos))\n             self.func_evals += 1\n\n           best_index = np.argmin(fitnesses)\n           best_ortho_position = positions[best_index]\n           best_ortho_fitness = fitnesses[best_index]\n           \n           if best_ortho_fitness < self.fitness[i]:\n                new_f = best_ortho_fitness\n                new_pos = best_ortho_position\n           else:\n                new_f = self.fitness[i]\n                new_pos = self.particles[i]\n\n\n        else:    \n            # Evaluate new position (using DE's mutated vector as the other alternative)\n            f_pso = func(new_position)\n            f_de = func(u)\n            self.func_evals += 2\n\n            #Greedy selection\n            if f_de < f_pso:\n                 new_f = f_de\n                 new_pos = u\n            else:\n                 new_f = f_pso\n                 new_pos = new_position\n\n        #Overall update\n        if 'new_f' in locals() and 'new_pos' in locals():   \n            if new_f < self.fitness[i]:\n                self.fitness[i] = new_f\n                self.particles[i] = new_pos.copy()\n\n                if new_f < self.personal_best_fitness[i]:\n                    self.personal_best_fitness[i] = new_f\n                    self.personal_best_positions[i] = self.particles[i].copy()\n\n    def generate_orthogonal_array(self, factors, levels=2):\n        \"\"\"\n        Generates a simple orthogonal array (L4).\n        \"\"\"\n        if factors == 1:\n            return np.array([[-1], [1],[-1], [1]])\n        elif factors == 2:\n             return np.array([[-1, -1], [-1, 1], [1, -1], [1, 1]])\n        elif factors == 3:\n             return np.array([[-1, -1, -1], [-1, 1, 1], [1, -1, 1], [1, 1, -1]]) \n        elif factors == 4:\n             return np.array([[-1, -1, -1, -1], [-1, -1, 1, 1], [-1, 1, -1, 1], [-1, 1, 1, -1]])\n        elif factors == 5:\n            return np.array([[-1, -1, -1, -1, -1], [-1, -1, 1, 1, 1], [-1, 1, -1, 1, 1], [-1, 1, 1, -1, -1]])\n        elif factors == 6:\n            return np.array([[-1, -1, -1, -1, -1, -1], [-1, -1, 1, 1, 1, 1], [-1, 1, -1, 1, 1, 1], [-1, 1, 1, -1, -1, -1]])\n        elif factors == 7:\n             return np.array([[-1, -1, -1, -1, -1, -1, -1], [-1, -1, 1, 1, 1, 1, 1], [-1, 1, -1, 1, 1, 1, 1], [-1, 1, 1, -1, -1, -1, -1]])\n        elif factors == 8:\n             return np.array([[-1, -1, -1, -1, -1, -1, -1, -1], [-1, -1, 1, 1, 1, 1, 1, 1], [-1, 1, -1, 1, 1, 1, 1, 1], [-1, 1, 1, -1, -1, -1, -1, -1]])\n        elif factors == 9:\n             return np.array([[-1, -1, -1, -1, -1, -1, -1, -1, -1], [-1, -1, 1, 1, 1, 1, 1, 1, 1], [-1, 1, -1, 1, 1, 1, 1, 1, 1], [-1, 1, 1, -1, -1, -1, -1, -1, -1]])\n        elif factors == 10:\n            return np.array([[-1, -1, -1, -1, -1, -1, -1, -1, -1, -1], [-1, -1, 1, 1, 1, 1, 1, 1, 1, 1], [-1, 1, -1, 1, 1, 1, 1, 1, 1, 1], [-1, 1, 1, -1, -1, -1, -1, -1, -1, -1]])\n        else:\n            return np.array([[-1, -1], [-1, 1], [1, -1], [1, 1]])\n\n\n    def __call__(self, func):\n        \"\"\"\n        Optimizes the given black-box function using the Adaptive Hybrid Optimizer algorithm.\n\n        Args:\n            func: The black-box optimization function.\n\n        Returns:\n            tuple: The best function value found and the corresponding solution.\n        \"\"\"\n        self.initialize_population(func)\n        self.update_global_best()\n\n        max_iter = self.budget // self.pop_size\n        iter = 0\n        while self.func_evals < self.budget:\n            for i in range(self.pop_size):\n                self.update_particle(i, func, iter, max_iter)\n            self.update_global_best()\n            iter += 1\n\n\n        return self.global_best_fitness, self.global_best_position", "configspace": "", "generation": 2, "feedback": "Evaluation timed out after 300 seconds.", "error": "Evaluation timed out after 300 seconds.", "parent_ids": ["00eae025-1129-4952-9fd3-ee30aa1ba18b"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "3b35783d-ba87-412b-b5a2-b1109afef207", "fitness": 0.0, "name": "AdaptiveDEORLSR", "description": "An adaptive Differential Evolution strategy with a success-history based parameter adaptation, orthogonal learning and stochastic ranking.", "code": "import numpy as np\n\nclass AdaptiveDEORLSR:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=25, memory_size=10, local_search_iterations=5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.memory_size = memory_size\n        self.local_search_iterations = local_search_iterations\n        self.F = 0.5 * np.ones(self.memory_size)\n        self.CR = 0.5 * np.ones(self.memory_size)\n        self.memory_idx = 0\n        self.population = None\n        self.fitness = None\n        self.archive = None\n\n        self.sf = np.zeros(self.memory_size)\n        self.scr = np.zeros(self.memory_size)\n        self.success_count = 0\n\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        self.archive = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.archive_size, self.dim))\n        self.archive_fitness = np.full(self.archive_size, np.inf)\n\n    def mutate(self, func, i):\n        # Strategy: DE/rand/1 with archive\n        indices = np.random.choice(self.pop_size + self.archive_size, 3, replace=False)\n\n        if indices[0] < self.pop_size:\n            x_r1 = self.population[indices[0]]\n        else:\n            x_r1 = self.archive[indices[0] - self.pop_size]\n\n        if indices[1] < self.pop_size:\n            x_r2 = self.population[indices[1]]\n        else:\n            x_r2 = self.archive[indices[1] - self.pop_size]\n\n        if indices[2] < self.pop_size:\n            x_r3 = self.population[indices[2]]\n        else:\n            x_r3 = self.archive[indices[2] - self.pop_size]\n\n        # Sample F and CR from memory\n        f = np.random.choice(self.F)\n        cr = np.random.choice(self.CR)\n\n        x_mutated = self.population[i] + f * (x_r2 - x_r3)\n\n        x_mutated = np.clip(x_mutated, func.bounds.lb, func.bounds.ub)\n        return x_mutated, cr\n\n    def crossover(self, x_mutated, i, cr):\n        x_trial = np.copy(self.population[i])\n        j_rand = np.random.randint(self.dim)\n        for j in range(self.dim):\n            if np.random.rand() < cr or j == j_rand:\n                x_trial[j] = x_mutated[j]\n        return x_trial\n\n    def orthogonal_learning(self, func, x):\n        # Generate an orthogonal array\n        # This example uses a simple 2-level orthogonal array\n        oa = np.array([[-1, -1], [-1, 1], [1, -1], [1, 1]])\n\n        best_x = x.copy()\n        best_f = func(x)\n        self.budget -= 1\n        if self.budget <= 0:\n            return best_f, best_x\n\n        for row in oa:\n            x_neighbor = x.copy()\n            for j in range(self.dim):\n              if np.random.rand() < 0.1: # Small prob to change each dimension\n                x_neighbor[j] += row[np.random.randint(0, 2)] * 0.05 #Small perturbation\n            x_neighbor = np.clip(x_neighbor, func.bounds.lb, func.bounds.ub)\n\n            f_neighbor = func(x_neighbor)\n            self.budget -= 1\n\n            if self.budget <= 0:\n                return best_f, best_x\n\n            if f_neighbor < best_f:\n                best_f = f_neighbor\n                best_x = x_neighbor.copy()\n        return best_f, best_x\n\n    def stochastic_ranking(self, f1, f2, p_cr=0.45):\n        if (f1 < 0 and f2 < 0) or (np.random.rand() > p_cr):\n            return f1 < f2\n        else:\n            return f1 < f2\n\n    def update_memory(self, success, f, cr):\n        self.sf[self.memory_idx] = f\n        self.scr[self.memory_idx] = cr\n\n        if success:\n            self.F[self.memory_idx] = f\n            self.CR[self.memory_idx] = cr\n            self.success_count += 1\n\n        self.memory_idx = (self.memory_idx + 1) % self.memory_size\n\n    def update_archive(self):\n        indices = np.argsort(self.fitness)[-self.archive_size:]\n        for i, idx in enumerate(indices):\n            if self.fitness[idx] < np.max(self.archive_fitness):\n                worst_archive_index = np.argmax(self.archive_fitness)\n                self.archive[worst_archive_index] = self.population[idx].copy()\n                self.archive_fitness[worst_archive_index] = self.fitness[idx]\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)].copy()\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                x_mutated, cr = self.mutate(func, i)\n                x_trial = self.crossover(x_mutated, i, cr)\n\n                f_trial = func(x_trial)\n                self.budget -= 1\n\n                if self.budget <= 0:\n                    break\n\n                # Orthogonal learning\n                f_ol, x_ol = self.orthogonal_learning(func, x_trial)\n\n                if self.stochastic_ranking(f_ol, self.fitness[i]):\n                    success = True\n                    self.population[i] = x_ol.copy()\n                    self.fitness[i] = f_ol\n\n                    if f_ol < self.f_opt:\n                        self.f_opt = f_ol\n                        self.x_opt = x_ol.copy()\n\n                else:\n                    success = False\n\n\n                self.update_memory(success, self.F[self.memory_idx-1], self.CR[self.memory_idx-1])\n\n\n            if self.budget <= 0:\n                break\n\n            self.update_archive()\n\n            best_index = np.argmin(self.fitness)\n            if self.fitness[best_index] < self.f_opt:\n                self.f_opt = self.fitness[best_index]\n                self.x_opt = self.population[best_index].copy()\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "The algorithm AdaptiveDEORLSR scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["59f780a6-0dd6-452f-998e-3a4daa71db8d"], "operator": null, "metadata": {"aucs": [0]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "f89c2094-f598-4fca-9f8c-e2515fc55795", "fitness": "-inf", "name": "AdaptiveHybridOptimizer", "description": "Adaptive Hybrid Optimizer that dynamically adjusts PSO and DE parameters based on performance feedback, prioritizing exploration early and exploitation later.", "code": "import numpy as np\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget=10000, dim=10, pop_size=40, w_init=0.9, w_final=0.2, c1_init=1.5, c1_final=0.5, c2_init=0.5, c2_final=1.5, cr_init=0.3, cr_final=0.9, f_init=0.5, f_final=0.9):\n        \"\"\"\n        Initializes the Adaptive Hybrid Optimizer.\n\n        Args:\n            budget (int): The maximum number of function evaluations.\n            dim (int): The dimensionality of the problem.\n            pop_size (int): The size of the population.\n            w_init (float): Initial inertia weight for PSO.\n            w_final (float): Final inertia weight for PSO.\n            c1_init (float): Initial cognitive coefficient for PSO.\n            c1_final (float): Final cognitive coefficient for PSO.\n            c2_init (float): Initial social coefficient for PSO.\n            c2_final (float): Final social coefficient for PSO.\n            cr_init (float): Initial crossover rate for DE.\n            cr_final (float): Final crossover rate for DE.\n            f_init (float): Initial mutation factor for DE.\n            f_final (float): Final mutation factor for DE.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.w_init = w_init\n        self.w_final = w_final\n        self.c1_init = c1_init\n        self.c1_final = c1_final\n        self.c2_init = c2_init\n        self.c2_final = c2_final\n        self.cr_init = cr_init\n        self.cr_final = cr_final\n        self.f_init = f_init\n        self.f_final = f_final\n\n        self.particles = None\n        self.velocities = None\n        self.fitness = None\n        self.personal_best_positions = None\n        self.personal_best_fitness = None\n        self.global_best_position = None\n        self.global_best_fitness = np.inf\n        self.func_evals = 0\n        self.generation = 0\n\n\n    def initialize_population(self, func):\n        \"\"\"\n        Initializes the population randomly within the bounds.\n\n        Args:\n            func: The black-box optimization function.\n        \"\"\"\n        self.particles = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, size=(self.pop_size, self.dim))  # Initialize velocities\n        self.fitness = np.array([func(x) for x in self.particles])\n        self.func_evals += self.pop_size  #update function evaluations\n        self.personal_best_positions = self.particles.copy()\n        self.personal_best_fitness = self.fitness.copy()\n\n        self.global_best_position = self.particles[np.argmin(self.fitness)].copy()\n        self.global_best_fitness = np.min(self.fitness)\n\n    def update_global_best(self):\n        \"\"\"\n        Updates the global best position and fitness.\n        \"\"\"\n        best_index = np.argmin(self.fitness)\n        if self.fitness[best_index] < self.global_best_fitness:\n            self.global_best_fitness = self.fitness[best_index]\n            self.global_best_position = self.particles[best_index].copy()\n\n    def get_adaptive_params(self):\n        \"\"\"\n        Calculates the adaptive parameters based on the current generation.\n        \"\"\"\n        progress = self.func_evals / self.budget\n        w = self.w_init - (self.w_init - self.w_final) * progress\n        c1 = self.c1_init - (self.c1_init - self.c1_final) * progress\n        c2 = self.c2_init - (self.c2_init - self.c2_final) * progress\n        cr = self.cr_init + (self.cr_final - self.cr_init) * progress\n        f = self.f_init + (self.f_final - self.f_init) * progress\n        return w, c1, c2, cr, f\n\n    def update_particle(self, i, func):\n        \"\"\"\n        Updates a single particle's position and velocity using a blend of PSO and DE.\n\n        Args:\n            i (int): Index of the particle to update.\n            func: The black-box optimization function.\n        \"\"\"\n        w, c1, c2, cr, f = self.get_adaptive_params()\n\n        # PSO velocity update\n        r1 = np.random.rand(self.dim)\n        r2 = np.random.rand(self.dim)\n        self.velocities[i] = (w * self.velocities[i]\n                              + c1 * r1 * (self.personal_best_positions[i] - self.particles[i])\n                              + c2 * r2 * (self.global_best_position - self.particles[i]))\n        \n        # Clip velocities (important for exploration and preventing divergence)\n        v_max = (func.bounds.ub - func.bounds.lb) * 0.1  #Limit velocity to 10% of search space\n        self.velocities[i] = np.clip(self.velocities[i], -v_max, v_max)\n\n        # Update position\n        new_position = self.particles[i] + self.velocities[i]\n        new_position = np.clip(new_position, func.bounds.lb, func.bounds.ub)\n        \n        # DE mutation with crossover\n        idxs = np.random.choice(self.pop_size, 3, replace=False)\n        while i in idxs:\n            idxs = np.random.choice(self.pop_size, 3, replace=False)\n\n        x1, x2, x3 = self.particles[idxs[0]], self.particles[idxs[1]], self.particles[idxs[2]]\n        v = x1 + f * (x2 - x3)\n        v = np.clip(v, func.bounds.lb, func.bounds.ub)\n\n        jrand = np.random.randint(self.dim)\n        u = np.array([v[j] if (np.random.rand() < cr or j == jrand) else new_position[j] for j in range(self.dim)]) # use new_position here.\n\n        # Evaluate new position\n        f_u = func(u)\n        self.func_evals += 1\n        if f_u < self.fitness[i]:\n            self.fitness[i] = f_u\n            self.particles[i] = u.copy()\n\n            if f_u < self.personal_best_fitness[i]:\n                self.personal_best_fitness[i] = f_u\n                self.personal_best_positions[i] = self.particles[i].copy()\n\n\n\n    def __call__(self, func):\n        \"\"\"\n        Optimizes the given black-box function using the Adaptive Hybrid Optimizer algorithm.\n\n        Args:\n            func: The black-box optimization function.\n\n        Returns:\n            tuple: The best function value found and the corresponding solution.\n        \"\"\"\n        self.initialize_population(func)\n        self.update_global_best()\n\n        while self.func_evals < self.budget:\n            for i in range(self.pop_size):\n                self.update_particle(i, func)\n            self.update_global_best()\n            self.generation += 1\n\n\n        return self.global_best_fitness, self.global_best_position", "configspace": "", "generation": 2, "feedback": "Evaluation timed out after 300 seconds.", "error": "Evaluation timed out after 300 seconds.", "parent_ids": ["00eae025-1129-4952-9fd3-ee30aa1ba18b"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "46647a49-c7e3-49dc-90b8-ce2bf4ee948b", "fitness": 0.43885823492027015, "name": "SelfAdaptiveDE", "description": "Simplified Self-Adaptive Differential Evolution with periodic population refreshment and focused local search, balancing exploration and exploitation efficiently.", "code": "import numpy as np\n\nclass SelfAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=30, archive_size=15, initial_f=0.6, cr=0.8, local_search_prob=0.1, refresh_interval=500):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.F = initial_f  # Initial differential weight\n        self.CR = cr  # Crossover rate\n        self.local_search_prob = local_search_prob\n        self.refresh_interval = refresh_interval\n        self.population = None\n        self.fitness = None\n        self.archive = None\n        self.archive_fitness = None\n        self.eval_count = 0\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.eval_count += self.pop_size\n        self.archive = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.archive_size, self.dim))\n        self.archive_fitness = np.full(self.archive_size, np.inf)\n\n    def mutate(self, i):\n        indices = np.random.choice(self.pop_size + self.archive_size, 3, replace=False)\n        \n        x_r1 = self.population[indices[0]] if indices[0] < self.pop_size else self.archive[indices[0] - self.pop_size]\n        x_r2 = self.population[indices[1]] if indices[1] < self.pop_size else self.archive[indices[1] - self.pop_size]\n        x_r3 = self.population[indices[2]] if indices[2] < self.pop_size else self.archive[indices[2] - self.pop_size]\n\n        self.F = np.clip(np.random.normal(self.F, 0.1), 0.1, 0.9)\n        x_mutated = self.population[i] + self.F * (x_r2 - x_r3)\n        return x_mutated\n\n    def crossover(self, x_mutated, i):\n        x_trial = np.copy(self.population[i])\n        j_rand = np.random.randint(self.dim)\n        for j in range(self.dim):\n            if np.random.rand() < self.CR or j == j_rand:\n                x_trial[j] = x_mutated[j]\n        return x_trial\n\n    def local_search(self, func, x):\n        x_neighbor = x + np.random.normal(0, 0.05, self.dim)\n        x_neighbor = np.clip(x_neighbor, func.bounds.lb, func.bounds.ub)\n        f_neighbor = func(x_neighbor)\n        self.eval_count += 1\n        return f_neighbor, x_neighbor\n\n    def update_archive(self):\n        if self.archive_size > 0 and self.pop_size > 0:\n            archive_indices = np.random.choice(self.archive_size, self.pop_size, replace=True)\n            for i in range(self.pop_size):\n                if self.fitness[i] < self.archive_fitness[archive_indices[i]]:\n                    self.archive[archive_indices[i]] = self.population[i].copy()\n                    self.archive_fitness[archive_indices[i]] = self.fitness[i]\n\n    def refresh_population(self, func):\n        # Replace bottom 20% of population with random individuals\n        num_to_replace = int(0.2 * self.pop_size)\n        worst_indices = np.argsort(self.fitness)[-num_to_replace:]\n        for i in worst_indices:\n            self.population[i] = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n            self.fitness[i] = func(self.population[i])\n            self.eval_count += 1\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)].copy()\n\n        while self.eval_count < self.budget:\n            for i in range(self.pop_size):\n                x_mutated = self.mutate(i)\n                x_trial = self.crossover(x_mutated, i)\n                x_trial = np.clip(x_trial, func.bounds.lb, func.bounds.ub)\n                f_trial = func(x_trial)\n                self.eval_count += 1\n                \n                if self.eval_count >= self.budget:\n                    break\n\n                if np.random.rand() < self.local_search_prob:\n                    f_ls, x_ls = self.local_search(func, x_trial)\n                    if f_ls < f_trial:\n                        f_trial = f_ls\n                        x_trial = x_ls.copy()\n\n                if f_trial < self.fitness[i]:\n                    self.population[i] = x_trial.copy()\n                    self.fitness[i] = f_trial\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = x_trial.copy()\n\n            if self.eval_count >= self.budget:\n                break\n\n            self.update_archive()\n\n            if self.eval_count % self.refresh_interval == 0:\n                self.refresh_population(func)\n\n            best_index = np.argmin(self.fitness)\n            if self.fitness[best_index] < self.f_opt:\n                self.f_opt = self.fitness[best_index]\n                self.x_opt = self.population[best_index].copy()\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "The algorithm SelfAdaptiveDE scored 0.439 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["b3123acc-b2e5-4059-b300-43148eb89d36"], "operator": null, "metadata": {"aucs": [0.1651429119305461, 0.374951230834623, 0.450129984966111, 0.7601047910803455, 0.3960584920059409, 0.5503432893436648, 0.30540917960400005, 0.39770559329562305, 0.43914779087020184, 0.22122153013851276, 0.6894978425566303, 0.9988975261625574, 0.3954051260950252, 0]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "26d3bbd6-7e95-4811-bac8-c08411385b77", "fitness": 0.0, "name": "SelfAdaptiveDE", "description": "A streamlined Self-Adaptive Differential Evolution with reduced population diversity loss using a combined archive update strategy and simplified parameter adaptation.", "code": "import numpy as np\n\nclass SelfAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=30, archive_size=15, initial_f=0.6, cr=0.8):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.F = initial_f\n        self.CR = cr\n        self.population = None\n        self.fitness = None\n        self.archive = None\n        self.archive_fitness = None\n        self.best_solution = None\n        self.best_fitness = np.inf\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        self.best_solution = self.population[np.argmin(self.fitness)].copy()\n        self.best_fitness = np.min(self.fitness)\n\n        self.archive = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.archive_size, self.dim))\n        self.archive_fitness = np.full(self.archive_size, np.inf)\n\n    def mutate(self, i):\n        indices = np.random.choice(self.pop_size, 3, replace=False)\n        x_r1, x_r2, x_r3 = self.population[indices[0]], self.population[indices[1]], self.population[indices[2]]\n\n        # Simplified F adaptation\n        self.F = np.clip(np.random.normal(self.F, 0.1), 0.1, 1.0)\n\n        x_mutated = self.population[i] + self.F * (x_r2 - x_r3)\n        return x_mutated\n\n    def crossover(self, x_mutated, i):\n        x_trial = np.copy(self.population[i])\n        crossover_points = np.random.rand(self.dim) < self.CR\n        x_trial[crossover_points] = x_mutated[crossover_points]\n        return x_trial\n\n    def update_archive(self):\n        # Combined Archive Update: Replace worst in archive or replace random, based on fitness.\n        if self.archive_size > 0 and self.pop_size > 0:\n            for i in range(self.pop_size):\n                if self.fitness[i] < np.max(self.archive_fitness):\n                    worst_archive_index = np.argmax(self.archive_fitness)\n                    if self.fitness[i] < self.archive_fitness[worst_archive_index]:\n                        self.archive[worst_archive_index] = self.population[i].copy()\n                        self.archive_fitness[worst_archive_index] = self.fitness[i]\n\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                x_mutated = self.mutate(i)\n                x_trial = self.crossover(x_mutated, i)\n                x_trial = np.clip(x_trial, func.bounds.lb, func.bounds.ub)\n                f_trial = func(x_trial)\n                self.budget -= 1\n\n                if f_trial < self.fitness[i]:\n                    self.population[i] = x_trial.copy()\n                    self.fitness[i] = f_trial\n\n                    if f_trial < self.best_fitness:\n                        self.best_fitness = f_trial\n                        self.best_solution = x_trial.copy()\n            \n            if self.budget <= 0:\n                break\n\n            self.update_archive()\n\n        return self.best_fitness, self.best_solution", "configspace": "", "generation": 3, "feedback": "The algorithm SelfAdaptiveDE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["b3123acc-b2e5-4059-b300-43148eb89d36"], "operator": null, "metadata": {"aucs": [0]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "0d3f88c0-20d7-4bdf-a049-236f193cdb85", "fitness": "-inf", "name": "AdaptiveHybridOptimizer", "description": "Adaptive Hybrid Optimization using a combined PSO-DE update with dynamic parameter adaptation and population management, focusing on maintaining diversity and intensifying search based on performance.", "code": "import numpy as np\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget=10000, dim=10, pop_size=50, w_init=0.9, w_end=0.4, c1=2.0, c2=2.0, cr_init=0.5, cr_end=0.9, f_init=0.5, f_end=0.9, local_search_prob=0.1):\n        \"\"\"\n        Initializes the Adaptive Hybrid Optimizer.\n\n        Args:\n            budget (int): The maximum number of function evaluations.\n            dim (int): The dimensionality of the problem.\n            pop_size (int): The initial size of the population.\n            w_init (float): Initial inertia weight for PSO.\n            w_end (float): Final inertia weight for PSO.\n            c1 (float): Cognitive coefficient for PSO.\n            c2 (float): Social coefficient for PSO.\n            cr_init (float): Initial crossover rate for DE.\n            cr_end (float): Final crossover rate for DE.\n            f_init (float): Initial mutation factor for DE.\n            f_end (float): Final mutation factor for DE.\n            local_search_prob (float): Probability of applying local search.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.w_init = w_init\n        self.w_end = w_end\n        self.c1 = c1\n        self.c2 = c2\n        self.cr_init = cr_init\n        self.cr_end = cr_end\n        self.f_init = f_init\n        self.f_end = f_end\n        self.local_search_prob = local_search_prob\n\n        self.particles = None\n        self.velocities = None\n        self.fitness = None\n        self.personal_best_positions = None\n        self.personal_best_fitness = None\n        self.global_best_position = None\n        self.global_best_fitness = np.inf\n        self.func_evals = 0\n\n    def initialize_population(self, func):\n        \"\"\"\n        Initializes the population randomly within the bounds.\n\n        Args:\n            func: The black-box optimization function.\n        \"\"\"\n        self.particles = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, size=(self.pop_size, self.dim))  # Initialize velocities\n        self.fitness = np.array([func(x) for x in self.particles])\n        self.func_evals += self.pop_size  #update function evaluations\n        self.personal_best_positions = self.particles.copy()\n        self.personal_best_fitness = self.fitness.copy()\n\n        self.global_best_position = self.particles[np.argmin(self.fitness)].copy()\n        self.global_best_fitness = np.min(self.fitness)\n\n\n    def update_global_best(self):\n        \"\"\"\n        Updates the global best position and fitness.\n        \"\"\"\n        best_index = np.argmin(self.fitness)\n        if self.fitness[best_index] < self.global_best_fitness:\n            self.global_best_fitness = self.fitness[best_index]\n            self.global_best_position = self.particles[best_index].copy()\n\n    def update_particle(self, i, func, generation, total_generations):\n        \"\"\"\n        Updates a single particle's position and velocity using a blend of PSO and DE.\n\n        Args:\n            i (int): Index of the particle to update.\n            func: The black-box optimization function.\n            generation (int): Current generation number.\n            total_generations (int): Total number of generations.\n        \"\"\"\n        # Adaptive parameters\n        w = self.w_init - (self.w_init - self.w_end) * (generation / total_generations)\n        cr = self.cr_init + (self.cr_end - self.cr_init) * (generation / total_generations)\n        f = self.f_init + (self.f_end - self.f_init) * (generation / total_generations)\n        \n        # PSO velocity update\n        r1 = np.random.rand(self.dim)\n        r2 = np.random.rand(self.dim)\n        self.velocities[i] = (w * self.velocities[i]\n                              + self.c1 * r1 * (self.personal_best_positions[i] - self.particles[i])\n                              + self.c2 * r2 * (self.global_best_position - self.particles[i]))\n        \n        new_position = self.particles[i] + self.velocities[i]\n        new_position = np.clip(new_position, func.bounds.lb, func.bounds.ub)\n\n        # DE crossover\n        if np.random.rand() < cr:\n            donor_vector = self.particles[i].copy()\n            idxs = np.random.choice(self.pop_size, 3, replace=False)\n            x1, x2, x3 = self.particles[idxs[0]], self.particles[idxs[1]], self.particles[idxs[2]]\n            donor_vector += f * (x2 - x3)\n            donor_vector = np.clip(donor_vector, func.bounds.lb, func.bounds.ub)\n            \n            j_rand = np.random.randint(self.dim)\n            for j in range(self.dim):\n                if np.random.rand() > cr and j != j_rand:\n                    donor_vector[j] = self.particles[i][j]\n\n            new_position = donor_vector\n\n        new_fitness = func(new_position)\n        self.func_evals += 1\n        \n        if new_fitness < self.fitness[i]:\n            self.fitness[i] = new_fitness\n            self.particles[i] = new_position.copy()\n\n            if new_fitness < self.personal_best_fitness[i]:\n                self.personal_best_fitness[i] = new_fitness\n                self.personal_best_positions[i] = self.particles[i].copy()\n                \n        # Local search around best\n        if np.random.rand() < self.local_search_prob:\n            perturbation = np.random.normal(0, 0.05 * (func.bounds.ub - func.bounds.lb), size=self.dim)\n            local_position = self.personal_best_positions[i] + perturbation\n            local_position = np.clip(local_position, func.bounds.lb, func.bounds.ub)\n            local_fitness = func(local_position)\n            self.func_evals += 1\n\n            if local_fitness < self.personal_best_fitness[i]:\n                self.personal_best_fitness[i] = local_fitness\n                self.personal_best_positions[i] = local_position.copy()\n                if local_fitness < self.global_best_fitness:\n                    self.global_best_fitness = local_fitness\n                    self.global_best_position = local_position.copy()\n\n\n    def __call__(self, func):\n        \"\"\"\n        Optimizes the given black-box function using the Adaptive Hybrid Optimizer algorithm.\n\n        Args:\n            func: The black-box optimization function.\n\n        Returns:\n            tuple: The best function value found and the corresponding solution.\n        \"\"\"\n        self.initialize_population(func)\n        self.update_global_best()\n\n        total_generations = self.budget // self.pop_size\n        generation = 0\n        while self.func_evals < self.budget:\n            for i in range(self.pop_size):\n                self.update_particle(i, func, generation, total_generations)\n            self.update_global_best()\n            generation += 1\n        return self.global_best_fitness, self.global_best_position", "configspace": "", "generation": 3, "feedback": "Evaluation timed out after 300 seconds.", "error": "Evaluation timed out after 300 seconds.", "parent_ids": ["61d81577-e2b2-41dc-a411-381492005100"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "421f2f35-ca0d-4f72-a076-25771fd8cec0", "fitness": 0.36835917811571095, "name": "EnhancedSelfAdaptiveDE", "description": "Enhanced Self-Adaptive Differential Evolution with orthogonal crossover, dynamic F/CR adaptation, and a diversity maintenance strategy to prevent premature convergence.", "code": "import numpy as np\n\nclass EnhancedSelfAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=40, archive_size=10, f_min=0.1, f_max=0.9, cr_min=0.1, cr_max=0.9, diversity_threshold=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.f_min = f_min\n        self.f_max = f_max\n        self.cr_min = cr_min\n        self.cr_max = cr_max\n        self.diversity_threshold = diversity_threshold\n        self.population = None\n        self.fitness = None\n        self.archive = None\n        self.archive_fitness = None\n        self.F = np.random.uniform(f_min, f_max, size=pop_size)\n        self.CR = np.random.uniform(cr_min, cr_max, size=pop_size)\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        self.archive = np.copy(self.population[:self.archive_size]) # Initialize archive with good solutions\n        self.archive_fitness = np.copy(self.fitness[:self.archive_size])\n        \n    def mutate(self, i):\n        indices = np.random.choice(self.pop_size + self.archive_size, 3, replace=False)\n        \n        if indices[0] < self.pop_size:\n            x_r1 = self.population[indices[0]]\n        else:\n            x_r1 = self.archive[indices[0] - self.pop_size]\n            \n        if indices[1] < self.pop_size:\n            x_r2 = self.population[indices[1]]\n        else:\n            x_r2 = self.archive[indices[1] - self.pop_size]\n\n        if indices[2] < self.pop_size:\n            x_r3 = self.population[indices[2]]\n        else:\n            x_r3 = self.archive[indices[2] - self.pop_size]\n\n        x_mutated = self.population[i] + self.F[i] * (x_r2 - x_r3)\n\n        x_mutated = np.clip(x_mutated, -5.0, 5.0)\n        return x_mutated\n\n    def crossover(self, x_mutated, i):\n        x_trial = np.copy(self.population[i])\n        \n        # Orthogonal Crossover\n        num_changes = np.random.randint(1, self.dim + 1)\n        indices = np.random.choice(self.dim, num_changes, replace=False)\n        \n        for j in indices:\n             if np.random.rand() < self.CR[i]:\n                 x_trial[j] = x_mutated[j]\n        \n        return x_trial\n\n    def update_archive(self):\n        combined_population = np.concatenate((self.population, self.archive), axis=0)\n        combined_fitness = np.concatenate((self.fitness, self.archive_fitness))\n        \n        sorted_indices = np.argsort(combined_fitness)[:self.archive_size]\n        \n        self.archive = combined_population[sorted_indices].copy()\n        self.archive_fitness = combined_fitness[sorted_indices].copy()\n        \n    def calculate_diversity(self):\n        centroid = np.mean(self.population, axis=0)\n        distances = np.linalg.norm(self.population - centroid, axis=1)\n        mean_distance = np.mean(distances)\n        return mean_distance\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)].copy()\n        \n        generation = 0\n        while self.budget > 0:\n            generation += 1\n            for i in range(self.pop_size):\n                x_mutated = self.mutate(i)\n                x_trial = self.crossover(x_mutated, i)\n                \n                f_trial = func(x_trial)\n                self.budget -= 1\n\n                if f_trial < self.fitness[i]:\n                    self.population[i] = x_trial.copy()\n                    self.fitness[i] = f_trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = x_trial.copy()\n                        \n                #Adapt F and CR based on success\n                if f_trial < self.fitness[i]:\n                  self.F[i] = 0.9 * self.F[i] + 0.1 * np.random.uniform(self.f_min, self.f_max)\n                  self.CR[i] = 0.9 * self.CR[i] + 0.1 * np.random.uniform(self.cr_min, self.cr_max)\n                else:\n                  self.F[i] = np.random.uniform(self.f_min, self.f_max)\n                  self.CR[i] = np.random.uniform(self.cr_min, self.cr_max)\n\n                \n                if self.budget <= 0:\n                    break\n            \n            self.update_archive()\n            \n            # Diversity Maintenance\n            diversity = self.calculate_diversity()\n            if diversity < self.diversity_threshold:\n                # Reset a portion of the population to improve diversity\n                num_reset = int(self.pop_size * 0.2)\n                indices_to_reset = np.random.choice(self.pop_size, num_reset, replace=False)\n                self.population[indices_to_reset] = np.random.uniform(-5.0, 5.0, size=(num_reset, self.dim))\n                self.fitness[indices_to_reset] = np.array([func(x) for x in self.population[indices_to_reset]])\n                self.budget -= num_reset\n\n            \n            if self.budget <= 0:\n                break\n        \n        best_index = np.argmin(self.fitness)\n        if self.fitness[best_index] < self.f_opt:\n            self.f_opt = self.fitness[best_index]\n            self.x_opt = self.population[best_index].copy()\n        \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "The algorithm EnhancedSelfAdaptiveDE scored 0.368 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["88a94d8e-71cf-4b05-bce1-3703f430c9e5"], "operator": null, "metadata": {"aucs": [0.13956435287556945, 0.26670418577749677, 0.3735458403829114, 0.2847019191736032, 0.28632176425545364, 0.4013289644838146, 0.2976978698654098, 0.31975760846289913, 0.2909279687104711, 0.19030359372071504, 0.3431736480054509, 0.9832547107796824, 0.3181546444179749, 0.2854869184121791, 0.6994676785401135, 0.393926786855875, 0.3416871591088292, 0.4454909965929953, 0.19146706978680972, 0.5142198821059649]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "1e1663ff-c964-49a6-92bf-7f26a3a695ff", "fitness": "-inf", "name": "SelfAdaptiveDE_OD", "description": "Self-adaptive Differential Evolution with orthogonal design-based mutation and adaptive parameter control for improved exploration and exploitation.", "code": "import numpy as np\n\nclass SelfAdaptiveDE_OD:\n    def __init__(self, budget=10000, dim=10, pop_size=40, archive_size=10, f_min=0.1, f_max=0.9, cr_min=0.1, cr_max=0.9, od_samples=5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.f_min = f_min\n        self.f_max = f_max\n        self.cr_min = cr_min\n        self.cr_max = cr_max\n        self.od_samples = od_samples\n        self.population = None\n        self.fitness = None\n        self.archive = None\n        self.archive_fitness = None\n        self.F = np.random.uniform(f_min, f_max, size=pop_size)\n        self.CR = np.random.uniform(cr_min, cr_max, size=pop_size)\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        self.archive = np.copy(self.population[:self.archive_size]) # Initialize archive with good solutions\n        self.archive_fitness = np.copy(self.fitness[:self.archive_size])\n\n    def mutate(self, i):\n        # Orthogonal Design-based Mutation\n        orthogonal_matrix = self.generate_orthogonal_matrix(self.od_samples, 3) # 3 factors: x_r1, x_r2, x_r3\n        mutated_vectors = []\n        for row in orthogonal_matrix:\n            indices = np.random.choice(self.pop_size + self.archive_size, 3, replace=False)\n\n            if indices[0] < self.pop_size:\n                x_r1 = self.population[indices[0]]\n            else:\n                x_r1 = self.archive[indices[0] - self.pop_size]\n\n            if indices[1] < self.pop_size:\n                x_r2 = self.population[indices[1]]\n            else:\n                x_r2 = self.archive[indices[1] - self.pop_size]\n\n            if indices[2] < self.pop_size:\n                x_r3 = self.population[indices[2]]\n            else:\n                x_r3 = self.archive[indices[2] - self.pop_size]\n\n            # Apply orthogonal design factors to the difference vector\n            x_mutated = self.population[i] + self.F[i] * (row[1] * x_r2 - row[2] * x_r3) # row[0] is unused, but kept for consistency, r1 not part of diff\n            x_mutated = np.clip(x_mutated, -5.0, 5.0)\n            mutated_vectors.append(x_mutated)\n\n        return mutated_vectors\n\n    def generate_orthogonal_matrix(self, num_samples, num_factors):\n        # A simple orthogonal design (can be replaced with more sophisticated designs)\n        if num_factors == 3:\n             if num_samples == 5: #Added check to avoid returning out of bounds\n                 return np.array([\n                     [0, -1, -1],\n                     [0, -1, 1],\n                     [0, 1, -1],\n                     [0, 1, 1],\n                     [0, 0, 0]\n                 ])\n             elif num_samples >= 4:\n                  return np.array([\n                     [0, -1, -1],\n                     [0, -1, 1],\n                     [0, 1, -1],\n                     [0, 1, 1]\n                     ])\n             else:\n                return np.array([[0,0,0]])\n\n\n    def crossover(self, x_mutated, i):\n        x_trial = np.copy(self.population[i])\n        j_rand = np.random.randint(self.dim)\n        for j in range(self.dim):\n            if np.random.rand() < self.CR[i] or j == j_rand:\n                x_trial[j] = x_mutated[0][j] # Use the first mutated vector for crossover\n        return x_trial\n\n    def update_archive(self):\n        combined_population = np.concatenate((self.population, self.archive), axis=0)\n        combined_fitness = np.concatenate((self.fitness, self.archive_fitness))\n\n        sorted_indices = np.argsort(combined_fitness)[:self.archive_size]\n\n        self.archive = combined_population[sorted_indices].copy()\n        self.archive_fitness = combined_fitness[sorted_indices].copy()\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)].copy()\n\n        generation = 0\n        while self.budget > 0:\n            generation += 1\n            for i in range(self.pop_size):\n                mutated_vectors = self.mutate(i) # Get multiple mutated vectors from OD\n                best_trial_fitness = float('inf')\n                best_trial_vector = None\n\n                # Evaluate all mutated vectors and choose the best one for crossover\n                for x_mutated in mutated_vectors:\n                    x_trial = self.crossover(x_mutated, i)\n\n                    f_trial = func(x_trial)\n                    self.budget -= 1\n\n                    if f_trial < best_trial_fitness:\n                        best_trial_fitness = f_trial\n                        best_trial_vector = x_trial.copy()\n\n                    if self.budget <= 0:\n                        break\n                if best_trial_vector is not None:\n                    if best_trial_fitness < self.fitness[i]:\n                        self.population[i] = best_trial_vector.copy()\n                        self.fitness[i] = best_trial_fitness\n\n                        if best_trial_fitness < self.f_opt:\n                            self.f_opt = best_trial_fitness\n                            self.x_opt = best_trial_vector.copy()\n\n                #Adapt F and CR\n                if np.random.rand() < 0.1:\n                  self.F[i] = np.random.uniform(self.f_min, self.f_max)\n                if np.random.rand() < 0.1:\n                  self.CR[i] = np.random.uniform(self.cr_min, self.cr_max)\n\n                if self.budget <= 0:\n                    break\n\n            self.update_archive()\n\n            if self.budget <= 0:\n                break\n\n        best_index = np.argmin(self.fitness)\n        if self.fitness[best_index] < self.f_opt:\n            self.f_opt = self.fitness[best_index]\n            self.x_opt = self.population[best_index].copy()\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "In the code, line 84, in crossover, the following error occurred:\nIndexError: invalid index to scalar variable.\nOn line: x_trial[j] = x_mutated[0][j] # Use the first mutated vector for crossover", "error": "In the code, line 84, in crossover, the following error occurred:\nIndexError: invalid index to scalar variable.\nOn line: x_trial[j] = x_mutated[0][j] # Use the first mutated vector for crossover", "parent_ids": ["88a94d8e-71cf-4b05-bce1-3703f430c9e5"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "044e7751-17ec-44d0-9822-6c2e2ced8022", "fitness": "-inf", "name": "ImprovedSelfAdaptiveDE", "description": "Improved Self-Adaptive Differential Evolution with orthogonal design-based mutation and periodic population rejuvenation for enhanced exploration and exploitation.", "code": "import numpy as np\nfrom scipy.stats import levy\n\nclass ImprovedSelfAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=40, archive_size=10, f_min=0.1, f_max=0.9, cr_min=0.1, cr_max=0.9, ortho_groups=3):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.f_min = f_min\n        self.f_max = f_max\n        self.cr_min = cr_min\n        self.cr_max = cr_max\n        self.ortho_groups = ortho_groups  # Number of orthogonal groups\n        self.population = None\n        self.fitness = None\n        self.archive = None\n        self.archive_fitness = None\n        self.F = np.random.uniform(f_min, f_max, size=pop_size)\n        self.CR = np.random.uniform(cr_min, cr_max, size=pop_size)\n        self.bounds_lb = -5.0\n        self.bounds_ub = 5.0\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        self.archive = np.copy(self.population[:self.archive_size]) # Initialize archive with good solutions\n        self.archive_fitness = np.copy(self.fitness[:self.archive_size])\n        \n    def mutate(self, i):\n        # Orthogonal design-based mutation\n        group_size = self.dim // self.ortho_groups\n        x_mutated = np.copy(self.population[i])\n\n        for group in range(self.ortho_groups):\n            start_index = group * group_size\n            end_index = (group + 1) * group_size if group < self.ortho_groups - 1 else self.dim\n            indices = np.arange(start_index, end_index)\n            \n            if len(indices) > 1:\n                np.random.shuffle(indices)\n                x_r1 = self.population[np.random.choice(self.pop_size)]\n                x_r2 = self.archive[np.random.choice(self.archive_size)] if self.archive_size > 0 else self.population[np.random.choice(self.pop_size)] #Use population if archive is empty\n                x_mutated[indices] += self.F[i] * (x_r1[indices] - x_r2[indices])\n\n        # Levy flight-based mutation for exploration\n        if np.random.rand() < 0.05:\n            step_size = 0.01 * levy.rvs(0.5, size=self.dim)\n            x_mutated += step_size\n                \n        x_mutated = np.clip(x_mutated, self.bounds_lb, self.bounds_ub)\n        return x_mutated\n\n    def crossover(self, x_mutated, i):\n        x_trial = np.copy(self.population[i])\n        j_rand = np.random.randint(self.dim)\n        for j in range(self.dim):\n            if np.random.rand() < self.CR[i] or j == j_rand:\n                x_trial[j] = x_mutated[j]\n        return x_trial\n\n    def update_archive(self):\n        combined_population = np.concatenate((self.population, self.archive), axis=0)\n        combined_fitness = np.concatenate((self.fitness, self.archive_fitness))\n        \n        sorted_indices = np.argsort(combined_fitness)[:self.archive_size]\n        \n        self.archive = combined_population[sorted_indices].copy()\n        self.archive_fitness = combined_fitness[sorted_indices].copy()\n        \n    def rejuvenate_population(self):\n        # Rejuvenate a portion of the population with random solutions\n        num_rejuvenate = int(0.1 * self.pop_size)\n        indices = np.random.choice(self.pop_size, num_rejuvenate, replace=False)\n        self.population[indices] = np.random.uniform(self.bounds_lb, self.bounds_ub, size=(num_rejuvenate, self.dim))\n        #Evaluate fitness of rejuvenated population\n        for i in indices:\n          self.fitness[i] = np.inf #Mark as un-evaluated so fitness is updated next DE iteration\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)].copy()\n        \n        generation = 0\n        rejuvenation_interval = 50 #Rejuvenate every 50 generations\n        while self.budget > 0:\n            generation += 1\n            for i in range(self.pop_size):\n                x_mutated = self.mutate(i)\n                x_trial = self.crossover(x_mutated, i)\n                \n                f_trial = func(x_trial)\n                self.budget -= 1\n\n                if f_trial < self.fitness[i]:\n                    self.population[i] = x_trial.copy()\n                    self.fitness[i] = f_trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = x_trial.copy()\n                        \n                #Adapt F and CR\n                if np.random.rand() < 0.1:\n                  self.F[i] = np.random.uniform(self.f_min, self.f_max)\n                if np.random.rand() < 0.1:\n                  self.CR[i] = np.random.uniform(self.cr_min, self.cr_max)\n                \n                if self.budget <= 0:\n                    break\n            \n            self.update_archive()\n            \n            if self.budget <= 0:\n                break\n            \n            if generation % rejuvenation_interval == 0:\n                self.rejuvenate_population()\n                #Re-evaluate fitness of rejuvenated population after rejuvenation\n                for i in range(self.pop_size):\n                    if self.fitness[i] == np.inf: #Check for un-evaluated solutions (due to rejuvenation)\n                        self.fitness[i] = func(self.population[i])\n                        self.budget -= 1\n                        if self.fitness[i] < self.f_opt:\n                            self.f_opt = self.fitness[i]\n                            self.x_opt = self.population[i].copy()\n                        if self.budget <= 0:\n                            break\n        \n        best_index = np.argmin(self.fitness)\n        if self.fitness[best_index] < self.f_opt:\n            self.f_opt = self.fitness[best_index]\n            self.x_opt = self.population[best_index].copy()\n        \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "In the code, line 49, in mutate, the following error occurred:\nNameError: name 'levy' is not defined\nOn line: step_size = 0.01 * levy.rvs(0.5, size=self.dim)", "error": "In the code, line 49, in mutate, the following error occurred:\nNameError: name 'levy' is not defined\nOn line: step_size = 0.01 * levy.rvs(0.5, size=self.dim)", "parent_ids": ["88a94d8e-71cf-4b05-bce1-3703f430c9e5"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "555dcdca-320a-4ce9-b585-17f051aba286", "fitness": 0.672582603817351, "name": "AdaptiveDE", "description": "Enhanced Adaptive Differential Evolution with simplified parameter adaptation and focused exploitation using a larger population and a success-history based adaptation of F and CR.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, initial_f=0.5, initial_cr=0.9, archive_size=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = initial_f  # Initial differential weight\n        self.CR = initial_cr # Crossover rate\n        self.population = None\n        self.fitness = None\n        self.archive = []\n        self.archive_size = archive_size\n        self.sf = []\n        self.scr = []\n        self.memory_f = [initial_f] * archive_size\n        self.memory_cr = [initial_cr] * archive_size\n        self.p_best = 0.05\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n\n    def mutate(self, i):\n        phi_idx = np.random.randint(0, self.archive_size)\n        self.F = self.memory_f[phi_idx]\n        self.CR = self.memory_cr[phi_idx]\n        self.F = np.clip(np.random.normal(self.F, 0.1), 0.1, 1.0)\n        self.CR = np.clip(np.random.normal(self.CR, 0.1), 0.0, 1.0)\n\n        idxs = np.random.choice(self.pop_size, 4, replace=False)\n        a, b, c, p_best_idx = idxs[0], idxs[1], idxs[2], idxs[3]\n\n        if np.random.rand() < self.p_best:\n            p_best_idx = np.argmin(self.fitness)\n\n        x_mutated = self.population[i] + self.F * (self.population[p_best_idx] - self.population[i]) + self.F * (self.population[a] - self.population[b])\n\n        return x_mutated, self.F, self.CR\n\n    def crossover(self, x_mutated, i, cr):\n        x_trial = np.copy(self.population[i])\n        j_rand = np.random.randint(self.dim)\n        for j in range(self.dim):\n            if np.random.rand() < cr or j == j_rand:\n                x_trial[j] = x_mutated[j]\n        return x_trial\n\n    def update_memory(self):\n        if self.sf:\n            self.memory_f.append(sum(val**2 for val in self.sf) / sum(self.sf))\n            self.memory_f.pop(0)\n            self.memory_cr.append(np.mean(self.scr))\n            self.memory_cr.pop(0)\n        self.sf = []\n        self.scr = []\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)].copy()\n        self.sf = []\n        self.scr = []\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                x_mutated, F, CR = self.mutate(i)\n                x_trial = self.crossover(x_mutated, i, CR)\n                x_trial = np.clip(x_trial, func.bounds.lb, func.bounds.ub)\n                f_trial = func(x_trial)\n                self.budget -= 1\n                if self.budget <= 0:\n                    break\n                if f_trial < self.fitness[i]:\n                    self.sf.append(F)\n                    self.scr.append(CR)\n                    delta = abs(self.fitness[i] - f_trial)\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(self.population[i].copy())\n                    else:\n                        j = np.random.randint(0, self.archive_size)\n                        self.archive[j] = self.population[i].copy()\n                    self.population[i] = x_trial.copy()\n                    self.fitness[i] = f_trial\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = x_trial.copy()\n            if self.budget <= 0:\n                break\n            self.update_memory()\n            best_index = np.argmin(self.fitness)\n            if self.fitness[best_index] < self.f_opt:\n                self.f_opt = self.fitness[best_index]\n                self.x_opt = self.population[best_index].copy()\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "The algorithm AdaptiveDE scored 0.673 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["7bd6e0b3-d47a-4fc1-a7fa-623c84b452b7"], "operator": null, "metadata": {"aucs": [0.2024175802847875, 0.7474802692500497, 0.6521215003880638, 0.8609467071188931, 0.7482177477280008, 0.8413669039242154, 0.6135298126842559, 0.7101828359164444, 0.7872939455117931, 0.2257605464409519, 0.8831590635466942, 0.9807528887943147, 0.790684061535773, 0.7060344459341432, 0.8847699112551847, 0.5937860205065677, 0.637604975507263, 0.8776383438399915, 0.20991969861625048, 0.49798481756338253]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "d55f99d3-ef26-4a01-a7e1-5bdaf584c296", "fitness": "-inf", "name": "SelfAdaptiveDE_OL", "description": "Self-adaptive Differential Evolution with orthogonal learning, a diversity-preserving mechanism, and a local search refinement.", "code": "import numpy as np\n\nclass SelfAdaptiveDE_OL:\n    def __init__(self, budget=10000, dim=10, pop_size=40, archive_size=20, initial_f=0.5, cr=0.9, local_search_prob=0.05, ol_prob=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.F = initial_f  # Initial differential weight, adaptive later\n        self.CR = cr # Crossover rate\n        self.local_search_prob = local_search_prob\n        self.ol_prob = ol_prob # Orthogonal Learning Probability\n        self.population = None\n        self.fitness = None\n        self.archive = None\n        self.archive_fitness = None\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        self.archive = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.archive_size, self.dim))\n        self.archive_fitness = np.full(self.archive_size, np.inf)\n\n    def mutate(self, i):\n        # Select three distinct individuals from the population and archive\n        indices = np.random.choice(self.pop_size + self.archive_size, 3, replace=False)\n        \n        if indices[0] < self.pop_size:\n            x_r1 = self.population[indices[0]]\n        else:\n            x_r1 = self.archive[indices[0] - self.pop_size]\n            \n        if indices[1] < self.pop_size:\n            x_r2 = self.population[indices[1]]\n        else:\n            x_r2 = self.archive[indices[1] - self.pop_size]\n\n        if indices[2] < self.pop_size:\n            x_r3 = self.population[indices[2]]\n        else:\n            x_r3 = self.archive[indices[2] - self.pop_size]\n\n        # Adaptive F: adjust based on success\n        self.F = np.clip(np.random.normal(self.F, 0.1), 0.1, 0.9)\n\n        x_mutated = self.population[i] + self.F * (x_r2 - x_r3)\n\n        return x_mutated\n\n    def crossover(self, x_mutated, i):\n        x_trial = np.copy(self.population[i])\n        j_rand = np.random.randint(self.dim)\n        for j in range(self.dim):\n            if np.random.rand() < self.CR or j == j_rand:\n                x_trial[j] = x_mutated[j]\n        return x_trial\n\n    def local_search(self, func, x):\n        x_neighbor = x + np.random.normal(0, 0.05, self.dim)  # Smaller perturbation\n        x_neighbor = np.clip(x_neighbor, func.bounds.lb, func.bounds.ub)\n        f_neighbor = func(x_neighbor)\n        self.budget -= 1\n        return f_neighbor, x_neighbor\n\n    def update_archive(self):\n        # Update the archive with diverse individuals from the population\n        # Replace a random element of the archive with a random element of the population\n        if self.archive_size > 0 and self.pop_size > 0: #Prevent errors when size is 0\n            archive_index = np.random.randint(self.archive_size)\n            population_index = np.random.randint(self.pop_size)\n            \n            if self.fitness[population_index] < self.archive_fitness[archive_index]:\n                self.archive[archive_index] = self.population[population_index].copy()\n                self.archive_fitness[archive_index] = self.fitness[population_index]\n\n    def orthogonal_learning(self, func, x):\n        # Simple orthogonal learning: sample points around the current solution\n        levels = 3  # Number of levels for each dimension\n        points = np.zeros((self.dim, levels))\n        for i in range(self.dim):\n            points[i, :] = np.linspace(max(func.bounds.lb, x[i] - 0.5), min(func.bounds.ub, x[i] + 0.5), levels)\n\n        design = np.array(np.meshgrid(*[points[i, :] for i in range(self.dim)])).T.reshape(-1, self.dim)\n        \n        fitnesses = np.array([func(xi) for xi in design])\n        self.budget -= len(fitnesses)\n        \n        best_index = np.argmin(fitnesses)\n        return fitnesses[best_index], design[best_index]\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)].copy()\n        \n        while self.budget > 0:\n            for i in range(self.pop_size):\n                x_mutated = self.mutate(i)\n                x_trial = self.crossover(x_mutated, i)\n\n                x_trial = np.clip(x_trial, func.bounds.lb, func.bounds.ub) #ensure bounds are not violated after crossover\n                f_trial = func(x_trial)\n                self.budget -= 1\n                \n                if self.budget <= 0:\n                  break # Exit inner loop\n\n                if np.random.rand() < self.local_search_prob: #Apply local search with a given probability\n                    f_ls, x_ls = self.local_search(func, x_trial)\n                    if f_ls < f_trial:\n                        f_trial = f_ls\n                        x_trial = x_ls.copy()\n                \n                if np.random.rand() < self.ol_prob and self.budget > self.dim * 3:\n                    f_ol, x_ol = self.orthogonal_learning(func, x_trial)\n                    if f_ol < f_trial:\n                        f_trial = f_ol\n                        x_trial = x_ol.copy()\n\n\n                if f_trial < self.fitness[i]:\n                    self.population[i] = x_trial.copy()\n                    self.fitness[i] = f_trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = x_trial.copy()\n            if self.budget <= 0:\n                break # Exit outer loop\n\n            self.update_archive()\n\n            best_index = np.argmin(self.fitness)\n            if self.fitness[best_index] < self.f_opt:\n                self.f_opt = self.fitness[best_index]\n                self.x_opt = self.population[best_index].copy()\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "In the code, line 82, in orthogonal_learning, the following error occurred:\nValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\nOn line: points[i, :] = np.linspace(max(func.bounds.lb, x[i] - 0.5), min(func.bounds.ub, x[i] + 0.5), levels)", "error": "In the code, line 82, in orthogonal_learning, the following error occurred:\nValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\nOn line: points[i, :] = np.linspace(max(func.bounds.lb, x[i] - 0.5), min(func.bounds.ub, x[i] + 0.5), levels)", "parent_ids": ["b3123acc-b2e5-4059-b300-43148eb89d36"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "b61f35c7-a4e3-4d6a-8aab-c34149170e78", "fitness": "-inf", "name": "AdaptiveDE", "description": "Adaptive Differential Evolution with orthogonal learning and dynamic parameter control for enhanced exploration and exploitation.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=30, initial_f=0.5, cr=0.9, local_search_prob=0.1, orthogonal_learning_prob=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = initial_f  # Initial differential weight, adaptive later\n        self.CR = cr # Crossover rate\n        self.local_search_prob = local_search_prob\n        self.orthogonal_learning_prob = orthogonal_learning_prob\n        self.population = None\n        self.fitness = None\n        self.archive = []  # Archive for storing discarded solutions\n\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        self.archive = []\n\n\n    def mutate(self, i):\n        # Select three distinct individuals from the population or archive\n        indices = np.random.choice(self.pop_size + len(self.archive), 3, replace=False)\n        \n        if indices[0] < self.pop_size:\n            x_r1 = self.population[indices[0]]\n        else:\n            x_r1 = self.archive[indices[0] - self.pop_size]\n            \n        if indices[1] < self.pop_size:\n            x_r2 = self.population[indices[1]]\n        else:\n            x_r2 = self.archive[indices[1] - self.pop_size]\n            \n        if indices[2] < self.pop_size:\n            x_r3 = self.population[indices[2]]\n        else:\n            x_r3 = self.archive[indices[2] - self.pop_size]\n\n\n        # Adaptive F: adjust based on success\n        self.F = np.clip(np.random.normal(self.F, 0.1), 0.1, 0.9)\n\n        x_mutated = self.population[i] + self.F * (x_r2 - x_r3)\n\n        return x_mutated\n\n    def crossover(self, x_mutated, i):\n        x_trial = np.copy(self.population[i])\n        j_rand = np.random.randint(self.dim)\n        for j in range(self.dim):\n            if np.random.rand() < self.CR or j == j_rand:\n                x_trial[j] = x_mutated[j]\n        return x_trial\n\n    def local_search(self, func, x):\n        # Focused local search around x with smaller steps\n        x_neighbor = x + np.random.normal(0, 0.01, self.dim)  # Smaller perturbation\n        x_neighbor = np.clip(x_neighbor, func.bounds.lb, func.bounds.ub)\n        f_neighbor = func(x_neighbor)\n        self.budget -= 1\n        return f_neighbor, x_neighbor\n    \n    def orthogonal_learning(self, func, x):\n        # Perform orthogonal learning around x\n        levels = 3  # Number of levels for each dimension\n        orthogonal_array = self.generate_orthogonal_array(self.dim, levels)\n        \n        best_neighbor = x\n        best_fitness = func(x)\n        self.budget -= 1\n        \n        for row in orthogonal_array:\n            x_neighbor = np.copy(x)\n            for j in range(self.dim):\n                level = row[j]\n                # Discretize the range into levels\n                range_width = (func.bounds.ub - func.bounds.lb) / levels\n                x_neighbor[j] = func.bounds.lb + level * range_width + np.random.uniform(0, range_width*0.9)\n                x_neighbor[j] = np.clip(x_neighbor[j], func.bounds.lb, func.bounds.ub)  # Clip to bounds\n            \n            f_neighbor = func(x_neighbor)\n            self.budget -= 1\n            \n            if f_neighbor < best_fitness:\n                best_fitness = f_neighbor\n                best_neighbor = x_neighbor\n                \n        return best_fitness, best_neighbor\n            \n\n    def generate_orthogonal_array(self, dim, levels):\n        # A simple orthogonal array generator (L9 array for up to 4 dimensions with 3 levels)\n        if dim <= 2:\n            if levels == 3:\n                return np.array([[0, 0], [0, 1], [0, 2], [1, 0], [1, 1], [1, 2], [2, 0], [2, 1], [2, 2]])\n            else:\n                raise ValueError(\"Unsupported number of levels for this dimension.\")\n        elif dim <= 4:\n            if levels == 3:\n                return np.array([[0, 0, 0, 0], [0, 1, 1, 1], [0, 2, 2, 2], [1, 0, 1, 2], [1, 1, 2, 0], [1, 2, 0, 1], [2, 0, 2, 1], [2, 1, 0, 2], [2, 2, 1, 0]])\n            else:\n                raise ValueError(\"Unsupported number of levels for this dimension.\")\n\n        else:\n            # Fallback: Random array, for dimensions > 4 or non-3 levels\n            return np.random.randint(0, levels, size=(levels**2, dim))\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)].copy()\n\n        generation = 0\n        while self.budget > 0:\n            generation += 1\n            for i in range(self.pop_size):\n                x_mutated = self.mutate(i)\n                x_trial = self.crossover(x_mutated, i)\n\n                x_trial = np.clip(x_trial, func.bounds.lb, func.bounds.ub) #ensure bounds are not violated after crossover\n                f_trial = func(x_trial)\n                self.budget -= 1\n                \n                if self.budget <= 0:\n                  break # Exit inner loop\n\n                if np.random.rand() < self.local_search_prob: #Apply local search with a given probability\n                    f_ls, x_ls = self.local_search(func, x_trial)\n                    if f_ls < f_trial:\n                        f_trial = f_ls\n                        x_trial = x_ls.copy()\n                \n                if np.random.rand() < self.orthogonal_learning_prob: #Apply orthogonal learning with a given probability\n                    f_ol, x_ol = self.orthogonal_learning(func, x_trial)\n                    if f_ol < f_trial:\n                        f_trial = f_ol\n                        x_trial = x_ol.copy()\n\n\n                if f_trial < self.fitness[i]:\n                    # Archive the replaced solution\n                    self.archive.append(self.population[i].copy())\n                    if len(self.archive) > self.pop_size:\n                        self.archive.pop(0)  # Keep archive size limited\n                    \n                    self.population[i] = x_trial.copy()\n                    self.fitness[i] = f_trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = x_trial.copy()\n            if self.budget <= 0:\n                break # Exit outer loop\n\n            best_index = np.argmin(self.fitness)\n            if self.fitness[best_index] < self.f_opt:\n                self.f_opt = self.fitness[best_index]\n                self.x_opt = self.population[best_index].copy()\n            \n            # Dynamic CR update: adapt crossover rate based on progress\n            if generation % 5 == 0:  # Adjust CR every 5 generations\n                if np.random.rand() < 0.3:\n                    self.CR = np.clip(self.CR + np.random.normal(0, 0.1), 0.1, 0.99)\n                else:\n                    self.CR = np.clip(self.CR - np.random.normal(0, 0.05), 0.1, 0.99)\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "In the code, line 82, in orthogonal_learning, the following error occurred:\nValueError: setting an array element with a sequence.\nOn line: x_neighbor[j] = func.bounds.lb + level * range_width + np.random.uniform(0, range_width*0.9)", "error": "In the code, line 82, in orthogonal_learning, the following error occurred:\nValueError: setting an array element with a sequence.\nOn line: x_neighbor[j] = func.bounds.lb + level * range_width + np.random.uniform(0, range_width*0.9)", "parent_ids": ["7bd6e0b3-d47a-4fc1-a7fa-623c84b452b7"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "ff94baf8-b368-40a6-80e9-b790f1254eab", "fitness": 0.3043943183437378, "name": "StochasticRankingDE", "description": "A Differential Evolution variant with a simplified self-adaptation mechanism for F and CR, coupled with a stochastic ranking of population members and archive candidates to enhance exploration and exploitation balance.", "code": "import numpy as np\n\nclass StochasticRankingDE:\n    def __init__(self, budget=10000, dim=10, pop_size=40, archive_size=10, f_min=0.1, f_max=0.9, cr_min=0.1, cr_max=0.9, p_rank=0.45):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.f_min = f_min\n        self.f_max = f_max\n        self.cr_min = cr_min\n        self.cr_max = cr_max\n        self.p_rank = p_rank # Probability of ranking based on fitness\n        self.population = None\n        self.fitness = None\n        self.archive = None\n        self.archive_fitness = None\n        self.F = np.random.uniform(f_min, f_max, size=pop_size)\n        self.CR = np.random.uniform(cr_min, cr_max, size=pop_size)\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        self.archive = np.copy(self.population[:self.archive_size]) # Initialize archive with good solutions\n        self.archive_fitness = np.copy(self.fitness[:self.archive_size])\n        \n    def mutate(self, i):\n        indices = np.random.choice(self.pop_size + self.archive_size, 3, replace=False)\n        \n        if indices[0] < self.pop_size:\n            x_r1 = self.population[indices[0]]\n        else:\n            x_r1 = self.archive[indices[0] - self.pop_size]\n            \n        if indices[1] < self.pop_size:\n            x_r2 = self.population[indices[1]]\n        else:\n            x_r2 = self.archive[indices[1] - self.pop_size]\n\n        if indices[2] < self.pop_size:\n            x_r3 = self.population[indices[2]]\n        else:\n            x_r3 = self.archive[indices[2] - self.pop_size]\n\n        x_mutated = self.population[i] + self.F[i] * (x_r2 - x_r3)\n        x_mutated = np.clip(x_mutated, -5.0, 5.0)\n        return x_mutated\n\n    def crossover(self, x_mutated, i):\n        x_trial = np.copy(self.population[i])\n        j_rand = np.random.randint(self.dim)\n        for j in range(self.dim):\n            if np.random.rand() < self.CR[i] or j == j_rand:\n                x_trial[j] = x_mutated[j]\n        return x_trial\n\n    def stochastic_ranking(self, f_trial, f_current):\n        \"\"\"Ranks solutions stochastically based on their fitness.\"\"\"\n        if (f_trial < f_current) or (np.random.rand() < self.p_rank):\n            return True # Accept the trial solution\n        else:\n            return False # Reject the trial solution\n\n    def update_archive(self):\n        combined_population = np.concatenate((self.population, self.archive), axis=0)\n        combined_fitness = np.concatenate((self.fitness, self.archive_fitness))\n        \n        sorted_indices = np.argsort(combined_fitness)[:self.archive_size]\n        \n        self.archive = combined_population[sorted_indices].copy()\n        self.archive_fitness = combined_fitness[sorted_indices].copy()\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)].copy()\n        \n        generation = 0\n        while self.budget > 0:\n            generation += 1\n            for i in range(self.pop_size):\n                x_mutated = self.mutate(i)\n                x_trial = self.crossover(x_mutated, i)\n                \n                f_trial = func(x_trial)\n                self.budget -= 1\n\n                if self.stochastic_ranking(f_trial, self.fitness[i]):\n                    self.population[i] = x_trial.copy()\n                    self.fitness[i] = f_trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = x_trial.copy()\n                        \n                #Adapt F and CR\n                if np.random.rand() < 0.1:\n                  self.F[i] = np.random.uniform(self.f_min, self.f_max)\n                if np.random.rand() < 0.1:\n                  self.CR[i] = np.random.uniform(self.cr_min, self.cr_max)\n                \n                if self.budget <= 0:\n                    break\n            \n            self.update_archive()\n            \n            if self.budget <= 0:\n                break\n        \n        best_index = np.argmin(self.fitness)\n        if self.fitness[best_index] < self.f_opt:\n            self.f_opt = self.fitness[best_index]\n            self.x_opt = self.population[best_index].copy()\n        \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "The algorithm StochasticRankingDE scored 0.304 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["88a94d8e-71cf-4b05-bce1-3703f430c9e5"], "operator": null, "metadata": {"aucs": [0.1252633888947532, 0.19129922166332347, 0.28983533013160123, 0.2525568352034897, 0.23607107497656554, 0.2556052153203333, 0.24763703470416865, 0.23198027822758305, 0.23004617897641932, 0.16203225908724694, 0.22868938978878117, 0.9881719676387204, 0.2652869774755754, 0.24154949593600883, 0.6358942502567583, 0.3065262779533734, 0.24907417154763667, 0.31797176733167776, 0.1659983135969949, 0.4663969381637455]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "2c4e0dd5-305a-4673-b6e6-79763f2dfde6", "fitness": "-inf", "name": "HybridDE", "description": "Population-based optimization using a combination of Differential Evolution mutation, covariance matrix adaptation for exploration, and a local search operator.", "code": "import numpy as np\n\nclass HybridDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, initial_f=0.5, initial_cr=0.9, archive_size=10, local_search_prob=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = initial_f\n        self.CR = initial_cr\n        self.population = None\n        self.fitness = None\n        self.archive = []\n        self.archive_size = archive_size\n        self.sf = []\n        self.scr = []\n        self.memory_f = [initial_f] * archive_size\n        self.memory_cr = [initial_cr] * archive_size\n        self.p_best = 0.05\n        self.local_search_prob = local_search_prob\n        self.cma_mean = None\n        self.cma_sigma = 0.1\n        self.cma_C = None # Covariance Matrix\n        self.cma_eigenvalues = None\n        self.cma_eigenbasis = None\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        self.cma_mean = np.mean(self.population, axis=0)\n        self.cma_C = np.eye(self.dim) # Identity matrix as initial covariance\n        self.update_covariance_matrix()\n\n    def update_covariance_matrix(self):\n        self.cma_eigenvalues, self.cma_eigenbasis = np.linalg.eig(self.cma_C)\n\n    def mutate(self, i):\n        phi_idx = np.random.randint(0, self.archive_size)\n        self.F = self.memory_f[phi_idx]\n        self.CR = self.memory_cr[phi_idx]\n        self.F = np.clip(np.random.normal(self.F, 0.1), 0.1, 1.0)\n        self.CR = np.clip(np.random.normal(self.CR, 0.1), 0.0, 1.0)\n\n        idxs = np.random.choice(self.pop_size, 4, replace=False)\n        a, b, c, p_best_idx = idxs[0], idxs[1], idxs[2], idxs[3]\n\n        if np.random.rand() < self.p_best:\n            p_best_idx = np.argmin(self.fitness)\n\n        x_mutated = self.population[i] + self.F * (self.population[p_best_idx] - self.population[i]) + self.F * (self.population[a] - self.population[b])\n\n        return x_mutated, self.F, self.CR\n\n    def cma_mutation(self):\n         z = np.random.normal(0, 1, self.dim)\n         x_mutated = self.cma_mean + self.cma_sigma * self.cma_eigenbasis @ np.diag(np.sqrt(self.cma_eigenvalues)) @ z\n         return x_mutated\n\n    def crossover(self, x_mutated, i, cr):\n        x_trial = np.copy(self.population[i])\n        j_rand = np.random.randint(self.dim)\n        for j in range(self.dim):\n            if np.random.rand() < cr or j == j_rand:\n                x_trial[j] = x_mutated[j]\n        return x_trial\n\n    def local_search(self, x, func):\n        x_new = x + np.random.normal(0, 0.01, self.dim)\n        x_new = np.clip(x_new, func.bounds.lb, func.bounds.ub)\n        return x_new\n\n    def update_memory(self):\n        if self.sf:\n            self.memory_f.append(sum(val**2 for val in self.sf) / sum(self.sf))\n            self.memory_f.pop(0)\n            self.memory_cr.append(np.mean(self.scr))\n            self.memory_cr.pop(0)\n        self.sf = []\n        self.scr = []\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)].copy()\n        self.sf = []\n        self.scr = []\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                if np.random.rand() < 0.5:\n                   x_mutated, F, CR = self.mutate(i)\n                   x_trial = self.crossover(x_mutated, i, CR)\n                else:\n                   x_mutated = self.cma_mutation()\n                   x_trial = x_mutated\n                   \n                x_trial = np.clip(x_trial, func.bounds.lb, func.bounds.ub)\n\n                if np.random.rand() < self.local_search_prob:\n                    x_trial = self.local_search(x_trial, func)\n\n                f_trial = func(x_trial)\n                self.budget -= 1\n                if self.budget <= 0:\n                    break\n\n                if f_trial < self.fitness[i]:\n                    self.sf.append(F)\n                    self.scr.append(CR)\n                    delta = abs(self.fitness[i] - f_trial)\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(self.population[i].copy())\n                    else:\n                        j = np.random.randint(0, self.archive_size)\n                        self.archive[j] = self.population[i].copy()\n                    self.population[i] = x_trial.copy()\n                    self.fitness[i] = f_trial\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = x_trial.copy()\n\n            if self.budget <= 0:\n                break\n\n            self.update_memory()\n\n            # CMA-ES adaptation\n            self.cma_mean = np.mean(self.population, axis=0)\n            diff = self.population - self.cma_mean\n            self.cma_C = np.cov(diff.T)\n            self.update_covariance_matrix()\n\n\n            best_index = np.argmin(self.fitness)\n            if self.fitness[best_index] < self.f_opt:\n                self.f_opt = self.fitness[best_index]\n                self.x_opt = self.population[best_index].copy()\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "In the code, line 108, in __call__, the following error occurred:\nUnboundLocalError: cannot access local variable 'F' where it is not associated with a value\nOn line: self.sf.append(F)", "error": "In the code, line 108, in __call__, the following error occurred:\nUnboundLocalError: cannot access local variable 'F' where it is not associated with a value\nOn line: self.sf.append(F)", "parent_ids": ["555dcdca-320a-4ce9-b585-17f051aba286"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "9dad5aa3-4b10-49c8-b303-c22499938e4b", "fitness": 0.5743692711306007, "name": "MirroredSelfAdaptiveDE", "description": "An enhanced Self-Adaptive Differential Evolution that uses a mirrored sampling technique to create a more diverse and robust search process, along with adaptive population size reduction to focus computational effort.", "code": "import numpy as np\n\nclass MirroredSelfAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size_max=50, pop_size_min=10, archive_size=10, f_min=0.1, f_max=0.9, cr_min=0.1, cr_max=0.9, reduction_factor = 0.95):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size_max = pop_size_max\n        self.pop_size_min = pop_size_min\n        self.pop_size = pop_size_max\n        self.archive_size = archive_size\n        self.f_min = f_min\n        self.f_max = f_max\n        self.cr_min = cr_min\n        self.cr_max = cr_max\n        self.reduction_factor = reduction_factor\n        self.population = None\n        self.fitness = None\n        self.archive = None\n        self.archive_fitness = None\n        self.F = None  # Initialize in initialize_population\n        self.CR = None # Initialize in initialize_population\n        self.lb = -5.0\n        self.ub = 5.0\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        self.archive = np.copy(self.population[:self.archive_size]) # Initialize archive with good solutions\n        self.archive_fitness = np.copy(self.fitness[:self.archive_size])\n        self.F = np.random.uniform(self.f_min, self.f_max, size=self.pop_size)\n        self.CR = np.random.uniform(self.cr_min, self.cr_max, size=self.pop_size)\n\n    def mutate(self, i):\n        indices = np.random.choice(self.pop_size + self.archive_size, 3, replace=False)\n\n        if indices[0] < self.pop_size:\n            x_r1 = self.population[indices[0]]\n        else:\n            x_r1 = self.archive[indices[0] - self.pop_size]\n\n        if indices[1] < self.pop_size:\n            x_r2 = self.population[indices[1]]\n        else:\n            x_r2 = self.archive[indices[1] - self.pop_size]\n\n        if indices[2] < self.pop_size:\n            x_r3 = self.population[indices[2]]\n        else:\n            x_r3 = self.archive[indices[2] - self.pop_size]\n\n        x_mutated = self.population[i] + self.F[i] * (x_r2 - x_r3)\n\n        # Mirrored Sampling to handle boundary violations\n        x_mutated = np.clip(x_mutated, self.lb, self.ub)\n        \n        # Apply mirroring if out of bounds after clipping\n        for j in range(self.dim):\n            if x_mutated[j] < self.lb:\n                x_mutated[j] = self.lb + (self.lb - x_mutated[j])\n            elif x_mutated[j] > self.ub:\n                x_mutated[j] = self.ub - (x_mutated[j] - self.ub)\n\n        return x_mutated\n\n    def crossover(self, x_mutated, i):\n        x_trial = np.copy(self.population[i])\n        j_rand = np.random.randint(self.dim)\n        for j in range(self.dim):\n            if np.random.rand() < self.CR[i] or j == j_rand:\n                x_trial[j] = x_mutated[j]\n        return x_trial\n\n    def update_archive(self):\n        combined_population = np.concatenate((self.population, self.archive), axis=0)\n        combined_fitness = np.concatenate((self.fitness, self.archive_fitness))\n\n        sorted_indices = np.argsort(combined_fitness)[:self.archive_size]\n\n        self.archive = combined_population[sorted_indices].copy()\n        self.archive_fitness = combined_fitness[sorted_indices].copy()\n        \n    def reduce_population(self):\n        if self.pop_size > self.pop_size_min:\n            num_to_remove = max(1, int((self.pop_size - self.pop_size_min) * (1 - self.reduction_factor)))\n            \n            # Remove worst performing individuals.\n            worst_indices = np.argsort(self.fitness)[-num_to_remove:]\n            \n            mask = np.ones(self.pop_size, dtype=bool)\n            mask[worst_indices] = False\n            \n            self.population = self.population[mask]\n            self.fitness = self.fitness[mask]\n            self.F = self.F[mask]\n            self.CR = self.CR[mask]\n            \n            self.pop_size = len(self.population)\n            \n\n    def __call__(self, func):\n        self.initialize_population(func)\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)].copy()\n\n        generation = 0\n        while self.budget > 0:\n            generation += 1\n            for i in range(self.pop_size):\n                x_mutated = self.mutate(i)\n                x_trial = self.crossover(x_mutated, i)\n\n                f_trial = func(x_trial)\n                self.budget -= 1\n\n                if f_trial < self.fitness[i]:\n                    self.population[i] = x_trial.copy()\n                    self.fitness[i] = f_trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = x_trial.copy()\n\n                #Adapt F and CR\n                if np.random.rand() < 0.1:\n                    self.F[i] = np.random.uniform(self.f_min, self.f_max)\n                if np.random.rand() < 0.1:\n                    self.CR[i] = np.random.uniform(self.cr_min, self.cr_max)\n\n                if self.budget <= 0:\n                    break\n\n            self.update_archive()\n            self.reduce_population()\n\n            if self.budget <= 0:\n                break\n\n        best_index = np.argmin(self.fitness)\n        if self.fitness[best_index] < self.f_opt:\n            self.f_opt = self.fitness[best_index]\n            self.x_opt = self.population[best_index].copy()\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "The algorithm MirroredSelfAdaptiveDE scored 0.574 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["88a94d8e-71cf-4b05-bce1-3703f430c9e5"], "operator": null, "metadata": {"aucs": [0.15863473967307395, 0.33607222987249197, 0.6990961925697384, 0.8392919649657319, 0.6279409237250202, 0.79504673665896, 0.3224362395514311, 0.5078322666857646, 0.7090589187977194, 0.21806188523912184, 0.8608055551440426, 0.9960886074591929, 0.41262047510489397, 0.3492261240566372, 0.9154873354060568, 0.7497366279326736, 0.46350133885164724, 0.8313719328014998, 0.19567682767858785, 0.49939850043772593]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "255691cc-55eb-4b87-917a-063d8fd42f78", "fitness": "-inf", "name": "AdaptiveCauchyLevyDE", "description": "Population-based search with Cauchy mutation and Lvy flight local search, adaptively adjusting parameters based on success history.", "code": "import numpy as np\n\nclass AdaptiveCauchyLevyDE:\n    def __init__(self, budget=10000, dim=10, pop_size=30, initial_cauchy_scale=0.1, levy_scale=0.01, cr=0.9, local_search_prob=0.1, archive_size=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.cauchy_scale = initial_cauchy_scale  # Scale for Cauchy mutation, adaptive later\n        self.levy_scale = levy_scale #Scale for Levy Flight Local Search\n        self.CR = cr  # Crossover rate\n        self.local_search_prob = local_search_prob\n        self.population = None\n        self.fitness = None\n        self.archive = []\n        self.archive_fitness = []\n        self.archive_size = archive_size\n        self.success_history = []\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n\n    def levy_flight(self, x, beta=1.5):\n        \"\"\"Generates a Levy flight step.\"\"\"\n        sigma = (np.math.gamma(1 + beta) * np.sin(np.pi * beta / 2) / (np.math.gamma((1 + beta) / 2) * beta * (2 ** ((beta - 1) / 2)))) ** (1 / beta)\n        u = np.random.normal(0, sigma, size=self.dim)\n        v = np.random.normal(0, 1, size=self.dim)\n        step = u / (np.abs(v) ** (1 / beta))\n        return step\n\n    def mutate(self, i):\n        # Select three distinct individuals from the population\n        indices = np.random.choice(self.pop_size, 3, replace=False)\n        x_r1 = self.population[indices[0]]\n        x_r2 = self.population[indices[1]]\n        x_r3 = self.population[indices[2]]\n\n        # Cauchy mutation\n        cauchy_step = self.cauchy_scale * np.random.standard_cauchy(size=self.dim)\n        x_mutated = self.population[i] + cauchy_step * (x_r2 - x_r3) #Cauchy mutation\n\n        return x_mutated\n\n    def crossover(self, x_mutated, i):\n        x_trial = np.copy(self.population[i])\n        j_rand = np.random.randint(self.dim)\n        for j in range(self.dim):\n            if np.random.rand() < self.CR or j == j_rand:\n                x_trial[j] = x_mutated[j]\n        return x_trial\n\n    def local_search(self, func, x):\n        # Levy flight local search around x\n        levy_step = self.levy_scale * self.levy_flight(x)\n        x_neighbor = x + levy_step\n        x_neighbor = np.clip(x_neighbor, func.bounds.lb, func.bounds.ub)\n        f_neighbor = func(x_neighbor)\n        self.budget -= 1\n        return f_neighbor, x_neighbor\n\n    def update_archive(self, x, f):\n        if len(self.archive) < self.archive_size:\n            self.archive.append(x)\n            self.archive_fitness.append(f)\n        else:\n            # Replace the worst member in the archive\n            max_index = np.argmax(self.archive_fitness)\n            if f < self.archive_fitness[max_index]:\n                self.archive[max_index] = x\n                self.archive_fitness[max_index] = f\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)].copy()\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                x_mutated = self.mutate(i)\n                x_trial = self.crossover(x_mutated, i)\n\n                x_trial = np.clip(x_trial, func.bounds.lb, func.bounds.ub)\n                f_trial = func(x_trial)\n                self.budget -= 1\n\n                if self.budget <= 0:\n                    break\n\n                if np.random.rand() < self.local_search_prob:\n                    f_ls, x_ls = self.local_search(func, x_trial)\n                    if f_ls < f_trial:\n                        f_trial = f_ls\n                        x_trial = x_ls.copy()\n\n                if f_trial < self.fitness[i]:\n                    self.success_history.append(True)\n                    self.population[i] = x_trial.copy()\n                    self.fitness[i] = f_trial\n                    self.update_archive(x_trial, f_trial)\n\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = x_trial.copy()\n\n                else:\n                    self.success_history.append(False)\n\n            if self.budget <= 0:\n                break\n\n            # Adaptive Cauchy scale based on success history\n            if len(self.success_history) > 50: #Adjust cauchy scale every 50 iterations\n                success_rate = np.mean(self.success_history[-50:])\n                self.cauchy_scale = self.cauchy_scale * (1 + 0.2 * (success_rate - 0.5)) #Increase if success rate is high, decrease otherwise\n                self.cauchy_scale = np.clip(self.cauchy_scale, 0.01, 0.5) #Keep scale within reasonable bounds\n            best_index = np.argmin(self.fitness)\n            if self.fitness[best_index] < self.f_opt:\n                self.f_opt = self.fitness[best_index]\n                self.x_opt = self.population[best_index].copy()\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "In the code, line 805, in __getattr__, the following error occurred:\nAttributeError: module 'numpy' has no attribute 'math'. Did you mean: 'emath'?", "error": "In the code, line 805, in __getattr__, the following error occurred:\nAttributeError: module 'numpy' has no attribute 'math'. Did you mean: 'emath'?", "parent_ids": ["7bd6e0b3-d47a-4fc1-a7fa-623c84b452b7"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "c95cd30d-863b-4092-8020-ca6b414137bc", "fitness": 0.5005616901401406, "name": "AdaptiveDE", "description": "Simplified Adaptive DE with Gaussian local search around the best solution and periodic population refreshment.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=25, initial_f=0.5, cr=0.7, local_search_freq=0.05, refresh_interval=500):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = initial_f  # Initial differential weight\n        self.CR = cr  # Crossover rate\n        self.local_search_freq = local_search_freq\n        self.refresh_interval = refresh_interval\n        self.population = None\n        self.fitness = None\n        self.eval_count = 0\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.eval_count += self.pop_size\n        return np.argmin(self.fitness)\n\n    def mutate(self, i):\n        indices = np.random.choice(self.pop_size, 3, replace=False)\n        x_r1 = self.population[indices[0]]\n        x_r2 = self.population[indices[1]]\n        x_r3 = self.population[indices[2]]\n\n        # Simplified F adaptation\n        self.F = np.clip(np.random.normal(self.F, 0.1), 0.1, 0.9)\n        x_mutated = self.population[i] + self.F * (x_r2 - x_r3)\n        return x_mutated\n\n    def crossover(self, x_mutated, i):\n        x_trial = np.copy(self.population[i])\n        j_rand = np.random.randint(self.dim)\n        for j in range(self.dim):\n            if np.random.rand() < self.CR or j == j_rand:\n                x_trial[j] = x_mutated[j]\n        return x_trial\n\n    def local_search(self, func, x_best):\n        # Gaussian local search around best solution\n        x_neighbor = x_best + np.random.normal(0, 0.05, self.dim)\n        x_neighbor = np.clip(x_neighbor, func.bounds.lb, func.bounds.ub)\n        f_neighbor = func(x_neighbor)\n        self.eval_count += 1\n        return f_neighbor, x_neighbor\n\n    def refresh_population(self, func):\n         # Replace a portion of the population with new random individuals\n        num_to_refresh = int(0.2 * self.pop_size)\n        indices_to_refresh = np.random.choice(self.pop_size, num_to_refresh, replace=False)\n        self.population[indices_to_refresh] = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(num_to_refresh, self.dim))\n        self.fitness[indices_to_refresh] = np.array([func(x) for x in self.population[indices_to_refresh]])\n        self.eval_count += num_to_refresh\n\n    def __call__(self, func):\n        best_idx = self.initialize_population(func)\n        self.f_opt = self.fitness[best_idx]\n        self.x_opt = self.population[best_idx].copy()\n\n        while self.eval_count < self.budget:\n            for i in range(self.pop_size):\n                x_mutated = self.mutate(i)\n                x_trial = self.crossover(x_mutated, i)\n                x_trial = np.clip(x_trial, func.bounds.lb, func.bounds.ub)\n                f_trial = func(x_trial)\n                self.eval_count += 1\n                \n                if f_trial < self.fitness[i]:\n                    self.population[i] = x_trial.copy()\n                    self.fitness[i] = f_trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = x_trial.copy()\n\n                if self.eval_count >= self.budget:\n                    break\n\n            if self.eval_count >= self.budget:\n                break\n            \n            if np.random.rand() < self.local_search_freq:\n                f_ls, x_ls = self.local_search(func, self.x_opt)\n                if f_ls < self.f_opt:\n                    self.f_opt = f_ls\n                    self.x_opt = x_ls.copy()\n\n            if self.eval_count % self.refresh_interval == 0:\n                self.refresh_population(func)\n\n            best_idx = np.argmin(self.fitness)\n            if self.fitness[best_idx] < self.f_opt:\n                self.f_opt = self.fitness[best_idx]\n                self.x_opt = self.population[best_idx].copy()\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "The algorithm AdaptiveDE scored 0.501 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["7bd6e0b3-d47a-4fc1-a7fa-623c84b452b7"], "operator": null, "metadata": {"aucs": [0.16267998986523102, 0.37032177358295804, 0.4986553489697012, 0.7270145816087783, 0.40917951029175514, 0.6195285094890083, 0.31236189258453506, 0.41644646789734263, 0.4426093481250064, 0.22585045873759768, 0.7649497053213443, 0.9970317507895754, 0.3951744478087341, 0.4098787969511565, 0.8448495551393267, 0.5876028154059558, 0.3868897749645025, 0.7158979803981713, 0.227490952942897, 0.49682014192923385]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "3809657e-0d9c-4459-b529-7f8d1b8afa0c", "fitness": 0.0, "name": "AdaptiveDECMA", "description": "Adaptive Differential Evolution with covariance matrix adaptation (CMA) for mutation and orthogonal crossover to enhance exploration and exploitation.", "code": "import numpy as np\n\nclass AdaptiveDECMA:\n    def __init__(self, budget=10000, dim=10, pop_size=30, initial_f=0.5, cr=0.9, local_search_prob=0.1, cma_lrate = 0.2):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = initial_f  # Initial differential weight, adaptive later\n        self.CR = cr  # Crossover rate\n        self.local_search_prob = local_search_prob\n        self.population = None\n        self.fitness = None\n        self.mean = None\n        self.covariance = None\n        self.cma_lrate = cma_lrate # Learning rate for CMA\n        self.D = None #Diagonal of the covariance matrix\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        self.mean = np.mean(self.population, axis=0)\n        self.covariance = np.eye(self.dim) #Initialize as identity matrix\n        self.D = np.ones(self.dim)\n\n    def mutate_cma(self, i):\n        # CMA-based mutation\n        z = np.random.normal(0, 1, self.dim)\n        x_mutated = self.mean + self.F * self.D * z # Scale the normal distribution\n        return x_mutated\n\n    def crossover_orthogonal(self, x_mutated, i):\n        # Orthogonal Crossover\n        x_trial = np.copy(self.population[i])\n        num_vectors = 2\n        indices = np.random.choice(self.dim, num_vectors, replace=False)\n        \n        # Rotate two random components of x_trial towards x_mutated\n        alpha = np.random.uniform(0, 2*np.pi) #random rotation angle\n\n        p = indices[0]\n        q = indices[1]\n        \n        x_trial_p_old = x_trial[p]\n        x_trial_q_old = x_trial[q]\n        \n        x_trial[p] = np.cos(alpha) * x_trial_p_old - np.sin(alpha) * x_trial[q]\n        x_trial[q] = np.sin(alpha) * x_trial_p_old + np.cos(alpha) * x_trial[q]\n\n        x_mutated_p_old = x_mutated[p]\n        x_mutated_q_old = x_mutated[q]\n        \n        x_mutated[p] = np.cos(alpha) * x_mutated_p_old - np.sin(alpha) * x_mutated[q]\n        x_mutated[q] = np.sin(alpha) * x_mutated_p_old + np.cos(alpha) * x_mutated[q]\n        \n        j_rand = np.random.randint(self.dim)\n        for j in range(self.dim):\n            if np.random.rand() < self.CR or j == j_rand:\n                x_trial[j] = x_mutated[j]\n        return x_trial\n\n    def local_search(self, func, x):\n        # Focused local search around x with smaller steps\n        x_neighbor = x + np.random.normal(0, 0.01, self.dim)  # Smaller perturbation\n        x_neighbor = np.clip(x_neighbor, func.bounds.lb, func.bounds.ub)\n        f_neighbor = func(x_neighbor)\n        self.budget -= 1\n        return f_neighbor, x_neighbor\n\n    def update_covariance(self):\n        # Update covariance matrix using CMA\n        best_index = np.argmin(self.fitness)\n        diff = self.population[best_index] - self.mean\n        self.mean = (1 - self.cma_lrate) * self.mean + self.cma_lrate * self.population[best_index]\n\n        z = diff / self.D\n        self.D = self.D * np.exp((self.cma_lrate/2) * ((z*z) - 1))\n        self.D = np.clip(self.D, 0.01, 10)\n\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)].copy()\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                x_mutated = self.mutate_cma(i)\n                x_trial = self.crossover_orthogonal(x_mutated, i)\n                x_trial = np.clip(x_trial, func.bounds.lb, func.bounds.ub)  # ensure bounds are not violated after crossover\n\n                f_trial = func(x_trial)\n                self.budget -= 1\n\n                if self.budget <= 0:\n                    break  # Exit inner loop\n\n                if np.random.rand() < self.local_search_prob:  # Apply local search with a given probability\n                    f_ls, x_ls = self.local_search(func, x_trial)\n                    if f_ls < f_trial:\n                        f_trial = f_ls\n                        x_trial = x_ls.copy()\n\n                if f_trial < self.fitness[i]:\n                    self.population[i] = x_trial.copy()\n                    self.fitness[i] = f_trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = x_trial.copy()\n\n            if self.budget <= 0:\n                break  # Exit outer loop\n\n            self.update_covariance()\n            best_index = np.argmin(self.fitness)\n            if self.fitness[best_index] < self.f_opt:\n                self.f_opt = self.fitness[best_index]\n                self.x_opt = self.population[best_index].copy()\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "The algorithm AdaptiveDECMA scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["7bd6e0b3-d47a-4fc1-a7fa-623c84b452b7"], "operator": null, "metadata": {"aucs": [0]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "2d140782-d4db-463e-8be7-bb4c67212b26", "fitness": "-inf", "name": "EnhancedSelfAdaptiveDE", "description": "An enhanced Self-Adaptive Differential Evolution with a combined mutation strategy and adaptive population size reduction.", "code": "import numpy as np\n\nclass EnhancedSelfAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=80, archive_size=20, f_min=0.1, f_max=0.9, cr_min=0.1, cr_max=0.9, adaptive_pop_reduction_freq = 10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.initial_pop_size = pop_size\n        self.archive_size = archive_size\n        self.f_min = f_min\n        self.f_max = f_max\n        self.cr_min = cr_min\n        self.cr_max = cr_max\n        self.population = None\n        self.fitness = None\n        self.archive = None\n        self.archive_fitness = None\n        self.F = np.random.uniform(f_min, f_max, size=pop_size)\n        self.CR = np.random.uniform(cr_min, cr_max, size=pop_size)\n        self.adaptive_pop_reduction_freq = adaptive_pop_reduction_freq\n        self.generation = 0\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        self.archive = np.copy(self.population[:self.archive_size]) # Initialize archive with good solutions\n        self.archive_fitness = np.copy(self.fitness[:self.archive_size])\n\n    def mutate(self, i):\n        # Combined mutation strategy: DE/rand/1 and DE/current-to-best/1\n        if np.random.rand() < 0.5:  # Choose mutation strategy with probability 0.5\n            # DE/rand/1\n            indices = np.random.choice(self.pop_size + self.archive_size, 3, replace=False)\n            \n            if indices[0] < self.pop_size:\n                x_r1 = self.population[indices[0]]\n            else:\n                x_r1 = self.archive[indices[0] - self.pop_size]\n            \n            if indices[1] < self.pop_size:\n                x_r2 = self.population[indices[1]]\n            else:\n                x_r2 = self.archive[indices[1] - self.pop_size]\n\n            if indices[2] < self.pop_size:\n                x_r3 = self.population[indices[2]]\n            else:\n                x_r3 = self.archive[indices[2] - self.pop_size]\n\n            x_mutated = self.population[i] + self.F[i] * (x_r2 - x_r3)\n        else:\n            # DE/current-to-best/1\n            best_idx = np.argmin(self.fitness)\n            x_best = self.population[best_idx]\n            \n            indices = np.random.choice(self.pop_size + self.archive_size, 2, replace=False)\n            \n            if indices[0] < self.pop_size:\n                x_r1 = self.population[indices[0]]\n            else:\n                x_r1 = self.archive[indices[0] - self.pop_size]\n                \n            if indices[1] < self.pop_size:\n                x_r2 = self.population[indices[1]]\n            else:\n                x_r2 = self.archive[indices[1] - self.pop_size]\n            \n            x_mutated = self.population[i] + self.F[i] * (x_best - self.population[i]) + self.F[i] * (x_r1 - x_r2)\n\n        x_mutated = np.clip(x_mutated, -5.0, 5.0)\n        return x_mutated\n\n    def crossover(self, x_mutated, i):\n        x_trial = np.copy(self.population[i])\n        j_rand = np.random.randint(self.dim)\n        for j in range(self.dim):\n            if np.random.rand() < self.CR[i] or j == j_rand:\n                x_trial[j] = x_mutated[j]\n        return x_trial\n\n    def update_archive(self):\n        combined_population = np.concatenate((self.population, self.archive), axis=0)\n        combined_fitness = np.concatenate((self.fitness, self.archive_fitness))\n        \n        sorted_indices = np.argsort(combined_fitness)[:self.archive_size]\n        \n        self.archive = combined_population[sorted_indices].copy()\n        self.archive_fitness = combined_fitness[sorted_indices].copy()\n\n    def reduce_population(self):\n        # Reduce population size by half\n        if self.pop_size > 10:  # Minimum population size\n            num_to_remove = self.pop_size // 4 # Reduce by a quarter\n            \n            # Remove the worst individuals\n            worst_indices = np.argsort(self.fitness)[-num_to_remove:]\n            \n            mask = np.ones(self.pop_size, dtype=bool)\n            mask[worst_indices] = False\n            \n            self.population = self.population[mask]\n            self.fitness = self.fitness[mask]\n            self.pop_size = len(self.population)\n            self.F = self.F[:self.pop_size]\n            self.CR = self.CR[:self.pop_size]\n            \n            print(f\"Population reduced to {self.pop_size}\")\n            \n    def __call__(self, func):\n        self.initialize_population(func)\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)].copy()\n        \n        self.generation = 0\n        while self.budget > 0:\n            self.generation += 1\n            for i in range(self.pop_size):\n                x_mutated = self.mutate(i)\n                x_trial = self.crossover(x_mutated, i)\n                \n                f_trial = func(x_trial)\n                self.budget -= 1\n\n                if f_trial < self.fitness[i]:\n                    self.population[i] = x_trial.copy()\n                    self.fitness[i] = f_trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = x_trial.copy()\n                        \n                #Adapt F and CR\n                if np.random.rand() < 0.1:\n                  self.F[i] = np.random.uniform(self.f_min, self.f_max)\n                if np.random.rand() < 0.1:\n                  self.CR[i] = np.random.uniform(self.cr_min, self.cr_max)\n                \n                if self.budget <= 0:\n                    break\n            \n            self.update_archive()\n            \n            if self.generation % self.adaptive_pop_reduction_freq == 0:\n                self.reduce_population()\n            \n            if self.budget <= 0:\n                break\n        \n        best_index = np.argmin(self.fitness)\n        if self.fitness[best_index] < self.f_opt:\n            self.f_opt = self.fitness[best_index]\n            self.x_opt = self.population[best_index].copy()\n        \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "Evaluation timed out after 300 seconds.", "error": "Evaluation timed out after 300 seconds.", "parent_ids": ["88a94d8e-71cf-4b05-bce1-3703f430c9e5"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "f8fd358a-ceea-4f1a-b334-8311f00ac74f", "fitness": 0.5042030570483907, "name": "AdaptiveDE", "description": "Enhanced Adaptive Differential Evolution with orthogonal design for parameter tuning and a restart mechanism triggered by stagnation, balancing exploration and exploitation.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=25, initial_f=0.5, cr=0.7, local_search_freq=0.05, refresh_interval=500, stagnation_threshold=1000):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = initial_f  # Initial differential weight\n        self.CR = cr  # Crossover rate\n        self.local_search_freq = local_search_freq\n        self.refresh_interval = refresh_interval\n        self.stagnation_threshold = stagnation_threshold\n        self.population = None\n        self.fitness = None\n        self.eval_count = 0\n        self.best_fitness_history = []\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.eval_count += self.pop_size\n        return np.argmin(self.fitness)\n\n    def mutate(self, i):\n        indices = np.random.choice(self.pop_size, 3, replace=False)\n        x_r1 = self.population[indices[0]]\n        x_r2 = self.population[indices[1]]\n        x_r3 = self.population[indices[2]]\n\n        # Simplified F adaptation\n        self.F = np.clip(np.random.normal(self.F, 0.1), 0.1, 0.9)\n        x_mutated = self.population[i] + self.F * (x_r2 - x_r3)\n        return x_mutated\n\n    def crossover(self, x_mutated, i):\n        x_trial = np.copy(self.population[i])\n        j_rand = np.random.randint(self.dim)\n        for j in range(self.dim):\n            if np.random.rand() < self.CR or j == j_rand:\n                x_trial[j] = x_mutated[j]\n        return x_trial\n\n    def local_search(self, func, x_best):\n        # Gaussian local search around best solution\n        x_neighbor = x_best + np.random.normal(0, 0.05, self.dim)\n        x_neighbor = np.clip(x_neighbor, func.bounds.lb, func.bounds.ub)\n        f_neighbor = func(x_neighbor)\n        self.eval_count += 1\n        return f_neighbor, x_neighbor\n\n    def refresh_population(self, func):\n         # Replace a portion of the population with new random individuals\n        num_to_refresh = int(0.2 * self.pop_size)\n        indices_to_refresh = np.random.choice(self.pop_size, num_to_refresh, replace=False)\n        self.population[indices_to_refresh] = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(num_to_refresh, self.dim))\n        self.fitness[indices_to_refresh] = np.array([func(x) for x in self.population[indices_to_refresh]])\n        self.eval_count += num_to_refresh\n\n    def orthogonal_design(self):\n        # Simple orthogonal design for parameter tuning (F, CR)\n        levels = [0.3, 0.6, 0.9]\n        design = [(levels[0], levels[0]), (levels[1], levels[1]), (levels[2], levels[2]), (levels[0], levels[1]), (levels[1], levels[2]), (levels[2], levels[0])]\n        return design[np.random.randint(len(design))]\n\n    def restart(self, func):\n        # Restart the algorithm with a new population\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.eval_count += self.pop_size\n        best_idx = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_idx]\n        self.x_opt = self.population[best_idx].copy()\n        self.best_fitness_history = [self.f_opt]\n\n    def __call__(self, func):\n        best_idx = self.initialize_population(func)\n        self.f_opt = self.fitness[best_idx]\n        self.x_opt = self.population[best_idx].copy()\n        self.best_fitness_history = [self.f_opt]\n\n        while self.eval_count < self.budget:\n            # Parameter tuning using orthogonal design\n            new_F, new_CR = self.orthogonal_design()\n            \n            for i in range(self.pop_size):\n                x_mutated = self.mutate(i)\n                \n                #Use the new parameters F and CR\n                self.F = new_F\n                self.CR = new_CR\n                x_trial = self.crossover(x_mutated, i)\n                x_trial = np.clip(x_trial, func.bounds.lb, func.bounds.ub)\n                f_trial = func(x_trial)\n                self.eval_count += 1\n                \n                if f_trial < self.fitness[i]:\n                    self.population[i] = x_trial.copy()\n                    self.fitness[i] = f_trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = x_trial.copy()\n\n                if self.eval_count >= self.budget:\n                    break\n\n            if self.eval_count >= self.budget:\n                break\n            \n            if np.random.rand() < self.local_search_freq:\n                f_ls, x_ls = self.local_search(func, self.x_opt)\n                if f_ls < self.f_opt:\n                    self.f_opt = f_ls\n                    self.x_opt = x_ls.copy()\n\n            if self.eval_count % self.refresh_interval == 0:\n                self.refresh_population(func)\n\n            best_idx = np.argmin(self.fitness)\n            if self.fitness[best_idx] < self.f_opt:\n                self.f_opt = self.fitness[best_idx]\n                self.x_opt = self.population[best_idx].copy()\n\n            self.best_fitness_history.append(self.f_opt)\n\n            # Stagnation check and restart\n            if len(self.best_fitness_history) > self.stagnation_threshold:\n                if np.std(self.best_fitness_history[-self.stagnation_threshold:]) < 1e-6:\n                    self.restart(func) #restart if stagnant\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "The algorithm AdaptiveDE scored 0.504 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["c95cd30d-863b-4092-8020-ca6b414137bc"], "operator": null, "metadata": {"aucs": [0.17174384649842545, 0.3318152385599237, 0.4778000111042122, 0.7384128852045997, 0.4488267130147727, 0.6289532800237057, 0.3514796239554515, 0.4277750054079593, 0.5086666830621731, 0.20511859957607326, 0.7670256878478505, 0.9996614793932316, 0.3759159952242388, 0.3677981361476558, 0.8787457821057103, 0.5770381626725591, 0.4119458644171856, 0.7166013462319714, 0.19812619839585177, 0.500610602124264]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "637a27a3-84ad-4864-82e2-283c287c5a4a", "fitness": "-inf", "name": "OrthogonalSelfAdaptiveDE", "description": "Self-Adaptive Differential Evolution with orthogonal design for parameter adaptation and a simplified population reduction strategy.", "code": "import numpy as np\n\nclass OrthogonalSelfAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size_max=50, pop_size_min=10, archive_size=10, f_min=0.1, f_max=0.9, cr_min=0.1, cr_max=0.9, reduction_factor=0.95):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size_max = pop_size_max\n        self.pop_size_min = pop_size_min\n        self.pop_size = pop_size_max\n        self.archive_size = archive_size\n        self.f_min = f_min\n        self.f_max = f_max\n        self.cr_min = cr_min\n        self.cr_max = cr_max\n        self.reduction_factor = reduction_factor\n        self.population = None\n        self.fitness = None\n        self.archive = None\n        self.archive_fitness = None\n        self.F = None\n        self.CR = None\n        self.lb = -5.0\n        self.ub = 5.0\n        self.orthogonal_matrix = self.generate_orthogonal_array(4) # Pre-generate orthogonal array\n        self.param_sets = self.generate_parameter_sets()\n\n    def generate_orthogonal_array(self, strength):\n        # Simplified orthogonal array generation for 4 parameters at 3 levels.\n        # In a real-world implementation, a more robust library or method is needed.\n        # This is just to illustrate the concept and keep the code simple\n        return np.array([\n            [0, 0, 0, 0],\n            [0, 1, 1, 1],\n            [0, 2, 2, 2],\n            [1, 0, 1, 2],\n            [1, 1, 2, 0],\n            [1, 2, 0, 1],\n            [2, 0, 2, 1],\n            [2, 1, 0, 2],\n            [2, 2, 1, 0]\n        ])\n\n    def generate_parameter_sets(self):\n          # Define parameter ranges for orthogonal design (F_min, F_max, CR_min, CR_max)\n          levels = [np.linspace(self.f_min, self.f_max, 3),\n                    np.linspace(self.f_min, self.f_max, 3),\n                    np.linspace(self.cr_min, self.cr_max, 3),\n                    np.linspace(self.cr_min, self.cr_max, 3)]\n          \n          parameter_sets = []\n          for row in self.orthogonal_matrix:\n              f1, f2, cr1, cr2 = [levels[i][int(row[i])] for i in range(4)]\n              parameter_sets.append((f1, f2, cr1, cr2))\n          return parameter_sets\n        \n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        self.archive = self.population[:self.archive_size].copy()\n        self.archive_fitness = self.fitness[:self.archive_size].copy()\n        self.F = np.random.uniform(self.f_min, self.f_max, size=self.pop_size)\n        self.CR = np.random.uniform(self.cr_min, self.cr_max, size=self.pop_size)\n\n    def mutate(self, i):\n        indices = np.random.choice(self.pop_size + self.archive_size, 3, replace=False)\n        \n        x_r1 = self.population[indices[0] if indices[0] < self.pop_size else indices[0] - self.pop_size] if indices[0] < self.pop_size + self.archive_size else self.population[0]  # handle out of bound error\n        x_r2 = self.population[indices[1] if indices[1] < self.pop_size else indices[1] - self.pop_size] if indices[1] < self.pop_size + self.archive_size else self.population[1]  # handle out of bound error\n        x_r3 = self.population[indices[2] if indices[2] < self.pop_size else indices[2] - self.pop_size] if indices[2] < self.pop_size + self.archive_size else self.population[2]  # handle out of bound error\n        \n\n        x_mutated = self.population[i] + self.F[i] * (x_r2 - x_r3)\n        x_mutated = np.clip(x_mutated, self.lb, self.ub)\n\n        return x_mutated\n\n    def crossover(self, x_mutated, i):\n        x_trial = np.copy(self.population[i])\n        j_rand = np.random.randint(self.dim)\n        for j in range(self.dim):\n            if np.random.rand() < self.CR[i] or j == j_rand:\n                x_trial[j] = x_mutated[j]\n        return x_trial\n\n    def update_archive(self):\n        combined_population = np.concatenate((self.population, self.archive), axis=0)\n        combined_fitness = np.concatenate((self.fitness, self.archive_fitness))\n\n        sorted_indices = np.argsort(combined_fitness)[:self.archive_size]\n\n        self.archive = combined_population[sorted_indices].copy()\n        self.archive_fitness = combined_fitness[sorted_indices].copy()\n\n    def reduce_population(self):\n        if self.pop_size > self.pop_size_min:\n            self.pop_size = max(self.pop_size_min, int(self.pop_size * self.reduction_factor))  # Simplified reduction\n            \n            # Reduce population by removing the worst individuals\n            indices = np.argsort(self.fitness)[:self.pop_size]\n            self.population = self.population[indices].copy()\n            self.fitness = self.fitness[indices].copy()\n            self.F = self.F[indices].copy()\n            self.CR = self.CR[indices].copy()\n\n    def adapt_parameters(self):\n        # Select a parameter set using orthogonal design\n        param_set_index = np.random.randint(len(self.orthogonal_matrix))\n        f_min, f_max, cr_min, cr_max = self.parameter_sets[param_set_index]\n        \n        # Adapt F and CR values of individuals\n        self.F = np.random.uniform(f_min, f_max, size=self.pop_size)\n        self.CR = np.random.uniform(cr_min, cr_max, size=self.pop_size)\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)].copy()\n\n        generation = 0\n        while self.budget > 0:\n            generation += 1\n\n            # Parameter adaptation using orthogonal design\n            if generation % 5 == 0:\n                self.adapt_parameters()\n\n            for i in range(self.pop_size):\n                x_mutated = self.mutate(i)\n                x_trial = self.crossover(x_mutated, i)\n\n                f_trial = func(x_trial)\n                self.budget -= 1\n\n                if f_trial < self.fitness[i]:\n                    self.population[i] = x_trial.copy()\n                    self.fitness[i] = f_trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = x_trial.copy()\n\n                if self.budget <= 0:\n                    break\n\n            self.update_archive()\n            self.reduce_population()\n\n            if self.budget <= 0:\n                break\n\n        best_index = np.argmin(self.fitness)\n        if self.fitness[best_index] < self.f_opt:\n            self.f_opt = self.fitness[best_index]\n            self.x_opt = self.population[best_index].copy()\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "In the code, line 109, in adapt_parameters, the following error occurred:\nAttributeError: 'OrthogonalSelfAdaptiveDE' object has no attribute 'parameter_sets'. Did you mean: 'param_sets'?\nOn line: f_min, f_max, cr_min, cr_max = self.parameter_sets[param_set_index]", "error": "In the code, line 109, in adapt_parameters, the following error occurred:\nAttributeError: 'OrthogonalSelfAdaptiveDE' object has no attribute 'parameter_sets'. Did you mean: 'param_sets'?\nOn line: f_min, f_max, cr_min, cr_max = self.parameter_sets[param_set_index]", "parent_ids": ["9dad5aa3-4b10-49c8-b303-c22499938e4b"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "a455edb1-df55-4b16-a8f8-150e6d5b699b", "fitness": 0.4609840554992073, "name": "AdaptiveDE", "description": "Adaptive Differential Evolution with orthogonal design-based mutation and dynamic parameter adjustment, leveraging past success for exploration-exploitation balance.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=25, initial_f=0.5, cr=0.7, local_search_freq=0.05, refresh_interval=500, archive_size=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = initial_f\n        self.CR = cr\n        self.local_search_freq = local_search_freq\n        self.refresh_interval = refresh_interval\n        self.population = None\n        self.fitness = None\n        self.eval_count = 0\n        self.archive = []  # Archive for successful F/CR values\n        self.archive_size = archive_size\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.eval_count += self.pop_size\n        return np.argmin(self.fitness)\n\n    def orthogonal_mutation(self, i):\n        # Orthogonal design-based mutation\n        indices = np.random.choice(self.pop_size, 4, replace=False)  # Increased to 4\n        x_r1 = self.population[indices[0]]\n        x_r2 = self.population[indices[1]]\n        x_r3 = self.population[indices[2]]\n        x_r4 = self.population[indices[3]]\n\n        F = self.F\n        if self.archive:\n            F = np.random.choice(self.archive) # sample from archive\n\n        x_mutated = self.population[i] + F * (x_r1 - x_r2) + F * (x_r3 - x_r4) # Adding another difference vector\n        return x_mutated\n\n    def crossover(self, x_mutated, i):\n        x_trial = np.copy(self.population[i])\n        j_rand = np.random.randint(self.dim)\n        cr = self.CR\n        if self.archive:\n            cr = np.random.choice(self.archive)\n\n        for j in range(self.dim):\n            if np.random.rand() < cr or j == j_rand:\n                x_trial[j] = x_mutated[j]\n        return x_trial\n\n    def local_search(self, func, x_best):\n        x_neighbor = x_best + np.random.normal(0, 0.05, self.dim)\n        x_neighbor = np.clip(x_neighbor, func.bounds.lb, func.bounds.ub)\n        f_neighbor = func(x_neighbor)\n        self.eval_count += 1\n        return f_neighbor, x_neighbor\n\n    def refresh_population(self, func):\n        num_to_refresh = int(0.1 * self.pop_size) # Reduced refresh rate\n        indices_to_refresh = np.random.choice(self.pop_size, num_to_refresh, replace=False)\n        self.population[indices_to_refresh] = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(num_to_refresh, self.dim))\n        self.fitness[indices_to_refresh] = np.array([func(x) for x in self.population[indices_to_refresh]])\n        self.eval_count += num_to_refresh\n\n    def __call__(self, func):\n        best_idx = self.initialize_population(func)\n        self.f_opt = self.fitness[best_idx]\n        self.x_opt = self.population[best_idx].copy()\n\n        while self.eval_count < self.budget:\n            for i in range(self.pop_size):\n                x_mutated = self.orthogonal_mutation(i)\n                x_trial = self.crossover(x_mutated, i)\n                x_trial = np.clip(x_trial, func.bounds.lb, func.bounds.ub)\n                f_trial = func(x_trial)\n                self.eval_count += 1\n\n                if f_trial < self.fitness[i]:\n                    # Success, update and archive F/CR\n                    delta_f = self.fitness[i] - f_trial\n                    self.population[i] = x_trial.copy()\n                    self.fitness[i] = f_trial\n\n                    self.archive.append(self.F)\n                    self.archive.append(self.CR)\n\n                    if len(self.archive) > self.archive_size:\n                        self.archive = self.archive[-self.archive_size:]\n\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = x_trial.copy()\n\n                if self.eval_count >= self.budget:\n                    break\n\n            if self.eval_count >= self.budget:\n                break\n\n            if np.random.rand() < self.local_search_freq:\n                f_ls, x_ls = self.local_search(func, self.x_opt)\n                if f_ls < self.f_opt:\n                    self.f_opt = f_ls\n                    self.x_opt = x_ls.copy()\n\n            if self.eval_count % self.refresh_interval == 0:\n                self.refresh_population(func)\n\n            best_idx = np.argmin(self.fitness)\n            if self.fitness[best_idx] < self.f_opt:\n                self.f_opt = self.fitness[best_idx]\n                self.x_opt = self.population[best_idx].copy()\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "The algorithm AdaptiveDE scored 0.461 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["c95cd30d-863b-4092-8020-ca6b414137bc"], "operator": null, "metadata": {"aucs": [0.15902384020158788, 0.25044412682010486, 0.4490147030456828, 0.6206283018880234, 0.35999140826300147, 0.6018916297996428, 0.31371624190303393, 0.41295658264168444, 0.45475795071265745, 0.20677829159650352, 0.6507193027829239, 0.9950088026959362, 0.2939126192207704, 0.3391237669794437, 0.7705769257783714, 0.5676469525861921, 0.3599024547612988, 0.704621515911311, 0.21630619743293233, 0.49265949496304307]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "73551b59-70ed-4fbd-a6e6-cc84bc34a85c", "fitness": 0.6871905290022154, "name": "AdaptiveDE", "description": "Adaptive Differential Evolution with improved parameter adaptation, a repair mechanism using the mean of successful solutions, and a strategy to jump out of local optima based on population diversity.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, initial_f=0.5, initial_cr=0.9, archive_size=10, p_best=0.05, gamma_f=0.1, gamma_cr=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = initial_f\n        self.CR = initial_cr\n        self.population = None\n        self.fitness = None\n        self.archive = []\n        self.archive_size = archive_size\n        self.sf = []\n        self.scr = []\n        self.memory_f = [initial_f] * archive_size\n        self.memory_cr = [initial_cr] * archive_size\n        self.p_best = p_best\n        self.gamma_f = gamma_f\n        self.gamma_cr = gamma_cr\n        self.success_solutions = []\n\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n\n    def mutate(self, i):\n        phi_idx = np.random.randint(0, self.archive_size)\n        self.F = self.memory_f[phi_idx]\n        self.CR = self.memory_cr[phi_idx]\n\n        self.F = np.clip(np.random.normal(self.F, self.gamma_f), 0.1, 1.0)\n        self.CR = np.clip(np.random.normal(self.CR, self.gamma_cr), 0.0, 1.0)\n\n        idxs = np.random.choice(self.pop_size, 4, replace=False)\n        a, b, c, p_best_idx = idxs[0], idxs[1], idxs[2], idxs[3]\n\n        if np.random.rand() < self.p_best:\n            p_best_idx = np.argmin(self.fitness)\n        \n        x_mutated = self.population[i] + self.F * (self.population[p_best_idx] - self.population[i]) + self.F * (self.population[a] - self.population[b])\n\n        return x_mutated, self.F, self.CR\n\n    def crossover(self, x_mutated, i, cr):\n        x_trial = np.copy(self.population[i])\n        j_rand = np.random.randint(self.dim)\n        for j in range(self.dim):\n            if np.random.rand() < cr or j == j_rand:\n                x_trial[j] = x_mutated[j]\n        return x_trial\n\n    def repair(self, x_trial, func):\n         # Repair mechanism using the mean of successful solutions\n        if self.success_solutions:\n            mean_success = np.mean(self.success_solutions, axis=0)\n            for j in range(self.dim):\n                if not func.bounds.lb[j] <= x_trial[j] <= func.bounds.ub[j]:\n                    x_trial[j] = 0.5 * (x_trial[j] + mean_success[j])\n\n        x_trial = np.clip(x_trial, func.bounds.lb, func.bounds.ub)\n        return x_trial\n\n\n    def update_memory(self):\n        if self.sf:\n            self.memory_f.append(sum(val**2 for val in self.sf) / sum(self.sf))\n            self.memory_f.pop(0)\n            self.memory_cr.append(np.mean(self.scr))\n            self.memory_cr.pop(0)\n        self.sf = []\n        self.scr = []\n\n    def jump_out_of_local_optima(self, func):\n        # Jump out of local optima if population diversity is low\n        diversity = np.std(self.fitness)\n        if diversity < 1e-6:  # Adjust threshold as needed\n            # Option 1: Randomize a portion of the population\n            num_to_randomize = int(0.1 * self.pop_size)\n            indices_to_randomize = np.random.choice(self.pop_size, num_to_randomize, replace=False)\n            self.population[indices_to_randomize] = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(num_to_randomize, self.dim))\n            self.fitness[indices_to_randomize] = [func(x) for x in self.population[indices_to_randomize]]\n            self.budget -= num_to_randomize\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)].copy()\n        self.sf = []\n        self.scr = []\n        self.success_solutions = [] # Reset successful solutions\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                x_mutated, F, CR = self.mutate(i)\n                x_trial = self.crossover(x_mutated, i, CR)\n                x_trial = self.repair(x_trial, func)\n                \n                f_trial = func(x_trial)\n                self.budget -= 1\n                if self.budget <= 0:\n                    break\n                if f_trial < self.fitness[i]:\n                    self.sf.append(F)\n                    self.scr.append(CR)\n                    self.success_solutions.append(x_trial.copy())  # Store successful solution\n                    \n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(self.population[i].copy())\n                    else:\n                        j = np.random.randint(0, self.archive_size)\n                        self.archive[j] = self.population[i].copy()\n                    self.population[i] = x_trial.copy()\n                    self.fitness[i] = f_trial\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = x_trial.copy()\n            if self.budget <= 0:\n                break\n            self.update_memory()\n            self.jump_out_of_local_optima(func)\n\n            best_index = np.argmin(self.fitness)\n            if self.fitness[best_index] < self.f_opt:\n                self.f_opt = self.fitness[best_index]\n                self.x_opt = self.population[best_index].copy()\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "The algorithm AdaptiveDE scored 0.687 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["555dcdca-320a-4ce9-b585-17f051aba286"], "operator": null, "metadata": {"aucs": [0.25056353657079145, 0.7297926420146384, 0.6545258757137675, 0.8792289280404857, 0.7427733597917694, 0.8362750648979665, 0.580345766120616, 0.7153323722988232, 0.8201036085375828, 0.23349116162387495, 0.8402912027508501, 0.9992454339436714, 0.7663990080212177, 0.7257806136794829, 0.9089139328950485, 0.7850014017549263, 0.6774780472686435, 0.8726885751918958, 0.22111567336675142, 0.5044643755615061]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "3b69d2d7-7614-4711-9091-e9dc08c91a59", "fitness": "-inf", "name": "EnhancedSelfAdaptiveDE", "description": "An Enhanced Self-Adaptive Differential Evolution with aging population, orthogonal learning, and covariance matrix adaptation for improved exploration and exploitation.", "code": "import numpy as np\n\nclass EnhancedSelfAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=20, initial_f=0.5, initial_cr=0.7, \n                 local_search_prob=0.1, refresh_interval=500, aging_rate=0.02, orthogonal_learning_prob=0.1,\n                 cma_learning_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.F = initial_f\n        self.CR = initial_cr\n        self.local_search_prob = local_search_prob\n        self.refresh_interval = refresh_interval\n        self.aging_rate = aging_rate\n        self.orthogonal_learning_prob = orthogonal_learning_prob\n        self.cma_learning_rate = cma_learning_rate\n        self.population = None\n        self.fitness = None\n        self.archive = None\n        self.archive_fitness = None\n        self.eval_count = 0\n        self.ages = None\n        self.covariance_matrix = None\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.eval_count += self.pop_size\n        self.archive = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.archive_size, self.dim))\n        self.archive_fitness = np.full(self.archive_size, np.inf)\n        self.ages = np.zeros(self.pop_size)\n        self.covariance_matrix = np.eye(self.dim)\n\n    def mutate(self, i):\n        indices = np.random.choice(self.pop_size + self.archive_size, 3, replace=False)\n        x_r1 = self.population[indices[0]] if indices[0] < self.pop_size else self.archive[indices[0] - self.pop_size]\n        x_r2 = self.population[indices[1]] if indices[1] < self.pop_size else self.archive[indices[1] - self.pop_size]\n        x_r3 = self.population[indices[2]] if indices[2] < self.pop_size else self.archive[indices[2] - self.pop_size]\n        \n        self.F = np.clip(np.random.normal(self.F, 0.1), 0.1, 0.9)\n        x_mutated = self.population[i] + self.F * (x_r2 - x_r3)\n        return x_mutated\n\n    def crossover(self, x_mutated, i):\n        x_trial = np.copy(self.population[i])\n        j_rand = np.random.randint(self.dim)\n        for j in range(self.dim):\n            if np.random.rand() < self.CR or j == j_rand:\n                x_trial[j] = x_mutated[j]\n        return x_trial\n\n    def local_search(self, func, x):\n        x_neighbor = x + np.random.normal(0, 0.05, self.dim)\n        x_neighbor = np.clip(x_neighbor, func.bounds.lb, func.bounds.ub)\n        f_neighbor = func(x_neighbor)\n        self.eval_count += 1\n        return f_neighbor, x_neighbor\n\n    def orthogonal_learning(self, func, x):\n        # Orthogonal array-based experimental design\n        levels = 3  # Example: low, medium, high\n        orthogonal_array = np.array([\n            [-1, -1],\n            [-1, 1],\n            [1, -1],\n            [1, 1]\n        ])  # 2 factors at 2 levels; L4 array\n\n        best_f = np.inf\n        best_x = x.copy()\n\n        for i in range(orthogonal_array.shape[0]):\n            x_trial = x.copy()\n            for j in range(orthogonal_array.shape[1]):\n                delta = 0.1 * (func.bounds.ub - func.bounds.lb) #scale of change\n                x_trial[j] = x[j] + orthogonal_array[i, j] * delta\n                x_trial = np.clip(x_trial, func.bounds.lb, func.bounds.ub)\n\n            f_trial = func(x_trial)\n            self.eval_count += 1\n\n            if f_trial < best_f:\n                best_f = f_trial\n                best_x = x_trial.copy()\n\n        return best_f, best_x\n    \n    def update_covariance_matrix(self):\n        diffs = self.population - np.mean(self.population, axis=0)\n        self.covariance_matrix = (1 - self.cma_learning_rate) * self.covariance_matrix + \\\n                                  self.cma_learning_rate * np.cov(diffs, rowvar=False)\n        # Ensure the covariance matrix is positive semi-definite\n        try:\n            np.linalg.cholesky(self.covariance_matrix)\n        except np.linalg.LinAlgError:\n            self.covariance_matrix = np.diag(np.diag(self.covariance_matrix)) # diagonal matrix\n\n    def update_archive(self):\n        if self.archive_size > 0 and self.pop_size > 0:\n            archive_indices = np.random.choice(self.archive_size, self.pop_size, replace=True)\n            for i in range(self.pop_size):\n                if self.fitness[i] < self.archive_fitness[archive_indices[i]]:\n                    self.archive[archive_indices[i]] = self.population[i].copy()\n                    self.archive_fitness[archive_indices[i]] = self.fitness[i]\n\n    def refresh_population(self, func):\n        num_to_replace = int(0.2 * self.pop_size)\n        worst_indices = np.argsort(self.fitness)[-num_to_replace:]\n        for i in worst_indices:\n            self.population[i] = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n            self.fitness[i] = func(self.population[i])\n            self.eval_count += 1\n            self.ages[i] = 0 #reset age\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)].copy()\n\n        while self.eval_count < self.budget:\n            for i in range(self.pop_size):\n                x_mutated = self.mutate(i)\n                x_trial = self.crossover(x_mutated, i)\n                x_trial = np.clip(x_trial, func.bounds.lb, func.bounds.ub)\n                f_trial = func(x_trial)\n                self.eval_count += 1\n                \n                if self.eval_count >= self.budget:\n                    break\n\n                if np.random.rand() < self.local_search_prob:\n                    f_ls, x_ls = self.local_search(func, x_trial)\n                    if f_ls < f_trial:\n                        f_trial = f_ls\n                        x_trial = x_ls.copy()\n                \n                if np.random.rand() < self.orthogonal_learning_prob:\n                    f_ol, x_ol = self.orthogonal_learning(func, x_trial)\n                    if f_ol < f_trial:\n                        f_trial = f_ol\n                        x_trial = x_ol.copy()\n\n                if f_trial < self.fitness[i]:\n                    self.population[i] = x_trial.copy()\n                    self.fitness[i] = f_trial\n                    self.ages[i] = 0 #reset age\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = x_trial.copy()\n                else:\n                    self.ages[i] += self.aging_rate #increase age if not improving\n\n            if self.eval_count >= self.budget:\n                break\n            \n            self.update_covariance_matrix() #update cma\n\n            # Age-based replacement\n            for i in range(self.pop_size):\n                if self.ages[i] > 1.0:  # Example aging threshold\n                    self.population[i] = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                    self.fitness[i] = func(self.population[i])\n                    self.eval_count += 1\n                    self.ages[i] = 0\n\n            self.update_archive()\n\n            if self.eval_count % self.refresh_interval == 0:\n                self.refresh_population(func)\n\n            best_index = np.argmin(self.fitness)\n            if self.fitness[best_index] < self.f_opt:\n                self.f_opt = self.fitness[best_index]\n                self.x_opt = self.population[best_index].copy()\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "In the code, line 77, in orthogonal_learning, the following error occurred:\nValueError: setting an array element with a sequence.\nOn line: x_trial[j] = x[j] + orthogonal_array[i, j] * delta", "error": "In the code, line 77, in orthogonal_learning, the following error occurred:\nValueError: setting an array element with a sequence.\nOn line: x_trial[j] = x[j] + orthogonal_array[i, j] * delta", "parent_ids": ["46647a49-c7e3-49dc-90b8-ce2bf4ee948b"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "bec6775a-1477-41a7-9eca-0b3ea45d64c9", "fitness": "-inf", "name": "HybridDE", "description": "A hybrid DE algorithm that combines a self-adaptive parameter control with a restart mechanism triggered by stagnation detection and enhanced with orthogonal learning to improve population diversity and convergence.", "code": "import numpy as np\n\nclass HybridDE:\n    def __init__(self, budget=10000, dim=10, pop_size=30, archive_size=15, initial_f=0.6, cr=0.8, local_search_prob=0.1, refresh_interval=500, stagnation_threshold=1e-5, stagnation_window=1000):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.F = initial_f  # Initial differential weight\n        self.CR = cr  # Crossover rate\n        self.local_search_prob = local_search_prob\n        self.refresh_interval = refresh_interval\n        self.stagnation_threshold = stagnation_threshold\n        self.stagnation_window = stagnation_window\n        self.population = None\n        self.fitness = None\n        self.archive = None\n        self.archive_fitness = None\n        self.eval_count = 0\n        self.best_fitness_history = []\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.eval_count += self.pop_size\n        self.archive = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.archive_size, self.dim))\n        self.archive_fitness = np.full(self.archive_size, np.inf)\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)].copy()\n        self.best_fitness_history.append(self.f_opt)\n\n    def mutate(self, i):\n        indices = np.random.choice(self.pop_size + self.archive_size, 3, replace=False)\n        \n        x_r1 = self.population[indices[0]] if indices[0] < self.pop_size else self.archive[indices[0] - self.pop_size]\n        x_r2 = self.population[indices[1]] if indices[1] < self.pop_size else self.archive[indices[1] - self.pop_size]\n        x_r3 = self.population[indices[2]] if indices[2] < self.pop_size else self.archive[indices[2] - self.pop_size]\n\n        self.F = np.clip(np.random.normal(self.F, 0.1), 0.1, 0.9)\n        x_mutated = self.population[i] + self.F * (x_r2 - x_r3)\n        return x_mutated\n\n    def crossover(self, x_mutated, i):\n        x_trial = np.copy(self.population[i])\n        j_rand = np.random.randint(self.dim)\n        for j in range(self.dim):\n            if np.random.rand() < self.CR or j == j_rand:\n                x_trial[j] = x_mutated[j]\n        return x_trial\n\n    def local_search(self, func, x):\n        x_neighbor = x + np.random.normal(0, 0.05, self.dim)\n        x_neighbor = np.clip(x_neighbor, func.bounds.lb, func.bounds.ub)\n        f_neighbor = func(x_neighbor)\n        self.eval_count += 1\n        return f_neighbor, x_neighbor\n\n    def update_archive(self):\n        if self.archive_size > 0 and self.pop_size > 0:\n            archive_indices = np.random.choice(self.archive_size, self.pop_size, replace=True)\n            for i in range(self.pop_size):\n                if self.fitness[i] < self.archive_fitness[archive_indices[i]]:\n                    self.archive[archive_indices[i]] = self.population[i].copy()\n                    self.archive_fitness[archive_indices[i]] = self.fitness[i]\n\n    def refresh_population(self, func):\n        # Replace bottom 50% of population with orthogonal design-based individuals\n        num_to_replace = int(0.5 * self.pop_size)\n        worst_indices = np.argsort(self.fitness)[-num_to_replace:]\n        \n        # Generate orthogonal design\n        orthogonal_matrix = self.generate_orthogonal_array(num_to_replace)\n        \n        for i, idx in enumerate(worst_indices):\n            # Map orthogonal design to the search space\n            self.population[idx] = func.bounds.lb + orthogonal_matrix[i] * (func.bounds.ub - func.bounds.lb)\n            self.fitness[idx] = func(self.population[idx])\n            self.eval_count += 1\n\n    def generate_orthogonal_array(self, n_points):\n        # A simplified orthogonal array generation (L_n(2^n)) - only powers of 2 are supported here for simplicity.\n        # For more general arrays, a library like pyDOE2 should be used.\n        n = self.dim # Use problem dimension\n        if n > 10:\n            n = 10  #Limit for basic implementation\n\n        matrix = np.zeros((n, n), dtype=int)\n\n        # First column is all 0s\n        for i in range(1, n):\n            matrix[i, 0] = 0\n\n        # Fill other columns\n        for j in range(1, n):\n            for i in range(n):\n                matrix[i, j] = (i // (2**(j-1))) % 2 # Binary representation of row index\n        \n        # Convert binary to continuous values in [0,1]\n        orthogonal_matrix = np.random.rand(n_points, self.dim)  #In case n_points < n\n        \n        for i in range(min(n_points,n)):\n            for j in range(self.dim):\n                if j < n:\n                    orthogonal_matrix[i, j] = matrix[j,i]\n                else:\n                    orthogonal_matrix[i, j] = np.random.rand() #Fill with randomness if problem dimension > n\n        \n        return orthogonal_matrix[:n_points, :]\n    \n    def check_stagnation(self):\n        if len(self.best_fitness_history) > self.stagnation_window:\n            std_dev = np.std(self.best_fitness_history[-self.stagnation_window:])\n            if std_dev < self.stagnation_threshold:\n                return True\n        return False\n            \n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.eval_count < self.budget:\n            for i in range(self.pop_size):\n                x_mutated = self.mutate(i)\n                x_trial = self.crossover(x_mutated, i)\n                x_trial = np.clip(x_trial, func.bounds.lb, func.bounds.ub)\n                f_trial = func(x_trial)\n                self.eval_count += 1\n                \n                if self.eval_count >= self.budget:\n                    break\n\n                if np.random.rand() < self.local_search_prob:\n                    f_ls, x_ls = self.local_search(func, x_trial)\n                    if f_ls < f_trial:\n                        f_trial = f_ls\n                        x_trial = x_ls.copy()\n\n                if f_trial < self.fitness[i]:\n                    self.population[i] = x_trial.copy()\n                    self.fitness[i] = f_trial\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = x_trial.copy()\n\n            if self.eval_count >= self.budget:\n                break\n\n            self.update_archive()\n\n            if self.eval_count % self.refresh_interval == 0:\n                self.refresh_population(func)\n\n            best_index = np.argmin(self.fitness)\n            if self.fitness[best_index] < self.f_opt:\n                self.f_opt = self.fitness[best_index]\n                self.x_opt = self.population[best_index].copy()\n                \n            self.best_fitness_history.append(self.f_opt)\n            \n            if self.check_stagnation():\n                self.refresh_population(func)  # More aggressive refresh on stagnation\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "Evaluation timed out after 300 seconds.", "error": "Evaluation timed out after 300 seconds.", "parent_ids": ["46647a49-c7e3-49dc-90b8-ce2bf4ee948b"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "0ff31b83-4571-40b3-9b3c-2d5d680cc085", "fitness": 0.5943448670623963, "name": "SimplifiedMirroredSelfAdaptiveDE", "description": "A simplified Self-Adaptive Differential Evolution with mirrored sampling and a success-history archive to guide the search, using adaptive F/CR updates and population reduction for efficiency.", "code": "import numpy as np\n\nclass SimplifiedMirroredSelfAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size_max=40, pop_size_min=10, archive_size=5, f_min=0.1, f_max=0.9, cr_min=0.1, cr_max=0.9, reduction_factor=0.95, p_adapt=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size_max = pop_size_max\n        self.pop_size_min = pop_size_min\n        self.pop_size = pop_size_max\n        self.archive_size = archive_size\n        self.f_min = f_min\n        self.f_max = f_max\n        self.cr_min = cr_min\n        self.cr_max = cr_max\n        self.reduction_factor = reduction_factor\n        self.p_adapt = p_adapt # Probability of adapting F/CR\n        self.population = None\n        self.fitness = None\n        self.archive = None\n        self.archive_fitness = None\n        self.F = None\n        self.CR = None\n        self.lb = -5.0\n        self.ub = 5.0\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        self.archive = self.population[:self.archive_size].copy()\n        self.archive_fitness = self.fitness[:self.archive_size].copy()\n        self.F = np.random.uniform(self.f_min, self.f_max, size=self.pop_size)\n        self.CR = np.random.uniform(self.cr_min, self.cr_max, size=self.pop_size)\n\n    def mutate(self, i):\n        indices = np.random.choice(self.pop_size + self.archive_size, 3, replace=False)\n        \n        # Simplify selection: prioritize population members.\n        r1 = self.population[indices[0] % self.pop_size] #Modulo for population selection\n        r2 = self.population[indices[1] % self.pop_size]\n        \n        #Chance to select from archive for r3\n        if indices[2] < self.pop_size:\n            r3 = self.population[indices[2]]\n        else:\n            r3 = self.archive[indices[2] - self.pop_size]\n\n        x_mutated = self.population[i] + self.F[i] * (r2 - r3)\n        x_mutated = np.clip(x_mutated, self.lb, self.ub) # Clipping\n\n        #Mirrored sampling\n        for j in range(self.dim):\n            if x_mutated[j] < self.lb:\n                x_mutated[j] = self.lb + (self.lb - x_mutated[j])\n            elif x_mutated[j] > self.ub:\n                x_mutated[j] = self.ub - (x_mutated[j] - self.ub)\n        return x_mutated\n\n    def crossover(self, x_mutated, i):\n        x_trial = self.population[i].copy()\n        j_rand = np.random.randint(self.dim)\n        for j in range(self.dim):\n            if np.random.rand() < self.CR[i] or j == j_rand:\n                x_trial[j] = x_mutated[j]\n        return x_trial\n\n    def update_archive(self):\n        combined_population = np.concatenate((self.population, self.archive), axis=0)\n        combined_fitness = np.concatenate((self.fitness, self.archive_fitness))\n        sorted_indices = np.argsort(combined_fitness)[:self.archive_size]\n        self.archive = combined_population[sorted_indices].copy()\n        self.archive_fitness = combined_fitness[sorted_indices].copy()\n\n    def reduce_population(self):\n        if self.pop_size > self.pop_size_min:\n            num_to_remove = max(1, int((self.pop_size - self.pop_size_min) * (1 - self.reduction_factor)))\n            worst_indices = np.argsort(self.fitness)[-num_to_remove:]\n            mask = np.ones(self.pop_size, dtype=bool)\n            mask[worst_indices] = False\n            self.population = self.population[mask]\n            self.fitness = self.fitness[mask]\n            self.F = self.F[mask]\n            self.CR = self.CR[mask]\n            self.pop_size = len(self.population)\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_index]\n        self.x_opt = self.population[best_index].copy()\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                x_mutated = self.mutate(i)\n                x_trial = self.crossover(x_mutated, i)\n                f_trial = func(x_trial)\n                self.budget -= 1\n\n                if f_trial < self.fitness[i]:\n                    self.population[i] = x_trial.copy()\n                    self.fitness[i] = f_trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = x_trial.copy()\n\n                # Adapt F and CR\n                if np.random.rand() < self.p_adapt:\n                    self.F[i] = np.random.uniform(self.f_min, self.f_max)\n                    self.CR[i] = np.random.uniform(self.cr_min, self.cr_max)\n                    \n                if self.budget <= 0:\n                    break\n                    \n            self.update_archive()\n            self.reduce_population()\n\n            if self.budget <= 0:\n                break\n\n        best_index = np.argmin(self.fitness)\n        if self.fitness[best_index] < self.f_opt:\n            self.f_opt = self.fitness[best_index]\n            self.x_opt = self.population[best_index].copy()\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "The algorithm SimplifiedMirroredSelfAdaptiveDE scored 0.594 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["9dad5aa3-4b10-49c8-b303-c22499938e4b"], "operator": null, "metadata": {"aucs": [0.19826039152674646, 0.36242459003635763, 0.6911142006246891, 0.848078658440751, 0.5209435840997626, 0.7828370640686458, 0.33806037087992824, 0.5778511517765152, 0.689109506689928, 0.3800535049090996, 0.8730330307682298, 0.9842154951516179, 0.37931905082697626, 0.5150000951327498, 0.8757169846957302, 0.7468157801289145, 0.4936104717317974, 0.863890176953066, 0.2650298181406028, 0.5015334146658198]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "3c377e79-26ed-41cf-8828-7863aaef3453", "fitness": 0.8018239315791102, "name": "DynamicResourceAllocationDE", "description": "Population-based algorithm with dynamic resource allocation, utilizing a multi-strategy approach combining global exploration with local exploitation and adaptive parameter control to efficiently navigate the search space.", "code": "import numpy as np\n\nclass DynamicResourceAllocationDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, num_strategies=3, local_search_probability=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.num_strategies = num_strategies\n        self.local_search_probability = local_search_probability\n        self.population = None\n        self.fitness = None\n        self.strategy_successes = np.zeros(num_strategies)\n        self.strategy_counts = np.zeros(num_strategies)\n        self.strategy_weights = np.ones(num_strategies) / num_strategies\n        self.F = 0.5\n        self.CR = 0.9\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n\n    def select_strategy(self):\n        return np.random.choice(self.num_strategies, p=self.strategy_weights)\n\n    def mutate(self, strategy, i):\n        if strategy == 0:  # DE/rand/1\n            idxs = np.random.choice(self.pop_size, 5, replace=False)\n            a, b, c, d, e = idxs\n            x_mutated = self.population[a] + self.F * (self.population[b] - self.population[c]) + self.F * (self.population[d] - self.population[e])\n        elif strategy == 1:  # DE/current-to-best/1\n            best_idx = np.argmin(self.fitness)\n            idxs = np.random.choice(self.pop_size, 3, replace=False)\n            a, b, c = idxs\n            x_mutated = self.population[i] + self.F * (self.population[best_idx] - self.population[i]) + self.F * (self.population[a] - self.population[b])\n        elif strategy == 2:  # DE/best/2\n             best_idx = np.argmin(self.fitness)\n             idxs = np.random.choice(self.pop_size, 4, replace=False)\n             a, b, c, d = idxs\n             x_mutated = self.population[best_idx] + self.F * (self.population[a] - self.population[b]) + self.F * (self.population[c] - self.population[d])\n\n        return x_mutated\n\n    def crossover(self, x_mutated, i):\n        x_trial = np.copy(self.population[i])\n        j_rand = np.random.randint(self.dim)\n        for j in range(self.dim):\n            if np.random.rand() < self.CR or j == j_rand:\n                x_trial[j] = x_mutated[j]\n        return x_trial\n\n    def local_search(self, x, func):\n        # Apply Gaussian local search around the current solution\n        x_ls = x + np.random.normal(0, 0.01, self.dim)  # Small perturbation\n        x_ls = np.clip(x_ls, func.bounds.lb, func.bounds.ub)\n        return x_ls\n\n    def update_strategy_weights(self):\n        total_successes = np.sum(self.strategy_successes)\n        if total_successes > 0:\n            self.strategy_weights = self.strategy_successes / total_successes\n        else:\n            self.strategy_weights = np.ones(self.num_strategies) / self.num_strategies\n\n        # Smoothing the weights to avoid premature convergence\n        self.strategy_weights = 0.9 * self.strategy_weights + 0.1 / self.num_strategies\n        self.strategy_weights /= np.sum(self.strategy_weights)\n\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)].copy()\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                strategy = self.select_strategy()\n                self.strategy_counts[strategy] += 1\n\n                x_mutated = self.mutate(strategy, i)\n                x_trial = self.crossover(x_mutated, i)\n                x_trial = np.clip(x_trial, func.bounds.lb, func.bounds.ub)\n\n                # Local Search\n                if np.random.rand() < self.local_search_probability:\n                    x_trial = self.local_search(x_trial, func)\n\n                f_trial = func(x_trial)\n                self.budget -= 1\n\n                if self.budget <= 0:\n                    break\n\n                if f_trial < self.fitness[i]:\n                    self.strategy_successes[strategy] += 1\n                    self.population[i] = x_trial.copy()\n                    self.fitness[i] = f_trial\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = x_trial.copy()\n\n            if self.budget <= 0:\n                break\n            \n            self.update_strategy_weights()\n            best_index = np.argmin(self.fitness)\n            if self.fitness[best_index] < self.f_opt:\n                self.f_opt = self.fitness[best_index]\n                self.x_opt = self.population[best_index].copy()\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "The algorithm DynamicResourceAllocationDE scored 0.802 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["555dcdca-320a-4ce9-b585-17f051aba286"], "operator": null, "metadata": {"aucs": [0.2819334076036172, 0.7839192220461414, 0.8044257331757125, 0.9143539479993675, 0.8488638364723747, 0.8799930903200349, 0.7554293431852545, 0.816676830374281, 0.850494650225175, 0.822037053545061, 0.9142775732632898, 0.9957599304850876, 0.7883186301083465, 0.7931240554599578, 0.9589115246255544, 0.8656632255215546, 0.7920180128143242, 0.9078303520436644, 0.7380727750050856, 0.5243754373083207]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "89cf3d05-e397-482e-b884-c3e3c7ab1700", "fitness": 0.6664248655551595, "name": "SimplifiedAdaptiveDE", "description": "Simplified Adaptive Differential Evolution with a focus on efficient local exploitation and adaptive parameter control for enhanced convergence.", "code": "import numpy as np\n\nclass SimplifiedAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=40, archive_size=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.population = None\n        self.fitness = None\n        self.F = 0.5\n        self.CR = 0.7\n        self.archive = []\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n\n    def mutate(self, i):\n        idxs = np.random.choice(self.pop_size, 3, replace=False)\n        a, b, c = idxs\n        x_mutated = self.population[a] + self.F * (self.population[b] - self.population[c])\n        return x_mutated\n\n    def crossover(self, x_mutated, i):\n        x_trial = np.copy(self.population[i])\n        j_rand = np.random.randint(self.dim)\n        for j in range(self.dim):\n            if np.random.rand() < self.CR or j == j_rand:\n                x_trial[j] = x_mutated[j]\n        return x_trial\n\n    def update_parameters(self, success):\n      if success:\n        self.F = np.clip(np.random.normal(0.5, 0.3), 0.1, 1.0)\n        self.CR = np.clip(np.random.normal(0.7, 0.1), 0.1, 1.0)\n\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)].copy()\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                x_mutated = self.mutate(i)\n                x_trial = self.crossover(x_mutated, i)\n                x_trial = np.clip(x_trial, func.bounds.lb, func.bounds.ub)\n\n                f_trial = func(x_trial)\n                self.budget -= 1\n\n                if self.budget <= 0:\n                    break\n\n                if f_trial < self.fitness[i]:\n                    self.update_parameters(True)\n                    self.population[i] = x_trial.copy()\n                    self.fitness[i] = f_trial\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = x_trial.copy()\n\n                    if len(self.archive) < self.archive_size:\n                      self.archive.append(x_trial.copy())\n                    else:\n                      self.archive[np.random.randint(self.archive_size)] = x_trial.copy()\n                else:\n                  self.update_parameters(False)\n\n            if self.budget <= 0:\n                break\n            \n            best_index = np.argmin(self.fitness)\n            if self.fitness[best_index] < self.f_opt:\n                self.f_opt = self.fitness[best_index]\n                self.x_opt = self.population[best_index].copy()\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "The algorithm SimplifiedAdaptiveDE scored 0.666 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["3c377e79-26ed-41cf-8828-7863aaef3453"], "operator": null, "metadata": {"aucs": [0.2422621754461245, 0.4929231834224199, 0.6507120404121798, 0.8378229495666547, 0.7218965305353192, 0.8056280275142756, 0.5872666922525793, 0.6531195340062445, 0.7280728252136535, 0.615903193492126, 0.8559373076606647, 0.998674153385442, 0.3405815844820572, 0.6840487661184125, 0.8913820551785727, 0.798008176880425, 0.5605729379491344, 0.8556961526042456, 0.4781223215412864, 0.5298667034413709]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "ef519632-a8bb-42ef-946e-4fa6ef8c31cf", "fitness": 0.611928800398682, "name": "AdaptiveDE", "description": "Self-Adaptive Differential Evolution with Simplified Adaptation and Ensemble Mutation, focusing on robust parameter control and enhanced exploration via multiple mutation strategies.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, initial_f=0.5, initial_cr=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = initial_f\n        self.CR = initial_cr\n        self.population = None\n        self.fitness = None\n        self.sf = []\n        self.scr = []\n        self.p_best = 0.05\n        self.mu_f = initial_f\n        self.mu_cr = initial_cr\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n\n    def mutate(self, i):\n        # Sample F and CR\n        self.F = np.clip(np.random.normal(self.mu_f, 0.1), 0.1, 1.0)\n        self.CR = np.clip(np.random.normal(self.mu_cr, 0.1), 0.0, 1.0)\n\n        idxs = np.random.choice(self.pop_size, 4, replace=False)\n        a, b, c, p_best_idx = idxs[0], idxs[1], idxs[2], idxs[3]\n\n        if np.random.rand() < self.p_best:\n            p_best_idx = np.argmin(self.fitness)\n\n        # Ensemble of mutation strategies\n        if np.random.rand() < 0.5:\n            x_mutated = self.population[i] + self.F * (self.population[p_best_idx] - self.population[i]) + self.F * (self.population[a] - self.population[b])\n        else:\n            x_mutated = self.population[a] + self.F * (self.population[b] - self.population[c])\n\n        return x_mutated, self.F, self.CR\n\n    def crossover(self, x_mutated, i, cr):\n        x_trial = np.copy(self.population[i])\n        j_rand = np.random.randint(self.dim)\n        for j in range(self.dim):\n            if np.random.rand() < cr or j == j_rand:\n                x_trial[j] = x_mutated[j]\n        return x_trial\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)].copy()\n        self.sf = []\n        self.scr = []\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                x_mutated, F, CR = self.mutate(i)\n                x_trial = self.crossover(x_mutated, i, CR)\n                x_trial = np.clip(x_trial, func.bounds.lb, func.bounds.ub)\n                f_trial = func(x_trial)\n                self.budget -= 1\n                if self.budget <= 0:\n                    break\n                if f_trial < self.fitness[i]:\n                    self.sf.append(F)\n                    self.scr.append(CR)\n                    self.population[i] = x_trial.copy()\n                    self.fitness[i] = f_trial\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = x_trial.copy()\n            if self.budget <= 0:\n                break\n\n            if self.sf:\n                self.mu_f = np.mean(self.sf)\n            if self.scr:\n                self.mu_cr = np.mean(self.scr)\n\n            self.sf = []\n            self.scr = []\n\n            best_index = np.argmin(self.fitness)\n            if self.fitness[best_index] < self.f_opt:\n                self.f_opt = self.fitness[best_index]\n                self.x_opt = self.population[best_index].copy()\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "The algorithm AdaptiveDE scored 0.612 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["555dcdca-320a-4ce9-b585-17f051aba286"], "operator": null, "metadata": {"aucs": [0.4165553965581207, 0.7433936368636004, 0.3830226319497848, 0.8904504974499022, 0.5437783791548474, 0.8293932985748884, 0.7031580700021676, 0.5287406385100155, 0.8100969782613603, 0.58928178132718, 0.37355419903706777, 0.9979815915511523, 0.391920097417755, 0.6610174190832516, 0.8229585233291222, 0.33090991686666615, 0.5373956003460565, 0.8658653877445801, 0.3168299287113254, 0.5022720352347942]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "ae751484-65a2-470e-a39d-8528f71719f8", "fitness": "-inf", "name": "AdaptiveDE", "description": "Adaptive Differential Evolution with dynamic population size adjustment based on stagnation detection and a niching strategy to maintain diversity.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, initial_f=0.5, initial_cr=0.9, archive_size=10, stagnation_threshold=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.initial_pop_size = pop_size\n        self.F = initial_f\n        self.CR = initial_cr\n        self.population = None\n        self.fitness = None\n        self.archive = []\n        self.archive_size = archive_size\n        self.sf = []\n        self.scr = []\n        self.memory_f = [initial_f] * archive_size\n        self.memory_cr = [initial_cr] * archive_size\n        self.p_best = 0.05\n        self.stagnation_threshold = stagnation_threshold\n        self.stagnation_counter = 0\n        self.previous_best_fitness = float('inf')\n        self.min_pop_size = 10\n        self.max_pop_size = 100\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n\n    def mutate(self, i):\n        phi_idx = np.random.randint(0, self.archive_size)\n        self.F = self.memory_f[phi_idx]\n        self.CR = self.memory_cr[phi_idx]\n        self.F = np.clip(np.random.normal(self.F, 0.1), 0.1, 1.0)\n        self.CR = np.clip(np.random.normal(self.CR, 0.1), 0.0, 1.0)\n\n        idxs = np.random.choice(self.pop_size, 4, replace=False)\n        a, b, c, p_best_idx = idxs[0], idxs[1], idxs[2], idxs[3]\n\n        if np.random.rand() < self.p_best:\n            p_best_idx = np.argmin(self.fitness)\n\n        x_mutated = self.population[i] + self.F * (self.population[p_best_idx] - self.population[i]) + self.F * (self.population[a] - self.population[b])\n\n        return x_mutated, self.F, self.CR\n\n    def crossover(self, x_mutated, i, cr):\n        x_trial = np.copy(self.population[i])\n        j_rand = np.random.randint(self.dim)\n        for j in range(self.dim):\n            if np.random.rand() < cr or j == j_rand:\n                x_trial[j] = x_mutated[j]\n        return x_trial\n\n    def update_memory(self):\n        if self.sf:\n            try:\n                self.memory_f.append(sum(val**2 for val in self.sf) / sum(self.sf))\n            except ZeroDivisionError:\n                self.memory_f.append(np.mean(self.memory_f))\n\n            self.memory_f.pop(0)\n            self.memory_cr.append(np.mean(self.scr))\n            self.memory_cr.pop(0)\n        self.sf = []\n        self.scr = []\n\n    def adjust_population_size(self):\n        current_best_fitness = np.min(self.fitness)\n\n        if current_best_fitness >= self.previous_best_fitness:\n            self.stagnation_counter += 1\n        else:\n            self.stagnation_counter = 0\n\n        self.previous_best_fitness = current_best_fitness\n\n        if self.stagnation_counter >= self.stagnation_threshold:\n            # Reduce population size to intensify search\n            self.pop_size = max(self.min_pop_size, int(self.pop_size * 0.8))\n            self.stagnation_counter = 0 # reset counter\n\n        elif self.pop_size < self.initial_pop_size and self.stagnation_counter == 0:\n             self.pop_size = min(self.max_pop_size, int(self.pop_size * 1.1))\n\n        if self.pop_size != len(self.population):\n            #Resample the population to fit the new size\n            new_population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n            new_fitness = np.array([func(x) for x in new_population])\n            \n            #Try to keep some good solutions\n            num_to_keep = min(self.pop_size, len(self.population))\n            best_indices = np.argsort(self.fitness)[:num_to_keep]\n            \n            new_population[:num_to_keep] = self.population[best_indices]\n            new_fitness[:num_to_keep] = self.fitness[best_indices]\n            \n            self.population = new_population\n            self.fitness = new_fitness\n            self.budget -= (self.pop_size - num_to_keep) if (self.pop_size - num_to_keep) > 0 else 0\n\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)].copy()\n        self.sf = []\n        self.scr = []\n        self.previous_best_fitness = self.f_opt\n\n        while self.budget > 0:\n            self.adjust_population_size()\n\n            for i in range(self.pop_size):\n                x_mutated, F, CR = self.mutate(i)\n                x_trial = self.crossover(x_mutated, i, CR)\n                x_trial = np.clip(x_trial, func.bounds.lb, func.bounds.ub)\n                f_trial = func(x_trial)\n                self.budget -= 1\n                if self.budget <= 0:\n                    break\n                if f_trial < self.fitness[i]:\n                    self.sf.append(F)\n                    self.scr.append(CR)\n                    delta = abs(self.fitness[i] - f_trial)\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(self.population[i].copy())\n                    else:\n                        j = np.random.randint(0, self.archive_size)\n                        self.archive[j] = self.population[i].copy()\n                    self.population[i] = x_trial.copy()\n                    self.fitness[i] = f_trial\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = x_trial.copy()\n            if self.budget <= 0:\n                break\n            self.update_memory()\n            best_index = np.argmin(self.fitness)\n            if self.fitness[best_index] < self.f_opt:\n                self.f_opt = self.fitness[best_index]\n                self.x_opt = self.population[best_index].copy()\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "In the code, line 89, in adjust_population_size, the following error occurred:\nNameError: name 'func' is not defined\nOn line: new_population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))", "error": "In the code, line 89, in adjust_population_size, the following error occurred:\nNameError: name 'func' is not defined\nOn line: new_population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))", "parent_ids": ["555dcdca-320a-4ce9-b585-17f051aba286"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "eab9e969-c83c-4e7b-a7f0-2366e7d348ae", "fitness": 0.32673375304191643, "name": "RestartSelfAdaptiveDE", "description": "Population-based algorithm using a simplified variant of Differential Evolution with self-adaptation and a restart mechanism based on population convergence to enhance exploration and exploitation.", "code": "import numpy as np\n\nclass RestartSelfAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=30, archive_size=5, f_min=0.1, f_max=0.9, cr_min=0.1, cr_max=0.9, restart_trigger=0.01, restart_pop_multiplier=2.0):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.f_min = f_min\n        self.f_max = f_max\n        self.cr_min = cr_min\n        self.cr_max = cr_max\n        self.restart_trigger = restart_trigger # Threshold for restarting population based on convergence\n        self.restart_pop_multiplier = restart_pop_multiplier # Multiplier for population size after restart\n        self.population = None\n        self.fitness = None\n        self.archive = None\n        self.archive_fitness = None\n        self.F = None\n        self.CR = None\n        self.lb = -5.0\n        self.ub = 5.0\n        self.best_fitness_history = []\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        self.archive = self.population[:self.archive_size].copy()\n        self.archive_fitness = self.fitness[:self.archive_size].copy()\n        self.F = np.random.uniform(self.f_min, self.f_max, size=self.pop_size)\n        self.CR = np.random.uniform(self.cr_min, self.cr_max, size=self.pop_size)\n\n    def mutate(self, i):\n        indices = np.random.choice(self.pop_size + self.archive_size, 3, replace=False)\n        r1 = self.population[indices[0] % self.pop_size]\n        r2 = self.population[indices[1] % self.pop_size]\n\n        if indices[2] < self.pop_size:\n            r3 = self.population[indices[2]]\n        else:\n            r3 = self.archive[indices[2] - self.pop_size]\n\n        x_mutated = self.population[i] + self.F[i] * (r2 - r3)\n        x_mutated = np.clip(x_mutated, self.lb, self.ub)\n        return x_mutated\n\n    def crossover(self, x_mutated, i):\n        x_trial = self.population[i].copy()\n        j_rand = np.random.randint(self.dim)\n        for j in range(self.dim):\n            if np.random.rand() < self.CR[i] or j == j_rand:\n                x_trial[j] = x_mutated[j]\n        return x_trial\n\n    def update_archive(self):\n        combined_population = np.concatenate((self.population, self.archive), axis=0)\n        combined_fitness = np.concatenate((self.fitness, self.archive_fitness))\n        sorted_indices = np.argsort(combined_fitness)[:self.archive_size]\n        self.archive = combined_population[sorted_indices].copy()\n        self.archive_fitness = combined_fitness[sorted_indices].copy()\n\n    def should_restart(self):\n        if len(self.best_fitness_history) < 10:\n            return False\n\n        recent_fitness = self.best_fitness_history[-10:]\n        fitness_range = np.max(recent_fitness) - np.min(recent_fitness)\n        mean_fitness = np.mean(recent_fitness)\n        \n        if mean_fitness == 0:\n            return False\n\n        relative_range = fitness_range / abs(mean_fitness)\n        return relative_range < self.restart_trigger\n\n    def restart_population(self, func):\n        new_pop_size = int(self.pop_size * self.restart_pop_multiplier)\n        self.pop_size = new_pop_size\n\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n\n        self.F = np.random.uniform(self.f_min, self.f_max, size=self.pop_size)\n        self.CR = np.random.uniform(self.cr_min, self.cr_max, size=self.pop_size)\n\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_index]\n        self.x_opt = self.population[best_index].copy()\n        self.best_fitness_history.append(self.f_opt)\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                x_mutated = self.mutate(i)\n                x_trial = self.crossover(x_mutated, i)\n                f_trial = func(x_trial)\n                self.budget -= 1\n\n                if f_trial < self.fitness[i]:\n                    self.population[i] = x_trial.copy()\n                    self.fitness[i] = f_trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = x_trial.copy()\n                \n                #Adapt F and CR\n                self.F[i] = np.clip(np.random.normal(self.F[i], 0.1), self.f_min, self.f_max)\n                self.CR[i] = np.clip(np.random.normal(self.CR[i], 0.1), self.cr_min, self.cr_max)\n\n                if self.budget <= 0:\n                    break\n            \n            self.update_archive()\n            self.best_fitness_history.append(self.f_opt)\n\n            if self.should_restart() and self.budget > self.pop_size:\n                self.restart_population(func)\n                best_index = np.argmin(self.fitness)\n                self.f_opt = self.fitness[best_index]\n                self.x_opt = self.population[best_index].copy()\n                self.best_fitness_history.append(self.f_opt)\n\n            if self.budget <= 0:\n                break\n\n        best_index = np.argmin(self.fitness)\n        if self.fitness[best_index] < self.f_opt:\n            self.f_opt = self.fitness[best_index]\n            self.x_opt = self.population[best_index].copy()\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "The algorithm RestartSelfAdaptiveDE scored 0.327 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["0ff31b83-4571-40b3-9b3c-2d5d680cc085"], "operator": null, "metadata": {"aucs": [0.12090754152910976, 0.2304924025353261, 0.32638288825993156, 0.2550018137309795, 0.2531390438600343, 0.2708285035118655, 0.2703129931483004, 0.2611986433971739, 0.22776586684647093, 0.1775628680648027, 0.3021319073230394, 0.9942677016451028, 0.2661296696481229, 0.24440423281581058, 0.6981710203928153, 0.38144400992916805, 0.25329519132920963, 0.36238297890329607, 0.1670031177550796, 0.47185266621268984]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "84f27f66-0100-40ec-b10d-9d3f546e1200", "fitness": "-inf", "name": "DualPopulationAdaptiveDE", "description": "A population-based algorithm employing a dual-population strategy with a dynamic adaptation mechanism based on fitness improvement, enhancing exploration and exploitation through mirrored sampling and differential evolution.", "code": "import numpy as np\n\nclass DualPopulationAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size_max=40, pop_size_min=10, archive_size=5, f_min=0.1, f_max=0.9, cr_min=0.1, cr_max=0.9, reduction_factor=0.95, p_adapt=0.1, dual_ratio=0.5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size_max = pop_size_max\n        self.pop_size_min = pop_size_min\n        self.pop_size = pop_size_max\n        self.archive_size = archive_size\n        self.f_min = f_min\n        self.f_max = f_max\n        self.cr_min = cr_min\n        self.cr_max = cr_max\n        self.reduction_factor = reduction_factor\n        self.p_adapt = p_adapt\n        self.dual_ratio = dual_ratio  # Ratio of dual population size\n        self.population = None\n        self.fitness = None\n        self.dual_population = None\n        self.dual_fitness = None\n        self.archive = None\n        self.archive_fitness = None\n        self.F = None\n        self.CR = None\n        self.lb = -5.0\n        self.ub = 5.0\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.dual_pop_size = int(self.pop_size * self.dual_ratio)\n        self.dual_population = np.random.uniform(self.lb, self.ub, size=(self.dual_pop_size, self.dim))\n        self.dual_fitness = np.array([func(x) for x in self.dual_population])\n        self.budget -= (self.pop_size + self.dual_pop_size)\n        self.archive = self.population[:self.archive_size].copy()\n        self.archive_fitness = self.fitness[:self.archive_size].copy()\n        self.F = np.random.uniform(self.f_min, self.f_max, size=self.pop_size)\n        self.CR = np.random.uniform(self.cr_min, self.cr_max, size=self.pop_size)\n\n    def mutate(self, i, pop, pop_size, archive):\n        indices = np.random.choice(pop_size + self.archive_size, 3, replace=False)\n        \n        r1 = pop[indices[0] % pop_size]\n        r2 = pop[indices[1] % pop_size]\n        \n        if indices[2] < pop_size:\n            r3 = pop[indices[2]]\n        else:\n            r3 = archive[indices[2] - pop_size]\n\n        x_mutated = pop[i] + self.F[i % self.pop_size] * (r2 - r3)  # Use modulo for F/CR indexing\n        x_mutated = np.clip(x_mutated, self.lb, self.ub)\n\n        for j in range(self.dim):\n            if x_mutated[j] < self.lb:\n                x_mutated[j] = self.lb + (self.lb - x_mutated[j])\n            elif x_mutated[j] > self.ub:\n                x_mutated[j] = self.ub - (x_mutated[j] - self.ub)\n        return x_mutated\n\n    def crossover(self, x_mutated, i):\n        x_trial = self.population[i].copy()\n        j_rand = np.random.randint(self.dim)\n        for j in range(self.dim):\n            if np.random.rand() < self.CR[i % self.pop_size] or j == j_rand:  # Use modulo for F/CR indexing\n                x_trial[j] = x_mutated[j]\n        return x_trial\n\n    def update_archive(self):\n        combined_population = np.concatenate((self.population, self.dual_population, self.archive), axis=0)\n        combined_fitness = np.concatenate((self.fitness, self.dual_fitness, self.archive_fitness))\n        sorted_indices = np.argsort(combined_fitness)[:self.archive_size]\n        self.archive = combined_population[sorted_indices].copy()\n        self.archive_fitness = combined_fitness[sorted_indices].copy()\n\n    def reduce_population(self):\n        if self.pop_size > self.pop_size_min:\n            num_to_remove = max(1, int((self.pop_size - self.pop_size_min) * (1 - self.reduction_factor)))\n            worst_indices = np.argsort(self.fitness)[-num_to_remove:]\n            mask = np.ones(self.pop_size, dtype=bool)\n            mask[worst_indices] = False\n            self.population = self.population[mask]\n            self.fitness = self.fitness[mask]\n            self.F = self.F[:len(self.population)] #Adapt F to pop size reduction\n            self.CR = self.CR[:len(self.population)] #Adapt CR to pop size reduction\n            self.pop_size = len(self.population)\n            self.dual_pop_size = int(self.pop_size * self.dual_ratio) # Adapt dual pop size\n\n    def adapt_dual_population(self):\n        # Select top individuals from the main population\n        num_to_transfer = min(len(self.population) // 4, len(self.dual_population) // 2)\n        best_indices = np.argsort(self.fitness)[:num_to_transfer]\n        worst_indices_dual = np.argsort(self.dual_fitness)[-num_to_transfer:]\n\n        # Replace worst individuals in dual population with best from main\n        self.dual_population[worst_indices_dual] = self.population[best_indices].copy()\n        self.dual_fitness[worst_indices_dual] = self.fitness[best_indices].copy()\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_index]\n        self.x_opt = self.population[best_index].copy()\n\n        while self.budget > 0:\n            # Evolve main population\n            for i in range(self.pop_size):\n                x_mutated = self.mutate(i, self.population, self.pop_size, self.archive)\n                x_trial = self.crossover(x_mutated, i)\n                f_trial = func(x_trial)\n                self.budget -= 1\n\n                if f_trial < self.fitness[i]:\n                    self.population[i] = x_trial.copy()\n                    self.fitness[i] = f_trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = x_trial.copy()\n\n                if np.random.rand() < self.p_adapt:\n                    self.F[i % self.pop_size] = np.random.uniform(self.f_min, self.f_max)\n                    self.CR[i % self.pop_size] = np.random.uniform(self.cr_min, self.cr_max)\n\n                if self.budget <= 0:\n                    break\n\n            # Evolve dual population\n            for i in range(self.dual_pop_size):\n                 x_mutated = self.mutate(i, self.dual_population, self.dual_pop_size, self.archive)\n                 x_trial = self.population[np.random.randint(self.pop_size)].copy() #Trial vector from main population\n                 j_rand = np.random.randint(self.dim)\n\n                 for j in range(self.dim):\n                     if np.random.rand() < self.CR[i % self.pop_size] or j == j_rand:\n                         x_trial[j] = x_mutated[j]\n                 \n                 f_trial = func(x_trial)\n                 self.budget -= 1\n\n                 if f_trial < self.dual_fitness[i]:\n                     self.dual_population[i] = x_trial.copy()\n                     self.dual_fitness[i] = f_trial\n\n                 if self.budget <= 0:\n                    break\n\n            self.update_archive()\n            self.reduce_population()\n            self.adapt_dual_population()  # Adapt dual population with best from main\n\n            if self.budget <= 0:\n                break\n\n        best_index = np.argmin(self.fitness)\n        if self.fitness[best_index] < self.f_opt:\n            self.f_opt = self.fitness[best_index]\n            self.x_opt = self.population[best_index].copy()\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "Evaluation timed out after 300 seconds.", "error": "Evaluation timed out after 300 seconds.", "parent_ids": ["0ff31b83-4571-40b3-9b3c-2d5d680cc085"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "6fa65df3-5a68-4850-84ac-083311ab3808", "fitness": "-inf", "name": "OrthogonalAdaptiveDE", "description": "Enhanced Adaptive Differential Evolution with orthogonal learning, employing a historical memory of successful parameters and a diversity-enhancing orthogonal experimental design to explore the search space more effectively.", "code": "import numpy as np\n\nclass OrthogonalAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, initial_f=0.5, initial_cr=0.9, archive_size=10, p_best=0.05, gamma_f=0.1, gamma_cr=0.1, orthogonal_sample_size=5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = initial_f\n        self.CR = initial_cr\n        self.population = None\n        self.fitness = None\n        self.archive = []\n        self.archive_size = archive_size\n        self.sf = []\n        self.scr = []\n        self.memory_f = [initial_f] * archive_size\n        self.memory_cr = [initial_cr] * archive_size\n        self.p_best = p_best\n        self.gamma_f = gamma_f\n        self.gamma_cr = gamma_cr\n        self.success_solutions = []\n        self.orthogonal_sample_size = orthogonal_sample_size\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n\n    def mutate(self, i):\n        phi_idx = np.random.randint(0, self.archive_size)\n        self.F = self.memory_f[phi_idx]\n        self.CR = self.memory_cr[phi_idx]\n\n        self.F = np.clip(np.random.normal(self.F, self.gamma_f), 0.1, 1.0)\n        self.CR = np.clip(np.random.normal(self.CR, self.gamma_cr), 0.0, 1.0)\n\n        idxs = np.random.choice(self.pop_size, 4, replace=False)\n        a, b, c, p_best_idx = idxs[0], idxs[1], idxs[2], idxs[3]\n\n        if np.random.rand() < self.p_best:\n            p_best_idx = np.argmin(self.fitness)\n        \n        x_mutated = self.population[i] + self.F * (self.population[p_best_idx] - self.population[i]) + self.F * (self.population[a] - self.population[b])\n\n        return x_mutated, self.F, self.CR\n\n    def crossover(self, x_mutated, i, cr):\n        x_trial = np.copy(self.population[i])\n        j_rand = np.random.randint(self.dim)\n        for j in range(self.dim):\n            if np.random.rand() < cr or j == j_rand:\n                x_trial[j] = x_mutated[j]\n        return x_trial\n\n    def repair(self, x_trial, func):\n        # Repair mechanism using the mean of successful solutions\n        if self.success_solutions:\n            mean_success = np.mean(self.success_solutions, axis=0)\n            for j in range(self.dim):\n                if not func.bounds.lb[j] <= x_trial[j] <= func.bounds.ub[j]:\n                    x_trial[j] = 0.5 * (x_trial[j] + mean_success[j])\n\n        x_trial = np.clip(x_trial, func.bounds.lb, func.bounds.ub)\n        return x_trial\n    \n    def orthogonal_design(self, func, current_x):\n        # Generate orthogonal samples around the current solution\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        levels = 3  # Number of levels for each factor\n        \n        # Create an orthogonal array (OA) using the Plackett-Burman design\n        # This design allows for a number of factors that is one less than a power of 2.\n        n_factors = self.dim\n        n_runs = (2**int(np.ceil(np.log2(n_factors + 1)))) # Minimum number of runs which is a power of 2\n\n        if n_runs > self.budget // 2:\n            n_runs = min(self.budget // 2, max(8, 2 * n_factors))  # Reduce n_runs to save budget\n        \n        if n_runs < n_factors + 1:\n            return [current_x]  # If orthogonal design impossible, return current solution only\n        \n        # Generate samples using a simple design matrix where each column has equal distribution of values\n        oa = np.zeros((n_runs, n_factors))\n        for col in range(n_factors):\n            oa[:, col] = np.tile(np.repeat(np.linspace(-1, 1, levels), n_runs // (levels)), n_factors)[:n_runs, ] [0:n_runs]\n\n        samples = np.zeros((n_runs, self.dim))\n        for i in range(n_runs):\n            sample = np.copy(current_x)\n            for j in range(self.dim):\n                delta = 0.1 * (ub[j] - lb[j]) * oa[i,j] # Scale OA values for exploration\n                sample[j] = current_x[j] + delta\n                \n            samples[i] = self.repair(sample, func)\n                \n        return samples\n    \n\n    def update_memory(self):\n        if self.sf:\n            self.memory_f.append(sum(val**2 for val in self.sf) / sum(self.sf))\n            self.memory_f.pop(0)\n            self.memory_cr.append(np.mean(self.scr))\n            self.memory_cr.pop(0)\n        self.sf = []\n        self.scr = []\n\n    def jump_out_of_local_optima(self, func):\n        # Jump out of local optima if population diversity is low\n        diversity = np.std(self.fitness)\n        if diversity < 1e-6:  # Adjust threshold as needed\n            # Option 1: Randomize a portion of the population\n            num_to_randomize = int(0.1 * self.pop_size)\n            indices_to_randomize = np.random.choice(self.pop_size, num_to_randomize, replace=False)\n            self.population[indices_to_randomize] = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(num_to_randomize, self.dim))\n            self.fitness[indices_to_randomize] = [func(x) for x in self.population[indices_to_randomize]]\n            self.budget -= num_to_randomize\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)].copy()\n        self.sf = []\n        self.scr = []\n        self.success_solutions = [] # Reset successful solutions\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                x_mutated, F, CR = self.mutate(i)\n                x_trial = self.crossover(x_mutated, i, CR)\n                x_trial = self.repair(x_trial, func)\n                \n                f_trial = func(x_trial)\n                self.budget -= 1\n                if self.budget <= 0:\n                    break\n                if f_trial < self.fitness[i]:\n                    self.sf.append(F)\n                    self.scr.append(CR)\n                    self.success_solutions.append(x_trial.copy())  # Store successful solution\n                    \n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(self.population[i].copy())\n                    else:\n                        j = np.random.randint(0, self.archive_size)\n                        self.archive[j] = self.population[i].copy()\n                    self.population[i] = x_trial.copy()\n                    self.fitness[i] = f_trial\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = x_trial.copy()\n            if self.budget <= 0:\n                break\n            self.update_memory()\n\n            # Orthogonal learning around the best solution\n            best_index = np.argmin(self.fitness)\n            orthogonal_samples = self.orthogonal_design(func, self.population[best_index])\n            \n            orthogonal_fitness = []\n            for sample in orthogonal_samples:\n                if self.budget > 0:\n                    f_sample = func(sample)\n                    self.budget -= 1\n                    orthogonal_fitness.append(f_sample)\n                else:\n                    break\n\n            if orthogonal_fitness:  # Only update if samples were evaluated\n                best_orthogonal_index = np.argmin(orthogonal_fitness)\n                if orthogonal_fitness[best_orthogonal_index] < self.f_opt:\n                    self.f_opt = orthogonal_fitness[best_orthogonal_index]\n                    self.x_opt = orthogonal_samples[best_orthogonal_index].copy()\n\n            self.jump_out_of_local_optima(func)\n\n\n            if self.fitness[best_index] < self.f_opt:\n                self.f_opt = self.fitness[best_index]\n                self.x_opt = self.population[best_index].copy()\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "Evaluation timed out after 300 seconds.", "error": "Evaluation timed out after 300 seconds.", "parent_ids": ["73551b59-70ed-4fbd-a6e6-cc84bc34a85c"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "6adec789-0cbf-469a-8386-caf57dc8d0df", "fitness": 0.5132894666421994, "name": "EnhancedMirroredSelfAdaptiveDE", "description": "Self-Adaptive Differential Evolution with orthogonal design for parameter adaptation and a Cauchy mutation operator to enhance exploration.", "code": "import numpy as np\n\nclass EnhancedMirroredSelfAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size_max=40, pop_size_min=10, archive_size=5, f_min=0.1, f_max=0.9, cr_min=0.1, cr_max=0.9, reduction_factor=0.95, p_adapt=0.1, ortho_levels=3):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size_max = pop_size_max\n        self.pop_size_min = pop_size_min\n        self.pop_size = pop_size_max\n        self.archive_size = archive_size\n        self.f_min = f_min\n        self.f_max = f_max\n        self.cr_min = cr_min\n        self.cr_max = cr_max\n        self.reduction_factor = reduction_factor\n        self.p_adapt = p_adapt\n        self.ortho_levels = ortho_levels\n        self.population = None\n        self.fitness = None\n        self.archive = None\n        self.archive_fitness = None\n        self.F = None\n        self.CR = None\n        self.lb = -5.0\n        self.ub = 5.0\n        self.ortho_matrix = self.generate_orthogonal_array()\n\n    def generate_orthogonal_array(self):\n          # Generate an orthogonal array using Plackett-Burman design (if possible)\n          # or a Hadamard matrix. If not possible, revert to random sampling.\n          try:\n              import oapackage\n              oa = oapackage.orthogonal_array(self.ortho_levels, self.dim) #N, k, s\n              ortho_matrix = np.array(oa).astype(float)\n              ortho_matrix = (ortho_matrix - (self.ortho_levels - 1)/2) / ((self.ortho_levels-1)/2) # Normalize to [-1, 1]\n\n          except ImportError:\n              ortho_matrix = np.random.uniform(-1, 1, size=(self.ortho_levels**2, self.dim))\n              \n          return ortho_matrix\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        self.archive = self.population[:self.archive_size].copy()\n        self.archive_fitness = self.fitness[:self.archive_size].copy()\n        self.F = np.random.uniform(self.f_min, self.f_max, size=self.pop_size)\n        self.CR = np.random.uniform(self.cr_min, self.cr_max, size=self.pop_size)\n\n    def mutate(self, i):\n        indices = np.random.choice(self.pop_size + self.archive_size, 3, replace=False)\n        r1 = self.population[indices[0] % self.pop_size]\n        r2 = self.population[indices[1] % self.pop_size]\n        \n        if indices[2] < self.pop_size:\n            r3 = self.population[indices[2]]\n        else:\n            r3 = self.archive[indices[2] - self.pop_size]\n\n        # Cauchy mutation\n        cauchy_rand = np.random.standard_cauchy(size=self.dim) * 0.01  # Scale Cauchy distribution\n        x_mutated = self.population[i] + self.F[i] * (r2 - r3) + cauchy_rand\n        x_mutated = np.clip(x_mutated, self.lb, self.ub)\n\n        # Mirrored sampling\n        for j in range(self.dim):\n            if x_mutated[j] < self.lb:\n                x_mutated[j] = self.lb + (self.lb - x_mutated[j])\n            elif x_mutated[j] > self.ub:\n                x_mutated[j] = self.ub - (x_mutated[j] - self.ub)\n        return x_mutated\n\n    def crossover(self, x_mutated, i):\n        x_trial = self.population[i].copy()\n        j_rand = np.random.randint(self.dim)\n        for j in range(self.dim):\n            if np.random.rand() < self.CR[i] or j == j_rand:\n                x_trial[j] = x_mutated[j]\n        return x_trial\n\n    def update_archive(self):\n        combined_population = np.concatenate((self.population, self.archive), axis=0)\n        combined_fitness = np.concatenate((self.fitness, self.archive_fitness))\n        sorted_indices = np.argsort(combined_fitness)[:self.archive_size]\n        self.archive = combined_population[sorted_indices].copy()\n        self.archive_fitness = combined_fitness[sorted_indices].copy()\n\n    def reduce_population(self):\n        if self.pop_size > self.pop_size_min:\n            num_to_remove = max(1, int((self.pop_size - self.pop_size_min) * (1 - self.reduction_factor)))\n            worst_indices = np.argsort(self.fitness)[-num_to_remove:]\n            mask = np.ones(self.pop_size, dtype=bool)\n            mask[worst_indices] = False\n            self.population = self.population[mask]\n            self.fitness = self.fitness[mask]\n            self.F = self.F[mask]\n            self.CR = self.CR[mask]\n            self.pop_size = len(self.population)\n\n    def adapt_parameters(self):\n        # Orthogonal design based parameter adaptation\n        num_designs = min(self.pop_size, self.ortho_matrix.shape[0])  # Use as many designs as possible\n        for i in range(num_designs):\n            self.F[i] = np.clip(self.F[i] * (1 + 0.1 * self.ortho_matrix[i, 0]), self.f_min, self.f_max)\n            self.CR[i] = np.clip(self.CR[i] + 0.1 * self.ortho_matrix[i, 1], self.cr_min, self.cr_max)\n\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_index]\n        self.x_opt = self.population[best_index].copy()\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                x_mutated = self.mutate(i)\n                x_trial = self.crossover(x_mutated, i)\n                f_trial = func(x_trial)\n                self.budget -= 1\n\n                if f_trial < self.fitness[i]:\n                    self.population[i] = x_trial.copy()\n                    self.fitness[i] = f_trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = x_trial.copy()\n\n                if self.budget <= 0:\n                    break\n\n            self.update_archive()\n            self.reduce_population()\n            self.adapt_parameters() # Adapt F and CR using orthogonal design\n\n            if self.budget <= 0:\n                break\n\n        best_index = np.argmin(self.fitness)\n        if self.fitness[best_index] < self.f_opt:\n            self.f_opt = self.fitness[best_index]\n            self.x_opt = self.population[best_index].copy()\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "The algorithm EnhancedMirroredSelfAdaptiveDE scored 0.513 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["0ff31b83-4571-40b3-9b3c-2d5d680cc085"], "operator": null, "metadata": {"aucs": [0.19633462818032377, 0.29605457624151965, 0.48826398998513054, 0.8469266937988718, 0.33716650899538114, 0.6259857482610389, 0.32232282529137946, 0.4106685301659976, 0.46340936298556956, 0.35184344730714256, 0.8584861207292807, 0.9756759014300778, 0.30685137234149584, 0.40855611263476843, 0.90160539151969, 0.6154129440679534, 0.4056528183660897, 0.7246205683186886, 0.23859249080004097, 0.491359301423548]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "55f43c04-02ab-4ffd-a761-e1d0d06ba932", "fitness": 0.6387303105524476, "name": "AdaptiveDE", "description": "Adaptive Differential Evolution with covariance matrix adaptation and orthogonal learning for improved exploration and exploitation.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, initial_f=0.5, initial_cr=0.9, archive_size=10, CMA_lrate=0.2):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = initial_f  # Initial differential weight\n        self.CR = initial_cr # Crossover rate\n        self.population = None\n        self.fitness = None\n        self.archive = []\n        self.archive_size = archive_size\n        self.sf = []\n        self.scr = []\n        self.memory_f = [initial_f] * archive_size\n        self.memory_cr = [initial_cr] * archive_size\n        self.p_best = 0.05\n        self.CMA_lrate = CMA_lrate\n        self.C = np.eye(dim)  # Covariance matrix for CMA\n        self.mean = None\n\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        self.mean = np.mean(self.population, axis=0)\n\n    def mutate(self, i):\n        phi_idx = np.random.randint(0, self.archive_size)\n        self.F = self.memory_f[phi_idx]\n        self.CR = self.memory_cr[phi_idx]\n        self.F = np.clip(np.random.normal(self.F, 0.1), 0.1, 1.0)\n        self.CR = np.clip(np.random.normal(self.CR, 0.1), 0.0, 1.0)\n\n        idxs = np.random.choice(self.pop_size, 4, replace=False)\n        a, b, c, p_best_idx = idxs[0], idxs[1], idxs[2], idxs[3]\n\n        if np.random.rand() < self.p_best:\n            p_best_idx = np.argmin(self.fitness)\n\n        x_mutated = self.population[i] + self.F * (self.population[p_best_idx] - self.population[i]) + self.F * (self.population[a] - self.population[b])\n\n        return x_mutated, self.F, self.CR\n\n    def crossover(self, x_mutated, i, cr):\n        x_trial = np.copy(self.population[i])\n        j_rand = np.random.randint(self.dim)\n        for j in range(self.dim):\n            if np.random.rand() < cr or j == j_rand:\n                x_trial[j] = x_mutated[j]\n        return x_trial\n\n    def orthogonal_learning(self):\n        # Select a random subset of the population\n        subset_size = min(10, self.pop_size)\n        subset_indices = np.random.choice(self.pop_size, subset_size, replace=False)\n        subset = self.population[subset_indices]\n\n        # Calculate the mean of the subset\n        subset_mean = np.mean(subset, axis=0)\n\n        # Generate an orthogonal vector\n        orthogonal_vector = np.random.randn(self.dim)\n        orthogonal_vector -= np.dot(orthogonal_vector, subset_mean) * subset_mean / np.linalg.norm(subset_mean)**2\n        orthogonal_vector /= np.linalg.norm(orthogonal_vector)\n\n        # Create a new solution by moving along the orthogonal vector\n        step_size = np.random.uniform(-0.1, 0.1)  # Small step size\n        new_solution = subset_mean + step_size * orthogonal_vector\n        return new_solution\n\n\n    def update_memory(self):\n        if self.sf:\n            self.memory_f.append(sum(val**2 for val in self.sf) / sum(self.sf))\n            self.memory_f.pop(0)\n            self.memory_cr.append(np.mean(self.scr))\n            self.memory_cr.pop(0)\n        self.sf = []\n        self.scr = []\n        \n    def update_covariance_matrix(self):\n        best_individual = self.population[np.argmin(self.fitness)]\n        diff = best_individual - self.mean\n        self.C = (1 - self.CMA_lrate) * self.C + self.CMA_lrate * np.outer(diff, diff)\n        self.mean = best_individual # simple mean adaptation\n\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)].copy()\n        self.sf = []\n        self.scr = []\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                x_mutated, F, CR = self.mutate(i)\n                x_trial = self.crossover(x_mutated, i, CR)\n\n                # Apply CMA\n                x_trial = np.clip(x_trial + np.random.multivariate_normal(np.zeros(self.dim), self.C), func.bounds.lb, func.bounds.ub)\n                \n                # Orthogonal Learning\n                if np.random.rand() < 0.1: #Probability of orthogonal learning\n                   x_orthogonal = self.orthogonal_learning()\n                   x_orthogonal = np.clip(x_orthogonal, func.bounds.lb, func.bounds.ub)\n                   f_orthogonal = func(x_orthogonal)\n                   self.budget -= 1\n                   if self.budget <=0:\n                       break\n                   if f_orthogonal < self.fitness[i]:\n                       x_trial = x_orthogonal\n                \n\n                x_trial = np.clip(x_trial, func.bounds.lb, func.bounds.ub)\n                f_trial = func(x_trial)\n                self.budget -= 1\n                if self.budget <= 0:\n                    break\n                if f_trial < self.fitness[i]:\n                    self.sf.append(F)\n                    self.scr.append(CR)\n                    delta = abs(self.fitness[i] - f_trial)\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(self.population[i].copy())\n                    else:\n                        j = np.random.randint(0, self.archive_size)\n                        self.archive[j] = self.population[i].copy()\n                    self.population[i] = x_trial.copy()\n                    self.fitness[i] = f_trial\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = x_trial.copy()\n            if self.budget <= 0:\n                break\n            self.update_memory()\n            self.update_covariance_matrix() #Update CMA\n            best_index = np.argmin(self.fitness)\n            if self.fitness[best_index] < self.f_opt:\n                self.f_opt = self.fitness[best_index]\n                self.x_opt = self.population[best_index].copy()\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "The algorithm AdaptiveDE scored 0.639 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["555dcdca-320a-4ce9-b585-17f051aba286"], "operator": null, "metadata": {"aucs": [0.305661361641573, 0.6310291445975329, 0.5970172466205608, 0.8573781590346778, 0.652103096436578, 0.7039405247278578, 0.5005342752720917, 0.5760566831042857, 0.7046308844524161, 0.5507556902766579, 0.8689125226125235, 0.9848214181857126, 0.5444393728190854, 0.6701315998621389, 0.8731541445632609, 0.6962671619447226, 0.5266089630135493, 0.7677084427118936, 0.26879878953892766, 0.49465672963290386]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "0409aa84-7f49-4716-b4be-b44fa7b0b1b2", "fitness": 0.4938887886145259, "name": "CooperativeSwarmAdaptiveExploration", "description": "Cooperative Swarm with Adaptive Exploration (CSAE) uses multiple interacting swarms with adaptive exploration rates based on swarm performance and inter-swarm communication to escape local optima and accelerate convergence.", "code": "import numpy as np\n\nclass CooperativeSwarmAdaptiveExploration:\n    def __init__(self, budget=10000, dim=10, num_swarms=3, swarm_size=20, inertia=0.7, cognitive_coeff=1.4, social_coeff=1.4, initial_exploration_rate=0.3, exploration_decay=0.99, inter_swarm_communication_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.num_swarms = num_swarms\n        self.swarm_size = swarm_size\n        self.inertia = inertia\n        self.cognitive_coeff = cognitive_coeff\n        self.social_coeff = social_coeff\n        self.initial_exploration_rate = initial_exploration_rate\n        self.exploration_decay = exploration_decay\n        self.inter_swarm_communication_rate = inter_swarm_communication_rate\n        self.swarms = []\n        self.velocities = []\n        self.local_best_positions = []\n        self.local_best_fitnesses = []\n        self.global_best_position = None\n        self.global_best_fitness = float('inf')\n        self.exploration_rates = np.full(num_swarms, initial_exploration_rate)\n\n    def initialize_swarms(self, func):\n        for _ in range(self.num_swarms):\n            swarm = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.swarm_size, self.dim))\n            self.swarms.append(swarm)\n            self.velocities.append(np.zeros((self.swarm_size, self.dim)))\n            self.local_best_positions.append(np.copy(swarm))\n            self.local_best_fitnesses.append(np.array([func(x) for x in swarm]))\n            \n            # Update global best during initialization\n            swarm_best_index = np.argmin(self.local_best_fitnesses[-1])\n            if self.local_best_fitnesses[-1][swarm_best_index] < self.global_best_fitness:\n                self.global_best_fitness = self.local_best_fitnesses[-1][swarm_best_index]\n                self.global_best_position = np.copy(self.local_best_positions[-1][swarm_best_index])\n        \n        self.budget -= self.num_swarms * self.swarm_size\n\n    def update_particle_velocity(self, swarm_index, particle_index):\n        inertia_term = self.inertia * self.velocities[swarm_index][particle_index]\n        cognitive_term = self.cognitive_coeff * np.random.rand() * (self.local_best_positions[swarm_index][particle_index] - self.swarms[swarm_index][particle_index])\n        social_term = self.social_coeff * np.random.rand() * (self.global_best_position - self.swarms[swarm_index][particle_index])\n        return inertia_term + cognitive_term + social_term\n\n    def explore(self, x, func):\n        # Add random exploration with adaptive rate\n        exploration_vector = np.random.uniform(-self.exploration_rates[0], self.exploration_rates[0], self.dim)\n        x_explored = x + exploration_vector\n        x_explored = np.clip(x_explored, func.bounds.lb, func.bounds.ub)\n        return x_explored\n\n    def inter_swarm_communication(self, swarm_index):\n        # Select a random particle from another swarm\n        other_swarm_index = np.random.choice([i for i in range(self.num_swarms) if i != swarm_index])\n        random_particle_index = np.random.randint(self.swarm_size)\n        return self.swarms[other_swarm_index][random_particle_index]\n        \n\n    def __call__(self, func):\n        self.initialize_swarms(func)\n\n        while self.budget > 0:\n            for swarm_index in range(self.num_swarms):\n                for particle_index in range(self.swarm_size):\n                    # Update velocity\n                    self.velocities[swarm_index][particle_index] = self.update_particle_velocity(swarm_index, particle_index)\n\n                    # Update position\n                    self.swarms[swarm_index][particle_index] += self.velocities[swarm_index][particle_index]\n                    self.swarms[swarm_index][particle_index] = np.clip(self.swarms[swarm_index][particle_index], func.bounds.lb, func.bounds.ub)\n\n                    # Exploration\n                    if np.random.rand() < self.exploration_rates[swarm_index]:\n                        self.swarms[swarm_index][particle_index] = self.explore(self.swarms[swarm_index][particle_index], func)\n                    \n                    #Inter swarm communication\n                    if np.random.rand() < self.inter_swarm_communication_rate:\n                        other_particle = self.inter_swarm_communication(swarm_index)\n                        self.swarms[swarm_index][particle_index] = 0.5 * self.swarms[swarm_index][particle_index] + 0.5* other_particle\n\n                    # Evaluate fitness\n                    fitness = func(self.swarms[swarm_index][particle_index])\n                    self.budget -= 1\n\n                    if self.budget <= 0:\n                        break\n\n                    # Update local best\n                    if fitness < self.local_best_fitnesses[swarm_index][particle_index]:\n                        self.local_best_fitnesses[swarm_index][particle_index] = fitness\n                        self.local_best_positions[swarm_index][particle_index] = np.copy(self.swarms[swarm_index][particle_index])\n\n                        # Update global best\n                        if fitness < self.global_best_fitness:\n                            self.global_best_fitness = fitness\n                            self.global_best_position = np.copy(self.swarms[swarm_index][particle_index])\n                if self.budget <= 0:\n                    break\n                #Adapt exploration rate based on swarm performance\n                swarm_improvement = np.mean(self.local_best_fitnesses[swarm_index])\n                if swarm_improvement < self.global_best_fitness:\n                    self.exploration_rates[swarm_index] *= self.exploration_decay  #Reduce exploration\n                else:\n                     self.exploration_rates[swarm_index] = min(self.initial_exploration_rate, self.exploration_rates[swarm_index] * (2-self.exploration_decay))\n\n\n        return self.global_best_fitness, self.global_best_position", "configspace": "", "generation": 7, "feedback": "The algorithm CooperativeSwarmAdaptiveExploration scored 0.494 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["3c377e79-26ed-41cf-8828-7863aaef3453"], "operator": null, "metadata": {"aucs": [0.15151716016868833, 0.4386025195591825, 0.4697063327822062, 0.8559224107845098, 0.4523562619336633, 0.5154080256207312, 0.31331210999820036, 0.40350266573796867, 0.3923699238951599, 0.2717410328666584, 0.8264432298842795, 0.9978499509830998, 0.3214112488161258, 0.3671828118388174, 0.8723033256387523, 0.48237253924390167, 0.37556867435474117, 0.6053616862799226, 0.2885012200737167, 0.4763426418301956]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "7fc51561-cf0e-4022-90d0-dd7384f5d31e", "fitness": 0.2436550355672667, "name": "SelfOrganizingDE", "description": "A population-based algorithm employing a novel self-organizing strategy that dynamically adjusts mutation parameters based on individual agent performance and neighborhood interaction, combined with a gradient-based local search to fine-tune solutions.", "code": "import numpy as np\n\nclass SelfOrganizingDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, lr_F=0.1, lr_CR=0.1, ls_prob=0.05, neighbor_ratio=0.2):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lr_F = lr_F\n        self.lr_CR = lr_CR\n        self.ls_prob = ls_prob\n        self.neighbor_ratio = neighbor_ratio\n        self.population = None\n        self.fitness = None\n        self.F = np.full(pop_size, 0.5)  # Individual F values\n        self.CR = np.full(pop_size, 0.9)  # Individual CR values\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n\n    def get_neighbors(self, i):\n        num_neighbors = int(self.pop_size * self.neighbor_ratio)\n        idxs = np.random.choice(self.pop_size, num_neighbors, replace=False)\n        return idxs\n\n    def mutate(self, i):\n        idxs = np.random.choice(self.pop_size, 3, replace=False)\n        a, b, c = idxs\n        x_mutated = self.population[i] + self.F[i] * (self.population[b] - self.population[c])\n        return x_mutated\n\n    def crossover(self, x_mutated, i):\n        x_trial = np.copy(self.population[i])\n        j_rand = np.random.randint(self.dim)\n        for j in range(self.dim):\n            if np.random.rand() < self.CR[i] or j == j_rand:\n                x_trial[j] = x_mutated[j]\n        return x_trial\n\n    def local_search(self, x, func):\n        # Gradient-based local search (simplified)\n        gradient = np.zeros(self.dim)\n        delta = 1e-5\n        for j in range(self.dim):\n            x_plus = x.copy()\n            x_minus = x.copy()\n            x_plus[j] += delta\n            x_minus[j] -= delta\n            x_plus = np.clip(x_plus, func.bounds.lb, func.bounds.ub)\n            x_minus = np.clip(x_minus, func.bounds.lb, func.bounds.ub)\n\n            f_plus = func(x_plus)\n            f_minus = func(x_minus)\n            \n            if self.budget <= 0:\n                return x\n            self.budget -= 2  # Approximate cost of gradient estimation is 2 function evaluations per dimension\n            gradient[j] = (f_plus - f_minus) / (2 * delta)\n\n        # Move against the gradient\n        step_size = 0.01\n        x_ls = x - step_size * gradient\n        x_ls = np.clip(x_ls, func.bounds.lb, func.bounds.ub)\n\n        return x_ls\n\n    def update_parameters(self, i, success):\n        if success:\n            neighbors = self.get_neighbors(i)\n            F_neighbors = self.F[neighbors]\n            CR_neighbors = self.CR[neighbors]\n            self.F[i] = np.clip(self.F[i] + self.lr_F * np.mean(F_neighbors - self.F[i]), 0.1, 0.9)\n            self.CR[i] = np.clip(self.CR[i] + self.lr_CR * np.mean(CR_neighbors - self.CR[i]), 0.1, 0.9)\n        else:\n            self.F[i] = np.clip(self.F[i] - self.lr_F * 0.1, 0.1, 0.9)  # Reduce F if unsuccessful\n            self.CR[i] = np.clip(self.CR[i] - self.lr_CR * 0.1, 0.1, 0.9) # Reduce CR if unsuccessful\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)].copy()\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                x_mutated = self.mutate(i)\n                x_trial = self.crossover(x_mutated, i)\n                x_trial = np.clip(x_trial, func.bounds.lb, func.bounds.ub)\n\n                if np.random.rand() < self.ls_prob:\n                    x_trial = self.local_search(x_trial, func)\n                    if self.budget <= 0:\n                        break\n\n                f_trial = func(x_trial)\n                self.budget -= 1\n                if self.budget <= 0:\n                    break\n\n                if f_trial < self.fitness[i]:\n                    self.update_parameters(i, True)\n                    self.population[i] = x_trial.copy()\n                    self.fitness[i] = f_trial\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = x_trial.copy()\n                else:\n                    self.update_parameters(i, False)\n            \n            best_index = np.argmin(self.fitness)\n            if self.fitness[best_index] < self.f_opt:\n                self.f_opt = self.fitness[best_index]\n                self.x_opt = self.population[best_index].copy()\n\n            if self.budget <= 0:\n                break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "The algorithm SelfOrganizingDE scored 0.244 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["3c377e79-26ed-41cf-8828-7863aaef3453"], "operator": null, "metadata": {"aucs": [0.14119505558114342, 0.26468066301422766, 0.3781823564181128, 0.43421710282284964, 0]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "b1035a21-7f93-411a-8c32-f80b40512d34", "fitness": 0.718267137612475, "name": "AdaptiveDE", "description": "Adaptive Differential Evolution with simplified parameter adaptation and focused exploitation using a larger population and success-history adaptation, further enhanced with a restart mechanism based on stagnation detection and adaptive population sizing.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, initial_f=0.5, initial_cr=0.9, archive_size=10, stagnation_threshold=100):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = initial_f\n        self.CR = initial_cr\n        self.population = None\n        self.fitness = None\n        self.archive = []\n        self.archive_size = archive_size\n        self.sf = []\n        self.scr = []\n        self.memory_f = [initial_f] * archive_size\n        self.memory_cr = [initial_cr] * archive_size\n        self.p_best = 0.05\n        self.stagnation_threshold = stagnation_threshold\n        self.stagnation_counter = 0\n        self.best_fitness_history = []\n        self.adaptive_pop_size = self.pop_size\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.adaptive_pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.adaptive_pop_size\n\n    def mutate(self, i):\n        phi_idx = np.random.randint(0, self.archive_size)\n        self.F = self.memory_f[phi_idx]\n        self.CR = self.memory_cr[phi_idx]\n        self.F = np.clip(np.random.normal(self.F, 0.1), 0.1, 1.0)\n        self.CR = np.clip(np.random.normal(self.CR, 0.1), 0.0, 1.0)\n\n        idxs = np.random.choice(self.adaptive_pop_size, 4, replace=False)\n        a, b, c, p_best_idx = idxs[0], idxs[1], idxs[2], idxs[3]\n\n        if np.random.rand() < self.p_best:\n            p_best_idx = np.argmin(self.fitness)\n\n        x_mutated = self.population[i] + self.F * (self.population[p_best_idx] - self.population[i]) + self.F * (self.population[a] - self.population[b])\n\n        return x_mutated, self.F, self.CR\n\n    def crossover(self, x_mutated, i, cr):\n        x_trial = np.copy(self.population[i])\n        j_rand = np.random.randint(self.dim)\n        for j in range(self.dim):\n            if np.random.rand() < cr or j == j_rand:\n                x_trial[j] = x_mutated[j]\n        return x_trial\n\n    def update_memory(self):\n        if self.sf:\n            self.memory_f.append(sum(val**2 for val in self.sf) / sum(self.sf) if sum(self.sf) != 0 else self.memory_f[-1])\n            self.memory_f.pop(0)\n            self.memory_cr.append(np.mean(self.scr))\n            self.memory_cr.pop(0)\n        self.sf = []\n        self.scr = []\n\n    def check_stagnation(self):\n        if len(self.best_fitness_history) > self.stagnation_threshold:\n            if np.std(self.best_fitness_history[-self.stagnation_threshold:]) < 1e-6:\n                self.stagnation_counter += 1\n                return True\n            else:\n                self.stagnation_counter = 0\n                return False\n        return False\n\n    def restart_population(self, func):\n        self.adaptive_pop_size = int(self.pop_size * 0.75) # Reduce population size upon restart\n        self.adaptive_pop_size = max(10, self.adaptive_pop_size)\n        new_population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.adaptive_pop_size, self.dim))\n        new_fitness = np.array([func(x) for x in new_population])\n        self.budget -= self.adaptive_pop_size\n        self.population = new_population\n        self.fitness = new_fitness\n\n        self.best_fitness_history = []\n        self.memory_f = [0.5] * self.archive_size\n        self.memory_cr = [0.9] * self.archive_size\n        self.sf = []\n        self.scr = []\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)].copy()\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)].copy()\n        self.sf = []\n        self.scr = []\n\n        while self.budget > 0:\n            for i in range(self.adaptive_pop_size):\n                x_mutated, F, CR = self.mutate(i)\n                x_trial = self.crossover(x_mutated, i, CR)\n                x_trial = np.clip(x_trial, func.bounds.lb, func.bounds.ub)\n                f_trial = func(x_trial)\n                self.budget -= 1\n                if self.budget <= 0:\n                    break\n                if f_trial < self.fitness[i]:\n                    self.sf.append(F)\n                    self.scr.append(CR)\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(self.population[i].copy())\n                    else:\n                        j = np.random.randint(0, self.archive_size)\n                        self.archive[j] = self.population[i].copy()\n                    self.population[i] = x_trial.copy()\n                    self.fitness[i] = f_trial\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = x_trial.copy()\n            if self.budget <= 0:\n                break\n            self.update_memory()\n            best_index = np.argmin(self.fitness)\n            if self.fitness[best_index] < self.f_opt:\n                self.f_opt = self.fitness[best_index]\n                self.x_opt = self.population[best_index].copy()\n\n            self.best_fitness_history.append(self.f_opt)\n\n            if self.check_stagnation() and self.budget > self.adaptive_pop_size * 2:\n                self.restart_population(func)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "The algorithm AdaptiveDE scored 0.718 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["555dcdca-320a-4ce9-b585-17f051aba286"], "operator": null, "metadata": {"aucs": [0.262485939505797, 0.7702664908781344, 0.5993336412907342, 0.8897065899386336, 0.7787886831485188, 0.8191988013029589, 0.7036925495612226, 0.7476453813310764, 0.7996338997150481, 0.6481639547065566, 0.8773146567138423, 0.9999186655697019, 0.7700368586570228, 0.7281613549377299, 0.9133957494896854, 0.8013015497091981, 0.6259949226696131, 0.8697474506841687, 0.26291253137133264, 0.49764308106852173]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "07cd3153-e7b6-4ae7-be24-7abb88bb10ef", "fitness": "-inf", "name": "AdaptiveOrthogonalDE", "description": "Adaptive Differential Evolution with orthogonal design-based mutation and dynamic parameter adaptation based on success history.", "code": "import numpy as np\nfrom scipy.stats import norm\n\nclass AdaptiveOrthogonalDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=10, F_mu=0.5, CR_mu=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.F_mu = F_mu\n        self.CR_mu = CR_mu\n        self.memory_F = np.ones(self.archive_size) * F_mu\n        self.memory_CR = np.ones(self.archive_size) * CR_mu\n        self.population = None\n        self.fitness = None\n        self.archive = []\n        self.archive_fitness = []\n        self.best_fitness = float('inf')\n        self.best_solution = None\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        self.best_index = np.argmin(self.fitness)\n        self.best_fitness = self.fitness[self.best_index]\n        self.best_solution = self.population[self.best_index].copy()\n\n    def orthogonal_design(self, x_best, x_r1, x_r2):\n        \"\"\"\n        Generate trial vector using orthogonal design.\n        \"\"\"\n        design = np.array([[-1, -1], [1, -1], [-1, 1], [1, 1]])\n        x_trial = np.zeros((4, self.dim))\n\n        for i in range(self.dim):\n            if np.random.rand() < 0.5:\n                level_minus = x_r1[i]\n                level_plus = x_r2[i]\n            else:\n                level_minus = x_r2[i]\n                level_plus = x_r1[i]\n            \n            x_trial[0, i] = x_best[i] + 0.5 * design[0, 0] * (level_plus - level_minus)\n            x_trial[1, i] = x_best[i] + 0.5 * design[1, 0] * (level_plus - level_minus)\n            x_trial[2, i] = x_best[i] + 0.5 * design[2, 0] * (level_plus - level_minus)\n            x_trial[3, i] = x_best[i] + 0.5 * design[3, 0] * (level_plus - level_minus)\n        \n        return x_trial\n\n    def mutate_and_crossover(self, i):\n        \n        p = np.random.rand()\n        if p < 0.2:\n            # global search\n            idxs = np.random.choice(self.pop_size, 3, replace=False)\n            x_r1, x_r2, x_r3 = self.population[idxs]\n            F = np.random.normal(self.F_mu, 0.1)\n            F = np.clip(F, 0.1, 1.0)\n            x_mutated = self.population[i] + F * (x_r1 - x_r2) + F*(x_r3 - self.population[i])\n        else:\n\n            # local search with orthogonal design\n            idxs = np.random.choice(self.pop_size, 2, replace=False)\n            x_r1, x_r2 = self.population[idxs]\n            x_best = self.population[self.best_index]\n            x_trials = self.orthogonal_design(x_best, x_r1, x_r2)\n            \n            fitness_vals = []\n            for x_t in x_trials:\n                x_t = np.clip(x_t, func.bounds.lb, func.bounds.ub)\n                fitness_vals.append(func(x_t))\n                self.budget -= 1\n            best_trial_index = np.argmin(fitness_vals)\n            x_mutated = x_trials[best_trial_index]\n            f_mutated = fitness_vals[best_trial_index]\n            return x_mutated, f_mutated\n           \n        CR = np.random.normal(self.CR_mu, 0.1)\n        CR = np.clip(CR, 0.1, 1.0)\n        x_trial = np.copy(self.population[i])\n        j_rand = np.random.randint(self.dim)\n        for j in range(self.dim):\n            if np.random.rand() < CR or j == j_rand:\n                x_trial[j] = x_mutated[j]\n        x_trial = np.clip(x_trial, func.bounds.lb, func.bounds.ub)\n        return x_trial, None\n\n    def update_memory(self, F, CR):\n        self.memory_F = np.roll(self.memory_F, 1)\n        self.memory_CR = np.roll(self.memory_CR, 1)\n        self.memory_F[0] = F\n        self.memory_CR[0] = CR\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        generation = 0\n        while self.budget > 0:\n            generation += 1\n            successful_F = []\n            successful_CR = []\n\n            for i in range(self.pop_size):\n                x_trial, f_trial = self.mutate_and_crossover(i)\n                if f_trial is None:\n                    f_trial = func(x_trial)\n                    self.budget -= 1\n                \n                if self.budget <= 0:\n                    break\n\n                if f_trial < self.fitness[i]:\n                    successful_F.append(np.random.choice(self.memory_F))\n                    successful_CR.append(np.random.choice(self.memory_CR))\n\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(self.population[i].copy())\n                        self.archive_fitness.append(self.fitness[i])\n                    else:\n                        idx_to_replace = np.random.randint(self.archive_size)\n                        self.archive[idx_to_replace] = self.population[i].copy()\n                        self.archive_fitness[idx_to_replace] = self.fitness[i]\n\n                    self.population[i] = x_trial.copy()\n                    self.fitness[i] = f_trial\n\n                    if f_trial < self.best_fitness:\n                        self.best_fitness = f_trial\n                        self.best_solution = x_trial.copy()\n                        self.best_index = i\n\n\n            if len(successful_F) > 0:\n                self.F_mu = np.mean(successful_F)\n                self.CR_mu = np.mean(successful_CR)\n\n            if self.budget <= 0:\n                break\n\n        return self.best_fitness, self.best_solution", "configspace": "", "generation": 7, "feedback": "In the code, line 86, in mutate_and_crossover, the following error occurred:\nNameError: name 'func' is not defined\nOn line: x_trial = np.clip(x_trial, func.bounds.lb, func.bounds.ub)", "error": "In the code, line 86, in mutate_and_crossover, the following error occurred:\nNameError: name 'func' is not defined\nOn line: x_trial = np.clip(x_trial, func.bounds.lb, func.bounds.ub)", "parent_ids": ["3c377e79-26ed-41cf-8828-7863aaef3453"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "b1ec03ac-4c9e-4712-8f61-e324c474b6e6", "fitness": 0.3218705100996343, "name": "NeighborhoodAdaptiveDE", "description": "Self-Adaptive Differential Evolution with Neighborhood Search and Periodic Restart, leveraging a small population with focused adaptation of parameters and a neighborhood search strategy to escape local optima, combined with periodic restarts to enhance exploration.", "code": "import numpy as np\n\nclass NeighborhoodAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=20, initial_f=0.5, initial_cr=0.9, neighborhood_size=3, restart_interval=500):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = initial_f\n        self.CR = initial_cr\n        self.population = None\n        self.fitness = None\n        self.neighborhood_size = neighborhood_size\n        self.restart_interval = restart_interval\n        self.generation = 0\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n\n    def mutate(self, i):\n        idxs = np.random.choice(self.pop_size, 3, replace=False)\n        a, b, c = idxs[0], idxs[1], idxs[2]\n\n        # Self-adaptive F and CR\n        self.F = np.clip(np.random.normal(self.F, 0.1), 0.1, 1.0)\n        self.CR = np.clip(np.random.normal(self.CR, 0.1), 0.0, 1.0)\n\n        x_mutated = self.population[i] + self.F * (self.population[a] - self.population[b])\n\n        return x_mutated\n\n    def crossover(self, x_mutated, i):\n        x_trial = np.copy(self.population[i])\n        j_rand = np.random.randint(self.dim)\n        for j in range(self.dim):\n            if np.random.rand() < self.CR or j == j_rand:\n                x_trial[j] = x_mutated[j]\n        return x_trial\n\n    def neighborhood_search(self, individual, func, radius=0.1):\n        \"\"\"Performs a local search around the given individual.\"\"\"\n        best_individual = individual.copy()\n        best_fitness = func(individual)\n        self.budget -= 1\n        if self.budget <= 0:\n            return best_fitness, best_individual\n\n        for _ in range(5): # Number of neighborhood search iterations.\n            neighbor = individual + np.random.uniform(-radius, radius, size=self.dim)\n            neighbor = np.clip(neighbor, func.bounds.lb, func.bounds.ub)\n            fitness = func(neighbor)\n            self.budget -= 1\n            if self.budget <= 0:\n                return best_fitness, best_individual\n            if fitness < best_fitness:\n                best_fitness = fitness\n                best_individual = neighbor.copy()\n        return best_fitness, best_individual\n\n    def restart_population(self, func):\n        \"\"\"Restarts the population with new random individuals.\"\"\"\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)].copy()\n\n        while self.budget > 0:\n            self.generation += 1\n            for i in range(self.pop_size):\n                x_mutated = self.mutate(i)\n                x_trial = self.crossover(x_mutated, i)\n                x_trial = np.clip(x_trial, func.bounds.lb, func.bounds.ub)\n\n                f_trial = func(x_trial)\n                self.budget -= 1\n                if self.budget <= 0:\n                    break\n\n                # Neighborhood Search\n                f_neighbor, x_neighbor = self.neighborhood_search(x_trial, func)\n                if self.budget <= 0:\n                   break\n\n                if f_neighbor < self.fitness[i]:\n                    self.population[i] = x_neighbor.copy()\n                    self.fitness[i] = f_neighbor\n                    if f_neighbor < self.f_opt:\n                        self.f_opt = f_neighbor\n                        self.x_opt = x_neighbor.copy()\n            if self.budget <= 0:\n                break\n\n            # Periodic Restart\n            if self.generation % self.restart_interval == 0:\n                self.restart_population(func)\n                if np.min(self.fitness) < self.f_opt:\n                    self.f_opt = np.min(self.fitness)\n                    self.x_opt = self.population[np.argmin(self.fitness)].copy()\n\n\n            best_index = np.argmin(self.fitness)\n            if self.fitness[best_index] < self.f_opt:\n                self.f_opt = self.fitness[best_index]\n                self.x_opt = self.population[best_index].copy()\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "The algorithm NeighborhoodAdaptiveDE scored 0.322 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["555dcdca-320a-4ce9-b585-17f051aba286"], "operator": null, "metadata": {"aucs": [0.14942694524032973, 0.20546748487872113, 0.3265246562646621, 0.25719818879570644, 0.24819687593466633, 0.2784689540427081, 0.2638998314802127, 0.2742034581696182, 0.2563754490392819, 0.17759834756617543, 0.2737159272325155, 0.9541792947249086, 0.2662181498978762, 0.2467587184393989, 0.6402265583338808, 0.32975363200652663, 0.26210757933018447, 0.36819231830432786, 0.182609407319681, 0.4762884249913045]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "a638027c-9f10-44d8-a44f-78111169e1e4", "fitness": 0.34790981234121876, "name": "AdaptiveExplorationDE", "description": "Population-based algorithm with adaptive exploration-exploitation balance, using a simplified strategy selection and parameter adaptation based on population diversity.", "code": "import numpy as np\n\nclass AdaptiveExplorationDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, CR=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.population = None\n        self.fitness = None\n        self.best_fitness = float('inf')\n        self.best_solution = None\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        self.best_fitness = np.min(self.fitness)\n        self.best_solution = self.population[np.argmin(self.fitness)].copy()\n\n    def diversity(self):\n        # Measure population diversity based on the variance of each dimension\n        return np.mean(np.var(self.population, axis=0))\n\n    def mutate(self, i):\n        idxs = np.random.choice(self.pop_size, 3, replace=False)\n        a, b, c = idxs\n        x_mutated = self.population[a] + self.F * (self.population[b] - self.population[c])\n        return x_mutated\n\n    def crossover(self, x_mutated, i):\n        x_trial = np.copy(self.population[i])\n        j_rand = np.random.randint(self.dim)\n        for j in range(self.dim):\n            if np.random.rand() < self.CR or j == j_rand:\n                x_trial[j] = x_mutated[j]\n        return x_trial\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.budget > 0:\n            diversity_val = self.diversity()\n            # Adaptive F and CR based on population diversity\n            self.F = 0.2 + 0.7 * diversity_val # F is between 0.2 and 0.9\n            self.CR = 0.1 + 0.8 * diversity_val # CR is between 0.1 and 0.9\n\n            for i in range(self.pop_size):\n                x_mutated = self.mutate(i)\n                x_trial = self.crossover(x_mutated, i)\n                x_trial = np.clip(x_trial, func.bounds.lb, func.bounds.ub)\n\n                f_trial = func(x_trial)\n                self.budget -= 1\n\n                if self.budget <= 0:\n                    break\n\n                if f_trial < self.fitness[i]:\n                    self.population[i] = x_trial.copy()\n                    self.fitness[i] = f_trial\n                    if f_trial < self.best_fitness:\n                        self.best_fitness = f_trial\n                        self.best_solution = x_trial.copy()\n\n            if self.budget <= 0:\n                break\n\n            best_index = np.argmin(self.fitness)\n            if self.fitness[best_index] < self.best_fitness:\n                self.best_fitness = self.fitness[best_index]\n                self.best_solution = self.population[best_index].copy()\n\n\n        return self.best_fitness, self.best_solution", "configspace": "", "generation": 7, "feedback": "The algorithm AdaptiveExplorationDE scored 0.348 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["3c377e79-26ed-41cf-8828-7863aaef3453"], "operator": null, "metadata": {"aucs": [0.10520867369889564, 0.15750638427549624, 0.2892963654488615, 0.6178140638842367, 0.23468239803086022, 0.5183934285304765, 0.30173341825418376, 0.27778635830840936, 0.17360927985116048, 0.17285121341921017, 0.2281598365522478, 0.9991683789087554, 0.20557460203954292, 0.26760831108063243, 0.6743529197439269, 0.32650759164988186, 0.2787256092297784, 0.5062235059677751, 0.16613562810650195, 0.4568582798435421]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "013165fd-9449-4ca6-a8ee-d88ca0144fc0", "fitness": "-inf", "name": "AdaptiveDELocalSearch", "description": "An adaptive DE algorithm that utilizes a simplified parameter adaptation, focused exploitation with success-history adaptation of F and CR, and a novel local search strategy based on perturbing the best solution found so far.", "code": "import numpy as np\n\nclass AdaptiveDELocalSearch:\n    def __init__(self, budget=10000, dim=10, pop_size=50, initial_f=0.5, initial_cr=0.9, archive_size=10, local_search_probability = 0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = initial_f  # Initial differential weight\n        self.CR = initial_cr # Crossover rate\n        self.population = None\n        self.fitness = None\n        self.archive = []\n        self.archive_size = archive_size\n        self.sf = []\n        self.scr = []\n        self.memory_f = [initial_f] * archive_size\n        self.memory_cr = [initial_cr] * archive_size\n        self.p_best = 0.05\n        self.local_search_probability = local_search_probability\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n\n    def mutate(self, i):\n        phi_idx = np.random.randint(0, self.archive_size)\n        self.F = self.memory_f[phi_idx]\n        self.CR = self.memory_cr[phi_idx]\n        self.F = np.clip(np.random.normal(self.F, 0.1), 0.1, 1.0)\n        self.CR = np.clip(np.random.normal(self.CR, 0.1), 0.0, 1.0)\n\n        idxs = np.random.choice(self.pop_size, 4, replace=False)\n        a, b, c, p_best_idx = idxs[0], idxs[1], idxs[2], idxs[3]\n\n        if np.random.rand() < self.p_best:\n            p_best_idx = np.argmin(self.fitness)\n\n        x_mutated = self.population[i] + self.F * (self.population[p_best_idx] - self.population[i]) + self.F * (self.population[a] - self.population[b])\n\n        return x_mutated, self.F, self.CR\n\n    def crossover(self, x_mutated, i, cr):\n        x_trial = np.copy(self.population[i])\n        j_rand = np.random.randint(self.dim)\n        for j in range(self.dim):\n            if np.random.rand() < cr or j == j_rand:\n                x_trial[j] = x_mutated[j]\n        return x_trial\n\n    def update_memory(self):\n        if self.sf:\n            self.memory_f.append(sum(val**2 for val in self.sf) / sum(self.sf))\n            self.memory_f.pop(0)\n            self.memory_cr.append(np.mean(self.scr))\n            self.memory_cr.pop(0)\n        self.sf = []\n        self.scr = []\n        \n    def local_search(self, x_best, func, lb, ub):\n        x_perturbed = x_best.copy()\n        for i in range(self.dim):\n            # Perturb each dimension with a small random value\n            x_perturbed[i] += np.random.normal(0, 0.05 * (ub - lb))\n            # Clip to bounds\n            x_perturbed[i] = np.clip(x_perturbed[i], lb, ub)\n        f_perturbed = func(x_perturbed)\n        self.budget -= 1\n        return x_perturbed, f_perturbed\n        \n\n    def __call__(self, func):\n        self.initialize_population(func)\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)].copy()\n        self.sf = []\n        self.scr = []\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                x_mutated, F, CR = self.mutate(i)\n                x_trial = self.crossover(x_mutated, i, CR)\n                x_trial = np.clip(x_trial, func.bounds.lb, func.bounds.ub)\n                f_trial = func(x_trial)\n                self.budget -= 1\n                if self.budget <= 0:\n                    break\n                if f_trial < self.fitness[i]:\n                    self.sf.append(F)\n                    self.scr.append(CR)\n                    delta = abs(self.fitness[i] - f_trial)\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(self.population[i].copy())\n                    else:\n                        j = np.random.randint(0, self.archive_size)\n                        self.archive[j] = self.population[i].copy()\n                    self.population[i] = x_trial.copy()\n                    self.fitness[i] = f_trial\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = x_trial.copy()\n            if self.budget <= 0:\n                break\n            self.update_memory()\n            best_index = np.argmin(self.fitness)\n            if self.fitness[best_index] < self.f_opt:\n                self.f_opt = self.fitness[best_index]\n                self.x_opt = self.population[best_index].copy()\n            \n            # Perform local search around the best solution with a certain probability\n            if np.random.rand() < self.local_search_probability and self.budget > 0:\n                x_perturbed, f_perturbed = self.local_search(self.x_opt, func, func.bounds.lb, func.bounds.ub)\n                if f_perturbed < self.f_opt:\n                    self.f_opt = f_perturbed\n                    self.x_opt = x_perturbed.copy()\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "In the code, line 64, in local_search, the following error occurred:\nValueError: setting an array element with a sequence.\nOn line: x_perturbed[i] += np.random.normal(0, 0.05 * (ub - lb))", "error": "In the code, line 64, in local_search, the following error occurred:\nValueError: setting an array element with a sequence.\nOn line: x_perturbed[i] += np.random.normal(0, 0.05 * (ub - lb))", "parent_ids": ["555dcdca-320a-4ce9-b585-17f051aba286"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "be2882fe-49d0-401f-b030-245cb2807869", "fitness": 0.0, "name": "OrthogonalRestartDE", "description": "Population-based algorithm with a restart mechanism triggered by stagnation, using orthogonal learning to generate diverse solutions and an archive to preserve promising regions.", "code": "import numpy as np\n\nclass OrthogonalRestartDE:\n    def __init__(self, budget=10000, dim=10, pop_size=40, archive_size=10, stagnation_limit=50):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.population = None\n        self.fitness = None\n        self.F = 0.5\n        self.CR = 0.7\n        self.archive = []\n        self.stagnation_limit = stagnation_limit\n        self.stagnation_counter = 0\n        self.best_fitness_history = []\n        self.learning_rate = 0.1\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)].copy()\n        self.best_fitness_history.append(self.f_opt)\n\n\n    def mutate(self, i):\n        idxs = np.random.choice(self.pop_size, 3, replace=False)\n        a, b, c = idxs\n        x_mutated = self.population[a] + self.F * (self.population[b] - self.population[c])\n        return x_mutated\n\n    def crossover(self, x_mutated, i):\n        x_trial = np.copy(self.population[i])\n        j_rand = np.random.randint(self.dim)\n        for j in range(self.dim):\n            if np.random.rand() < self.CR or j == j_rand:\n                x_trial[j] = x_mutated[j]\n        return x_trial\n\n    def orthogonal_learning(self, func):\n        # Select two parents from the population\n        parent1_idx, parent2_idx = np.random.choice(self.pop_size, 2, replace=False)\n        parent1 = self.population[parent1_idx]\n        parent2 = self.population[parent2_idx]\n\n        # Generate new individuals based on orthogonal design\n        new_individuals = []\n        for _ in range(self.pop_size // 2):\n            alpha = np.random.rand(self.dim)\n            new_individual = parent1 + alpha * (parent2 - parent1)\n            new_individual = np.clip(new_individual, func.bounds.lb, func.bounds.ub)\n            new_individuals.append(new_individual)\n\n        return np.array(new_individuals)\n\n    def restart_population(self, func):\n        # Combine the archive and the current population\n        combined_population = np.concatenate((self.population, np.array(self.archive)), axis=0) if self.archive else self.population\n\n        # Select the best individuals from the combined population\n        num_best = min(self.pop_size // 2, len(combined_population))\n        fitness_values = np.array([func(x) for x in combined_population])\n        best_indices = np.argsort(fitness_values)[:num_best]\n        best_individuals = combined_population[best_indices]\n\n        # Generate new individuals using orthogonal learning\n        new_individuals = self.orthogonal_learning(func)\n\n        # Combine the best individuals and new individuals to form the new population\n        remaining = self.pop_size - len(best_individuals)\n        if remaining > 0:\n          self.population = np.concatenate((best_individuals, new_individuals[:remaining]), axis=0)\n        else:\n          self.population = best_individuals[:self.pop_size]\n\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size - num_best if remaining > 0 else self.pop_size\n        self.archive = []\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)].copy()\n        self.stagnation_counter = 0\n        self.best_fitness_history = [self.f_opt]\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                x_mutated = self.mutate(i)\n                x_trial = self.crossover(x_mutated, i)\n                x_trial = np.clip(x_trial, func.bounds.lb, func.bounds.ub)\n\n                f_trial = func(x_trial)\n                self.budget -= 1\n\n                if self.budget <= 0:\n                    break\n\n                if f_trial < self.fitness[i]:\n                    self.population[i] = x_trial.copy()\n                    self.fitness[i] = f_trial\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = x_trial.copy()\n\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(x_trial.copy())\n                    else:\n                        self.archive[np.random.randint(self.archive_size)] = x_trial.copy()\n\n            if self.budget <= 0:\n                break\n\n            best_index = np.argmin(self.fitness)\n            if self.fitness[best_index] < self.f_opt:\n                self.f_opt = self.fitness[best_index]\n                self.x_opt = self.population[best_index].copy()\n                self.stagnation_counter = 0\n            else:\n                self.stagnation_counter += 1\n\n            self.best_fitness_history.append(self.f_opt)\n\n            if self.stagnation_counter > self.stagnation_limit:\n                self.restart_population(func)\n                self.stagnation_counter = 0\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "The algorithm OrthogonalRestartDE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["89cf3d05-e397-482e-b884-c3e3c7ab1700"], "operator": null, "metadata": {"aucs": [0]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "413bfb46-40f4-4f71-abaf-fc13deaf5811", "fitness": 0.0, "name": "HybridDEGradient", "description": "Population-based algorithm that employs a novel strategy combining differential evolution with a gradient-based local search, adaptively switching between exploration and exploitation phases based on population diversity and success rates of each phase.", "code": "import numpy as np\n\nclass HybridDEGradient:\n    def __init__(self, budget=10000, dim=10, pop_size=50, initial_f=0.5, initial_cr=0.9, grad_prob=0.1, grad_step=0.01, diversity_threshold=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = initial_f\n        self.CR = initial_cr\n        self.population = None\n        self.fitness = None\n        self.grad_prob = grad_prob  # Probability of applying gradient-based local search\n        self.grad_step = grad_step # Step size for gradient descent\n        self.diversity_threshold = diversity_threshold # Threshold for population diversity\n        self.exploration_weight = 0.5\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)].copy()\n\n    def calculate_diversity(self):\n        \"\"\"Calculates the average pairwise distance between individuals in the population.\"\"\"\n        distances = []\n        for i in range(self.pop_size):\n            for j in range(i + 1, self.pop_size):\n                distances.append(np.linalg.norm(self.population[i] - self.population[j]))\n        return np.mean(distances) if distances else 0\n\n    def mutate(self, i):\n        idxs = np.random.choice(self.pop_size, 3, replace=False)\n        a, b, c = idxs[0], idxs[1], idxs[2]\n        x_mutated = self.population[a] + self.F * (self.population[b] - self.population[c])\n        return x_mutated\n\n    def crossover(self, x_mutated, i):\n        x_trial = np.copy(self.population[i])\n        j_rand = np.random.randint(self.dim)\n        for j in range(self.dim):\n            if np.random.rand() < self.CR or j == j_rand:\n                x_trial[j] = x_mutated[j]\n        return x_trial\n\n    def gradient_descent(self, x, func):\n        \"\"\"Performs a simple gradient descent step.\"\"\"\n        gradient = np.zeros_like(x)\n        for j in range(self.dim):\n            x_plus = x.copy()\n            x_minus = x.copy()\n            x_plus[j] += self.grad_step\n            x_minus[j] -= self.grad_step\n            x_plus = np.clip(x_plus, func.bounds.lb, func.bounds.ub)\n            x_minus = np.clip(x_minus, func.bounds.lb, func.bounds.ub)\n            gradient[j] = (func(x_plus) - func(x_minus)) / (2 * self.grad_step)\n\n        x_new = x - self.grad_step * gradient\n        x_new = np.clip(x_new, func.bounds.lb, func.bounds.ub)\n        return x_new\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        exploration_success = 0\n        exploitation_success = 0\n        exploration_attempts = 0\n        exploitation_attempts = 0\n\n        while self.budget > 0:\n            diversity = self.calculate_diversity()\n            for i in range(self.pop_size):\n                if self.budget <= 0:\n                    break\n\n                if np.random.rand() < self.exploration_weight or diversity > self.diversity_threshold:\n                    # Exploration Phase: Differential Evolution\n                    exploration_attempts += 1\n                    x_mutated = self.mutate(i)\n                    x_trial = self.crossover(x_mutated, i)\n                    x_trial = np.clip(x_trial, func.bounds.lb, func.bounds.ub)\n                    f_trial = func(x_trial)\n                    self.budget -= 1\n                    if f_trial < self.fitness[i]:\n                        self.population[i] = x_trial.copy()\n                        self.fitness[i] = f_trial\n                        exploration_success += 1\n                else:\n                    # Exploitation Phase: Gradient-based Local Search\n                    exploitation_attempts += 1\n                    if np.random.rand() < self.grad_prob:\n                        x_trial = self.gradient_descent(self.population[i].copy(), func)\n                        f_trial = func(x_trial)\n                        self.budget -= 1\n                        if f_trial < self.fitness[i]:\n                            self.population[i] = x_trial.copy()\n                            self.fitness[i] = f_trial\n                            exploitation_success += 1\n                        else:\n                            f_trial = self.fitness[i]  #Revert back\n                    else:\n                        f_trial = self.fitness[i]\n\n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = self.population[i].copy()\n\n            # Adjust exploration/exploitation balance based on success rates\n            if exploration_attempts > 0 and exploitation_attempts > 0:\n                exploration_rate = exploration_success / exploration_attempts if exploration_attempts > 0 else 0\n                exploitation_rate = exploitation_success / exploitation_attempts if exploitation_attempts > 0 else 0\n                if exploration_rate > exploitation_rate:\n                    self.exploration_weight = min(1, self.exploration_weight + 0.05)\n                else:\n                    self.exploration_weight = max(0, self.exploration_weight - 0.05)\n\n            exploration_success = 0\n            exploitation_success = 0\n            exploration_attempts = 0\n            exploitation_attempts = 0\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "The algorithm HybridDEGradient scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["555dcdca-320a-4ce9-b585-17f051aba286"], "operator": null, "metadata": {"aucs": [0]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "54672061-7521-42b6-a4b8-50d2f4f14aad", "fitness": "-inf", "name": "HybridAdaptiveDE", "description": "A hybrid DE algorithm that combines aspects of multiple DE variants with a novel selection mechanism favoring both exploration and exploitation, dynamically adapting search parameters based on population diversity and success rates.", "code": "import numpy as np\n\nclass HybridAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, initial_f=0.5, initial_cr=0.9, archive_size=10, stagnation_threshold=100, p_random=0.1, p_pbest=0.2):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = initial_f\n        self.CR = initial_cr\n        self.population = None\n        self.fitness = None\n        self.archive = []\n        self.archive_size = archive_size\n        self.sf = []\n        self.scr = []\n        self.memory_f = [initial_f] * archive_size\n        self.memory_cr = [initial_cr] * archive_size\n        self.stagnation_threshold = stagnation_threshold\n        self.stagnation_counter = 0\n        self.best_fitness_history = []\n        self.adaptive_pop_size = self.pop_size\n        self.p_random = p_random  # Probability of using a completely random vector in mutation\n        self.p_pbest = p_pbest # Probability of using pbest instead of random individual\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.adaptive_pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.adaptive_pop_size\n\n    def mutate(self, i):\n        phi_idx = np.random.randint(0, self.archive_size)\n        self.F = self.memory_f[phi_idx]\n        self.CR = self.memory_cr[phi_idx]\n        self.F = np.clip(np.random.normal(self.F, 0.1), 0.1, 1.0)\n        self.CR = np.clip(np.random.normal(self.CR, 0.1), 0.0, 1.0)\n\n        if np.random.rand() < self.p_random:\n            # Option 1: Mutation with a random vector for exploration\n            x_rand = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n            idxs = np.random.choice(self.adaptive_pop_size, 2, replace=False)\n            a, b = idxs[0], idxs[1]\n            x_mutated = x_rand + self.F * (self.population[a] - self.population[b])\n        else:\n            # Option 2: Standard DE mutation with p_best influence\n            idxs = np.random.choice(self.adaptive_pop_size, 3, replace=False)\n            a, b, c = idxs[0], idxs[1], idxs[2]\n            \n            if np.random.rand() < self.p_pbest:\n                 p_best_idx = np.argmin(self.fitness)\n                 x_mutated = self.population[i] + self.F * (self.population[p_best_idx] - self.population[i]) + self.F * (self.population[a] - self.population[b])\n            else:\n                 x_mutated = self.population[a] + self.F * (self.population[b] - self.population[c])\n           \n\n        return x_mutated, self.F, self.CR\n\n    def crossover(self, x_mutated, i, cr):\n        x_trial = np.copy(self.population[i])\n        j_rand = np.random.randint(self.dim)\n        for j in range(self.dim):\n            if np.random.rand() < cr or j == j_rand:\n                x_trial[j] = x_mutated[j]\n        return x_trial\n\n    def update_memory(self):\n        if self.sf:\n            self.memory_f.append(sum(val**2 for val in self.sf) / sum(self.sf) if sum(self.sf) != 0 else self.memory_f[-1])\n            self.memory_f.pop(0)\n            self.memory_cr.append(np.mean(self.scr))\n            self.memory_cr.pop(0)\n        self.sf = []\n        self.scr = []\n\n    def check_stagnation(self):\n        if len(self.best_fitness_history) > self.stagnation_threshold:\n            if np.std(self.best_fitness_history[-self.stagnation_threshold:]) < 1e-6:\n                self.stagnation_counter += 1\n                return True\n            else:\n                self.stagnation_counter = 0\n                return False\n        return False\n\n    def restart_population(self, func):\n        self.adaptive_pop_size = int(self.pop_size * 0.75) # Reduce population size upon restart\n        self.adaptive_pop_size = max(10, self.adaptive_pop_size)\n        new_population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.adaptive_pop_size, self.dim))\n        new_fitness = np.array([func(x) for x in new_population])\n        self.budget -= self.adaptive_pop_size\n        self.population = new_population\n        self.fitness = new_fitness\n\n        self.best_fitness_history = []\n        self.memory_f = [0.5] * self.archive_size\n        self.memory_cr = [0.9] * self.archive_size\n        self.sf = []\n        self.scr = []\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)].copy()\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)].copy()\n        self.sf = []\n        self.scr = []\n\n        while self.budget > 0:\n            for i in range(self.adaptive_pop_size):\n                x_mutated, F, CR = self.mutate(i)\n                x_trial = self.crossover(x_mutated, i, CR)\n                x_trial = np.clip(x_trial, func.bounds.lb, func.bounds.ub)\n                f_trial = func(x_trial)\n                self.budget -= 1\n                if self.budget <= 0:\n                    break\n                if f_trial < self.fitness[i]:\n                    self.sf.append(F)\n                    self.scr.append(CR)\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(self.population[i].copy())\n                    else:\n                        j = np.random.randint(0, self.archive_size)\n                        self.archive[j] = self.population[i].copy()\n                    self.population[i] = x_trial.copy()\n                    self.fitness[i] = f_trial\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = x_trial.copy()\n            if self.budget <= 0:\n                break\n            self.update_memory()\n            best_index = np.argmin(self.fitness)\n            if self.fitness[best_index] < self.f_opt:\n                self.f_opt = self.fitness[best_index]\n                self.x_opt = self.population[best_index].copy()\n\n            self.best_fitness_history.append(self.f_opt)\n\n            if self.check_stagnation() and self.budget > self.adaptive_pop_size * 2:\n                self.restart_population(func)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "In the code, line 39, in mutate, the following error occurred:\nNameError: name 'func' is not defined\nOn line: x_rand = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)", "error": "In the code, line 39, in mutate, the following error occurred:\nNameError: name 'func' is not defined\nOn line: x_rand = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)", "parent_ids": ["b1035a21-7f93-411a-8c32-f80b40512d34"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "3da6135f-cd31-4d9c-876d-0e08c81d514c", "fitness": 0.7581164741182045, "name": "AdaptiveDE", "description": "Adaptive Differential Evolution with a combined mutation strategy using both current-to-pbest and random differential vectors, dynamically adjusted parameters based on success history, and orthogonal learning to enhance population diversity and convergence speed.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, initial_f=0.5, initial_cr=0.9, archive_size=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = initial_f  # Initial differential weight\n        self.CR = initial_cr # Crossover rate\n        self.population = None\n        self.fitness = None\n        self.archive = []\n        self.archive_size = archive_size\n        self.sf = []\n        self.scr = []\n        self.memory_f = [initial_f] * archive_size\n        self.memory_cr = [initial_cr] * archive_size\n        self.p_best = 0.05\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n\n    def mutate(self, i):\n        phi_idx = np.random.randint(0, self.archive_size)\n        self.F = self.memory_f[phi_idx]\n        self.CR = self.memory_cr[phi_idx]\n        self.F = np.clip(np.random.normal(self.F, 0.1), 0.1, 1.0)\n        self.CR = np.clip(np.random.normal(self.CR, 0.1), 0.0, 1.0)\n\n        idxs = np.random.choice(self.pop_size, 4, replace=False, p=None)\n        a, b, c, d = idxs[0], idxs[1], idxs[2], idxs[3]\n\n        p_best_idx = np.argmin(self.fitness)\n        if np.random.rand() > self.p_best:\n            x_mutated = self.population[i] + self.F * (self.population[p_best_idx] - self.population[i]) + self.F * (self.population[a] - self.population[b])\n        else:\n            x_mutated = self.population[i] + self.F * (self.population[a] - self.population[b]) + self.F * (self.population[c] - self.population[d])\n\n\n        return x_mutated, self.F, self.CR\n\n    def crossover(self, x_mutated, i, cr):\n        x_trial = np.copy(self.population[i])\n        j_rand = np.random.randint(self.dim)\n        for j in range(self.dim):\n            if np.random.rand() < cr or j == j_rand:\n                x_trial[j] = x_mutated[j]\n        return x_trial\n\n    def orthogonal_learning(self):\n        # Orthogonal experimental design for generating diverse candidates\n        num_samples = min(self.pop_size, 2 * self.dim)\n        candidates = np.zeros((num_samples, self.dim))\n\n        for i in range(num_samples):\n            # Select a random individual from the population as the base\n            base_individual = self.population[np.random.randint(self.pop_size)].copy()\n\n            # Generate a random perturbation vector\n            perturbation = np.random.uniform(-0.1, 0.1, size=self.dim) #Smaller perturbation\n\n            # Create a candidate solution\n            candidates[i] = base_individual + perturbation\n            candidates[i] = np.clip(candidates[i], -5.0, 5.0) #Respect the bounds\n\n        return candidates\n\n\n    def update_memory(self):\n        if self.sf:\n            self.memory_f.append(sum(val**2 for val in self.sf) / sum(self.sf))\n            self.memory_f.pop(0)\n            self.memory_cr.append(np.mean(self.scr))\n            self.memory_cr.pop(0)\n        self.sf = []\n        self.scr = []\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)].copy()\n        self.sf = []\n        self.scr = []\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                x_mutated, F, CR = self.mutate(i)\n                x_trial = self.crossover(x_mutated, i, CR)\n                x_trial = np.clip(x_trial, func.bounds.lb, func.bounds.ub)\n                f_trial = func(x_trial)\n                self.budget -= 1\n                if self.budget <= 0:\n                    break\n                if f_trial < self.fitness[i]:\n                    self.sf.append(F)\n                    self.scr.append(CR)\n                    delta = abs(self.fitness[i] - f_trial)\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(self.population[i].copy())\n                    else:\n                        j = np.random.randint(0, self.archive_size)\n                        self.archive[j] = self.population[i].copy()\n                    self.population[i] = x_trial.copy()\n                    self.fitness[i] = f_trial\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = x_trial.copy()\n            if self.budget <= 0:\n                break\n\n            # Orthogonal Learning Phase\n            candidates = self.orthogonal_learning()\n            candidate_fitness = np.array([func(x) for x in candidates])\n            self.budget -= len(candidates)\n            if self.budget <= 0:\n                break\n            for j in range(len(candidates)):\n                if candidate_fitness[j] < np.max(self.fitness):\n                    worst_index = np.argmax(self.fitness)\n                    self.population[worst_index] = candidates[j].copy()\n                    self.fitness[worst_index] = candidate_fitness[j]\n                    if candidate_fitness[j] < self.f_opt:\n                        self.f_opt = candidate_fitness[j]\n                        self.x_opt = candidates[j].copy()\n\n\n            self.update_memory()\n            best_index = np.argmin(self.fitness)\n            if self.fitness[best_index] < self.f_opt:\n                self.f_opt = self.fitness[best_index]\n                self.x_opt = self.population[best_index].copy()\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "The algorithm AdaptiveDE scored 0.758 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["555dcdca-320a-4ce9-b585-17f051aba286"], "operator": null, "metadata": {"aucs": [0.46088146017988396, 0.8520015803255122, 0.8337289293890605, 0.9385697407918699, 0.8497468193015317, 0.8801310117685734, 0.7165917880032672, 0.7953113721097752, 0.8564158536212764, 0.4601769582851428, 0.9288030009673024, 0.9954472932411746, 0.8472475826630186, 0.8073981489083817, 0.957318649872949, 0.8739081516032907, 0.44759057734971874, 0.9150519488440249, 0.23306622055921455, 0.5129423945791196]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "c0b296af-cb68-4772-91cc-914a2d0b1663", "fitness": "-inf", "name": "CMAES_OL", "description": "Population-based algorithm with covariance matrix adaptation and orthogonal learning, utilizing a focused sampling strategy to enhance exploitation around promising regions and maintaining population diversity to avoid premature convergence.", "code": "import numpy as np\n\nclass CMAES_OL:\n    def __init__(self, budget=10000, dim=10, pop_size=None, mu=None, sigma=0.5, cs=0.3, damps=1.0, ccov1=0.0, ccovmu=0.0, restarts=2):\n        self.budget = budget\n        self.dim = dim\n        self.sigma = sigma\n        self.restarts = restarts\n\n        if pop_size is None:\n            self.pop_size = 4 + int(3 * np.log(self.dim))\n        else:\n            self.pop_size = pop_size\n\n        if mu is None:\n            self.mu = self.pop_size // 2\n        else:\n            self.mu = mu\n\n        self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n        self.weights /= np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n\n        self.cs = cs\n        self.damps = damps + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1)\n        self.ccov1 = 2 / ((self.dim + 1.3)**2 + self.mueff)\n        self.ccovmu = min(1 - self.ccov1, 2 * (self.mueff - 2 + 1/self.mueff) / ((self.dim + 2.0)**2 + self.mueff))\n        self.chiN = self.dim**0.5 * (1 - 1/(4*self.dim) + 1/(21*self.dim**2))\n\n    def initialize(self, func):\n        self.mean = np.random.uniform(func.bounds.lb, func.bounds.ub, self.dim)\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.invC = np.linalg.inv(self.C)\n        self.B = None\n        self.D = None\n        self.f_opt = float('inf')\n        self.x_opt = None\n        self.lb = func.bounds.lb\n        self.ub = func.bounds.ub\n\n    def sample_population(self):\n        z = np.random.normal(0, 1, size=(self.pop_size, self.dim))\n        x = self.mean + self.sigma * z @ self.get_B_matrix().T\n        return x, z\n\n    def orthogonal_learning(self, x, func):\n         # Orthogonal learning to enhance population diversity\n        rand_idx = np.random.choice(self.pop_size, size=2, replace=False)\n        x1, x2 = x[rand_idx[0]], x[rand_idx[1]]\n        \n        d = x2 - x1\n        if np.linalg.norm(d) == 0:\n            return\n\n        alpha = np.random.uniform(-1, 1)\n        x_new = x1 + alpha * d\n        x_new = np.clip(x_new, self.lb, self.ub)\n        f_new = func(x_new)\n        self.budget -= 1\n        \n        return x_new, f_new\n\n    def get_B_matrix(self):\n        if self.B is None or self.D is None:\n            self.D, self.B = np.linalg.eigh(self.C)\n            self.D = np.sqrt(self.D)\n        return self.B * self.D\n\n    def update(self, x, z, fitness):\n        x_sorted = x[np.argsort(fitness)]\n        z_sorted = z[np.argsort(fitness)]\n\n        y = z_sorted[:self.mu]\n\n        y_w = np.sum(self.weights[:, None] * y, axis=0)\n        self.ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs) * self.mueff) * y_w\n        hsig = np.linalg.norm(self.ps) / np.sqrt(1 - (1 - self.cs)**2) / self.chiN < 1.4 + 2 / (self.dim + 1)\n        self.pc = (1 - 1) * self.pc + hsig * np.sqrt(1 * (2 - 1) * self.mueff) * self.get_B_matrix() @ y_w\n\n        artw = self.weights * y\n        self.C = (1 - self.ccov1 - self.ccovmu + self.ccov1 * 1 * (self.pc @ self.pc.T + (1-hsig) * self.C)\n                  + self.ccovmu * artw @ artw.T)\n        self.sigma *= np.exp((self.cs / self.damps) * (np.linalg.norm(self.ps) / self.chiN - 1))\n        self.mean = np.sum(self.weights[:, None] * x_sorted[:self.mu], axis=0)\n        self.B = None\n        self.D = None\n\n    def __call__(self, func):\n        self.initialize(func)\n        \n        for _ in range(self.restarts):\n          while self.budget > 0:\n              x, z = self.sample_population()\n              fitness = np.array([func(xi) for xi in x])\n              self.budget -= self.pop_size\n\n              if self.budget <= 0:\n                  break\n              \n              # Orthogonal learning step\n              x_new, f_new = self.orthogonal_learning(x, func)\n              if x_new is not None:\n                  x = np.vstack((x, x_new))\n                  fitness = np.append(fitness, f_new)\n\n              best_idx = np.argmin(fitness)\n              if fitness[best_idx] < self.f_opt:\n                  self.f_opt = fitness[best_idx]\n                  self.x_opt = x[best_idx].copy()\n              \n              self.update(x, z, fitness)\n\n          if self.budget <= 0:\n             break\n          self.initialize(func)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "In the code, line 73, in update, the following error occurred:\nIndexError: index 6 is out of bounds for axis 0 with size 6\nOn line: z_sorted = z[np.argsort(fitness)]", "error": "In the code, line 73, in update, the following error occurred:\nIndexError: index 6 is out of bounds for axis 0 with size 6\nOn line: z_sorted = z[np.argsort(fitness)]", "parent_ids": ["555dcdca-320a-4ce9-b585-17f051aba286"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "1995630b-666d-41c8-836b-4324900c182f", "fitness": "-inf", "name": "PSOCMAES", "description": "Hybrid algorithm blending Particle Swarm Optimization with Covariance Matrix Adaptation Evolution Strategy (PSO-CMA-ES) for enhanced exploration and exploitation.", "code": "import numpy as np\n\nclass PSOCMAES:\n    def __init__(self, budget=10000, dim=10, pop_size=50, pso_omega=0.7, pso_phi_p=1.5, pso_phi_g=1.5, cma_sigma=0.5, cma_mu=None):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.pso_omega = pso_omega\n        self.pso_phi_p = pso_phi_p\n        self.pso_phi_g = pso_phi_g\n        self.cma_sigma = cma_sigma\n        self.cma_mu = cma_mu if cma_mu is not None else pop_size // 2 # Number of parents for recombination\n        self.population = None\n        self.fitness = None\n        self.velocities = None\n        self.personal_best_positions = None\n        self.personal_best_fitness = None\n        self.global_best_position = None\n        self.global_best_fitness = float('inf')\n\n        self.cma_C = None # Covariance Matrix\n        self.cma_D = None # Diagonal matrix with standard deviations\n        self.cma_B = None # Matrix of eigenvectors\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.velocities = np.random.uniform(-0.1, 0.1, size=(self.pop_size, self.dim))  # Initialize velocities\n        self.personal_best_positions = self.population.copy()\n        self.personal_best_fitness = self.fitness.copy()\n\n        best_index = np.argmin(self.fitness)\n        self.global_best_position = self.population[best_index].copy()\n        self.global_best_fitness = self.fitness[best_index]\n        self.budget -= self.pop_size\n\n        # Initialize CMA-ES parameters\n        self.cma_C = np.eye(self.dim)\n        self.cma_D = np.ones(self.dim) # Initial standard deviations\n        self.cma_B = np.eye(self.dim)  # Initial eigenvectors\n        self.update_eigenvectors()\n\n\n\n    def pso_update(self, func):\n        r_p = np.random.rand(self.pop_size, self.dim)\n        r_g = np.random.rand(self.pop_size, self.dim)\n\n        self.velocities = (self.pso_omega * self.velocities +\n                           self.pso_phi_p * r_p * (self.personal_best_positions - self.population) +\n                           self.pso_phi_g * r_g * (self.global_best_position - self.population))\n\n        self.population += self.velocities\n\n        # Clip the population within the bounds\n        self.population = np.clip(self.population, func.bounds.lb, func.bounds.ub)\n\n        new_fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n\n        for i in range(self.pop_size):\n            if new_fitness[i] < self.personal_best_fitness[i]:\n                self.personal_best_fitness[i] = new_fitness[i]\n                self.personal_best_positions[i] = self.population[i].copy()\n\n            if new_fitness[i] < self.global_best_fitness:\n                self.global_best_fitness = new_fitness[i]\n                self.global_best_position = self.population[i].copy()\n        \n        self.fitness = new_fitness\n\n    def cma_sample(self):\n        z = np.random.normal(0, 1, size=(self.pop_size, self.dim))\n        y = self.cma_B @ (self.cma_D * z).T\n        x = self.cma_sigma * y.T + self.global_best_position\n        return x\n\n    def cma_update(self, func):\n\n        offspring = self.cma_sample()\n        offspring = np.clip(offspring, func.bounds.lb, func.bounds.ub)\n        offspring_fitness = np.array([func(x) for x in offspring])\n\n        self.budget -= self.pop_size\n\n        # Selection and Recombination (simplified: just pick top mu)\n        sorted_indices = np.argsort(offspring_fitness)\n        parents = offspring[sorted_indices[:self.cma_mu]]\n        \n        # Update the global best\n        best_index = np.argmin(offspring_fitness)\n        if offspring_fitness[best_index] < self.global_best_fitness:\n            self.global_best_fitness = offspring_fitness[best_index]\n            self.global_best_position = offspring[best_index].copy()\n        \n        # Adapt Covariance Matrix\n        diff = parents - self.global_best_position\n        self.cma_C = (1 - 1/self.dim) * self.cma_C + (1/self.dim) * (diff @ diff.T) / self.cma_sigma**2\n        \n        self.update_eigenvectors()\n\n    def update_eigenvectors(self):\n         # Eigenvalue decomposition of C\n        self.cma_D, self.cma_B = np.linalg.eig(self.cma_C)\n        self.cma_D = np.sqrt(np.abs(self.cma_D))  # Ensure positive standard deviations\n\n\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.budget > 0:\n            if self.budget > 0:\n              self.pso_update(func)\n            if self.budget > 0:\n              self.cma_update(func)\n\n        return self.global_best_fitness, self.global_best_position", "configspace": "", "generation": 8, "feedback": "In the code, line 98, in cma_update, the following error occurred:\nValueError: operands could not be broadcast together with shapes (5,5) (25,25) \nOn line: self.cma_C = (1 - 1/self.dim) * self.cma_C + (1/self.dim) * (diff @ diff.T) / self.cma_sigma**2", "error": "In the code, line 98, in cma_update, the following error occurred:\nValueError: operands could not be broadcast together with shapes (5,5) (25,25) \nOn line: self.cma_C = (1 - 1/self.dim) * self.cma_C + (1/self.dim) * (diff @ diff.T) / self.cma_sigma**2", "parent_ids": ["73551b59-70ed-4fbd-a6e6-cc84bc34a85c"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "959042e0-c7fd-4f7d-91da-a41fdc341f98", "fitness": 0.7646398238153435, "name": "SimplifiedAdaptiveDE", "description": "A simplified, self-adaptive Differential Evolution with a combined mutation strategy and dynamic F/CR adaptation based on success history and population diversity.", "code": "import numpy as np\n\nclass SimplifiedAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.population = None\n        self.fitness = None\n        self.memory_F = np.ones(self.archive_size) * 0.5\n        self.memory_CR = np.ones(self.archive_size) * 0.9\n        self.archive = []\n        self.F = 0.5\n        self.CR = 0.9\n        self.p = 0.1  # Probability for choosing F and CR from memory\n        self.min_CR = 0.1\n        self.stagnation_counter = 0\n        self.max_stagnation = 50 #restart if stagnant for longer\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n\n    def mutate(self, i):\n        # Combined mutation: DE/rand/1 OR DE/current-to-best/1\n        if np.random.rand() < 0.5:  # Switch between strategies\n            idxs = np.random.choice(self.pop_size, 5, replace=False)\n            a, b, c, d, e = idxs\n            x_mutated = self.population[a] + self.F * (self.population[b] - self.population[c]) + self.F * (self.population[d] - self.population[e])\n        else:\n            best_idx = np.argmin(self.fitness)\n            idxs = np.random.choice(self.pop_size, 3, replace=False)\n            a, b, c = idxs\n            x_mutated = self.population[i] + self.F * (self.population[best_idx] - self.population[i]) + self.F * (self.population[a] - self.population[b])\n\n        return x_mutated\n\n    def crossover(self, x_mutated, i):\n        x_trial = np.copy(self.population[i])\n        j_rand = np.random.randint(self.dim)\n        for j in range(self.dim):\n            if np.random.rand() < self.CR or j == j_rand:\n                x_trial[j] = x_mutated[j]\n        return x_trial\n\n    def update_parameters(self, success, F, CR):\n         if success:\n            self.archive.append((F, CR))\n            if len(self.archive) > self.archive_size:\n                self.archive.pop(0)\n\n            if len(self.archive) > 0:\n                F_vals = [item[0] for item in self.archive]\n                CR_vals = [item[1] for item in self.archive]\n                self.F = np.mean(F_vals)\n                self.CR = max(self.min_CR, np.mean(CR_vals)) # Ensure CR doesn't get too small\n\n    def check_stagnation(self):\n        if len(set(self.fitness)) <= 1:\n            self.stagnation_counter +=1\n        else:\n            self.stagnation_counter = 0\n\n        if self.stagnation_counter > self.max_stagnation:\n            return True\n        else:\n            return False\n\n    def restart_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        self.stagnation_counter = 0\n        self.archive = []  # Clear the archive on restart\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)].copy()\n\n        while self.budget > 0:\n            \n            if self.check_stagnation() and self.budget > self.pop_size:\n                self.restart_population(func)\n                self.f_opt = np.min(self.fitness)\n                self.x_opt = self.population[np.argmin(self.fitness)].copy()\n\n            for i in range(self.pop_size):\n                #Parameter Adaptation\n                if np.random.rand() < self.p and len(self.archive)>0:\n                     idx = np.random.randint(len(self.archive))\n                     F, CR = self.archive[idx]\n                else:\n                     F = self.F\n                     CR = self.CR\n\n                x_mutated = self.mutate(i)\n                x_trial = self.crossover(x_mutated, i)\n                x_trial = np.clip(x_trial, func.bounds.lb, func.bounds.ub)\n\n                f_trial = func(x_trial)\n                self.budget -= 1\n\n                if self.budget <= 0:\n                    break\n\n                if f_trial < self.fitness[i]:\n                    self.update_parameters(True, F, CR)\n                    self.population[i] = x_trial.copy()\n                    self.fitness[i] = f_trial\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = x_trial.copy()\n                else:\n                    self.update_parameters(False, F, CR)\n\n            if self.budget <= 0:\n                break\n\n            best_index = np.argmin(self.fitness)\n            if self.fitness[best_index] < self.f_opt:\n                self.f_opt = self.fitness[best_index]\n                self.x_opt = self.population[best_index].copy()\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "The algorithm SimplifiedAdaptiveDE scored 0.765 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["3c377e79-26ed-41cf-8828-7863aaef3453"], "operator": null, "metadata": {"aucs": [0.3891613136182619, 0.7661380391684925, 0.8028475112927218, 0.910234875289394, 0.8171713575431707, 0.8531617319074285, 0.7259025970883228, 0.7618479980963386, 0.8266197836951431, 0.762130605303694, 0.9094153715520856, 0.9961859161529406, 0.7801146763852714, 0.8034716463193814, 0.9460521013776305, 0.839603518618509, 0.7519917719105897, 0.891640288082263, 0.24018960861665006, 0.5189157642885809]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "9bfc7670-142f-4e52-831e-44b1953ac659", "fitness": 0.7828624016356106, "name": "AdaptiveDE", "description": "Self-adaptive Differential Evolution with a diversity-enhanced mutation strategy, archive utilization and periodic population rejuvenation for improved exploration and exploitation.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=5, f_base=0.5, cr_base=0.9, diversity_threshold=1e-5, rejuvenation_rate=0.05):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.population = None\n        self.fitness = None\n        self.archive = []\n        self.memory_f = [f_base] * archive_size\n        self.memory_cr = [cr_base] * archive_size\n        self.sf = []\n        self.scr = []\n        self.diversity_threshold = diversity_threshold\n        self.rejuvenation_rate = rejuvenation_rate\n        self.f_base = f_base\n        self.cr_base = cr_base\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        self.best_idx = np.argmin(self.fitness)\n        self.best_fitness = self.fitness[self.best_idx]\n        self.best_solution = self.population[self.best_idx].copy()\n\n\n    def mutate(self, i):\n        phi_idx = np.random.randint(0, self.archive_size)\n        F = np.clip(np.random.normal(self.memory_f[phi_idx], 0.1), 0.1, 1.0)\n        CR = np.clip(np.random.normal(self.memory_cr[phi_idx], 0.1), 0.0, 1.0)\n\n\n        idxs = np.random.choice(self.pop_size, 3, replace=False)\n        a, b, c = idxs[0], idxs[1], idxs[2]\n\n        # Diversity-enhanced mutation using best solution\n        x_mutated = self.population[i] + F * (self.best_solution - self.population[i]) + F * (self.population[a] - self.population[b])\n\n\n        return x_mutated, F, CR\n\n    def crossover(self, x_mutated, i, cr):\n        x_trial = np.copy(self.population[i])\n        j_rand = np.random.randint(self.dim)\n        for j in range(self.dim):\n            if np.random.rand() < cr or j == j_rand:\n                x_trial[j] = x_mutated[j]\n        return x_trial\n\n    def repair(self, x_trial, func):\n        x_trial = np.clip(x_trial, func.bounds.lb, func.bounds.ub)\n        return x_trial\n\n    def update_memory(self):\n        if self.sf:\n            self.memory_f.append(sum(val**2 for val in self.sf) / sum(self.sf) if sum(self.sf) > 0 else self.f_base)\n            self.memory_f.pop(0)\n            self.memory_cr.append(np.mean(self.scr))\n            self.memory_cr.pop(0)\n        self.sf = []\n        self.scr = []\n\n    def rejuvenate_population(self, func):\n        # Rejuvenate a portion of the population to enhance exploration\n        num_to_rejuvenate = int(self.rejuvenation_rate * self.pop_size)\n        indices_to_rejuvenate = np.random.choice(self.pop_size, num_to_rejuvenate, replace=False)\n        for i in indices_to_rejuvenate:\n            self.population[i] = np.random.uniform(func.bounds.lb, func.bounds.ub)\n            self.fitness[i] = func(self.population[i])\n            self.budget -= 1\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.budget > 0:\n            self.sf = []\n            self.scr = []\n            for i in range(self.pop_size):\n                x_mutated, F, CR = self.mutate(i)\n                x_trial = self.crossover(x_mutated, i, CR)\n                x_trial = self.repair(x_trial, func)\n\n                f_trial = func(x_trial)\n                self.budget -= 1\n                if self.budget <= 0:\n                    break\n\n                if f_trial < self.fitness[i]:\n                    self.sf.append(F)\n                    self.scr.append(CR)\n\n                    # Archive update\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(self.population[i].copy())\n                    else:\n                        j = np.random.randint(0, self.archive_size)\n                        self.archive[j] = self.population[i].copy()\n\n                    self.population[i] = x_trial.copy()\n                    self.fitness[i] = f_trial\n\n                    if f_trial < self.best_fitness:\n                        self.best_fitness = f_trial\n                        self.best_solution = x_trial.copy()\n                        self.best_idx = i\n\n            if self.budget <= 0:\n                break\n\n            self.update_memory()\n\n            # Population rejuvenation based on diversity\n            if np.std(self.fitness) < self.diversity_threshold:\n                self.rejuvenate_population(func)\n                self.best_idx = np.argmin(self.fitness)\n                self.best_fitness = self.fitness[self.best_idx]\n                self.best_solution = self.population[self.best_idx].copy()\n\n\n\n\n        return self.best_fitness, self.best_solution", "configspace": "", "generation": 8, "feedback": "The algorithm AdaptiveDE scored 0.783 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["73551b59-70ed-4fbd-a6e6-cc84bc34a85c"], "operator": null, "metadata": {"aucs": [0.519808611200326, 0.8538070217320688, 0.8366210100303997, 0.9301704936144072, 0.8632591802516942, 0.8912368239884677, 0.8010374244571256, 0.825384669824927, 0.866544912789938, 0.354843011294391, 0.932923753153889, 0.9917104577008555, 0.8474664976589805, 0.8291664453736903, 0.9594819597063635, 0.8685425241155644, 0.801045263441947, 0.9219257155972624, 0.25430484410651033, 0.507967412673404]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "ca85b7ad-0f37-47f9-9049-9648e4f1e39a", "fitness": 0.8025498787083164, "name": "EnhancedAdaptiveDE", "description": "Enhanced Adaptive Differential Evolution with dynamic population sizing, archive-based learning, and a novel mutation strategy incorporating information from top-performing solutions to accelerate convergence.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=10, stagnation_threshold=50):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.stagnation_threshold = stagnation_threshold\n        self.stagnation_counter = 0\n        self.best_fitness_history = []\n        self.pop_factor = 1.0\n        self.archive = []\n        self.memory_f = [0.5] * archive_size\n        self.memory_cr = [0.9] * archive_size\n        self.sf = []\n        self.scr = []\n        self.population = None\n        self.fitness = None\n\n    def initialize_population(self, func):\n        current_pop_size = int(self.pop_size * self.pop_factor)\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(current_pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= current_pop_size\n\n    def mutate(self, i):\n        current_pop_size = int(self.pop_size * self.pop_factor)\n        phi_idx = np.random.randint(0, self.archive_size)\n        F = np.clip(np.random.normal(self.memory_f[phi_idx], 0.1), 0.1, 1.0)\n        CR = np.clip(np.random.normal(self.memory_cr[phi_idx], 0.1), 0.0, 1.0)\n\n        idxs = np.random.choice(current_pop_size, 3, replace=False)\n        a, b, c = idxs[0], idxs[1], idxs[2]\n\n        # Incorporate information from the top 20% performing solutions\n        top_indices = np.argsort(self.fitness)[:max(1, int(0.2 * current_pop_size))]\n        p_best_idx = np.random.choice(top_indices)\n\n        x_mutated = self.population[i] + F * (self.population[p_best_idx] - self.population[i]) + F * (self.population[a] - self.population[b])\n        return x_mutated, F, CR\n\n    def crossover(self, x_mutated, i, cr):\n        x_trial = np.copy(self.population[i])\n        j_rand = np.random.randint(self.dim)\n        for j in range(self.dim):\n            if np.random.rand() < cr or j == j_rand:\n                x_trial[j] = x_mutated[j]\n        return x_trial\n\n    def update_memory(self):\n        if self.sf:\n            self.memory_f.append(sum(val**2 for val in self.sf) / sum(self.sf) if sum(self.sf) != 0 else self.memory_f[-1])\n            self.memory_f.pop(0)\n            self.memory_cr.append(np.mean(self.scr))\n            self.memory_cr.pop(0)\n        self.sf = []\n        self.scr = []\n\n    def check_stagnation(self):\n        if len(self.best_fitness_history) > self.stagnation_threshold:\n            if np.std(self.best_fitness_history[-self.stagnation_threshold:]) < 1e-6:\n                self.stagnation_counter += 1\n                return True\n            else:\n                self.stagnation_counter = 0\n                return False\n        return False\n\n    def restart_population(self, func):\n        self.pop_factor *= 0.75  # Reduce population size\n        self.pop_factor = max(0.2, self.pop_factor) # Prevent too small pop size\n        self.initialize_population(func)\n        self.best_fitness_history = []\n        self.memory_f = [0.5] * self.archive_size\n        self.memory_cr = [0.9] * self.archive_size\n        self.sf = []\n        self.scr = []\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)].copy()\n        self.stagnation_counter = 0\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)].copy()\n        self.sf = []\n        self.scr = []\n\n        while self.budget > 0:\n            current_pop_size = int(self.pop_size * self.pop_factor)\n            for i in range(current_pop_size):\n                x_mutated, F, CR = self.mutate(i)\n                x_trial = self.crossover(x_mutated, i, CR)\n                x_trial = np.clip(x_trial, func.bounds.lb, func.bounds.ub)\n                f_trial = func(x_trial)\n                self.budget -= 1\n                if self.budget <= 0:\n                    break\n\n                if f_trial < self.fitness[i]:\n                    self.sf.append(F)\n                    self.scr.append(CR)\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(self.population[i].copy())\n                    else:\n                        j = np.random.randint(0, self.archive_size)\n                        self.archive[j] = self.population[i].copy()\n                    self.population[i] = x_trial.copy()\n                    self.fitness[i] = f_trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = x_trial.copy()\n\n            if self.budget <= 0:\n                break\n\n            self.update_memory()\n            best_index = np.argmin(self.fitness)\n            if self.fitness[best_index] < self.f_opt:\n                self.f_opt = self.fitness[best_index]\n                self.x_opt = self.population[best_index].copy()\n\n            self.best_fitness_history.append(self.f_opt)\n\n            if self.check_stagnation() and self.budget > int(self.pop_size * self.pop_factor) * 2:\n                self.restart_population(func)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "The algorithm EnhancedAdaptiveDE scored 0.803 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["b1035a21-7f93-411a-8c32-f80b40512d34"], "operator": null, "metadata": {"aucs": [0.3116433221531254, 0.8207972787351255, 0.8305635780657511, 0.9286556487883807, 0.8668568979116084, 0.8785062374656882, 0.7587164065316929, 0.8098526238474882, 0.8469187082545884, 0.8036111322908566, 0.9328772755741994, 0.9888171212864405, 0.8343738563336566, 0.8436764984231312, 0.9432946066620125, 0.8873486305527587, 0.7457242290666846, 0.9152551086299031, 0.588100778873019, 0.5154076347202161]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "b7a5451b-c1d9-4a0d-b5c9-878cbd17337f", "fitness": "-inf", "name": "OrthogonalAdaptiveDE", "description": "A self-adaptive Differential Evolution with orthogonal learning, archive of promising solutions, and a restart mechanism based on stagnation detection to enhance exploration and exploitation.", "code": "import numpy as np\n\nclass OrthogonalAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=5, f_base=0.5, cr_base=0.9, stagnation_threshold=1e-6, restart_trigger=50, orthogonal_learning_size=5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.population = None\n        self.fitness = None\n        self.archive = []\n        self.memory_f = [f_base] * archive_size\n        self.memory_cr = [cr_base] * archive_size\n        self.sf = []\n        self.scr = []\n        self.stagnation_threshold = stagnation_threshold\n        self.restart_trigger = restart_trigger\n        self.f_base = f_base\n        self.cr_base = cr_base\n        self.orthogonal_learning_size = orthogonal_learning_size\n        self.last_improvement = 0\n        self.generation = 0\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        self.best_idx = np.argmin(self.fitness)\n        self.best_fitness = self.fitness[self.best_idx]\n        self.best_solution = self.population[self.best_idx].copy()\n\n    def mutate(self, i):\n        phi_idx = np.random.randint(0, self.archive_size)\n        F = np.clip(np.random.normal(self.memory_f[phi_idx], 0.1), 0.1, 1.0)\n        CR = np.clip(np.random.normal(self.memory_cr[phi_idx], 0.1), 0.0, 1.0)\n\n        idxs = np.random.choice(self.pop_size, 3, replace=False)\n        a, b, c = idxs[0], idxs[1], idxs[2]\n\n        x_mutated = self.population[i] + F * (self.best_solution - self.population[i]) + F * (self.population[a] - self.population[b])\n        return x_mutated, F, CR\n\n    def crossover(self, x_mutated, i, cr):\n        x_trial = np.copy(self.population[i])\n        j_rand = np.random.randint(self.dim)\n        for j in range(self.dim):\n            if np.random.rand() < cr or j == j_rand:\n                x_trial[j] = x_mutated[j]\n        return x_trial\n\n    def repair(self, x_trial, func):\n        x_trial = np.clip(x_trial, func.bounds.lb, func.bounds.ub)\n        return x_trial\n\n    def update_memory(self):\n        if self.sf:\n            self.memory_f.append(sum(val**2 for val in self.sf) / sum(self.sf) if sum(self.sf) > 0 else self.f_base)\n            self.memory_f.pop(0)\n            self.memory_cr.append(np.mean(self.scr))\n            self.memory_cr.pop(0)\n        self.sf = []\n        self.scr = []\n\n    def orthogonal_learning(self, func):\n        # Select a subset of individuals for orthogonal learning\n        indices = np.random.choice(self.pop_size, self.orthogonal_learning_size, replace=False)\n        sample = self.population[indices]\n\n        # Generate orthogonal array (simplified for demonstration)\n        levels = 2\n        factors = self.dim\n        orthogonal_matrix = np.random.randint(0, levels, size=(levels**2, factors))  # Very simple OA, replace with real OA\n\n        # Evaluate all combinations defined by orthogonal array\n        trial_solutions = []\n        trial_fitness = []\n        for row in orthogonal_matrix:\n            trial = np.copy(self.best_solution) # Start from the best\n            for j in range(self.dim):\n                if row[j] == 1:  # Example: if level is 1, perturb dimension j\n                    trial[j] = np.random.uniform(func.bounds.lb, func.bounds.ub) #Random perturbation\n            trial = self.repair(trial, func)\n            trial_solutions.append(trial)\n            trial_fitness.append(func(trial))\n            self.budget -= 1\n            if self.budget <= 0:\n                return\n\n        # Update population with the best solution found during orthogonal learning\n        best_ol_idx = np.argmin(trial_fitness)\n        best_ol_solution = trial_solutions[best_ol_idx]\n        best_ol_fitness = trial_fitness[best_ol_idx]\n\n        if best_ol_fitness < self.best_fitness:\n            self.best_fitness = best_ol_fitness\n            self.best_solution = best_ol_solution.copy()\n            self.population[indices[0]] = best_ol_solution.copy() # Replace a random individual with the OL solution\n            self.fitness[indices[0]] = best_ol_fitness\n\n    def restart_population(self, func):\n         # Re-initialize the population\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        self.best_idx = np.argmin(self.fitness)\n        self.best_fitness = self.fitness[self.best_idx]\n        self.best_solution = self.population[self.best_idx].copy()\n        self.last_improvement = self.generation\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        self.last_improvement = 0\n        self.generation = 0\n\n        while self.budget > 0:\n            self.sf = []\n            self.scr = []\n            for i in range(self.pop_size):\n                x_mutated, F, CR = self.mutate(i)\n                x_trial = self.crossover(x_mutated, i, CR)\n                x_trial = self.repair(x_trial, func)\n\n                f_trial = func(x_trial)\n                self.budget -= 1\n                if self.budget <= 0:\n                    break\n\n                if f_trial < self.fitness[i]:\n                    self.sf.append(F)\n                    self.scr.append(CR)\n\n                    # Archive update\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(self.population[i].copy())\n                    else:\n                        j = np.random.randint(0, self.archive_size)\n                        self.archive[j] = self.population[i].copy()\n\n                    self.population[i] = x_trial.copy()\n                    self.fitness[i] = f_trial\n\n                    if f_trial < self.best_fitness:\n                        self.best_fitness = f_trial\n                        self.best_solution = x_trial.copy()\n                        self.best_idx = i\n                        self.last_improvement = self.generation\n\n            if self.budget <= 0:\n                break\n\n            self.update_memory()\n            self.orthogonal_learning(func)\n\n            self.generation += 1\n            if self.generation - self.last_improvement > self.restart_trigger:\n                self.restart_population(func)\n\n\n\n        return self.best_fitness, self.best_solution", "configspace": "", "generation": 9, "feedback": "In the code, line 81, in orthogonal_learning, the following error occurred:\nValueError: setting an array element with a sequence.\nOn line: trial[j] = np.random.uniform(func.bounds.lb, func.bounds.ub) #Random perturbation", "error": "In the code, line 81, in orthogonal_learning, the following error occurred:\nValueError: setting an array element with a sequence.\nOn line: trial[j] = np.random.uniform(func.bounds.lb, func.bounds.ub) #Random perturbation", "parent_ids": ["9bfc7670-142f-4e52-831e-44b1953ac659"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "c27ac343-5a6d-43b5-ba94-da84902fa3f1", "fitness": 0.7615844198143221, "name": "HybridDEGLS", "description": "Hybrid Differential Evolution with Gaussian Local Search, dynamically balancing exploration and exploitation based on success rates.", "code": "import numpy as np\n\nclass HybridDEGLS:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=10, ls_prob=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.ls_prob = ls_prob # Probability of performing local search\n        self.population = None\n        self.fitness = None\n        self.memory_F = np.ones(self.archive_size) * 0.5\n        self.memory_CR = np.ones(self.archive_size) * 0.9\n        self.archive = []\n        self.F = 0.5\n        self.CR = 0.9\n        self.p = 0.1  # Probability for choosing F and CR from memory\n        self.min_CR = 0.1\n        self.stagnation_counter = 0\n        self.max_stagnation = 50 #restart if stagnant for longer\n        self.success_rate = 0.0\n        self.success_history = []\n        self.success_window = 10\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n\n    def mutate(self, i):\n        # Combined mutation: DE/rand/1 OR DE/current-to-best/1\n        if np.random.rand() < 0.5:  # Switch between strategies\n            idxs = np.random.choice(self.pop_size, 5, replace=False)\n            a, b, c, d, e = idxs\n            x_mutated = self.population[a] + self.F * (self.population[b] - self.population[c]) + self.F * (self.population[d] - self.population[e])\n        else:\n            best_idx = np.argmin(self.fitness)\n            idxs = np.random.choice(self.pop_size, 3, replace=False)\n            a, b, c = idxs\n            x_mutated = self.population[i] + self.F * (self.population[best_idx] - self.population[i]) + self.F * (self.population[a] - self.population[b])\n\n        return x_mutated\n\n    def crossover(self, x_mutated, i):\n        x_trial = np.copy(self.population[i])\n        j_rand = np.random.randint(self.dim)\n        for j in range(self.dim):\n            if np.random.rand() < self.CR or j == j_rand:\n                x_trial[j] = x_mutated[j]\n        return x_trial\n    \n    def gaussian_local_search(self, x, func):\n        x_ls = x + np.random.normal(0, 0.1, size=self.dim)  # Gaussian perturbation\n        x_ls = np.clip(x_ls, func.bounds.lb, func.bounds.ub)\n        return x_ls\n    \n\n    def update_parameters(self, success, F, CR):\n         if success:\n            self.archive.append((F, CR))\n            if len(self.archive) > self.archive_size:\n                self.archive.pop(0)\n\n            if len(self.archive) > 0:\n                F_vals = [item[0] for item in self.archive]\n                CR_vals = [item[1] for item in self.archive]\n                self.F = np.mean(F_vals)\n                self.CR = max(self.min_CR, np.mean(CR_vals)) # Ensure CR doesn't get too small\n\n            self.success_history.append(1)\n         else:\n            self.success_history.append(0)\n\n         if len(self.success_history) > self.success_window:\n             self.success_history.pop(0)\n         \n         self.success_rate = np.mean(self.success_history)\n\n\n    def check_stagnation(self):\n        if len(set(self.fitness)) <= 1:\n            self.stagnation_counter +=1\n        else:\n            self.stagnation_counter = 0\n\n        if self.stagnation_counter > self.max_stagnation:\n            return True\n        else:\n            return False\n\n    def restart_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        self.stagnation_counter = 0\n        self.archive = []  # Clear the archive on restart\n        self.success_history = []\n        self.success_rate = 0.0\n\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)].copy()\n\n        while self.budget > 0:\n            \n            if self.check_stagnation() and self.budget > self.pop_size:\n                self.restart_population(func)\n                self.f_opt = np.min(self.fitness)\n                self.x_opt = self.population[np.argmin(self.fitness)].copy()\n\n            for i in range(self.pop_size):\n                #Parameter Adaptation\n                if np.random.rand() < self.p and len(self.archive)>0:\n                     idx = np.random.randint(len(self.archive))\n                     F, CR = self.archive[idx]\n                else:\n                     F = self.F\n                     CR = self.CR\n\n                x_mutated = self.mutate(i)\n                x_trial = self.crossover(x_mutated, i)\n                x_trial = np.clip(x_trial, func.bounds.lb, func.bounds.ub)\n\n                # Local Search with dynamic probability based on success rate\n                if np.random.rand() < self.ls_prob * (1 - self.success_rate):\n                    x_trial = self.gaussian_local_search(x_trial, func)\n                    x_trial = np.clip(x_trial, func.bounds.lb, func.bounds.ub)\n\n                f_trial = func(x_trial)\n                self.budget -= 1\n\n                if self.budget <= 0:\n                    break\n\n                if f_trial < self.fitness[i]:\n                    self.update_parameters(True, F, CR)\n                    self.population[i] = x_trial.copy()\n                    self.fitness[i] = f_trial\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = x_trial.copy()\n                else:\n                    self.update_parameters(False, F, CR)\n\n            if self.budget <= 0:\n                break\n\n            best_index = np.argmin(self.fitness)\n            if self.fitness[best_index] < self.f_opt:\n                self.f_opt = self.fitness[best_index]\n                self.x_opt = self.population[best_index].copy()\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 9, "feedback": "The algorithm HybridDEGLS scored 0.762 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["959042e0-c7fd-4f7d-91da-a41fdc341f98"], "operator": null, "metadata": {"aucs": [0.38503327045824176, 0.79306684488791, 0.7883565232855363, 0.9101897108993727, 0.8081339011128152, 0.8585405055153081, 0.684586261973873, 0.7601711573893615, 0.8213103372533942, 0.7541703244316198, 0.8952597577618763, 0.9961744948127846, 0.7668812316785809, 0.7971407506608942, 0.9395040668383761, 0.8223945170059969, 0.7326769006215974, 0.8832984658035664, 0.32379238737553984, 0.5110069865197938]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "324f3604-eedf-4a73-ad39-97749c1ae21d", "fitness": "-inf", "name": "HybridAdaptiveDE", "description": "A hybrid DE algorithm with a dynamically adjusted ensemble of mutation strategies and a self-adjusting local search intensity based on population diversity.", "code": "import numpy as np\n\nclass HybridAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, num_strategies=4, initial_local_search_probability=0.1, local_search_decay=0.995):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.num_strategies = num_strategies\n        self.local_search_probability = initial_local_search_probability\n        self.local_search_decay = local_search_decay\n        self.population = None\n        self.fitness = None\n        self.strategy_successes = np.zeros(num_strategies)\n        self.strategy_counts = np.zeros(num_strategies)\n        self.strategy_weights = np.ones(num_strategies) / num_strategies\n        self.F = 0.5\n        self.CR = 0.9\n        self.archive = []\n        self.archive_fitness = []\n        self.archive_size = pop_size  # Archive size is same as population size\n\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n\n    def select_strategy(self):\n        return np.random.choice(self.num_strategies, p=self.strategy_weights)\n\n    def mutate(self, strategy, i):\n        if strategy == 0:  # DE/rand/1\n            idxs = np.random.choice(self.pop_size, 5, replace=False)\n            a, b, c, d, e = idxs\n            x_mutated = self.population[a] + self.F * (self.population[b] - self.population[c]) + self.F * (self.population[d] - self.population[e])\n        elif strategy == 1:  # DE/current-to-best/1\n            best_idx = np.argmin(self.fitness)\n            idxs = np.random.choice(self.pop_size, 3, replace=False)\n            a, b, c = idxs\n            x_mutated = self.population[i] + self.F * (self.population[best_idx] - self.population[i]) + self.F * (self.population[a] - self.population[b])\n        elif strategy == 2:  # DE/best/2\n             best_idx = np.argmin(self.fitness)\n             idxs = np.random.choice(self.pop_size, 4, replace=False)\n             a, b, c, d = idxs\n             x_mutated = self.population[best_idx] + self.F * (self.population[a] - self.population[b]) + self.F * (self.population[c] - self.population[d])\n        elif strategy == 3: # DE/rand-to-archive/1\n            idxs = np.random.choice(self.pop_size, 3, replace=False)\n            a, b, c = idxs\n            if len(self.archive) > 0:\n                arch_idx = np.random.randint(len(self.archive))\n                x_mutated = self.population[a] + self.F * (self.archive[arch_idx] - self.population[b]) + self.F * (self.population[c] - self.population[a])\n            else:\n                x_mutated = self.population[a] + self.F * (self.population[b] - self.population[c]) # Fallback to rand/1 if archive is empty\n\n        return x_mutated\n\n    def crossover(self, x_mutated, i):\n        x_trial = np.copy(self.population[i])\n        j_rand = np.random.randint(self.dim)\n        for j in range(self.dim):\n            if np.random.rand() < self.CR or j == j_rand:\n                x_trial[j] = x_mutated[j]\n        return x_trial\n\n    def local_search(self, x, func):\n        # Apply Gaussian local search around the current solution\n        x_ls = x + np.random.normal(0, 0.01, self.dim)  # Small perturbation\n        x_ls = np.clip(x_ls, func.bounds.lb, func.bounds.ub)\n        return x_ls\n\n    def update_strategy_weights(self):\n        total_successes = np.sum(self.strategy_successes)\n        if total_successes > 0:\n            self.strategy_weights = self.strategy_successes / total_successes\n        else:\n            self.strategy_weights = np.ones(self.num_strategies) / self.num_strategies\n\n        # Smoothing the weights to avoid premature convergence\n        self.strategy_weights = 0.9 * self.strategy_weights + 0.1 / self.num_strategies\n        self.strategy_weights /= np.sum(self.strategy_weights)\n\n    def update_archive(self):\n        # Combine population and archive, then select the best\n        combined_population = np.concatenate((self.population, np.array(self.archive) if self.archive else self.population[:0]))  # Handle empty archive case\n        combined_fitness = np.concatenate((self.fitness, np.array(self.archive_fitness) if self.archive_fitness else self.fitness[:0]))\n\n        sorted_indices = np.argsort(combined_fitness)\n        \n        self.archive = combined_population[sorted_indices[:self.archive_size]].tolist()\n        self.archive_fitness = combined_fitness[sorted_indices[:self.archive_size]].tolist()\n\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)].copy()\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                strategy = self.select_strategy()\n                self.strategy_counts[strategy] += 1\n\n                x_mutated = self.mutate(strategy, i)\n                x_trial = self.crossover(x_mutated, i)\n                x_trial = np.clip(x_trial, func.bounds.lb, func.bounds.ub)\n\n                # Local Search\n                if np.random.rand() < self.local_search_probability:\n                    x_trial = self.local_search(x_trial, func)\n\n                f_trial = func(x_trial)\n                self.budget -= 1\n\n                if self.budget <= 0:\n                    break\n\n                if f_trial < self.fitness[i]:\n                    self.strategy_successes[strategy] += 1\n                    self.population[i] = x_trial.copy()\n                    self.fitness[i] = f_trial\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = x_trial.copy()\n\n            if self.budget <= 0:\n                break\n            \n            self.update_strategy_weights()\n            self.update_archive()\n\n            # Adjust local search probability based on population diversity\n            pop_std = np.std(self.fitness)  # Simple measure of diversity\n            if pop_std < 1e-3:  # Reduced local search when population converges\n                self.local_search_probability *= self.local_search_decay\n            else:\n                self.local_search_probability = min(0.2, self.local_search_probability * 1.01)  # Increase if diverse\n\n            best_index = np.argmin(self.fitness)\n            if self.fitness[best_index] < self.f_opt:\n                self.f_opt = self.fitness[best_index]\n                self.x_opt = self.population[best_index].copy()\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 9, "feedback": "Evaluation timed out after 300 seconds.", "error": "Evaluation timed out after 300 seconds.", "parent_ids": ["3c377e79-26ed-41cf-8828-7863aaef3453"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "83d625b1-9a2a-4615-8af7-afb9a918fa35", "fitness": 0.5755053856593016, "name": "SimplifiedAdaptiveDE", "description": "A simplified Differential Evolution with self-adaptive mutation strategies, parameter control, and periodic population perturbation to maintain diversity and enhance exploration-exploitation balance.", "code": "import numpy as np\n\nclass SimplifiedAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.population = None\n        self.fitness = None\n        self.archive = None\n        self.archive_fitness = None\n        self.F = 0.5\n        self.CR = 0.9\n        self.success_F = 0.5 * np.ones(archive_size)\n        self.success_CR = 0.9 * np.ones(archive_size)\n        self.success_history_idx = 0\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        self.archive = np.zeros((self.archive_size, self.dim))\n        self.archive_fitness = np.full(self.archive_size, np.inf)\n\n    def mutate(self, i):\n        idxs = np.random.choice(self.pop_size, 3, replace=False)\n        a, b, c = idxs\n        x_mutated = self.population[a] + self.F * (self.population[b] - self.population[c])\n        return x_mutated\n\n    def crossover(self, x_mutated, i):\n        x_trial = np.copy(self.population[i])\n        j_rand = np.random.randint(self.dim)\n        for j in range(self.dim):\n            if np.random.rand() < self.CR or j == j_rand:\n                x_trial[j] = x_mutated[j]\n        return x_trial\n\n    def update_archive(self, x, f):\n        if f < np.max(self.archive_fitness):\n            worst_idx = np.argmax(self.archive_fitness)\n            self.archive[worst_idx] = x\n            self.archive_fitness[worst_idx] = f\n\n    def update_parameters(self, success, F, CR):\n        self.success_F[self.success_history_idx] = F\n        self.success_CR[self.success_history_idx] = CR\n        self.success_history_idx = (self.success_history_idx + 1) % self.archive_size\n\n        self.F = np.mean(self.success_F)\n        self.CR = np.mean(self.success_CR)\n        self.F = np.clip(self.F, 0.1, 0.9)\n        self.CR = np.clip(self.CR, 0.1, 0.9)\n        if np.random.rand() < 0.1:\n            self.F = np.random.uniform(0.1, 0.9)\n            self.CR = np.random.uniform(0.1, 0.9)\n    def perturb_population(self, func):\n        # Periodically introduce small perturbations to the population\n        for i in range(self.pop_size):\n            if np.random.rand() < 0.05:  # 5% chance of perturbation\n                self.population[i] = np.random.uniform(func.bounds.lb, func.bounds.ub, self.dim)\n                self.fitness[i] = func(self.population[i])\n                self.budget -= 1\n                if self.budget <= 0:\n                    break\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)].copy()\n\n        generation = 0\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                F = self.F\n                CR = self.CR\n                x_mutated = self.mutate(i)\n                x_trial = self.crossover(x_mutated, i)\n                x_trial = np.clip(x_trial, func.bounds.lb, func.bounds.ub)\n\n                f_trial = func(x_trial)\n                self.budget -= 1\n\n                if self.budget <= 0:\n                    break\n\n                if f_trial < self.fitness[i]:\n                    self.update_parameters(True, F, CR)\n                    self.population[i] = x_trial.copy()\n                    self.fitness[i] = f_trial\n                    self.update_archive(x_trial, f_trial)\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = x_trial.copy()\n                else:\n                     self.update_parameters(False, F, CR)\n\n            if self.budget <= 0:\n                break\n\n            best_index = np.argmin(self.fitness)\n            if self.fitness[best_index] < self.f_opt:\n                self.f_opt = self.fitness[best_index]\n                self.x_opt = self.population[best_index].copy()\n\n            generation += 1\n            if generation % 10 == 0:\n                self.perturb_population(func)\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 9, "feedback": "The algorithm SimplifiedAdaptiveDE scored 0.576 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["3c377e79-26ed-41cf-8828-7863aaef3453"], "operator": null, "metadata": {"aucs": [0.20962546693810313, 0.37332435948427267, 0.5084408051654592, 0.7795636762458968, 0.6315233817070665, 0.727068948853473, 0.4823154604373324, 0.5311258407162167, 0.6502852258723786, 0.4733530402092291, 0.7612397345848562, 0.9915359909487304, 0.2660131618148317, 0.5646484983121689, 0.8226041043925161, 0.7504311303915849, 0.46878246184662165, 0.7848476970736116, 0.21688860555094136, 0.5164901226407412]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "34489361-82ce-4c87-8884-6f0bd0990079", "fitness": 0.41992476750328855, "name": "OrthogonalAdaptiveDE", "description": "An Adaptive Differential Evolution with orthogonal learning and a self-regulating strategy for population diversity and parameter control.", "code": "import numpy as np\n\nclass OrthogonalAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=10, stagnation_threshold=50, orthogonal_components=5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.stagnation_threshold = stagnation_threshold\n        self.stagnation_counter = 0\n        self.best_fitness_history = []\n        self.pop_factor = 1.0\n        self.archive = []\n        self.memory_f = [0.5] * archive_size\n        self.memory_cr = [0.9] * archive_size\n        self.sf = []\n        self.scr = []\n        self.population = None\n        self.fitness = None\n        self.orthogonal_components = orthogonal_components\n\n    def initialize_population(self, func):\n        current_pop_size = int(self.pop_size * self.pop_factor)\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(current_pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= current_pop_size\n\n    def mutate(self, i):\n        current_pop_size = int(self.pop_size * self.pop_factor)\n        phi_idx = np.random.randint(0, self.archive_size)\n        F = np.clip(np.random.normal(self.memory_f[phi_idx], 0.1), 0.1, 1.0)\n        CR = np.clip(np.random.normal(self.memory_cr[phi_idx], 0.1), 0.0, 1.0)\n\n        idxs = np.random.choice(current_pop_size, 3, replace=False)\n        a, b, c = idxs[0], idxs[1], idxs[2]\n\n        # Orthogonal learning component\n        orthogonal_vector = np.zeros(self.dim)\n        for _ in range(self.orthogonal_components):\n            j = np.random.randint(0, self.dim)\n            orthogonal_vector[j] = np.random.uniform(-1, 1)  # Random orthogonal direction\n\n        x_mutated = self.population[i] + F * (self.population[a] - self.population[b]) + 0.01 * orthogonal_vector\n        return x_mutated, F, CR\n\n    def crossover(self, x_mutated, i, cr):\n        x_trial = np.copy(self.population[i])\n        j_rand = np.random.randint(self.dim)\n        for j in range(self.dim):\n            if np.random.rand() < cr or j == j_rand:\n                x_trial[j] = x_mutated[j]\n        return x_trial\n\n    def update_memory(self):\n        if self.sf:\n            self.memory_f.append(sum(val**2 for val in self.sf) / sum(self.sf) if sum(self.sf) != 0 else self.memory_f[-1])\n            self.memory_f.pop(0)\n            self.memory_cr.append(np.mean(self.scr))\n            self.memory_cr.pop(0)\n        self.sf = []\n        self.scr = []\n\n    def check_stagnation(self):\n        if len(self.best_fitness_history) > self.stagnation_threshold:\n            if np.std(self.best_fitness_history[-self.stagnation_threshold:]) < 1e-6:\n                self.stagnation_counter += 1\n                return True\n            else:\n                self.stagnation_counter = 0\n                return False\n        return False\n\n    def restart_population(self, func):\n        self.pop_factor *= 0.75  # Reduce population size\n        self.pop_factor = max(0.2, self.pop_factor) # Prevent too small pop size\n        self.initialize_population(func)\n        self.best_fitness_history = []\n        self.memory_f = [0.5] * self.archive_size\n        self.memory_cr = [0.9] * self.archive_size\n        self.sf = []\n        self.scr = []\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)].copy()\n        self.stagnation_counter = 0\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)].copy()\n        self.sf = []\n        self.scr = []\n\n        while self.budget > 0:\n            current_pop_size = int(self.pop_size * self.pop_factor)\n            for i in range(current_pop_size):\n                x_mutated, F, CR = self.mutate(i)\n                x_trial = self.crossover(x_mutated, i, CR)\n                x_trial = np.clip(x_trial, func.bounds.lb, func.bounds.ub)\n                f_trial = func(x_trial)\n                self.budget -= 1\n                if self.budget <= 0:\n                    break\n\n                if f_trial < self.fitness[i]:\n                    self.sf.append(F)\n                    self.scr.append(CR)\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(self.population[i].copy())\n                    else:\n                        j = np.random.randint(0, self.archive_size)\n                        self.archive[j] = self.population[i].copy()\n                    self.population[i] = x_trial.copy()\n                    self.fitness[i] = f_trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = x_trial.copy()\n\n            if self.budget <= 0:\n                break\n\n            self.update_memory()\n            best_index = np.argmin(self.fitness)\n            if self.fitness[best_index] < self.f_opt:\n                self.f_opt = self.fitness[best_index]\n                self.x_opt = self.population[best_index].copy()\n\n            self.best_fitness_history.append(self.f_opt)\n\n            if self.check_stagnation() and self.budget > int(self.pop_size * self.pop_factor) * 2:\n                self.restart_population(func)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 9, "feedback": "The algorithm OrthogonalAdaptiveDE scored 0.420 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["ca85b7ad-0f37-47f9-9049-9648e4f1e39a"], "operator": null, "metadata": {"aucs": [0.16489079349257807, 0.3114703165469276, 0.37448226181906763, 0.5929056819118514, 0.31265316610023597, 0.4253039048369891, 0.3018577904901907, 0.3520148111892628, 0.31297197389419273, 0.20718036936517903, 0.6508316088951305, 0.997996335563317, 0.3526860371110182, 0.3295381646256377, 0.7357347393560225, 0.4511141666469922, 0.3278826247963721, 0.5269828203668435, 0.19180509454536332, 0.47819268851259866]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "bd36975a-97a0-4dea-a619-9554dee4b1d6", "fitness": "-inf", "name": "NovelAdaptiveDE", "description": "Adaptive Differential Evolution with orthogonal learning, covariance matrix adaptation for mutation, and a restart strategy based on population diversity.", "code": "import numpy as np\n\nclass NovelAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=10, ortho_group_size=5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.ortho_group_size = ortho_group_size\n        self.population = None\n        self.fitness = None\n        self.memory_F = np.ones(self.archive_size) * 0.5\n        self.memory_CR = np.ones(self.archive_size) * 0.9\n        self.archive = []\n        self.F = 0.5\n        self.CR = 0.9\n        self.p = 0.1  # Probability for choosing F and CR from memory\n        self.min_CR = 0.1\n        self.stagnation_counter = 0\n        self.max_stagnation = 50  # restart if stagnant for longer\n        self.C = np.eye(dim)  # Covariance matrix for CMA\n        self.learning_rate = 0.1\n        self.diversity_threshold = 1e-6\n\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        self.f_hist = [np.min(self.fitness)]\n\n    def mutate(self, i):\n        # CMA-ES like mutation\n        z = np.random.normal(0, 1, self.dim)\n        x_mutated = self.population[i] + self.F * np.dot(self.C, z)\n        return x_mutated\n\n    def crossover(self, x_mutated, i):\n        x_trial = np.copy(self.population[i])\n        j_rand = np.random.randint(self.dim)\n        for j in range(self.dim):\n            if np.random.rand() < self.CR or j == j_rand:\n                x_trial[j] = x_mutated[j]\n        return x_trial\n\n    def update_parameters(self, success, F, CR):\n        if success:\n            self.archive.append((F, CR))\n            if len(self.archive) > self.archive_size:\n                self.archive.pop(0)\n\n            if len(self.archive) > 0:\n                F_vals = [item[0] for item in self.archive]\n                CR_vals = [item[1] for item in self.archive]\n                self.F = np.mean(F_vals)\n                self.CR = max(self.min_CR, np.mean(CR_vals))  # Ensure CR doesn't get too small\n\n    def update_covariance(self, x_trial, x_i):\n        # Rank-one update of covariance matrix\n        diff = x_trial - x_i\n        self.C = (1 - self.learning_rate) * self.C + self.learning_rate * np.outer(diff, diff)\n\n    def orthogonal_learning(self, func):\n        # Apply orthogonal learning on a subset of the population\n        idxs = np.random.choice(self.pop_size, self.ortho_group_size, replace=False)\n        group = self.population[idxs]\n        center = np.mean(group, axis=0)\n\n        # Generate orthogonal directions (simplified)\n        directions = np.random.normal(0, 1, size=(self.ortho_group_size, self.dim))\n        Q, _ = np.linalg.qr(directions.T)\n        orthogonal_directions = Q.T\n\n        for i, idx in enumerate(idxs):\n            # Explore along the orthogonal direction\n            step_size = np.random.uniform(-0.1, 0.1)  # Small step size\n            x_ortho = group[i] + step_size * orthogonal_directions[i]\n            x_ortho = np.clip(x_ortho, func.bounds.lb, func.bounds.ub)\n\n            f_ortho = func(x_ortho)\n            self.budget -= 1\n\n            if self.budget <= 0:\n                return\n\n            if f_ortho < self.fitness[idx]:\n                self.population[idx] = x_ortho.copy()\n                self.fitness[idx] = f_ortho\n                if f_ortho < self.f_opt:\n                    self.f_opt = f_ortho\n                    self.x_opt = x_ortho.copy()\n\n    def check_stagnation(self):\n        if len(set(self.fitness)) <= 1:\n            self.stagnation_counter += 1\n        else:\n            self.stagnation_counter = 0\n\n        if self.stagnation_counter > self.max_stagnation:\n            return True\n        else:\n            return False\n\n    def restart_population(self, func):\n         self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n         self.fitness = np.array([func(x) for x in self.population])\n         self.budget -= self.pop_size\n         self.stagnation_counter = 0\n         self.archive = []  # Clear the archive on restart\n         self.C = np.eye(self.dim)\n\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)].copy()\n\n        while self.budget > 0:\n\n            if self.check_stagnation() and self.budget > self.pop_size:\n                self.restart_population(func)\n                self.f_opt = np.min(self.fitness)\n                self.x_opt = self.population[np.argmin(self.fitness)].copy()\n                self.f_hist = [self.f_opt]\n\n            for i in range(self.pop_size):\n                # Parameter Adaptation\n                if np.random.rand() < self.p and len(self.archive) > 0:\n                    idx = np.random.randint(len(self.archive))\n                    F, CR = self.archive[idx]\n                else:\n                    F = self.F\n                    CR = self.CR\n\n                x_mutated = self.mutate(i)\n                x_trial = self.crossover(x_mutated, i)\n                x_trial = np.clip(x_trial, func.bounds.lb, func.bounds.ub)\n\n                f_trial = func(x_trial)\n                self.budget -= 1\n\n                if self.budget <= 0:\n                    break\n\n                if f_trial < self.fitness[i]:\n                    self.update_parameters(True, F, CR)\n                    self.update_covariance(x_trial, self.population[i])\n                    self.population[i] = x_trial.copy()\n                    self.fitness[i] = f_trial\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = x_trial.copy()\n                else:\n                    self.update_parameters(False, F, CR)\n\n            if self.budget <= 0:\n                break\n\n            # Orthogonal learning\n            if self.budget > self.ortho_group_size:\n                self.orthogonal_learning(func)\n            \n            self.f_hist.append(self.f_opt)\n\n            best_index = np.argmin(self.fitness)\n            if self.fitness[best_index] < self.f_opt:\n                self.f_opt = self.fitness[best_index]\n                self.x_opt = self.population[best_index].copy()\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 9, "feedback": "Evaluation timed out after 300 seconds.", "error": "Evaluation timed out after 300 seconds.", "parent_ids": ["959042e0-c7fd-4f7d-91da-a41fdc341f98"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "1e25df2b-2cdd-47c9-9068-3890416a86ae", "fitness": 0.570355570483431, "name": "HybridCauchyDE", "description": "A hybrid DE algorithm with dynamic parameter adaptation, a distance-based population diversity maintenance strategy, and a Cauchy mutation operator for enhanced exploration.", "code": "import numpy as np\n\nclass HybridCauchyDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, diversity_threshold=0.1, F=0.5, CR=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.diversity_threshold = diversity_threshold\n        self.F = F\n        self.CR = CR\n        self.population = None\n        self.fitness = None\n        self.archive = []\n        self.archive_fitness = []\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)].copy()\n\n    def cauchy_mutation(self, i, best_idx):\n        idxs = np.random.choice(self.pop_size, 2, replace=False)\n        a, b = idxs\n        cauchy = np.random.standard_cauchy(size=self.dim)\n        x_mutated = self.population[i] + self.F * (self.population[best_idx] - self.population[i]) + self.F * (self.population[a] - self.population[b]) + 0.01 * cauchy\n        return x_mutated\n\n    def crossover(self, x_mutated, i):\n        x_trial = np.copy(self.population[i])\n        j_rand = np.random.randint(self.dim)\n        for j in range(self.dim):\n            if np.random.rand() < self.CR or j == j_rand:\n                x_trial[j] = x_mutated[j]\n        return x_trial\n\n    def calculate_diversity(self):\n        centroid = np.mean(self.population, axis=0)\n        distances = np.linalg.norm(self.population - centroid, axis=1)\n        mean_distance = np.mean(distances)\n        return mean_distance\n\n    def maintain_diversity(self, func):\n        diversity = self.calculate_diversity()\n        if diversity < self.diversity_threshold:\n            # Introduce new random solutions to increase diversity\n            num_new = int(self.pop_size * 0.1)  # Replace 10% of the population\n            new_solutions = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(num_new, self.dim))\n            new_fitness = np.array([func(x) for x in new_solutions])\n            self.budget -= num_new\n\n            worst_indices = np.argsort(self.fitness)[-num_new:]\n            self.population[worst_indices] = new_solutions\n            self.fitness[worst_indices] = new_fitness\n\n    def adapt_parameters(self):\n        # Simple adaptation of F and CR based on success\n        success_indices = np.where(self.fitness < self.archive_fitness)[0]\n\n        if len(success_indices) > 0:\n            self.F = np.clip(np.mean(np.random.choice([0.5, self.F], size=len(success_indices))), 0.1, 0.9)\n            self.CR = np.clip(np.mean(np.random.choice([0.9, self.CR], size=len(success_indices))), 0.1, 0.9)\n        else:\n             self.F = np.clip(self.F + np.random.normal(0, 0.01), 0.1, 0.9)\n             self.CR = np.clip(self.CR + np.random.normal(0, 0.01), 0.1, 0.9)\n\n    def update_archive(self):\n        # Update archive with current population\n        self.archive = np.copy(self.population)\n        self.archive_fitness = np.copy(self.fitness)\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        self.update_archive()\n\n        while self.budget > 0:\n            best_idx = np.argmin(self.fitness)\n            for i in range(self.pop_size):\n                x_mutated = self.cauchy_mutation(i, best_idx)\n                x_trial = self.crossover(x_mutated, i)\n                x_trial = np.clip(x_trial, func.bounds.lb, func.bounds.ub)\n\n                f_trial = func(x_trial)\n                self.budget -= 1\n\n                if self.budget <= 0:\n                    break\n\n                if f_trial < self.fitness[i]:\n                    self.population[i] = x_trial.copy()\n                    self.fitness[i] = f_trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = x_trial.copy()\n\n            if self.budget <= 0:\n                break\n                \n            self.maintain_diversity(func)\n            self.adapt_parameters()\n            \n            best_index = np.argmin(self.fitness)\n            if self.fitness[best_index] < self.f_opt:\n                self.f_opt = self.fitness[best_index]\n                self.x_opt = self.population[best_index].copy()\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 9, "feedback": "The algorithm HybridCauchyDE scored 0.570 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["3c377e79-26ed-41cf-8828-7863aaef3453"], "operator": null, "metadata": {"aucs": [0.26381262564310337, 0.5315959419535998, 0.513908149346368, 0.9264326408611727, 0.4960071536482382, 0.574137386766306, 0.3389907730650914, 0.45750396220542566, 0.50201754237245, 0.4361160429555301, 0.9170333089922708, 0.9959599538049092, 0.393417010187856, 0.5150314727727585, 0.934069526333843, 0.5901190102457495, 0.45854845752434736, 0.6895814387106645, 0.3372736020244461, 0.5355554102544883]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "2dc149b7-ee69-42ff-9944-3cec60108ab0", "fitness": 0.7051698260403902, "name": "SimplifiedAdaptiveDE", "description": "A simplified Adaptive DE with enhanced exploration through orthogonal design and adaptive parameter control based on fitness improvement.", "code": "import numpy as np\nfrom scipy.stats import norm\n\nclass SimplifiedAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_rate=0.2, f_base=0.5, cr_base=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = int(pop_size * archive_rate)  # Archive size as a fraction of pop_size\n        self.population = None\n        self.fitness = None\n        self.archive = []\n        self.f_base = f_base\n        self.cr_base = cr_base\n        self.best_fitness = float('inf')\n        self.best_solution = None\n        self.success_f = []\n        self.success_cr = []\n        self.success_count = 0\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        self.best_idx = np.argmin(self.fitness)\n        self.best_fitness = self.fitness[self.best_idx]\n        self.best_solution = self.population[self.best_idx].copy()\n\n    def mutate(self, i, F, CR, func):\n        idxs = np.random.choice(self.pop_size, 3, replace=False)\n        a, b, c = idxs[0], idxs[1], idxs[2]\n        mutant = self.population[a] + F * (self.population[b] - self.population[c])\n        return mutant\n\n    def crossover(self, mutant, i, CR):\n         # Orthogonal Crossover: Create a trial vector\n        trial = np.copy(self.population[i])\n        j_rand = np.random.randint(self.dim)\n        for j in range(self.dim):\n            if np.random.rand() < CR or j == j_rand:\n                trial[j] = mutant[j]\n        return trial\n\n    def repair(self, x_trial, func):\n        return np.clip(x_trial, func.bounds.lb, func.bounds.ub)\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        F = self.f_base\n        CR = self.cr_base\n        generation = 0\n        while self.budget > 0:\n            generation += 1\n            self.success_f = []\n            self.success_cr = []\n            self.success_count = 0\n            for i in range(self.pop_size):\n                mutant = self.mutate(i, F, CR, func)\n                trial = self.crossover(mutant, i, CR)\n                trial = self.repair(trial, func)\n                f_trial = func(trial)\n                self.budget -= 1\n                if f_trial < self.fitness[i]:\n                    self.success_count += 1\n                    self.success_f.append(F)\n                    self.success_cr.append(CR)\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(self.population[i].copy())\n                    else:\n                        j = np.random.randint(0, self.archive_size)\n                        self.archive[j] = self.population[i].copy()\n                    self.population[i] = trial\n                    self.fitness[i] = f_trial\n                    if f_trial < self.best_fitness:\n                        self.best_fitness = f_trial\n                        self.best_solution = trial.copy()\n\n                if self.budget <= 0:\n                    break\n\n            # Adapt F and CR\n            if self.success_count > 0:\n                F = np.mean(self.success_f) if self.success_f else self.f_base\n                CR = np.mean(self.success_cr) if self.success_cr else self.cr_base\n            else:\n                 F = np.clip(np.random.normal(F, 0.3), 0.1, 1.0) # Adapt even without success\n                 CR = np.clip(np.random.normal(CR, 0.2), 0.0, 1.0)\n\n        return self.best_fitness, self.best_solution", "configspace": "", "generation": 9, "feedback": "The algorithm SimplifiedAdaptiveDE scored 0.705 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["9bfc7670-142f-4e52-831e-44b1953ac659"], "operator": null, "metadata": {"aucs": [0.33507853039842406, 0.6346031050068817, 0.6818351368656711, 0.8874002371833172, 0.7353847872690091, 0.785135622748012, 0.6642024152650023, 0.6620886619930804, 0.7242098255865759, 0.6393379466211805, 0.8632688224678152, 0.9998208740697984, 0.6738958346270462, 0.7473428697428235, 0.9277509256224618, 0.80411322019879, 0.6499371182545997, 0.8522016513656228, 0.31427980494330054, 0.5215091305783889]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "ea845882-2d10-4da0-8b92-76413c770afb", "fitness": "-inf", "name": "EnhancedAdaptiveDE", "description": "Enhanced Adaptive Differential Evolution with orthogonal learning, dynamic population sizing, archive, and a restart mechanism to escape stagnation.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=10, stagnation_threshold=50, ol_samples=5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.stagnation_threshold = stagnation_threshold\n        self.stagnation_counter = 0\n        self.best_fitness_history = []\n        self.pop_factor = 1.0\n        self.archive = []\n        self.memory_f = [0.5] * archive_size\n        self.memory_cr = [0.9] * archive_size\n        self.sf = []\n        self.scr = []\n        self.population = None\n        self.fitness = None\n        self.ol_samples = ol_samples # Number of samples for orthogonal learning\n\n    def initialize_population(self, func):\n        current_pop_size = int(self.pop_size * self.pop_factor)\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(current_pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= current_pop_size\n\n    def mutate(self, i):\n        current_pop_size = int(self.pop_size * self.pop_factor)\n        phi_idx = np.random.randint(0, self.archive_size)\n        F = np.clip(np.random.normal(self.memory_f[phi_idx], 0.1), 0.1, 1.0)\n        CR = np.clip(np.random.normal(self.memory_cr[phi_idx], 0.1), 0.0, 1.0)\n\n        idxs = np.random.choice(current_pop_size, 3, replace=False)\n        a, b, c = idxs[0], idxs[1], idxs[2]\n\n        # Incorporate information from the top 20% performing solutions\n        top_indices = np.argsort(self.fitness)[:max(1, int(0.2 * current_pop_size))]\n        p_best_idx = np.random.choice(top_indices)\n\n        x_mutated = self.population[i] + F * (self.population[p_best_idx] - self.population[i]) + F * (self.population[a] - self.population[b])\n        return x_mutated, F, CR\n\n    def crossover(self, x_mutated, i, cr):\n        x_trial = np.copy(self.population[i])\n        j_rand = np.random.randint(self.dim)\n        for j in range(self.dim):\n            if np.random.rand() < cr or j == j_rand:\n                x_trial[j] = x_mutated[j]\n        return x_trial\n\n    def orthogonal_learning(self, func, x_current):\n        levels = np.linspace(func.bounds.lb, func.bounds.ub, self.ol_samples)\n        \n        best_fitness = float('inf')\n        best_sample = None\n\n        for j in range(self.dim):\n            original_value = x_current[j]\n            fitnesses = []\n            samples = []\n            \n            for level in levels:\n                x_sample = x_current.copy()\n                x_sample[j] = level\n                x_sample = np.clip(x_sample, func.bounds.lb, func.bounds.ub)\n                fitness = func(x_sample)\n\n                self.budget -= 1\n                if self.budget <= 0:\n                    return best_sample if best_sample is not None else x_current, best_fitness < func(x_current)\n\n                fitnesses.append(fitness)\n                samples.append(x_sample)\n            \n            best_index = np.argmin(fitnesses)\n            \n            if fitnesses[best_index] < best_fitness:\n                best_fitness = fitnesses[best_index]\n                best_sample = samples[best_index]\n\n        return best_sample, best_fitness < func(x_current)\n\n    def update_memory(self):\n        if self.sf:\n            self.memory_f.append(sum(val**2 for val in self.sf) / sum(self.sf) if sum(self.sf) != 0 else self.memory_f[-1])\n            self.memory_f.pop(0)\n            self.memory_cr.append(np.mean(self.scr))\n            self.memory_cr.pop(0)\n        self.sf = []\n        self.scr = []\n\n    def check_stagnation(self):\n        if len(self.best_fitness_history) > self.stagnation_threshold:\n            if np.std(self.best_fitness_history[-self.stagnation_threshold:]) < 1e-6:\n                self.stagnation_counter += 1\n                return True\n            else:\n                self.stagnation_counter = 0\n                return False\n        return False\n\n    def restart_population(self, func):\n        self.pop_factor *= 0.75  # Reduce population size\n        self.pop_factor = max(0.2, self.pop_factor) # Prevent too small pop size\n        self.initialize_population(func)\n        self.best_fitness_history = []\n        self.memory_f = [0.5] * self.archive_size\n        self.memory_cr = [0.9] * self.archive_size\n        self.sf = []\n        self.scr = []\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)].copy()\n        self.stagnation_counter = 0\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)].copy()\n        self.sf = []\n        self.scr = []\n\n        while self.budget > 0:\n            current_pop_size = int(self.pop_size * self.pop_factor)\n            for i in range(current_pop_size):\n                x_mutated, F, CR = self.mutate(i)\n                x_trial = self.crossover(x_mutated, i, CR)\n                x_trial = np.clip(x_trial, func.bounds.lb, func.bounds.ub)\n\n                # Orthogonal learning on the trial vector\n                x_ol, improved = self.orthogonal_learning(func, x_trial)\n                if self.budget <= 0:\n                    break\n\n                f_trial = func(x_ol) if improved else func(x_trial) # evaluate only the OL candidate if it was improved by orthogonal learning\n                self.budget -= 1 if improved else 1\n\n                if self.budget <= 0:\n                    break\n\n                if f_trial < self.fitness[i]:\n                    self.sf.append(F)\n                    self.scr.append(CR)\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(self.population[i].copy())\n                    else:\n                        j = np.random.randint(0, self.archive_size)\n                        self.archive[j] = self.population[i].copy()\n                    self.population[i] = x_ol.copy() if improved else x_trial.copy()\n                    self.fitness[i] = f_trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = (x_ol.copy() if improved else x_trial.copy())\n\n            if self.budget <= 0:\n                break\n\n            self.update_memory()\n            best_index = np.argmin(self.fitness)\n            if self.fitness[best_index] < self.f_opt:\n                self.f_opt = self.fitness[best_index]\n                self.x_opt = self.population[best_index].copy()\n\n            self.best_fitness_history.append(self.f_opt)\n\n            if self.check_stagnation() and self.budget > int(self.pop_size * self.pop_factor) * 2:\n                self.restart_population(func)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 10, "feedback": "In the code, line 65, in orthogonal_learning, the following error occurred:\nValueError: setting an array element with a sequence.\nOn line: x_sample[j] = level", "error": "In the code, line 65, in orthogonal_learning, the following error occurred:\nValueError: setting an array element with a sequence.\nOn line: x_sample[j] = level", "parent_ids": ["ca85b7ad-0f37-47f9-9049-9648e4f1e39a"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "b4c90b18-4411-4397-89eb-a6a4c60a25a9", "fitness": 0.5026201606763814, "name": "AdaptiveDE", "description": "Self-adaptive Differential Evolution with dynamic parameter adaptation, covariance matrix adaptation mutation, and an external archive for enhanced exploration and exploitation.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=5, f_base=0.5, cr_base=0.9, diversity_threshold=1e-5, rejuvenation_rate=0.05, adaptation_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.population = None\n        self.fitness = None\n        self.archive = []\n        self.memory_f = [f_base] * archive_size\n        self.memory_cr = [cr_base] * archive_size\n        self.sf = []\n        self.scr = []\n        self.diversity_threshold = diversity_threshold\n        self.rejuvenation_rate = rejuvenation_rate\n        self.f_base = f_base\n        self.cr_base = cr_base\n        self.adaptation_rate = adaptation_rate  # Rate for adapting F and CR\n        self.covariance_matrix = np.eye(dim)  # Initialize covariance matrix for CMA-ES-like mutation\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        self.best_idx = np.argmin(self.fitness)\n        self.best_fitness = self.fitness[self.best_idx]\n        self.best_solution = self.population[self.best_idx].copy()\n\n\n    def mutate(self, i):\n        phi_idx = np.random.randint(0, self.archive_size)\n        F = np.clip(np.random.normal(self.memory_f[phi_idx], 0.1), 0.1, 1.0)\n        CR = np.clip(np.random.normal(self.memory_cr[phi_idx], 0.1), 0.0, 1.0)\n\n\n        idxs = np.random.choice(self.pop_size, 3, replace=False)\n        a, b, c = idxs[0], idxs[1], idxs[2]\n\n        # Diversity-enhanced mutation using best solution and CMA-ES-like adaptation\n        z = np.random.multivariate_normal(np.zeros(self.dim), self.covariance_matrix)\n        x_mutated = self.population[i] + F * (self.best_solution - self.population[i]) + F * (self.population[a] - self.population[b]) + 0.1 * z\n\n\n        return x_mutated, F, CR\n\n    def crossover(self, x_mutated, i, cr):\n        x_trial = np.copy(self.population[i])\n        j_rand = np.random.randint(self.dim)\n        for j in range(self.dim):\n            if np.random.rand() < cr or j == j_rand:\n                x_trial[j] = x_mutated[j]\n        return x_trial\n\n    def repair(self, x_trial, func):\n        x_trial = np.clip(x_trial, func.bounds.lb, func.bounds.ub)\n        return x_trial\n\n    def update_memory(self):\n        if self.sf:\n            self.memory_f.append(sum(val**2 for val in self.sf) / sum(self.sf) if sum(self.sf) > 0 else self.f_base)\n            self.memory_f.pop(0)\n            self.memory_cr.append(np.mean(self.scr))\n            self.memory_cr.pop(0)\n\n            # Adaptive F and CR\n            self.f_base = (1 - self.adaptation_rate) * self.f_base + self.adaptation_rate * np.mean(self.sf) if self.sf else self.f_base\n            self.cr_base = (1 - self.adaptation_rate) * self.cr_base + self.adaptation_rate * np.mean(self.scr) if self.scr else self.cr_base\n\n        self.sf = []\n        self.scr = []\n\n    def rejuvenate_population(self, func):\n        # Rejuvenate a portion of the population to enhance exploration\n        num_to_rejuvenate = int(self.rejuvenation_rate * self.pop_size)\n        indices_to_rejuvenate = np.random.choice(self.pop_size, num_to_rejuvenate, replace=False)\n        for i in indices_to_rejuvenate:\n            self.population[i] = np.random.uniform(func.bounds.lb, func.bounds.ub)\n            self.fitness[i] = func(self.population[i])\n            self.budget -= 1\n\n    def update_covariance_matrix(self):\n        # Update covariance matrix based on successful mutations\n        if self.sf:\n            # Calculate the mean shift of successful individuals\n            shifts = [self.population[i] - self.population[i] for i in range(self.pop_size) if i in [j for j in range(len(self.sf))]] #Problem in list comp. This line should store the shift from old to new pop if a better solution was found\n            \n            if shifts:\n                mean_shift = np.mean(shifts, axis=0)\n                # Update covariance matrix\n                self.covariance_matrix = (1 - self.adaptation_rate) * self.covariance_matrix + self.adaptation_rate * np.outer(mean_shift, mean_shift)\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.budget > 0:\n            self.sf = []\n            self.scr = []\n            for i in range(self.pop_size):\n                x_mutated, F, CR = self.mutate(i)\n                x_trial = self.crossover(x_mutated, i, CR)\n                x_trial = self.repair(x_trial, func)\n\n                f_trial = func(x_trial)\n                self.budget -= 1\n                if self.budget <= 0:\n                    break\n\n                if f_trial < self.fitness[i]:\n                    self.sf.append(F)\n                    self.scr.append(CR)\n\n                    # Archive update\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(self.population[i].copy())\n                    else:\n                        j = np.random.randint(0, self.archive_size)\n                        self.archive[j] = self.population[i].copy()\n\n                    \n                    self.population[i] = x_trial.copy()\n                    self.fitness[i] = f_trial\n\n                    if f_trial < self.best_fitness:\n                        self.best_fitness = f_trial\n                        self.best_solution = x_trial.copy()\n                        self.best_idx = i\n                \n\n            if self.budget <= 0:\n                break\n\n            self.update_memory()\n            self.update_covariance_matrix()\n\n            # Population rejuvenation based on diversity\n            if np.std(self.fitness) < self.diversity_threshold:\n                self.rejuvenate_population(func)\n                self.best_idx = np.argmin(self.fitness)\n                self.best_fitness = self.fitness[self.best_idx]\n                self.best_solution = self.population[self.best_idx].copy()\n\n\n\n\n        return self.best_fitness, self.best_solution", "configspace": "", "generation": 10, "feedback": "The algorithm AdaptiveDE scored 0.503 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["9bfc7670-142f-4e52-831e-44b1953ac659"], "operator": null, "metadata": {"aucs": [0.22107152292016474, 0.45020341968991484, 0.4741402022202672, 0.809087968386216, 0.43441913918019015, 0.532751365366509, 0.34715802738344337, 0.4115150382949603, 0.42697953054324445, 0.19576876534674925, 0.8184440933441526, 0.9942480542524197, 0.3202061948403083, 0.42121763424824443, 0.8744052332684693, 0.5559478662350584, 0.40362586406736856, 0.6262866046848106, 0.23513482349925474, 0.49979186575588164]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "c12af88d-3e55-491a-b0db-03599411d886", "fitness": "-inf", "name": "EnhancedDynamicResourceAllocationDE", "description": "Enhanced Dynamic Resource Allocation DE with adaptive local search, orthogonal learning, and a restart mechanism for escaping local optima.", "code": "import numpy as np\n\nclass EnhancedDynamicResourceAllocationDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, num_strategies=3, local_search_probability=0.1,\n                 local_search_intensity=0.01, orthogonal_learning_probability=0.05, restart_trigger=0.005):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.num_strategies = num_strategies\n        self.local_search_probability = local_search_probability\n        self.local_search_intensity = local_search_intensity\n        self.orthogonal_learning_probability = orthogonal_learning_probability\n        self.restart_trigger = restart_trigger\n        self.population = None\n        self.fitness = None\n        self.strategy_successes = np.zeros(num_strategies)\n        self.strategy_counts = np.zeros(num_strategies)\n        self.strategy_weights = np.ones(num_strategies) / num_strategies\n        self.F = 0.5\n        self.CR = 0.9\n        self.no_improvement_count = 0\n        self.best_fitness_history = []\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        self.f_opt = np.min(self.fitness)\n        self.best_fitness_history.append(self.f_opt)\n\n    def select_strategy(self):\n        return np.random.choice(self.num_strategies, p=self.strategy_weights)\n\n    def mutate(self, strategy, i):\n        if strategy == 0:  # DE/rand/1\n            idxs = np.random.choice(self.pop_size, 5, replace=False)\n            a, b, c, d, e = idxs\n            x_mutated = self.population[a] + self.F * (self.population[b] - self.population[c]) + self.F * (self.population[d] - self.population[e])\n        elif strategy == 1:  # DE/current-to-best/1\n            best_idx = np.argmin(self.fitness)\n            idxs = np.random.choice(self.pop_size, 3, replace=False)\n            a, b, c = idxs\n            x_mutated = self.population[i] + self.F * (self.population[best_idx] - self.population[i]) + self.F * (self.population[a] - self.population[b])\n        elif strategy == 2:  # DE/best/2\n             best_idx = np.argmin(self.fitness)\n             idxs = np.random.choice(self.pop_size, 4, replace=False)\n             a, b, c, d = idxs\n             x_mutated = self.population[best_idx] + self.F * (self.population[a] - self.population[b]) + self.F * (self.population[c] - self.population[d])\n\n        return x_mutated\n\n    def crossover(self, x_mutated, i):\n        x_trial = np.copy(self.population[i])\n        j_rand = np.random.randint(self.dim)\n        for j in range(self.dim):\n            if np.random.rand() < self.CR or j == j_rand:\n                x_trial[j] = x_mutated[j]\n        return x_trial\n\n    def local_search(self, x, func):\n        # Apply adaptive Gaussian local search around the current solution\n        x_ls = x + np.random.normal(0, self.local_search_intensity, self.dim)  # Adaptive perturbation\n        x_ls = np.clip(x_ls, func.bounds.lb, func.bounds.ub)\n        return x_ls\n\n    def orthogonal_learning(self, x, func):\n        # Orthogonal experimental design for learning\n        level_num = 3  # Number of levels for each dimension\n        levels = np.linspace(func.bounds.lb, func.bounds.ub, level_num)\n        ol_design = np.array([[-1, -1], [-1, 1], [1, -1], [1, 1]])  # Example orthogonal array (L4)\n\n        best_f = float('inf')\n        best_x = None\n\n        for i in range(ol_design.shape[0]):\n            x_ol = np.copy(x)\n            for j in range(min(self.dim, ol_design.shape[1])):  # Use only first few dimensions if dim > ol_design size\n                level_index = (ol_design[i, j] + 1) // 2\n                x_ol[j] = levels[level_index]\n            x_ol = np.clip(x_ol, func.bounds.lb, func.bounds.ub)\n            f_ol = func(x_ol)\n            self.budget -= 1\n            if f_ol < best_f:\n                best_f = f_ol\n                best_x = x_ol.copy()\n            if self.budget <= 0:\n                break\n        if best_x is not None:\n            return best_x, best_f\n        else:\n            return x, func(x)\n\n\n    def update_strategy_weights(self):\n        total_successes = np.sum(self.strategy_successes)\n        if total_successes > 0:\n            self.strategy_weights = self.strategy_successes / total_successes\n        else:\n            self.strategy_weights = np.ones(self.num_strategies) / self.num_strategies\n\n        # Smoothing the weights to avoid premature convergence\n        self.strategy_weights = 0.9 * self.strategy_weights + 0.1 / self.num_strategies\n        self.strategy_weights /= np.sum(self.strategy_weights)\n\n    def should_restart(self):\n        if len(self.best_fitness_history) > 100:\n            # Restart if no significant improvement in the last 100 iterations\n            improvement = self.best_fitness_history[-100] - self.best_fitness_history[-1]\n            if improvement / abs(self.best_fitness_history[-100]) < self.restart_trigger:\n                return True\n        return False\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)].copy()\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                strategy = self.select_strategy()\n                self.strategy_counts[strategy] += 1\n\n                x_mutated = self.mutate(strategy, i)\n                x_trial = self.crossover(x_mutated, i)\n                x_trial = np.clip(x_trial, func.bounds.lb, func.bounds.ub)\n\n                # Local Search\n                if np.random.rand() < self.local_search_probability:\n                    x_trial = self.local_search(x_trial, func)\n\n                # Orthogonal Learning\n                if np.random.rand() < self.orthogonal_learning_probability and self.budget > 4: # Ensure budget for orthogonal learning\n                    x_trial, f_trial = self.orthogonal_learning(x_trial, func)\n                else:\n                    f_trial = func(x_trial)\n                    self.budget -= 1\n\n                if self.budget <= 0:\n                    break\n\n                if f_trial < self.fitness[i]:\n                    self.strategy_successes[strategy] += 1\n                    self.population[i] = x_trial.copy()\n                    self.fitness[i] = f_trial\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = x_trial.copy()\n                        self.no_improvement_count = 0  # Reset no_improvement counter\n                else:\n                    self.no_improvement_count += 1\n\n            if self.budget <= 0:\n                break\n\n            self.update_strategy_weights()\n            best_index = np.argmin(self.fitness)\n            if self.fitness[best_index] < self.f_opt:\n                self.f_opt = self.fitness[best_index]\n                self.x_opt = self.population[best_index].copy()\n                self.no_improvement_count = 0\n            else:\n                self.no_improvement_count += 1\n            self.best_fitness_history.append(self.f_opt)\n\n            # Restart Mechanism\n            if self.should_restart():\n                self.initialize_population(func)\n                self.no_improvement_count = 0\n                self.best_fitness_history = [self.f_opt]\n\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 10, "feedback": "In the code, line 79, in orthogonal_learning, the following error occurred:\nValueError: setting an array element with a sequence.\nOn line: x_ol[j] = levels[level_index]", "error": "In the code, line 79, in orthogonal_learning, the following error occurred:\nValueError: setting an array element with a sequence.\nOn line: x_ol[j] = levels[level_index]", "parent_ids": ["3c377e79-26ed-41cf-8828-7863aaef3453"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "34e4946c-93ff-4832-af19-2f57700345c7", "fitness": 0.0, "name": "SOM_CMAES", "description": "A hybrid metaheuristic algorithm that combines a self-organizing map (SOM) for population diversification with a covariance matrix adaptation evolution strategy (CMA-ES) for efficient local search and adaptation of the search distribution.", "code": "import numpy as np\n\nclass SOM_CMAES:\n    def __init__(self, budget=10000, dim=10, pop_size=50, som_grid_size=10, sigma0=0.5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.som_grid_size = som_grid_size\n        self.sigma0 = sigma0  # Initial step size for CMA-ES\n        self.population = None\n        self.fitness = None\n        self.som = None\n        self.weights = np.random.rand(som_grid_size * som_grid_size, dim) * 10 - 5  # Initialize SOM weights\n        self.learning_rate = 0.1 # Initial learning rate for SOM\n\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n\n    def som_train(self, func, epochs=5):\n        # Train the SOM to represent the search space distribution\n        for epoch in range(epochs):\n            for i in range(self.pop_size):\n                sample = self.population[i]\n                # Find the best matching unit (BMU)\n                distances = np.sum((self.weights - sample) ** 2, axis=1)\n                bmu_index = np.argmin(distances)\n                \n                # Update the BMU and its neighbors\n                for j in range(self.som_grid_size * self.som_grid_size):\n                    distance_to_bmu = np.linalg.norm(np.array(np.unravel_index(bmu_index, (self.som_grid_size, self.som_grid_size))) - np.array(np.unravel_index(j, (self.som_grid_size, self.som_grid_size))))\n                    influence = np.exp(-distance_to_bmu ** 2 / (2 * (epoch + 1)**2))  # Time-dependent neighborhood radius\n                    self.weights[j] += self.learning_rate * influence * (sample - self.weights[j])\n        self.learning_rate *= 0.9 # Decay the learning rate\n\n\n    def cma_es(self, func, x_start, sigma, eval_budget):\n        # Covariance Matrix Adaptation Evolution Strategy for local search\n        x_best = x_start.copy()\n        f_best = func(x_start)\n        eval_budget -=1\n\n        mean = x_start.copy()\n        C = np.eye(self.dim)  # Initial covariance matrix\n        path_c = np.zeros(self.dim)\n        path_sigma = np.zeros(self.dim)\n        mu = self.pop_size // 4\n        weights = np.log(mu + 1/2) - np.log(np.arange(1, mu + 1))\n        weights /= np.sum(weights)\n        \n        c_sigma = (mu / self.dim + mu) / (self.dim + mu + 2)\n        c_c = (4 + mu/self.dim) / (self.dim + 4 + 2 * mu/self.dim)\n        d_sigma = 1 + 2 * np.max([0, np.sqrt((mu - 1)/(self.dim + 1)) - 1])\n        c_1 = 2 / ((self.dim + 1.3)**2 + mu)\n        c_mu = min(1 - c_1, 2 * (mu - 2 + 1/mu) / ((self.dim + 2)**2 + mu))\n        \n        for i in range(eval_budget):\n            z = np.random.multivariate_normal(np.zeros(self.dim), C, self.pop_size)\n            x = mean + sigma * z\n            x = np.clip(x, func.bounds.lb, func.bounds.ub)\n            f = np.array([func(xi) for xi in x])\n            \n            indices = np.argsort(f)\n            x_mu = x[indices[:mu]]\n            z_mu = z[indices[:mu]]\n            \n            mean_new = np.sum(weights[:, None] * x_mu, axis=0)\n            zmean = np.sum(weights[:, None] * z_mu, axis=0)\n            \n            path_sigma = (1 - c_sigma) * path_sigma + np.sqrt(c_sigma * (2 - c_sigma)) * np.sqrt(mu) * zmean\n            C = (1 - c_1 - c_mu) * C + c_1 * (np.outer(path_c, path_c) + (c_mu / sigma**2) * np.sum(weights[:, None, None] * (x_mu - mean)[..., None] * (x_mu - mean)[:, None, :], axis=0))\n            \n            mean = mean_new\n            if eval_budget > 0:\n                f_current_best = np.min(f)\n                x_current_best = x[np.argmin(f)]\n                if f_current_best < f_best:\n                    f_best = f_current_best\n                    x_best = x_current_best\n            sigma *= np.exp((c_sigma / d_sigma) * (np.linalg.norm(path_sigma) / np.sqrt(self.dim) - 1))\n            sigma = max(sigma, 1e-6)\n                \n\n        return f_best, x_best\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)].copy()\n\n        while self.budget > 0:\n            # Train SOM based on current population\n            self.som_train(func, epochs=2)\n\n            # Sample individuals from SOM's weight vectors\n            som_indices = np.random.choice(self.som_grid_size * self.som_grid_size, self.pop_size, replace=True)\n            som_individuals = self.weights[som_indices]\n            som_individuals = np.clip(som_individuals, func.bounds.lb, func.bounds.ub)\n\n            # Perform CMA-ES local search around best SOM individual\n            \n            best_som_idx = np.argmin([func(x) for x in som_individuals])\n            \n            f_cmaes, x_cmaes = self.cma_es(func, som_individuals[best_som_idx], self.sigma0, min(self.budget, self.pop_size))\n            self.budget -= min(self.budget, self.pop_size)\n            \n\n            if f_cmaes < self.f_opt:\n                self.f_opt = f_cmaes\n                self.x_opt = x_cmaes.copy()\n            \n            \n            # Replace worst individuals in the population with CMA-ES result to ensure population diversity\n            worst_idx = np.argmax(self.fitness)\n            self.population[worst_idx] = x_cmaes.copy()\n            self.fitness[worst_idx] = f_cmaes\n            \n            if self.budget <= 0:\n                break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 10, "feedback": "The algorithm SOM_CMAES scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["3c377e79-26ed-41cf-8828-7863aaef3453"], "operator": null, "metadata": {"aucs": [0]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "743565f0-0557-43d7-b856-1271f2e5048a", "fitness": "-inf", "name": "SOM_DE", "description": "A Differential Evolution variant incorporating a self-organizing map (SOM) for population topology and migration, enhancing exploration and exploitation based on cluster characteristics.", "code": "import numpy as np\nfrom minisom import MiniSom\n\nclass SOM_DE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, som_grid_size=10, migration_probability=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.som_grid_size = som_grid_size\n        self.migration_probability = migration_probability\n        self.population = None\n        self.fitness = None\n        self.som = None\n        self.F = 0.5\n        self.CR = 0.9\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n\n        # Train the SOM\n        self.som = MiniSom(self.som_grid_size, self.som_grid_size, self.dim, sigma=0.3, learning_rate=0.5)\n        self.som.train(self.population, num_iteration=100) # Reduced iterations, was 1000\n\n    def mutate(self, i):\n        idxs = np.random.choice(self.pop_size, 3, replace=False)\n        a, b, c = idxs\n        x_mutated = self.population[a] + self.F * (self.population[b] - self.population[c])\n        return x_mutated\n\n    def crossover(self, x_mutated, i):\n        x_trial = np.copy(self.population[i])\n        j_rand = np.random.randint(self.dim)\n        for j in range(self.dim):\n            if np.random.rand() < self.CR or j == j_rand:\n                x_trial[j] = x_mutated[j]\n        return x_trial\n\n    def migrate(self):\n        for i in range(self.pop_size):\n            if np.random.rand() < self.migration_probability:\n                # Find the best matching unit (BMU) for the current individual\n                winner = self.som.winner(self.population[i])\n                \n                # Find individuals mapped to the same BMU\n                cluster_indices = [idx for idx in range(self.pop_size) if self.som.winner(self.population[idx]) == winner]\n                \n                if len(cluster_indices) > 1:\n                    # Select a random individual from the same cluster (excluding the current one)\n                    other_index = np.random.choice([idx for idx in cluster_indices if idx != i])\n                    \n                    # Replace the current individual with the other individual from the same cluster\n                    self.population[i] = self.population[other_index].copy()\n                    self.fitness[i] = self.fitness[other_index]\n\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)].copy()\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                x_mutated = self.mutate(i)\n                x_trial = self.crossover(x_mutated, i)\n                x_trial = np.clip(x_trial, func.bounds.lb, func.bounds.ub)\n                \n                f_trial = func(x_trial)\n                self.budget -= 1\n\n                if self.budget <= 0:\n                    break\n\n                if f_trial < self.fitness[i]:\n                    self.population[i] = x_trial.copy()\n                    self.fitness[i] = f_trial\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = x_trial.copy()\n\n            if self.budget <= 0:\n                break\n\n            self.migrate() # Reduced frequency, was every generation\n\n            best_index = np.argmin(self.fitness)\n            if self.fitness[best_index] < self.f_opt:\n                self.f_opt = self.fitness[best_index]\n                self.x_opt = self.population[best_index].copy()\n\n            # Retrain SOM periodically\n            if self.budget % (self.pop_size*5) == 0: #reduced frequency\n                self.som.train(self.population, num_iteration=100)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 10, "feedback": "In the code, line 2, in <module>, the following error occurred:\nModuleNotFoundError: No module named 'minisom'\nOn line: from minisom import MiniSom", "error": "In the code, line 2, in <module>, the following error occurred:\nModuleNotFoundError: No module named 'minisom'\nOn line: from minisom import MiniSom", "parent_ids": ["3c377e79-26ed-41cf-8828-7863aaef3453"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "7209ac5a-e9fa-495f-b21e-cdee6f2018e1", "fitness": 0.42839655900611306, "name": "SimplifiedSelfAdaptiveDE", "description": "A simplified self-adaptive differential evolution with a population-weighted mutation strategy and adaptive CR/F parameters based on successful generations.", "code": "import numpy as np\n\nclass SimplifiedSelfAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=40):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.population = None\n        self.fitness = None\n        self.F = 0.5 * np.ones(pop_size)\n        self.CR = 0.7 * np.ones(pop_size)\n        self.archive = []\n        self.archive_size = int(pop_size * 2)\n        self.success_F = []\n        self.success_CR = []\n        self.weights = np.ones(pop_size) / pop_size\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        self.best_idx = np.argmin(self.fitness)\n\n    def mutate(self, i):\n        idxs = np.random.choice(self.pop_size, 4, replace=False, p=self.weights)\n        a, b, c, d = idxs\n        x_mutated = self.population[i] + self.F[i] * (self.population[a] - self.population[b]) + self.F[i] * (self.population[c] - self.population[d])\n        return x_mutated\n\n    def crossover(self, x_mutated, i):\n        x_trial = np.copy(self.population[i])\n        j_rand = np.random.randint(self.dim)\n        for j in range(self.dim):\n            if np.random.rand() < self.CR[i] or j == j_rand:\n                x_trial[j] = x_mutated[j]\n        return x_trial\n\n    def update_archive(self, x):\n        self.archive.append(x)\n        if len(self.archive) > self.archive_size:\n            self.archive.pop(0)\n\n    def update_parameters(self):\n        if self.success_F:\n            self.F = np.clip(np.random.normal(np.mean(self.success_F), 0.1, self.pop_size), 0.1, 1.0)\n            self.CR = np.clip(np.random.normal(np.mean(self.success_CR), 0.1, self.pop_size), 0.1, 1.0)\n        else:\n            self.F = 0.5 * np.ones(self.pop_size)\n            self.CR = 0.7 * np.ones(self.pop_size)\n\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)].copy()\n\n        generation = 0\n        while self.budget > 0:\n            generation += 1\n            self.success_F = []\n            self.success_CR = []\n            new_fitness = np.copy(self.fitness)\n            new_population = np.copy(self.population)\n\n\n            for i in range(self.pop_size):\n                x_mutated = self.mutate(i)\n                x_trial = self.crossover(x_mutated, i)\n                x_trial = np.clip(x_trial, func.bounds.lb, func.bounds.ub)\n\n                f_trial = func(x_trial)\n                self.budget -= 1\n\n                if self.budget <= 0:\n                    break\n\n                if f_trial < self.fitness[i]:\n                    self.success_F.append(self.F[i])\n                    self.success_CR.append(self.CR[i])\n                    new_population[i] = x_trial.copy()\n                    new_fitness[i] = f_trial\n                    self.update_archive(self.population[i].copy())\n\n\n            self.fitness = new_fitness\n            self.population = new_population\n\n            self.update_parameters()\n\n            self.best_idx = np.argmin(self.fitness)\n            if self.fitness[self.best_idx] < self.f_opt:\n                self.f_opt = self.fitness[self.best_idx]\n                self.x_opt = self.population[self.best_idx].copy()\n\n            self.weights = np.exp(-((self.fitness - np.min(self.fitness)) / (np.max(self.fitness) - np.min(self.fitness) + 1e-8)))\n            self.weights /= np.sum(self.weights)\n\n            if self.budget <= 0:\n                break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 10, "feedback": "The algorithm SimplifiedSelfAdaptiveDE scored 0.428 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["3c377e79-26ed-41cf-8828-7863aaef3453"], "operator": null, "metadata": {"aucs": [0.1511891737262453, 0.35012863991849064, 0.4170654640296717, 0.4215402356992166, 0.31043503860651367, 0.4914438623792561, 0.2921710254933585, 0.3514755625374314, 0.3498723297598616, 0.2076939780545023, 0.6558177070946019, 0.9918643285163433, 0.45241594681226904, 0.3176640028782415, 0.7393542841501826, 0.49654933445954963, 0.33428011255707957, 0.5569189939243219, 0.19003558597893488, 0.4900155735461904]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "2f1d2588-7857-4646-91ad-17c5613e3b72", "fitness": "-inf", "name": "EnhancedAdaptiveDE", "description": "Adaptive Differential Evolution with orthogonal learning, covariance matrix adaptation for mutation, and a self-regulating archive to improve exploration and exploitation.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=5, f_base=0.5, cr_base=0.9, diversity_threshold=1e-5, rejuvenation_rate=0.05, orthogonal_learning_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.population = None\n        self.fitness = None\n        self.archive = []\n        self.memory_f = [f_base] * archive_size\n        self.memory_cr = [cr_base] * archive_size\n        self.sf = []\n        self.scr = []\n        self.diversity_threshold = diversity_threshold\n        self.rejuvenation_rate = rejuvenation_rate\n        self.f_base = f_base\n        self.cr_base = cr_base\n        self.orthogonal_learning_rate = orthogonal_learning_rate\n        self.C = np.eye(dim)  # Covariance matrix for CMA-ES-like mutation\n        self.learning_rate_cma = 0.1\n        self.success_ratio = 0.0  # Track success ratio for CMA-ES-like adaptation\n        self.success_history = []\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        self.best_idx = np.argmin(self.fitness)\n        self.best_fitness = self.fitness[self.best_idx]\n        self.best_solution = self.population[self.best_idx].copy()\n\n\n    def mutate(self, i):\n        phi_idx = np.random.randint(0, self.archive_size)\n        F = np.clip(np.random.normal(self.memory_f[phi_idx], 0.1), 0.1, 1.0)\n        CR = np.clip(np.random.normal(self.memory_cr[phi_idx], 0.1), 0.0, 1.0)\n\n\n        idxs = np.random.choice(self.pop_size, 3, replace=False, size=3 if self.pop_size > 3 else self.pop_size)\n        a, b, c = idxs[0], idxs[1], idxs[2] if self.pop_size > 3 else (idxs[0] if self.pop_size > 1 else i)\n\n        # Diversity-enhanced mutation using best solution\n        # And CMA-ES-like mutation using covariance matrix\n        z = np.random.multivariate_normal(np.zeros(self.dim), self.C)\n        x_mutated = self.population[i] + F * (self.best_solution - self.population[i]) + F * (self.population[a] - self.population[b]) + 0.1 * z\n\n\n        return x_mutated, F, CR\n\n    def orthogonal_learning(self, x, func):\n        # Simple orthogonal array-based learning\n        d = self.dim\n        level = 3  # Reduced level to avoid excessive function calls\n        oa = self.generate_orthogonal_array(d, level)\n        points = np.zeros((level, d))\n        for i in range(level):\n            points[i, :] = x.copy()\n            for j in range(d):\n                points[i, j] = func.bounds.lb + (func.bounds.ub - func.bounds.lb) * oa[i, j] / (level - 1)\n        fitnesses = [func(point) for point in points]\n        self.budget -= level\n        best_index = np.argmin(fitnesses)\n        if fitnesses[best_index] < func(x):\n            return points[best_index, :], fitnesses[best_index]\n        else:\n            return x, func(x)\n\n\n    def generate_orthogonal_array(self, dimensions, level):\n        # Simplified OA generation (L9 array) for demonstration\n        oa = np.array([[0, 0, 0, 0],\n                       [0, 1, 1, 1],\n                       [0, 2, 2, 2],\n                       [1, 0, 1, 2],\n                       [1, 1, 2, 0],\n                       [1, 2, 0, 1],\n                       [2, 0, 2, 1],\n                       [2, 1, 0, 2],\n                       [2, 2, 1, 0]])\n        oa = np.tile(oa, (1, (dimensions + 3) // 4))[:, :dimensions]  # Expanding dimensions if necessary\n        return oa[:level]\n\n\n    def crossover(self, x_mutated, i, cr):\n        x_trial = np.copy(self.population[i])\n        j_rand = np.random.randint(self.dim)\n        for j in range(self.dim):\n            if np.random.rand() < cr or j == j_rand:\n                x_trial[j] = x_mutated[j]\n        return x_trial\n\n    def repair(self, x_trial, func):\n        x_trial = np.clip(x_trial, func.bounds.lb, func.bounds.ub)\n        return x_trial\n\n    def update_memory(self):\n        if self.sf:\n            self.memory_f.append(sum(val**2 for val in self.sf) / sum(self.sf) if sum(self.sf) > 0 else self.f_base)\n            self.memory_f.pop(0)\n            self.memory_cr.append(np.mean(self.scr))\n            self.memory_cr.pop(0)\n        self.sf = []\n        self.scr = []\n\n    def rejuvenate_population(self, func):\n        # Rejuvenate a portion of the population to enhance exploration\n        num_to_rejuvenate = int(self.rejuvenation_rate * self.pop_size)\n        indices_to_rejuvenate = np.random.choice(self.pop_size, num_to_rejuvenate, replace=False)\n        for i in indices_to_rejuvenate:\n            self.population[i] = np.random.uniform(func.bounds.lb, func.bounds.ub)\n            self.fitness[i] = func(self.population[i])\n            self.budget -= 1\n\n    def update_covariance_matrix(self, success):\n        self.success_history.append(success)\n        if len(self.success_history) > 10:  # Track last 10 iterations\n            self.success_history.pop(0)\n        self.success_ratio = np.mean(self.success_history)\n        \n        # Adjust learning rate\n        lr_c = self.learning_rate_cma * (self.success_ratio < 0.2 or self.success_ratio > 0.8) # Reduce when extreme\n\n        if success:\n            delta = (self.population[self.best_idx] - self.population[np.random.choice(self.pop_size)]) #Difference vector\n            self.C = (1 - lr_c) * self.C + lr_c * np.outer(delta, delta)\n        else:\n            self.C = (1 - lr_c) * self.C + lr_c * np.eye(self.dim)  # Dampen when no success\n\n        # Ensure positive definiteness\n        try:\n            np.linalg.cholesky(self.C)\n        except np.linalg.LinAlgError:\n            self.C = self.C + 1e-6 * np.eye(self.dim) # Add a small diagonal to ensure positive definiteness\n\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.budget > 0:\n            self.sf = []\n            self.scr = []\n            for i in range(self.pop_size):\n                x_mutated, F, CR = self.mutate(i)\n                x_trial = self.crossover(x_mutated, i, CR)\n                x_trial = self.repair(x_trial, func)\n\n                f_trial = func(x_trial)\n                self.budget -= 1\n                if self.budget <= 0:\n                    break\n\n                if f_trial < self.fitness[i]:\n                    self.sf.append(F)\n                    self.scr.append(CR)\n\n                    # Archive update\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(self.population[i].copy())\n                    else:\n                        j = np.random.randint(0, self.archive_size)\n                        self.archive[j] = self.population[i].copy()\n\n                    self.population[i] = x_trial.copy()\n                    self.fitness[i] = f_trial\n\n                    if f_trial < self.best_fitness:\n                        self.best_fitness = f_trial\n                        self.best_solution = x_trial.copy()\n                        self.best_idx = i\n                        self.update_covariance_matrix(True) #Success in this generation\n\n                else:\n                      self.update_covariance_matrix(False)\n\n                # Orthogonal learning\n                if np.random.rand() < self.orthogonal_learning_rate and self.budget > 9: #Reduced level requires at least 9 evaluations\n                    x_ol, f_ol = self.orthogonal_learning(self.population[i].copy(), func)\n                    if f_ol < self.fitness[i]:\n                        self.population[i] = x_ol\n                        self.fitness[i] = f_ol\n                        if f_ol < self.best_fitness:\n                            self.best_fitness = f_ol\n                            self.best_solution = x_ol.copy()\n                            self.best_idx = i\n\n\n\n            if self.budget <= 0:\n                break\n\n            self.update_memory()\n\n            # Population rejuvenation based on diversity\n            if np.std(self.fitness) < self.diversity_threshold:\n                self.rejuvenate_population(func)\n                self.best_idx = np.argmin(self.fitness)\n                self.best_fitness = self.fitness[self.best_idx]\n                self.best_solution = self.population[self.best_idx].copy()\n\n\n\n\n        return self.best_fitness, self.best_solution", "configspace": "", "generation": 10, "feedback": "In the code, line 860, in numpy.random.mtrand.RandomState.choice, the following error occurred:\nTypeError: choice() got multiple values for keyword argument 'size'", "error": "In the code, line 860, in numpy.random.mtrand.RandomState.choice, the following error occurred:\nTypeError: choice() got multiple values for keyword argument 'size'", "parent_ids": ["9bfc7670-142f-4e52-831e-44b1953ac659"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "39a16587-8904-4204-98d1-1cab7198b37c", "fitness": "-inf", "name": "EnhancedAdaptiveDE", "description": "An enhanced Adaptive Differential Evolution with dynamic population sizing, archive-based learning using a weighted average recombination, stagnation detection and a migration strategy utilizing information from multiple top-performing solutions, enhancing both exploration and exploitation.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=10, stagnation_threshold=50, migration_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.stagnation_threshold = stagnation_threshold\n        self.stagnation_counter = 0\n        self.best_fitness_history = []\n        self.pop_factor = 1.0\n        self.archive = []\n        self.memory_f = [0.5] * archive_size\n        self.memory_cr = [0.9] * archive_size\n        self.sf = []\n        self.scr = []\n        self.population = None\n        self.fitness = None\n        self.migration_rate = migration_rate\n\n    def initialize_population(self, func):\n        current_pop_size = int(self.pop_size * self.pop_factor)\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(current_pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= current_pop_size\n\n    def mutate(self, i):\n        current_pop_size = int(self.pop_size * self.pop_factor)\n        phi_idx = np.random.randint(0, self.archive_size)\n        F = np.clip(np.random.normal(self.memory_f[phi_idx], 0.1), 0.1, 1.0)\n        CR = np.clip(np.random.normal(self.memory_cr[phi_idx], 0.1), 0.0, 1.0)\n\n        idxs = np.random.choice(current_pop_size, 3, replace=False)\n        a, b, c = idxs[0], idxs[1], idxs[2]\n\n        # Incorporate information from the top 20% performing solutions\n        top_indices = np.argsort(self.fitness)[:max(1, int(0.2 * current_pop_size))]\n        p_best_idx = np.random.choice(top_indices)\n\n        # Weighted average recombination with multiple top solutions\n        num_top = min(5, len(top_indices))\n        top_solutions = self.population[top_indices[:num_top]]\n        weights = np.random.rand(num_top)\n        weights /= np.sum(weights)\n        weighted_avg = np.sum(top_solutions * weights[:, np.newaxis], axis=0)\n\n        x_mutated = self.population[i] + F * (weighted_avg - self.population[i]) + F * (self.population[a] - self.population[b])\n        return x_mutated, F, CR\n\n    def crossover(self, x_mutated, i, cr):\n        x_trial = np.copy(self.population[i])\n        j_rand = np.random.randint(self.dim)\n        for j in range(self.dim):\n            if np.random.rand() < cr or j == j_rand:\n                x_trial[j] = x_mutated[j]\n        return x_trial\n\n    def update_memory(self):\n        if self.sf:\n            self.memory_f.append(sum(val**2 for val in self.sf) / sum(self.sf) if sum(self.sf) != 0 else self.memory_f[-1])\n            self.memory_f.pop(0)\n            self.memory_cr.append(np.mean(self.scr))\n            self.memory_cr.pop(0)\n        self.sf = []\n        self.scr = []\n\n    def check_stagnation(self):\n        if len(self.best_fitness_history) > self.stagnation_threshold:\n            if np.std(self.best_fitness_history[-self.stagnation_threshold:]) < 1e-6:\n                self.stagnation_counter += 1\n                return True\n            else:\n                self.stagnation_counter = 0\n                return False\n        return False\n\n    def restart_population(self, func):\n        self.pop_factor *= 0.75  # Reduce population size\n        self.pop_factor = max(0.2, self.pop_factor) # Prevent too small pop size\n        self.initialize_population(func)\n        self.best_fitness_history = []\n        self.memory_f = [0.5] * self.archive_size\n        self.memory_cr = [0.9] * self.archive_size\n        self.sf = []\n        self.scr = []\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)].copy()\n        self.stagnation_counter = 0\n\n    def migrate_individuals(self):\n        current_pop_size = int(self.pop_size * self.pop_factor)\n        num_migrate = int(self.migration_rate * current_pop_size)\n        if num_migrate > 0 and len(self.archive) > 0:\n            indices_to_replace = np.random.choice(current_pop_size, num_migrate, replace=False)\n            archive_indices = np.random.choice(len(self.archive), num_migrate, replace=True)\n            for i, archive_idx in zip(indices_to_replace, archive_indices):\n                self.population[i] = self.archive[archive_idx].copy()\n                # Re-evaluate fitness after migration\n                #self.fitness[i] = func(self.population[i])  # Requires func to be accessible here!\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)].copy()\n        self.sf = []\n        self.scr = []\n\n        while self.budget > 0:\n            current_pop_size = int(self.pop_size * self.pop_factor)\n            for i in range(current_pop_size):\n                x_mutated, F, CR = self.mutate(i)\n                x_trial = self.crossover(x_mutated, i, CR)\n                x_trial = np.clip(x_trial, func.bounds.lb, func.bounds.ub)\n                f_trial = func(x_trial)\n                self.budget -= 1\n                if self.budget <= 0:\n                    break\n\n                if f_trial < self.fitness[i]:\n                    self.sf.append(F)\n                    self.scr.append(CR)\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(self.population[i].copy())\n                    else:\n                        j = np.random.randint(0, self.archive_size)\n                        self.archive[j] = self.population[i].copy()\n                    self.population[i] = x_trial.copy()\n                    self.fitness[i] = f_trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = x_trial.copy()\n\n            if self.budget <= 0:\n                break\n\n            self.update_memory()\n            best_index = np.argmin(self.fitness)\n            if self.fitness[best_index] < self.f_opt:\n                self.f_opt = self.fitness[best_index]\n                self.x_opt = self.population[best_index].copy()\n\n            self.best_fitness_history.append(self.f_opt)\n\n            if self.check_stagnation() and self.budget > int(self.pop_size * self.pop_factor) * 2:\n                self.restart_population(func)\n            else:\n                self.migrate_individuals()\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 10, "feedback": "Evaluation timed out after 300 seconds.", "error": "Evaluation timed out after 300 seconds.", "parent_ids": ["ca85b7ad-0f37-47f9-9049-9648e4f1e39a"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "2397d392-ba44-4fea-9c38-af93a8dafdbd", "fitness": 0.4723695509279689, "name": "CooperativeSwarmEnhancedDE", "description": "Cooperative Swarm Enhanced Differential Evolution: Combines the strengths of particle swarm optimization and differential evolution within a cooperative swarm framework, where particles share information and dynamically adjust their search strategies based on individual and swarm performance.", "code": "import numpy as np\n\nclass CooperativeSwarmEnhancedDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, swarm_size=5, inertia_weight=0.7, cognitive_coeff=1.4, social_coeff=1.4, de_mutation_factor=0.8, de_crossover_rate=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.swarm_size = swarm_size\n        self.inertia_weight = inertia_weight\n        self.cognitive_coeff = cognitive_coeff\n        self.social_coeff = social_coeff\n        self.de_mutation_factor = de_mutation_factor\n        self.de_crossover_rate = de_crossover_rate\n        self.population = None\n        self.fitness = None\n        self.velocity = None\n        self.personal_best_position = None\n        self.personal_best_fitness = None\n        self.global_best_position = None\n        self.global_best_fitness = float('inf')\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        self.velocity = np.random.uniform(-1, 1, size=(self.pop_size, self.dim))  # Initialize velocities\n        self.personal_best_position = self.population.copy()\n        self.personal_best_fitness = self.fitness.copy()\n        self.global_best_position = self.population[np.argmin(self.fitness)].copy()\n        self.global_best_fitness = np.min(self.fitness)\n\n    def pso_update(self, i):\n        r1 = np.random.rand(self.dim)\n        r2 = np.random.rand(self.dim)\n        self.velocity[i] = (self.inertia_weight * self.velocity[i] +\n                            self.cognitive_coeff * r1 * (self.personal_best_position[i] - self.population[i]) +\n                            self.social_coeff * r2 * (self.global_best_position - self.population[i]))\n        return self.population[i] + self.velocity[i]\n    \n    def de_mutation(self, i):\n        idxs = np.random.choice(self.pop_size, 3, replace=False)\n        a, b, c = idxs[0], idxs[1], idxs[2]\n        mutated = self.population[a] + self.de_mutation_factor * (self.population[b] - self.population[c])\n        return mutated\n\n    def de_crossover(self, individual, mutated):\n        trial = individual.copy()\n        j_rand = np.random.randint(self.dim)\n        for j in range(self.dim):\n            if np.random.rand() < self.de_crossover_rate or j == j_rand:\n                trial[j] = mutated[j]\n        return trial\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Divide population into swarms\n                swarm_id = i % self.swarm_size\n\n                # Apply PSO update\n                x_trial_pso = self.pso_update(i)\n                x_trial_pso = np.clip(x_trial_pso, func.bounds.lb, func.bounds.ub)\n                f_trial_pso = func(x_trial_pso)\n                self.budget -= 1\n                if self.budget <= 0:\n                    break\n\n                # Apply DE mutation and crossover\n                x_mutated_de = self.de_mutation(i)\n                x_trial_de = self.de_crossover(self.population[i], x_mutated_de)\n                x_trial_de = np.clip(x_trial_de, func.bounds.lb, func.bounds.ub)\n\n                f_trial_de = func(x_trial_de)\n                self.budget -= 1\n                if self.budget <= 0:\n                    break\n                \n                #Select between PSO and DE based on fitness improvement\n                if f_trial_pso < f_trial_de:\n                    x_trial = x_trial_pso\n                    f_trial = f_trial_pso\n                else:\n                    x_trial = x_trial_de\n                    f_trial = f_trial_de\n\n\n                # Update personal best\n                if f_trial < self.personal_best_fitness[i]:\n                    self.personal_best_fitness[i] = f_trial\n                    self.personal_best_position[i] = x_trial.copy()\n\n                # Update global best\n                if f_trial < self.global_best_fitness:\n                    self.global_best_fitness = f_trial\n                    self.global_best_position = x_trial.copy()\n\n                # Update population\n                self.population[i] = x_trial.copy()\n                self.fitness[i] = f_trial\n\n            if self.budget <= 0:\n                break\n\n        return self.global_best_fitness, self.global_best_position", "configspace": "", "generation": 11, "feedback": "The algorithm CooperativeSwarmEnhancedDE scored 0.472 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["9bfc7670-142f-4e52-831e-44b1953ac659"], "operator": null, "metadata": {"aucs": [0.18666145512518073, 0.21359841823441517, 0.5084545068227035, 0.8280815296971479, 0.3850575386803251, 0.6083654392574145, 0.3116174975134016, 0.45421953345298793, 0.5401071635636707, 0.21735323509005144, 0.6349117783706646, 0.9965665820892222, 0.3589411414787682, 0.46303193408575083, 0.645965650326091, 0.61037424053537, 0.402581427520032, 0.3749317445266741, 0.20331374288177562, 0.5032564593077311]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "fa497b47-d9a9-4798-9cc9-76b1dd6b34fa", "fitness": 0.4787296657207344, "name": "AdaptiveDE", "description": "Adaptive Differential Evolution with a self-adaptive strategy for selecting the mutation operator, dynamically adjusting F and CR based on individual success, and incorporating a restart mechanism triggered by stagnation detection.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=5, f_base=0.5, cr_base=0.9, diversity_threshold=1e-5, stagnation_threshold=100, restart_probability=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.population = None\n        self.fitness = None\n        self.archive = []\n        self.memory_f = [f_base] * archive_size\n        self.memory_cr = [cr_base] * archive_size\n        self.sf = []\n        self.scr = []\n        self.diversity_threshold = diversity_threshold\n        self.stagnation_threshold = stagnation_threshold\n        self.f_base = f_base\n        self.cr_base = cr_base\n        self.stagnation_counter = 0\n        self.best_fitness_history = []\n        self.restart_probability = restart_probability\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        self.best_idx = np.argmin(self.fitness)\n        self.best_fitness = self.fitness[self.best_idx]\n        self.best_solution = self.population[self.best_idx].copy()\n        self.best_fitness_history.append(self.best_fitness)\n\n\n    def mutate(self, i):\n        phi_idx = np.random.randint(0, self.archive_size)\n        F = np.clip(np.random.normal(self.memory_f[phi_idx], 0.1), 0.1, 1.0)\n        CR = np.clip(np.random.normal(self.memory_cr[phi_idx], 0.1), 0.0, 1.0)\n\n        idxs = np.random.choice(self.pop_size, 4, replace=False)\n        a, b, c, d = idxs[0], idxs[1], idxs[2], idxs[3]\n\n        # Self-adaptive mutation selection\n        mutation_strategy = np.random.choice([1, 2, 3], p=[0.4, 0.3, 0.3])\n\n        if mutation_strategy == 1:\n            # DE/rand/1\n            x_mutated = self.population[a] + F * (self.population[b] - self.population[c])\n        elif mutation_strategy == 2:\n            # DE/current-to-rand/1\n             x_mutated = self.population[i] + F * (self.population[a] - self.population[i]) + F * (self.population[b] - self.population[c])\n        else:\n            # DE/best/1\n            x_mutated = self.best_solution + F * (self.population[a] - self.population[b])\n\n        return x_mutated, F, CR\n\n    def crossover(self, x_mutated, i, cr):\n        x_trial = np.copy(self.population[i])\n        j_rand = np.random.randint(self.dim)\n        for j in range(self.dim):\n            if np.random.rand() < cr or j == j_rand:\n                x_trial[j] = x_mutated[j]\n        return x_trial\n\n    def repair(self, x_trial, func):\n        x_trial = np.clip(x_trial, func.bounds.lb, func.bounds.ub)\n        return x_trial\n\n    def update_memory(self):\n        if self.sf:\n            self.memory_f.append(sum(val**2 for val in self.sf) / sum(self.sf) if sum(self.sf) > 0 else self.f_base)\n            self.memory_f.pop(0)\n            self.memory_cr.append(np.mean(self.scr))\n            self.memory_cr.pop(0)\n        self.sf = []\n        self.scr = []\n\n    def check_stagnation(self):\n        if len(self.best_fitness_history) > self.stagnation_threshold:\n            if np.std(self.best_fitness_history[-self.stagnation_threshold:]) < self.diversity_threshold:\n                self.stagnation_counter += 1\n            else:\n                self.stagnation_counter = 0\n        else:\n            self.stagnation_counter = 0\n\n        return self.stagnation_counter > self.stagnation_threshold\n\n    def restart_population(self, func):\n         if np.random.rand() < self.restart_probability or self.check_stagnation():\n            self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n            self.fitness = np.array([func(x) for x in self.population])\n            self.budget -= self.pop_size\n            self.best_idx = np.argmin(self.fitness)\n            self.best_fitness = self.fitness[self.best_idx]\n            self.best_solution = self.population[self.best_idx].copy()\n            self.best_fitness_history = [self.best_fitness]\n            self.stagnation_counter = 0\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.budget > 0:\n            self.sf = []\n            self.scr = []\n            for i in range(self.pop_size):\n                x_mutated, F, CR = self.mutate(i)\n                x_trial = self.crossover(x_mutated, i, CR)\n                x_trial = self.repair(x_trial, func)\n\n                f_trial = func(x_trial)\n                self.budget -= 1\n                if self.budget <= 0:\n                    break\n\n                if f_trial < self.fitness[i]:\n                    self.sf.append(F)\n                    self.scr.append(CR)\n\n                    # Archive update\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(self.population[i].copy())\n                    else:\n                        j = np.random.randint(0, self.archive_size)\n                        self.archive[j] = self.population[i].copy()\n\n                    self.population[i] = x_trial.copy()\n                    self.fitness[i] = f_trial\n\n                    if f_trial < self.best_fitness:\n                        self.best_fitness = f_trial\n                        self.best_solution = x_trial.copy()\n                        self.best_idx = i\n\n            if self.budget <= 0:\n                break\n\n            self.update_memory()\n            self.best_fitness_history.append(self.best_fitness)\n\n            self.restart_population(func)\n\n\n        return self.best_fitness, self.best_solution", "configspace": "", "generation": 11, "feedback": "The algorithm AdaptiveDE scored 0.479 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["9bfc7670-142f-4e52-831e-44b1953ac659"], "operator": null, "metadata": {"aucs": [0.18061031768745017, 0.24322638803078034, 0.5263459371978089, 0.7335361240229668, 0.37819575488896573, 0.5759398654901013, 0.3462474213054464, 0.42800459338670993, 0.41276159764605613, 0.2253123238441006, 0.42706433969429813, 0.9966153414944801, 0.33778744841622943, 0.5004952710308238, 0.915140683850147, 0.46332128398624384, 0.4023042045174513, 0.7632947738051588, 0.21932214601866118, 0.49906749810080786]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "ed62b737-97e6-4a36-93eb-da06d2d58f75", "fitness": 0.7377620930770873, "name": "EnhancedSimplifiedAdaptiveDE", "description": "An enhanced self-adaptive Differential Evolution with a mirrored population for intensified local search and an improved stagnation detection mechanism.", "code": "import numpy as np\n\nclass EnhancedSimplifiedAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=10, mirror_ratio = 0.2):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.mirror_ratio = int(mirror_ratio * pop_size)\n        self.population = None\n        self.fitness = None\n        self.memory_F = np.ones(self.archive_size) * 0.5\n        self.memory_CR = np.ones(self.archive_size) * 0.9\n        self.archive = []\n        self.F = 0.5\n        self.CR = 0.9\n        self.p = 0.1  # Probability for choosing F and CR from memory\n        self.min_CR = 0.1\n        self.stagnation_counter = 0\n        self.max_stagnation = 50 #restart if stagnant for longer\n        self.stagnation_fitness_history = [] # track fitness history\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n\n    def mutate(self, i):\n        # Combined mutation: DE/rand/1 OR DE/current-to-best/1\n        if np.random.rand() < 0.5:  # Switch between strategies\n            idxs = np.random.choice(self.pop_size, 5, replace=False)\n            a, b, c, d, e = idxs\n            x_mutated = self.population[a] + self.F * (self.population[b] - self.population[c]) + self.F * (self.population[d] - self.population[e])\n        else:\n            best_idx = np.argmin(self.fitness)\n            idxs = np.random.choice(self.pop_size, 3, replace=False)\n            a, b, c = idxs\n            x_mutated = self.population[i] + self.F * (self.population[best_idx] - self.population[i]) + self.F * (self.population[a] - self.population[b])\n\n        return x_mutated\n\n    def crossover(self, x_mutated, i):\n        x_trial = np.copy(self.population[i])\n        j_rand = np.random.randint(self.dim)\n        for j in range(self.dim):\n            if np.random.rand() < self.CR or j == j_rand:\n                x_trial[j] = x_mutated[j]\n        return x_trial\n\n    def update_parameters(self, success, F, CR):\n         if success:\n            self.archive.append((F, CR))\n            if len(self.archive) > self.archive_size:\n                self.archive.pop(0)\n\n            if len(self.archive) > 0:\n                F_vals = [item[0] for item in self.archive]\n                CR_vals = [item[1] for item in self.archive]\n                self.F = np.mean(F_vals)\n                self.CR = max(self.min_CR, np.mean(CR_vals)) # Ensure CR doesn't get too small\n\n    def check_stagnation(self):\n        self.stagnation_fitness_history.append(self.f_opt)\n        if len(self.stagnation_fitness_history) > self.max_stagnation:\n            self.stagnation_fitness_history.pop(0)\n\n        if len(self.stagnation_fitness_history) == self.max_stagnation:\n            if np.std(self.stagnation_fitness_history) < 1e-6:\n                return True\n\n        return False\n\n    def restart_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        self.stagnation_counter = 0\n        self.archive = []  # Clear the archive on restart\n        self.stagnation_fitness_history = []\n\n\n    def mirrored_sampling(self, func):\n        best_idx = np.argmin(self.fitness)\n        mirrored_points = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.mirror_ratio, self.dim))\n        mirrored_fitness = np.array([func(x) for x in mirrored_points])\n        self.budget -= self.mirror_ratio\n\n        #Replace worst individuals with mirrored samples\n        worst_indices = np.argsort(self.fitness)[-self.mirror_ratio:]\n        for i in range(self.mirror_ratio):\n            if mirrored_fitness[i] < self.fitness[worst_indices[i]]:\n                self.population[worst_indices[i]] = mirrored_points[i].copy()\n                self.fitness[worst_indices[i]] = mirrored_fitness[i]\n\n\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)].copy()\n\n        while self.budget > 0:\n            \n            if self.check_stagnation() and self.budget > self.pop_size:\n                self.restart_population(func)\n                self.f_opt = np.min(self.fitness)\n                self.x_opt = self.population[np.argmin(self.fitness)].copy()\n\n            for i in range(self.pop_size):\n                #Parameter Adaptation\n                if np.random.rand() < self.p and len(self.archive)>0:\n                     idx = np.random.randint(len(self.archive))\n                     F, CR = self.archive[idx]\n                else:\n                     F = self.F\n                     CR = self.CR\n\n                x_mutated = self.mutate(i)\n                x_trial = self.crossover(x_mutated, i)\n                x_trial = np.clip(x_trial, func.bounds.lb, func.bounds.ub)\n\n                f_trial = func(x_trial)\n                self.budget -= 1\n\n                if self.budget <= 0:\n                    break\n\n                if f_trial < self.fitness[i]:\n                    self.update_parameters(True, F, CR)\n                    self.population[i] = x_trial.copy()\n                    self.fitness[i] = f_trial\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = x_trial.copy()\n                else:\n                    self.update_parameters(False, F, CR)\n            \n            if self.budget > self.mirror_ratio:\n                self.mirrored_sampling(func)\n\n            if self.budget <= 0:\n                break\n\n            best_index = np.argmin(self.fitness)\n            if self.fitness[best_index] < self.f_opt:\n                self.f_opt = self.fitness[best_index]\n                self.x_opt = self.population[best_index].copy()\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 11, "feedback": "The algorithm EnhancedSimplifiedAdaptiveDE scored 0.738 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["959042e0-c7fd-4f7d-91da-a41fdc341f98"], "operator": null, "metadata": {"aucs": [0.3003000374476861, 0.7392939103468027, 0.7511080934257781, 0.8967019638758531, 0.7743485964158656, 0.8236178017819944, 0.6309170179432156, 0.7249504462290637, 0.7769965704214293, 0.7100088522197232, 0.8784081187737194, 0.9952896583553204, 0.7365697237373847, 0.7716736357969243, 0.9302150545848294, 0.8079087027629894, 0.6802529696645884, 0.8815530048781453, 0.19405250804714425, 0.7510751948332872]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "a807e5c0-78d2-452c-b78d-21d9958ecd6b", "fitness": 0.3857154203800052, "name": "AdaptiveDEOL", "description": "Adaptive Differential Evolution with orthogonal learning, simplified parameter control, and dynamic population size adjustment for efficient exploration and exploitation.", "code": "import numpy as np\n\nclass AdaptiveDEOL:\n    def __init__(self, budget=10000, dim=10, pop_size=50, stagnation_threshold=50):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.stagnation_threshold = stagnation_threshold\n        self.stagnation_counter = 0\n        self.best_fitness_history = []\n        self.pop_factor = 1.0\n        self.population = None\n        self.fitness = None\n        self.F = 0.5\n        self.CR = 0.9\n\n    def initialize_population(self, func):\n        current_pop_size = int(self.pop_size * self.pop_factor)\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(current_pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= current_pop_size\n\n    def mutate(self, i):\n        current_pop_size = int(self.pop_size * self.pop_factor)\n        idxs = np.random.choice(current_pop_size, 4, replace=False)\n        a, b, c, d = idxs[0], idxs[1], idxs[2], idxs[3]\n\n        # Orthogonal learning: create a trial vector based on differences between individuals\n        x_mutated = self.population[i] + self.F * (self.population[a] - self.population[b] + self.population[c] - self.population[d])\n        return x_mutated\n\n    def crossover(self, x_mutated, i):\n        x_trial = np.copy(self.population[i])\n        j_rand = np.random.randint(self.dim)\n        for j in range(self.dim):\n            if np.random.rand() < self.CR or j == j_rand:\n                x_trial[j] = x_mutated[j]\n        return x_trial\n\n    def check_stagnation(self):\n        if len(self.best_fitness_history) > self.stagnation_threshold:\n            if np.std(self.best_fitness_history[-self.stagnation_threshold:]) < 1e-6:\n                self.stagnation_counter += 1\n                return True\n            else:\n                self.stagnation_counter = 0\n                return False\n        return False\n\n    def restart_population(self, func):\n        self.pop_factor *= 0.75  # Reduce population size\n        self.pop_factor = max(0.2, self.pop_factor)  # Prevent too small pop size\n        self.initialize_population(func)\n        self.best_fitness_history = []\n        self.F = 0.5\n        self.CR = 0.9\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)].copy()\n        self.stagnation_counter = 0\n\n    def update_parameters(self):\n      #Simple adaptive parameter control\n      self.F = np.clip(np.random.normal(0.5, 0.1), 0.1, 1.0)\n      self.CR = np.clip(np.random.normal(0.9, 0.1), 0.0, 1.0)\n\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)].copy()\n\n        while self.budget > 0:\n            current_pop_size = int(self.pop_size * self.pop_factor)\n            for i in range(current_pop_size):\n                x_mutated = self.mutate(i)\n                x_trial = self.crossover(x_mutated, i)\n                x_trial = np.clip(x_trial, func.bounds.lb, func.bounds.ub)\n                f_trial = func(x_trial)\n                self.budget -= 1\n                if self.budget <= 0:\n                    break\n\n                if f_trial < self.fitness[i]:\n                    self.population[i] = x_trial.copy()\n                    self.fitness[i] = f_trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = x_trial.copy()\n\n            if self.budget <= 0:\n                break\n\n            self.update_parameters()\n            best_index = np.argmin(self.fitness)\n            if self.fitness[best_index] < self.f_opt:\n                self.f_opt = self.fitness[best_index]\n                self.x_opt = self.population[best_index].copy()\n\n            self.best_fitness_history.append(self.f_opt)\n\n            if self.check_stagnation() and self.budget > int(self.pop_size * self.pop_factor) * 2:\n                self.restart_population(func)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 11, "feedback": "The algorithm AdaptiveDEOL scored 0.386 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["ca85b7ad-0f37-47f9-9049-9648e4f1e39a"], "operator": null, "metadata": {"aucs": [0.14143075332684274, 0.2504565705189926, 0.36400546417895085, 0.3982817944453477, 0.29551530458426967, 0.4401386557720768, 0.28999940783728195, 0.3286009229574365, 0.31341321359342644, 0.20701953500652603, 0.4985244816188108, 0.9984586450282247, 0.343572903300318, 0.2970424057768203, 0.6992196070487733, 0.3802798499066372, 0.31341047683433965, 0.5050582352538749, 0.1645745830261398, 0.4853055975850159]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "97a44bb2-b415-4e9e-adfc-8667f83d8357", "fitness": "-inf", "name": "EnhancedSimplifiedAdaptiveDE", "description": "Population-based algorithm with self-adaptive parameters and a diversity-promoting selection mechanism, enhanced with a Cauchy mutation operator for broader exploration.", "code": "import numpy as np\n\nclass EnhancedSimplifiedAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.population = None\n        self.fitness = None\n        self.memory_F = np.ones(self.archive_size) * 0.5\n        self.memory_CR = np.ones(self.archive_size) * 0.9\n        self.archive = []\n        self.F = 0.5\n        self.CR = 0.9\n        self.p = 0.1  # Probability for choosing F and CR from memory\n        self.min_CR = 0.1\n        self.stagnation_counter = 0\n        self.max_stagnation = 50 #restart if stagnant for longer\n        self.cauchy_scale = 0.1\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n\n    def mutate(self, i):\n        # Combined mutation: DE/rand/1 OR DE/current-to-best/1, with Cauchy perturbation\n        if np.random.rand() < 0.5:\n            idxs = np.random.choice(self.pop_size, 5, replace=False)\n            a, b, c, d, e = idxs\n            x_mutated = self.population[a] + self.F * (self.population[b] - self.population[c]) + self.F * (self.population[d] - self.population[e])\n        else:\n            best_idx = np.argmin(self.fitness)\n            idxs = np.random.choice(self.pop_size, 3, replace=False)\n            a, b, c = idxs\n            x_mutated = self.population[i] + self.F * (self.population[best_idx] - self.population[i]) + self.F * (self.population[a] - self.population[b])\n\n        # Cauchy mutation for enhanced exploration\n        x_mutated += self.cauchy_scale * np.random.standard_cauchy(size=self.dim)\n        return x_mutated\n\n    def crossover(self, x_mutated, i):\n        x_trial = np.copy(self.population[i])\n        j_rand = np.random.randint(self.dim)\n        for j in range(self.dim):\n            if np.random.rand() < self.CR or j == j_rand:\n                x_trial[j] = x_mutated[j]\n        return x_trial\n\n    def update_parameters(self, success, F, CR):\n         if success:\n            self.archive.append((F, CR))\n            if len(self.archive) > self.archive_size:\n                self.archive.pop(0)\n\n            if len(self.archive) > 0:\n                F_vals = [item[0] for item in self.archive]\n                CR_vals = [item[1] for item in self.archive]\n                self.F = np.mean(F_vals)\n                self.CR = max(self.min_CR, np.mean(CR_vals))\n\n    def check_stagnation(self):\n        if len(set(self.fitness)) <= 1:\n            self.stagnation_counter +=1\n        else:\n            self.stagnation_counter = 0\n\n        if self.stagnation_counter > self.max_stagnation:\n            return True\n        else:\n            return False\n\n    def restart_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        self.stagnation_counter = 0\n        self.archive = []\n\n    def selection(self):\n        # Diversity-promoting selection: Replace worst individual if trial vector is better\n        worst_idx = np.argmax(self.fitness)\n        if self.fitness[best_index] < self.fitness[worst_idx]:\n            self.population[worst_idx] = self.population[best_index].copy()\n            self.fitness[worst_idx] = self.fitness[best_index]\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)].copy()\n        best_index = np.argmin(self.fitness)\n\n\n        while self.budget > 0:\n            \n            if self.check_stagnation() and self.budget > self.pop_size:\n                self.restart_population(func)\n                self.f_opt = np.min(self.fitness)\n                self.x_opt = self.population[np.argmin(self.fitness)].copy()\n\n            for i in range(self.pop_size):\n                #Parameter Adaptation\n                if np.random.rand() < self.p and len(self.archive)>0:\n                     idx = np.random.randint(len(self.archive))\n                     F, CR = self.archive[idx]\n                else:\n                     F = self.F\n                     CR = self.CR\n\n                x_mutated = self.mutate(i)\n                x_trial = self.crossover(x_mutated, i)\n                x_trial = np.clip(x_trial, func.bounds.lb, func.bounds.ub)\n\n                f_trial = func(x_trial)\n                self.budget -= 1\n\n                if self.budget <= 0:\n                    break\n\n                if f_trial < self.fitness[i]:\n                    self.update_parameters(True, F, CR)\n                    self.population[i] = x_trial.copy()\n                    self.fitness[i] = f_trial\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = x_trial.copy()\n                        best_index = i\n\n                else:\n                    self.update_parameters(False, F, CR)\n            \n            if self.budget <= 0:\n                break\n\n            best_index = np.argmin(self.fitness)\n            if self.fitness[best_index] < self.f_opt:\n                self.f_opt = self.fitness[best_index]\n                self.x_opt = self.population[best_index].copy()\n            \n            self.selection()\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 11, "feedback": "In the code, line 84, in selection, the following error occurred:\nNameError: name 'best_index' is not defined\nOn line: if self.fitness[best_index] < self.fitness[worst_idx]:", "error": "In the code, line 84, in selection, the following error occurred:\nNameError: name 'best_index' is not defined\nOn line: if self.fitness[best_index] < self.fitness[worst_idx]:", "parent_ids": ["959042e0-c7fd-4f7d-91da-a41fdc341f98"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "9b006674-3ec9-409a-b034-6d6cfd16d571", "fitness": "-inf", "name": "EnhancedAdaptiveDE", "description": "A Differential Evolution strategy incorporating a dynamic population size adjustment based on success rate, orthogonal learning for enhanced exploration, and a combined mutation strategy with archive utilization and stagnation detection for improved convergence.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=10, stagnation_threshold=50, success_threshold=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.stagnation_threshold = stagnation_threshold\n        self.success_threshold = success_threshold\n        self.stagnation_counter = 0\n        self.best_fitness_history = []\n        self.pop_factor = 1.0\n        self.archive = []\n        self.memory_f = [0.5] * archive_size\n        self.memory_cr = [0.9] * archive_size\n        self.sf = []\n        self.scr = []\n        self.success_rate = 0.0\n        self.population = None\n        self.fitness = None\n\n    def initialize_population(self, func):\n        current_pop_size = int(self.pop_size * self.pop_factor)\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(current_pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= current_pop_size\n\n    def mutate(self, i):\n        current_pop_size = int(self.pop_size * self.pop_factor)\n        phi_idx = np.random.randint(0, self.archive_size)\n        F = np.clip(np.random.normal(self.memory_f[phi_idx], 0.1), 0.1, 1.0)\n        CR = np.clip(np.random.normal(self.memory_cr[phi_idx], 0.1), 0.0, 1.0)\n\n        idxs = np.random.choice(current_pop_size, 3, replace=False)\n        a, b, c = idxs[0], idxs[1], idxs[2]\n\n        # Incorporate information from the top 20% performing solutions\n        top_indices = np.argsort(self.fitness)[:max(1, int(0.2 * current_pop_size))]\n        p_best_idx = np.random.choice(top_indices)\n\n        x_mutated = self.population[i] + F * (self.population[p_best_idx] - self.population[i]) + F * (self.population[a] - self.population[b])\n\n        # Orthogonal Learning\n        orthogonal_vector = np.random.uniform(-1, 1, self.dim)\n        x_mutated += 0.05 * orthogonal_vector  # Scale the orthogonal vector\n        return x_mutated, F, CR\n\n    def crossover(self, x_mutated, i, cr):\n        x_trial = np.copy(self.population[i])\n        j_rand = np.random.randint(self.dim)\n        for j in range(self.dim):\n            if np.random.rand() < cr or j == j_rand:\n                x_trial[j] = x_mutated[j]\n        return x_trial\n\n    def update_memory(self):\n        if self.sf:\n            self.memory_f.append(sum(val**2 for val in self.sf) / sum(self.sf) if sum(self.sf) != 0 else self.memory_f[-1])\n            self.memory_f.pop(0)\n            self.memory_cr.append(np.mean(self.scr))\n            self.memory_cr.pop(0)\n        self.sf = []\n        self.scr = []\n\n    def check_stagnation(self):\n        if len(self.best_fitness_history) > self.stagnation_threshold:\n            if np.std(self.best_fitness_history[-self.stagnation_threshold:]) < 1e-6:\n                self.stagnation_counter += 1\n                return True\n            else:\n                self.stagnation_counter = 0\n                return False\n        return False\n    \n    def adjust_population_size(self):\n        if self.success_rate > self.success_threshold and self.pop_factor < 2.0:\n            self.pop_factor *= 1.1  # Increase population size if successful\n        elif self.success_rate < self.success_threshold and self.pop_factor > 0.2:\n            self.pop_factor *= 0.9  # Decrease population size if unsuccessful\n        self.pop_factor = np.clip(self.pop_factor, 0.2, 2.0)\n\n    def restart_population(self, func):\n        self.pop_factor *= 0.75  # Reduce population size\n        self.pop_factor = max(0.2, self.pop_factor) # Prevent too small pop size\n        self.initialize_population(func)\n        self.best_fitness_history = []\n        self.memory_f = [0.5] * self.archive_size\n        self.memory_cr = [0.9] * self.archive_size\n        self.sf = []\n        self.scr = []\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)].copy()\n        self.stagnation_counter = 0\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)].copy()\n        self.sf = []\n        self.scr = []\n        success_count = 0\n\n        while self.budget > 0:\n            current_pop_size = int(self.pop_size * self.pop_factor)\n            for i in range(current_pop_size):\n                x_mutated, F, CR = self.mutate(i)\n                x_trial = self.crossover(x_mutated, i, CR)\n                x_trial = np.clip(x_trial, func.bounds.lb, func.bounds.ub)\n                f_trial = func(x_trial)\n                self.budget -= 1\n                if self.budget <= 0:\n                    break\n\n                if f_trial < self.fitness[i]:\n                    self.sf.append(F)\n                    self.scr.append(CR)\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(self.population[i].copy())\n                    else:\n                        j = np.random.randint(0, self.archive_size)\n                        self.archive[j] = self.population[i].copy()\n                    self.population[i] = x_trial.copy()\n                    self.fitness[i] = f_trial\n                    success_count += 1\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = x_trial.copy()\n\n            if self.budget <= 0:\n                break\n                \n            self.success_rate = success_count / current_pop_size\n            success_count = 0 #reset success count\n            self.adjust_population_size() #adjust pop size\n\n            self.update_memory()\n            best_index = np.argmin(self.fitness)\n            if self.fitness[best_index] < self.f_opt:\n                self.f_opt = self.fitness[best_index]\n                self.x_opt = self.population[best_index].copy()\n\n            self.best_fitness_history.append(self.f_opt)\n\n            if self.check_stagnation() and self.budget > int(self.pop_size * self.pop_factor) * 2:\n                self.restart_population(func)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 11, "feedback": "In the code, line 42, in mutate, the following error occurred:\nIndexError: index 52 is out of bounds for axis 0 with size 50\nOn line: x_mutated = self.population[i] + F * (self.population[p_best_idx] - self.population[i]) + F * (self.population[a] - self.population[b])", "error": "In the code, line 42, in mutate, the following error occurred:\nIndexError: index 52 is out of bounds for axis 0 with size 50\nOn line: x_mutated = self.population[i] + F * (self.population[p_best_idx] - self.population[i]) + F * (self.population[a] - self.population[b])", "parent_ids": ["ca85b7ad-0f37-47f9-9049-9648e4f1e39a"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "b9b9465e-c535-4a3a-911c-bb80483be837", "fitness": 0.783505163917563, "name": "SimplifiedAdaptiveDE", "description": "Simplified Adaptive Differential Evolution with adaptive F/CR, best-member-based mutation, and periodic restarts, focusing on balancing exploration and exploitation without an archive.", "code": "import numpy as np\n\nclass SimplifiedAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, f_base=0.5, cr_base=0.9, restart_trigger=0.1, restart_probability=0.05):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.population = None\n        self.fitness = None\n        self.best_idx = None\n        self.best_fitness = float('inf')\n        self.best_solution = None\n        self.f_base = f_base\n        self.cr_base = cr_base\n        self.sf = []\n        self.scr = []\n        self.restart_trigger = restart_trigger\n        self.restart_probability = restart_probability\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        self.best_idx = np.argmin(self.fitness)\n        self.best_fitness = self.fitness[self.best_idx]\n        self.best_solution = self.population[self.best_idx].copy()\n\n    def mutate(self, i):\n        F = np.clip(np.random.normal(self.f_base, 0.1), 0.1, 1.0)\n        CR = np.clip(np.random.normal(self.cr_base, 0.1), 0.0, 1.0)\n\n        idxs = np.random.choice(self.pop_size, 2, replace=False)\n        a, b = idxs[0], idxs[1]\n\n        # Mutation using best solution and two other individuals\n        x_mutated = self.population[i] + F * (self.best_solution - self.population[i]) + F * (self.population[a] - self.population[b])\n\n        return x_mutated, F, CR\n\n    def crossover(self, x_mutated, i, cr):\n        x_trial = np.copy(self.population[i])\n        j_rand = np.random.randint(self.dim)\n        for j in range(self.dim):\n            if np.random.rand() < cr or j == j_rand:\n                x_trial[j] = x_mutated[j]\n        return x_trial\n\n    def repair(self, x_trial, func):\n        x_trial = np.clip(x_trial, func.bounds.lb, func.bounds.ub)\n        return x_trial\n\n    def update_parameters(self):\n        if self.sf:\n            self.f_base = sum(val**2 for val in self.sf) / sum(self.sf) if sum(self.sf) > 0 else 0.5\n            self.cr_base = np.mean(self.scr)\n        self.sf = []\n        self.scr = []\n\n    def maybe_restart(self, func):\n        if np.random.rand() < self.restart_probability:\n            # Restart a portion of the population around the best solution\n            num_to_restart = int(self.restart_trigger * self.pop_size)\n            indices_to_restart = np.random.choice(self.pop_size, num_to_restart, replace=False)\n            for i in indices_to_restart:\n                self.population[i] = self.best_solution + np.random.uniform(-1.0, 1.0, size=self.dim) * (func.bounds.ub - func.bounds.lb) * 0.1 # Smaller range for restart\n                self.population[i] = self.repair(self.population[i], func)\n                self.fitness[i] = func(self.population[i])\n                self.budget -= 1\n\n                if self.fitness[i] < self.best_fitness:\n                    self.best_fitness = self.fitness[i]\n                    self.best_solution = self.population[i].copy()\n                    self.best_idx = i\n            \n\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.budget > 0:\n            self.sf = []\n            self.scr = []\n            for i in range(self.pop_size):\n                x_mutated, F, CR = self.mutate(i)\n                x_trial = self.crossover(x_mutated, i, CR)\n                x_trial = self.repair(x_trial, func)\n\n                f_trial = func(x_trial)\n                self.budget -= 1\n                if self.budget <= 0:\n                    break\n\n                if f_trial < self.fitness[i]:\n                    self.sf.append(F)\n                    self.scr.append(CR)\n                    self.population[i] = x_trial.copy()\n                    self.fitness[i] = f_trial\n\n                    if f_trial < self.best_fitness:\n                        self.best_fitness = f_trial\n                        self.best_solution = x_trial.copy()\n                        self.best_idx = i\n\n            if self.budget <= 0:\n                break\n\n            self.update_parameters()\n            self.maybe_restart(func) # Probabilistic restart\n\n\n\n        return self.best_fitness, self.best_solution", "configspace": "", "generation": 11, "feedback": "The algorithm SimplifiedAdaptiveDE scored 0.784 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["9bfc7670-142f-4e52-831e-44b1953ac659"], "operator": null, "metadata": {"aucs": [0.37576624492512145, 0.8219241466526983, 0.802909375001561, 0.9416892938735002, 0.8477997002286052, 0.8555824691712647, 0.6730191667258615, 0.7827685057030741, 0.7482228434334692, 0.7554260953799004, 0.9213000205887835, 0.9952470267475013, 0.8000224442200046, 0.8209394636849513, 0.9613821400451638, 0.8430310408773185, 0.7415810802559168, 0.9054106783751488, 0.5675921216344377, 0.5084894208269759]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "a94bd6b9-44c9-4461-a256-ae070ec2c085", "fitness": "-inf", "name": "AdaptiveDE", "description": "Adaptive Differential Evolution with covariance matrix adaptation (CMA) inspired mutation and orthogonal crossover for enhanced exploration and exploitation.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=5, f_base=0.5, cr_base=0.9, diversity_threshold=1e-5, rejuvenation_rate=0.05, cma_learning_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.population = None\n        self.fitness = None\n        self.archive = []\n        self.memory_f = [f_base] * archive_size\n        self.memory_cr = [cr_base] * archive_size\n        self.sf = []\n        self.scr = []\n        self.diversity_threshold = diversity_threshold\n        self.rejuvenation_rate = rejuvenation_rate\n        self.f_base = f_base\n        self.cr_base = cr_base\n        self.cma_learning_rate = cma_learning_rate\n        self.C = np.eye(dim)  # Covariance matrix for CMA-like mutation\n        self.mean = None  # Mean of the population (used in CMA)\n\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        self.best_idx = np.argmin(self.fitness)\n        self.best_fitness = self.fitness[self.best_idx]\n        self.best_solution = self.population[self.best_idx].copy()\n        self.mean = np.mean(self.population, axis=0)\n\n\n    def mutate(self, i):\n        phi_idx = np.random.randint(0, self.archive_size)\n        F = np.clip(np.random.normal(self.memory_f[phi_idx], 0.1), 0.1, 1.0)\n        CR = np.clip(np.random.normal(self.memory_cr[phi_idx], 0.1), 0.0, 1.0)\n\n\n        idxs = np.random.choice(self.pop_size, 3, replace=False)\n        a, b, c = idxs[0], idxs[1], idxs[2]\n\n        # CMA-inspired mutation\n        z = np.random.multivariate_normal(np.zeros(self.dim), self.C)\n        x_mutated = self.population[i] + F * (self.best_solution - self.population[i]) + F * z\n\n        return x_mutated, F, CR\n\n    def crossover(self, x_mutated, i, cr):\n       # Orthogonal Crossover\n        num_vectors = 2\n        indices = np.random.choice(self.pop_size, num_vectors, replace=False)\n        vectors = self.population[indices]\n\n        # Create orthogonal basis\n        Q, _ = np.linalg.qr(np.vstack([vectors - self.population[i] for vectors in vectors]))\n        \n        # Generate trial vector using orthogonal basis\n        trial_vector = self.population[i] + np.sum(np.random.rand(num_vectors)[:, np.newaxis] * Q[:num_vectors], axis=0)\n\n        x_trial = np.copy(self.population[i])\n        j_rand = np.random.randint(self.dim)\n        for j in range(self.dim):\n            if np.random.rand() < cr or j == j_rand:\n                x_trial[j] = trial_vector[j]\n        return x_trial\n\n\n    def repair(self, x_trial, func):\n        x_trial = np.clip(x_trial, func.bounds.lb, func.bounds.ub)\n        return x_trial\n\n    def update_memory(self):\n        if self.sf:\n            self.memory_f.append(sum(val**2 for val in self.sf) / sum(self.sf) if sum(self.sf) > 0 else self.f_base)\n            self.memory_f.pop(0)\n            self.memory_cr.append(np.mean(self.scr))\n            self.memory_cr.pop(0)\n        self.sf = []\n        self.scr = []\n\n    def rejuvenate_population(self, func):\n        # Rejuvenate a portion of the population to enhance exploration\n        num_to_rejuvenate = int(self.rejuvenation_rate * self.pop_size)\n        indices_to_rejuvenate = np.random.choice(self.pop_size, num_to_rejuvenate, replace=False)\n        for i in indices_to_rejuvenate:\n            self.population[i] = np.random.uniform(func.bounds.lb, func.bounds.ub)\n            self.fitness[i] = func(self.population[i])\n            self.budget -= 1\n\n    def update_covariance_matrix(self):\n        # Update covariance matrix using successful mutations\n        if self.sf:\n            delta = self.best_solution - self.mean\n            self.C = (1 - self.cma_learning_rate) * self.C + self.cma_learning_rate * np.outer(delta, delta)\n            self.mean = self.best_solution.copy()\n\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.budget > 0:\n            self.sf = []\n            self.scr = []\n            for i in range(self.pop_size):\n                x_mutated, F, CR = self.mutate(i)\n                x_trial = self.crossover(x_mutated, i, CR)\n                x_trial = self.repair(x_trial, func)\n\n                f_trial = func(x_trial)\n                self.budget -= 1\n                if self.budget <= 0:\n                    break\n\n                if f_trial < self.fitness[i]:\n                    self.sf.append(F)\n                    self.scr.append(CR)\n\n                    # Archive update\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(self.population[i].copy())\n                    else:\n                        j = np.random.randint(0, self.archive_size)\n                        self.archive[j] = self.population[i].copy()\n\n                    self.population[i] = x_trial.copy()\n                    self.fitness[i] = f_trial\n\n                    if f_trial < self.best_fitness:\n                        self.best_fitness = f_trial\n                        self.best_solution = x_trial.copy()\n                        self.best_idx = i\n\n            if self.budget <= 0:\n                break\n\n            self.update_memory()\n            self.update_covariance_matrix()\n\n            # Population rejuvenation based on diversity\n            if np.std(self.fitness) < self.diversity_threshold:\n                self.rejuvenate_population(func)\n                self.best_idx = np.argmin(self.fitness)\n                self.best_fitness = self.fitness[self.best_idx]\n                self.best_solution = self.population[self.best_idx].copy()\n\n\n\n\n        return self.best_fitness, self.best_solution", "configspace": "", "generation": 11, "feedback": "In the code, line 60, in crossover, the following error occurred:\nValueError: operands could not be broadcast together with shapes (5,) (2,) \nOn line: trial_vector = self.population[i] + np.sum(np.random.rand(num_vectors)[:, np.newaxis] * Q[:num_vectors], axis=0)", "error": "In the code, line 60, in crossover, the following error occurred:\nValueError: operands could not be broadcast together with shapes (5,) (2,) \nOn line: trial_vector = self.population[i] + np.sum(np.random.rand(num_vectors)[:, np.newaxis] * Q[:num_vectors], axis=0)", "parent_ids": ["9bfc7670-142f-4e52-831e-44b1953ac659"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "3e4133c6-0aca-49eb-8fe6-bc36704cb36f", "fitness": "-inf", "name": "ExplorationEnhancedDE", "description": "Exploration-Enhanced Differential Evolution with decaying exploration rate, adaptive population resizing based on success rate, and a niching mechanism to maintain diversity.", "code": "import numpy as np\n\nclass ExplorationEnhancedDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, exploration_rate=0.5, decay_rate=0.99, niche_radius=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.exploration_rate = exploration_rate\n        self.decay_rate = decay_rate\n        self.niche_radius = niche_radius\n        self.population = None\n        self.fitness = None\n        self.pop_factor = 1.0\n        self.success_rate = 0.5\n        self.success_history = []\n        self.best_fitness = float('inf')\n        self.best_solution = None\n\n    def initialize_population(self, func):\n        current_pop_size = int(self.pop_size * self.pop_factor)\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(current_pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= current_pop_size\n\n    def mutate(self, i):\n        current_pop_size = int(self.pop_size * self.pop_factor)\n        idxs = np.random.choice(current_pop_size, 4, replace=False)\n        a, b, c, d = idxs[0], idxs[1], idxs[2], idxs[3]\n\n        # Enhanced exploration with decaying exploration rate\n        if np.random.rand() < self.exploration_rate:\n            x_mutated = self.population[i] + self.exploration_rate * (np.random.rand(self.dim) * (func.bounds.ub - func.bounds.lb) + func.bounds.lb - self.population[i])\n        else:\n            x_mutated = self.population[a] + 0.5 * (self.population[b] - self.population[c]) + 0.5 * (self.best_solution - self.population[a]) if self.best_solution is not None else self.population[a] + 0.5 * (self.population[b] - self.population[c])\n        return x_mutated\n\n    def crossover(self, x_mutated, i):\n        x_trial = np.copy(self.population[i])\n        cross_prob = np.random.rand(self.dim)\n        x_trial[cross_prob < 0.9] = x_mutated[cross_prob < 0.9]\n        return x_trial\n\n    def niching(self, x_trial, f_trial):\n        current_pop_size = int(self.pop_size * self.pop_factor)\n        for j in range(current_pop_size):\n            if np.linalg.norm(x_trial - self.population[j]) < self.niche_radius:\n                if f_trial < self.fitness[j]:\n                    return True\n                else:\n                    return False\n        return True\n\n    def adjust_population_size(self):\n        if len(self.success_history) > 10:\n            self.success_rate = np.mean(self.success_history[-10:])\n\n            if self.success_rate > 0.6 and self.pop_factor < 2.0:\n                self.pop_factor *= 1.1\n            elif self.success_rate < 0.2 and self.pop_factor > 0.2:\n                self.pop_factor *= 0.9\n\n            self.pop_factor = np.clip(self.pop_factor, 0.2, 2.0)\n            self.success_history = []\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        self.best_fitness = np.min(self.fitness)\n        self.best_solution = self.population[np.argmin(self.fitness)].copy()\n\n        while self.budget > 0:\n            current_pop_size = int(self.pop_size * self.pop_factor)\n            success_count = 0\n            for i in range(current_pop_size):\n                x_mutated = self.mutate(i)\n                x_trial = self.crossover(x_mutated, i)\n                x_trial = np.clip(x_trial, func.bounds.lb, func.bounds.ub)\n                f_trial = func(x_trial)\n                self.budget -= 1\n                if self.budget <= 0:\n                    break\n\n                if f_trial < self.fitness[i] and self.niching(x_trial, f_trial):\n                    self.population[i] = x_trial.copy()\n                    self.fitness[i] = f_trial\n                    success_count += 1\n                    if f_trial < self.best_fitness:\n                        self.best_fitness = f_trial\n                        self.best_solution = x_trial.copy()\n\n            self.exploration_rate *= self.decay_rate\n            self.success_history.append(success_count / current_pop_size)\n            self.adjust_population_size()\n\n            if self.budget <= 0:\n                break\n\n        return self.best_fitness, self.best_solution", "configspace": "", "generation": 12, "feedback": "In the code, line 32, in mutate, the following error occurred:\nNameError: name 'func' is not defined\nOn line: x_mutated = self.population[i] + self.exploration_rate * (np.random.rand(self.dim) * (func.bounds.ub - func.bounds.lb) + func.bounds.lb - self.population[i])", "error": "In the code, line 32, in mutate, the following error occurred:\nNameError: name 'func' is not defined\nOn line: x_mutated = self.population[i] + self.exploration_rate * (np.random.rand(self.dim) * (func.bounds.ub - func.bounds.lb) + func.bounds.lb - self.population[i])", "parent_ids": ["ca85b7ad-0f37-47f9-9049-9648e4f1e39a"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "d47a8fde-4cb3-4819-a162-c1ed8739cdf0", "fitness": "-inf", "name": "EnhancedAdaptiveDE", "description": "Adaptive Differential Evolution with orthogonal learning, dynamic population sizing, and aging to enhance exploration and exploitation.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=10, stagnation_threshold=50, aging_rate=0.01):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.stagnation_threshold = stagnation_threshold\n        self.stagnation_counter = 0\n        self.best_fitness_history = []\n        self.pop_factor = 1.0\n        self.archive = []\n        self.memory_f = [0.5] * archive_size\n        self.memory_cr = [0.9] * archive_size\n        self.sf = []\n        self.scr = []\n        self.population = None\n        self.fitness = None\n        self.ages = None\n        self.aging_rate = aging_rate\n\n    def initialize_population(self, func):\n        current_pop_size = int(self.pop_size * self.pop_factor)\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(current_pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.ages = np.zeros(current_pop_size)\n        self.budget -= current_pop_size\n\n    def mutate(self, i):\n        current_pop_size = int(self.pop_size * self.pop_factor)\n        phi_idx = np.random.randint(0, self.archive_size)\n        F = np.clip(np.random.normal(self.memory_f[phi_idx], 0.1), 0.1, 1.0)\n        CR = np.clip(np.random.normal(self.memory_cr[phi_idx], 0.1), 0.0, 1.0)\n\n        idxs = np.random.choice(current_pop_size, 3, replace=False)\n        a, b, c = idxs[0], idxs[1], idxs[2]\n\n        # Incorporate information from the top 20% performing solutions\n        top_indices = np.argsort(self.fitness)[:max(1, int(0.2 * current_pop_size))]\n        p_best_idx = np.random.choice(top_indices)\n\n        x_mutated = self.population[i] + F * (self.population[p_best_idx] - self.population[i]) + F * (self.population[a] - self.population[b])\n        return x_mutated, F, CR\n\n    def crossover(self, x_mutated, i, cr):\n        x_trial = np.copy(self.population[i])\n        j_rand = np.random.randint(self.dim)\n        for j in range(self.dim):\n            if np.random.rand() < cr or j == j_rand:\n                x_trial[j] = x_mutated[j]\n        return x_trial\n\n    def orthogonal_learning(self, func, x_current):\n        # Generate orthogonal array (OA)\n        n_factors = self.dim\n        n_levels = 3  # Example: 3 levels for each dimension\n        oa = self.generate_orthogonal_array(n_factors, n_levels)\n\n        # Evaluate points based on OA\n        best_fitness = float('inf')\n        best_x = x_current\n\n        for row in oa:\n            x_trial = np.copy(x_current)\n            for j in range(self.dim):\n                level = row[j]\n                # Map OA levels to search space (e.g., lb, midpoint, ub)\n                if level == 0:\n                    x_trial[j] = func.bounds.lb\n                elif level == 1:\n                    x_trial[j] = (func.bounds.lb + func.bounds.ub) / 2.0\n                else:\n                    x_trial[j] = func.bounds.ub\n            \n            x_trial = np.clip(x_trial, func.bounds.lb, func.bounds.ub)\n            f_trial = func(x_trial)\n            self.budget -= 1\n\n            if f_trial < best_fitness:\n                best_fitness = f_trial\n                best_x = x_trial\n\n            if self.budget <= 0:\n                break\n\n        return best_x, best_fitness\n\n\n    def generate_orthogonal_array(self, n_factors, n_levels):\n          #This is a simplified example and may need adjustments for different n_factors and n_levels\n          #using precalculated orthogonal array if possible is recommended for efficiency\n          oa = np.random.randint(0, n_levels, size=(n_factors + 1, n_factors)) #Example OA: needs proper construction or lookup\n          return oa\n\n    def update_memory(self):\n        if self.sf:\n            self.memory_f.append(sum(val**2 for val in self.sf) / sum(self.sf) if sum(self.sf) != 0 else self.memory_f[-1])\n            self.memory_f.pop(0)\n            self.memory_cr.append(np.mean(self.scr))\n            self.memory_cr.pop(0)\n        self.sf = []\n        self.scr = []\n\n    def check_stagnation(self):\n        if len(self.best_fitness_history) > self.stagnation_threshold:\n            if np.std(self.best_fitness_history[-self.stagnation_threshold:]) < 1e-6:\n                self.stagnation_counter += 1\n                return True\n            else:\n                self.stagnation_counter = 0\n                return False\n        return False\n\n    def restart_population(self, func):\n        self.pop_factor *= 0.75  # Reduce population size\n        self.pop_factor = max(0.2, self.pop_factor) # Prevent too small pop size\n        self.initialize_population(func)\n        self.best_fitness_history = []\n        self.memory_f = [0.5] * self.archive_size\n        self.memory_cr = [0.9] * self.archive_size\n        self.sf = []\n        self.scr = []\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)].copy()\n        self.stagnation_counter = 0\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)].copy()\n        self.sf = []\n        self.scr = []\n\n        while self.budget > 0:\n            current_pop_size = int(self.pop_size * self.pop_factor)\n            for i in range(current_pop_size):\n                x_mutated, F, CR = self.mutate(i)\n                x_trial = self.crossover(x_mutated, i, CR)\n                x_trial = np.clip(x_trial, func.bounds.lb, func.bounds.ub)\n                f_trial = func(x_trial)\n                self.budget -= 1\n                if self.budget <= 0:\n                    break\n\n                # Orthogonal learning\n                x_ol, f_ol = self.orthogonal_learning(func, x_trial)\n\n                if self.budget <= 0:\n                    break\n\n                if f_ol < self.fitness[i]:\n                    self.sf.append(F)\n                    self.scr.append(CR)\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(self.population[i].copy())\n                    else:\n                        j = np.random.randint(0, self.archive_size)\n                        self.archive[j] = self.population[i].copy()\n                    self.population[i] = x_ol.copy()\n                    self.fitness[i] = f_ol\n\n                    if f_ol < self.f_opt:\n                        self.f_opt = f_ol\n                        self.x_opt = x_ol.copy()\n                else:\n                     # Aging mechanism: increase age if not improved\n                    self.ages[i] += self.aging_rate\n\n                if self.budget <= 0:\n                    break\n\n            if self.budget <= 0:\n                break\n\n            # Remove old individuals based on age\n            age_threshold = np.mean(self.ages) + np.std(self.ages) #Dynamic age threshold\n            indices_to_remove = np.where(self.ages > age_threshold)[0]\n            if len(indices_to_remove) > 0:\n\n                num_to_replace = min(len(indices_to_remove), current_pop_size // 4 ) #limit removal\n\n                for _ in range(num_to_replace):\n                    worst_index = indices_to_remove[np.argmax(self.ages[indices_to_remove])] #remove oldest\n                    new_individual = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.dim))\n                    new_fitness = func(new_individual)\n                    self.budget -= 1\n                    if self.budget <= 0:\n                        break\n                    self.population[worst_index] = new_individual\n                    self.fitness[worst_index] = new_fitness\n                    self.ages[worst_index] = 0 #reset age\n                    indices_to_remove = np.delete(indices_to_remove, np.argmax(self.ages[indices_to_remove]))\n\n\n            self.update_memory()\n            best_index = np.argmin(self.fitness)\n            if self.fitness[best_index] < self.f_opt:\n                self.f_opt = self.fitness[best_index]\n                self.x_opt = self.population[best_index].copy()\n\n            self.best_fitness_history.append(self.f_opt)\n\n            if self.check_stagnation() and self.budget > int(self.pop_size * self.pop_factor) * 2:\n                self.restart_population(func)\n            \n            self.ages *= (1 - self.aging_rate) #gradual aging for surviving members\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 12, "feedback": "In the code, line 70, in orthogonal_learning, the following error occurred:\nValueError: setting an array element with a sequence.\nOn line: x_trial[j] = func.bounds.lb", "error": "In the code, line 70, in orthogonal_learning, the following error occurred:\nValueError: setting an array element with a sequence.\nOn line: x_trial[j] = func.bounds.lb", "parent_ids": ["ca85b7ad-0f37-47f9-9049-9648e4f1e39a"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "17081254-ac05-402c-8a10-ed87753050be", "fitness": "-inf", "name": "HybridDESA", "description": "A hybrid Differential Evolution with Simulated Annealing, integrating adaptive mutation and crossover with a temperature-controlled acceptance criterion for enhanced exploration and exploitation.", "code": "import numpy as np\nimport math\n\nclass HybridDESA:\n    def __init__(self, budget=10000, dim=10, pop_size=50, f_base=0.5, cr_base=0.9, temp_init=1.0, temp_decay=0.995):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.population = None\n        self.fitness = None\n        self.best_idx = None\n        self.best_fitness = float('inf')\n        self.best_solution = None\n        self.f_base = f_base\n        self.cr_base = cr_base\n        self.temp = temp_init\n        self.temp_decay = temp_decay\n        self.sf = []\n        self.scr = []\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        self.best_idx = np.argmin(self.fitness)\n        self.best_fitness = self.fitness[self.best_idx]\n        self.best_solution = self.population[self.best_idx].copy()\n\n    def mutate(self, i):\n        F = np.clip(np.random.normal(self.f_base, 0.1), 0.1, 1.0)\n        CR = np.clip(np.random.normal(self.cr_base, 0.1), 0.0, 1.0)\n\n        idxs = np.random.choice(self.pop_size, 3, replace=False)\n        a, b, c = idxs[0], idxs[1], idxs[2]\n\n        # Mutation using current individual, best solution and two other individuals\n        x_mutated = self.population[i] + F * (self.best_solution - self.population[i]) + F * (self.population[a] - self.population[b]) + F * (self.population[c] - self.population[i])\n\n\n        return x_mutated, F, CR\n\n    def crossover(self, x_mutated, i, cr):\n        x_trial = np.copy(self.population[i])\n        j_rand = np.random.randint(self.dim)\n        for j in range(self.dim):\n            if np.random.rand() < cr or j == j_rand:\n                x_trial[j] = x_mutated[j]\n        return x_trial\n\n    def repair(self, x_trial, func):\n        x_trial = np.clip(x_trial, func.bounds.lb, func.bounds.ub)\n        return x_trial\n\n    def update_parameters(self):\n        if self.sf:\n            self.f_base = sum(val**2 for val in self.sf) / sum(self.sf) if sum(self.sf) > 0 else 0.5\n            self.cr_base = np.mean(self.scr)\n        self.sf = []\n        self.scr = []\n\n    def acceptance_probability(self, old_fitness, new_fitness):\n        if new_fitness < old_fitness:\n            return 1.0\n        else:\n            return math.exp((old_fitness - new_fitness) / self.temp)\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.budget > 0:\n            self.sf = []\n            self.scr = []\n            for i in range(self.pop_size):\n                x_mutated, F, CR = self.mutate(i)\n                x_trial = self.crossover(x_mutated, i, CR)\n                x_trial = self.repair(x_trial, func)\n\n                f_trial = func(x_trial)\n                self.budget -= 1\n                if self.budget <= 0:\n                    break\n                    \n                ap = self.acceptance_probability(self.fitness[i], f_trial)\n\n                if ap > np.random.rand():\n                    self.sf.append(F)\n                    self.scr.append(CR)\n                    self.population[i] = x_trial.copy()\n                    self.fitness[i] = f_trial\n\n                    if f_trial < self.best_fitness:\n                        self.best_fitness = f_trial\n                        self.best_solution = x_trial.copy()\n                        self.best_idx = i\n\n            if self.budget <= 0:\n                break\n            \n            self.update_parameters()\n\n            self.temp *= self.temp_decay # cool down the temperature\n\n        return self.best_fitness, self.best_solution", "configspace": "", "generation": 12, "feedback": "In the code, line 65, in acceptance_probability, the following error occurred:\nNameError: name 'math' is not defined. Did you forget to import 'math'?\nOn line: return math.exp((old_fitness - new_fitness) / self.temp)", "error": "In the code, line 65, in acceptance_probability, the following error occurred:\nNameError: name 'math' is not defined. Did you forget to import 'math'?\nOn line: return math.exp((old_fitness - new_fitness) / self.temp)", "parent_ids": ["b9b9465e-c535-4a3a-911c-bb80483be837"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "e5b44c0a-06dd-4f29-9059-ebc281520eef", "fitness": "-inf", "name": "EnhancedAdaptiveDE", "description": "Adaptive Differential Evolution with orthogonal learning, enhanced restart mechanism, and covariance matrix adaptation for improved exploration and exploitation.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, f_base=0.5, cr_base=0.9, restart_trigger=0.1, restart_probability=0.05, orthogonal_learning_probability=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.population = None\n        self.fitness = None\n        self.best_idx = None\n        self.best_fitness = float('inf')\n        self.best_solution = None\n        self.f_base = f_base\n        self.cr_base = cr_base\n        self.sf = []\n        self.scr = []\n        self.restart_trigger = restart_trigger\n        self.restart_probability = restart_probability\n        self.orthogonal_learning_probability = orthogonal_learning_probability\n        self.covariance_matrix = None\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        self.best_idx = np.argmin(self.fitness)\n        self.best_fitness = self.fitness[self.best_idx]\n        self.best_solution = self.population[self.best_idx].copy()\n        self.covariance_matrix = np.eye(self.dim)  # Initialize covariance matrix\n\n    def mutate(self, i):\n        F = np.clip(np.random.normal(self.f_base, 0.1), 0.1, 1.0)\n        CR = np.clip(np.random.normal(self.cr_base, 0.1), 0.0, 1.0)\n\n        idxs = np.random.choice(self.pop_size, 2, replace=False)\n        a, b = idxs[0], idxs[1]\n\n        # Mutation using best solution and two other individuals\n        x_mutated = self.population[i] + F * (self.best_solution - self.population[i]) + F * (self.population[a] - self.population[b])\n\n        # Add a component based on covariance matrix adaptation\n        x_mutated += 0.1 * np.random.multivariate_normal(np.zeros(self.dim), self.covariance_matrix)\n\n        return x_mutated, F, CR\n\n    def crossover(self, x_mutated, i, cr):\n        x_trial = np.copy(self.population[i])\n        j_rand = np.random.randint(self.dim)\n        for j in range(self.dim):\n            if np.random.rand() < cr or j == j_rand:\n                x_trial[j] = x_mutated[j]\n        return x_trial\n\n    def repair(self, x_trial, func):\n        x_trial = np.clip(x_trial, func.bounds.lb, func.bounds.ub)\n        return x_trial\n\n    def update_parameters(self):\n        if self.sf:\n            self.f_base = sum(val**2 for val in self.sf) / sum(self.sf) if sum(self.sf) > 0 else 0.5\n            self.cr_base = np.mean(self.scr)\n        self.sf = []\n        self.scr = []\n\n    def orthogonal_learning(self, func):\n        if np.random.rand() < self.orthogonal_learning_probability:\n            # Select a random individual\n            i = np.random.randint(self.pop_size)\n            \n            # Generate an orthogonal array (simplified for demonstration)\n            oa = np.array([[-1, -1], [-1, 1], [1, -1], [1, 1]])  # 2^2 OA\n            \n            # Select two random dimensions\n            dims = np.random.choice(self.dim, 2, replace=False)\n            \n            # Create new candidate solutions based on OA\n            candidates = np.zeros((4, self.dim))\n            for j in range(4):\n                candidates[j] = self.population[i].copy()\n                candidates[j][dims[0]] += 0.05 * oa[j, 0] * (func.bounds.ub - func.bounds.lb) #Scale factor added here\n                candidates[j][dims[1]] += 0.05 * oa[j, 1] * (func.bounds.ub - func.bounds.lb) #Scale factor added here\n                candidates[j] = self.repair(candidates[j], func)\n\n            # Evaluate candidates\n            fitness_candidates = [func(x) for x in candidates]\n            self.budget -= 4\n\n            # Select the best candidate\n            best_candidate_idx = np.argmin(fitness_candidates)\n            if fitness_candidates[best_candidate_idx] < self.fitness[i]:\n                self.population[i] = candidates[best_candidate_idx].copy()\n                self.fitness[i] = fitness_candidates[best_candidate_idx]\n\n                if self.fitness[i] < self.best_fitness:\n                    self.best_fitness = self.fitness[i]\n                    self.best_solution = self.population[i].copy()\n                    self.best_idx = i\n\n    def maybe_restart(self, func):\n        if np.random.rand() < self.restart_probability:\n            # Restart a portion of the population, biased towards worst solutions.\n            num_to_restart = int(self.restart_trigger * self.pop_size)\n            # Calculate probabilities inversely proportional to fitness\n            probabilities = np.max(self.fitness) - self.fitness + 1e-9  # Add a small value to avoid division by zero\n            probabilities /= np.sum(probabilities)\n            \n            indices_to_restart = np.random.choice(self.pop_size, num_to_restart, replace=False, p=probabilities) # biased towards worst individuals\n\n            for i in indices_to_restart:\n                self.population[i] = self.best_solution + np.random.uniform(-1.0, 1.0, size=self.dim) * (func.bounds.ub - func.bounds.lb) * 0.1 # Smaller range for restart\n                self.population[i] = self.repair(self.population[i], func)\n                self.fitness[i] = func(self.population[i])\n                self.budget -= 1\n\n                if self.fitness[i] < self.best_fitness:\n                    self.best_fitness = self.fitness[i]\n                    self.best_solution = self.population[i].copy()\n                    self.best_idx = i\n\n    def update_covariance_matrix(self):\n        # Simple covariance matrix adaptation: moving average of differences.\n        if len(self.sf) > 0:  # Update only if successful mutations occurred\n            delta = self.population - np.mean(self.population, axis=0)\n            self.covariance_matrix = 0.9 * self.covariance_matrix + 0.1 * np.cov(delta.T)\n            # Ensure covariance matrix remains positive definite\n            self.covariance_matrix += np.eye(self.dim) * 1e-6\n\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.budget > 0:\n            self.sf = []\n            self.scr = []\n            for i in range(self.pop_size):\n                x_mutated, F, CR = self.mutate(i)\n                x_trial = self.crossover(x_mutated, i, CR)\n                x_trial = self.repair(x_trial, func)\n\n                f_trial = func(x_trial)\n                self.budget -= 1\n                if self.budget <= 0:\n                    break\n\n                if f_trial < self.fitness[i]:\n                    self.sf.append(F)\n                    self.scr.append(CR)\n                    self.population[i] = x_trial.copy()\n                    self.fitness[i] = f_trial\n\n                    if f_trial < self.best_fitness:\n                        self.best_fitness = f_trial\n                        self.best_solution = x_trial.copy()\n                        self.best_idx = i\n\n            if self.budget <= 0:\n                break\n\n            self.update_parameters()\n            self.orthogonal_learning(func)\n            self.maybe_restart(func)\n            self.update_covariance_matrix()\n\n\n        return self.best_fitness, self.best_solution", "configspace": "", "generation": 12, "feedback": "In the code, line 80, in orthogonal_learning, the following error occurred:\nValueError: setting an array element with a sequence.\nOn line: candidates[j][dims[0]] += 0.05 * oa[j, 0] * (func.bounds.ub - func.bounds.lb) #Scale factor added here", "error": "In the code, line 80, in orthogonal_learning, the following error occurred:\nValueError: setting an array element with a sequence.\nOn line: candidates[j][dims[0]] += 0.05 * oa[j, 0] * (func.bounds.ub - func.bounds.lb) #Scale factor added here", "parent_ids": ["b9b9465e-c535-4a3a-911c-bb80483be837"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "3995f434-2c11-4c12-b959-94d787914e56", "fitness": "-inf", "name": "SOM_LevyDE", "description": "Exploration-Exploitation Balancing using Lvy flights and a self-organizing map to adaptively guide the search based on function landscape characteristics.", "code": "import numpy as np\nfrom scipy.stats import levy\n\nclass SOM_LevyDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, som_grid_size=10, levy_exponent=1.5, learning_rate=0.1, sigma=1.0):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.som_grid_size = som_grid_size\n        self.levy_exponent = levy_exponent\n        self.learning_rate = learning_rate\n        self.sigma = sigma  # Initial neighborhood radius for SOM\n        self.population = None\n        self.fitness = None\n        self.som = None  # Self-Organizing Map\n        self.som_weights = None\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n\n    def initialize_som(self):\n        self.som_weights = np.random.rand(self.som_grid_size, self.som_grid_size, self.dim)\n\n    def find_best_matching_unit(self, individual):\n        distances = np.sum((self.som_weights - individual)**2, axis=2)\n        return np.unravel_index(np.argmin(distances), distances.shape)\n\n    def update_som(self, individual, bmu, learning_rate, sigma):\n        for i in range(self.som_grid_size):\n            for j in range(self.som_grid_size):\n                distance = np.sqrt((i - bmu[0])**2 + (j - bmu[1])**2)\n                influence = np.exp(-distance**2 / (2 * sigma**2))\n                self.som_weights[i, j] += learning_rate * influence * (individual - self.som_weights[i, j])\n\n    def levy_flight(self, beta=1.5):\n        sigma = (np.math.gamma(1 + beta) * np.sin(np.pi * beta / 2) / (np.math.gamma((1 + beta) / 2) * beta * (2**((beta - 1) / 2))))**(1 / beta)\n        u = np.random.normal(0, sigma, size=self.dim)\n        v = np.random.normal(0, 1, size=self.dim)\n        step = u / (np.abs(v)**(1 / beta))\n        return step\n\n    def mutate(self, i, func):\n        # Levy flight-based mutation\n        step_size = self.levy_flight(self.levy_exponent)\n        mutated_individual = self.population[i] + 0.01 * step_size  # Scale step size\n\n        # Best Matching Unit influence\n        bmu = self.find_best_matching_unit(self.population[i])\n        bmu_influence = self.som_weights[bmu]\n\n        mutated_individual += 0.1 * (bmu_influence - self.population[i])\n\n        mutated_individual = np.clip(mutated_individual, func.bounds.lb, func.bounds.ub)\n        return mutated_individual\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        self.initialize_som()\n        self.x_opt = self.population[np.argmin(self.fitness)].copy()\n        self.f_opt = np.min(self.fitness)\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                mutated_individual = self.mutate(i, func)\n                trial_fitness = func(mutated_individual)\n                self.budget -= 1\n\n                if self.budget <= 0:\n                    break\n\n                if trial_fitness < self.fitness[i]:\n                    self.update_som(self.population[i], self.find_best_matching_unit(self.population[i]), self.learning_rate, self.sigma)\n                    self.population[i] = mutated_individual.copy()\n                    self.fitness[i] = trial_fitness\n\n                    if trial_fitness < self.f_opt:\n                        self.f_opt = trial_fitness\n                        self.x_opt = mutated_individual.copy()\n\n            self.sigma = max(0.1, self.sigma * 0.995) # cool down sigma\n\n            if self.budget <= 0:\n                break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 12, "feedback": "In the code, line 805, in __getattr__, the following error occurred:\nAttributeError: module 'numpy' has no attribute 'math'. Did you mean: 'emath'?", "error": "In the code, line 805, in __getattr__, the following error occurred:\nAttributeError: module 'numpy' has no attribute 'math'. Did you mean: 'emath'?", "parent_ids": ["ca85b7ad-0f37-47f9-9049-9648e4f1e39a"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "53895a66-df7f-4907-9fd3-70aed4d67ded", "fitness": "-inf", "name": "AdaptiveDEWithLocalSearch", "description": "A Differential Evolution variant with self-adaptive mutation strategies, dynamic F/CR adaptation based on success history, and a local search refinement step using Nelder-Mead simplex for exploitation.", "code": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveDEWithLocalSearch:\n    def __init__(self, budget=10000, dim=10, pop_size=50, f_base=0.5, cr_base=0.9, local_search_prob=0.05):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.population = None\n        self.fitness = None\n        self.best_idx = None\n        self.best_fitness = float('inf')\n        self.best_solution = None\n        self.f_base = f_base\n        self.cr_base = cr_base\n        self.sf = []\n        self.scr = []\n        self.local_search_prob = local_search_prob\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        self.best_idx = np.argmin(self.fitness)\n        self.best_fitness = self.fitness[self.best_idx]\n        self.best_solution = self.population[self.best_idx].copy()\n\n    def mutate(self, i):\n        # Adaptive mutation strategy selection\n        mutation_strategy = np.random.choice(['current-to-rand', 'rand-to-best'])\n\n        if mutation_strategy == 'current-to-rand':\n            idxs = np.random.choice(self.pop_size, 3, replace=False)\n            a, b, c = idxs[0], idxs[1], idxs[2]\n            F = np.clip(np.random.normal(self.f_base, 0.1), 0.1, 1.0)\n            x_mutated = self.population[i] + F * (self.population[a] - self.population[i]) + F * (self.population[b] - self.population[c])\n        elif mutation_strategy == 'rand-to-best':\n            idxs = np.random.choice(self.pop_size, 2, replace=False)\n            a, b = idxs[0], idxs[1]\n            F = np.clip(np.random.normal(self.f_base, 0.1), 0.1, 1.0)\n            x_mutated = self.population[np.random.randint(self.pop_size)] + F * (self.best_solution - self.population[i]) + F * (self.population[a] - self.population[b])\n\n        CR = np.clip(np.random.normal(self.cr_base, 0.1), 0.0, 1.0)\n\n        return x_mutated, F, CR\n\n    def crossover(self, x_mutated, i, cr):\n        x_trial = np.copy(self.population[i])\n        j_rand = np.random.randint(self.dim)\n        for j in range(self.dim):\n            if np.random.rand() < cr or j == j_rand:\n                x_trial[j] = x_mutated[j]\n        return x_trial\n\n    def repair(self, x_trial, func):\n        x_trial = np.clip(x_trial, func.bounds.lb, func.bounds.ub)\n        return x_trial\n\n    def update_parameters(self):\n        if self.sf:\n            self.f_base = sum(val**2 for val in self.sf) / sum(self.sf) if sum(self.sf) > 0 else 0.5\n            self.cr_base = np.mean(self.scr)\n        self.sf = []\n        self.scr = []\n\n    def local_search(self, func, x):\n        # Local search using Nelder-Mead simplex\n        bounds = [(func.bounds.lb, func.bounds.ub)] * self.dim\n        result = minimize(func, x, method='Nelder-Mead', bounds=bounds, options={'maxfev': min(50, self.budget)})  # Limit FE\n        self.budget -= result.nfev\n        return result.x, result.fun\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.budget > 0:\n            self.sf = []\n            self.scr = []\n            for i in range(self.pop_size):\n                x_mutated, F, CR = self.mutate(i)\n                x_trial = self.crossover(x_mutated, i, CR)\n                x_trial = self.repair(x_trial, func)\n\n                f_trial = func(x_trial)\n                self.budget -= 1\n                if self.budget <= 0:\n                    break\n\n                if f_trial < self.fitness[i]:\n                    self.sf.append(F)\n                    self.scr.append(CR)\n                    self.population[i] = x_trial.copy()\n                    self.fitness[i] = f_trial\n\n                    if f_trial < self.best_fitness:\n                        self.best_fitness = f_trial\n                        self.best_solution = x_trial.copy()\n                        self.best_idx = i\n\n            if self.budget <= 0:\n                break\n\n            # Local search around best solution\n            if np.random.rand() < self.local_search_prob:\n                x_local, f_local = self.local_search(func, self.best_solution.copy())\n                if f_local < self.best_fitness:\n                     self.best_fitness = f_local\n                     self.best_solution = x_local.copy()\n                     self.population[self.best_idx] = x_local.copy()\n                     self.fitness[self.best_idx] = f_local\n\n            self.update_parameters()\n\n\n        return self.best_fitness, self.best_solution", "configspace": "", "generation": 12, "feedback": "In the code, line 69, in local_search, the following error occurred:\nNameError: name 'minimize' is not defined\nOn line: result = minimize(func, x, method='Nelder-Mead', bounds=bounds, options={'maxfev': min(50, self.budget)})  # Limit FE", "error": "In the code, line 69, in local_search, the following error occurred:\nNameError: name 'minimize' is not defined\nOn line: result = minimize(func, x, method='Nelder-Mead', bounds=bounds, options={'maxfev': min(50, self.budget)})  # Limit FE", "parent_ids": ["b9b9465e-c535-4a3a-911c-bb80483be837"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "2181026a-d48e-4919-b8de-af9ee462a791", "fitness": "-inf", "name": "HybridDE_NM", "description": "A hybrid algorithm combining a global search using Differential Evolution with a local search refinement based on Nelder-Mead Simplex, dynamically switching between them based on stagnation and diversity measures.", "code": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridDE_NM:\n    def __init__(self, budget=10000, dim=10, pop_size=50, stagnation_threshold=50, de_cr=0.7, de_f=0.5, nm_iterations=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.stagnation_threshold = stagnation_threshold\n        self.de_cr = de_cr\n        self.de_f = de_f\n        self.nm_iterations = nm_iterations\n        self.stagnation_counter = 0\n        self.best_fitness_history = []\n        self.population = None\n        self.fitness = None\n        self.bounds = None\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        self.bounds = func.bounds\n\n    def mutate(self, i):\n        idxs = np.random.choice(self.pop_size, 3, replace=False)\n        a, b, c = idxs[0], idxs[1], idxs[2]\n        x_mutated = self.population[a] + self.de_f * (self.population[b] - self.population[c])\n        return x_mutated\n\n    def crossover(self, x_mutated, i):\n        x_trial = np.copy(self.population[i])\n        j_rand = np.random.randint(self.dim)\n        for j in range(self.dim):\n            if np.random.rand() < self.de_cr or j == j_rand:\n                x_trial[j] = x_mutated[j]\n        return x_trial\n\n    def local_search(self, func, x):\n        nm_result = minimize(func, x, method='Nelder-Mead', bounds=self.bounds, options={'maxiter': self.nm_iterations})\n        return nm_result.fun, nm_result.x\n\n    def check_stagnation(self):\n        if len(self.best_fitness_history) > self.stagnation_threshold:\n            if np.std(self.best_fitness_history[-self.stagnation_threshold:]) < 1e-6:\n                self.stagnation_counter += 1\n                return True\n            else:\n                self.stagnation_counter = 0\n                return False\n        return False\n    \n    def calculate_diversity(self):\n        centroid = np.mean(self.population, axis=0)\n        distances = np.linalg.norm(self.population - centroid, axis=1)\n        diversity = np.mean(distances)\n        return diversity\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)].copy()\n\n        while self.budget > 0:\n            diversity = self.calculate_diversity()\n\n            for i in range(self.pop_size):\n                x_mutated = self.mutate(i)\n                x_trial = self.crossover(x_mutated, i)\n                x_trial = np.clip(x_trial, self.bounds.lb, self.bounds.ub)\n                f_trial = func(x_trial)\n                self.budget -= 1\n                if self.budget <= 0:\n                    break\n\n                if f_trial < self.fitness[i]:\n                    self.population[i] = x_trial.copy()\n                    self.fitness[i] = f_trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = x_trial.copy()\n            \n            # Local Search with Nelder-Mead\n            if diversity < 0.1 or self.check_stagnation():\n                best_index = np.argmin(self.fitness)\n                f_nm, x_nm = self.local_search(func, self.population[best_index])\n                if f_nm < self.f_opt:\n                    self.f_opt = f_nm\n                    self.x_opt = x_nm.copy()\n                    self.population[best_index] = x_nm.copy()\n                    self.fitness[best_index] = f_nm\n\n            best_index = np.argmin(self.fitness)\n            if self.fitness[best_index] < self.f_opt:\n                self.f_opt = self.fitness[best_index]\n                self.x_opt = self.population[best_index].copy()\n\n            self.best_fitness_history.append(self.f_opt)\n            if self.check_stagnation():\n                self.stagnation_counter +=1\n            else:\n                self.stagnation_counter = 0\n            if self.stagnation_counter > self.stagnation_threshold // 2 and self.budget > self.pop_size: #adjusting condition\n              self.population = np.random.uniform(self.bounds.lb, self.bounds.ub, size=(self.pop_size, self.dim))\n              self.fitness = np.array([func(x) for x in self.population])\n              self.budget -= self.pop_size\n              self.best_fitness_history = []\n              self.stagnation_counter = 0\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 12, "feedback": "In the code, line 40, in local_search, the following error occurred:\nNameError: name 'minimize' is not defined\nOn line: nm_result = minimize(func, x, method='Nelder-Mead', bounds=self.bounds, options={'maxiter': self.nm_iterations})", "error": "In the code, line 40, in local_search, the following error occurred:\nNameError: name 'minimize' is not defined\nOn line: nm_result = minimize(func, x, method='Nelder-Mead', bounds=self.bounds, options={'maxiter': self.nm_iterations})", "parent_ids": ["ca85b7ad-0f37-47f9-9049-9648e4f1e39a"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "6f327ea9-3138-4b7a-adf7-acdadd6f60d4", "fitness": 0.7259300932444632, "name": "DynamicResourceAllocationDE", "description": "Dynamic DE with covariance matrix adaptation for enhanced exploration and exploitation, adjusting strategy weights based on success and diversity.", "code": "import numpy as np\n\nclass DynamicResourceAllocationDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, num_strategies=4, local_search_probability=0.1, archive_size=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.num_strategies = num_strategies\n        self.local_search_probability = local_search_probability\n        self.archive_size = archive_size\n        self.population = None\n        self.fitness = None\n        self.strategy_successes = np.zeros(num_strategies)\n        self.strategy_counts = np.zeros(num_strategies)\n        self.strategy_weights = np.ones(num_strategies) / num_strategies\n        self.F = 0.5\n        self.CR = 0.9\n        self.archive = []  # Archive for past solutions\n        self.archive_fitness = []\n        self.C = np.eye(dim)  # Covariance matrix for CMA\n        self.cma_learning_rate = 0.1 # Learning rate for CMA adaptation\n\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)].copy()\n\n\n    def select_strategy(self):\n        return np.random.choice(self.num_strategies, p=self.strategy_weights)\n\n    def mutate(self, strategy, i, func):\n        if strategy == 0:  # DE/rand/1\n            idxs = np.random.choice(self.pop_size, 5, replace=False)\n            a, b, c, d, e = idxs\n            x_mutated = self.population[a] + self.F * (self.population[b] - self.population[c]) + self.F * (self.population[d] - self.population[e])\n        elif strategy == 1:  # DE/current-to-best/1\n            best_idx = np.argmin(self.fitness)\n            idxs = np.random.choice(self.pop_size, 3, replace=False)\n            a, b, c = idxs\n            x_mutated = self.population[i] + self.F * (self.population[best_idx] - self.population[i]) + self.F * (self.population[a] - self.population[b])\n        elif strategy == 2:  # CMA-ES-like mutation\n            z = np.random.normal(0, 1, self.dim)\n            x_mutated = self.population[i] + self.F * np.dot(self.C, z)\n        elif strategy == 3: # Archive based mutation\n            if self.archive:\n                archive_idx = np.random.randint(len(self.archive))\n                x_mutated = self.archive[archive_idx] + self.F * (self.population[i] - self.population[np.random.choice(self.pop_size)])\n            else:\n                x_mutated = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim) # fallback if archive is empty\n\n\n        x_mutated = np.clip(x_mutated, func.bounds.lb, func.bounds.ub)\n        return x_mutated\n\n    def crossover(self, x_mutated, i):\n        x_trial = np.copy(self.population[i])\n        j_rand = np.random.randint(self.dim)\n        for j in range(self.dim):\n            if np.random.rand() < self.CR or j == j_rand:\n                x_trial[j] = x_mutated[j]\n        return x_trial\n\n    def local_search(self, x, func):\n        # Apply Gaussian local search around the current solution\n        x_ls = x + np.random.normal(0, 0.01, self.dim)  # Small perturbation\n        x_ls = np.clip(x_ls, func.bounds.lb, func.bounds.ub)\n        return x_ls\n\n    def update_strategy_weights(self):\n        total_successes = np.sum(self.strategy_successes)\n        if total_successes > 0:\n            self.strategy_weights = self.strategy_successes / total_successes\n        else:\n            self.strategy_weights = np.ones(self.num_strategies) / self.num_strategies\n\n        # Smoothing the weights to avoid premature convergence\n        self.strategy_weights = 0.9 * self.strategy_weights + 0.1 / self.num_strategies\n        self.strategy_weights /= np.sum(self.strategy_weights)\n\n    def update_archive(self, x, f):\n        if len(self.archive) < self.archive_size:\n            self.archive.append(x)\n            self.archive_fitness.append(f)\n        else:\n            max_fitness_index = np.argmax(self.archive_fitness)\n            if f < self.archive_fitness[max_fitness_index]:\n                self.archive[max_fitness_index] = x\n                self.archive_fitness[max_fitness_index] = f\n\n    def update_covariance_matrix(self):\n        # CMA-ES like covariance matrix adaptation\n        best_idx = np.argmin(self.fitness)\n        z = (self.population[best_idx] - np.mean(self.population, axis=0))\n        self.C = (1 - self.cma_learning_rate) * self.C + self.cma_learning_rate * np.outer(z, z)\n\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                strategy = self.select_strategy()\n                self.strategy_counts[strategy] += 1\n\n                x_mutated = self.mutate(strategy, i, func)\n                x_trial = self.crossover(x_mutated, i)\n                x_trial = np.clip(x_trial, func.bounds.lb, func.bounds.ub)\n\n                # Local Search\n                if np.random.rand() < self.local_search_probability:\n                    x_trial = self.local_search(x_trial, func)\n\n                f_trial = func(x_trial)\n                self.budget -= 1\n\n                if self.budget <= 0:\n                    break\n\n                if f_trial < self.fitness[i]:\n                    self.strategy_successes[strategy] += 1\n                    self.population[i] = x_trial.copy()\n                    self.fitness[i] = f_trial\n                    self.update_archive(x_trial.copy(), f_trial)\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = x_trial.copy()\n\n            if self.budget <= 0:\n                break\n\n            self.update_strategy_weights()\n            self.update_covariance_matrix()\n            best_index = np.argmin(self.fitness)\n            if self.fitness[best_index] < self.f_opt:\n                self.f_opt = self.fitness[best_index]\n                self.x_opt = self.population[best_index].copy()\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 12, "feedback": "The algorithm DynamicResourceAllocationDE scored 0.726 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["3c377e79-26ed-41cf-8828-7863aaef3453"], "operator": null, "metadata": {"aucs": [0.4488332523036175, 0.2341538013454909, 0.8055780417782814, 0.9261198169452391, 0.8538357778006934, 0.8695713006424137, 0.7872693939858088, 0.8032806699036676, 0.8358867820042022, 0.21525931042089386, 0.8926615895988461, 0.9885367922189855, 0.7980936490644368, 0.8301145241427909, 0.9504677109866742, 0.8587874314413787, 0.786437197176144, 0.9101545045675015, 0.21791857702179418, 0.5056417415404043]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
