{"id": "acc162f0-7d50-4bcf-873d-7723f312e8fd", "fitness": 0.37905768915578913, "name": "AdaptiveExploration", "description": "No description provided.", "code": "import numpy as np\n\nclass AdaptiveExploration:\n    def __init__(self, budget=10000, dim=10, pop_size=20, exploration_rate=0.5, local_search_radius=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.exploration_rate = exploration_rate\n        self.local_search_radius = local_search_radius\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        \n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n        \n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index].copy()\n\n        while self.budget > 0:\n            # Exploration phase\n            if np.random.rand() < self.exploration_rate:\n                new_population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n            \n            # Exploitation phase (local search around the best individual)\n            else:\n                new_population = np.clip(np.random.normal(self.x_opt, self.local_search_radius, size=(self.pop_size, self.dim)), func.bounds.lb, func.bounds.ub)\n\n            new_fitness = np.array([func(x) for x in new_population])\n            self.budget -= self.pop_size\n\n            # Update population\n            for i in range(self.pop_size):\n                if new_fitness[i] < fitness.max():\n                    worst_index = np.argmax(fitness)\n                    fitness[worst_index] = new_fitness[i]\n                    population[worst_index] = new_population[i].copy()\n\n            # Update best solution\n            best_index = np.argmin(fitness)\n            if fitness[best_index] < self.f_opt:\n                self.f_opt = fitness[best_index]\n                self.x_opt = population[best_index].copy()\n\n            # Adapt exploration rate based on success\n            improvement_ratio = np.sum(new_fitness < fitness) / self.pop_size\n            self.exploration_rate = np.clip(self.exploration_rate + 0.1 * (improvement_ratio - 0.5), 0.1, 0.9)\n        \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveExploration scored 0.379 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.11974456992971116, 0.18303798667362803, 0.46246371551937215, 0.3612925641178182, 0.22583755389135984, 0.2220703701602309, 0.27370808336529917, 0.2070737548995626, 0.37803679490021924, 0.16592671133477, 0.8840860203043195, 0.9984699756454666, 0.26334878062094647, 0.2455174182270774, 0.8316302754314328, 0.32987861464536505, 0.24404948606147037, 0.5596124985023487, 0.16425875143430146, 0.46110985745108324]}, "task_prompt": ""}
{"id": "58fdee90-151c-452c-ab23-92c42d00c0d5", "fitness": 0.5657085797715482, "name": "AdaptiveDE", "description": "No description provided.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, CR=0.7, stagnation_threshold=0.001, stagnation_iter=500):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.lb = -5.0\n        self.ub = 5.0\n        self.stagnation_threshold = stagnation_threshold\n        self.stagnation_iter = stagnation_iter\n        self.best_history = []\n\n    def __call__(self, func):\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.f_vals = np.array([func(x) for x in self.population])\n        self.f_opt = np.min(self.f_vals)\n        self.x_opt = self.population[np.argmin(self.f_vals)]\n        self.eval_count = self.pop_size #Initial population evaluation\n        \n        self.best_history.append(self.f_opt)\n        \n        iter_since_last_improvement = 0\n\n        while self.eval_count < self.budget:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n                mutant = a + self.F * (b - c)\n                mutant = np.clip(mutant, self.lb, self.ub)\n\n                # Crossover\n                crossover_mask = np.random.rand(self.dim) < self.CR\n                trial_vector = np.where(crossover_mask, mutant, self.population[i])\n\n                # Selection\n                f_trial = func(trial_vector)\n                self.eval_count += 1\n\n                if f_trial < self.f_vals[i]:\n                    self.population[i] = trial_vector\n                    self.f_vals[i] = f_trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial_vector\n                        iter_since_last_improvement = 0\n                    \n                        # Adaptive CR and F: If improvement found, modify params\n                        self.CR = np.random.normal(0.7, 0.1) \n                        self.F = np.random.normal(0.5, 0.1)\n                        self.CR = np.clip(self.CR, 0.0, 1.0)\n                        self.F = np.clip(self.F, 0.1, 1.0)\n                else:\n                    iter_since_last_improvement += 1\n                    \n                    # If parameters are stuck and aren't improving, modify params more aggressively\n                    if iter_since_last_improvement > 200:\n                         self.CR = np.random.rand()\n                         self.F = np.random.uniform(0.1, 0.9)\n\n                if self.eval_count >= self.budget:\n                    break\n            \n            # Stagnation check\n            if iter_since_last_improvement > self.stagnation_iter:\n                # Restart strategy\n                self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n                self.f_vals = np.array([func(x) for x in self.population])\n                self.eval_count += self.pop_size\n                best_index = np.argmin(self.f_vals)\n                \n                if self.f_vals[best_index] < self.f_opt:\n                    self.f_opt = self.f_vals[best_index]\n                    self.x_opt = self.population[best_index]\n                \n                iter_since_last_improvement = 0\n                # Reset the CR and F too.\n                self.CR = np.random.normal(0.7, 0.1) \n                self.F = np.random.normal(0.5, 0.1)\n                self.CR = np.clip(self.CR, 0.0, 1.0)\n                self.F = np.clip(self.F, 0.1, 1.0)\n                    \n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveDE scored 0.566 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.19335151138850581, 0.21591560158858314, 0.4659361383437889, 0.8349264562309239, 0.6973255393083303, 0.7369325871535297, 0.6169206186965057, 0.6037704972629281, 0.669128532334841, 0.1954772382404657, 0.8019212955942006, 0.9989241735286281, 0.261344541769721, 0.6619343268785636, 0.7451863744218901, 0.7589301918934392, 0.4135654286688386, 0.7808410618095867, 0.1753447516608745, 0.4864947286568191]}, "task_prompt": ""}
{"id": "d83802a2-5594-42fe-a0bf-18730b94c24c", "fitness": "-inf", "name": "AdaptiveDE_NM", "description": "No description provided.", "code": "import numpy as np\n\nclass AdaptiveDE_NM:\n    def __init__(self, budget=10000, dim=10, pop_size=15, F=0.5, CR=0.7, nm_init_size=0.5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.nm_init_size = nm_init_size\n        self.lb = -5.0\n        self.ub = 5.0\n        self.eval_count = 0\n        self.simplex = None\n        self.simplex_size = self.nm_init_size\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.eval_count += self.pop_size\n\n        if np.min(self.fitness) < self.f_opt:\n            self.f_opt = np.min(self.fitness)\n            self.x_opt = self.population[np.argmin(self.fitness)]\n\n        while self.eval_count < self.budget:\n            for i in range(self.pop_size):\n                if self.eval_count >= self.budget:\n                    break\n                # Differential Evolution Mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = np.random.choice(idxs, 3, replace=False)\n                mutant = self.population[a] + self.F * (self.population[b] - self.population[c])\n                mutant = np.clip(mutant, self.lb, self.ub)\n\n                # Crossover\n                trial = np.copy(self.population[i])\n                mask = np.random.rand(self.dim) < self.CR\n                trial[mask] = mutant[mask]\n\n                # Selection\n                f_trial = func(trial)\n                self.eval_count += 1\n\n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n            # Local Search with Nelder-Mead on the best individual\n            best_idx = np.argmin(self.fitness)\n            best_x = self.population[best_idx].copy()\n\n            if self.simplex is None:\n                 self.simplex = self.initialize_simplex(best_x, self.simplex_size)\n            \n            self.simplex, improved = self.nelder_mead(func, self.simplex)\n            \n            \n            if improved:\n                best_simplex_fitness = func(self.simplex[0])\n                if best_simplex_fitness < self.f_opt:\n                    self.f_opt = best_simplex_fitness\n                    self.x_opt = self.simplex[0]\n                self.simplex_size = self.nm_init_size # Reset simplex size after improvement\n            else:\n                # Shrink the simplex if no improvement\n                self.simplex_size *= 0.9\n                self.simplex = self.initialize_simplex(best_x, self.simplex_size)\n\n            for i in range(len(self.simplex)):\n              self.simplex[i] = np.clip(self.simplex[i], self.lb, self.ub)\n        return self.f_opt, self.x_opt\n\n    def initialize_simplex(self, x0, step_size):\n        simplex = [x0]\n        for i in range(self.dim):\n            x = x0.copy()\n            x[i] += step_size\n            simplex.append(x)\n        return np.array(simplex)\n    \n\n    def nelder_mead(self, func, simplex, alpha=1, beta=0.5, gamma=2):\n      \n        improved = False\n        for _ in range(min(self.dim+1, self.budget - self.eval_count)):\n\n            # 1. Order the simplex\n            fitness_values = np.array([func(x) for x in simplex])\n            self.eval_count += len(simplex) - len(fitness_values)\n            if self.eval_count > self.budget:\n              break\n\n            sorted_indices = np.argsort(fitness_values)\n            simplex = simplex[sorted_indices]\n            fitness_values = fitness_values[sorted_indices]\n\n            best = simplex[0]\n            worst = simplex[-1]\n\n            # 2. Calculate centroid (excluding the worst point)\n            centroid = np.mean(simplex[:-1], axis=0)\n\n            # 3. Reflection\n            reflected_point = centroid + alpha * (centroid - worst)\n            reflected_point = np.clip(reflected_point, self.lb, self.ub)\n            f_reflected = func(reflected_point)\n            self.eval_count += 1\n\n            if self.eval_count > self.budget:\n              break\n            \n            if fitness_values[0] <= f_reflected < fitness_values[-2]:\n                simplex[-1] = reflected_point\n                improved = True\n                continue\n\n            # 4. Expansion\n            if f_reflected < fitness_values[0]:\n                expanded_point = centroid + gamma * (reflected_point - centroid)\n                expanded_point = np.clip(expanded_point, self.lb, self.ub)\n                f_expanded = func(expanded_point)\n                self.eval_count += 1\n                if self.eval_count > self.budget:\n                  break\n\n                if f_expanded < f_reflected:\n                    simplex[-1] = expanded_point\n                    improved = True\n                    continue\n                else:\n                    simplex[-1] = reflected_point\n                    improved = True\n                    continue\n\n            # 5. Contraction\n            if f_reflected >= fitness_values[-2]:\n                contracted_point = centroid + beta * (worst - centroid)\n                contracted_point = np.clip(contracted_point, self.lb, self.ub)\n\n                f_contracted = func(contracted_point)\n                self.eval_count += 1\n\n                if self.eval_count > self.budget:\n                  break\n\n                if f_contracted < fitness_values[-1]:\n                    simplex[-1] = contracted_point\n                    improved = True\n                    continue\n                else:\n                   # 6. Shrink\n                    for i in range(1, len(simplex)):\n                        simplex[i] = best + beta * (simplex[i] - best)\n                        simplex[i] = np.clip(simplex[i], self.lb, self.ub)\n                    \n                    new_fitness_values = np.array([func(x) for x in simplex[1:]])\n\n                    self.eval_count += len(simplex) - 1\n                    if self.eval_count > self.budget:\n                      break\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "In the code, line 65, in __call__, the following error occurred:\nValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\nOn line: if improved:", "error": "In the code, line 65, in __call__, the following error occurred:\nValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\nOn line: if improved:", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "98f0f362-e4bc-47bf-a607-a1f1a3d24461", "fitness": 0.7049472635430609, "name": "AdaptiveDE", "description": "No description provided.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, CR=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.lb = -5.0\n        self.ub = 5.0\n\n    def __call__(self, func):\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n        eval_count = self.pop_size\n\n        while eval_count < self.budget:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + self.F * (b - c), self.lb, self.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, self.population[i])\n\n                # Selection\n                f = func(trial)\n                eval_count += 1\n\n                if f < self.fitness[i]:\n                    self.fitness[i] = f\n                    self.population[i] = trial\n\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = trial\n            \n            # Adaptive F and CR\n            self.F = np.clip(np.random.normal(0.5, 0.1), 0.1, 1.0)\n            self.CR = np.clip(np.random.normal(0.9, 0.1), 0.1, 1.0)\n\n            #Population Size Reduction\n            if eval_count > self.budget * 0.75 and self.pop_size > 10:\n               worst_index = np.argmax(self.fitness)\n               self.population = np.delete(self.population, worst_index, axis = 0)\n               self.fitness = np.delete(self.fitness, worst_index)\n               self.pop_size -= 1\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveDE scored 0.705 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.351604436312977, 0.6662141594122245, 0.7311664798981237, 0.8663852422592808, 0.7509123750085697, 0.7837769856586964, 0.6067883123852826, 0.7003790876298495, 0.7338259425860367, 0.6802911164695169, 0.8507218012319171, 0.9853463936088566, 0.6773347873952261, 0.7227026688393487, 0.9139943571237956, 0.7840461333744533, 0.6325668158890017, 0.8334097967385959, 0.2999604667196941, 0.5275179123197711]}, "task_prompt": ""}
{"id": "e137c9c2-56ea-49e6-b41c-5a1954fa79ea", "fitness": 0.0, "name": "AdaptiveStepSizeES", "description": "No description provided.", "code": "import numpy as np\n\nclass AdaptiveStepSizeES:\n    def __init__(self, budget=10000, dim=10, pop_size=20, step_size=0.5, success_threshold=0.2):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.step_size = step_size\n        self.success_threshold = success_threshold\n        self.archive_x = []\n        self.archive_f = []\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        \n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.archive_x.extend(population.tolist())\n        self.archive_f.extend(fitness.tolist())\n        self.budget -= self.pop_size\n\n        # Main loop\n        while self.budget > 0:\n            # Generate offspring\n            noise = np.random.normal(0, self.step_size, size=(self.pop_size, self.dim))\n            offspring = population + noise\n            offspring = np.clip(offspring, func.bounds.lb, func.bounds.ub)\n\n            # Evaluate offspring\n            offspring_fitness = np.array([func(x) for x in offspring])\n            self.archive_x.extend(offspring.tolist())\n            self.archive_f.extend(offspring_fitness.tolist())\n            self.budget -= self.pop_size\n\n            # Selection (replace parents with better offspring)\n            successful_steps = 0\n            for i in range(self.pop_size):\n                if offspring_fitness[i] < fitness[i]:\n                    population[i] = offspring[i]\n                    fitness[i] = offspring_fitness[i]\n                    successful_steps += 1\n\n            # Adapt step size\n            success_rate = successful_steps / self.pop_size\n            if success_rate > self.success_threshold:\n                self.step_size *= 1.1  # Increase step size\n            else:\n                self.step_size *= 0.9  # Decrease step size\n\n            # Local Search (every few iterations)\n            if self.budget > 0 and (self.budget % (self.pop_size*5)) == 0: \n                best_index = np.argmin(fitness)\n                x_local = population[best_index]\n                noise_local = np.random.normal(0, self.step_size/5, size=self.dim)\n                x_local_new = x_local + noise_local\n                x_local_new = np.clip(x_local_new, func.bounds.lb, func.bounds.ub)\n                f_local_new = func(x_local_new)\n                self.archive_x.append(x_local_new.tolist())\n                self.archive_f.append(f_local_new)\n                self.budget -= 1\n\n                if f_local_new < fitness[best_index]:\n                    population[best_index] = x_local_new\n                    fitness[best_index] = f_local_new\n                    \n            # Update best solution\n            best_fitness = np.min(fitness)\n            best_index = np.argmin(fitness)\n            if best_fitness < self.f_opt:\n                self.f_opt = best_fitness\n                self.x_opt = population[best_index]\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveStepSizeES scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0]}, "task_prompt": ""}
{"id": "83b6a10b-a382-4815-a082-9bd84d269d71", "fitness": 0.6544105838320119, "name": "AdaptiveDE", "description": "No description provided.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, CR=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.lb = -5.0\n        self.ub = 5.0\n        self.population = None\n        self.fitness = None\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.success_F = []\n        self.success_CR = []\n        self.archive = []\n        self.archive_size = 10\n        self.restart_iter = budget // 10\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.pop_size\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n\n    def mutate(self, pop, i):\n        idxs = [idx for idx in range(self.pop_size) if idx != i]\n        a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n        return np.clip(a + self.F * (b - c), self.lb, self.ub)\n\n    def crossover(self, mutant, target):\n        cross_points = np.random.rand(self.dim) < self.CR\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        return np.where(cross_points, mutant, target)\n\n    def update_parameters(self):\n        if self.success_F:\n            self.F = np.mean(self.success_F)\n            self.CR = np.mean(self.success_CR)\n        else:\n            self.F = 0.5\n            self.CR = 0.9\n        \n        self.F = np.clip(self.F, 0.1, 0.9)\n        self.CR = np.clip(self.CR, 0.1, 0.9)\n        self.success_F = []\n        self.success_CR = []\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        iter_count = 0\n        while self.evals < self.budget:\n            iter_count += 1\n            if iter_count % self.restart_iter == 0:\n                self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n                self.fitness = np.array([func(x) for x in self.population])\n                self.evals += self.pop_size\n                self.f_opt = np.min(self.fitness)\n                self.x_opt = self.population[np.argmin(self.fitness)]\n                self.update_parameters()\n                continue\n\n            new_population = []\n            new_fitness = []\n\n            for i in range(self.pop_size):\n                mutant = self.mutate(self.population, i)\n                trial = self.crossover(mutant, self.population[i])\n                f = func(trial)\n                self.evals += 1\n\n                if f < self.fitness[i]:\n                    new_population.append(trial)\n                    new_fitness.append(f)\n\n                    self.success_F.append(self.F)\n                    self.success_CR.append(self.CR)\n\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = trial\n                else:\n                    new_population.append(self.population[i])\n                    new_fitness.append(self.fitness[i])\n            \n            self.population = np.array(new_population)\n            self.fitness = np.array(new_fitness)\n            self.update_parameters()\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveDE scored 0.654 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.2829835521320415, 0.5820528052720009, 0.5973271454560594, 0.8686788690537736, 0.6778805836142207, 0.7289616467239485, 0.5854371555690268, 0.5610712479874065, 0.6960384941866067, 0.6248548080566477, 0.8416949191353953, 0.9998246888285107, 0.5899132694698114, 0.6749550350993483, 0.8837710952980382, 0.7533096756393661, 0.5506234229308595, 0.7944981155908157, 0.28267606769903775, 0.5116590788973221]}, "task_prompt": ""}
{"id": "e1bf3d37-1ae2-43c5-bec9-c3daf6259ee0", "fitness": 0.0, "name": "AdaptivePSO", "description": "No description provided.", "code": "import numpy as np\n\nclass AdaptivePSO:\n    def __init__(self, budget=10000, dim=10, n_particles=20, inertia=0.7, c1=1.5, c2=1.5):\n        self.budget = budget\n        self.dim = dim\n        self.n_particles = n_particles\n        self.inertia = inertia\n        self.c1 = c1\n        self.c2 = c2\n        self.lb = -5.0\n        self.ub = 5.0\n        self.x = np.random.uniform(self.lb, self.ub, size=(self.n_particles, self.dim))\n        self.v = np.random.uniform(-1, 1, size=(self.n_particles, self.dim))\n        self.pbest_x = self.x.copy()\n        self.pbest_f = np.full(self.n_particles, np.inf)\n        self.gbest_x = None\n        self.gbest_f = np.inf\n        self.eval_count = 0\n        self.step_size = 1.0\n\n    def __call__(self, func):\n        while self.eval_count < self.budget:\n            for i in range(self.n_particles):\n                f = func(self.x[i])\n                self.eval_count += 1\n                if f < self.pbest_f[i]:\n                    self.pbest_f[i] = f\n                    self.pbest_x[i] = self.x[i].copy()\n\n                if f < self.gbest_f:\n                    self.gbest_f = f\n                    self.gbest_x = self.x[i].copy()\n\n            self.f_opt = self.gbest_f\n            self.x_opt = self.gbest_x\n\n            for i in range(self.n_particles):\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                self.v[i] = self.inertia * self.v[i] + \\\n                            self.c1 * r1 * (self.pbest_x[i] - self.x[i]) + \\\n                            self.c2 * r2 * (self.gbest_x - self.x[i])\n\n                self.x[i] = self.x[i] + self.v[i]\n\n                self.x[i] = np.clip(self.x[i], self.lb, self.ub)\n\n            # Local search around the best particle\n            if self.eval_count < self.budget:\n                x_local = self.gbest_x + np.random.normal(0, self.step_size, self.dim)\n                x_local = np.clip(x_local, self.lb, self.ub)\n                f_local = func(x_local)\n                self.eval_count += 1\n                if f_local < self.gbest_f:\n                    self.gbest_f = f_local\n                    self.gbest_x = x_local.copy()\n                    self.step_size *= 1.1 # Increase step size if improving\n                else:\n                    self.step_size *= 0.9 # Reduce step size if not improving\n\n                self.f_opt = self.gbest_f\n                self.x_opt = self.gbest_x\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptivePSO scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0]}, "task_prompt": ""}
{"id": "598bc2c5-2974-4699-8990-3c0057e0f0e2", "fitness": 0.0, "name": "DifferentialEvolutionLocalSearch", "description": "No description provided.", "code": "import numpy as np\n\nclass DifferentialEvolutionLocalSearch:\n    def __init__(self, budget=10000, dim=10, pop_size=20, F=0.7, CR=0.9, local_search_iters=5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.local_search_iters = local_search_iters\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        \n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        # Main optimization loop\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Differential Evolution\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                \n                mutant = np.clip(a + self.F * (b - c), func.bounds.lb, func.bounds.ub)\n                \n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Evaluation\n                f = func(trial)\n                self.budget -= 1\n                \n                if f < fitness[i]:\n                    fitness[i] = f\n                    population[i] = trial\n                \n                    # Local Search (optional, apply only if improvement)\n                    for _ in range(self.local_search_iters):\n                        if self.budget <= 0:\n                            break\n                        \n                        # Create a small perturbation around the current best\n                        perturbation = np.random.normal(0, 0.01, size=self.dim)\n                        local_trial = np.clip(trial + perturbation, func.bounds.lb, func.bounds.ub)\n                        local_f = func(local_trial)\n                        self.budget -= 1\n                        \n                        if local_f < fitness[i]:\n                            fitness[i] = local_f\n                            population[i] = local_trial\n                            trial = local_trial # update trial for further local search\n\n            # Update overall best\n            best_idx = np.argmin(fitness)\n            if fitness[best_idx] < self.f_opt:\n                self.f_opt = fitness[best_idx]\n                self.x_opt = population[best_idx]\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm DifferentialEvolutionLocalSearch scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0]}, "task_prompt": ""}
{"id": "351c2a20-36e4-43d9-be1c-cbb770efc334", "fitness": 0.0, "name": "HybridOptimization", "description": "No description provided.", "code": "import numpy as np\n\nclass HybridOptimization:\n    def __init__(self, budget=10000, dim=10, pop_size=20, lr=0.01, exploration_prob=0.3, diversity_threshold=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lr = lr\n        self.exploration_prob = exploration_prob\n        self.diversity_threshold = diversity_threshold\n        self.lb = -5.0\n        self.ub = 5.0\n        self.population = None\n        self.fitness = None\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.evals = 0\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals += self.pop_size\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)].copy()\n\n    def calculate_diversity(self):\n        centroid = np.mean(self.population, axis=0)\n        distances = np.linalg.norm(self.population - centroid, axis=1)\n        diversity = np.mean(distances)\n        return diversity\n\n    def gradient_descent(self, func, x):\n        # Estimate gradient (simple finite difference)\n        h = 1e-5\n        gradient = np.zeros(self.dim)\n        for i in range(self.dim):\n            x_plus_h = x.copy()\n            x_plus_h[i] += h\n            f_plus_h = func(x_plus_h)\n            self.evals += 1\n\n            x_minus_h = x.copy()\n            x_minus_h[i] -= h\n            f_minus_h = func(x_minus_h)\n            self.evals += 1\n           \n            gradient[i] = (f_plus_h - f_minus_h) / (2 * h)\n           \n            if self.evals >= self.budget:\n                break\n        \n        # Gradient Descent step\n        x_new = x - self.lr * gradient\n        x_new = np.clip(x_new, self.lb, self.ub)\n\n        return x_new\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.evals < self.budget:\n            diversity = self.calculate_diversity()\n            for i in range(self.pop_size):\n                if np.random.rand() < self.exploration_prob or diversity < self.diversity_threshold:\n                    # Exploration: Random jump\n                    new_x = np.random.uniform(self.lb, self.ub, size=self.dim)\n                else:\n                    # Exploitation: Gradient-based local search\n                    new_x = self.gradient_descent(func, self.population[i].copy())\n                \n                if self.evals >= self.budget:\n                    break\n\n                f_new = func(new_x)\n                self.evals += 1\n                if f_new < self.fitness[i]:\n                    self.population[i] = new_x\n                    self.fitness[i] = f_new\n\n                    if f_new < self.f_opt:\n                        self.f_opt = f_new\n                        self.x_opt = new_x.copy()\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm HybridOptimization scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0]}, "task_prompt": ""}
{"id": "be0903b3-0c51-4fd3-8f6c-cb4b28a0d586", "fitness": 0.3221784055170086, "name": "AdaptiveSimulatedAnnealing", "description": "No description provided.", "code": "import numpy as np\n\nclass AdaptiveSimulatedAnnealing:\n    def __init__(self, budget=10000, dim=10, initial_temp=1.0, temp_decay=0.99):\n        self.budget = budget\n        self.dim = dim\n        self.initial_temp = initial_temp\n        self.temp_decay = temp_decay\n        self.lb = -5.0\n        self.ub = 5.0\n\n    def __call__(self, func):\n        self.x_opt = np.random.uniform(self.lb, self.ub, size=self.dim)\n        self.f_opt = func(self.x_opt)\n        eval_count = 1\n        \n        current_x = self.x_opt.copy()\n        current_f = self.f_opt\n\n        temperature = self.initial_temp\n\n        while eval_count < self.budget:\n            # Generate a neighbor solution\n            new_x = current_x + np.random.normal(0, temperature, size=self.dim)\n            new_x = np.clip(new_x, self.lb, self.ub)\n            \n            new_f = func(new_x)\n            eval_count += 1\n\n            # Acceptance probability\n            delta_f = new_f - current_f\n            if delta_f < 0 or np.random.rand() < np.exp(-delta_f / temperature):\n                current_x = new_x.copy()\n                current_f = new_f\n\n                if new_f < self.f_opt:\n                    self.f_opt = new_f\n                    self.x_opt = new_x.copy()\n\n            # Adaptive temperature schedule based on acceptance rate\n            if eval_count % 100 == 0:\n                acceptance_rate = np.mean(np.exp(-np.maximum(0, delta_f) / temperature) > np.random.rand(100))\n                if acceptance_rate > 0.5:\n                    temperature *= 1.05  # Increase temperature if accepting too often\n                else:\n                    temperature *= self.temp_decay # Decrease temperature\n                temperature = min(temperature, self.initial_temp)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveSimulatedAnnealing scored 0.322 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.1777140023647411, 0.24158227205914895, 0.27356425823398356, 0.2737676036715856, 0.2393676062640242, 0.26248374159876864, 0.2729360492287508, 0.2561396987887966, 0.22850982331635694, 0.19532744263317048, 0.2919896536215679, 0.9987305137650825, 0.253293928172431, 0.2421145499148637, 0.6767942082701286, 0.3029673398493684, 0.25975405421444164, 0.33806471220547096, 0.19760513323544193, 0.46086151893204985]}, "task_prompt": ""}
{"id": "e4271674-5a27-4d77-8294-d0008dfd3ccf", "fitness": "-inf", "name": "GaussianMixtureOptimiser", "description": "No description provided.", "code": "import numpy as np\nfrom sklearn.mixture import GaussianMixture\n\nclass GaussianMixtureOptimiser:\n    def __init__(self, budget=10000, dim=10, pop_size=50, n_components=5, local_search_iterations=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.n_components = n_components\n        self.lb = -5.0\n        self.ub = 5.0\n        self.gmm = None\n        self.population = None\n        self.fitness = None\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.local_search_iterations = local_search_iterations\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.pop_size\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n\n    def fit_gmm(self):\n        self.gmm = GaussianMixture(n_components=self.n_components, covariance_type='full', max_iter=100, random_state=42)\n        self.gmm.fit(self.population)\n\n    def sample_new_solutions(self, n_samples):\n        new_solutions = self.gmm.sample(n_samples)[0]\n        return np.clip(new_solutions, self.lb, self.ub)\n    \n    def local_search(self, func, x):\n        best_x = x\n        best_f = func(x)\n        self.evals += 1\n        \n        for _ in range(self.local_search_iterations):\n            # Generate a random perturbation\n            perturbation = np.random.normal(0, 0.1, size=self.dim)\n            new_x = np.clip(x + perturbation, self.lb, self.ub)\n            new_f = func(new_x)\n            self.evals += 1\n            \n            if new_f < best_f:\n                best_f = new_f\n                best_x = new_x\n        \n        return best_x, best_f\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.evals < self.budget:\n            self.fit_gmm()\n            new_solutions = self.sample_new_solutions(self.pop_size)\n            \n            for i in range(self.pop_size):\n                # Apply local search\n                x, f = self.local_search(func, new_solutions[i])\n                \n                if f < self.f_opt:\n                    self.f_opt = f\n                    self.x_opt = x\n                \n                if f < np.max(self.fitness):\n                    worst_index = np.argmax(self.fitness)\n                    self.population[worst_index] = x\n                    self.fitness[worst_index] = f\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "In the code, line 27, in fit_gmm, the following error occurred:\nNameError: name 'GaussianMixture' is not defined\nOn line: self.gmm = GaussianMixture(n_components=self.n_components, covariance_type='full', max_iter=100, random_state=42)", "error": "In the code, line 27, in fit_gmm, the following error occurred:\nNameError: name 'GaussianMixture' is not defined\nOn line: self.gmm = GaussianMixture(n_components=self.n_components, covariance_type='full', max_iter=100, random_state=42)", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "e3b3d7f2-a09f-40bf-af00-f3f69ccab214", "fitness": 0.11852736898948033, "name": "AdaptiveSA", "description": "No description provided.", "code": "import numpy as np\n\nclass AdaptiveSA:\n    def __init__(self, budget=10000, dim=10, initial_temp=100.0, cooling_rate=0.95):\n        self.budget = budget\n        self.dim = dim\n        self.initial_temp = initial_temp\n        self.cooling_rate = cooling_rate\n        self.lb = -5.0\n        self.ub = 5.0\n\n    def __call__(self, func):\n        self.x_opt = np.random.uniform(self.lb, self.ub, size=self.dim)\n        self.f_opt = func(self.x_opt)\n        eval_count = 1\n        temperature = self.initial_temp\n\n        while eval_count < self.budget:\n            # Generate neighbor solution\n            x_new = self.x_opt + np.random.normal(0, temperature/self.initial_temp, size=self.dim)\n            x_new = np.clip(x_new, self.lb, self.ub)\n\n            f_new = func(x_new)\n            eval_count += 1\n\n            # Acceptance probability\n            delta_f = f_new - self.f_opt\n            if delta_f < 0 or np.random.rand() < np.exp(-delta_f / temperature):\n                self.x_opt = x_new\n                self.f_opt = f_new\n\n            # Adaptive temperature adjustment\n            temperature *= self.cooling_rate\n            if eval_count > self.budget*0.75:\n                self.cooling_rate = 0.99 # Lower cooling rate after 75% of budget\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveSA scored 0.119 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [9.999999999998899e-05, 0.03429052547973366, 0.19766195806463094, 0.20307074659334912, 0.033734207286583495, 0.1442847556513298, 0.04034051154155838, 0.0766087018898397, 0.15478412506964323, 0.1126009899586512, 0.12572567216000663, 0.15941500990542534, 0.22804713296640544, 0.07371111262275976, 0.08677731238450759, 0.16282734037386726, 0.12182855553077865, 0.15240213419953075, 0.14895310198742662, 0.11338348612357929]}, "task_prompt": ""}
{"id": "6cc75462-87c2-4710-ae23-2c6229908926", "fitness": 0.0, "name": "PatternAdaptiveSearch", "description": "No description provided.", "code": "import numpy as np\n\nclass PatternAdaptiveSearch:\n    def __init__(self, budget=10000, dim=10, initial_step_size=1.0, reduction_factor=0.5, expansion_factor=1.2):\n        self.budget = budget\n        self.dim = dim\n        self.step_size = initial_step_size\n        self.reduction_factor = reduction_factor\n        self.expansion_factor = expansion_factor\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        self.f_opt = func(self.x_opt)\n        self.budget -= 1\n        \n        while self.budget > 0:\n            improved = False\n            for i in range(self.dim):\n                # Explore positive direction\n                x_new_pos = self.x_opt.copy()\n                x_new_pos[i] += self.step_size\n                x_new_pos = np.clip(x_new_pos, func.bounds.lb, func.bounds.ub)\n                f_new_pos = func(x_new_pos)\n                self.budget -= 1\n\n                if f_new_pos < self.f_opt:\n                    self.f_opt = f_new_pos\n                    self.x_opt = x_new_pos\n                    improved = True\n                    continue  # Continue to the next dimension\n\n                # Explore negative direction\n                x_new_neg = self.x_opt.copy()\n                x_new_neg[i] -= self.step_size\n                x_new_neg = np.clip(x_new_neg, func.bounds.lb, func.bounds.ub)\n                f_new_neg = func(x_new_neg)\n                self.budget -= 1\n\n                if f_new_neg < self.f_opt:\n                    self.f_opt = f_new_neg\n                    self.x_opt = x_new_neg\n                    improved = True\n                \n            if not improved and self.budget > 0:\n                self.step_size *= self.reduction_factor  # Reduce step size if no improvement\n                if self.step_size < 1e-6:\n                    self.x_opt = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                    self.f_opt = func(self.x_opt)\n                    self.budget -= 1\n                    self.step_size = 1.0\n            elif improved:\n                self.step_size *= self.expansion_factor\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm PatternAdaptiveSearch scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0]}, "task_prompt": ""}
{"id": "084c3e9f-6131-4766-b878-51b4a369aea9", "fitness": 0.0, "name": "AdaptivePSO", "description": "No description provided.", "code": "import numpy as np\n\nclass AdaptivePSO:\n    def __init__(self, budget=10000, dim=10, pop_size=50, w=0.7, c1=1.5, c2=1.5, stagnation_iter=500):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.w = w\n        self.c1 = c1\n        self.c2 = c2\n        self.lb = -5.0\n        self.ub = 5.0\n        self.stagnation_iter = stagnation_iter\n        self.particles = None\n        self.velocities = None\n        self.fitness = None\n        self.personal_best_positions = None\n        self.personal_best_fitness = None\n        self.global_best_position = None\n        self.global_best_fitness = np.inf\n        self.evals = 0\n        self.stagnation_count = 0\n\n    def initialize_population(self, func):\n        self.particles = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, size=(self.pop_size, self.dim))  # Initialize velocities\n        self.fitness = np.array([func(x) for x in self.particles])\n        self.evals += self.pop_size\n        self.personal_best_positions = self.particles.copy()\n        self.personal_best_fitness = self.fitness.copy()\n        self.global_best_position = self.particles[np.argmin(self.fitness)].copy()\n        self.global_best_fitness = np.min(self.fitness)\n        self.x_opt = self.global_best_position\n        self.f_opt = self.global_best_fitness\n\n    def update_velocity(self, i):\n        r1 = np.random.rand(self.dim)\n        r2 = np.random.rand(self.dim)\n        cognitive_component = self.c1 * r1 * (self.personal_best_positions[i] - self.particles[i])\n        social_component = self.c2 * r2 * (self.global_best_position - self.particles[i])\n        self.velocities[i] = self.w * self.velocities[i] + cognitive_component + social_component\n        \n        # Velocity clamping to prevent explosion\n        v_max = (self.ub - self.lb) * 0.1  # Example clamping value\n        self.velocities[i] = np.clip(self.velocities[i], -v_max, v_max)\n\n    def update_position(self, i):\n        self.particles[i] = self.particles[i] + self.velocities[i]\n        self.particles[i] = np.clip(self.particles[i], self.lb, self.ub)\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.evals < self.budget:\n            for i in range(self.pop_size):\n                self.update_velocity(i)\n                self.update_position(i)\n\n                f = func(self.particles[i])\n                self.evals += 1\n\n                if f < self.personal_best_fitness[i]:\n                    self.personal_best_fitness[i] = f\n                    self.personal_best_positions[i] = self.particles[i].copy()\n\n                    if f < self.global_best_fitness:\n                        self.global_best_fitness = f\n                        self.global_best_position = self.particles[i].copy()\n                        self.x_opt = self.global_best_position\n                        self.f_opt = self.global_best_fitness\n                        self.stagnation_count = 0  # Reset stagnation count\n                else:\n                    self.stagnation_count +=1\n\n                if self.evals >= self.budget:\n                    break\n\n            if self.stagnation_count > self.stagnation_iter:\n                # Restart strategy - re-initialize particles except best one\n                best_index = np.argmin(self.fitness)\n                self.particles = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n                self.particles[best_index] = self.global_best_position.copy()  # Keep the best\n                \n                self.velocities = np.random.uniform(-1, 1, size=(self.pop_size, self.dim))\n                self.fitness = np.array([func(x) for x in self.particles])\n                self.evals += self.pop_size - 1  # Minus 1 as we kept the best\n                self.personal_best_positions = self.particles.copy()\n                self.personal_best_fitness = self.fitness.copy()\n                \n                if self.global_best_fitness < np.min(self.fitness):\n                    pass #global best stays as is\n                else:\n                    self.global_best_position = self.particles[np.argmin(self.fitness)].copy()\n                    self.global_best_fitness = np.min(self.fitness)\n                    self.x_opt = self.global_best_position\n                    self.f_opt = self.global_best_fitness\n                self.stagnation_count = 0  # Reset stagnation count\n                \n                # Adjust inertia weight after restart\n                self.w = np.clip(self.w * 0.9, 0.4, 0.9)\n            else:\n                # Linearly decrease the inertia weight\n                self.w = np.clip(self.w - (0.3 / (self.budget / self.pop_size)), 0.4, 0.9)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptivePSO scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0]}, "task_prompt": ""}
{"id": "bde8c989-9d8b-4e42-819e-9cc4d8b2af13", "fitness": 0.0, "name": "GradientEstimationSearch", "description": "No description provided.", "code": "import numpy as np\n\nclass GradientEstimationSearch:\n    def __init__(self, budget=10000, dim=10, step_size=0.1, num_samples=10):\n        self.budget = budget\n        self.dim = dim\n        self.step_size = step_size\n        self.num_samples = num_samples\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        x = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        f = func(x)\n        self.budget -= 1\n        \n        self.f_opt = f\n        self.x_opt = x.copy()\n\n        while self.budget > 0:\n            # Estimate gradient using finite differences\n            gradient = np.zeros(self.dim)\n            for i in range(self.dim):\n                x_plus = x.copy()\n                x_minus = x.copy()\n                \n                x_plus[i] += self.step_size\n                x_minus[i] -= self.step_size\n\n                x_plus = np.clip(x_plus, func.bounds.lb, func.bounds.ub)\n                x_minus = np.clip(x_minus, func.bounds.lb, func.bounds.ub)\n\n                f_plus = func(x_plus)\n                f_minus = func(x_minus)\n                self.budget -= 2\n\n                gradient[i] = (f_plus - f_minus) / (2 * self.step_size)\n\n                if self.budget <= 0:\n                    return self.f_opt, self.x_opt\n\n            # Move in the direction of the negative gradient\n            x_new = x - self.step_size * gradient\n            x_new = np.clip(x_new, func.bounds.lb, func.bounds.ub)\n            \n            f_new = func(x_new)\n            self.budget -= 1\n\n            # Accept the new solution if it's better\n            if f_new < self.f_opt:\n                self.f_opt = f_new\n                self.x_opt = x_new.copy()\n                x = x_new.copy()\n            else:\n                # Random perturbation if no improvement\n                x_rand = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                f_rand = func(x_rand)\n                self.budget -= 1\n                if f_rand < self.f_opt:\n                   self.f_opt = f_rand\n                   self.x_opt = x_rand.copy()\n                   x = x_rand.copy()\n\n\n            # Adapt step size\n            if self.budget > 0:\n                self.step_size *= 0.99\n        \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm GradientEstimationSearch scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0]}, "task_prompt": ""}
{"id": "0a9e13ec-eea8-4756-a9b1-51cf8b8dd6de", "fitness": 0.25101256126974936, "name": "PopulationAnnealing", "description": "No description provided.", "code": "import numpy as np\n\nclass PopulationAnnealing:\n    def __init__(self, budget=10000, dim=10, pop_size=20, initial_temp=100, temp_decay=0.95):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.initial_temp = initial_temp\n        self.temp_decay = temp_decay\n        self.lb = -5.0\n        self.ub = 5.0\n        self.temp = initial_temp\n        self.acceptance_rate = 0.0\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.eval_count = self.pop_size\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n        accepted_count = 0\n\n        while self.eval_count < self.budget:\n            for i in range(self.pop_size):\n                # Perturbation using DE-like mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n                mutant = a + 0.5 * (b - c)\n                mutant = np.clip(mutant, self.lb, self.ub)\n\n                f_mutant = func(mutant)\n                self.eval_count += 1\n\n                delta_e = f_mutant - self.fitness[i]\n\n                if delta_e < 0 or np.random.rand() < np.exp(-delta_e / self.temp):\n                    self.population[i] = mutant\n                    self.fitness[i] = f_mutant\n                    accepted_count += 1\n\n                    if f_mutant < self.f_opt:\n                        self.f_opt = f_mutant\n                        self.x_opt = mutant\n\n                if self.eval_count >= self.budget:\n                    break\n            \n            self.acceptance_rate = accepted_count / self.pop_size\n            accepted_count = 0\n            #Adaptive temperature\n            if self.acceptance_rate > 0.5:\n                self.temp *= 1.05 #Slow down cooling if too many accepted\n            else:\n                self.temp *= self.temp_decay\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm PopulationAnnealing scored 0.251 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.11518719124652688, 0.1704965307063503, 0.2747503253658009, 0.1527521652402638, 0.19165030159235463, 0.18173492226546517, 0.21985491327668427, 0.17801869294188177, 0.16095270323037747, 0.13809832711910452, 0.18093641350803824, 0.9984611333276281, 0.24019656947219847, 0.19369010219472926, 0.3179955040960001, 0.2264082192243967, 0.2299346107994079, 0.2374570258525871, 0.16204807446396718, 0.4496274994712248]}, "task_prompt": ""}
{"id": "5d84303a-b189-461f-96e6-799b09f954e8", "fitness": 0.14047028011198054, "name": "BudgetAwareCMAES", "description": "No description provided.", "code": "import numpy as np\n\nclass BudgetAwareCMAES:\n    def __init__(self, budget=10000, dim=10, pop_size=None, initial_sigma=0.5, cs=0.3, damps=1.0, ccov1=None, ccovmu=None, restart_trigger=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size if pop_size is not None else 4 + int(3 * np.log(dim))\n        self.initial_sigma = initial_sigma\n        self.lb = -5.0\n        self.ub = 5.0\n        self.mean = np.random.uniform(self.lb, self.ub, size=self.dim)\n        self.sigma = self.initial_sigma\n        self.C = np.eye(self.dim)  # Covariance matrix\n        self.pc = np.zeros(self.dim)  # Evolution path for covariance matrix\n        self.ps = np.zeros(self.dim)  # Evolution path for step size\n        self.chiN = np.sqrt(self.dim) * (1 - (1/(4*self.dim)) + 1/(21*self.dim**2))\n\n        self.cs = cs\n        self.damps = damps + 2 * max(0, np.sqrt((self.pop_size - 1)/(self.dim + 1)) - 1)\n        self.ccov1 = 2 / ((self.dim + 1.3)**2 + self.pop_size) if ccov1 is None else ccov1\n        self.ccovmu = min(1-self.ccov1, 2 * (self.pop_size-2 + 1/self.pop_size) / ((self.dim + 2)**2 + self.pop_size)) if ccovmu is None else ccovmu\n\n        self.eval_count = 0\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.restart_trigger = restart_trigger # percentage of budget at which restart should occur if no improvements\n\n    def __call__(self, func):\n        weights = np.log(self.pop_size + 1/2) - np.log(np.arange(1, self.pop_size + 1))\n        weights = weights / np.sum(weights)\n        mu = int(self.pop_size / 2)\n        weights = weights[:mu]\n\n        B, D = np.linalg.eig(self.C)\n        B = np.real(B)\n        D = np.real(D)\n        D = np.sqrt(D)\n        B = np.real(B)\n        \n        iter_since_last_improvement = 0\n\n        while self.eval_count < self.budget:\n            # Sample population\n            z = np.random.normal(0, 1, size=(self.dim, self.pop_size))\n            x = self.mean[:, np.newaxis] + self.sigma * (B * D) @ z\n            x = np.clip(x, self.lb, self.ub)\n            f_vals = np.array([func(xi) for xi in x.T])\n            self.eval_count += self.pop_size\n\n            # Selection and update\n            idx = np.argsort(f_vals)\n            x_k = x[:, idx]\n            f_vals_k = f_vals[idx]\n\n            if f_vals_k[0] < self.f_opt:\n                self.f_opt = f_vals_k[0]\n                self.x_opt = x_k[:, 0]\n                iter_since_last_improvement = 0\n            else:\n                iter_since_last_improvement += 1\n\n            mean_old = self.mean.copy()\n            self.mean = x_k[:, :mu] @ weights\n            \n            # Update evolution paths\n            self.ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs)) * (B * D) @ ( (self.mean - mean_old) / self.sigma ) * np.sqrt(self.dim)\n            hsig = (np.linalg.norm(self.ps) / np.sqrt(1 - (1 - self.cs)**(2 * (self.eval_count / self.pop_size))) / self.chiN < 1.4 + 2/(self.dim + 1))\n            self.pc = (1 - self.ccov1) * self.pc + hsig * np.sqrt(self.ccov1 * (2 - self.ccov1)) * ((self.mean - mean_old) / self.sigma)\n\n            # Update covariance matrix\n            y = x_k[:, :mu] - mean_old[:, np.newaxis]\n            self.C = (1 - self.ccov1 - self.ccovmu) * self.C + self.ccov1 * (self.pc[:, np.newaxis] @ self.pc[np.newaxis, :]) + self.ccovmu * (y @ np.diag(weights) @ y.T) / self.sigma**2\n\n            # Update step size\n            self.sigma *= np.exp((self.cs / self.damps) * (np.linalg.norm(self.ps) / self.chiN - 1))\n\n            if np.any(np.diag(self.C) <= 0) or np.isinf(self.sigma) or np.isnan(self.sigma) : # handles numerical issues\n                self.C = np.eye(self.dim)\n                self.sigma = self.initial_sigma\n\n            if self.eval_count > self.budget * self.restart_trigger and iter_since_last_improvement > self.pop_size * 5:  # Restart if no improvement\n                self.mean = np.random.uniform(self.lb, self.ub, size=self.dim)\n                self.sigma = self.initial_sigma\n                self.C = np.eye(self.dim)\n                self.pc = np.zeros(self.dim)\n                self.ps = np.zeros(self.dim)\n                B, D = np.linalg.eig(self.C)\n                B = np.real(B)\n                D = np.real(D)\n                D = np.sqrt(D)\n                B = np.real(B)\n                iter_since_last_improvement = 0\n                \n            if self.eval_count >= self.budget:\n                break\n\n            B, D = np.linalg.eig(self.C)\n            B = np.real(B)\n            D = np.real(D)\n            D = np.sqrt(D)\n            B = np.real(B)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm BudgetAwareCMAES scored 0.140 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.0718953217180146, 0.12905131273760595, 0.223857877952779, 0.08679810582661429, 0.10442687838281339, 0.13709970864834997, 0.15692139388139836, 0.14575380303085672, 0.15263089479122627, 0.1133842971606498, 0.11698446961411701, 0.17603969529938823, 0.19823366134925335, 0.08112463702119188, 0.1670144696953172, 0.16864915823574556, 0.1421621796902578, 0.17318533327699748, 0.12872885988496385, 0.13546354404207073]}, "task_prompt": ""}
{"id": "c7770f58-b97d-4318-b68f-7204d41f6c07", "fitness": 0.7532891592300526, "name": "AdaptiveDifferentialEvolution", "description": "No description provided.", "code": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget=10000, dim=10, pop_size=20, F=0.5, CR=0.7):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F  # Differential weight\n        self.CR = CR  # Crossover rate\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index].copy()\n\n        while self.budget > 0:\n            new_population = np.zeros((self.pop_size, self.dim))\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = np.random.choice(idxs, 3, replace=False)\n                mutant = population[a] + self.F * (population[b] - population[c])\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                new_population[i] = trial\n\n            new_fitness = np.array([func(x) for x in new_population])\n            self.budget -= self.pop_size\n\n            # Selection\n            for i in range(self.pop_size):\n                if new_fitness[i] < fitness[i]:\n                    fitness[i] = new_fitness[i]\n                    population[i] = new_population[i].copy()\n\n            # Update best solution\n            best_index = np.argmin(fitness)\n            if fitness[best_index] < self.f_opt:\n                self.f_opt = fitness[best_index]\n                self.x_opt = population[best_index].copy()\n\n            # Adapt parameters (simple adaptation - can be improved)\n            self.F = np.clip(self.F + np.random.normal(0, 0.01), 0.1, 0.9)\n            self.CR = np.clip(self.CR + np.random.normal(0, 0.01), 0.1, 0.9)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveDifferentialEvolution scored 0.753 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.34383042300077116, 0.7920502355052244, 0.7642696620592633, 0.9164526272032206, 0.839452221372724, 0.8865061940190504, 0.7766997455419775, 0.8153903645760973, 0.8617380845625777, 0.8542265260777673, 0.9139362360968293, 0.9990686593922126, 0.28603013239979136, 0.7815642702966528, 0.7316055070247716, 0.8955332654363787, 0.7120756273027816, 0.7260471911388497, 0.6407903666365082, 0.5285158449576025]}, "task_prompt": ""}
{"id": "d1d1deb7-da3d-4fba-99c4-7b3cbfb6bcb4", "fitness": "-inf", "name": "EnhancedDE", "description": "No description provided.", "code": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass EnhancedDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, CR=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.lb = -5.0\n        self.ub = 5.0\n\n    def __call__(self, func):\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n        eval_count = self.pop_size\n\n        while eval_count < self.budget:\n            for i in range(self.pop_size):\n                # Self-adaptive mutation\n                if np.random.rand() < 0.1:\n                  F_i = np.random.uniform(0.5,1.0)\n                else:\n                  F_i = self.F\n\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + F_i * (b - c), self.lb, self.ub)\n\n                # Dynamic Crossover\n                CR_i = self.CR * (1 + 0.1 * np.random.randn())\n\n                cross_points = np.random.rand(self.dim) < CR_i\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, self.population[i])\n\n                # Selection\n                f = func(trial)\n                eval_count += 1\n\n                if f < self.fitness[i]:\n                    self.fitness[i] = f\n                    self.population[i] = trial\n\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = trial\n\n            #Local Search with Nelder-Mead every 20% of budget\n            if eval_count > self.budget * 0.2:\n                res = minimize(func, self.x_opt, method='Nelder-Mead', bounds=[(self.lb, self.ub)] * self.dim, options={'maxfev':max(1,int(0.01*self.budget))})\n                if res.fun < self.f_opt:\n                  self.f_opt = res.fun\n                  self.x_opt = res.x\n                eval_count += res.nfev\n                if eval_count >= self.budget:\n                  break\n            \n            #Adaptive F and CR\n            self.F = np.clip(np.random.normal(0.5, 0.1), 0.1, 1.0)\n            self.CR = np.clip(np.random.normal(0.9, 0.1), 0.1, 1.0)\n                \n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "In the code, line 55, in __call__, the following error occurred:\nNameError: name 'minimize' is not defined\nOn line: res = minimize(func, self.x_opt, method='Nelder-Mead', bounds=[(self.lb, self.ub)] * self.dim, options={'maxfev':max(1,int(0.01*self.budget))})", "error": "In the code, line 55, in __call__, the following error occurred:\nNameError: name 'minimize' is not defined\nOn line: res = minimize(func, self.x_opt, method='Nelder-Mead', bounds=[(self.lb, self.ub)] * self.dim, options={'maxfev':max(1,int(0.01*self.budget))})", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "74cd5a05-fc0b-4f7d-99a6-a98445756426", "fitness": 0.3893284771587035, "name": "AdaptiveCMAES", "description": "No description provided.", "code": "import numpy as np\n\nclass AdaptiveCMAES:\n    def __init__(self, budget=10000, dim=10, pop_size=None, sigma=0.3, cs=0.8, c_cov=0.2, mu_ratio=0.25, stagnation_threshold=1e-6, stagnation_iter=500):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size if pop_size is not None else 4 + int(3 * np.log(dim))\n        self.sigma = sigma\n        self.cs = cs\n        self.c_cov = c_cov\n        self.mu = int(self.pop_size * mu_ratio)\n        self.weights = np.log(self.mu + 1) - np.log(np.arange(1, self.mu + 1))\n        self.weights /= np.sum(self.weights)\n        self.m = np.zeros(dim)\n        self.C = np.eye(dim)\n        self.lb = -5.0\n        self.ub = 5.0\n        self.stagnation_threshold = stagnation_threshold\n        self.stagnation_iter = stagnation_iter\n\n    def __call__(self, func):\n        self.eval_count = 0\n        self.f_opt = np.inf\n        self.x_opt = None\n        iter_since_last_improvement = 0\n\n        while self.eval_count < self.budget:\n            Z = np.random.randn(self.dim, self.pop_size)\n            try:\n                A = np.linalg.cholesky(self.C)\n            except np.linalg.LinAlgError:\n                self.C = self.C + 1e-6 * np.eye(self.dim)\n                A = np.linalg.cholesky(self.C)\n\n            X = self.m[:, np.newaxis] + self.sigma * A @ Z\n            X = np.clip(X, self.lb, self.ub)\n            \n            f_vals = np.array([func(x) for x in X.T])\n            self.eval_count += self.pop_size\n\n            idx = np.argsort(f_vals)\n            x_mu = X[:, idx[:self.mu]]\n            f_mu = f_vals[idx[:self.mu]]\n            \n            m_old = self.m.copy()\n            self.m = x_mu @ self.weights\n            \n            z_mu = Z[:, idx[:self.mu]]\n            \n            C_update = np.sum([w * (z_mu[:, i][:, np.newaxis] @ z_mu[:, i][np.newaxis, :]) for i, w in enumerate(self.weights)], axis=0)\n            \n            self.C = (1 - self.c_cov) * self.C + self.c_cov * C_update\n\n            if np.min(f_vals) < self.f_opt:\n                self.f_opt = np.min(f_vals)\n                self.x_opt = X[:, np.argmin(f_vals)]\n                iter_since_last_improvement = 0\n            else:\n                iter_since_last_improvement += 1\n            \n            if iter_since_last_improvement > self.stagnation_iter:\n                self.m = np.random.uniform(self.lb, self.ub, size=self.dim)\n                self.C = np.eye(self.dim)\n                self.sigma *= 0.8\n                iter_since_last_improvement = 0\n            \n            if self.eval_count >= self.budget:\n                break\n            \n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveCMAES scored 0.389 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.18837092893689056, 0.4001189748509624, 0.416835203583241, 0.17118652987659022, 0.32484728598999835, 0.42559259890872025, 0.3033600443386205, 0.3539502908864314, 0.325976518835812, 0.19002847864541006, 0.5063583346324299, 0.9886996073669613, 0.29584447095846444, 0.34577405822016616, 0.7849876872124831, 0.38490744748383654, 0.33720692513764505, 0.3002665722231904, 0.2346498436253972, 0.5076077414608191]}, "task_prompt": ""}
{"id": "b15c90d1-610f-4b7c-8924-7af6ce5f9ff6", "fitness": "-inf", "name": "CMAES", "description": "No description provided.", "code": "import numpy as np\n\nclass CMAES:\n    def __init__(self, budget=10000, dim=10, pop_size=None, sigma=0.5, cs=0.3, damps=None, c_cov=None):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size if pop_size is not None else 4 + int(3 * np.log(dim))\n        self.sigma = sigma\n        self.lb = -5.0\n        self.ub = 5.0\n        self.mean = np.random.uniform(self.lb, self.ub, size=dim)\n        self.C = np.eye(dim)\n        self.pc = np.zeros(dim)\n        self.ps = np.zeros(dim)\n        self.chiN = np.sqrt(dim) * (1 - (1 / (4 * dim)) + (1 / (21 * dim**2)))\n\n        if damps is None:\n            self.damps = 1 + 2 * max(0, np.sqrt((self.pop_size - 1) / (dim + 1)) - 1) + cs\n        else:\n            self.damps = damps\n\n        self.cs = cs\n        self.c_cov = 2 / ((dim + np.sqrt(2))**2) if c_cov is None else c_cov\n        self.c_cov_mu = min(1, self.c_cov * (self.pop_size / 4))\n\n        self.weights = np.log(self.pop_size + 1/2) - np.log(np.arange(1, self.pop_size + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mu = self.pop_size // 2\n\n        self.f_opt = np.inf\n        self.x_opt = None\n\n\n    def __call__(self, func):\n        eval_count = 0\n        while eval_count < self.budget:\n            z = np.random.multivariate_normal(np.zeros(self.dim), np.eye(self.dim), size=self.pop_size)\n            x = self.mean + self.sigma * np.dot(z, np.linalg.cholesky(self.C).T)\n            x = np.clip(x, self.lb, self.ub)\n            \n            f = np.array([func(xi) for xi in x])\n            eval_count += self.pop_size\n\n            if np.any(f < self.f_opt):\n                best_index = np.argmin(f)\n                if f[best_index] < self.f_opt:\n                    self.f_opt = f[best_index]\n                    self.x_opt = x[best_index]\n\n\n            idx = np.argsort(f)\n            x_sorted = x[idx]\n            z_sorted = z[idx]\n\n            # Update mean\n            mean_old = self.mean.copy()\n            self.mean = np.sum(self.weights[:, None] * x_sorted[:self.mu], axis=0)\n\n            # Update evolution paths\n            self.ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs)) * np.sqrt(self.weights[0]) * (self.mean - mean_old) / self.sigma\n            hsig = (np.linalg.norm(self.ps) / np.sqrt(1 - (1 - self.cs)**(2 * (eval_count / self.pop_size))) / self.chiN) < (1.4 + 2/(self.dim + 1))\n            self.pc = (1 - self.c_cov) * self.pc + hsig * np.sqrt(self.c_cov * (2 - self.c_cov)) * (self.mean - mean_old) / self.sigma\n\n            # Update covariance matrix\n            artmp = (1 / self.sigma) * (x_sorted[:self.mu] - mean_old)\n            self.C = (1 - self.c_cov) * self.C + self.c_cov_mu * np.dot(artmp.T, np.diag(self.weights[:self.mu])).dot(artmp) + self.c_cov * (1 - hsig) * self.pc[:, None].dot(self.pc[None, :])\n\n            # Update step size\n            self.sigma *= np.exp((self.cs / self.damps) * (np.linalg.norm(self.ps) / self.chiN - 1))\n            self.sigma = np.clip(self.sigma, 1e-10, 1e10)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "In the code, line 57, in __call__, the following error occurred:\nValueError: operands could not be broadcast together with shapes (6,1) (3,2) \nOn line: self.mean = np.sum(self.weights[:, None] * x_sorted[:self.mu], axis=0)", "error": "In the code, line 57, in __call__, the following error occurred:\nValueError: operands could not be broadcast together with shapes (6,1) (3,2) \nOn line: self.mean = np.sum(self.weights[:, None] * x_sorted[:self.mu], axis=0)", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "49b461fe-0f13-4741-8adb-9d00f1a495f1", "fitness": "-inf", "name": "BudgetAwareCMAES", "description": "No description provided.", "code": "import numpy as np\n\nclass BudgetAwareCMAES:\n    def __init__(self, budget=10000, dim=10, pop_size=None, initial_sigma=0.5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size if pop_size else 4 + int(3 * np.log(dim))\n        self.initial_sigma = initial_sigma\n        self.lb = -5.0\n        self.ub = 5.0\n        self.m = None  # Mean\n        self.C = None  # Covariance matrix\n        self.sigma = None  # Step size\n        self.p_sigma = None # Evolution path for sigma\n        self.p_c = None # Evolution path for covariance\n        self.evals = 0\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.c_sigma = None\n        self.d_sigma = None\n        self.c_c = None\n        self.c_mu = None\n        self.weights = None\n        self.mu = self.pop_size // 2\n        self.c_1 = None\n        self.c_mu_eff = None\n        self.mu_eff = None\n        self.B = None\n        self.D = None\n        self.eigen_updated = False\n        self.initialize()\n\n    def initialize(self):\n        self.m = np.random.uniform(self.lb, self.ub, size=self.dim)\n        self.C = np.eye(self.dim)\n        self.sigma = self.initial_sigma\n        self.p_sigma = np.zeros(self.dim)\n        self.p_c = np.zeros(self.dim)\n\n        self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mu_eff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n\n        self.c_sigma = (self.mu_eff + 2) / (self.dim + self.mu_eff + 5)\n        self.d_sigma = 1 + 2 * max(0, np.sqrt((self.mu_eff - 1) / (self.dim + 1)) - 1) + self.c_sigma\n        self.c_c = (4 + self.mu_eff / self.dim) / (self.dim + 4 + 2 * self.mu_eff / self.dim)\n        self.c_1 = 2 / ((self.dim + 1.3)**2 + self.mu_eff)\n        self.c_mu = min(1 - self.c_1, 2 * (self.mu_eff - 2 + 1 / self.mu_eff) / ((self.dim + 2)**2 + self.mu_eff))\n        self.c_mu_eff = 1\n\n    def sample_population(self):\n        z = np.random.normal(0, 1, size=(self.pop_size, self.dim))\n        y = np.dot(z, self.B * np.sqrt(self.D))\n        x = self.m + self.sigma * y\n        return np.clip(x, self.lb, self.ub)\n\n    def update_parameters(self, x, fitness_values):\n        x_k = x[np.argsort(fitness_values)]\n        y_k = (x_k - self.m) / self.sigma\n        \n        self.m = np.sum(x_k[:self.mu] * self.weights[:self.mu, None], axis=0)\n        \n        self.p_sigma = (1 - self.c_sigma) * self.p_sigma + np.sqrt(self.c_sigma * (2 - self.c_sigma) * self.mu_eff) * np.sum(y_k[:self.mu] * self.weights[:self.mu, None], axis=0)\n        \n        sigma_norm = np.linalg.norm(self.p_sigma) / np.sqrt(self.dim)\n        \n        self.sigma *= np.exp(self.c_sigma / self.d_sigma * (sigma_norm - 1))\n        self.sigma = np.clip(self.sigma, 1e-10, 5) \n\n        self.p_c = (1 - self.c_c) * self.p_c + np.sqrt(self.c_c * (2 - self.c_c) * self.mu_eff) * np.sum(y_k[:self.mu] * self.weights[:self.mu, None], axis=0)\n        \n        delta = y_k[:self.mu].T @ (self.weights[:self.mu] * y_k[:self.mu])\n        self.C = (1 - self.c_1 - self.c_mu) * self.C + self.c_1 * (self.p_c[:, None] @ self.p_c[None, :]) + self.c_mu * delta\n        \n        self.C = np.triu(self.C) + np.triu(self.C, 1).T\n        \n        self.eigen_updated = False\n\n    def check_eigen_update(self):\n        if self.evals % (self.budget / 20) == 0 or not self.eigen_updated:\n            self.eigen_decomposition()\n            self.eigen_updated = True\n\n    def eigen_decomposition(self):\n        self.D, self.B = np.linalg.eigh(self.C)\n        self.D = np.sqrt(np.maximum(self.D, 1e-10))\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.evals = 0\n        \n        while self.evals < self.budget:\n            self.check_eigen_update()\n            if not self.eigen_updated:\n                self.eigen_decomposition()\n                self.eigen_updated = True\n            \n            x = self.sample_population()\n            fitness_values = np.array([func(xi) for xi in x])\n            self.evals += self.pop_size\n\n            best_index = np.argmin(fitness_values)\n            if fitness_values[best_index] < self.f_opt:\n                self.f_opt = fitness_values[best_index]\n                self.x_opt = x[best_index]\n\n            self.update_parameters(x, fitness_values)\n            \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "In the code, line 72, in update_parameters, the following error occurred:\nValueError: operands could not be broadcast together with shapes (3,) (3,2) \nOn line: delta = y_k[:self.mu].T @ (self.weights[:self.mu] * y_k[:self.mu])", "error": "In the code, line 72, in update_parameters, the following error occurred:\nValueError: operands could not be broadcast together with shapes (3,) (3,2) \nOn line: delta = y_k[:self.mu].T @ (self.weights[:self.mu] * y_k[:self.mu])", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "6be2ac81-69d6-44f0-8071-f30c11be64ba", "fitness": 0.248659543415297, "name": "AdaptiveSimulatedAnnealing", "description": "No description provided.", "code": "import numpy as np\n\nclass AdaptiveSimulatedAnnealing:\n    def __init__(self, budget=10000, dim=10, initial_temp=100.0, alpha=0.99, temp_min=1e-5):\n        self.budget = budget\n        self.dim = dim\n        self.initial_temp = initial_temp\n        self.alpha = alpha\n        self.temp_min = temp_min\n        self.lb = -5.0\n        self.ub = 5.0\n        self.acceptance_history = []\n\n    def __call__(self, func):\n        self.x_current = np.random.uniform(self.lb, self.ub, size=self.dim)\n        self.f_current = func(self.x_current)\n        self.f_opt = self.f_current\n        self.x_opt = self.x_current\n        self.temp = self.initial_temp\n        self.eval_count = 1\n\n        while self.eval_count < self.budget and self.temp > self.temp_min:\n            x_new = self.x_current + np.random.normal(0, 0.1, size=self.dim)\n            x_new = np.clip(x_new, self.lb, self.ub)\n            f_new = func(x_new)\n            self.eval_count += 1\n\n            delta_f = f_new - self.f_current\n\n            if delta_f < 0:\n                self.x_current = x_new\n                self.f_current = f_new\n                if f_new < self.f_opt:\n                    self.f_opt = f_new\n                    self.x_opt = x_new\n                acceptance = 1.0\n            else:\n                try:\n                    acceptance_prob = np.exp(-delta_f / self.temp)\n                except OverflowError:\n                    acceptance_prob = 0.0\n                if np.random.rand() < acceptance_prob:\n                    self.x_current = x_new\n                    self.f_current = f_new\n                    acceptance = acceptance_prob\n                else:\n                    acceptance = 0.0\n\n            self.acceptance_history.append(acceptance)\n\n            #Adaptive Temperature Decay\n            if len(self.acceptance_history) > 100:\n                acceptance_rate = np.mean(self.acceptance_history[-100:])\n                if acceptance_rate > 0.96:\n                    self.alpha = 0.9\n                elif acceptance_rate < 0.04:\n                    self.alpha = 0.999\n                else:\n                    self.alpha = 0.99\n\n            self.temp *= self.alpha\n            if self.eval_count >= self.budget:\n                break\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveSimulatedAnnealing scored 0.249 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.07587853991060378, 0.16885416218933058, 0.4480293195338374, 0.19184715782919837, 0.09894216589042659, 0.1475967963528101, 0.25434896427630427, 0.12285965458102799, 0.16477437559208263, 0.12348016324177025, 0.9070097872864274, 0.1729539565446575, 0.3077173730525272, 0.20842900770651218, 0.5390721847999014, 0.3063399812144425, 0.23389489594928603, 0.16670928591760947, 0.1271102164082729, 0.20734288002891188]}, "task_prompt": ""}
{"id": "21fd0d0e-f74e-4303-9d8d-8895f752860b", "fitness": "-inf", "name": "CMAES", "description": "No description provided.", "code": "import numpy as np\nimport cma\n\nclass CMAES:\n    def __init__(self, budget, dim):\n        self.budget = budget\n        self.dim = dim\n        self.x0 = np.zeros(dim)\n        self.sigma = 0.5\n        self.popsize = 4 + int(3 * np.log(dim))\n        self.es = None\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.restart_trigger = 100 * dim  # Evaluations before restart check\n\n\n    def __call__(self, func):\n        self.es = cma.optimization_tools.CMAParameters(self.x0, self.sigma, {'bounds': [func.bounds.lb, func.bounds.ub], 'popsize': self.popsize, 'maxfevals': self.budget, 'verbose':-9}).instantiate()\n        \n        fevals_since_last_restart = 0\n        \n        while self.es.evaluations < self.budget:\n            solutions = []\n            for i in range(self.es.population_size):\n                x = self.es.ask()\n                f = func(x)\n                solutions.append((x,f))\n                \n                if f < self.f_opt:\n                    self.f_opt = f\n                    self.x_opt = x\n                fevals_since_last_restart += 1\n            \n            self.es.tell([(x,f) for x,f in solutions])\n            self.es.adapt()\n            \n            # Restart strategy based on stagnation\n            if fevals_since_last_restart > self.restart_trigger:\n                if self.es.best.f is not None and self.es.best.f >= self.f_opt:\n                    self.es.restart(seed=np.random.randint(0, 2**32))\n                    fevals_since_last_restart = 0\n                \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "In the code, line 2, in <module>, the following error occurred:\nModuleNotFoundError: No module named 'cma'\nOn line: import cma", "error": "In the code, line 2, in <module>, the following error occurred:\nModuleNotFoundError: No module named 'cma'\nOn line: import cma", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "427c7c26-b8dc-433b-86ae-92719e3e4ee8", "fitness": "-inf", "name": "CMAES", "description": "No description provided.", "code": "import numpy as np\n\nclass CMAES:\n    def __init__(self, budget=10000, dim=10, pop_size=None, sigma=0.3, cs=0.8, damps=1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size if pop_size else 4 + int(3 * np.log(dim))\n        self.sigma = sigma\n        self.cs = cs\n        self.damps = damps\n        self.lb = -5.0\n        self.ub = 5.0\n        self.mean = np.random.uniform(self.lb, self.ub, size=self.dim)\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.chiN = np.sqrt(self.dim) * (1 - (1 / (4 * self.dim)) + (1 / (21 * self.dim**2)))\n        self.mu = self.pop_size // 2\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        self.cc = (4 + self.mueff/self.dim) / (self.dim + 4 + 2*self.mueff/self.dim)\n        self.c1 = 2 / ((self.dim + 1.3)**2 + self.mueff)\n        self.cmu = min(1 - self.c1, 2 * (self.mueff - 2 + 1/self.mueff) / ((self.dim + 2.3)**2 + self.mueff))\n        self.dampss = 1 + 2*max(0, np.sqrt((self.mueff - 1)/(self.dim + 1)) - 1) + self.cs\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        eval_count = 0\n\n        while eval_count < self.budget:\n            # Sample population\n            z = np.random.randn(self.pop_size, self.dim)\n            y = np.dot(z, np.linalg.cholesky(self.C).T)\n            x = self.mean + self.sigma * y\n            x = np.clip(x, self.lb, self.ub)\n\n            # Evaluate population\n            fitness = np.array([func(xi) for xi in x])\n            eval_count += self.pop_size\n            if eval_count > self.budget:\n                fitness = fitness[:self.pop_size - (eval_count-self.budget)]\n                x = x[:self.pop_size - (eval_count-self.budget)]\n                self.pop_size = self.pop_size - (eval_count-self.budget)\n                eval_count = self.budget\n                \n            \n            # Update best\n            best_idx = np.argmin(fitness)\n            if fitness[best_idx] < self.f_opt:\n                self.f_opt = fitness[best_idx]\n                self.x_opt = x[best_idx].copy()\n                \n            # Selection and Recombination\n            idx = np.argsort(fitness)\n            x_mu = x[idx[:self.mu]]\n            y_mu = y[idx[:self.mu]]\n\n            self.mean = np.sum(self.weights[:, None] * x_mu, axis=0)\n\n            # Update Evolution Path\n            self.ps = (1 - self.cs) * self.ps + np.sqrt(self.cc * (2 - self.cc) * self.mueff) * (self.mean - self.mean) / self.sigma\n            self.pc = (1 - self.cc) * self.pc + np.sqrt(self.cc * (2 - self.cc) * self.mueff) * np.sum(self.weights[:, None] * y_mu, axis=0)\n            \n            # Update Covariance Matrix\n            self.C = (1 - self.c1 - self.cmu) * self.C + self.c1 * (np.outer(self.pc, self.pc) + (self.cc * (2 - self.cc)) * self.C)\n            self.C += self.cmu * np.sum(self.weights[:, None, None] * y_mu[:, :, None] * y_mu[:, None, :], axis=0)\n            \n            # Update Step Size\n            self.sigma *= np.exp((self.cs / self.dampss) * (np.linalg.norm(self.ps) / self.chiN - 1))\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "In the code, line 148, in _raise_linalgerror_nonposdef, the following error occurred:\nLinAlgError: Matrix is not positive definite", "error": "In the code, line 148, in _raise_linalgerror_nonposdef, the following error occurred:\nLinAlgError: Matrix is not positive definite", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "99cd77ab-69fa-4836-959c-7b419cc33670", "fitness": 0.2891694844040844, "name": "AdaptiveSimulatedAnnealing", "description": "No description provided.", "code": "import numpy as np\n\nclass AdaptiveSimulatedAnnealing:\n    def __init__(self, budget=10000, dim=10, initial_temp=100.0, cooling_rate=0.95, step_size=1.0):\n        self.budget = budget\n        self.dim = dim\n        self.initial_temp = initial_temp\n        self.cooling_rate = cooling_rate\n        self.step_size = step_size\n        self.lb = -5.0\n        self.ub = 5.0\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        x = np.random.uniform(self.lb, self.ub, size=self.dim)\n        f = func(x)\n        self.f_opt = f\n        self.x_opt = x.copy()\n        temp = self.initial_temp\n        eval_count = 1\n        success_count = 0\n        iteration = 0\n\n        while eval_count < self.budget:\n            iteration += 1\n            x_new = x + np.random.uniform(-self.step_size, self.step_size, size=self.dim)\n            x_new = np.clip(x_new, self.lb, self.ub)\n            f_new = func(x_new)\n            eval_count += 1\n\n            delta_f = f_new - f\n            if delta_f < 0:\n                x = x_new.copy()\n                f = f_new\n                success_count += 1\n                if f < self.f_opt:\n                    self.f_opt = f\n                    self.x_opt = x.copy()\n            else:\n                acceptance_probability = np.exp(-delta_f / temp)\n                if np.random.rand() < acceptance_probability:\n                    x = x_new.copy()\n                    f = f_new\n            \n            # Adaptive temperature and step size\n            if iteration % 100 == 0:\n                success_rate = success_count / 100\n                if success_rate > 0.6:\n                    self.step_size *= 1.1  # Intensification: Increase step size\n                elif success_rate < 0.4:\n                    self.step_size *= 0.9  # Diversification: Decrease step size\n                temp *= self.cooling_rate  #Cooling\n                success_count = 0\n            \n            self.step_size = np.clip(self.step_size, 0.001, 2.0) # bound the step size\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveSimulatedAnnealing scored 0.289 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.11552883327559871, 0.21881135885862757, 0.26808907920080094, 0.22051543192244938, 0.19657606976266306, 0.23740773551891503, 0.23792305437457706, 0.22204384792572485, 0.19188549795708032, 0.15548022208132328, 0.2791736002270846, 0.9884785192707566, 0.2585371343609816, 0.20249393381587533, 0.5822036196602813, 0.2773869451504848, 0.23067163504962296, 0.2897685522456158, 0.17858697487213882, 0.4318276425510863]}, "task_prompt": ""}
{"id": "4266d562-930b-4213-a04a-5919fe004f4d", "fitness": "-inf", "name": "SimplifiedCMAES", "description": "No description provided.", "code": "import numpy as np\n\nclass SimplifiedCMAES:\n    def __init__(self, budget=10000, dim=10, pop_size=None, sigma0=0.5, cs=0.3, restart_threshold=1e-12):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size if pop_size is not None else 4 + int(3 * np.log(dim))\n        self.sigma = sigma0\n        self.cs = cs\n        self.mu = self.pop_size // 2\n        self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n        self.weights /= np.sum(self.weights)\n        self.m = np.zeros(dim)\n        self.C = np.eye(dim)\n        self.restart_threshold = restart_threshold\n        self.lb = -5.0\n        self.ub = 5.0\n\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        eval_count = 0\n        restart_count = 0\n\n        while eval_count < self.budget:\n            # Sample population\n            z = np.random.randn(self.pop_size, self.dim)\n            x = self.m + self.sigma * np.dot(z, np.linalg.cholesky(self.C).T)\n            x = np.clip(x, self.lb, self.ub)\n            fitness = np.array([func(xi) for xi in x])\n            eval_count += self.pop_size\n\n            # Sort by fitness\n            indices = np.argsort(fitness)\n            fitness = fitness[indices]\n            x = x[indices]\n\n            # Update best solution\n            if fitness[0] < self.f_opt:\n                self.f_opt = fitness[0]\n                self.x_opt = x[0].copy()\n\n            # Update mean\n            m_old = self.m.copy()\n            self.m = np.sum(self.weights[:, None] * x[:self.mu], axis=0)\n\n            # Simplified covariance matrix adaptation\n            d = self.m - m_old\n            self.C = (1 - self.cs) * self.C + self.cs * (d[:, None] @ d[None, :]) / (self.sigma**2)\n\n            # Update step size\n            self.sigma *= np.exp(self.cs / 2 * (np.sum(self.weights * np.sum(z[:self.mu]**2, axis=1)) - self.dim) / self.dim)\n            \n            # Check for stagnation and restart if needed\n            if np.linalg.norm(d) < self.restart_threshold * self.sigma:\n                restart_count += 1\n                self.m = np.random.uniform(self.lb, self.ub, size=self.dim)\n                self.C = np.eye(self.dim)\n                self.sigma = 0.5\n                \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "In the code, line 148, in _raise_linalgerror_nonposdef, the following error occurred:\nLinAlgError: Matrix is not positive definite", "error": "In the code, line 148, in _raise_linalgerror_nonposdef, the following error occurred:\nLinAlgError: Matrix is not positive definite", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "e522e600-b763-40ed-8adb-67410dabac02", "fitness": 0.5469809697154597, "name": "AdaptiveSampling", "description": "No description provided.", "code": "import numpy as np\n\nclass AdaptiveSampling:\n    def __init__(self, budget=10000, dim=10, initial_radius=1.0, shrink_factor=0.9, expand_factor=1.1, success_threshold=0.2):\n        self.budget = budget\n        self.dim = dim\n        self.radius = initial_radius\n        self.shrink_factor = shrink_factor\n        self.expand_factor = expand_factor\n        self.success_threshold = success_threshold\n        self.lb = -5.0\n        self.ub = 5.0\n        self.x_opt = np.random.uniform(self.lb, self.ub, size=self.dim)\n        self.f_opt = np.inf\n        self.evals = 0\n        self.successes = 0\n\n    def sample(self):\n        x = self.x_opt + np.random.normal(0, self.radius, size=self.dim)\n        return np.clip(x, self.lb, self.ub)\n\n    def __call__(self, func):\n        self.f_opt = func(self.x_opt)\n        self.evals += 1\n\n        while self.evals < self.budget:\n            x = self.sample()\n            f = func(x)\n            self.evals += 1\n\n            if f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = x\n                self.successes += 1\n\n            if self.evals % 100 == 0:\n                success_rate = self.successes / 100\n                if success_rate > self.success_threshold:\n                    self.radius *= self.expand_factor\n                else:\n                    self.radius *= self.shrink_factor\n                self.successes = 0\n                self.radius = np.clip(self.radius, 1e-6, 5.0)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveSampling scored 0.547 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.255408727277856, 0.2898947892038418, 0.575919566321183, 0.8596002362080841, 0.5801114246394612, 0.6328184346594221, 0.2940214186816916, 0.49648987047809046, 0.5904593210509006, 0.27878022602706365, 0.8552705972844409, 0.9970335571996705, 0.22216153455763166, 0.5100585599327907, 0.9373848362242283, 0.35690283398040223, 0.5063777620777203, 0.7576413121167286, 0.2414467170229484, 0.7018376693650369]}, "task_prompt": ""}
{"id": "6eeea15d-97d7-40d4-96e5-f49964dea109", "fitness": 0.0, "name": "GradientEnhancedEvolution", "description": "No description provided.", "code": "import numpy as np\n\nclass GradientEnhancedEvolution:\n    def __init__(self, budget=10000, dim=10, pop_size=20, step_size=0.1, perturbation_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.step_size = step_size\n        self.perturbation_rate = perturbation_rate\n        self.lb = -5.0\n        self.ub = 5.0\n\n    def __call__(self, func):\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n        eval_count = self.pop_size\n\n        while eval_count < self.budget:\n            for i in range(self.pop_size):\n                # Gradient-based search (approximate gradient)\n                x = self.population[i]\n                gradient = np.zeros(self.dim)\n                for j in range(self.dim):\n                    x_plus = x.copy()\n                    x_minus = x.copy()\n                    delta = self.step_size\n                    x_plus[j] += delta\n                    x_minus[j] -= delta\n\n                    x_plus = np.clip(x_plus, self.lb, self.ub)\n                    x_minus = np.clip(x_minus, self.lb, self.ub)\n\n                    f_plus = func(x_plus)\n                    f_minus = func(x_minus)\n                    eval_count += 2 #Corrected evaluation counter\n\n                    gradient[j] = (f_plus - f_minus) / (2 * delta)\n                    if eval_count >= self.budget:\n                        return self.f_opt, self.x_opt\n\n                # Update solution based on gradient\n                new_x = x - self.step_size * gradient\n                new_x = np.clip(new_x, self.lb, self.ub)\n\n                # Random perturbation for exploration\n                if np.random.rand() < self.perturbation_rate:\n                    new_x += np.random.uniform(-self.step_size, self.step_size, size=self.dim)\n                    new_x = np.clip(new_x, self.lb, self.ub)\n\n                f = func(new_x)\n                eval_count += 1\n\n                if f < self.fitness[i]:\n                    self.fitness[i] = f\n                    self.population[i] = new_x\n\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = new_x\n            self.step_size *= 0.99 # Optional Step Size Reduction\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm GradientEnhancedEvolution scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0]}, "task_prompt": ""}
{"id": "d25be094-2a60-40b0-b18d-ed1dd28fd4bd", "fitness": 0.6195315327157976, "name": "CMAES", "description": "No description provided.", "code": "import numpy as np\n\nclass CMAES:\n    def __init__(self, budget=10000, dim=10, pop_size=None, sigma=0.3, cs=0.8, damp=None):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size or (4 + int(3 * np.log(dim)))\n        self.sigma = sigma\n        self.mean = None\n        self.C = None\n        self.ps = None\n        self.pc = None\n        self.chiN = dim**0.5 * (1 - 1/(4*dim) + 1/(21*dim**2))\n        self.mu = self.pop_size // 2\n        self.weights = np.log(self.mu+1/2) - np.log(np.arange(1, self.mu+1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n\n        self.cs = cs\n        self.damp = damp or 1 + 2*max(0, np.sqrt((self.mueff-1)/(self.dim+1))-1) + self.cs\n        self.cc = (4 + self.mueff/self.dim) / (self.dim + 4 + 2*self.mueff/self.dim)\n        self.c1 = 2 / ((self.dim + 1.3)**2 + self.mueff)\n        self.cmu = min(1-self.c1, 2 * (self.mueff-2+1/self.mueff) / ((self.dim+2.3)**2 + self.mueff))\n        self.lb = -5.0\n        self.ub = 5.0\n\n    def __call__(self, func):\n        self.mean = np.random.uniform(self.lb, self.ub, size=self.dim)\n        self.C = np.eye(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.f_opt = np.inf\n        self.x_opt = None\n        eval_count = 0\n\n        while eval_count < self.budget:\n            # Generate population\n            z = np.random.normal(0, 1, size=(self.pop_size, self.dim))\n            x = self.mean + self.sigma * z @ np.linalg.cholesky(self.C).T\n            x = np.clip(x, self.lb, self.ub)\n\n            # Evaluate population\n            fitness = np.array([func(xi) for xi in x])\n            eval_count += self.pop_size\n\n            # Sort by fitness\n            indices = np.argsort(fitness)\n            x = x[indices]\n            fitness = fitness[indices]\n            \n            #Update best solution\n            if fitness[0] < self.f_opt:\n                self.f_opt = fitness[0]\n                self.x_opt = x[0].copy()\n\n            # Update mean\n            xmean = np.sum(x[:self.mu] * self.weights[:, None], axis=0)\n            y = xmean - self.mean\n\n            # Update evolution paths\n            self.ps = (1-self.cs)*self.ps + np.sqrt(self.cs*(2-self.cs)*self.mueff) * y @ np.linalg.inv(np.linalg.cholesky(self.C)).T / self.sigma\n            self.pc = (1-self.cc)*self.pc + np.sqrt(self.cc*(2-self.cc)*self.mueff) * y / self.sigma\n\n            # Update covariance matrix\n            C_temp = self.c1 * (self.pc[:, None] @ self.pc[None, :])\n            C_temp += self.cmu * (x[:self.mu] - self.mean).T @ np.diag(self.weights) @ (x[:self.mu] - self.mean) / self.sigma**2\n\n            self.C = (1-self.c1-self.cmu) * self.C + C_temp\n\n            # Update step size\n            self.sigma *= np.exp((self.cs/self.damp)*(np.linalg.norm(self.ps)/self.chiN - 1))\n\n            # Update mean\n            self.mean = xmean\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm CMAES scored 0.620 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.2774656071642184, 0.1523468744693982, 0.9053019433369234, 0.16526756210215532, 0.9400258274408997, 0.9495851164469803, 0.27276558386521343, 0.7427409493804197, 0.9483601429765838, 0.1897034370489914, 0.32494197330377705, 0.9781212256368953, 0.8622053525603793, 0.9251299472691953, 0.9715236939343588, 0.8354635874269859, 0.2863719007319968, 0.9621868342757551, 0.18238667443709233, 0.5187364205077323]}, "task_prompt": ""}
{"id": "0b67570d-ea74-4d2f-90ec-a612fa9eea64", "fitness": "-inf", "name": "BayesianOptimizationLocalSearch", "description": "No description provided.", "code": "import numpy as np\nfrom sklearn.gaussian_process import GaussianProcessRegressor\nfrom sklearn.gaussian_process.kernels import RBF, ConstantKernel\nfrom scipy.optimize import minimize\n\nclass BayesianOptimizationLocalSearch:\n    def __init__(self, budget=10000, dim=10, n_initial_samples=10):\n        self.budget = budget\n        self.dim = dim\n        self.n_initial_samples = n_initial_samples\n        self.lb = -5.0\n        self.ub = 5.0\n        self.kernel = ConstantKernel(1.0) * RBF(length_scale=1.0)\n        self.gp = GaussianProcessRegressor(kernel=self.kernel, n_restarts_optimizer=5)\n        self.X = None\n        self.y = None\n        self.x_opt = None\n        self.f_opt = np.inf\n\n    def acquisition_function(self, x, gp):\n        mu, sigma = gp.predict(x.reshape(1, -1), return_std=True)\n        return -mu + 2 * sigma\n\n    def local_search(self, func, x_start, max_iter=5):\n        x_current = x_start.copy()\n        f_current = func(x_current)\n        for _ in range(max_iter):\n            x_neighbor = x_current + np.random.normal(0, 0.1, size=self.dim)\n            x_neighbor = np.clip(x_neighbor, self.lb, self.ub)\n            f_neighbor = func(x_neighbor)\n            if f_neighbor < f_current:\n                x_current = x_neighbor\n                f_current = f_neighbor\n            else:\n                break \n        return f_current, x_current\n\n    def __call__(self, func):\n        # Initial sampling\n        self.X = np.random.uniform(self.lb, self.ub, size=(self.n_initial_samples, self.dim))\n        self.y = np.array([func(x) for x in self.X])\n        self.f_opt = np.min(self.y)\n        self.x_opt = self.X[np.argmin(self.y)]\n        eval_count = self.n_initial_samples\n\n        # Bayesian optimization loop\n        while eval_count < self.budget:\n            self.gp.fit(self.X, self.y)\n            \n            # Find next point to evaluate by maximizing acquisition function\n            x_next = None\n            best_acq = np.inf\n            for _ in range(10): #optimize multiple times and choose the best\n                x0 = np.random.uniform(self.lb, self.ub, size=self.dim)\n                res = minimize(self.acquisition_function, x0, args=(self.gp,), bounds=[(self.lb, self.ub)] * self.dim)\n                if res.fun < best_acq:\n                    x_next = res.x\n                    best_acq = res.fun\n\n\n            # Local Search from x_next\n            f_local, x_local = self.local_search(func, x_next)\n            eval_count +=1\n\n            if f_local < self.f_opt:\n                self.f_opt = f_local\n                self.x_opt = x_local\n            \n            # Update data\n            self.X = np.vstack((self.X, x_local))\n            self.y = np.append(self.y, f_local)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "In the code, line 13, in __init__, the following error occurred:\nNameError: name 'ConstantKernel' is not defined\nOn line: self.kernel = ConstantKernel(1.0) * RBF(length_scale=1.0)", "error": "In the code, line 13, in __init__, the following error occurred:\nNameError: name 'ConstantKernel' is not defined\nOn line: self.kernel = ConstantKernel(1.0) * RBF(length_scale=1.0)", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "4ba5dc8d-5ea5-490b-ac37-ff142f9c5407", "fitness": 0.2796260180479446, "name": "GaussianAdaptation", "description": "No description provided.", "code": "import numpy as np\n\nclass GaussianAdaptation:\n    def __init__(self, budget=10000, dim=10, pop_size=50, initial_step_size=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.initial_step_size = initial_step_size\n        self.lb = -5.0\n        self.ub = 5.0\n\n    def __call__(self, func):\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.step_sizes = np.full((self.pop_size, self.dim), self.initial_step_size)\n        self.f_vals = np.array([func(x) for x in self.population])\n        self.f_opt = np.min(self.f_vals)\n        self.x_opt = self.population[np.argmin(self.f_vals)]\n        self.eval_count = self.pop_size\n\n        while self.eval_count < self.budget:\n            for i in range(self.pop_size):\n                # Mutation with adaptive step size\n                mutation = np.random.normal(0, self.step_sizes[i], size=self.dim)\n                trial_vector = self.population[i] + mutation\n                trial_vector = np.clip(trial_vector, self.lb, self.ub)\n\n                # Selection\n                f_trial = func(trial_vector)\n                self.eval_count += 1\n\n                if f_trial < self.f_vals[i]:\n                    self.population[i] = trial_vector\n                    self.f_vals[i] = f_trial\n\n                    # Adapt step size (success)\n                    self.step_sizes[i] *= np.exp(0.1)\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial_vector\n                else:\n                    # Adapt step size (failure)\n                    self.step_sizes[i] *= np.exp(-0.1)\n\n                # Ensure step sizes remain within reasonable bounds\n                self.step_sizes[i] = np.clip(self.step_sizes[i], 1e-6, 1.0)\n\n                if self.eval_count >= self.budget:\n                    break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm GaussianAdaptation scored 0.280 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.10459369149000342, 0.18548092268131855, 0.2804089326056034, 0.18292821027835626, 0.153325646648783, 0.18353869993175675, 0.20771716927570505, 0.19069080066171495, 0.17602829230622807, 0.15593195588539766, 0.2533133195265683, 0.9483120702486735, 0.27203937712202897, 0.17271438575862574, 0.5046843497361124, 0.28737635884139723, 0.23018212734313404, 0.49094824127128733, 0.14235082847875236, 0.4699549808674447]}, "task_prompt": ""}
{"id": "ba4cf7c1-623b-418b-939b-170ca4fb2e63", "fitness": 0.0, "name": "SelfAdaptiveDE", "description": "No description provided.", "code": "import numpy as np\n\nclass SelfAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, CR=0.7, reduction_factor=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F  # Differential weight\n        self.CR = CR  # Crossover rate\n        self.reduction_factor = reduction_factor\n        self.min_pop_size = 10\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index].copy()\n\n        while self.budget > 0 and self.pop_size >= self.min_pop_size:\n            new_population = np.zeros((self.pop_size, self.dim))\n            new_fitness = np.zeros(self.pop_size)\n\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = np.random.choice(idxs, 3, replace=False)\n                mutant = population[a] + self.F * (population[b] - population[c])\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                new_population[i] = trial\n\n                new_fitness[i] = func(trial)\n                self.budget -= 1\n                if new_fitness[i] < fitness[i]:\n                    fitness[i] = new_fitness[i]\n                    population[i] = trial.copy()\n\n                    if new_fitness[i] < self.f_opt:\n                        self.f_opt = new_fitness[i]\n                        self.x_opt = trial.copy()\n\n            # Adapt parameters\n            self.F = np.clip(self.F + np.random.normal(0, 0.01), 0.1, 0.9)\n            self.CR = np.clip(self.CR + np.random.normal(0, 0.01), 0.1, 0.9)\n\n            # Population reduction\n            if self.budget > 0 and self.pop_size > self.min_pop_size:\n                num_to_reduce = max(1, int(self.pop_size * (1 - self.reduction_factor)))\n                if self.pop_size - num_to_reduce >= self.min_pop_size:\n\n                    sorted_indices = np.argsort(fitness)[::-1]  # sort from worst to best\n                    population = population[sorted_indices[:-num_to_reduce]]\n                    fitness = fitness[sorted_indices[:-num_to_reduce]]\n                    self.pop_size -= num_to_reduce\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm SelfAdaptiveDE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0]}, "task_prompt": ""}
{"id": "f2bdd08c-4dc5-4365-aea3-c172071f1a3e", "fitness": 0.7353187227547426, "name": "AdaptiveRestartDE", "description": "No description provided.", "code": "import numpy as np\n\nclass AdaptiveRestartDE:\n    def __init__(self, budget=10000, dim=10, pop_size=20, F=0.5, CR=0.7, stagnation_threshold=500, pop_resize_freq=1000):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F  # Differential weight\n        self.CR = CR  # Crossover rate\n        self.stagnation_threshold = stagnation_threshold\n        self.stagnation_counter = 0\n        self.pop_resize_freq = pop_resize_freq\n        self.last_improvement = 0\n        self.initial_pop_size = pop_size\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        \n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index].copy()\n        \n        generation = 0\n\n        while self.budget > 0:\n            new_population = np.zeros((self.pop_size, self.dim))\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = np.random.choice(idxs, 3, replace=False)\n                mutant = population[a] + self.F * (population[b] - population[c])\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                new_population[i] = trial\n\n            new_fitness = np.array([func(x) for x in new_population])\n            self.budget -= self.pop_size\n\n            # Selection\n            for i in range(self.pop_size):\n                if new_fitness[i] < fitness[i]:\n                    fitness[i] = new_fitness[i]\n                    population[i] = new_population[i].copy()\n\n            # Update best solution\n            best_index = np.argmin(fitness)\n            if fitness[best_index] < self.f_opt:\n                self.f_opt = fitness[best_index]\n                self.x_opt = population[best_index].copy()\n                self.stagnation_counter = 0\n                self.last_improvement = generation\n            else:\n                self.stagnation_counter += 1\n\n            # Adapt parameters (simple adaptation - can be improved)\n            self.F = np.clip(self.F + np.random.normal(0, 0.01), 0.1, 0.9)\n            self.CR = np.clip(self.CR + np.random.normal(0, 0.01), 0.1, 0.9)\n            \n            # Stagnation check and restart\n            if self.stagnation_counter > self.stagnation_threshold:\n                # Restart the population\n                population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                fitness = np.array([func(x) for x in population])\n                self.budget -= self.pop_size\n                \n                best_index = np.argmin(fitness)\n                if fitness[best_index] < self.f_opt:\n                    self.f_opt = fitness[best_index]\n                    self.x_opt = population[best_index].copy()\n                self.stagnation_counter = 0\n                self.last_improvement = generation\n\n            # Dynamic population size adjustment\n            if generation > 0 and generation % self.pop_resize_freq == 0:\n                improvement_ratio = (generation - self.last_improvement) / self.pop_resize_freq\n                if improvement_ratio < 0.2:  # Example threshold\n                    self.pop_size = max(10, int(self.pop_size * 0.8))  # Reduce population size\n                    # Reinitialize population (smaller)\n                    population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                    fitness = np.array([func(x) for x in population])\n                    self.budget -= self.pop_size  # Account for new evals\n                    best_index = np.argmin(fitness)\n                    if fitness[best_index] < self.f_opt:\n                        self.f_opt = fitness[best_index]\n                        self.x_opt = population[best_index].copy()\n\n                elif improvement_ratio > 0.5:  # Example threshold\n                    self.pop_size = min(self.initial_pop_size, int(self.pop_size * 1.2)) # Increase population size\n                    # Reinitialize population (larger)\n                    population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                    fitness = np.array([func(x) for x in population])\n                    self.budget -= self.pop_size # Account for new evals\n                    best_index = np.argmin(fitness)\n                    if fitness[best_index] < self.f_opt:\n                        self.f_opt = fitness[best_index]\n                        self.x_opt = population[best_index].copy()\n            generation += 1\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveRestartDE scored 0.735 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.3207418742453204, 0.6112249964957054, 0.5234634245217769, 0.9212462929160632, 0.8506050947956533, 0.8677980817886533, 0.8031311597210763, 0.78433739359364, 0.8744609565288562, 0.8213560271537631, 0.9073723490643332, 0.9995143920661104, 0.3959914084184426, 0.7543580996887118, 0.8715075925375152, 0.8728950604715888, 0.6769966600379022, 0.905666125327381, 0.41195411419317696, 0.5317533515291823]}, "task_prompt": ""}
{"id": "5b81ee9d-6b74-49bd-8538-fda094976095", "fitness": "-inf", "name": "SelfAdaptiveDifferentialEvolution", "description": "No description provided.", "code": "import numpy as np\n\nclass SelfAdaptiveDifferentialEvolution:\n    def __init__(self, budget=10000, dim=10, pop_size=20, F=0.5, CR=0.7, mutation_strategies=None):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F  # Differential weight\n        self.CR = CR  # Crossover rate\n        if mutation_strategies is None:\n            self.mutation_strategies = [\n                lambda pop, a, b, c, F: pop[a] + F * (pop[b] - pop[c]),\n                lambda pop, a, b, c, F, best: best + F * (pop[a] - pop[b]) + F * (pop[c] - pop[best]),\n                lambda pop, a, b, c, F: pop[a] + F * (pop[b] - pop[c]),\n            ]\n        else:\n            self.mutation_strategies = mutation_strategies\n        self.num_strategies = len(self.mutation_strategies)\n        self.strategy_successes = np.zeros(self.num_strategies)\n        self.strategy_counts = np.zeros(self.num_strategies)\n        self.epsilon = 1e-6\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index].copy()\n        best_position = population[best_index]\n\n        while self.budget > 0:\n            new_population = np.zeros((self.pop_size, self.dim))\n            strategy_indices = np.random.randint(0, self.num_strategies, size=self.pop_size)\n            for i in range(self.pop_size):\n                # Mutation strategy selection\n                strategy_index = strategy_indices[i]\n                self.strategy_counts[strategy_index] += 1\n\n                # Mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = np.random.choice(idxs, 3, replace=False)\n\n                try:\n                    mutant = self.mutation_strategies[strategy_index](population, a, b, c, self.F, best_position)\n                except TypeError:\n                     mutant = self.mutation_strategies[strategy_index](population, a, b, c, self.F)\n\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                new_population[i] = trial\n\n            new_fitness = np.array([func(x) for x in new_population])\n            self.budget -= self.pop_size\n\n            # Selection\n            for i in range(self.pop_size):\n                if new_fitness[i] < fitness[i]:\n                    fitness[i] = new_fitness[i]\n                    population[i] = new_population[i].copy()\n                    self.strategy_successes[strategy_indices[i]] += 1\n\n\n            # Update best solution\n            best_index = np.argmin(fitness)\n            if fitness[best_index] < self.f_opt:\n                self.f_opt = fitness[best_index]\n                self.x_opt = population[best_index].copy()\n                best_position = population[best_index]\n\n\n            # Adapt parameters (simple adaptation - can be improved)\n            self.F = np.clip(self.F + np.random.normal(0, 0.01), 0.1, 0.9)\n            self.CR = np.clip(self.CR + np.random.normal(0, 0.01), 0.1, 0.9)\n            #Adapt strategy probabilities, not needed.\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "In the code, line 13, in <lambda>, the following error occurred:\nIndexError: arrays used as indices must be of integer (or boolean) type\nOn line: lambda pop, a, b, c, F, best: best + F * (pop[a] - pop[b]) + F * (pop[c] - pop[best]),", "error": "In the code, line 13, in <lambda>, the following error occurred:\nIndexError: arrays used as indices must be of integer (or boolean) type\nOn line: lambda pop, a, b, c, F, best: best + F * (pop[a] - pop[b]) + F * (pop[c] - pop[best]),", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "efefc8f2-e08b-4d9a-bb4f-38d497d87c53", "fitness": "-inf", "name": "AdaptiveCMAES", "description": "No description provided.", "code": "import numpy as np\n\nclass AdaptiveCMAES:\n    def __init__(self, budget=10000, dim=10, initial_pop_size=None, sigma=0.3, cs=0.8, damp=None, adaptation_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.initial_pop_size = initial_pop_size or (4 + int(3 * np.log(dim)))\n        self.pop_size = self.initial_pop_size\n        self.sigma = sigma\n        self.mean = None\n        self.C = None\n        self.ps = None\n        self.pc = None\n        self.chiN = dim**0.5 * (1 - 1/(4*dim) + 1/(21*dim**2))\n        self.mu = self.pop_size // 2\n        self.weights = np.log(self.mu+1/2) - np.log(np.arange(1, self.mu+1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n\n        self.cs = cs\n        self.damp = damp or 1 + 2*max(0, np.sqrt((self.mueff-1)/(self.dim+1))-1) + self.cs\n        self.cc = (4 + self.mueff/self.dim) / (self.dim + 4 + 2*self.mueff/self.dim)\n        self.c1 = 2 / ((self.dim + 1.3)**2 + self.mueff)\n        self.cmu = min(1-self.c1, 2 * (self.mueff-2+1/self.mueff) / ((self.dim+2.3)**2 + self.mueff))\n        self.lb = -5.0\n        self.ub = 5.0\n        self.adaptation_rate = adaptation_rate\n\n    def __call__(self, func):\n        self.mean = np.random.uniform(self.lb, self.ub, size=self.dim)\n        self.C = np.eye(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.f_opt = np.inf\n        self.x_opt = None\n        eval_count = 0\n\n        while eval_count < self.budget:\n            # Generate population\n            z = np.random.normal(0, 1, size=(self.pop_size, self.dim))\n            x = self.mean + self.sigma * z @ np.linalg.cholesky(self.C).T\n            x = np.clip(x, self.lb, self.ub)\n\n            # Evaluate population\n            fitness = np.array([func(xi) for xi in x])\n            eval_count += self.pop_size\n\n            # Sort by fitness\n            indices = np.argsort(fitness)\n            x = x[indices]\n            fitness = fitness[indices]\n\n            #Update best solution\n            if fitness[0] < self.f_opt:\n                self.f_opt = fitness[0]\n                self.x_opt = x[0].copy()\n\n            # Update mean\n            xmean = np.sum(x[:self.mu] * self.weights[:, None], axis=0)\n            y = xmean - self.mean\n\n            # Update evolution paths\n            self.ps = (1-self.cs)*self.ps + np.sqrt(self.cs*(2-self.cs)*self.mueff) * y @ np.linalg.inv(np.linalg.cholesky(self.C)).T / self.sigma\n            self.pc = (1-self.cc)*self.pc + np.sqrt(self.cc*(2-self.cc)*self.mueff) * y / self.sigma\n\n            # Update covariance matrix\n            delta = x - self.mean\n            rank_one = self.c1 * np.outer(self.pc, self.pc)\n            rank_mu = self.cmu * np.sum(w * np.outer(d, d) for w, d in zip(self.weights, delta[:self.mu]))\n            self.C = (1 - self.c1 - self.cmu) * self.C + rank_one + rank_mu / self.sigma**2\n\n            # Update step size\n            self.sigma *= np.exp((self.cs/self.damp)*(np.linalg.norm(self.ps)/self.chiN - 1))\n\n            # Update mean\n            self.mean = xmean\n\n            # Adapt population size\n            success_rate = np.mean(fitness < np.median(fitness))\n            if success_rate > 0.7:\n                self.pop_size = min(self.initial_pop_size * 2, self.pop_size + int(self.adaptation_rate * self.initial_pop_size))\n                self.mu = self.pop_size // 2\n                self.weights = np.log(self.mu+1/2) - np.log(np.arange(1, self.mu+1))\n                self.weights = self.weights / np.sum(self.weights)\n                self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n            elif success_rate < 0.3:\n                self.pop_size = max(self.initial_pop_size // 2, self.pop_size - int(self.adaptation_rate * self.initial_pop_size))\n                self.mu = self.pop_size // 2\n                self.weights = np.log(self.mu+1/2) - np.log(np.arange(1, self.mu+1))\n                self.weights = self.weights / np.sum(self.weights)\n                self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "In the code, line 2419, in sum, the following error occurred:\nTypeError: Calling np.sum(generator) is deprecated.Use np.sum(np.fromiter(generator)) or the python sum builtin instead.", "error": "In the code, line 2419, in sum, the following error occurred:\nTypeError: Calling np.sum(generator) is deprecated.Use np.sum(np.fromiter(generator)) or the python sum builtin instead.", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "631762bb-2b06-41d6-b4c4-eeeee88ff18f", "fitness": 0.1060343504767974, "name": "CMAES", "description": "No description provided.", "code": "import numpy as np\n\nclass CMAES:\n    def __init__(self, budget=10000, dim=10, pop_size=None, sigma=0.5, cs=0.3, damps=1, c_cov_mean=None, c_cov_rank_one=None, c_cov_rank_mu=None, mu_factor=4):\n        self.budget = budget\n        self.dim = dim\n        self.sigma = sigma\n        self.cs = cs\n        self.damps = damps\n        self.lb = -5.0\n        self.ub = 5.0\n        \n        if pop_size is None:\n          self.pop_size = 4 + int(3 * np.log(self.dim))\n        else:\n            self.pop_size = pop_size\n        \n        self.mu = self.pop_size // mu_factor # number of parents/elite individuals\n        self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n        self.weights /= np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        \n        self.c_cov_mean = c_cov_mean if c_cov_mean is not None else self.cs\n        self.c_cov_rank_one = c_cov_rank_one if c_cov_rank_one is not None else min(1, self.mu / (self.dim + 6))\n        self.c_cov_rank_mu = c_cov_rank_mu if c_cov_rank_mu is not None else min(1 - self.c_cov_rank_one, self.mueff / (self.dim + 13))\n        self.damps = self.damps + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1) * self.damps * (1 - self.cs)\n        self.mean = np.random.uniform(self.lb, self.ub, size=self.dim)\n        self.C = np.eye(self.dim)\n        self.B = None # eigenvectors of C\n        self.D = None # diagonal of eigenvalues of C\n        self.evals = 0\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.restart_threshold = budget // 10\n        self.stagnation_counter = 0\n        self.min_delta = 1e-12\n\n    def sample_population(self):\n        if self.B is None or self.D is None:\n            self.D, self.B = np.linalg.eigh(self.C)\n            self.D = np.sqrt(self.D)\n        z = np.random.normal(0, 1, size=(self.pop_size, self.dim))\n        y = self.B @ (self.D[:, None] * z.T)\n        x = self.mean + self.sigma * y.T\n        return np.clip(x, self.lb, self.ub)\n\n    def update_distribution(self, population, fitness):\n        idx = np.argsort(fitness)\n        elite_idx = idx[:self.mu]\n        elite_pop = population[elite_idx]\n\n        y_mean = np.sum(self.weights[:, None] * (elite_pop - self.mean), axis=0)\n        self.mean = self.mean + self.cs * y_mean\n\n        C_rank_one = self.c_cov_rank_one * y_mean[:, None] @ y_mean[None, :]\n        \n        z = (elite_pop - self.mean) / self.sigma\n        C_rank_mu = self.c_cov_rank_mu * np.sum(self.weights[:, None, None] * z[:, :, None] @ z[:, None, :], axis=0)\n\n        self.C = (1 - self.c_cov_rank_one - self.c_cov_rank_mu) * self.C + C_rank_one + C_rank_mu\n        self.sigma *= np.exp((self.cs / self.damps) * (np.linalg.norm(y_mean) / np.linalg.norm(np.random.normal(0, 1, self.dim)) - 1))\n\n        delta = np.max(np.abs(elite_pop[0] - self.mean))\n        if delta < self.min_delta:\n              self.stagnation_counter += 1\n        else:\n              self.stagnation_counter = 0\n        \n        if self.stagnation_counter > self.restart_threshold:\n            self.mean = np.random.uniform(self.lb, self.ub, size=self.dim)\n            self.C = np.eye(self.dim)\n            self.sigma = 0.5\n            self.stagnation_counter = 0\n            self.B = None\n            self.D = None\n\n    def __call__(self, func):\n        while self.evals < self.budget:\n            population = self.sample_population()\n            fitness = np.array([func(x) for x in population])\n            self.evals += self.pop_size\n            \n            best_idx = np.argmin(fitness)\n            if fitness[best_idx] < self.f_opt:\n                self.f_opt = fitness[best_idx]\n                self.x_opt = population[best_idx]\n\n            self.update_distribution(population, fitness)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm CMAES scored 0.106 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.03297622704832681, 0.011922585959187382, 0.19876933881188286, 0.07143693961393505, 0.018265598385907134, 0.1277639567226514, 0.18371358628026158, 0.1350542701985178, 0.20545800906170375, 0.07911534070139936, 0.11737425871011586, 0.15709202933907762, 9.999999999998899e-05, 0.05941995676054701, 0.12565381995279346, 0.1718957938139447, 0.10946465380154002, 0.09001715692977452, 0.11431749028954508, 0.11087599715483698]}, "task_prompt": ""}
{"id": "61aef190-1779-473d-b781-894bbf3a5fa6", "fitness": 0.5477945865978765, "name": "ClampedDynamicPSO", "description": "No description provided.", "code": "import numpy as np\n\nclass ClampedDynamicPSO:\n    def __init__(self, budget=10000, dim=10, swarm_size=20, inertia_max=0.9, inertia_min=0.4, \n                 cognitive_coeff=2.0, social_coeff=2.0, velocity_clamp=0.5, stagnation_threshold=500):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = swarm_size\n        self.inertia_max = inertia_max\n        self.inertia_min = inertia_min\n        self.cognitive_coeff = cognitive_coeff\n        self.social_coeff = social_coeff\n        self.velocity_clamp = velocity_clamp\n        self.stagnation_threshold = stagnation_threshold\n        self.stagnation_counter = 0\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.global_best_position = None\n\n    def __call__(self, func):\n        # Initialize swarm\n        swarm_position = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.swarm_size, self.dim))\n        swarm_velocity = np.random.uniform(-self.velocity_clamp, self.velocity_clamp, size=(self.swarm_size, self.dim))\n        personal_best_position = swarm_position.copy()\n        personal_best_fitness = np.array([func(x) for x in swarm_position])\n        self.budget -= self.swarm_size\n\n        # Initialize global best\n        best_index = np.argmin(personal_best_fitness)\n        self.f_opt = personal_best_fitness[best_index]\n        self.x_opt = personal_best_position[best_index].copy()\n        self.global_best_position = self.x_opt.copy()\n        \n        generation = 0\n        while self.budget > 0:\n            # Update inertia weight (linearly decreasing)\n            inertia_weight = self.inertia_max - (self.inertia_max - self.inertia_min) * generation / (self.budget // self.swarm_size + generation)\n\n            for i in range(self.swarm_size):\n                # Update velocity\n                cognitive_component = self.cognitive_coeff * np.random.rand(self.dim) * (personal_best_position[i] - swarm_position[i])\n                social_component = self.social_coeff * np.random.rand(self.dim) * (self.global_best_position - swarm_position[i])\n                swarm_velocity[i] = inertia_weight * swarm_velocity[i] + cognitive_component + social_component\n                \n                # Velocity clamping\n                swarm_velocity[i] = np.clip(swarm_velocity[i], -self.velocity_clamp, self.velocity_clamp)\n\n                # Update position\n                swarm_position[i] = swarm_position[i] + swarm_velocity[i]\n                swarm_position[i] = np.clip(swarm_position[i], func.bounds.lb, func.bounds.ub)\n\n                # Evaluate fitness\n                fitness = func(swarm_position[i])\n                self.budget -= 1\n                if fitness < personal_best_fitness[i]:\n                    personal_best_fitness[i] = fitness\n                    personal_best_position[i] = swarm_position[i].copy()\n\n                    # Update global best\n                    if fitness < self.f_opt:\n                        self.f_opt = fitness\n                        self.x_opt = swarm_position[i].copy()\n                        self.global_best_position = self.x_opt.copy()\n                        self.stagnation_counter = 0 # Reset stagnation counter\n                else:\n                    self.stagnation_counter +=1 # Increment stagnation counter\n\n                if self.budget <= 0:\n                  break;\n            \n            if self.stagnation_counter > self.stagnation_threshold:\n                # Re-initialize a fraction of the swarm\n                num_reinitialize = self.swarm_size // 4\n                indices_to_reinitialize = np.random.choice(self.swarm_size, num_reinitialize, replace=False)\n                \n                swarm_position[indices_to_reinitialize] = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(num_reinitialize, self.dim))\n                swarm_velocity[indices_to_reinitialize] = np.random.uniform(-self.velocity_clamp, self.velocity_clamp, size=(num_reinitialize, self.dim))\n                \n                for i in indices_to_reinitialize:\n                    personal_best_position[i] = swarm_position[i].copy()\n                    personal_best_fitness[i] = func(swarm_position[i])\n                    self.budget -=1\n                    if personal_best_fitness[i] < self.f_opt:\n                        self.f_opt = personal_best_fitness[i]\n                        self.x_opt = swarm_position[i].copy()\n                        self.global_best_position = self.x_opt.copy()\n                self.stagnation_counter = 0\n            generation += 1\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm ClampedDynamicPSO scored 0.548 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.22335634544075655, 0.6475905344374095, 0.6447724548132627, 0.19829778043642432, 0.5976092119954575, 0.7045439200215258, 0.35733569413487565, 0.544611068137353, 0.6735141594175368, 0.18956614449894715, 0.8866892690956217, 0.994470586386679, 0.2828698387789321, 0.5350655813643792, 0.857070578693757, 0.6940541270465591, 0.40959316110179134, 0.8023378911659719, 0.22108623193172294, 0.4914571530585671]}, "task_prompt": ""}
{"id": "fde7524a-8fd8-43d1-9a27-21db79d08a25", "fitness": 0.20089310054303833, "name": "BudgetAwareCMAES", "description": "No description provided.", "code": "import numpy as np\n\nclass BudgetAwareCMAES:\n    def __init__(self, budget=10000, dim=10, pop_size=None):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size if pop_size is not None else 4 + int(3 * np.log(dim))\n        self.lb = -5.0\n        self.ub = 5.0\n        self.mean = np.random.uniform(self.lb, self.ub, size=dim)\n        self.sigma = 0.5 * (self.ub - self.lb) # Initial step size\n        self.C = np.eye(dim)  # Covariance matrix\n        self.pc = np.zeros(dim)  # Evolution path for C\n        self.ps = np.zeros(dim)  # Evolution path for sigma\n        self.chiN = dim**0.5 * (1 - 1/(4*dim) + 1/(21*dim**2))\n        self.c_sigma = (self.budget/self.dim)**(-0.25) # Adaptive step size dampening\n        self.c_c = 4 / (dim + 4)\n        self.c_1 = 2 / ((dim + np.sqrt(2))**2)\n        self.c_mu = 2 * (min(1 - self.c_1, (self.budget / self.dim)**(-0.25)))/ ((dim + 2)**2)\n        self.mu = self.pop_size // 2\n        self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.evals = 0\n\n    def sample_population(self):\n        z = np.random.normal(0, 1, size=(self.pop_size, self.dim))\n        x = self.mean + self.sigma * np.dot(z, np.linalg.cholesky(self.C).T)\n        x = np.clip(x, self.lb, self.ub)\n        return x, z\n\n    def update_parameters(self, x, z, fitness):\n        idx = np.argsort(fitness)\n        x_sorted = x[idx]\n        z_sorted = z[idx]\n\n        y = x_sorted[:self.mu] - self.mean\n        self.mean = np.sum(self.weights[:, None] * x_sorted[:self.mu], axis=0)\n        \n        zmean = np.sum(self.weights[:, None] * z_sorted[:self.mu], axis=0)\n        self.ps = (1 - self.c_sigma) * self.ps + (self.c_sigma * (2 - self.c_sigma))**0.5 * zmean\n        self.sigma *= np.exp((self.c_sigma / self.chiN) * (np.linalg.norm(self.ps) - self.chiN))\n        self.sigma = np.clip(self.sigma, 1e-10, 1000)  #Prevent sigma from becoming too small/large\n\n        self.pc = (1 - self.c_c) * self.pc + (self.c_c * (2 - self.c_c))**0.5 * np.linalg.solve(self.C**0.5, y[0])\n\n        C_temp = self.c_1 * (self.pc[:, None] @ self.pc[None, :])\n        for i in range(self.mu):\n            C_temp += self.c_mu * self.weights[i] * (y[i, :, None] @ y[i, None, :]) / self.sigma**2\n        self.C = (1 - self.c_1 - self.c_mu) * self.C + C_temp\n        self.C = np.triu(self.C) + np.triu(self.C, 1).T  #Enforce symmetry\n\n        try: #handle ill-conditioned matrix\n            if np.linalg.det(self.C) <= 0:\n                self.C = self.C + 1e-6 * np.eye(self.dim)\n            self.C = self.C * (np.linalg.norm(self.ps)**2 < self.dim) + self.C * (np.linalg.norm(self.ps)**2 >= self.dim) #Dampen C for explosion\n        except: #Restart if matrix is invalid\n            self.C = np.eye(self.dim)\n\n    def __call__(self, func):\n        while self.evals < self.budget:\n            x, z = self.sample_population()\n            fitness = np.array([func(xi) for xi in x])\n            self.evals += self.pop_size\n\n            if np.min(fitness) < self.f_opt:\n                self.f_opt = np.min(fitness)\n                self.x_opt = x[np.argmin(fitness)]\n\n            self.update_parameters(x, z, fitness)\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm BudgetAwareCMAES scored 0.201 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.03148278081887146, 0.08263691252781336, 0.19657193642046966, 0.11523131643578022, 0.11911165859527717, 0.15140081947190864, 0.14606993202172125, 0.11554930011114029, 0.15838630059936498, 0.1314792853796274, 0.11434701045906637, 0.9987997451802794, 0.040416923812107264, 0.08547936069847006, 0.5471363429138625, 0.19646309459705558, 0.15426875330531653, 0.17300780084435774, 0.09497856997827347, 0.36504416669000306]}, "task_prompt": ""}
{"id": "275a669a-d2a7-4a95-b7aa-6d859f1e37ce", "fitness": 0.7656062570613431, "name": "SelfAdaptiveDE", "description": "No description provided.", "code": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass SelfAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=20, F=0.5, CR=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.lb = -5.0\n        self.ub = 5.0\n        self.population = None\n        self.fitness = None\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.success_F = []\n        self.success_CR = []\n        self.archive = []\n        self.archive_size = 10\n        self.adaptation_rate = 0.1\n        self.local_search_interval = budget // 20\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.pop_size\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n\n    def mutate(self, pop, i):\n        idxs = [idx for idx in range(self.pop_size) if idx != i]\n        a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n        return np.clip(a + self.F * (b - c), self.lb, self.ub)\n\n    def crossover(self, mutant, target):\n        cross_points = np.random.rand(self.dim) < self.CR\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        return np.where(cross_points, mutant, target)\n\n    def update_parameters(self):\n        if self.success_F:\n            mean_F = np.mean(self.success_F)\n            mean_CR = np.mean(self.success_CR)\n\n            self.F = (1 - self.adaptation_rate) * self.F + self.adaptation_rate * mean_F\n            self.CR = (1 - self.adaptation_rate) * self.CR + self.adaptation_rate * mean_CR\n        else:\n            self.F = 0.5\n            self.CR = 0.9\n        \n        self.F = np.clip(self.F, 0.1, 0.9)\n        self.CR = np.clip(self.CR, 0.1, 0.9)\n        self.success_F = []\n        self.success_CR = []\n\n    def local_search(self, func):\n        for i in range(self.pop_size):\n            if self.evals >= self.budget:\n                break\n            x0 = self.population[i].copy()\n            bounds = [(self.lb, self.ub)] * self.dim\n            res = minimize(func, x0, method='Nelder-Mead', bounds=bounds, options={'maxfev': min(self.budget - self.evals, 50)})\n\n            if res.fun < self.fitness[i]:\n                self.population[i] = res.x\n                self.fitness[i] = res.fun\n                self.evals += res.nfev\n\n                if res.fun < self.f_opt:\n                    self.f_opt = res.fun\n                    self.x_opt = res.x\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        iter_count = 0\n        while self.evals < self.budget:\n            iter_count += 1\n\n            if iter_count % self.local_search_interval == 0:\n                self.local_search(func)\n\n            new_population = []\n            new_fitness = []\n\n            for i in range(self.pop_size):\n                if self.evals >= self.budget:\n                  new_population.append(self.population[i])\n                  new_fitness.append(self.fitness[i])\n                  continue\n\n                mutant = self.mutate(self.population, i)\n                trial = self.crossover(mutant, self.population[i])\n                f = func(trial)\n                self.evals += 1\n\n                if f < self.fitness[i]:\n                    new_population.append(trial)\n                    new_fitness.append(f)\n\n                    self.success_F.append(self.F)\n                    self.success_CR.append(self.CR)\n\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = trial\n                else:\n                    new_population.append(self.population[i])\n                    new_fitness.append(self.fitness[i])\n            \n            self.population = np.array(new_population)\n            self.fitness = np.array(new_fitness)\n            self.update_parameters()\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm SelfAdaptiveDE scored 0.766 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.5035145073175193, 0.36303122777508956, 0.631286918790263, 0.9213000191465122, 0.8620240407650324, 0.898521484314686, 0.8194137306788537, 0.841185044740985, 0.8519126618765407, 0.8270769496143758, 0.9364868308306006, 0.997993772300845, 0.41453167558853476, 0.868085711642909, 0.5862817348658129, 0.892669132996887, 0.8448071769637503, 0.9098331819707602, 0.8077140300080037, 0.534455309038901]}, "task_prompt": ""}
{"id": "2768d959-5ec5-447a-9290-33b66978563b", "fitness": "-inf", "name": "GaussianProcessOptimization", "description": "No description provided.", "code": "import numpy as np\nfrom sklearn.gaussian_process import GaussianProcessRegressor\nfrom sklearn.gaussian_process.kernels import RBF, ConstantKernel as C\nfrom scipy.stats import norm\n\nclass GaussianProcessOptimization:\n    def __init__(self, budget=10000, dim=10, n_initial=10, xi=0.01):\n        self.budget = budget\n        self.dim = dim\n        self.n_initial = n_initial\n        self.xi = xi # Exploration-exploitation trade-off parameter\n        self.X = None\n        self.y = None\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.lb = -5.0\n        self.ub = 5.0\n        self.evals = 0\n\n    def expected_improvement(self, x, gp):\n        mu, sigma = gp.predict(x.reshape(1, -1), return_std=True)\n        if sigma == 0:\n            return 0\n        imp = (mu - self.f_opt - self.xi)\n        Z = imp / sigma\n        ei = imp * norm.cdf(Z) + sigma * norm.pdf(Z)\n        return ei\n\n    def propose_location(self, gp, func, n_restarts=25):\n        best_ei = -np.inf\n        best_x = None\n        bounds = np.array([[self.lb, self.ub]] * self.dim)\n\n        for restart in range(n_restarts):\n            x0 = np.random.uniform(bounds[:, 0], bounds[:, 1], size=self.dim)\n\n            res = minimize(lambda x: -self.expected_improvement(x, gp), x0=x0,\n                           bounds=bounds, method='L-BFGS-B')\n\n            if -res.fun > best_ei:\n                best_ei = -res.fun\n                best_x = res.x\n\n        return best_x\n\n    def __call__(self, func):\n        # Initial sampling\n        self.X = np.random.uniform(self.lb, self.ub, size=(self.n_initial, self.dim))\n        self.y = np.array([func(x) for x in self.X])\n        self.evals = self.n_initial\n\n        best_index = np.argmin(self.y)\n        self.f_opt = self.y[best_index]\n        self.x_opt = self.X[best_index].copy()\n\n        # Gaussian process\n        kernel = C(1.0, (1e-3, 1e3)) * RBF(1.0, (1e-3, 1e3))\n        gp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=10)\n\n        # Optimization loop\n        while self.evals < self.budget:\n            gp.fit(self.X, self.y)\n            x_next = self.propose_location(gp, func)\n            f_next = func(x_next)\n            self.evals += 1\n\n            self.X = np.vstack((self.X, x_next))\n            self.y = np.append(self.y, f_next)\n\n            if f_next < self.f_opt:\n                self.f_opt = f_next\n                self.x_opt = x_next.copy()\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "In the code, line 57, in __call__, the following error occurred:\nNameError: name 'C' is not defined\nOn line: kernel = C(1.0, (1e-3, 1e3)) * RBF(1.0, (1e-3, 1e3))", "error": "In the code, line 57, in __call__, the following error occurred:\nNameError: name 'C' is not defined\nOn line: kernel = C(1.0, (1e-3, 1e3)) * RBF(1.0, (1e-3, 1e3))", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "666bc346-b5bb-4b05-b758-e95a52d09de3", "fitness": "-inf", "name": "AdaptiveSimulatedAnnealing", "description": "No description provided.", "code": "import numpy as np\n\nclass AdaptiveSimulatedAnnealing:\n    def __init__(self, budget=10000, dim=10, initial_temp=100.0, cooling_rate=0.95, step_size=1.0):\n        self.budget = budget\n        self.dim = dim\n        self.initial_temp = initial_temp\n        self.cooling_rate = cooling_rate\n        self.step_size = step_size\n        self.lb = -5.0\n        self.ub = 5.0\n\n    def __call__(self, func):\n        self.x_opt = np.random.uniform(self.lb, self.ub, size=self.dim)\n        self.f_opt = func(self.x_opt)\n        eval_count = 1\n        temp = self.initial_temp\n\n        while eval_count < self.budget:\n            # Generate neighbor\n            x_new = self.x_opt + np.random.normal(0, self.step_size, size=self.dim)\n            x_new = np.clip(x_new, self.lb, self.ub)\n            f_new = func(x_new)\n            eval_count += 1\n\n            # Acceptance probability\n            delta_f = f_new - self.f_opt\n            if delta_f < 0 or np.random.rand() < np.exp(-delta_f / temp):\n                self.x_opt = x_new\n                self.f_opt = f_new\n\n            # Local Search (occasional refinement)\n            if eval_count % 100 == 0:\n                x_local = self.local_search(func, self.x_opt, budget=min(100, self.budget - eval_count))\n                f_local = func(x_local)\n                eval_count += 1\n                if f_local < self.f_opt:\n                    self.x_opt = x_local\n                    self.f_opt = f_local\n                    \n\n            # Cooling\n            temp *= self.cooling_rate\n            self.step_size *= 0.99 #Reduce step size\n\n        return self.f_opt, self.x_opt\n\n    def local_search(self, func, x_start, budget):\n        x_best = x_start.copy()\n        f_best = func(x_start)\n        eval_count = 0\n        while eval_count < budget:\n             x_new = x_best + np.random.normal(0, 0.1, size=self.dim)\n             x_new = np.clip(x_new, self.lb, self.ub)\n             f_new = func(x_new)\n             eval_count+=1\n             if f_new < f_best:\n                  f_best = f_new\n                  x_best = x_new\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "Traceback (most recent call last):\n  File \"C:\\Users\\Kukoy\\AppData\\Local\\Temp\\blade_env_vttbffww\\run_eval.py\", line 10, in <module>\n    result=problem.evaluate(solution)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Kukoy\\OneDrive\\Documents\\BLADE-WORKING\\BLADE\\iohblade\\problems\\mabbob.py\", line 187, in evaluate\n    algorithm(f_new)\n  File \"<string>\", line 35, in __call__\nTypeError: __call__(): incompatible function arguments. The following argument types are supported:\n    1. (self: ioh.iohcpp.problem.RealSingleObjective, arg0: List[float]) -> float\n    2. (self: ioh.iohcpp.problem.RealSingleObjective, arg0: List[List[float]]) -> List[float]\n\nInvoked with: <RealSingleObjectiveProblem 100. ManyAffine (iid=0 dim=5)>, (32.98131214646812, array([-1.2351531 , -1.50494557,  2.55152158,  2.05649601, -3.10522104]))", "error": "Traceback (most recent call last):\n  File \"C:\\Users\\Kukoy\\AppData\\Local\\Temp\\blade_env_vttbffww\\run_eval.py\", line 10, in <module>\n    result=problem.evaluate(solution)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Kukoy\\OneDrive\\Documents\\BLADE-WORKING\\BLADE\\iohblade\\problems\\mabbob.py\", line 187, in evaluate\n    algorithm(f_new)\n  File \"<string>\", line 35, in __call__\nTypeError: __call__(): incompatible function arguments. The following argument types are supported:\n    1. (self: ioh.iohcpp.problem.RealSingleObjective, arg0: List[float]) -> float\n    2. (self: ioh.iohcpp.problem.RealSingleObjective, arg0: List[List[float]]) -> List[float]\n\nInvoked with: <RealSingleObjectiveProblem 100. ManyAffine (iid=0 dim=5)>, (32.98131214646812, array([-1.2351531 , -1.50494557,  2.55152158,  2.05649601, -3.10522104]))", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "15381c69-7974-437d-8b80-f598feb08c24", "fitness": 0.4463691011626808, "name": "DynamicSwarm", "description": "No description provided.", "code": "import numpy as np\n\nclass DynamicSwarm:\n    def __init__(self, budget=10000, dim=10, swarm_size=30, w_max=0.9, w_min=0.4, c1=2, c2=2):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = swarm_size\n        self.w_max = w_max\n        self.w_min = w_min\n        self.c1 = c1\n        self.c2 = c2\n        self.lb = -5.0\n        self.ub = 5.0\n        self.positions = None\n        self.velocities = None\n        self.fitness = None\n        self.pbest_positions = None\n        self.pbest_fitness = None\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.evals = 0\n        self.neighborhood_size = int(swarm_size * 0.2) # Initial neighborhood size\n\n    def initialize_swarm(self, func):\n        self.positions = np.random.uniform(self.lb, self.ub, size=(self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, size=(self.swarm_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.positions])\n        self.evals = self.swarm_size\n        self.pbest_positions = self.positions.copy()\n        self.pbest_fitness = self.fitness.copy()\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.positions[np.argmin(self.fitness)]\n\n    def update_velocity(self, i, gbest_position):\n        w = self.w_max - (self.w_max - self.w_min) * (self.evals / self.budget)  # Dynamic inertia weight\n\n        # Neighborhood best\n        neighbors = np.random.choice(self.swarm_size, self.neighborhood_size, replace=False)\n        neighbor_fitness = self.pbest_fitness[neighbors]\n        nbest_index = neighbors[np.argmin(neighbor_fitness)]\n        nbest_position = self.pbest_positions[nbest_index]\n      \n        r1 = np.random.rand(self.dim)\n        r2 = np.random.rand(self.dim)\n        self.velocities[i] = w * self.velocities[i] + \\\n                            self.c1 * r1 * (self.pbest_positions[i] - self.positions[i]) + \\\n                            self.c2 * r2 * (nbest_position - self.positions[i])\n                            #self.c2 * r2 * (gbest_position - self.positions[i])\n        \n        #Velocity clipping\n        v_max = (self.ub - self.lb) * 0.1\n        self.velocities[i] = np.clip(self.velocities[i], -v_max, v_max)\n\n    def update_position(self, i):\n        self.positions[i] = np.clip(self.positions[i] + self.velocities[i], self.lb, self.ub)\n\n    def __call__(self, func):\n        self.initialize_swarm(func)\n\n        while self.evals < self.budget:\n            # Find global best\n            gbest_index = np.argmin(self.pbest_fitness)\n            gbest_position = self.pbest_positions[gbest_index]\n\n            for i in range(self.swarm_size):\n                if self.evals >= self.budget:\n                    break\n\n                self.update_velocity(i, gbest_position)\n                self.update_position(i)\n\n                f = func(self.positions[i])\n                self.evals += 1\n\n                if f < self.fitness[i]:\n                    self.fitness[i] = f\n\n                    if f < self.pbest_fitness[i]:\n                        self.pbest_fitness[i] = f\n                        self.pbest_positions[i] = self.positions[i].copy()\n\n                        if f < self.f_opt:\n                            self.f_opt = f\n                            self.x_opt = self.positions[i].copy()\n\n            # Adjust neighborhood size based on swarm diversity\n            if self.evals % (self.budget//10) == 0:\n                diversity = np.mean(np.std(self.positions, axis=0))\n                if diversity < 0.1 * (self.ub - self.lb):\n                    self.neighborhood_size = min(self.neighborhood_size + 1, self.swarm_size)  # Increase exploration\n                else:\n                    self.neighborhood_size = max(int(self.swarm_size * 0.1), self.neighborhood_size - 1) # Decrease exploration\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm DynamicSwarm scored 0.446 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.18857970966024973, 0.38316806877893583, 0.4289413833833462, 0.6831175723847684, 0.41364339038294307, 0.512429668234405, 0.3070544893000299, 0.36802780737969876, 0.4015494212076246, 0.20704611184491162, 0.593311146317586, 0.9991927782073786, 0.2701357831366662, 0.3513320007918824, 0.7300974533872172, 0.48930993207850115, 0.36514307561761794, 0.5378121270384455, 0.192325028354271, 0.5051650757671369]}, "task_prompt": ""}
{"id": "629b88a9-ad54-450c-8337-7ffb443755ba", "fitness": "-inf", "name": "GaussianMixtureOptim", "description": "No description provided.", "code": "import numpy as np\nfrom sklearn.mixture import GaussianMixture\n\nclass GaussianMixtureOptim:\n    def __init__(self, budget=10000, dim=10, pop_size=50, n_components=5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.n_components = n_components\n        self.lb = -5.0\n        self.ub = 5.0\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        \n        # Initialize population\n        population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n        \n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index].copy()\n        \n        while self.budget > 0:\n            # Select top individuals\n            num_elites = int(self.pop_size * 0.2)  # 20% elites\n            elite_indices = np.argsort(fitness)[:num_elites]\n            elites = population[elite_indices]\n            \n            # Learn GMM from elites\n            gmm = GaussianMixture(n_components=self.n_components, covariance_type='full', random_state=0, max_iter=100, n_init=1, tol=1e-3)\n            gmm.fit(elites)\n            \n            # Sample new individuals from GMM\n            new_population = gmm.sample(self.pop_size)[0]\n            new_population = np.clip(new_population, self.lb, self.ub)\n            \n            # Evaluate new individuals\n            new_fitness = np.array([func(x) for x in new_population])\n            self.budget -= self.pop_size\n            \n            # Combine old and new populations\n            combined_population = np.vstack((population, new_population))\n            combined_fitness = np.concatenate((fitness, new_fitness))\n            \n            # Select the best individuals for the next generation\n            indices = np.argsort(combined_fitness)[:self.pop_size]\n            population = combined_population[indices]\n            fitness = combined_fitness[indices]\n            \n            # Update best solution\n            best_index = np.argmin(fitness)\n            if fitness[best_index] < self.f_opt:\n                self.f_opt = fitness[best_index]\n                self.x_opt = population[best_index].copy()\n        \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "In the code, line 33, in __call__, the following error occurred:\nNameError: name 'GaussianMixture' is not defined\nOn line: gmm = GaussianMixture(n_components=self.n_components, covariance_type='full', random_state=0, max_iter=100, n_init=1, tol=1e-3)", "error": "In the code, line 33, in __call__, the following error occurred:\nNameError: name 'GaussianMixture' is not defined\nOn line: gmm = GaussianMixture(n_components=self.n_components, covariance_type='full', random_state=0, max_iter=100, n_init=1, tol=1e-3)", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "5c8311bb-a63c-420d-9ab8-ad092217b065", "fitness": "-inf", "name": "GaussianProcessOptimization", "description": "No description provided.", "code": "import numpy as np\nfrom sklearn.gaussian_process import GaussianProcessRegressor\nfrom sklearn.gaussian_process.kernels import RBF, ConstantKernel, WhiteKernel\nfrom scipy.stats import norm\n\nclass GaussianProcessOptimization:\n    def __init__(self, budget=10000, dim=10, n_initial=10, exploration_weight=2.0):\n        self.budget = budget\n        self.dim = dim\n        self.n_initial = n_initial\n        self.exploration_weight = exploration_weight\n        self.lb = -5.0\n        self.ub = 5.0\n\n        self.kernel = ConstantKernel(1.0, constant_value_bounds=\"fixed\") * RBF(length_scale=1.0, length_scale_bounds=(1e-1, 10)) + WhiteKernel(noise_level=1e-3, noise_level_bounds=(1e-5, 1e-1))\n\n        self.gp = GaussianProcessRegressor(kernel=self.kernel, n_restarts_optimizer=5)\n        self.X = []\n        self.y = []\n        self.f_opt = np.inf\n        self.x_opt = None\n\n\n    def acquisition_function(self, x):\n        mu, sigma = self.gp.predict(x.reshape(1, -1), return_std=True)\n        if sigma==0:\n          return 0\n        return mu - self.exploration_weight * sigma\n\n\n    def __call__(self, func):\n        # Initial random sampling\n        X_initial = np.random.uniform(self.lb, self.ub, size=(self.n_initial, self.dim))\n        y_initial = np.array([func(x) for x in X_initial])\n\n        self.X = X_initial.tolist()\n        self.y = y_initial.tolist()\n\n        self.f_opt = np.min(self.y)\n        self.x_opt = self.X[np.argmin(self.y)]\n\n        evals = self.n_initial\n\n        while evals < self.budget:\n            self.gp.fit(self.X, self.y)\n\n            # Find next point to evaluate using acquisition function\n            best_x = None\n            best_acq = np.inf\n\n            for _ in range(1000):  # Sample potential next points\n                x_candidate = np.random.uniform(self.lb, self.ub, size=self.dim)\n                acq_value = self.acquisition_function(x_candidate)\n\n                if acq_value < best_acq:\n                    best_acq = acq_value\n                    best_x = x_candidate\n\n\n            f_new = func(best_x)\n            evals += 1\n\n            self.X.append(best_x)\n            self.y.append(f_new)\n\n\n            if f_new < self.f_opt:\n                self.f_opt = f_new\n                self.x_opt = best_x\n\n            # Adapt exploration weight (decay over time)\n            self.exploration_weight = max(0.1, self.exploration_weight * 0.99)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "In the code, line 15, in __init__, the following error occurred:\nNameError: name 'ConstantKernel' is not defined\nOn line: self.kernel = ConstantKernel(1.0, constant_value_bounds=\"fixed\") * RBF(length_scale=1.0, length_scale_bounds=(1e-1, 10)) + WhiteKernel(noise_level=1e-3, noise_level_bounds=(1e-5, 1e-1))", "error": "In the code, line 15, in __init__, the following error occurred:\nNameError: name 'ConstantKernel' is not defined\nOn line: self.kernel = ConstantKernel(1.0, constant_value_bounds=\"fixed\") * RBF(length_scale=1.0, length_scale_bounds=(1e-1, 10)) + WhiteKernel(noise_level=1e-3, noise_level_bounds=(1e-5, 1e-1))", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "22460997-48da-4838-bf8d-f7eabf097813", "fitness": 0.25903702523334254, "name": "PerturbationBasedOptimization", "description": "No description provided.", "code": "import numpy as np\n\nclass PerturbationBasedOptimization:\n    def __init__(self, budget=10000, dim=10, pop_size=20, step_size=0.1, success_rate_threshold=0.2):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.step_size = step_size\n        self.success_rate_threshold = success_rate_threshold\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index].copy()\n        \n        successes = 0\n        iterations = 0\n\n        while self.budget > 0:\n            iterations += 1\n            for i in range(self.pop_size):\n                if self.budget <= 0:\n                    break\n\n                # Perturb towards the best solution\n                perturbation = self.step_size * (self.x_opt - population[i])\n                trial = population[i] + perturbation\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                f = func(trial)\n                self.budget -= 1\n\n                if f < fitness[i]:\n                    fitness[i] = f\n                    population[i] = trial.copy()\n                    successes += 1\n\n                    # Update best solution\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = trial.copy()\n            \n            success_rate = successes / iterations if iterations > 0 else 0\n            \n            # Adjust step size based on success rate\n            if success_rate > self.success_rate_threshold:\n                self.step_size *= 1.05  # Increase step size\n            else:\n                self.step_size *= 0.95   # Decrease step size\n            \n            self.step_size = np.clip(self.step_size, 0.001, 1.0) # Prevent step size from going too low or high\n            \n            if iterations > 1000:\n                break\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm PerturbationBasedOptimization scored 0.259 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.10119898898332846, 0.17978883103958432, 0.2780388279284919, 0.13673590003073166, 0.17055135123028153, 0.16190575583061562, 0.2542605150772571, 0.17732136244511754, 0.1745807580192793, 0.15981005643561907, 0.22002534444641453, 0.9891565241556249, 0.23864916690811333, 0.16718188663635558, 0.5519918130677175, 0.2570473236553399, 0.19633668640663693, 0.16111480220707874, 0.16340727987862935, 0.4416373302846338]}, "task_prompt": ""}
{"id": "2bb9c133-5708-4726-902e-eb90b80b9b4f", "fitness": 0.5429299212249237, "name": "PSO_SA", "description": "No description provided.", "code": "import numpy as np\n\nclass PSO_SA:\n    def __init__(self, budget=10000, dim=10, pop_size=30, w=0.7, c1=1.5, c2=1.5, initial_temp=100, cooling_rate=0.95):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.w = w  # Inertia weight\n        self.c1 = c1  # Cognitive coefficient\n        self.c2 = c2  # Social coefficient\n        self.initial_temp = initial_temp\n        self.cooling_rate = cooling_rate\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        \n        # Initialize population and velocities\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        velocities = np.random.uniform(-1, 1, size=(self.pop_size, self.dim))\n        \n        # Initialize personal best positions and values\n        personal_best_positions = population.copy()\n        personal_best_values = np.array([func(x) for x in population])\n        \n        # Find global best\n        global_best_index = np.argmin(personal_best_values)\n        global_best_position = personal_best_positions[global_best_index].copy()\n        global_best_value = personal_best_values[global_best_index]\n\n        self.f_opt = global_best_value\n        self.x_opt = global_best_position\n        \n        eval_count = self.pop_size\n        temperature = self.initial_temp\n\n        while eval_count < self.budget:\n            for i in range(self.pop_size):\n                # Update velocity\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                velocities[i] = self.w * velocities[i] + \\\n                              self.c1 * r1 * (personal_best_positions[i] - population[i]) + \\\n                              self.c2 * r2 * (global_best_position - population[i])\n                \n                # Update position\n                new_position = population[i] + velocities[i]\n                new_position = np.clip(new_position, func.bounds.lb, func.bounds.ub)\n                \n                # Evaluate new position\n                new_value = func(new_position)\n                eval_count += 1\n\n                # Simulated Annealing acceptance criterion\n                delta_e = new_value - personal_best_values[i]\n                if delta_e < 0 or np.random.rand() < np.exp(-delta_e / temperature):\n                    population[i] = new_position.copy()\n                    if new_value < personal_best_values[i]:\n                        personal_best_values[i] = new_value\n                        personal_best_positions[i] = new_position.copy()\n                        \n                        if new_value < global_best_value:\n                            global_best_value = new_value\n                            global_best_position = new_position.copy()\n\n                            self.f_opt = global_best_value\n                            self.x_opt = global_best_position\n\n                if eval_count >= self.budget:\n                    break\n            \n            #Cooling\n            temperature *= self.cooling_rate\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm PSO_SA scored 0.543 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.15104562688845347, 0.3482167957679194, 0.811634590818725, 0.22882141259084987, 0.8641440164552485, 0.866620597119184, 0.3325524709890295, 0.7283077922715795, 0.8189161522418705, 0.2557049586839758, 0.8125171486004362, 0.9967204929472089, 0.22868085698793061, 0.26221036850322266, 0.7438599222556148, 0.8577516863543662, 0.5109468367752326, 0.38060215076069503, 0.14075937620391032, 0.5185851712830216]}, "task_prompt": ""}
{"id": "a7b03b17-bfee-4d56-aff2-3dd471bd74ec", "fitness": 0.0, "name": "AdaptivePSO", "description": "No description provided.", "code": "import numpy as np\n\nclass AdaptivePSO:\n    def __init__(self, budget=10000, dim=10, swarm_size=30, inertia=0.7, cognitive_coeff=1.4, social_coeff=1.4, local_search_prob=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = swarm_size\n        self.inertia = inertia\n        self.cognitive_coeff = cognitive_coeff\n        self.social_coeff = social_coeff\n        self.local_search_prob = local_search_prob\n        self.lb = -5.0\n        self.ub = 5.0\n\n    def __call__(self, func):\n        # Initialize particles and velocities\n        self.particles = np.random.uniform(self.lb, self.ub, size=(self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, size=(self.swarm_size, self.dim))  # Initialize velocities\n\n        # Evaluate initial fitness\n        self.fitness = np.array([func(x) for x in self.particles])\n        self.pbest_fitness = self.fitness.copy()\n        self.pbest_positions = self.particles.copy()\n\n        # Find global best\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.particles[np.argmin(self.fitness)]\n        self.gbest_position = self.x_opt.copy()\n\n        eval_count = self.swarm_size\n\n        while eval_count < self.budget:\n            # Update inertia weight (linearly decreasing)\n            inertia = self.inertia - (self.inertia - 0.4) * (eval_count / self.budget)\n\n            for i in range(self.swarm_size):\n                # Update velocity\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n\n                cognitive_component = self.cognitive_coeff * r1 * (self.pbest_positions[i] - self.particles[i])\n                social_component = self.social_coeff * r2 * (self.gbest_position - self.particles[i])\n\n                self.velocities[i] = inertia * self.velocities[i] + cognitive_component + social_component\n\n                # Update position\n                self.particles[i] = self.particles[i] + self.velocities[i]\n                self.particles[i] = np.clip(self.particles[i], self.lb, self.ub)\n\n                # Local Search\n                if np.random.rand() < self.local_search_prob:\n                    # Randomly select a dimension for local search\n                    dim_index = np.random.randint(0, self.dim)\n                    # Small random perturbation\n                    perturbation = np.random.uniform(-0.1, 0.1)\n                    self.particles[i, dim_index] = np.clip(self.particles[i, dim_index] + perturbation, self.lb, self.ub)\n\n                # Evaluate fitness\n                f = func(self.particles[i])\n                eval_count += 1\n\n                # Update personal best\n                if f < self.pbest_fitness[i]:\n                    self.pbest_fitness[i] = f\n                    self.pbest_positions[i] = self.particles[i].copy()\n\n                    # Update global best\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = self.particles[i].copy()\n                        self.gbest_position = self.x_opt.copy()\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptivePSO scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0]}, "task_prompt": ""}
{"id": "10f09204-7e0b-4eb8-b125-149f5588d4ca", "fitness": "-inf", "name": "DynDE_CMA", "description": "No description provided.", "code": "import numpy as np\n\nclass DynDE_CMA:\n    def __init__(self, budget=10000, dim=10, initial_pop_size=20, restart_trigger=0.2):\n        self.budget = budget\n        self.dim = dim\n        self.lb = -5.0\n        self.ub = 5.0\n        self.pop_size = initial_pop_size\n        self.population = None\n        self.fitness = None\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.evals = 0\n        self.restart_trigger = restart_trigger\n        self.stagnation_counter = 0\n        self.max_stagnation = budget // 100\n\n        self.C = np.eye(dim)\n        self.mu = 0\n        self.step_size = 1.0\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals += self.pop_size\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n        self.mu = self.population[np.argmin(self.fitness)]\n\n    def mutate(self):\n        z = np.random.normal(0, 1, size=(self.pop_size, self.dim))\n        return self.mu + self.step_size * np.dot(z, np.linalg.cholesky(self.C).T)\n\n    def crossover(self, mutant, target):\n        cross_points = np.random.rand(self.pop_size, self.dim) < 0.9\n        repair = np.random.randint(0, self.dim, self.pop_size)\n        for i in range(self.pop_size):\n            cross_points[i, repair[i]] = True\n        return np.where(cross_points, mutant, target)\n\n    def update_parameters(self):\n        best_idx = np.argmin(self.fitness)\n        delta = self.population[best_idx] - self.mu\n        self.mu = self.population[best_idx]\n\n        self.C = (1 - 0.1) * self.C + 0.1 * np.outer(delta / self.step_size, delta / self.step_size)\n        self.step_size *= np.exp(0.1 * (np.linalg.norm(delta) / self.dim - 1))\n        self.step_size = np.clip(self.step_size, 0.1, 5.0)\n        \n        if self.fitness[best_idx] < self.f_opt:\n            self.f_opt = self.fitness[best_idx]\n            self.x_opt = self.population[best_idx]\n            self.stagnation_counter = 0\n        else:\n            self.stagnation_counter += 1\n\n        if self.stagnation_counter > self.max_stagnation:\n            self.reset()\n\n    def reset(self):\n        self.C = np.eye(self.dim)\n        self.step_size = 1.0\n        self.stagnation_counter = 0\n        self.pop_size = int(self.pop_size * (1 - self.restart_trigger))\n        self.pop_size = max(10, self.pop_size)\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        while self.evals < self.budget:\n            mutant = self.mutate()\n            mutant = np.clip(mutant, self.lb, self.ub)\n            trial = self.crossover(mutant, self.population)\n\n            new_fitness = np.array([func(x) for x in trial])\n            self.evals += self.pop_size\n\n            improved = new_fitness < self.fitness\n            self.population[improved] = trial[improved]\n            self.fitness[improved] = new_fitness[improved]\n\n            self.update_parameters()\n\n            if self.evals >= self.budget:\n                break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "In the code, line 40, in crossover, the following error occurred:\nValueError: operands could not be broadcast together with shapes (16,2) (16,2) (20,2) \nOn line: return np.where(cross_points, mutant, target)", "error": "In the code, line 40, in crossover, the following error occurred:\nValueError: operands could not be broadcast together with shapes (16,2) (16,2) (20,2) \nOn line: return np.where(cross_points, mutant, target)", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "20885ab4-c43d-46a4-b886-f2241f98dfae", "fitness": 0.4531806836754688, "name": "AdaptivePSO_LS", "description": "No description provided.", "code": "import numpy as np\n\nclass AdaptivePSO_LS:\n    def __init__(self, budget=10000, dim=10, pop_size=20, w=0.7, c1=1.5, c2=1.5, ls_frequency=500, stagnation_threshold=500, ls_radius=0.5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.w = w  # Inertia weight\n        self.c1 = c1  # Cognitive coefficient\n        self.c2 = c2  # Social coefficient\n        self.ls_frequency = ls_frequency\n        self.stagnation_threshold = stagnation_threshold\n        self.stagnation_counter = 0\n        self.ls_radius = ls_radius\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def local_search(self, func, x_current):\n        x_new = x_current.copy()\n        for i in range(self.dim):\n            delta = np.random.uniform(-self.ls_radius, self.ls_radius)\n            x_new[i] = np.clip(x_current[i] + delta, func.bounds.lb, func.bounds.ub)\n        f_new = func(x_new)\n        self.budget -= 1\n        return f_new, x_new\n    \n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        # Initialize population (particles) and velocities\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        velocities = np.random.uniform(-1, 1, size=(self.pop_size, self.dim))\n        \n        # Initialize personal best positions and fitness values\n        pbest_positions = population.copy()\n        pbest_fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        # Initialize global best position and fitness value\n        gbest_index = np.argmin(pbest_fitness)\n        gbest_position = pbest_positions[gbest_index].copy()\n        self.f_opt = pbest_fitness[gbest_index]\n        self.x_opt = gbest_position.copy()\n        \n        generation = 0\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Update velocities\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                velocities[i] = self.w * velocities[i] + \\\n                                self.c1 * r1 * (pbest_positions[i] - population[i]) + \\\n                                self.c2 * r2 * (gbest_position - population[i])\n                \n                # Update positions\n                population[i] = np.clip(population[i] + velocities[i], func.bounds.lb, func.bounds.ub)\n                \n                # Evaluate fitness\n                fitness = func(population[i])\n                self.budget -= 1\n                \n                # Update personal best\n                if fitness < pbest_fitness[i]:\n                    pbest_fitness[i] = fitness\n                    pbest_positions[i] = population[i].copy()\n                    \n                    # Update global best\n                    if fitness < self.f_opt:\n                        self.f_opt = fitness\n                        self.x_opt = population[i].copy()\n                        gbest_position = population[i].copy()\n                        self.stagnation_counter = 0\n                    else:\n                        self.stagnation_counter += 1\n\n            # Local Search application\n            if generation > 0 and generation % self.ls_frequency == 0:\n                 for i in range(self.pop_size):\n                    if self.budget > 0:\n                        new_f, new_x = self.local_search(func, population[i])\n                        if new_f < pbest_fitness[i]:\n                            pbest_fitness[i] = new_f\n                            pbest_positions[i] = new_x.copy()\n\n                            if new_f < self.f_opt:\n                                self.f_opt = new_f\n                                self.x_opt = new_x.copy()\n                                gbest_position = new_x.copy()\n                                self.stagnation_counter = 0\n                        else:\n                            self.stagnation_counter +=1\n\n            if self.stagnation_counter > self.stagnation_threshold:\n                # Restart PSO if stagnated\n                population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                velocities = np.random.uniform(-1, 1, size=(self.pop_size, self.dim))\n                pbest_positions = population.copy()\n                pbest_fitness = np.array([func(x) for x in population])\n                self.budget -= self.pop_size\n                gbest_index = np.argmin(pbest_fitness)\n                gbest_position = pbest_positions[gbest_index].copy()\n                self.f_opt = pbest_fitness[gbest_index]\n                self.x_opt = gbest_position.copy()\n                self.stagnation_counter = 0 #Reset Stagnation Counter\n\n            generation += 1\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptivePSO_LS scored 0.453 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.20887671446213418, 0.3088214715224672, 0.854815849905443, 0.20583869054673165, 0.27124314800453253, 0.8994885133345698, 0.3288025039535615, 0.2232023188672847, 0.32559209351491003, 0.282640714638442, 0.9399640166958464, 0.9980724077608708, 0.2569154703844151, 0.2892311258720084, 0.7350554626615609, 0.390331424237645, 0.4051360607226484, 0.3814316224940004, 0.24659715692242257, 0.51155690700788]}, "task_prompt": ""}
{"id": "690e5bf3-8c8c-4f37-addf-b0633eaffc81", "fitness": 0.0, "name": "AdaptiveDEOL", "description": "No description provided.", "code": "import numpy as np\n\nclass AdaptiveDEOL:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, CR=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.lb = -5.0\n        self.ub = 5.0\n\n    def orthogonal_learning(self, func, x, eval_count, num_samples=5):\n        best_f = func(x)\n        eval_count += 1\n        best_x = x.copy()\n        for _ in range(num_samples):\n            x_new = x + np.random.normal(0, 0.1, size=self.dim)  # Small perturbation\n            x_new = np.clip(x_new, self.lb, self.ub)\n            f_new = func(x_new)\n            eval_count += 1\n            if f_new < best_f:\n                best_f = f_new\n                best_x = x_new.copy()\n        return best_f, best_x, eval_count\n\n    def __call__(self, func):\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n        eval_count = self.pop_size\n\n        while eval_count < self.budget:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + self.F * (b - c), self.lb, self.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, self.population[i])\n\n                # Selection and Orthogonal Learning\n                f, trial, eval_count = self.orthogonal_learning(func, trial, eval_count)\n\n                if f < self.fitness[i]:\n                    self.fitness[i] = f\n                    self.population[i] = trial\n\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = trial\n            \n            # Adaptive F and CR\n            self.F = np.clip(np.random.normal(0.5, 0.1), 0.1, 1.0)\n            self.CR = np.clip(np.random.normal(0.9, 0.1), 0.1, 1.0)\n\n            #Population Size Reduction\n            if eval_count > self.budget * 0.75 and self.pop_size > 10:\n               worst_index = np.argmax(self.fitness)\n               self.population = np.delete(self.population, worst_index, axis = 0)\n               self.fitness = np.delete(self.fitness, worst_index)\n               self.pop_size -= 1\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveDEOL scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0]}, "task_prompt": ""}
{"id": "a9d5e038-d920-4730-a443-467cf5fe647b", "fitness": 0.5462768022741944, "name": "SelfAdaptiveDifferentialEvolution", "description": "No description provided.", "code": "import numpy as np\n\nclass SelfAdaptiveDifferentialEvolution:\n    def __init__(self, budget=10000, dim=10, initial_pop_size=50, F=0.5, CR=0.7, reduction_factor=0.5, restart_trigger=100):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = initial_pop_size\n        self.initial_pop_size = initial_pop_size\n        self.F = F\n        self.CR = CR\n        self.reduction_factor = reduction_factor\n        self.restart_trigger = restart_trigger\n        self.no_improvement_count = 0\n        self.best_fitness_history = []\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index].copy()\n        self.best_fitness_history.append(self.f_opt)\n\n        while self.budget > 0:\n            new_population = np.zeros((self.pop_size, self.dim))\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = np.random.choice(idxs, 3, replace=False)\n                mutant = population[a] + self.F * (population[b] - population[c])\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                new_population[i] = trial\n\n            new_fitness = np.array([func(x) for x in new_population])\n            self.budget -= self.pop_size\n\n            # Selection\n            for i in range(self.pop_size):\n                if new_fitness[i] < fitness[i]:\n                    fitness[i] = new_fitness[i]\n                    population[i] = new_population[i].copy()\n\n            # Update best solution\n            best_index = np.argmin(fitness)\n            if fitness[best_index] < self.f_opt:\n                self.f_opt = fitness[best_index]\n                self.x_opt = population[best_index].copy()\n                self.no_improvement_count = 0\n            else:\n                self.no_improvement_count += 1\n            \n            self.best_fitness_history.append(self.f_opt)\n\n            # Adapt population size\n            if self.no_improvement_count > self.restart_trigger and self.pop_size > 10:\n                self.pop_size = int(self.pop_size * self.reduction_factor)\n                self.pop_size = max(10, self.pop_size)  # Ensure a minimum population size\n                population = population[np.argsort(fitness)[:self.pop_size]]\n                fitness = fitness[np.argsort(fitness)[:self.pop_size]]\n                self.no_improvement_count = 0\n\n            # Restart mechanism\n            if self.no_improvement_count > 2 * self.restart_trigger:\n                population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                fitness = np.array([func(x) for x in population])\n                self.budget -= self.pop_size\n                best_index = np.argmin(fitness)\n                if fitness[best_index] < self.f_opt:\n                    self.f_opt = fitness[best_index]\n                    self.x_opt = population[best_index].copy()\n                self.no_improvement_count = 0\n            # Adapt parameters (simple adaptation - can be improved)\n            self.F = np.clip(self.F + np.random.normal(0, 0.01), 0.1, 0.9)\n            self.CR = np.clip(self.CR + np.random.normal(0, 0.01), 0.1, 0.9)\n            if self.budget <= 0:\n                break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm SelfAdaptiveDifferentialEvolution scored 0.546 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.2243182982387476, 0.4409016254635557, 0.5768480541154499, 0.8095229330001388, 0.6557561953480877, 0.7467840319757537, 0.49881124056207815, 0.5380679388426772, 0.5929732886303771, 0.6447063751119333, 0.8266316460015339, 0]}, "task_prompt": ""}
{"id": "74360c6a-97ce-4541-890c-fb3aef2f37c4", "fitness": "-inf", "name": "RestartCMAES", "description": "No description provided.", "code": "import numpy as np\nimport cma\n\nclass RestartCMAES:\n    def __init__(self, budget=10000, dim=10, sigma=0.5, popsize=None, inc_popsize=2, restart_trigger=1e-9):\n        self.budget = budget\n        self.dim = dim\n        self.sigma = sigma\n        self.popsize = popsize if popsize else 4 + int(3 * np.log(dim))\n        self.inc_popsize = inc_popsize\n        self.restart_trigger = restart_trigger\n        self.lb = -5.0\n        self.ub = 5.0\n        self.x_opt = None\n        self.f_opt = np.inf\n        self.evals = 0\n\n    def __call__(self, func):\n        x0 = np.random.uniform(self.lb, self.ub, size=self.dim)\n        es = cma.PureCMAES(x0, self.sigma, {\n            'bounds': [self.lb, self.ub],\n            'popsize': self.popsize,\n            'max_evaluations': self.budget,\n            'verbose': -9,\n        })\n\n        while self.evals < self.budget and not es.stop():\n            solutions = []\n            for i in range(es.popsize):\n                if self.evals >= self.budget:\n                    break\n                x = es.ask()\n                f = func(x)\n                self.evals += 1\n                solutions.append((x, f))\n                if f < self.f_opt:\n                    self.f_opt = f\n                    self.x_opt = x\n\n            es.tell([s[0] for s in solutions], [s[1] for s in solutions])\n            es.disp()\n\n            if es.result.fbest - self.f_opt > self.restart_trigger:\n                x0 = np.random.uniform(self.lb, self.ub, size=self.dim)\n                self.popsize *= self.inc_popsize\n                self.popsize = min(self.popsize, self.budget//2)\n\n                es = cma.PureCMAES(x0, self.sigma, {\n                    'bounds': [self.lb, self.ub],\n                    'popsize': self.popsize,\n                    'max_evaluations': self.budget - self.evals,\n                    'verbose': -9,\n                })\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "In the code, line 2, in <module>, the following error occurred:\nModuleNotFoundError: No module named 'cma'\nOn line: import cma", "error": "In the code, line 2, in <module>, the following error occurred:\nModuleNotFoundError: No module named 'cma'\nOn line: import cma", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "d60067fe-5201-4783-b0b5-8657941af240", "fitness": 0.26981164812920866, "name": "BudgetCMAES", "description": "No description provided.", "code": "import numpy as np\n\nclass BudgetCMAES:\n    def __init__(self, budget=10000, dim=10, initial_pop_size=None, cs=0.3, damps=1.0):\n        self.budget = budget\n        self.dim = dim\n        self.mu = int(np.floor(self.dim / 2 + 1)) # Number of parents/selected points\n        if initial_pop_size is None:\n             self.lambda_ = int(4 + np.floor(3 * np.log(self.dim))) # Pop size\n        else:\n            self.lambda_ = initial_pop_size\n        self.C = np.eye(self.dim) # Covariance matrix\n        self.m = np.zeros(self.dim) # Mean\n        self.sigma = 0.5  # Step size\n        self.cs = cs # Learning rate for the step size\n        self.damps = damps # Dampening for the step size\n        self.pc = np.zeros(self.dim) # Evolution path for C\n        self.B = None # Eigenvectors of C\n        self.D = None # Eigenvalues of C\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.gen = 0\n\n    def __call__(self, func):\n        while self.budget > 0:\n            # Generate lambda offspring\n            z = np.random.multivariate_normal(np.zeros(self.dim), np.eye(self.dim), size=self.lambda_)\n            x = self.m + self.sigma * z\n            x = np.clip(x, func.bounds.lb, func.bounds.ub)\n            \n            # Evaluate offspring\n            fitness = np.array([func(xi) for xi in x])\n            self.budget -= self.lambda_\n            \n            # Sort by fitness\n            idx = np.argsort(fitness)\n            fitness = fitness[idx]\n            x = x[idx]\n\n            # Update best solution\n            if fitness[0] < self.f_opt:\n                self.f_opt = fitness[0]\n                self.x_opt = x[0].copy()\n\n            # Select parents\n            x_m = x[:self.mu]\n\n            # Update mean\n            m_old = self.m.copy()\n            self.m = np.mean(x_m, axis=0)\n\n            # Update evolution path and step size\n            self.pc = (1 - self.cs) * self.pc + np.sqrt(self.cs * (2 - self.cs)) * (self.m - m_old) / self.sigma\n            self.sigma *= np.exp((self.cs / self.damps) * (np.linalg.norm(self.pc)**2 / self.dim - 1))\n\n            self.gen += 1\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm BudgetCMAES scored 0.270 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.06349176923477207, 0.178887180081318, 0.4413727404533262, 0.15954901485463346, 0.16607927903595954, 0.14890528743792275, 0.2255377551668769, 0.18858483353652988, 0.1417530066146928, 0.1356614951582933, 0.1681620907495459, 0.1913872415949064, 0.2549116902301315, 0.17849487562745048, 0.669801465138502, 0.25751684198907465, 0.2854990367106478, 0.9415493849606593, 0.13905778731573593, 0.46003018669319506]}, "task_prompt": ""}
{"id": "4930784c-34ba-4472-adf9-62fb785d71df", "fitness": "-inf", "name": "CMAES", "description": "No description provided.", "code": "import numpy as np\n\nclass CMAES:\n    def __init__(self, budget=10000, dim=10, pop_size=None, initial_step_size=1.0, restarts=3):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size if pop_size is not None else 4 + int(3 * np.log(dim))\n        self.initial_step_size = initial_step_size\n        self.restarts = restarts\n        self.lb = -5.0\n        self.ub = 5.0\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        evals = 0\n        \n        for restart in range(self.restarts):\n            mean = np.random.uniform(self.lb, self.ub, size=self.dim)\n            sigma = self.initial_step_size\n            C = np.eye(self.dim)\n            \n            mu = self.pop_size // 2\n\n            c_m = 1 / mu\n            c_sigma = (mu + 2) / (self.dim + mu + 5)\n            d_sigma = 1 + 2 * max(0, np.sqrt((mu - 1) / (self.dim + 1)) - 1) + c_sigma\n            c_c = (4 + mu / self.dim) / (self.dim + 4 + 2 * mu / self.dim)\n            c_1 = 2 / ((self.dim + 1.3)**2 + mu)\n            c_mu = min(1 - c_1, 2 * (mu - 2 + 1 / mu) / ((self.dim + 2)**2 + 2 * mu))\n            \n            P_sigma = np.zeros(self.dim)\n            P_c = np.zeros(self.dim)\n            B = None\n            D = None\n            eigen_updated = 0\n\n            while evals < self.budget:\n                z = np.random.multivariate_normal(np.zeros(self.dim), np.eye(self.dim), size=self.pop_size)\n                x = mean + sigma * z @ np.linalg.cholesky(C).T\n\n                # Clipping\n                x = np.clip(x, self.lb, self.ub)\n                \n                f = np.array([func(xi) for xi in x])\n                evals += self.pop_size\n\n                if np.min(f) < self.f_opt:\n                    self.f_opt = np.min(f)\n                    self.x_opt = x[np.argmin(f)].copy()\n\n                idx = np.argsort(f)\n                x_mu = x[idx[:mu]]\n\n                mean_old = mean.copy()\n                mean = np.mean(x_mu, axis=0)\n\n                z_mu = (x_mu - mean_old) / sigma\n                \n                P_sigma = (1 - c_sigma) * P_sigma + np.sqrt(c_sigma * (2 - c_sigma)) * (B @ D @ np.mean(z_mu, axis=0))\n                \n                hsig = np.linalg.norm(P_sigma) / np.sqrt(1 - (1 - c_sigma)**(evals/self.pop_size)) / 1.4 > 2 + 4/(self.dim+1)\n                P_c = (1-c_c) * P_c + hsig * np.sqrt(c_c * (2 - c_c)) * (mean - mean_old) / sigma\n\n                C = (1 - c_1 - c_mu + c_1 * (c_c * (2 - c_c)) * np.sum(P_c**2) ) * C \\\n                  + c_1 * np.outer(P_c, P_c) \\\n                  + c_mu * np.mean(np.array([np.outer(z_mu[i], z_mu[i]) for i in range(mu)]), axis=0)\n\n                sigma *= np.exp((c_sigma / d_sigma) * (np.linalg.norm(P_sigma) / np.sqrt(self.dim) - 1))\n                sigma = min(sigma, abs(self.ub - self.lb))\n                \n                if evals - eigen_updated > self.pop_size / c_1 / self.dim / 10:\n                    eigen_updated = evals\n                    C = np.triu(C) + np.triu(C, 1).T\n                    D, B = np.linalg.eigh(C)\n                    D = np.sqrt(np.maximum(D, 1e-16))\n        \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "In the code, line 60, in __call__, the following error occurred:\nTypeError: unsupported operand type(s) for @: 'NoneType' and 'NoneType'\nOn line: P_sigma = (1 - c_sigma) * P_sigma + np.sqrt(c_sigma * (2 - c_sigma)) * (B @ D @ np.mean(z_mu, axis=0))", "error": "In the code, line 60, in __call__, the following error occurred:\nTypeError: unsupported operand type(s) for @: 'NoneType' and 'NoneType'\nOn line: P_sigma = (1 - c_sigma) * P_sigma + np.sqrt(c_sigma * (2 - c_sigma)) * (B @ D @ np.mean(z_mu, axis=0))", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "665da1c8-4413-4fd6-8390-77bded5e1a2b", "fitness": 0.11376257863359755, "name": "CMAES", "description": "No description provided.", "code": "import numpy as np\n\nclass CMAES:\n    def __init__(self, budget=10000, dim=10, initial_sigma=0.3, pop_size=None):\n        self.budget = budget\n        self.dim = dim\n        self.initial_sigma = initial_sigma\n        self.lb = -5.0\n        self.ub = 5.0\n        if pop_size is None:\n            self.pop_size = 4 + int(3 * np.log(self.dim))  # Default CMA-ES pop size\n        else:\n            self.pop_size = pop_size\n        self.mu = self.pop_size // 2\n\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n\n        self.m = np.random.uniform(self.lb, self.ub, size=self.dim)\n        self.sigma = self.initial_sigma\n        self.C = np.eye(self.dim)\n\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n\n        self.chiN = self.dim**0.5 * (1 - 1/(4*self.dim) + 1/(21*self.dim**2))\n\n        self.c_sigma = (self.mu / (self.dim + (np.sqrt(2) * self.mu / (self.dim + 1))**2)) if self.dim > 1 else 1\n        self.d_sigma = 1 + 2 * max(0, np.sqrt((self.mu - 1)/(self.dim + 1)) - 1) + self.c_sigma\n        self.c_c = (self.mu / (self.dim + 2)) if self.dim > 1 else 1\n        self.c_1 = 2 / ((self.dim + 1.3)**2 + self.mu)\n        self.c_mu = min(1 - self.c_1, 2 * (self.mu - 1 + 1/self.mu) / ((self.dim + 2.3)**2 + self.mu))\n\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.restart_iter = 0\n\n    def __call__(self, func):\n        while self.eval_count < self.budget:\n            # Sample population\n            z = np.random.normal(0, 1, size=(self.pop_size, self.dim))\n            D, B = np.linalg.eig(self.C)\n            D = np.diag(np.sqrt(D))\n            x = self.m + self.sigma * z @ B @ D\n            x = np.clip(x, self.lb, self.ub)\n\n            # Evaluate population\n            fitness = np.array([func(xi) for xi in x])\n            self.eval_count += self.pop_size\n\n            # Sort by fitness\n            idx = np.argsort(fitness)\n            fitness = fitness[idx]\n            x = x[idx]\n\n            # Update optimal solution\n            if fitness[0] < self.f_opt:\n                self.f_opt = fitness[0]\n                self.x_opt = x[0]\n\n            # Update CMA-ES parameters\n            xmean = np.sum(x[:self.mu].T * self.weights, axis=1)\n            zmean = np.sum(z[idx[:self.mu]].T * self.weights, axis=1)\n\n            self.ps = (1 - self.c_sigma) * self.ps + np.sqrt(self.c_sigma * (2 - self.c_sigma)) * zmean\n            self.pc = (1 - self.c_c) * self.pc + np.sqrt(self.c_c * (2 - self.c_c)) * (xmean - self.m) / self.sigma\n\n            hsig = (np.linalg.norm(self.ps) / np.sqrt(1 - (1 - self.c_sigma)**(2 * (self.eval_count / self.pop_size))) / self.chiN < 1.4 + 2/(self.dim + 1))\n\n            self.m = xmean\n\n            dC = (self.c_1 * (self.pc[:, None] @ self.pc[None, :])) + (self.c_mu * (x[:self.mu] - self.m).T @ np.diag(self.weights) @ (x[:self.mu] - self.m)) / self.sigma**2\n\n            self.C = (1 - self.c_1 - self.c_mu) * self.C + dC\n\n            self.sigma = self.sigma * np.exp((self.c_sigma / self.d_sigma) * (np.linalg.norm(self.ps) / self.chiN - 1))\n            self.sigma = min(self.sigma, (self.ub-self.lb)/3)\n            \n            if np.min(np.diag(self.C)) < 1e-16:\n                self.C += 1e-16 * np.eye(self.dim)\n\n            if np.isinf(self.f_opt) or np.isnan(self.f_opt):\n                self.restart_iter += 1\n                self.m = np.random.uniform(self.lb, self.ub, size=self.dim)\n                self.sigma = self.initial_sigma\n                self.C = np.eye(self.dim)\n                self.pc = np.zeros(self.dim)\n                self.ps = np.zeros(self.dim)\n                \n            # Budget-aware population size adaptation (optional)\n            remaining_evals = self.budget - self.eval_count\n            if remaining_evals < self.pop_size and self.pop_size > 2:\n                self.pop_size = max(2, remaining_evals // 2)  # Reduce pop size towards the end\n                self.mu = self.pop_size // 2\n                self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n                self.weights = self.weights / np.sum(self.weights)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm CMAES scored 0.114 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [9.999999999998899e-05, 0.04515097463611184, 0.2336071323378851, 0.121249260320617, 0.052303836733547326, 0.13016110454688545, 0.14387668453588998, 0.05381312923530346, 0.15193314866826801, 0.13164602329757125, 0.1358081751721244, 0.13821834839359726, 0.006446937759001958, 0.1426132824988363, 0.12112381518064141, 0.196524064713258, 0.09147705831440545, 0.15704907125706646, 0.09269687310631014, 0.12945265196462985]}, "task_prompt": ""}
{"id": "051565e8-6850-4c7d-a12e-47070e91687a", "fitness": "-inf", "name": "GaussianMixtureOptimization", "description": "No description provided.", "code": "import numpy as np\nfrom sklearn.mixture import GaussianMixture\n\nclass GaussianMixtureOptimization:\n    def __init__(self, budget=10000, dim=10, pop_size=50, n_components=5, adaptation_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.n_components = n_components\n        self.adaptation_rate = adaptation_rate\n        self.lb = -5.0\n        self.ub = 5.0\n        self.population = None\n        self.fitness = None\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.evals = 0\n        self.gmm = None\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.pop_size\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n\n    def update_gmm(self):\n        if self.evals > self.pop_size: # Ensure sufficient data points to train GMM.\n            # Select top individuals to fit the GMM\n            top_indices = np.argsort(self.fitness)[:self.pop_size // 2]\n            top_individuals = self.population[top_indices]\n\n            self.gmm = GaussianMixture(n_components=self.n_components, covariance_type='full', max_iter=100, random_state=42)\n            try:\n                self.gmm.fit(top_individuals)\n            except ValueError as e:\n                print(f\"GMM fit failed: {e}. Resetting GMM.\")\n                self.gmm = None # Reset gmm if fit fails\n            except Exception as e:\n                print(f\"An unexpected error occurred during GMM fitting: {e}\")\n                self.gmm = None\n\n    def sample_new_population(self, func):\n        new_population = []\n        new_fitness = []\n\n        if self.gmm is None: # Handle the case where GMM fitting failed or hasn't been trained yet.\n            for _ in range(self.pop_size):\n                if self.evals >= self.budget:\n                    break\n                x = np.random.uniform(self.lb, self.ub, size=self.dim)\n                f = func(x)\n                self.evals += 1\n                new_population.append(x)\n                new_fitness.append(f)\n        else:\n            num_samples_needed = self.pop_size\n            while num_samples_needed > 0 and self.evals < self.budget:\n                try:\n                    samples = self.gmm.sample(num_samples_needed)[0]\n                except Exception as e:\n                    print(f\"Sampling from GMM failed: {e}. Resetting GMM.\")\n                    self.gmm = None  # Reset GMM if sampling fails\n                    return self.sample_new_population(func) # Recursive call to re-sample randomly\n\n                for x in samples:\n                    if self.evals >= self.budget:\n                        break\n                    x = np.clip(x, self.lb, self.ub)  # Clip to bounds\n                    f = func(x)\n                    self.evals += 1\n                    new_population.append(x)\n                    new_fitness.append(f)\n                    num_samples_needed -= 1 # Reduce needed sample count\n        return np.array(new_population), np.array(new_fitness)\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        while self.evals < self.budget:\n            self.update_gmm()\n            new_population, new_fitness = self.sample_new_population(func)\n\n            # Combine old and new populations (Elitism)\n            combined_population = np.concatenate((self.population, new_population)) if len(new_population) > 0 else self.population\n            combined_fitness = np.concatenate((self.fitness, new_fitness)) if len(new_fitness) > 0 else self.fitness\n\n            # Select the best individuals for the next generation\n            top_indices = np.argsort(combined_fitness)[:self.pop_size]\n            self.population = combined_population[top_indices]\n            self.fitness = combined_fitness[top_indices]\n\n            # Update the best solution found so far\n            best_index = np.argmin(self.fitness)\n            if self.fitness[best_index] < self.f_opt:\n                self.f_opt = self.fitness[best_index]\n                self.x_opt = self.population[best_index]\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "In the code, line 33, in update_gmm, the following error occurred:\nNameError: name 'GaussianMixture' is not defined\nOn line: self.gmm = GaussianMixture(n_components=self.n_components, covariance_type='full', max_iter=100, random_state=42)", "error": "In the code, line 33, in update_gmm, the following error occurred:\nNameError: name 'GaussianMixture' is not defined\nOn line: self.gmm = GaussianMixture(n_components=self.n_components, covariance_type='full', max_iter=100, random_state=42)", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "1f5c3510-d871-461e-93c2-f65b8634378e", "fitness": "-inf", "name": "CMAES", "description": "No description provided.", "code": "import numpy as np\n\nclass CMAES:\n    def __init__(self, budget=10000, dim=10, pop_size=None, sigma=0.5, cs=0.3, damps=None, c_cov_mu=None, c_cov_one=None):\n        self.budget = budget\n        self.dim = dim\n        self.sigma = sigma\n        if pop_size is None:\n            self.pop_size = 4 + int(3 * np.log(self.dim))\n        else:\n            self.pop_size = pop_size\n        self.mu = self.pop_size // 2\n        self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.m = None  # Mean vector\n        self.C = None  # Covariance matrix\n        self.pc = None # Evolution path for C\n        self.ps = None # Evolution path for sigma\n        self.chiN = None\n        self.cs = cs # Cumulation factor for sigma\n        if damps is None:\n            self.damps = 1 + 2 * max(0, np.sqrt((self.mu - 1) / (self.dim + 1)) - 1) + self.cs\n        else:\n            self.damps = damps\n        \n        self.c_cov_mu = c_cov_mu if c_cov_mu is not None else 2 / (self.dim**2 + 6)\n        self.c_cov_one = c_cov_one if c_cov_one is not None else 2 / ((self.dim + 1.3)**2 + self.mu)\n        self.B = None # Eigenvectors of C\n        self.D = None # Eigenvalues of C (sqrt)\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        \n        # Initialize mean and covariance matrix\n        self.m = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.chiN = np.sqrt(self.dim) * (1 - (1/(4*self.dim)) + 1/(21*self.dim**2))\n        \n        self.D, self.B = np.linalg.eig(self.C)\n        self.D = np.sqrt(self.D)\n\n        while self.budget > 0:\n            # Generate and evaluate offspring\n            z = np.random.randn(self.dim, self.pop_size)\n            y = self.B @ np.diag(self.D) @ z\n            x = self.m + self.sigma * y\n            x = np.clip(x, func.bounds.lb, func.bounds.ub)\n\n            fitness = np.array([func(xi) for xi in x.T])\n            self.budget -= self.pop_size\n\n            # Sort by fitness\n            indices = np.argsort(fitness)\n            fitness = fitness[indices]\n            x = x[:, indices]\n\n            # Update mean\n            m_old = self.m.copy()\n            self.m = np.sum(x[:, :self.mu] * self.weights[None, :], axis=1)\n\n            # Update evolution paths\n            y_mean = np.mean(y[:, :self.mu], axis=1)\n            self.ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs)) * (self.B @ y_mean)\n            \n            c_sig = (np.linalg.norm(self.ps)/self.chiN) < (1.4 + 2/(self.dim+1))\n            \n            self.pc = (1-self.c_cov_one) * self.pc + c_sig * np.sqrt(self.c_cov_one * (2-self.c_cov_one)) * (self.m - m_old) / self.sigma\n\n            # Update covariance matrix\n            artmp = (x[:, :self.mu] - m_old[:, None]) / self.sigma\n            self.C = (1 - self.c_cov_one - self.c_cov_mu) * self.C \\\n                     + self.c_cov_one * np.outer(self.pc, self.pc) \\\n                     + self.c_cov_mu * artmp @ np.diag(self.weights) @ artmp.T\n            \n            # Adapt step size\n            self.sigma *= np.exp((self.cs/self.damps) * (np.linalg.norm(self.ps)/self.chiN - 1))\n            \n            self.D, self.B = np.linalg.eig(self.C)\n            self.D = np.sqrt(self.D)\n\n            # Update best solution\n            if fitness[0] < self.f_opt:\n                self.f_opt = fitness[0]\n                self.x_opt = x[:, 0].copy()\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "In the code, line 49, in __call__, the following error occurred:\nValueError: operands could not be broadcast together with shapes (2,) (2,6) \nOn line: x = self.m + self.sigma * y", "error": "In the code, line 49, in __call__, the following error occurred:\nValueError: operands could not be broadcast together with shapes (2,) (2,6) \nOn line: x = self.m + self.sigma * y", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "8ba887e0-70cb-449c-8634-fb9ed80d8018", "fitness": 0.5614956538839935, "name": "CooperativeSwarm", "description": "No description provided.", "code": "import numpy as np\n\nclass CooperativeSwarm:\n    def __init__(self, budget=10000, dim=10, num_swarms=5, swarm_size=10):\n        self.budget = budget\n        self.dim = dim\n        self.num_swarms = num_swarms\n        self.swarm_size = swarm_size\n        self.lb = -5.0\n        self.ub = 5.0\n        self.swarms = []\n        self.fitness = []\n        self.velocities = []\n        self.pbest_positions = []\n        self.pbest_fitness = []\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.evals = 0\n        self.inertia = 0.7\n        self.cognitive_coeff = 1.5\n        self.social_coeff = 1.5\n        self.exploration_rate = 0.3\n        self.exploitation_rate = 0.7\n\n        for _ in range(num_swarms):\n            self.swarms.append(np.random.uniform(self.lb, self.ub, size=(swarm_size, dim)))\n            self.fitness.append(np.zeros(swarm_size))\n            self.velocities.append(np.random.uniform(-1, 1, size=(swarm_size, dim)))\n            self.pbest_positions.append(self.swarms[-1].copy())\n            self.pbest_fitness.append(np.full(swarm_size, np.inf))\n\n    def evaluate_swarm(self, func, swarm_index):\n        for i in range(self.swarm_size):\n            if self.evals >= self.budget:\n                return\n            self.fitness[swarm_index][i] = func(self.swarms[swarm_index][i])\n            self.evals += 1\n\n            if self.fitness[swarm_index][i] < self.pbest_fitness[swarm_index][i]:\n                self.pbest_fitness[swarm_index][i] = self.fitness[swarm_index][i]\n                self.pbest_positions[swarm_index][i] = self.swarms[swarm_index][i].copy()\n\n            if self.fitness[swarm_index][i] < self.f_opt:\n                self.f_opt = self.fitness[swarm_index][i]\n                self.x_opt = self.swarms[swarm_index][i].copy()\n\n    def update_velocity(self, swarm_index, gbest):\n        r1 = np.random.rand(self.swarm_size, self.dim)\n        r2 = np.random.rand(self.swarm_size, self.dim)\n        cognitive_component = self.cognitive_coeff * r1 * (self.pbest_positions[swarm_index] - self.swarms[swarm_index])\n        social_component = self.social_coeff * r2 * (gbest - self.swarms[swarm_index])\n        self.velocities[swarm_index] = self.inertia * self.velocities[swarm_index] + cognitive_component + social_component\n\n    def update_position(self, swarm_index):\n        self.swarms[swarm_index] = np.clip(self.swarms[swarm_index] + self.velocities[swarm_index], self.lb, self.ub)\n\n    def migrate_information(self):\n        best_swarm_index = np.argmin([np.min(fitness) for fitness in self.fitness])\n        worst_swarm_index = np.argmax([np.max(fitness) for fitness in self.fitness])\n\n        best_particle_index = np.argmin(self.fitness[best_swarm_index])\n        worst_particle_index = np.argmax(self.fitness[worst_swarm_index])\n\n        self.swarms[worst_swarm_index][worst_particle_index] = self.pbest_positions[best_swarm_index][best_particle_index].copy()\n        self.fitness[worst_swarm_index][worst_particle_index] = self.pbest_fitness[best_swarm_index][best_particle_index]\n\n    def __call__(self, func):\n        for i in range(self.num_swarms):\n            self.evaluate_swarm(func, i)\n\n        while self.evals < self.budget:\n            gbest_swarm_index = np.argmin([np.min(fitness) for fitness in self.fitness])\n            gbest = self.pbest_positions[gbest_swarm_index][np.argmin(self.pbest_fitness[gbest_swarm_index])]\n\n            for i in range(self.num_swarms):\n                self.update_velocity(i, gbest)\n                self.update_position(i)\n                self.evaluate_swarm(func, i)\n\n            if np.random.rand() < 0.1:\n                self.migrate_information()\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm CooperativeSwarm scored 0.561 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.20110207052568818, 0.30413116417644337, 0.6514209685708645, 0.8604289127091705, 0.6818585547474453, 0.7394983514688083, 0.3202535208956355, 0.6142215815169327, 0.6890107182984128, 0.5586789100670343, 0.8766354279835817, 0.9934292426534653, 0.2580751377965852, 0.29933328774693035, 0.7265387896401656, 0.74516261177397, 0.5630451167885806, 0.3779004279230187, 0.2568149397381547, 0.5123733426589827]}, "task_prompt": ""}
{"id": "21a06fc7-98ef-4e6f-9a31-f042dacdd585", "fitness": 0.0, "name": "PSO", "description": "No description provided.", "code": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass PSO:\n    def __init__(self, budget=10000, dim=10, pop_size=30, w=0.7, c1=1.5, c2=1.5, v_max=1.0):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.w = w  # Inertia weight\n        self.c1 = c1  # Cognitive coefficient\n        self.c2 = c2  # Social coefficient\n        self.v_max = v_max\n        self.lb = -5.0\n        self.ub = 5.0\n        self.particles = None\n        self.velocities = None\n        self.fitness = None\n        self.pbest_positions = None\n        self.pbest_fitness = None\n        self.gbest_position = None\n        self.gbest_fitness = np.inf\n        self.evals = 0\n        self.inertia_reduction_rate = 0.995\n        self.local_search_interval = budget // 15\n        self.local_search_fev = 25\n\n    def initialize_particles(self, func):\n        self.particles = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.velocities = np.random.uniform(-self.v_max, self.v_max, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.particles])\n        self.evals += self.pop_size\n\n        self.pbest_positions = self.particles.copy()\n        self.pbest_fitness = self.fitness.copy()\n        self.gbest_position = self.pbest_positions[np.argmin(self.pbest_fitness)].copy()\n        self.gbest_fitness = np.min(self.pbest_fitness)\n\n        self.f_opt = self.gbest_fitness\n        self.x_opt = self.gbest_position\n\n    def update_velocities(self):\n        r1 = np.random.rand(self.pop_size, self.dim)\n        r2 = np.random.rand(self.pop_size, self.dim)\n\n        cognitive_component = self.c1 * r1 * (self.pbest_positions - self.particles)\n        social_component = self.c2 * r2 * (self.gbest_position - self.particles)\n\n        self.velocities = self.w * self.velocities + cognitive_component + social_component\n        self.velocities = np.clip(self.velocities, -self.v_max, self.v_max)\n\n    def update_positions(self):\n        self.particles = self.particles + self.velocities\n        self.particles = np.clip(self.particles, self.lb, self.ub)\n\n    def local_search(self, func):\n        for i in range(self.pop_size):\n            if self.evals >= self.budget:\n                break\n            x0 = self.particles[i].copy()\n            bounds = [(self.lb, self.ub)] * self.dim\n            res = minimize(func, x0, method='L-BFGS-B', bounds=bounds, options={'maxfun': min(self.budget - self.evals, self.local_search_fev)})\n\n            if res.fun < self.fitness[i]:\n                self.particles[i] = res.x\n                self.fitness[i] = res.fun\n                self.evals += res.nfev\n                self.pbest_positions[i] = res.x\n                self.pbest_fitness[i] = res.fun\n\n                if res.fun < self.gbest_fitness:\n                    self.gbest_fitness = res.fun\n                    self.gbest_position = res.x\n                    self.f_opt = res.fun\n                    self.x_opt = res.x\n\n\n    def __call__(self, func):\n        self.initialize_particles(func)\n        iter_count = 0\n\n        while self.evals < self.budget:\n            iter_count += 1\n\n            if iter_count % self.local_search_interval == 0:\n                self.local_search(func)\n\n            self.update_velocities()\n            self.update_positions()\n\n            new_fitness = np.array([func(x) for x in self.particles])\n            self.evals += self.pop_size\n\n            for i in range(self.pop_size):\n                if new_fitness[i] < self.pbest_fitness[i]:\n                    self.pbest_fitness[i] = new_fitness[i]\n                    self.pbest_positions[i] = self.particles[i].copy()\n\n                    if new_fitness[i] < self.gbest_fitness:\n                        self.gbest_fitness = new_fitness[i]\n                        self.gbest_position = self.particles[i].copy()\n                        self.f_opt = new_fitness[i]\n                        self.x_opt = self.particles[i].copy()\n            \n            self.fitness = new_fitness.copy()\n            self.w *= self.inertia_reduction_rate\n            self.w = max(0.4, self.w)\n\n            if self.evals >= self.budget:\n                break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm PSO scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0]}, "task_prompt": ""}
{"id": "5f1f6e1d-97eb-44af-9a86-38b39a953a58", "fitness": 0.16676661343180074, "name": "GaussianMutationES", "description": "No description provided.", "code": "import numpy as np\n\nclass GaussianMutationES:\n    def __init__(self, budget=10000, dim=10, pop_size=20, sigma=0.5, restart_trigger=500):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.sigma = sigma\n        self.lb = -5.0\n        self.ub = 5.0\n        self.restart_trigger = restart_trigger\n        self.eval_count = 0\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.eval_count += self.pop_size\n        best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_index]\n        self.x_opt = self.population[best_index].copy()\n        \n        no_improvement_count = 0\n\n        while self.eval_count < self.budget:\n            new_population = np.zeros((self.pop_size, self.dim))\n            for i in range(self.pop_size):\n                mutation = np.random.normal(0, self.sigma, size=self.dim)\n                new_individual = self.population[i] + mutation\n                new_individual = np.clip(new_individual, self.lb, self.ub)\n                new_population[i] = new_individual\n            \n            new_fitness = np.array([func(x) for x in new_population])\n            self.eval_count += self.pop_size\n            \n            for i in range(self.pop_size):\n                if new_fitness[i] < self.fitness[i]:\n                    self.fitness[i] = new_fitness[i]\n                    self.population[i] = new_population[i].copy()\n            \n            current_best_index = np.argmin(self.fitness)\n            if self.fitness[current_best_index] < self.f_opt:\n                self.f_opt = self.fitness[current_best_index]\n                self.x_opt = self.population[current_best_index].copy()\n                no_improvement_count = 0\n            else:\n                no_improvement_count += self.pop_size\n            \n            if no_improvement_count > self.restart_trigger:\n                self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n                self.fitness = np.array([func(x) for x in self.population])\n                self.eval_count += self.pop_size\n                best_index = np.argmin(self.fitness)\n                \n                if self.fitness[best_index] < self.f_opt:\n                    self.f_opt = self.fitness[best_index]\n                    self.x_opt = self.population[best_index].copy()\n                no_improvement_count = 0\n        \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm GaussianMutationES scored 0.167 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.12961018511528277, 0.19722424340975808, 0.3402320252021621, 0]}, "task_prompt": ""}
{"id": "d684d921-b46b-4a19-b759-88ceaf503762", "fitness": "-inf", "name": "SimplexEnhancedEvolution", "description": "No description provided.", "code": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass SimplexEnhancedEvolution:\n    def __init__(self, budget=10000, dim=10, pop_size=20, simplex_iters=5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.simplex_iters = simplex_iters\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index].copy()\n\n        while self.budget > 0:\n            # Select individuals for simplex-based search (e.g., top 50% + some random)\n            num_elite = self.pop_size // 2\n            elite_indices = np.argsort(fitness)[:num_elite]\n            random_indices = np.random.choice(self.pop_size, self.pop_size - num_elite, replace=False)\n            selected_indices = np.concatenate([elite_indices, random_indices])\n\n            for i in selected_indices:\n                if self.budget <= 0:\n                  break\n\n                # Nelder-Mead Simplex\n                initial_simplex = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.dim + 1, self.dim))\n                initial_simplex[0] = population[i]\n                \n                def simplex_objective(x):\n                  val = func(x)\n                  return val\n\n                res = minimize(simplex_objective, population[i], method='Nelder-Mead',\n                                bounds=[(func.bounds.lb, func.bounds.ub)] * self.dim,\n                                options={'maxiter': self.simplex_iters, 'maxfev': self.simplex_iters})\n\n                self.budget -= res.nfev\n\n                if res.fun < fitness[i]:\n                    fitness[i] = res.fun\n                    population[i] = res.x.copy()\n\n                    if res.fun < self.f_opt:\n                        self.f_opt = res.fun\n                        self.x_opt = res.x.copy()\n            \n            # Global Exploration: Introduce some diversity (e.g., replace worst individuals)\n            num_replace = self.pop_size // 4\n            worst_indices = np.argsort(fitness)[-num_replace:]\n            for i in worst_indices:\n                population[i] = np.random.uniform(func.bounds.lb, func.bounds.ub)\n                fitness[i] = func(population[i])\n                self.budget -= 1\n\n                if fitness[i] < self.f_opt:\n                    self.f_opt = fitness[i]\n                    self.x_opt = population[i].copy()\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "In the code, line 43, in __call__, the following error occurred:\nNameError: name 'minimize' is not defined\nOn line: res = minimize(simplex_objective, population[i], method='Nelder-Mead',", "error": "In the code, line 43, in __call__, the following error occurred:\nNameError: name 'minimize' is not defined\nOn line: res = minimize(simplex_objective, population[i], method='Nelder-Mead',", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "58c0f3b8-497d-4153-b5fe-d204009a93f8", "fitness": 0.24810580394932202, "name": "AdaptiveES", "description": "No description provided.", "code": "import numpy as np\n\nclass AdaptiveES:\n    def __init__(self, budget=10000, dim=10, pop_size=20, sigma=0.1, decay_rate=0.99):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.sigma = sigma\n        self.decay_rate = decay_rate\n        self.lb = -5.0\n        self.ub = 5.0\n        self.population = None\n        self.fitness = None\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.evals = 0\n        self.restart_trigger = 100\n        self.stagnation_counter = 0\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.pop_size\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n\n    def mutate(self, individual):\n        noise = np.random.normal(0, self.sigma, size=self.dim)\n        return np.clip(individual + noise, self.lb, self.ub)\n\n    def evaluate(self, func, individual):\n        f = func(individual)\n        self.evals += 1\n        return f\n\n    def restart_population(self, func):\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals += self.pop_size\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n        self.sigma = 0.1\n        self.stagnation_counter = 0\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        \n        while self.evals < self.budget:\n            \n            parents_fitness = self.fitness.copy()\n            children = np.array([self.mutate(x) for x in self.population])\n            children_fitness = np.array([self.evaluate(func, x) for x in children])\n            \n            for i in range(self.pop_size):\n              if children_fitness[i] < parents_fitness[i]:\n                self.population[i] = children[i]\n                self.fitness[i] = children_fitness[i]\n            \n                if children_fitness[i] < self.f_opt:\n                    self.f_opt = children_fitness[i]\n                    self.x_opt = children[i]\n                    self.stagnation_counter = 0\n\n            self.sigma *= self.decay_rate\n            self.stagnation_counter += 1\n\n            if self.stagnation_counter > self.restart_trigger or self.sigma < 1e-5:\n              if self.evals + self.pop_size < self.budget:\n                self.restart_population(func)\n              else:\n                break\n              \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveES scored 0.248 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.062289174718525686, 0.18205836349504234, 0.3034870317301126, 0.15628337713451512, 0.14288961051888416, 0.16809889884174334, 0.20920168000616746, 0.127527215178469, 0.19033257927853486, 0.1830766857945031, 0.21155966689252137, 0.7261660381839778, 0.28043051277159214, 0.13393306153598095, 0.5481398323033071, 0.25969684371858903, 0.24178101110784156, 0.21594312737136834, 0.14314960350223593, 0.4760717649025279]}, "task_prompt": ""}
{"id": "5c221cf3-3785-40be-a82e-5c80f7e521b9", "fitness": "-inf", "name": "AdaptiveClusteringSearch", "description": "No description provided.", "code": "import numpy as np\nfrom sklearn.cluster import MeanShift, estimate_bandwidth\nfrom scipy.spatial.distance import cdist\n\nclass AdaptiveClusteringSearch:\n    def __init__(self, budget=10000, dim=10, pop_size=50, num_clusters=5, repulsion_factor=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.num_clusters = num_clusters\n        self.repulsion_factor = repulsion_factor\n        self.lb = -5.0\n        self.ub = 5.0\n        self.population = None\n        self.fitness = None\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.evals = 0\n        self.successful_points = []\n        self.bandwidth = None\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.pop_size\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n        self.successful_points.extend(self.population[self.fitness < np.median(self.fitness)])\n\n    def generate_samples(self, func):\n        new_samples = []\n        if not self.successful_points:\n            new_samples = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        else:\n            bandwidth = estimate_bandwidth(self.successful_points, quantile=0.2)\n            if bandwidth == 0:\n                bandwidth = 0.1\n            ms = MeanShift(bandwidth=bandwidth, bin_seeding=True)\n            ms.fit(self.successful_points)\n            cluster_centers = ms.cluster_centers_\n            \n            if len(cluster_centers) == 0:\n                 new_samples = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n            else:\n                num_per_cluster = self.pop_size // len(cluster_centers)\n                for center in cluster_centers:\n                    for _ in range(num_per_cluster):\n                        sample = np.random.normal(center, 0.5, size=self.dim)\n                        sample = np.clip(sample, self.lb, self.ub)\n                        new_samples.append(sample)\n\n                while len(new_samples) < self.pop_size:\n                    sample = np.random.uniform(self.lb, self.ub, size=self.dim)\n                    new_samples.append(sample)\n\n        # Repulsion mechanism\n        if self.successful_points:\n            for i in range(len(new_samples)):\n                distances = cdist([new_samples[i]], self.successful_points)[0]\n                closest_idx = np.argmin(distances)\n                repulsion_vector = new_samples[i] - self.successful_points[closest_idx]\n                new_samples[i] = np.clip(new_samples[i] + self.repulsion_factor * repulsion_vector, self.lb, self.ub)\n        return np.array(new_samples)\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.evals < self.budget:\n            new_population = self.generate_samples(func)\n            new_fitness = np.array([func(x) for x in new_population])\n            self.evals += self.pop_size\n            \n            for i in range(self.pop_size):\n                if new_fitness[i] < self.f_opt:\n                    self.f_opt = new_fitness[i]\n                    self.x_opt = new_population[i]\n\n            self.population = new_population\n            self.fitness = new_fitness\n            self.successful_points.extend(self.population[self.fitness < np.median(self.fitness)])\n            if len(self.successful_points) > 100:\n                self.successful_points = self.successful_points[-100:]\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "In the code, line 35, in generate_samples, the following error occurred:\nNameError: name 'estimate_bandwidth' is not defined\nOn line: bandwidth = estimate_bandwidth(self.successful_points, quantile=0.2)", "error": "In the code, line 35, in generate_samples, the following error occurred:\nNameError: name 'estimate_bandwidth' is not defined\nOn line: bandwidth = estimate_bandwidth(self.successful_points, quantile=0.2)", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "2f8ff5f8-2043-4724-ba02-b3e2060f11cb", "fitness": 0.0, "name": "AdaptivePSO", "description": "No description provided.", "code": "import numpy as np\n\nclass AdaptivePSO:\n    def __init__(self, budget=10000, dim=10, swarm_size=30, inertia_max=0.9, inertia_min=0.4, c1=2.0, c2=2.0, velocity_clamp=0.5, stagnation_threshold=500, diversity_threshold=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = swarm_size\n        self.inertia_max = inertia_max\n        self.inertia_min = inertia_min\n        self.c1 = c1\n        self.c2 = c2\n        self.velocity_clamp = velocity_clamp\n        self.stagnation_threshold = stagnation_threshold\n        self.stagnation_counter = 0\n        self.diversity_threshold = diversity_threshold\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        # Initialize swarm\n        swarm = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.swarm_size, self.dim))\n        velocities = np.random.uniform(-self.velocity_clamp * (func.bounds.ub - func.bounds.lb), self.velocity_clamp * (func.bounds.ub - func.bounds.lb), size=(self.swarm_size, self.dim))\n        fitness = np.array([func(x) for x in swarm])\n        self.budget -= self.swarm_size\n\n        personal_best_positions = swarm.copy()\n        personal_best_fitness = fitness.copy()\n\n        global_best_index = np.argmin(fitness)\n        global_best_position = swarm[global_best_index].copy()\n        self.f_opt = fitness[global_best_index]\n        self.x_opt = global_best_position.copy()\n\n        while self.budget > 0:\n            # Calculate diversity\n            diversity = np.std(swarm)\n\n            # Adaptive inertia weight\n            if diversity < self.diversity_threshold:\n                inertia = self.inertia_min  # Reduce inertia for exploitation\n                self.stagnation_counter +=1\n            else:\n                inertia = self.inertia_max  # Increase inertia for exploration\n                self.stagnation_counter = 0\n                \n            #Velocity clamping and constriction factor\n\n            constriction_factor = 0.729  # Standard constriction factor\n\n            for i in range(self.swarm_size):\n                # Update velocity\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                velocities[i] = constriction_factor * (inertia * velocities[i] +\n                                        self.c1 * r1 * (personal_best_positions[i] - swarm[i]) +\n                                        self.c2 * r2 * (global_best_position - swarm[i]))\n                \n                # Clamp velocities\n                velocities[i] = np.clip(velocities[i], -self.velocity_clamp * (func.bounds.ub - func.bounds.lb), self.velocity_clamp * (func.bounds.ub - func.bounds.lb))\n\n                # Update position\n                new_position = swarm[i] + velocities[i]\n                new_position = np.clip(new_position, func.bounds.lb, func.bounds.ub)\n                \n                swarm[i] = new_position\n                \n            # Evaluate fitness\n            new_fitness = np.array([func(x) for x in swarm])\n            self.budget -= self.swarm_size\n\n            # Update personal best\n            for i in range(self.swarm_size):\n                if new_fitness[i] < personal_best_fitness[i]:\n                    personal_best_fitness[i] = new_fitness[i]\n                    personal_best_positions[i] = swarm[i].copy()\n\n            # Update global best\n            global_best_index = np.argmin(personal_best_fitness)\n            if personal_best_fitness[global_best_index] < self.f_opt:\n                self.f_opt = personal_best_fitness[global_best_index]\n                self.x_opt = personal_best_positions[global_best_index].copy()\n                self.stagnation_counter = 0\n\n            #Stagnation Detection and swarm restart\n            if self.stagnation_counter > self.stagnation_threshold:\n                swarm = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.swarm_size, self.dim))\n                velocities = np.random.uniform(-self.velocity_clamp * (func.bounds.ub - func.bounds.lb), self.velocity_clamp * (func.bounds.ub - func.bounds.lb), size=(self.swarm_size, self.dim))\n                fitness = np.array([func(x) for x in swarm])\n                self.budget -= self.swarm_size\n\n                personal_best_positions = swarm.copy()\n                personal_best_fitness = fitness.copy()\n\n                global_best_index = np.argmin(fitness)\n                global_best_position = swarm[global_best_index].copy()\n                self.f_opt = fitness[global_best_index]\n                self.x_opt = global_best_position.copy()\n                self.stagnation_counter = 0\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptivePSO scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0]}, "task_prompt": ""}
{"id": "f2c44490-44fa-4857-b096-ab96464df3d0", "fitness": 0.3322567424036818, "name": "DynamicDE", "description": "No description provided.", "code": "import numpy as np\n\nclass DynamicDE:\n    def __init__(self, budget=10000, dim=10, initial_pop_size=50, min_pop_size=10, F=0.5, CR=0.9, restart_prob=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = initial_pop_size\n        self.min_pop_size = min_pop_size\n        self.F = F\n        self.CR = CR\n        self.lb = -5.0\n        self.ub = 5.0\n        self.population = None\n        self.fitness = None\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.evals = 0\n        self.restart_prob = restart_prob\n        self.archive = []\n        self.archive_size = 10\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals += self.pop_size\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n\n    def mutate(self, pop, i):\n        if np.random.rand() < 0.5: # Strategy 1: classic DE\n            idxs = [idx for idx in range(self.pop_size) if idx != i]\n            a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n            return np.clip(a + self.F * (b - c), self.lb, self.ub)\n        else: # Strategy 2: current-to-best\n            idxs = [idx for idx in range(self.pop_size) if idx != i]\n            a, b = pop[np.random.choice(idxs, 2, replace=False)]\n            return np.clip(pop[i] + self.F * (self.x_opt - pop[i]) + self.F * (a - b), self.lb, self.ub)\n\n    def crossover(self, mutant, target):\n        cross_points = np.random.rand(self.dim) < self.CR\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        return np.where(cross_points, mutant, target)\n    \n    def adjust_population_size(self):\n        if len(self.archive) > self.archive_size:\n            self.pop_size = max(self.min_pop_size, self.pop_size // 2)\n            self.population = self.population[np.argsort(self.fitness)[:self.pop_size]]\n            self.fitness = self.fitness[np.argsort(self.fitness)[:self.pop_size]]\n    \n    def restart(self, func):\n        if np.random.rand() < self.restart_prob:\n            self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n            self.fitness = np.array([func(x) for x in self.population])\n            self.evals += self.pop_size\n            best_idx = np.argmin(self.fitness)\n            if self.fitness[best_idx] < self.f_opt:\n                self.f_opt = self.fitness[best_idx]\n                self.x_opt = self.population[best_idx]\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        while self.evals < self.budget:\n            new_population = []\n            new_fitness = []\n            for i in range(self.pop_size):\n                if self.evals >= self.budget:\n                    break\n                mutant = self.mutate(self.population, i)\n                trial = self.crossover(mutant, self.population[i])\n                f = func(trial)\n                self.evals += 1\n\n                if f < self.fitness[i]:\n                    new_population.append(trial)\n                    new_fitness.append(f)\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = trial\n                        if len(self.archive) < self.archive_size:\n                            self.archive.append(trial)\n                        else:\n                            self.archive[np.random.randint(0, self.archive_size)] = trial\n\n                else:\n                    new_population.append(self.population[i])\n                    new_fitness.append(self.fitness[i])\n            \n            self.population = np.array(new_population)\n            self.fitness = np.array(new_fitness)\n            self.adjust_population_size()\n            self.restart(func)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm DynamicDE scored 0.332 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.18468821255181234, 0.26969803989287444, 0.419820645797192, 0.5019997038357832, 0.5912777002585312, 0.35831289448957937, 0]}, "task_prompt": ""}
{"id": "306973ce-a229-4a75-b2df-31971907e1e7", "fitness": "-inf", "name": "BudgetAwareCMAES", "description": "No description provided.", "code": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass BudgetAwareCMAES:\n    def __init__(self, budget=10000, dim=10, pop_size=None, initial_sigma=0.5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size if pop_size is not None else 4 + int(3 * np.log(dim))\n        self.initial_sigma = initial_sigma\n        self.lb = -5.0\n        self.ub = 5.0\n        self.mean = np.random.uniform(self.lb, self.ub, size=dim)\n        self.sigma = initial_sigma\n        self.C = np.eye(dim)\n        self.pc = np.zeros(dim)\n        self.ps = np.zeros(dim)\n        self.chiN = dim**0.5 * (1 - 1/(4*dim) + 1/(21*dim**2))\n        self.c_sigma = (self.pop_size + 2) / (dim + 5) / 3\n        self.d_sigma = 1 + 2 * max(0, np.sqrt((self.c_sigma * (2 - self.c_sigma) * self.chiN**2 - 1)))\n        self.c_c = 4 / (dim + 4)\n        self.mu = self.pop_size // 2\n        self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n        self.weights /= np.sum(self.weights)\n        self.c_mu = min(1 - self.c_c, 2 * (self.mu - 1 + 1/(self.mu + 1)) / ( (dim + 2)**2 + self.mu))\n        self.c_1 = 0.1\n        self.evals = 0\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.local_search_interval = budget // 15\n\n    def sample_population(self):\n        z = np.random.normal(0, 1, size=(self.pop_size, self.dim))\n        return self.mean + self.sigma * np.dot(z, np.linalg.cholesky(self.C).T)\n\n    def update_distribution(self, population, fitness_values):\n        idx = np.argsort(fitness_values)\n        elite_population = population[idx[:self.mu]]\n        y = elite_population - self.mean\n        self.pc = (1 - self.c_c) * self.pc + (self.c_c * (2 - self.c_c))**0.5 * np.sum(self.weights[:, None] * y, axis=0) / self.sigma\n        self.ps = (1 - self.c_sigma) * self.ps + (self.c_sigma * (2 - self.c_sigma))**0.5 * self.pc / (np.linalg.det(self.C)**(1 / (2 * self.dim)))\n        \n        hsig = np.linalg.norm(self.ps) / (1 - (1-self.c_sigma)**(self.evals/self.pop_size)) / self.chiN < 1.4 + 2/(self.dim + 1)\n\n        self.sigma *= np.exp(min(0.5, self.c_sigma / self.d_sigma * (np.linalg.norm(self.ps)/self.chiN - 1)))\n\n        delta = y / self.sigma\n        \n        self.C = (1 - self.c_1 - self.c_mu + self.c_1 * self.c_c * (2 - self.c_c) * (not hsig)) * self.C + self.c_1 * np.outer(self.pc, self.pc)\n        self.C += self.c_mu * np.sum(self.weights[:, None, None] * delta[:, :, None] * delta[:, None, :], axis=0)\n\n        self.mean = np.sum(self.weights[:, None] * elite_population, axis=0)\n\n    def local_search(self, func):\n        bounds = [(self.lb, self.ub)] * self.dim\n        res = minimize(func, self.x_opt, method='L-BFGS-B', bounds=bounds, options={'maxfun': min(self.budget - self.evals, 50)})\n\n        if res.fun < self.f_opt:\n            self.f_opt = res.fun\n            self.x_opt = res.x\n        self.evals += res.nfev\n\n    def __call__(self, func):\n        self.mean = np.random.uniform(self.lb, self.ub, size=self.dim)\n        self.x_opt = self.mean.copy()\n        self.f_opt = func(self.mean)\n        self.evals +=1\n\n        iter_count = 0\n        while self.evals < self.budget:\n            iter_count += 1\n            population = self.sample_population()\n            population = np.clip(population, self.lb, self.ub)\n            fitness_values = np.array([func(x) for x in population])\n            self.evals += self.pop_size\n\n            if np.min(fitness_values) < self.f_opt:\n                self.f_opt = np.min(fitness_values)\n                self.x_opt = population[np.argmin(fitness_values)].copy()\n            \n            self.update_distribution(population, fitness_values)\n            \n            if iter_count % self.local_search_interval == 0 and self.evals < self.budget:\n                self.local_search(func)\n\n            if self.evals >= self.budget:\n                break\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "In the code, line 55, in local_search, the following error occurred:\nNameError: name 'minimize' is not defined\nOn line: res = minimize(func, self.x_opt, method='L-BFGS-B', bounds=bounds, options={'maxfun': min(self.budget - self.evals, 50)})", "error": "In the code, line 55, in local_search, the following error occurred:\nNameError: name 'minimize' is not defined\nOn line: res = minimize(func, self.x_opt, method='L-BFGS-B', bounds=bounds, options={'maxfun': min(self.budget - self.evals, 50)})", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "d9b897e9-c823-4d81-b518-b380be9d2541", "fitness": "-inf", "name": "HybridDE_CMAES", "description": "No description provided.", "code": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridDE_CMAES:\n    def __init__(self, budget=10000, dim=10, pop_size=20, F=0.5, CR=0.9, cmaes_interval=500):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.lb = -5.0\n        self.ub = 5.0\n        self.population = None\n        self.fitness = None\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.evals = 0\n        self.cmaes_interval = cmaes_interval\n        self.cmaes_sigma = 0.5 #initial sigma for CMA-ES\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals += self.pop_size\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n\n    def mutate(self, pop, i):\n        idxs = [idx for idx in range(self.pop_size) if idx != i]\n        a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n        return np.clip(a + self.F * (b - c), self.lb, self.ub)\n\n    def crossover(self, mutant, target):\n        cross_points = np.random.rand(self.dim) < self.CR\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        return np.where(cross_points, mutant, target)\n\n    def cmaes_step(self, func):\n        x0 = self.x_opt.copy()\n        bounds = [(self.lb, self.ub)] * self.dim\n\n        def cmaes_objective(x):\n            return func(x)\n\n        options = {'maxfev': min(self.cmaes_interval, self.budget - self.evals), 'sigma': self.cmaes_sigma}\n        res = minimize(cmaes_objective, x0, method='trust-constr', bounds=bounds, options=options) # or method='SLSQP' or 'TNC'\n\n        if res.fun < self.f_opt:\n            self.f_opt = res.fun\n            self.x_opt = res.x\n            \n        self.evals += res.nfev\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        generation = 0\n        while self.evals < self.budget:\n            generation += 1\n            if generation % (self.cmaes_interval // self.pop_size) == 0:\n                self.cmaes_step(func)\n\n            new_population = []\n            new_fitness = []\n\n            for i in range(self.pop_size):\n                if self.evals >= self.budget:\n                    new_population.append(self.population[i])\n                    new_fitness.append(self.fitness[i])\n                    continue\n\n                mutant = self.mutate(self.population, i)\n                trial = self.crossover(mutant, self.population[i])\n                f = func(trial)\n                self.evals += 1\n\n                if f < self.fitness[i]:\n                    new_population.append(trial)\n                    new_fitness.append(f)\n\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = trial\n                else:\n                    new_population.append(self.population[i])\n                    new_fitness.append(self.fitness[i])\n\n            self.population = np.array(new_population)\n            self.fitness = np.array(new_fitness)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "In the code, line 47, in cmaes_step, the following error occurred:\nNameError: name 'minimize' is not defined\nOn line: res = minimize(cmaes_objective, x0, method='trust-constr', bounds=bounds, options=options) # or method='SLSQP' or 'TNC'", "error": "In the code, line 47, in cmaes_step, the following error occurred:\nNameError: name 'minimize' is not defined\nOn line: res = minimize(cmaes_objective, x0, method='trust-constr', bounds=bounds, options=options) # or method='SLSQP' or 'TNC'", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "03fea808-efb8-4dc9-8916-399bb66bfd61", "fitness": 0.19288577234340643, "name": "CMAES", "description": "No description provided.", "code": "import numpy as np\n\nclass CMAES:\n    def __init__(self, budget=10000, dim=10, pop_size=10, sigma=0.5, cs=0.1, damps=0.1, ccov1=0.02, ccovmu=0.01, restarts=3):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.sigma = sigma\n        self.cs = cs\n        self.damps = damps\n        self.ccov1 = ccov1\n        self.ccovmu = ccovmu\n        self.restarts = restarts\n        self.lb = -5.0\n        self.ub = 5.0\n\n        self.mu = None\n        self.C = None\n        self.pc = None\n        self.ps = None\n        self.evals = 0\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def initialize(self):\n        self.mu = np.random.uniform(self.lb, self.ub, size=self.dim)\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.evals = 0\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def sample_population(self):\n        z = np.random.multivariate_normal(np.zeros(self.dim), self.C, size=self.pop_size)\n        x = self.mu + self.sigma * z\n        x = np.clip(x, self.lb, self.ub)\n        return x, z\n\n    def update_parameters(self, x, z, fitness):\n        idx = np.argsort(fitness)\n        x_sorted = x[idx]\n        z_sorted = z[idx]\n\n        weights = np.log(self.pop_size + 1) - np.log(np.arange(1, self.pop_size + 1))\n        weights = weights / np.sum(weights)\n\n        mu_old = self.mu.copy()\n        self.mu = np.sum(x_sorted[:self.pop_size] * weights[:, np.newaxis], axis=0)\n\n        self.ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs) * np.sum(weights**2)) * (self.mu - mu_old) / self.sigma\n        self.pc = (1 - self.damps) * self.pc + np.sqrt(self.damps * (2 - self.damps)) * (self.mu - mu_old)\n\n        artw = z_sorted[:self.pop_size] * weights[:, np.newaxis]\n        self.C = (1 - self.ccov1 - self.ccovmu) * self.C + self.ccov1 * (np.outer(self.pc, self.pc) + 0.01*np.eye(self.dim)) + self.ccovmu * np.sum(artw[:, :, np.newaxis] @ artw[:, np.newaxis, :], axis=0)\n\n        self.sigma *= np.exp((np.linalg.norm(self.ps) - np.sqrt(self.dim)) / self.cs / self.damps)\n\n    def __call__(self, func):\n        self.initialize()\n        \n        restart_count = 0\n        while restart_count < self.restarts:\n            while self.evals < self.budget // self.restarts:\n                x, z = self.sample_population()\n                fitness = np.array([func(xi) for xi in x])\n                self.evals += self.pop_size\n\n                if np.min(fitness) < self.f_opt:\n                    self.f_opt = np.min(fitness)\n                    self.x_opt = x[np.argmin(fitness)]\n\n                self.update_parameters(x, z, fitness)\n\n                if np.any(np.isnan(self.C)):\n                    self.initialize()\n                    break\n            \n            if self.evals >= self.budget // self.restarts:\n                break\n            \n            restart_count += 1\n            self.initialize()\n        \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm CMAES scored 0.193 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.026972563813553063, 0.077506517177237, 0.2105121071898145, 0.12332744641463234, 0.0945964427059185, 0.1385305232635734, 0.16714770870276585, 0.0950176311023373, 0.11977593762022265, 0.10337738596448276, 0.1318490747924156, 0.9952306025004661, 0.0, 0.1374896409234636, 0.5392029819207518, 0.14254483533156037, 0.1207262320061967, 0.15800093021263495, 0.11522257053058416, 0.360684314695518]}, "task_prompt": ""}
{"id": "4a899258-c1eb-40aa-b2df-c28b647bc831", "fitness": 0.4978028772214726, "name": "AdaptivePSO", "description": "No description provided.", "code": "import numpy as np\n\nclass AdaptivePSO:\n    def __init__(self, budget=10000, dim=10, pop_size=20, w=0.7, c1=1.5, c2=1.5, v_max_ratio=0.2):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.w = w  # Inertia weight\n        self.c1 = c1  # Cognitive coefficient\n        self.c2 = c2  # Social coefficient\n        self.v_max_ratio = v_max_ratio\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        # Initialize population and velocities\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        velocities = np.random.uniform(-self.v_max_ratio * (func.bounds.ub - func.bounds.lb), self.v_max_ratio * (func.bounds.ub - func.bounds.lb), size=(self.pop_size, self.dim))\n        pbest_positions = population.copy()\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n        pbest_fitness = fitness.copy()\n\n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index].copy()\n        gbest_position = population[best_index].copy()\n\n        v_max = self.v_max_ratio * (func.bounds.ub - func.bounds.lb)\n        while self.budget > 0:\n            # Update velocities and positions\n            r1 = np.random.rand(self.pop_size, self.dim)\n            r2 = np.random.rand(self.pop_size, self.dim)\n            velocities = self.w * velocities + self.c1 * r1 * (pbest_positions - population) + self.c2 * r2 * (gbest_position - population)\n            velocities = np.clip(velocities, -v_max, v_max)  # Clamp velocities\n\n            population = population + velocities\n            population = np.clip(population, func.bounds.lb, func.bounds.ub)\n\n            # Evaluate new positions\n            new_fitness = np.array([func(x) for x in population])\n            self.budget -= self.pop_size\n\n            # Update personal best positions\n            for i in range(self.pop_size):\n                if new_fitness[i] < pbest_fitness[i]:\n                    pbest_fitness[i] = new_fitness[i]\n                    pbest_positions[i] = population[i].copy()\n\n            # Update global best position\n            best_index = np.argmin(pbest_fitness)\n            if pbest_fitness[best_index] < self.f_opt:\n                self.f_opt = pbest_fitness[best_index]\n                self.x_opt = pbest_positions[best_index].copy()\n                gbest_position = pbest_positions[best_index].copy()\n\n            # Adapt inertia weight\n            self.w = np.clip(self.w * 0.99, 0.4, 0.9)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptivePSO scored 0.498 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.1748173543553685, 0.17526856916843625, 0.9293677042126501, 0.9647594958163856, 0.27930575027460125, 0.2500178692775049, 0.31404872909236214, 0.4466479981193735, 0.7874612774690927, 0.21849621483141168, 0.9508915796431411, 0.9941475160094804, 0.22120344724000773, 0.3414667725329136, 0.5890575420336269, 0.9455683515607812, 0.32897036560168225, 0.38107121989861126, 0.17324761655809473, 0.49024217073392506]}, "task_prompt": ""}
{"id": "9de4aa29-0510-4c9e-96fb-146100e646df", "fitness": 0.5146975216618496, "name": "AdaptivePSO", "description": "No description provided.", "code": "import numpy as np\n\nclass AdaptivePSO:\n    def __init__(self, budget=10000, dim=10, pop_size=20, c1=1.5, c2=1.5, w_start=0.9, w_end=0.4, v_max_ratio=0.2):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.c1 = c1  # Cognitive coefficient\n        self.c2 = c2  # Social coefficient\n        self.w_start = w_start # initial inertia weight\n        self.w_end = w_end # final inertia weight\n        self.v_max_ratio = v_max_ratio\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        # Initialize population and velocities\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        velocities = np.random.uniform(-self.v_max_ratio * (func.bounds.ub - func.bounds.lb), self.v_max_ratio * (func.bounds.ub - func.bounds.lb), size=(self.pop_size, self.dim))\n        \n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n        \n        personal_best_positions = population.copy()\n        personal_best_fitness = fitness.copy()\n        \n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index].copy()\n        global_best_position = population[best_index].copy()\n\n        iteration = 0\n        while self.budget > 0:\n            # Adaptive inertia weight\n            w = self.w_start - (self.w_start - self.w_end) * iteration / (self.budget / self.pop_size + iteration)\n\n            for i in range(self.pop_size):\n                # Update velocities\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                \n                velocities[i] = w * velocities[i] + \\\n                                self.c1 * r1 * (personal_best_positions[i] - population[i]) + \\\n                                self.c2 * r2 * (global_best_position - population[i])\n\n                # Velocity clamping\n                v_max = self.v_max_ratio * (func.bounds.ub - func.bounds.lb)\n                velocities[i] = np.clip(velocities[i], -v_max, v_max)\n                \n                # Update positions\n                population[i] = population[i] + velocities[i]\n                population[i] = np.clip(population[i], func.bounds.lb, func.bounds.ub)\n\n            new_fitness = np.array([func(x) for x in population])\n            self.budget -= self.pop_size\n\n            # Update personal best positions\n            for i in range(self.pop_size):\n                if new_fitness[i] < personal_best_fitness[i]:\n                    personal_best_fitness[i] = new_fitness[i]\n                    personal_best_positions[i] = population[i].copy()\n\n            # Update global best position\n            best_index = np.argmin(personal_best_fitness)\n            if personal_best_fitness[best_index] < self.f_opt:\n                self.f_opt = personal_best_fitness[best_index]\n                self.x_opt = personal_best_positions[best_index].copy()\n                global_best_position = personal_best_positions[best_index].copy()\n            \n            iteration += 1\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptivePSO scored 0.515 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.12610206469737806, 0.22004418701602113, 0.7665232878564775, 0.19721034565730844, 0.2887311121250916, 0.7928147939786288, 0.7242706933949865, 0.5123569715224661, 0.7648738749320603, 0.233750862615562, 0.823881803932516, 0.9948030880066595, 0.23991912000961424, 0.3067673882621317, 0.6502581684580087, 0.7989937862277994, 0.5932508187576044, 0.37991684335359643, 0.36279199945482465, 0.5166892229782558]}, "task_prompt": ""}
{"id": "40d8bc2f-6b20-40ee-8bd7-405249501c4b", "fitness": "-inf", "name": "CMAES", "description": "No description provided.", "code": "import numpy as np\nimport cma\n\nclass CMAES:\n    def __init__(self, budget=10000, dim=10, sigma0=0.5):\n        self.budget = budget\n        self.dim = dim\n        self.sigma0 = sigma0\n        self.lb = -5.0\n        self.ub = 5.0\n        self.es = None\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        x0 = np.random.uniform(self.lb, self.ub, size=self.dim)\n        self.es = cma.purecma.CMAEvolutionStrategy(x0, self.sigma0,\n                                             {'bounds': [self.lb, self.ub],\n                                              'verbose': -9,\n                                              'maxfevals': self.budget})\n\n        while self.es.result[0] is None and self.es.countevals < self.budget:\n            solutions = self.es.ask()\n            fitness_list = []\n            for s in solutions:\n                fitness_list.append(func(s))\n            self.es.tell(solutions, fitness_list)\n            if min(fitness_list) < self.f_opt:\n                self.f_opt = min(fitness_list)\n                self.x_opt = solutions[np.argmin(fitness_list)]\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "In the code, line 2, in <module>, the following error occurred:\nModuleNotFoundError: No module named 'cma'\nOn line: import cma", "error": "In the code, line 2, in <module>, the following error occurred:\nModuleNotFoundError: No module named 'cma'\nOn line: import cma", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "ff806db4-349d-4bee-a252-854087a32577", "fitness": "-inf", "name": "AdaptiveNelderMead", "description": "No description provided.", "code": "import numpy as np\n\nclass AdaptiveNelderMead:\n    def __init__(self, budget=10000, dim=10, initial_simplex_size=0.1, reflection=1.0, expansion=2.0, contraction=0.5, shrinkage=0.5, volume_threshold=1e-8):\n        self.budget = budget\n        self.dim = dim\n        self.initial_simplex_size = initial_simplex_size\n        self.reflection = reflection\n        self.expansion = expansion\n        self.contraction = contraction\n        self.shrinkage = shrinkage\n        self.volume_threshold = volume_threshold\n        self.simplex = None\n        self.values = None\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def initialize_simplex(self, func):\n        # Initialize simplex vertices around a random point\n        initial_point = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        self.simplex = np.zeros((self.dim + 1, self.dim))\n        self.simplex[0] = initial_point\n        for i in range(1, self.dim + 1):\n            self.simplex[i] = initial_point.copy()\n            self.simplex[i][i-1] += self.initial_simplex_size * (func.bounds.ub - func.bounds.lb)\n            self.simplex[i] = np.clip(self.simplex[i], func.bounds.lb, func.bounds.ub) # Keep in bounds\n        self.values = np.array([func(x) for x in self.simplex])\n        self.budget -= self.dim + 1\n        \n        best_index = np.argmin(self.values)\n        self.f_opt = self.values[best_index]\n        self.x_opt = self.simplex[best_index].copy()\n\n    def calculate_simplex_volume(self):\n        # Calculate the volume of the simplex\n        if self.dim == 1:\n           return np.abs(self.simplex[1] - self.simplex[0])\n        matrix = self.simplex[1:] - self.simplex[0]\n        return np.abs(np.linalg.det(matrix))\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        self.initialize_simplex(func)\n        \n        while self.budget > 0:\n            # Order vertices by value\n            order = np.argsort(self.values)\n            self.simplex = self.simplex[order]\n            self.values = self.values[order]\n            \n            best = self.simplex[0]\n            worst = self.simplex[-1]\n            second_worst = self.simplex[-2]\n            centroid = np.mean(self.simplex[:-1], axis=0)\n            \n            # Reflection\n            reflected = centroid + self.reflection * (centroid - worst)\n            reflected = np.clip(reflected, func.bounds.lb, func.bounds.ub)\n            f_reflected = func(reflected)\n            self.budget -= 1\n            if self.budget <= 0:\n                break\n            \n            if self.values[0] <= f_reflected < self.values[-2]:\n                self.simplex[-1] = reflected\n                self.values[-1] = f_reflected\n            elif f_reflected < self.values[0]:\n                # Expansion\n                expanded = centroid + self.expansion * (reflected - centroid)\n                expanded = np.clip(expanded, func.bounds.lb, func.bounds.ub)\n                f_expanded = func(expanded)\n                self.budget -= 1\n                if self.budget <= 0:\n                    break\n\n                if f_expanded < f_reflected:\n                    self.simplex[-1] = expanded\n                    self.values[-1] = f_expanded\n                else:\n                    self.simplex[-1] = reflected\n                    self.values[-1] = f_reflected\n            else:\n                # Contraction\n                contracted = centroid + self.contraction * (worst - centroid)\n                contracted = np.clip(contracted, func.bounds.lb, func.bounds.ub)\n                f_contracted = func(contracted)\n                self.budget -= 1\n                if self.budget <= 0:\n                    break\n\n                if f_contracted < self.values[-1]:\n                    self.simplex[-1] = contracted\n                    self.values[-1] = f_contracted\n                else:\n                    # Shrink\n                    for i in range(1, self.dim + 1):\n                        self.simplex[i] = best + self.shrinkage * (self.simplex[i] - best)\n                        self.simplex[i] = np.clip(self.simplex[i], func.bounds.lb, func.bounds.ub)\n                        self.values[i] = func(self.simplex[i])\n                        self.budget -= 1\n                        if self.budget <= 0:\n                            break\n                    if self.budget <= 0:\n                        break\n\n            # Update best solution\n            best_index = np.argmin(self.values)\n            if self.values[best_index] < self.f_opt:\n                self.f_opt = self.values[best_index]\n                self.x_opt = self.simplex[best_index].copy()\n            \n            # Check for stagnation and restart based on simplex volume\n            volume = self.calculate_simplex_volume()\n            if volume < self.volume_threshold:\n                self.initialize_simplex(func) #restart\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "In the code, line 25, in initialize_simplex, the following error occurred:\nValueError: setting an array element with a sequence.\nOn line: self.simplex[i][i-1] += self.initial_simplex_size * (func.bounds.ub - func.bounds.lb)", "error": "In the code, line 25, in initialize_simplex, the following error occurred:\nValueError: setting an array element with a sequence.\nOn line: self.simplex[i][i-1] += self.initial_simplex_size * (func.bounds.ub - func.bounds.lb)", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "cc4d6ea2-fe88-4df0-b72c-597543d231cc", "fitness": "-inf", "name": "CMAES", "description": "No description provided.", "code": "import numpy as np\n\nclass CMAES:\n    def __init__(self, budget=10000, dim=10, pop_size=None, sigma=0.5, cs=0.3, damps=1, c_cov1=0.1, c_covmu=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size or (4 + int(3 * np.log(dim)))\n        self.sigma = sigma\n        self.cs = cs\n        self.damps = damps\n        self.c_cov1 = c_cov1\n        self.c_covmu = c_covmu\n        self.mu = self.pop_size // 2\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.m = np.zeros(dim)\n        self.C = np.eye(dim)\n        self.p_sigma = np.zeros(dim)\n        self.p_c = np.zeros(dim)\n        self.chiN = dim**0.5 * (1 - 1/(4*dim) + 1/(21*dim**2))\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        \n        while self.budget > 0:\n            # Sample population\n            A = np.linalg.cholesky(self.C)\n            z = np.random.normal(0, 1, size=(self.pop_size, self.dim))\n            x = self.m + self.sigma * A @ z.T\n            x = x.T\n            \n            # Clip to bounds\n            x = np.clip(x, func.bounds.lb, func.bounds.ub)\n            \n            # Evaluate population\n            fitness = np.array([func(xi) for xi in x])\n            self.budget -= self.pop_size\n            \n            # Sort by fitness\n            idx = np.argsort(fitness)\n            x = x[idx]\n            fitness = fitness[idx]\n            \n            # Update best solution\n            if fitness[0] < self.f_opt:\n                self.f_opt = fitness[0]\n                self.x_opt = x[0].copy()\n\n            # Update distribution parameters\n            xmean = np.sum(self.weights[:, None] * x[:self.mu], axis=0)\n            \n            y = x[:self.mu] - self.m\n            \n            # Update evolution path\n            self.p_sigma = (1 - self.cs) * self.p_sigma + (self.cs * (2 - self.cs))**0.5 * A @ z[idx[:self.mu]].T @ self.weights\n            \n            # Update step size\n            self.sigma *= np.exp((self.cs / self.damps) * (np.linalg.norm(self.p_sigma) / self.chiN - 1))\n\n            # Update covariance matrix\n            self.p_c = (1 - self.c_cov1) * self.p_c + (self.c_cov1 * (2 - self.c_cov1))**0.5 * (xmean - self.m) / self.sigma\n            self.C = (1 - self.c_cov1 - self.c_covmu) * self.C + self.c_cov1 * np.outer(self.p_c, self.p_c)\n            \n            for k in range(self.mu):\n                 artmp = (y[k] / self.sigma).reshape(-1, 1)\n                 self.C += self.c_covmu * self.weights[k] * (artmp @ artmp.T)\n            \n            # Ensure covariance matrix is positive definite\n            self.C = np.triu(self.C) + np.triu(self.C, 1).T\n            \n            # Update mean\n            self.m = xmean.copy()\n            \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "In the code, line 30, in __call__, the following error occurred:\nValueError: operands could not be broadcast together with shapes (2,) (2,6) \nOn line: x = self.m + self.sigma * A @ z.T", "error": "In the code, line 30, in __call__, the following error occurred:\nValueError: operands could not be broadcast together with shapes (2,) (2,6) \nOn line: x = self.m + self.sigma * A @ z.T", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "61e15fee-c47f-4954-8e65-7e3668217854", "fitness": 0.0, "name": "AdaptiveSimulatedAnnealing", "description": "No description provided.", "code": "import numpy as np\n\nclass AdaptiveSimulatedAnnealing:\n    def __init__(self, budget=10000, dim=10, temp_init=100.0, alpha=0.99, restarts=5):\n        self.budget = budget\n        self.dim = dim\n        self.temp_init = temp_init\n        self.alpha = alpha\n        self.restarts = restarts\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n\n        for _ in range(self.restarts):\n            x = np.random.uniform(lb, ub, size=self.dim)\n            f = func(x)\n            eval_count = 1\n\n            temp = self.temp_init\n\n            while eval_count < self.budget:\n                x_new = x + np.random.normal(0, temp**0.5, size=self.dim)\n                x_new = np.clip(x_new, lb, ub)\n                f_new = func(x_new)\n                eval_count += 1\n\n                delta_e = f_new - f\n\n                if delta_e < 0 or np.random.rand() < np.exp(-delta_e / temp):\n                    x = x_new\n                    f = f_new\n\n                if f < self.f_opt:\n                    self.f_opt = f\n                    self.x_opt = x\n\n                temp *= self.alpha\n                if temp < 1e-5:\n                    temp = self.temp_init\n                    x = np.random.uniform(lb, ub, size=self.dim)\n                    f = func(x)\n                    eval_count += 1\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveSimulatedAnnealing scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0]}, "task_prompt": ""}
{"id": "bceab1dc-6441-419d-9dff-c677f51e3a3e", "fitness": 0.2814579847186355, "name": "AdaptiveSimulatedAnnealing", "description": "No description provided.", "code": "import numpy as np\n\nclass AdaptiveSimulatedAnnealing:\n    def __init__(self, budget=10000, dim=10, initial_temp=100.0, alpha=0.95, step_size=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.initial_temp = initial_temp\n        self.alpha = alpha\n        self.step_size = step_size\n        self.lb = -5.0\n        self.ub = 5.0\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        x = np.random.uniform(self.lb, self.ub, size=self.dim)\n        f = func(x)\n        self.f_opt = f\n        self.x_opt = x\n        \n        temp = self.initial_temp\n        eval_count = 1\n        \n        acceptance_count = 0\n        \n        while eval_count < self.budget:\n            x_new = x + np.random.normal(0, self.step_size, size=self.dim)\n            x_new = np.clip(x_new, self.lb, self.ub)\n            f_new = func(x_new)\n            eval_count += 1\n\n            delta_f = f_new - f\n            if delta_f < 0:\n                x = x_new\n                f = f_new\n                acceptance_count += 1\n                if f < self.f_opt:\n                    self.f_opt = f\n                    self.x_opt = x\n            else:\n                probability = np.exp(-delta_f / temp)\n                if np.random.rand() < probability:\n                    x = x_new\n                    f = f_new\n                    acceptance_count += 1\n\n            # Adaptive temperature and step size\n            temp *= self.alpha\n            \n            if eval_count % 100 == 0:\n                acceptance_rate = acceptance_count / 100.0\n                if acceptance_rate > 0.6:\n                    self.step_size *= 1.1\n                elif acceptance_rate < 0.4:\n                    self.step_size *= 0.9\n                acceptance_count = 0\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveSimulatedAnnealing scored 0.281 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.03826690852603731, 0.16196292916176047, 0.6720626699075659, 0.14191527625254874, 0.12010861093204461, 0.14326116986777104, 0.171389277863626, 0.5762699761311977, 0.1437763587872527, 0.1333084805629653, 0.9260651177932614, 0.17215164708164699, 0.25007991912221406, 0.17075133650891672, 0.8993066756771776, 0.33027776223142935, 0.17825354908619118, 0.15406698634001803, 0.10330604593185833, 0.14257899660722617]}, "task_prompt": ""}
{"id": "22dfacce-320a-44c1-84f8-6963ffac727e", "fitness": "-inf", "name": "AdaptiveCMAES", "description": "No description provided.", "code": "import numpy as np\n\nclass AdaptiveCMAES:\n    def __init__(self, budget=10000, dim=10, pop_size=None, sigma=0.5, mu_factor=0.25, cs=0.3, damps=1.0, ccov1=None, ccovmu=None):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size if pop_size is not None else 4 + int(3 * np.log(dim))\n        self.sigma = sigma\n        self.mu = int(self.pop_size * mu_factor)\n        self.mu_weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n        self.mu_weights /= np.sum(self.mu_weights)\n        self.m = np.zeros(dim)\n        self.P_sigma = np.zeros(dim)\n        self.P_c = np.zeros(dim)\n        self.C = np.eye(dim)\n        self.c_sigma = (damps * (self.mu_weights[0] + self.dim / 3)) / (np.linalg.norm(self.P_sigma) ** 2 + damps) if np.linalg.norm(self.P_sigma) > 0 else cs #cs\n        self.damps = 1 + 2 * max(0, np.sqrt((self.mu_weights[0] + self.dim / 3 - 1) / (self.dim + 1)) - 1) + cs\n        self.chiN = dim**0.5 * (1 - 1/(4*dim) + 1/(21*dim**2))\n        self.ccov1 = 2 / ((dim + 1.3)**2 + self.mu) if ccov1 is None else ccov1\n        self.ccovmu = min(1-self.ccov1, 2 * (self.mu - 1 + 1/self.mu_weights[0]) / ((dim+2)**2 + 2*self.mu)) if ccovmu is None else ccovmu\n        self.B = None\n        self.D = None\n        self.restart_trigger = False\n        self.lb = -5.0\n        self.ub = 5.0\n\n    def sample_population(self, func):\n        z = np.random.randn(self.pop_size, self.dim)\n        y = self.B @ (self.D * z.T)\n        x = self.m + self.sigma * y.T\n        x = np.clip(x, self.lb, self.ub)\n        fitness = np.array([func(xi) for xi in x])\n        self.budget -= self.pop_size\n        return x, fitness\n\n    def update_distribution(self, x, fitness):\n        idx = np.argsort(fitness)\n        x_mu = x[idx[:self.mu]]\n        y_mu = (x_mu - self.m) @ self.B @ np.diag(1/self.D) / self.sigma\n        self.m = np.sum(self.mu_weights[:,None] * x_mu, axis=0)\n\n        # Cumulation\n        self.P_sigma = (1-self.c_sigma) * self.P_sigma + np.sqrt(self.c_sigma * (2 - self.c_sigma)) * (self.B @ y_mu.mean(axis=0))\n        hsig = (np.linalg.norm(self.P_sigma)/np.sqrt(1-(1-self.c_sigma)**(2*self.budget/self.pop_size))/self.chiN < 1.4 + 2/(self.dim+1))\n        self.P_c = (1-self.ccov1) * self.P_c + hsig * np.sqrt(self.ccov1 * (2 - self.ccov1)) * (x_mu.mean(axis=0) - self.m) / self.sigma\n\n        # Adaptation covariance matrix\n        C_temp = self.ccov1 * (self.P_c[:,None] @ self.P_c[None,:])\n        C_temp += self.ccovmu * (x_mu - self.m).T @ np.diag(self.mu_weights) @ (x_mu - self.m) / self.sigma**2\n        self.C = (1-self.ccov1-self.ccovmu) * self.C + C_temp\n\n        self.sigma *= np.exp((self.c_sigma/self.damps) * (np.linalg.norm(self.P_sigma)/self.chiN - 1))\n\n        self.D, self.B = np.linalg.eigh(self.C)\n        self.D = np.sqrt(np.maximum(self.D, 1e-16))\n\n    def check_restart(self, f_best_list, tolerance=1e-12, patience=1000):\n            if len(f_best_list) > patience:\n                recent_values = f_best_list[-patience:]\n                if np.std(recent_values) < tolerance:\n                    return True\n            return False\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.m = np.random.uniform(self.lb, self.ub, size=self.dim) # Initialize mean vector\n        self.B, self.D = np.linalg.eig(self.C)\n        self.D = np.sqrt(np.maximum(self.D, 1e-16)) # Ensure positivity\n        \n        f_best_list = []\n        generation = 0\n        \n        while self.budget > 0:\n            x, fitness = self.sample_population(func)\n            \n            best_idx = np.argmin(fitness)\n            if fitness[best_idx] < self.f_opt:\n                self.f_opt = fitness[best_idx]\n                self.x_opt = x[best_idx].copy()\n                \n            f_best_list.append(self.f_opt)\n\n            self.update_distribution(x, fitness)\n\n            if self.check_restart(f_best_list):\n                self.m = np.random.uniform(self.lb, self.ub, size=self.dim)\n                self.sigma = 0.5\n                self.P_sigma = np.zeros(self.dim)\n                self.P_c = np.zeros(self.dim)\n                self.C = np.eye(self.dim)\n                self.B, self.D = np.linalg.eig(self.C)\n                self.D = np.sqrt(np.maximum(self.D, 1e-16))\n                f_best_list = []\n                \n\n            generation+=1\n            \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "In the code, line 29, in sample_population, the following error occurred:\nValueError: operands could not be broadcast together with shapes (2,2) (2,6) \nOn line: y = self.B @ (self.D * z.T)", "error": "In the code, line 29, in sample_population, the following error occurred:\nValueError: operands could not be broadcast together with shapes (2,2) (2,6) \nOn line: y = self.B @ (self.D * z.T)", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "44d84934-3514-4901-83ba-24c79b59c1a8", "fitness": 0.29235552116904556, "name": "AdaptiveScatterSearch", "description": "No description provided.", "code": "import numpy as np\n\nclass AdaptiveScatterSearch:\n    def __init__(self, budget=10000, dim=10, pop_size=50, subset_size=10, diversification_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.subset_size = subset_size\n        self.diversification_rate = diversification_rate\n        self.lb = -5.0\n        self.ub = 5.0\n        self.population = None\n        self.fitness = None\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.evals = 0\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.pop_size\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n\n    def refine_subset(self, func):\n        indices = np.argsort(self.fitness)[:self.subset_size]\n        subset = self.population[indices]\n        centroid = np.mean(subset, axis=0)\n\n        for i in range(self.pop_size):\n            if self.evals >= self.budget:\n                break\n            \n            # Move towards centroid with probability, diversify otherwise\n            if np.random.rand() > self.diversification_rate:\n                direction = centroid - self.population[i]\n                step_size = np.random.rand()  # Adaptive step size\n                new_x = self.population[i] + step_size * direction\n            else:\n                new_x = np.random.uniform(self.lb, self.ub, size=self.dim)\n\n            new_x = np.clip(new_x, self.lb, self.ub)\n            new_f = func(new_x)\n            self.evals += 1\n\n            if new_f < self.fitness[i]:\n                self.population[i] = new_x\n                self.fitness[i] = new_f\n\n                if new_f < self.f_opt:\n                    self.f_opt = new_f\n                    self.x_opt = new_x\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.evals < self.budget:\n            self.refine_subset(func)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveScatterSearch scored 0.292 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.11598682861133724, 0.18733878390348668, 0.3085991553424362, 0.20984998617761363, 0.19259860586576671, 0.24849043357083456, 0.2266790038306854, 0.24428002034712393, 0.20293173604984938, 0.17119378053910228, 0.21634370359187216, 0.9870011558168922, 0.2525202444609813, 0.19927831496670267, 0.6887766194792682, 0.28435452690294405, 0.2362122843702419, 0.22092215794477743, 0.20297656576214507, 0.4507765158468504]}, "task_prompt": ""}
{"id": "f6894786-fb45-4de9-be3c-ce41b71d9f0b", "fitness": 0.2838971787782863, "name": "AdaptiveSA", "description": "No description provided.", "code": "import numpy as np\n\nclass AdaptiveSA:\n    def __init__(self, budget=10000, dim=10, initial_temp=100.0, cooling_rate=0.95, restart_prob=0.05):\n        self.budget = budget\n        self.dim = dim\n        self.initial_temp = initial_temp\n        self.cooling_rate = cooling_rate\n        self.restart_prob = restart_prob\n        self.lb = -5.0\n        self.ub = 5.0\n\n    def __call__(self, func):\n        self.x_opt = np.random.uniform(self.lb, self.ub, size=self.dim)\n        self.f_opt = func(self.x_opt)\n        eval_count = 1\n        temperature = self.initial_temp\n\n        while eval_count < self.budget:\n            x_new = self.x_opt + np.random.normal(0, temperature/self.initial_temp, size=self.dim)\n            x_new = np.clip(x_new, self.lb, self.ub)\n            f_new = func(x_new)\n            eval_count += 1\n\n            delta_e = f_new - self.f_opt\n            if delta_e < 0 or np.random.rand() < np.exp(-delta_e / temperature):\n                self.x_opt = x_new\n                self.f_opt = f_new\n\n            temperature *= self.cooling_rate\n\n            if np.random.rand() < self.restart_prob:\n                self.x_opt = np.random.uniform(self.lb, self.ub, size=self.dim)\n                self.f_opt = func(self.x_opt)\n                eval_count += 1\n                temperature = self.initial_temp\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveSA scored 0.284 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.12414019073995408, 0.2150460121709633, 0.2571165610266928, 0.20549377409237035, 0.17312212832446527, 0.2287667573856098, 0.22823426957705983, 0.22857951138477106, 0.2024605632354659, 0.1547240788518477, 0.20814474455556942, 0.9883431552603833, 0.2789105879702912, 0.21234212977056977, 0.5640689980552425, 0.2841376434849331, 0.22792758516835587, 0.2791740125972104, 0.1607916878631569, 0.45641918405081183]}, "task_prompt": ""}
{"id": "eda59a2e-2bfa-4775-a581-40bc2e0e122f", "fitness": 0.1452064367262411, "name": "BudgetAwareCMAES", "description": "No description provided.", "code": "import numpy as np\n\nclass BudgetAwareCMAES:\n    def __init__(self, budget=10000, dim=10, initial_pop_size=None, restart_threshold=1000):\n        self.budget = budget\n        self.dim = dim\n        self.mu = 0.5 # Proportion of top individuals to use for recombination\n        self.initial_pop_size = initial_pop_size if initial_pop_size is not None else 4 + int(3 * np.log(self.dim))\n        self.pop_size = self.initial_pop_size\n        self.mean = np.zeros(self.dim)\n        self.sigma = 0.5\n        self.C = np.eye(self.dim)  # Covariance matrix\n        self.pc = np.zeros(self.dim)  # Evolution path for C\n        self.ps = np.zeros(self.dim)  # Evolution path for sigma\n        self.chiN = self.dim**0.5 * (1 - 1/(4*self.dim) + 1/(21*self.dim**2))\n        self.c_sigma = (self.mu * 2) / (self.dim + 2)**2  # Learning rate for sigma\n        self.c_c = (4 + self.mu/self.dim) / (self.dim + 4) # Learning rate for C\n        self.d_sigma = 1 + 2 * max(0, np.sqrt((self.mu - 1)/(self.dim + 1)) - 1) + self.c_sigma # Damping for sigma\n        self.mu_eff = np.sum(self.mu)**2 / np.sum(self.mu**2)\n        self.c1 = 2 / ((self.dim + 1.3)**2 + self.mu_eff)\n        self.cmu = min(1 - self.c1, 2 * (self.mu_eff - 2 + 1/self.mu_eff) / ((self.dim + 2)**2 + self.mu_eff))\n        self.weights = np.log(self.mu+1/2) - np.log(np.arange(1, int(self.mu+1)))\n        self.weights = self.weights / np.sum(self.weights)\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.restart_threshold = restart_threshold\n        self.stagnation_counter = 0\n        self.last_improvement = 0\n        self.lb = -5.0\n        self.ub = 5.0\n\n    def __call__(self, func):\n        generation = 0\n\n        while self.budget > 0:\n            # Sample population\n            z = np.random.normal(0, 1, size=(self.pop_size, self.dim))\n            population = self.mean + self.sigma * z @ np.linalg.cholesky(self.C).T\n            population = np.clip(population, self.lb, self.ub)\n            \n            # Evaluate population\n            fitness = np.array([func(x) for x in population])\n            self.budget -= self.pop_size\n\n            # Sort population\n            indices = np.argsort(fitness)\n            fitness = fitness[indices]\n            population = population[indices]\n\n            # Update best solution\n            if fitness[0] < self.f_opt:\n                self.f_opt = fitness[0]\n                self.x_opt = population[0].copy()\n                self.stagnation_counter = 0\n                self.last_improvement = generation\n            else:\n                self.stagnation_counter += 1\n\n            # Recombination\n            mean_old = self.mean.copy()\n            self.mean = np.sum(self.weights[:, None] * population[:int(self.mu)], axis=0)\n\n            # Update evolution paths\n            B = np.linalg.cholesky(self.C)\n            z_mean = (self.mean - mean_old) / self.sigma @ np.linalg.inv(B).T\n            self.ps = (1 - self.c_sigma) * self.ps + np.sqrt(self.c_sigma * (2 - self.c_sigma)) * z_mean\n            \n            if np.sum(self.ps**2) / self.dim < 2 + 4/(self.dim + 1):\n                self.pc = (1 - self.c_c) * self.pc + np.sqrt(self.c_c * (2 - self.c_c)) * (self.mean - mean_old) / self.sigma\n                hsig = 1\n            else:\n                self.pc = (1 - self.c_c) * self.pc\n                hsig = 0\n\n            # Update covariance matrix\n            dC = np.sum(self.weights[:, None] * (population[:int(self.mu)] - mean_old) * (population[:int(self.mu)] - mean_old), axis=0)\n            self.C = (1 - self.c1 - self.cmu) * self.C + self.c1 * (self.pc[:, None] * self.pc)\n            self.C += self.cmu * (population[:int(self.mu)] - mean_old).T @ np.diag(self.weights) @ (population[:int(self.mu)] - mean_old)\n\n            # Update step size\n            self.sigma = self.sigma * np.exp((self.c_sigma / self.d_sigma) * (np.linalg.norm(self.ps) / self.chiN - 1))\n\n            # Dynamic Population Size Adjustment\n            remaining_budget_ratio = self.budget / 10000 # Assuming initial budget is 10000\n            improvement_ratio = (generation - self.last_improvement) / (generation + 1e-9)\n\n            if remaining_budget_ratio < 0.5 and improvement_ratio < 0.2:\n                self.pop_size = max(4, int(self.pop_size * 0.8))\n            elif remaining_budget_ratio > 0.75 and improvement_ratio > 0.5:\n                self.pop_size = min(self.initial_pop_size, int(self.pop_size * 1.2))\n            \n            self.pop_size = int(min(self.pop_size, self.budget))\n            \n            #Stagnation Check and Restart\n            if self.stagnation_counter > self.restart_threshold:\n                self.mean = np.zeros(self.dim)\n                self.sigma = 0.5\n                self.C = np.eye(self.dim)\n                self.pc = np.zeros(self.dim)\n                self.ps = np.zeros(self.dim)\n                self.stagnation_counter = 0\n                self.last_improvement = generation\n            \n            generation += 1\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm BudgetAwareCMAES scored 0.145 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.047693590206566494, 0.12803561390021478, 0.26843892082959453, 0.09753943432472001, 0.1043478907733475, 0.1455111526273849, 0.16822816367468985, 0.13444685448338622, 0.1435667054923213, 0.1402571868229483, 0.15357837900719373, 0.1703701792032467, 0.2480964092413569, 0.09941144072861385, 0.11297552234791963, 0.20911105367830174, 0.12261790603532785, 0.1413017725548692, 0.12341898893749093, 0.1451815696553278]}, "task_prompt": ""}
{"id": "f1267997-db45-4c31-bf9e-075dfc6ddee1", "fitness": 0.7443265658769684, "name": "EnhancedDifferentialEvolution", "description": "No description provided.", "code": "import numpy as np\n\nclass EnhancedDifferentialEvolution:\n    def __init__(self, budget=10000, dim=10, pop_size=20, F=0.5, CR=0.7, restart_trigger=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F  # Differential weight\n        self.CR = CR  # Crossover rate\n        self.restart_trigger = restart_trigger  # Percentage of budget used before considering a restart\n        self.initial_F = F\n        self.initial_CR = CR\n        self.restart_count = 0\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        \n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index].copy()\n        \n        initial_budget = self.budget + self.pop_size # to compute restart threshold\n\n        while self.budget > 0:\n            new_population = np.zeros((self.pop_size, self.dim))\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = np.random.choice(idxs, 3, replace=False)\n                mutant = population[a] + self.F * (population[b] - population[c])\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                new_population[i] = trial\n\n            new_fitness = np.array([func(x) for x in new_population])\n            self.budget -= self.pop_size\n\n            # Selection\n            for i in range(self.pop_size):\n                if new_fitness[i] < fitness[i]:\n                    fitness[i] = new_fitness[i]\n                    population[i] = new_population[i].copy()\n\n            # Update best solution\n            best_index = np.argmin(fitness)\n            if fitness[best_index] < self.f_opt:\n                self.f_opt = fitness[best_index]\n                self.x_opt = population[best_index].copy()\n\n            # Adapt parameters based on population diversity\n            diversity = np.std(population)\n            self.F = np.clip(self.initial_F + np.random.normal(0, 0.01 * diversity), 0.1, 0.9)\n            self.CR = np.clip(self.initial_CR + np.random.normal(0, 0.01 * diversity), 0.1, 0.9)\n\n            # Restart mechanism\n            if (initial_budget - self.budget) / initial_budget > self.restart_trigger:\n                if diversity < 0.01:  # If population has converged\n                    population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                    fitness = np.array([func(x) for x in population])\n                    self.budget -= self.pop_size\n                    best_index = np.argmin(fitness)\n                    self.f_opt = fitness[best_index]\n                    self.x_opt = population[best_index].copy()\n                    self.F = self.initial_F  # Reset parameters\n                    self.CR = self.initial_CR\n                    self.restart_count += 1\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm EnhancedDifferentialEvolution scored 0.744 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.38246141115280397, 0.7184012175020872, 0.7355423159996953, 0.9172668104905009, 0.8406443151747232, 0.8833346798007413, 0.7577808040122945, 0.7982682923214779, 0.8639816456669029, 0.8208100827630402, 0.8952030175787579, 0.9929354565203317, 0.29635813515721643, 0.8316151522144766, 0.9118553851342196, 0.8761272624038585, 0.8060769797404466, 0.6045759776657377, 0.34194224215562286, 0.6113501340844338]}, "task_prompt": ""}
{"id": "a3bd847d-afc7-4eb3-86d9-c3be5456e63d", "fitness": "-inf", "name": "CMAES", "description": "No description provided.", "code": "import numpy as np\nimport cma\n\nclass CMAES:\n    def __init__(self, budget=10000, dim=10, sigma=0.5):\n        self.budget = budget\n        self.dim = dim\n        self.sigma = sigma\n        self.lb = -5.0\n        self.ub = 5.0\n\n    def __call__(self, func):\n        es = cma.PureCMAES(mean=np.zeros(self.dim), sigma=self.sigma,\n                           bounds=[self.lb, self.ub],\n                           inopts={'maxfevals': self.budget, 'verbose': -9})  # Disable verbose output\n\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        while es.result.fbest == None or es.result.fbest > self.f_opt and es.countevals() < self.budget :\n             solutions = []\n             for candidate in es.ask():\n                value = func(candidate)\n                solutions.append(value)\n                if value < self.f_opt:\n                    self.f_opt = value\n                    self.x_opt = candidate\n             es.tell(es.ask(), solutions)  # pass the objective function values to update the distribution\n\n        if es.result.fbest < self.f_opt:\n             self.f_opt = es.result.fbest\n             self.x_opt = es.result.xbest\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "In the code, line 2, in <module>, the following error occurred:\nModuleNotFoundError: No module named 'cma'\nOn line: import cma", "error": "In the code, line 2, in <module>, the following error occurred:\nModuleNotFoundError: No module named 'cma'\nOn line: import cma", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "4690cbeb-a3df-4f48-b4c5-3c7e0dc26f39", "fitness": 0.531015242738248, "name": "AdaptiveDE_CMA", "description": "No description provided.", "code": "import numpy as np\n\nclass AdaptiveDE_CMA:\n    def __init__(self, budget=10000, dim=10, pop_size=None, F=0.5, CR=0.7, c_sigma=0.1, restart_threshold=1e-8):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size if pop_size is not None else 4 + int(3 * np.log(dim))\n        self.F = F\n        self.CR = CR\n        self.c_sigma = c_sigma\n        self.sigma = 0.1  # Initial step size\n        self.restart_threshold = restart_threshold\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        \n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index].copy()\n        \n        generation = 0\n\n        while self.budget > 0:\n            new_population = np.zeros((self.pop_size, self.dim))\n            \n            # Mutation and Crossover\n            for i in range(self.pop_size):\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = np.random.choice(idxs, 3, replace=False)\n                mutant = population[a] + self.F * (population[b] - population[c])\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                new_population[i] = trial\n\n            new_fitness = np.array([func(x) for x in new_population])\n            self.budget -= self.pop_size\n            \n            # Selection\n            improved = new_fitness < fitness\n            fitness[improved] = new_fitness[improved]\n            population[improved] = new_population[improved].copy()\n\n            # Update best solution\n            best_index = np.argmin(fitness)\n            if fitness[best_index] < self.f_opt:\n                self.f_opt = fitness[best_index]\n                self.x_opt = population[best_index].copy()\n\n            # Step size adaptation (CMA-ES inspired)\n            success_ratio = np.sum(improved) / self.pop_size\n            self.sigma *= np.exp(self.c_sigma * (success_ratio - 0.2))  # Simple rule\n\n            # Restart based on fitness variance\n            if np.var(fitness) < self.restart_threshold:\n                population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                fitness = np.array([func(x) for x in population])\n                self.budget -= self.pop_size\n                best_index = np.argmin(fitness)\n                if fitness[best_index] < self.f_opt:\n                    self.f_opt = fitness[best_index]\n                    self.x_opt = population[best_index].copy()\n                self.sigma = 0.1 #reset step size\n                \n            generation += 1\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveDE_CMA scored 0.531 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.2644572507947851, 0.2397966638002944, 0.40791588777320764, 0.9728937654851596, 0.5965804388866469, 0.5308856495650845, 0.5717360784323038, 0.4065633878917829, 0.5653793356866448, 0.434694571653925, 0.8434141200302184, 0.9979900565896813, 0.3775777872602998, 0.46809236460507864, 0.6894460470232755, 0.5331956748868772, 0.33968316721488667, 0.5542568397138742, 0.31434804045453946, 0.5113977270163941]}, "task_prompt": ""}
{"id": "e8f5e206-051c-4086-8ece-05e3a0c0a778", "fitness": "-inf", "name": "SelfAdaptiveCMAESDE", "description": "No description provided.", "code": "import numpy as np\n\nclass SelfAdaptiveCMAESDE:\n    def __init__(self, budget=10000, dim=10, pop_size=None, F=0.5, CR=0.7, mu=None):\n        self.budget = budget\n        self.dim = dim\n        self.F = F\n        self.CR = CR\n        if pop_size is None:\n            self.pop_size = 4 + int(3 * np.log(self.dim))  # reasonable default\n        else:\n            self.pop_size = pop_size\n        if mu is None:\n            self.mu = self.pop_size // 2\n        else:\n            self.mu = mu\n\n        self.C = np.eye(self.dim)  # Covariance matrix\n        self.pc = np.zeros(self.dim) # evolution path for C\n        self.ps = np.zeros(self.dim) # evolution path for sigma\n        self.sigma = 0.3 # overall standard deviation\n        self.damps = 1 + 2 * max(0, np.sqrt((self.mu - 1) / (self.dim + 1)) - 1) + self.CR # damping for sigma\n        self.cc = (4 + self.mu / self.dim) / (self.dim + 4 + 2 * self.mu / self.dim)\n        self.cs = (self.mu + 2) / (self.dim + self.mu + 5)\n        self.c1 = 2 / ((self.dim + 1.3)**2 + self.mu)\n        self.cmu = min(1 - self.c1, 2 * (self.mu - 1 + 1/self.mu) / ((self.dim + 2)**2 + self.mu))\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index].copy()\n\n        while self.budget > 0:\n            # Generate offspring\n            z = np.random.normal(0, 1, size=(self.pop_size, self.dim))\n            population = self.x_opt + self.sigma * np.dot(z, np.linalg.cholesky(self.C).T)\n            population = np.clip(population, func.bounds.lb, func.bounds.ub)\n\n            fitness = np.array([func(x) for x in population])\n            self.budget -= self.pop_size\n\n            # Selection and update best\n            best_index = np.argmin(fitness)\n            if fitness[best_index] < self.f_opt:\n                self.f_opt = fitness[best_index]\n                self.x_opt = population[best_index].copy()\n            \n            # Sort population and fitness\n            indices = np.argsort(fitness)\n            population = population[indices]\n            fitness = fitness[indices]\n\n            # Weighted recombination\n            weights = np.log(self.mu + 1) - np.log(np.arange(1, self.mu + 1))\n            weights /= np.sum(weights)\n\n            y = population[:self.mu] - self.x_opt\n            z = np.dot(y.T, weights)\n\n            # Update evolution paths\n            self.ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs)) * z / self.sigma\n            self.pc = (1 - self.cc) * self.pc + np.sqrt(self.cc * (2 - self.cc)) * z\n\n            # Update covariance matrix\n            self.C = (1 - self.c1 - self.cmu) * self.C + self.c1 * np.outer(self.pc, self.pc)\n            self.C += self.cmu * np.dot((weights * y).T, y) / self.sigma**2\n\n            # Update step size\n            self.sigma *= np.exp((self.cs / self.damps) * (np.linalg.norm(self.ps) / np.sqrt(self.dim) - 1))\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "In the code, line 73, in __call__, the following error occurred:\nValueError: operands could not be broadcast together with shapes (3,) (3,2) \nOn line: self.C += self.cmu * np.dot((weights * y).T, y) / self.sigma**2", "error": "In the code, line 73, in __call__, the following error occurred:\nValueError: operands could not be broadcast together with shapes (3,) (3,2) \nOn line: self.C += self.cmu * np.dot((weights * y).T, y) / self.sigma**2", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "e83ae022-3cc3-48e8-93d5-61d3f807c0d1", "fitness": "-inf", "name": "HybridDECMAS", "description": "No description provided.", "code": "import numpy as np\nfrom scipy.optimize import minimize\nimport cma\n\nclass HybridDECMAS:\n    def __init__(self, budget=10000, dim=10, pop_size=15, F=0.5, CR=0.7, cma_sigma=0.5, switch_interval=500):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.lb = -5.0\n        self.ub = 5.0\n        self.population = None\n        self.fitness = None\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.evals = 0\n        self.cma_sigma = cma_sigma\n        self.switch_interval = switch_interval\n        self.de_phase = True\n        self.cma_es = None\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals += self.pop_size\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n\n    def mutate(self, pop, i):\n        idxs = [idx for idx in range(self.pop_size) if idx != i]\n        a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n        return np.clip(a + self.F * (b - c), self.lb, self.ub)\n\n    def crossover(self, mutant, target):\n        cross_points = np.random.rand(self.dim) < self.CR\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        return np.where(cross_points, mutant, target)\n\n    def de_step(self, func):\n        new_population = []\n        new_fitness = []\n\n        for i in range(self.pop_size):\n            if self.evals >= self.budget:\n                new_population.append(self.population[i])\n                new_fitness.append(self.fitness[i])\n                continue\n\n            mutant = self.mutate(self.population, i)\n            trial = self.crossover(mutant, self.population[i])\n            f = func(trial)\n            self.evals += 1\n\n            if f < self.fitness[i]:\n                new_population.append(trial)\n                new_fitness.append(f)\n\n                if f < self.f_opt:\n                    self.f_opt = f\n                    self.x_opt = trial\n            else:\n                new_population.append(self.population[i])\n                new_fitness.append(self.fitness[i])\n        \n        self.population = np.array(new_population)\n        self.fitness = np.array(new_fitness)\n\n    def cma_es_step(self, func):\n        if self.cma_es is None:\n            self.cma_es = cma.optimization_tools.ES(self.x_opt, self.cma_sigma,\n                                                     {'bounds': [self.lb, self.ub], 'verbose': -9})\n\n        solutions = []\n        for _ in range(self.pop_size):\n            if self.evals >= self.budget:\n                break\n            x = self.cma_es.ask()\n            f = func(x)\n            self.evals += 1\n            solutions.append((x,f))\n\n            if f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = x\n        \n        if solutions:\n           self.cma_es.tell(solutions)\n           self.cma_es.logger.add()\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        iteration = 0\n        while self.evals < self.budget:\n            iteration += 1\n            if self.de_phase:\n                self.de_step(func)\n            else:\n                self.cma_es_step(func)\n\n            if iteration % self.switch_interval == 0:\n                self.de_phase = not self.de_phase\n                if not self.de_phase:\n                  self.cma_es = None # Reset CMA-ES for a fresh start\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "In the code, line 3, in <module>, the following error occurred:\nModuleNotFoundError: No module named 'cma'\nOn line: import cma", "error": "In the code, line 3, in <module>, the following error occurred:\nModuleNotFoundError: No module named 'cma'\nOn line: import cma", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "31b30eec-82b0-460a-9289-7122292445b0", "fitness": "-inf", "name": "CMAES", "description": "No description provided.", "code": "import numpy as np\nimport cma\n\nclass CMAES:\n    def __init__(self, budget=10000, dim=10, sigma0=0.5, popsize=None, restarts=3):\n        self.budget = budget\n        self.dim = dim\n        self.sigma0 = sigma0\n        self.restarts = restarts\n        if popsize is None:\n            self.popsize = 4 + int(3 * np.log(self.dim))\n        else:\n            self.popsize = popsize\n        self.lb = -5.0\n        self.ub = 5.0\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        evals = 0\n        \n        for r in range(self.restarts):\n            x0 = np.random.uniform(self.lb, self.ub, size=self.dim)\n            es = cma.PureCMAES(x0, self.sigma0, \n                               {'bounds': [self.lb, self.ub], \n                                'popsize': self.popsize,\n                                'maxfevals': (self.budget // self.restarts) if r < self.restarts -1 else (self.budget - evals),\n                                'verbose': -9})\n\n            while es.result is None and evals < self.budget:\n                solutions = []\n                for i in range(es.population_size):\n                    x = es.ask()\n                    f = func(x)\n                    solutions.append((x, f))\n                    evals += 1\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = x\n                es.tell(solutions)\n                \n            if evals >= self.budget:\n                break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "In the code, line 2, in <module>, the following error occurred:\nModuleNotFoundError: No module named 'cma'\nOn line: import cma", "error": "In the code, line 2, in <module>, the following error occurred:\nModuleNotFoundError: No module named 'cma'\nOn line: import cma", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "ac11f0a8-79ae-4881-86b7-a0885e9ee031", "fitness": 0.36712543923420093, "name": "DynamicDE", "description": "No description provided.", "code": "import numpy as np\n\nclass DynamicDE:\n    def __init__(self, budget=10000, dim=10, initial_pop_size=50, F=0.7, CR=0.8, reduction_factor=0.9, restart_prob=0.05):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = initial_pop_size\n        self.initial_pop_size = initial_pop_size\n        self.F = F\n        self.CR = CR\n        self.lb = -5.0\n        self.ub = 5.0\n        self.population = None\n        self.fitness = None\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.evals = 0\n        self.reduction_factor = reduction_factor\n        self.restart_prob = restart_prob\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals += self.pop_size\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n\n    def mutate(self, pop, i):\n        idxs = [idx for idx in range(self.pop_size) if idx != i]\n        a, b, c = pop[np.random.choice(idxs, 3, replace=False)]\n        return np.clip(a + self.F * (b - c), self.lb, self.ub)\n\n    def crossover(self, mutant, target):\n        cross_points = np.random.rand(self.dim) < self.CR\n        if not np.any(cross_points):\n            cross_points[np.random.randint(0, self.dim)] = True\n        return np.where(cross_points, mutant, target)\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.evals < self.budget:\n            new_population = []\n            new_fitness = []\n\n            for i in range(self.pop_size):\n                if self.evals >= self.budget:\n                    break\n\n                mutant = self.mutate(self.population, i)\n                trial = self.crossover(mutant, self.population[i])\n                f = func(trial)\n                self.evals += 1\n\n                if f < self.fitness[i]:\n                    new_population.append(trial)\n                    new_fitness.append(f)\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = trial\n                    self.F = np.clip(self.F * np.random.normal(1, 0.1), 0.1, 0.9)\n                    self.CR = np.clip(self.CR * np.random.normal(1, 0.1), 0.1, 0.9)\n                else:\n                    new_population.append(self.population[i])\n                    new_fitness.append(self.fitness[i])\n\n            self.population = np.array(new_population)\n            self.fitness = np.array(new_fitness)\n\n            if np.random.rand() < self.restart_prob:\n                self.pop_size = self.initial_pop_size\n                self.initialize_population(func)\n            elif self.evals < self.budget // 2 and len(new_population) < self.pop_size // 4:\n                self.pop_size = int(self.pop_size * self.reduction_factor)\n                if self.pop_size < 5:\n                  self.pop_size = 5\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm DynamicDE scored 0.367 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.16129434022258882, 0.2533106263561661, 0.3578870825221636, 0.3523767792841739, 0.3009297876243666, 0.35761819426781016, 0.29610870607103346, 0.31218247463050564, 0.2671183951441911, 0.17743940326513674, 0.2931403778369146, 0.9884620210743832, 0.2511049023212506, 0.27897744793426316, 0.7198196960313175, 0.38672735977203276, 0.32946055472728697, 0.576492015438117, 0.18956042376002857, 0.49249819640028836]}, "task_prompt": ""}
{"id": "af1169d0-3ac2-4ddc-9480-96df864ed578", "fitness": 0.5099154155985675, "name": "HybridPSO_NM", "description": "No description provided.", "code": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridPSO_NM:\n    def __init__(self, budget=10000, dim=10, pop_size=20, w=0.7, c1=1.4, c2=1.4, stagnation_threshold=500, nm_freq=500):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.w = w  # Inertia weight\n        self.c1 = c1  # Cognitive coefficient\n        self.c2 = c2  # Social coefficient\n        self.stagnation_threshold = stagnation_threshold\n        self.stagnation_counter = 0\n        self.nm_freq = nm_freq\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        # Initialize particles\n        particles = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        velocities = np.random.uniform(-1, 1, size=(self.pop_size, self.dim))  # Initialize velocities\n        fitness = np.array([func(x) for x in particles])\n        self.budget -= self.pop_size\n\n        personal_best_positions = particles.copy()\n        personal_best_fitness = fitness.copy()\n\n        global_best_index = np.argmin(fitness)\n        global_best_position = particles[global_best_index].copy()\n        self.f_opt = fitness[global_best_index]\n        self.x_opt = global_best_position.copy()\n        generation = 0\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Update velocity\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                velocities[i] = self.w * velocities[i] + \\\n                                self.c1 * r1 * (personal_best_positions[i] - particles[i]) + \\\n                                self.c2 * r2 * (global_best_position - particles[i])\n\n                # Update position\n                particles[i] = particles[i] + velocities[i]\n                particles[i] = np.clip(particles[i], func.bounds.lb, func.bounds.ub)\n\n                # Evaluate fitness\n                new_fitness = func(particles[i])\n                self.budget -= 1\n                if self.budget <= 0:\n                    break\n                \n                # Update personal best\n                if new_fitness < personal_best_fitness[i]:\n                    personal_best_fitness[i] = new_fitness\n                    personal_best_positions[i] = particles[i].copy()\n\n                # Update global best\n                if new_fitness < self.f_opt:\n                    self.f_opt = new_fitness\n                    self.x_opt = particles[i].copy()\n                    global_best_position = particles[i].copy()\n                    self.stagnation_counter = 0\n                else:\n                    self.stagnation_counter += 1\n\n            if self.budget <= 0:\n                break\n            # Nelder-Mead local search\n            if generation > 0 and generation % self.nm_freq == 0:\n                # Apply Nelder-Mead around the global best\n                nm_result = minimize(func, self.x_opt, method='Nelder-Mead',\n                                    bounds=np.stack((func.bounds.lb * np.ones(self.dim), func.bounds.ub * np.ones(self.dim)), axis=-1),\n                                    options={'maxfev': min(self.budget // 2, 500)})\n                \n                if nm_result.fun < self.f_opt:\n                    self.f_opt = nm_result.fun\n                    self.x_opt = nm_result.x.copy()\n                    global_best_position = nm_result.x.copy()\n                    self.stagnation_counter = 0  # Reset stagnation counter\n                self.budget -= nm_result.nfev\n\n            # Stagnation check and adjust parameters\n            if self.stagnation_counter > self.stagnation_threshold:\n                # Perturb particles to escape local optima\n                for i in range(self.pop_size):\n                    particles[i] = np.random.uniform(func.bounds.lb, func.bounds.ub)\n                fitness = np.array([func(x) for x in particles])\n                self.budget -= self.pop_size\n                \n                personal_best_positions = particles.copy()\n                personal_best_fitness = fitness.copy()\n\n                global_best_index = np.argmin(fitness)\n                global_best_position = particles[global_best_index].copy()\n                if fitness[global_best_index] < self.f_opt:\n                    self.f_opt = fitness[global_best_index]\n                    self.x_opt = global_best_position.copy()\n\n                self.stagnation_counter = 0  # Reset counter\n            generation += 1\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm HybridPSO_NM scored 0.510 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.20636222538689342, 0.4202842576763144, 0.880751501537552, 0.259758968686538, 0.29119836858799975, 0.9169754628458768, 0.3211981073235026, 0.23696213439761182, 0.34965302580828816, 0.8166398515898631, 0.8419812185045926, 0.9984750877920245, 0.3074570898569846, 0.29411493480139317, 0.5861855837140786, 0.9090135167107215, 0.41524473505764503, 0.4487829205113123, 0.19734985756800072, 0.49991946361415296]}, "task_prompt": ""}
{"id": "cb560127-becd-462e-b122-c2f08f1bc2d8", "fitness": "-inf", "name": "GaussianProcessOptimization", "description": "No description provided.", "code": "import numpy as np\nfrom sklearn.gaussian_process import GaussianProcessRegressor\nfrom sklearn.gaussian_process.kernels import RBF, ConstantKernel as C\n\nclass GaussianProcessOptimization:\n    def __init__(self, budget=10000, dim=10, n_initial_samples=10):\n        self.budget = budget\n        self.dim = dim\n        self.n_initial_samples = n_initial_samples\n        self.kernel = C(1.0, (1e-3, 1e3)) * RBF(1.0, (1e-2, 1e2))\n        self.gp = GaussianProcessRegressor(kernel=self.kernel, n_restarts_optimizer=10)\n        self.X = None\n        self.Y = None\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def acquisition_function(self, x, gp, xi=0.01):\n        mu, sigma = gp.predict(x.reshape(1, -1), return_std=True)\n        return mu - xi * sigma\n\n    def __call__(self, func):\n\n        # Initial sampling\n        X_initial = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.n_initial_samples, self.dim))\n        Y_initial = np.array([func(x) for x in X_initial])\n        self.budget -= self.n_initial_samples\n\n        best_index = np.argmin(Y_initial)\n        self.f_opt = Y_initial[best_index]\n        self.x_opt = X_initial[best_index].copy()\n        self.X = X_initial\n        self.Y = Y_initial\n\n        # Optimization loop\n        while self.budget > 0:\n            # Fit Gaussian process\n            self.gp.fit(self.X, self.Y)\n\n            # Find next point to evaluate by maximizing the acquisition function\n            x_next = self.find_next_point(func)\n\n            # Evaluate the function at the new point\n            y_next = func(x_next)\n            self.budget -= 1\n\n            # Update data\n            self.X = np.vstack((self.X, x_next))\n            self.Y = np.append(self.Y, y_next)\n\n            # Update best solution\n            if y_next < self.f_opt:\n                self.f_opt = y_next\n                self.x_opt = x_next.copy()\n\n        return self.f_opt, self.x_opt\n\n    def find_next_point(self, func, n_sample=1000):\n        \"\"\"\n        Find the next point to sample by maximizing the acquisition function.\n        \"\"\"\n        X_sample = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(n_sample, self.dim))\n        acq_values = np.array([self.acquisition_function(x, self.gp) for x in X_sample])\n        \n        # Return best found point\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "In the code, line 10, in __init__, the following error occurred:\nNameError: name 'C' is not defined\nOn line: self.kernel = C(1.0, (1e-3, 1e3)) * RBF(1.0, (1e-2, 1e2))", "error": "In the code, line 10, in __init__, the following error occurred:\nNameError: name 'C' is not defined\nOn line: self.kernel = C(1.0, (1e-3, 1e3)) * RBF(1.0, (1e-2, 1e2))", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "ea4323fe-3a96-4308-91db-7ab7d68c64bf", "fitness": 0.0, "name": "HybridPSO", "description": "No description provided.", "code": "import numpy as np\n\nclass HybridPSO:\n    def __init__(self, budget=10000, dim=10, pop_size=20, w=0.7, c1=1.5, c2=1.5, local_search_prob=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.w = w  # Inertia weight\n        self.c1 = c1  # Cognitive coefficient\n        self.c2 = c2  # Social coefficient\n        self.local_search_prob = local_search_prob\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        # Initialize population and velocities\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        velocities = np.random.uniform(-1, 1, size=(self.pop_size, self.dim))\n        \n        # Evaluate initial population\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n        \n        # Initialize personal best positions and fitnesses\n        personal_best_positions = population.copy()\n        personal_best_fitness = fitness.copy()\n        \n        # Find global best\n        global_best_index = np.argmin(fitness)\n        global_best_position = population[global_best_index].copy()\n        self.f_opt = fitness[global_best_index]\n        self.x_opt = global_best_position.copy()\n        \n\n        while self.budget > 0:\n            # Update velocities and positions\n            for i in range(self.pop_size):\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                \n                velocities[i] = self.w * velocities[i] + \\\n                                self.c1 * r1 * (personal_best_positions[i] - population[i]) + \\\n                                self.c2 * r2 * (global_best_position - population[i])\n                \n                population[i] = population[i] + velocities[i]\n                population[i] = np.clip(population[i], func.bounds.lb, func.bounds.ub)\n                \n                # Local search with a probability\n                if np.random.rand() < self.local_search_prob:\n                    # Perform a small random perturbation\n                    perturbation = np.random.normal(0, 0.05, size=self.dim)\n                    local_point = population[i] + perturbation\n                    local_point = np.clip(local_point, func.bounds.lb, func.bounds.ub)\n                    \n                    local_fitness = func(local_point)\n                    self.budget -= 1\n                    if local_fitness < fitness[i] and self.budget > 0:\n                        fitness[i] = local_fitness\n                        population[i] = local_point.copy()\n                        \n            # Evaluate updated population\n            new_fitness = np.array([func(x) for x in population])\n            self.budget -= self.pop_size\n            \n            # Update personal best positions\n            for i in range(self.pop_size):\n                if new_fitness[i] < personal_best_fitness[i]:\n                    personal_best_fitness[i] = new_fitness[i]\n                    personal_best_positions[i] = population[i].copy()\n            \n            # Update global best\n            global_best_index = np.argmin(personal_best_fitness)\n            if personal_best_fitness[global_best_index] < self.f_opt:\n                self.f_opt = personal_best_fitness[global_best_index]\n                global_best_position = personal_best_positions[global_best_index].copy()\n                self.x_opt = global_best_position.copy()\n\n            # Adaptive inertia weight adjustment\n            self.w = 0.9 - (0.5 / self.budget)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm HybridPSO scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0]}, "task_prompt": ""}
{"id": "86ac2e68-125e-488f-869e-c4c884d44260", "fitness": 0.32379729784572264, "name": "CMAES", "description": "No description provided.", "code": "import numpy as np\n\nclass CMAES:\n    def __init__(self, budget=10000, dim=10, pop_size=None, sigma=0.5):\n        self.budget = budget\n        self.dim = dim\n        self.sigma = sigma\n        self.pop_size = pop_size if pop_size is not None else 4 + int(3 * np.log(dim))\n        self.mu = self.pop_size // 2\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.m = None\n        self.C = None\n        self.pc = None\n        self.ps = None\n        self.chiN = dim**0.5 * (1 - 1/(4*dim) + 1/(21*dim**2))\n        self.c_sigma = (self.mu / (self.dim + (np.sqrt(self.mu) / self.sigma)))**2\n        self.d_sigma = 1 + 2*max(0, np.sqrt((self.mu-1)/(self.dim+1)) - 1) + self.c_sigma\n        self.c_c = (4 + self.mu/self.dim)/(self.dim + 4 + 2*self.mu/self.dim)\n        self.c_1 = 2 / ((self.dim + 1.3)**2 + self.mu)\n        self.c_mu = min(1 - self.c_1, 2 * (self.mu - 2 + 1/self.mu) / ((self.dim + 2)**2 + self.mu))\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        self.m = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n\n        while self.budget > 0:\n            # Sample population\n            z = np.random.normal(0, 1, size=(self.pop_size, self.dim))\n            B = np.linalg.cholesky(self.C) # Cholesky decomposition\n            x = self.m + self.sigma * z @ B.T\n            x = np.clip(x, func.bounds.lb, func.bounds.ub)\n\n            # Evaluate population\n            fitness = np.array([func(xi) for xi in x])\n            self.budget -= self.pop_size\n\n            # Sort population\n            idx = np.argsort(fitness)\n            x = x[idx]\n            fitness = fitness[idx]\n            \n            if fitness[0] < self.f_opt:\n                self.f_opt = fitness[0]\n                self.x_opt = x[0].copy()\n            \n            # Update mean\n            m_old = self.m.copy()\n            self.m = np.sum(self.weights[:, None] * x[:self.mu], axis=0)\n\n            # Update evolution path for covariance matrix\n            z_mean = np.mean(z[idx[:self.mu]], axis=0)\n            self.ps = (1 - self.c_sigma) * self.ps + np.sqrt(self.c_sigma * (2 - self.c_sigma)) * (B @ z_mean)\n            \n            # Adapt step size\n            norm_ps = np.linalg.norm(self.ps)\n            self.sigma *= np.exp((self.c_sigma / self.d_sigma) * (norm_ps / self.chiN - 1))\n\n            # Update evolution path for mean\n            self.pc = (1 - self.c_c) * self.pc + np.sqrt(self.c_c * (2 - self.c_c)) * (self.m - m_old) / self.sigma\n\n            # Update covariance matrix\n            self.C = (1 - self.c_1 - self.c_mu) * self.C + self.c_1 * np.outer(self.pc, self.pc)\n            for i in range(self.mu):\n                self.C += self.c_mu * self.weights[i] * np.outer((x[i] - m_old) / self.sigma, (x[i] - m_old) / self.sigma)\n            \n            self.C = np.triu(self.C) + np.triu(self.C, 1).T\n\n            # Keep covariance matrix positive definite\n            try:\n                np.linalg.cholesky(self.C)\n            except np.linalg.LinAlgError:\n                self.C = self.C + 1e-6 * np.eye(self.dim)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm CMAES scored 0.324 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.12614380860868346, 0.4126704532877399, 0.31614262888490186, 0.14349519837902258, 0.4437727046225829, 0.1364965729992056, 0.20969649986729189, 0.45062722010392686, 0.31235525717682233, 0.11991867503213072, 0.24818645378896398, 0.1824501701873209, 0.3633092867979393, 0.3914073004002421, 0.7635714156434898, 0.2710044393772143, 0.41873546245233706, 0.6383347910337653, 0.08252307891395816, 0.4451045393569141]}, "task_prompt": ""}
{"id": "db1c1b83-64ac-4322-874a-93f2b8f6732d", "fitness": 0.2863613430240516, "name": "GOBLRestart", "description": "No description provided.", "code": "import numpy as np\n\nclass GOBLRestart:\n    def __init__(self, budget=10000, dim=10, pop_size=20, exploration_rate=0.3, restart_threshold=0.8):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lb = -5.0\n        self.ub = 5.0\n        self.population = None\n        self.fitness = None\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.evals = 0\n        self.exploration_rate = exploration_rate\n        self.restart_threshold = restart_threshold\n        self.restart_count = 0\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals += self.pop_size\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n\n    def gaussian_mutation(self, individual):\n        mutation = np.random.normal(0, self.exploration_rate * (self.ub - self.lb), size=self.dim)\n        mutated_individual = np.clip(individual + mutation, self.lb, self.ub)\n        return mutated_individual\n\n    def opposition_based_learning(self, individual):\n        opposition = self.lb + self.ub - individual\n        return opposition\n\n    def restart_population(self, func):\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals += self.pop_size\n        best_index = np.argmin(self.fitness)\n        if self.fitness[best_index] < self.f_opt:\n            self.f_opt = self.fitness[best_index]\n            self.x_opt = self.population[best_index]\n        self.restart_count += 1\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.evals < self.budget:\n            new_population = []\n            new_fitness = []\n\n            for i in range(self.pop_size):\n                if self.evals >= self.budget:\n                  new_population.append(self.population[i])\n                  new_fitness.append(self.fitness[i])\n                  continue\n                mutated_individual = self.gaussian_mutation(self.population[i])\n                opposition_individual = self.opposition_based_learning(self.population[i])\n\n                f_mutated = func(mutated_individual)\n                self.evals += 1\n                if self.evals >= self.budget:\n                  new_population.append(self.population[i])\n                  new_fitness.append(self.fitness[i])\n                  continue\n                f_opposition = func(opposition_individual)\n                self.evals += 1\n                \n                if f_mutated < self.fitness[i] and f_mutated <= f_opposition:\n                    new_population.append(mutated_individual)\n                    new_fitness.append(f_mutated)\n                    if f_mutated < self.f_opt:\n                        self.f_opt = f_mutated\n                        self.x_opt = mutated_individual\n                elif f_opposition < self.fitness[i]:\n                    new_population.append(opposition_individual)\n                    new_fitness.append(f_opposition)\n                    if f_opposition < self.f_opt:\n                        self.f_opt = f_opposition\n                        self.x_opt = opposition_individual\n                else:\n                    new_population.append(self.population[i])\n                    new_fitness.append(self.fitness[i])\n\n            self.population = np.array(new_population)\n            self.fitness = np.array(new_fitness)\n            \n            if np.random.rand() < self.restart_threshold:\n                if self.evals < self.budget:\n                    self.restart_population(func)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm GOBLRestart scored 0.286 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.10343537510239353, 0.18022730008311594, 0.2780820727707345, 0.21320060440083788, 0.19601792988611455, 0.24731731677139546, 0.23766201148934618, 0.21014684639966752, 0.19636630096221586, 0.1692110723531528, 0.21921982649498628, 0.9965749315793654, 0.26406181323811884, 0.20069295208225268, 0.6028821043104422, 0.2660079114883257, 0.22685820099176002, 0.2951429355215418, 0.16378477387443835, 0.4603345806808269]}, "task_prompt": ""}
{"id": "e934140c-9a5a-4ac7-8165-bd016d783bb9", "fitness": "-inf", "name": "GP_Optimizer", "description": "No description provided.", "code": "import numpy as np\nfrom sklearn.gaussian_process import GaussianProcessRegressor\nfrom sklearn.gaussian_process.kernels import RBF, ConstantKernel, Matern\nfrom scipy.stats import norm\n\nclass GP_Optimizer:\n    def __init__(self, budget=10000, dim=10, n_initial=10, kernel='RBF'):\n        self.budget = budget\n        self.dim = dim\n        self.n_initial = n_initial\n        self.lb = -5.0\n        self.ub = 5.0\n        self.X = None\n        self.y = None\n        self.gp = None\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.evals = 0\n\n        if kernel == 'RBF':\n            self.kernel = ConstantKernel(1.0, constant_value_bounds=\"fixed\") * RBF(length_scale=1.0, length_scale_bounds=(1e-1, 10.0))\n        elif kernel == 'Matern':\n            self.kernel = ConstantKernel(1.0, constant_value_bounds=\"fixed\") * Matern(length_scale=1.0, length_scale_bounds=(1e-1, 10.0), nu=1.5)\n        else:\n            raise ValueError(\"Invalid kernel type. Choose 'RBF' or 'Matern'.\")\n\n    def initialize(self, func):\n        self.X = np.random.uniform(self.lb, self.ub, size=(self.n_initial, self.dim))\n        self.y = np.array([func(x) for x in self.X])\n        self.evals = self.n_initial\n        self.f_opt = np.min(self.y)\n        self.x_opt = self.X[np.argmin(self.y)]\n        self.gp = GaussianProcessRegressor(kernel=self.kernel, n_restarts_optimizer=5)\n        self.gp.fit(self.X, self.y)\n\n    def acquisition_function(self, x):\n        mu, sigma = self.gp.predict(x.reshape(1, -1), return_std=True)\n        if sigma == 0:\n            return 0\n        z = (self.f_opt - mu) / sigma\n        return (self.f_opt - mu) * norm.cdf(z) + sigma * norm.pdf(z)\n\n    def find_next_point(self, func, num_restarts=20):\n        best_x = None\n        best_acq = -np.inf\n        for _ in range(num_restarts):\n            x0 = np.random.uniform(self.lb, self.ub, size=self.dim)\n            res = minimize(lambda x: -self.acquisition_function(x), x0, bounds=[(self.lb, self.ub)] * self.dim, method='L-BFGS-B')\n            if -res.fun > best_acq:\n                best_acq = -res.fun\n                best_x = res.x\n        return best_x\n\n    def __call__(self, func):\n        self.initialize(func)\n        while self.evals < self.budget:\n            x_next = self.find_next_point(func)\n            f_next = func(x_next)\n            self.evals += 1\n            self.X = np.vstack((self.X, x_next))\n            self.y = np.append(self.y, f_next)\n            self.gp.fit(self.X, self.y)\n\n            if f_next < self.f_opt:\n                self.f_opt = f_next\n                self.x_opt = x_next\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "In the code, line 21, in __init__, the following error occurred:\nNameError: name 'ConstantKernel' is not defined\nOn line: self.kernel = ConstantKernel(1.0, constant_value_bounds=\"fixed\") * RBF(length_scale=1.0, length_scale_bounds=(1e-1, 10.0))", "error": "In the code, line 21, in __init__, the following error occurred:\nNameError: name 'ConstantKernel' is not defined\nOn line: self.kernel = ConstantKernel(1.0, constant_value_bounds=\"fixed\") * RBF(length_scale=1.0, length_scale_bounds=(1e-1, 10.0))", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "70c2774a-4bf0-4cf9-bc3f-5f1fa51de622", "fitness": 0.29039235463579216, "name": "VoronoiOptimization", "description": "No description provided.", "code": "import numpy as np\nfrom scipy.spatial import Voronoi, voronoi_plot_2d\n\nclass VoronoiOptimization:\n    def __init__(self, budget=10000, dim=10, pop_size=20, exploration_weight=0.1, voronoi_update_frequency=50):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.exploration_weight = exploration_weight\n        self.voronoi_update_frequency = voronoi_update_frequency\n        self.points = None\n        self.fitness = None\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.vor = None\n        self.cell_volumes = None\n\n    def __call__(self, func):\n\n        # Initialize population\n        self.points = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.points])\n        self.budget -= self.pop_size\n        \n        best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_index]\n        self.x_opt = self.points[best_index].copy()\n\n        iteration = 0\n        while self.budget > 0:\n            iteration += 1\n            \n            # Voronoi decomposition and cell volume calculation\n            try:\n                self.vor = Voronoi(self.points)\n                self.cell_volumes = self._calculate_cell_volumes()\n            except Exception as e:\n                # Handle potential issues with Voronoi calculation (e.g., all points collinear)\n                # If voronoi calculation fails, default to random sampling\n                new_point = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                f = func(new_point)\n                self.budget -= 1\n                if f < self.f_opt:\n                    self.f_opt = f\n                    self.x_opt = new_point.copy()\n                continue\n            \n            # Sample new points based on Voronoi cell volumes and fitness\n            probabilities = self.cell_volumes * np.exp(-self.exploration_weight * self.fitness)\n            probabilities /= np.sum(probabilities)\n\n            new_points = np.zeros((self.pop_size, self.dim))\n            new_fitness = np.zeros(self.pop_size)\n            \n            for i in range(self.pop_size):\n                chosen_cell_index = np.random.choice(range(self.pop_size), p=probabilities)\n                \n                # Sample within the chosen Voronoi cell. If cell is unbounded, fallback to global sampling.\n                if np.isinf(self.cell_volumes[chosen_cell_index]):\n                    new_point = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                else:\n                    center = self.points[chosen_cell_index]\n                    # Sample near the center of the Voronoi cell\n                    new_point = np.random.normal(center, scale=np.sqrt(self.cell_volumes[chosen_cell_index]) / self.dim, size=self.dim)\n                    new_point = np.clip(new_point, func.bounds.lb, func.bounds.ub)\n\n                new_points[i] = new_point\n                new_fitness[i] = func(new_point)\n                self.budget -= 1\n\n                if new_fitness[i] < self.f_opt:\n                    self.f_opt = new_fitness[i]\n                    self.x_opt = new_point.copy()\n\n\n            # Update population: Replace worst performing points with new points\n            worst_indices = np.argsort(self.fitness)[-self.pop_size // 2:]  # Replace half the population\n            self.points[worst_indices] = new_points[:self.pop_size // 2]\n            self.fitness[worst_indices] = new_fitness[:self.pop_size // 2]\n\n\n            # Periodically update exploration weight to balance exploration and exploitation\n            if iteration % self.voronoi_update_frequency == 0:\n                self.exploration_weight = np.clip(self.exploration_weight * (1 - 0.05), 0.01, 0.5) # Gradually decrease exploration\n\n        return self.f_opt, self.x_opt\n\n    def _calculate_cell_volumes(self):\n        \"\"\"Calculates the volume of each Voronoi cell. Uses a simple approximation.\"\"\"\n        volumes = np.zeros(self.pop_size)\n        for i in range(self.pop_size):\n            if self.vor.regions[self.vor.point_region[i]]:\n                vertices = self.vor.vertices[self.vor.regions[self.vor.point_region[i]]]\n                # Approximate volume by the average distance to the vertices\n                if vertices.shape[0] > 0:\n                    distances = np.linalg.norm(vertices - self.points[i], axis=1)\n                    volumes[i] = np.mean(distances)\n                else:\n                    volumes[i] = np.inf #Unbounded region\n            else:\n                volumes[i] = np.inf  # Unbounded region.\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm VoronoiOptimization scored 0.290 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.12157809592681967, 0.20239269742306776, 0.2668117708902582, 0.2347882632649847, 0.2008699773309136, 0.22637131774994768, 0.22947274502508663, 0.23424152583483826, 0.19517558975982685, 0.1645480149782652, 0.29366938008529864, 0.9989954211758805, 0.2552377961225639, 0.20288087843857683, 0.5787763531729694, 0.26511830363108424, 0.24986403674327518, 0.26168988712141694, 0.1564315068919292, 0.4689335311488412]}, "task_prompt": ""}
{"id": "255088e7-2c9b-44aa-a1b8-12f140174184", "fitness": "-inf", "name": "HybridPSO", "description": "No description provided.", "code": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridPSO:\n    def __init__(self, budget=10000, dim=10, pop_size=20, w=0.7, c1=1.5, c2=1.5, local_search_prob=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.w = w  # Inertia weight\n        self.c1 = c1  # Cognitive coefficient\n        self.c2 = c2  # Social coefficient\n        self.lb = -5.0\n        self.ub = 5.0\n        self.particles = None\n        self.velocities = None\n        self.personal_best_positions = None\n        self.personal_best_fitness = None\n        self.global_best_position = None\n        self.global_best_fitness = np.inf\n        self.evals = 0\n        self.local_search_prob = local_search_prob\n\n    def initialize_particles(self, func):\n        self.particles = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, size=(self.pop_size, self.dim))  # Initialize velocities\n        self.personal_best_positions = self.particles.copy()\n        self.personal_best_fitness = np.array([func(x) for x in self.particles])\n        self.evals += self.pop_size\n        self.global_best_position = self.personal_best_positions[np.argmin(self.personal_best_fitness)].copy()\n        self.global_best_fitness = np.min(self.personal_best_fitness)\n\n    def update_particle(self, i, func):\n        r1 = np.random.rand(self.dim)\n        r2 = np.random.rand(self.dim)\n\n        # Update velocity\n        self.velocities[i] = (self.w * self.velocities[i]\n                              + self.c1 * r1 * (self.personal_best_positions[i] - self.particles[i])\n                              + self.c2 * r2 * (self.global_best_position - self.particles[i]))\n\n        # Clip velocities (optional, but can help with exploration)\n        # self.velocities[i] = np.clip(self.velocities[i], -1, 1)\n\n        # Update position\n        self.particles[i] = self.particles[i] + self.velocities[i]\n        self.particles[i] = np.clip(self.particles[i], self.lb, self.ub)\n\n        # Evaluate fitness\n        fitness = func(self.particles[i])\n        self.evals += 1\n\n        # Update personal best\n        if fitness < self.personal_best_fitness[i]:\n            self.personal_best_fitness[i] = fitness\n            self.personal_best_positions[i] = self.particles[i].copy()\n\n            # Update global best\n            if fitness < self.global_best_fitness:\n                self.global_best_fitness = fitness\n                self.global_best_position = self.particles[i].copy()\n\n    def local_search(self, func, x0):\n        bounds = [(self.lb, self.ub)] * self.dim\n        res = minimize(func, x0, method='L-BFGS-B', bounds=bounds, options={'maxfun': min(self.budget - self.evals, 50)})\n        self.evals += res.nfev\n        return res.fun, res.x\n\n    def __call__(self, func):\n        self.initialize_particles(func)\n        while self.evals < self.budget:\n            for i in range(self.pop_size):\n                if self.evals >= self.budget:\n                    break\n                if np.random.rand() < self.local_search_prob:\n                    f, x = self.local_search(func, self.particles[i].copy())\n                    if f < self.personal_best_fitness[i]:\n                        self.personal_best_fitness[i] = f\n                        self.personal_best_positions[i] = x.copy()\n                        if f < self.global_best_fitness:\n                            self.global_best_fitness = f\n                            self.global_best_position = x.copy()\n                        self.particles[i] = x.copy()\n                else:\n                    self.update_particle(i, func)\n\n        self.f_opt = self.global_best_fitness\n        self.x_opt = self.global_best_position\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "In the code, line 64, in local_search, the following error occurred:\nNameError: name 'minimize' is not defined\nOn line: res = minimize(func, x0, method='L-BFGS-B', bounds=bounds, options={'maxfun': min(self.budget - self.evals, 50)})", "error": "In the code, line 64, in local_search, the following error occurred:\nNameError: name 'minimize' is not defined\nOn line: res = minimize(func, x0, method='L-BFGS-B', bounds=bounds, options={'maxfun': min(self.budget - self.evals, 50)})", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "bf1806b1-b2cc-4c50-a465-2ee9b1b110fd", "fitness": 0.3043886892871663, "name": "AdaptiveSampling", "description": "No description provided.", "code": "import numpy as np\nfrom scipy.stats import norm\n\nclass AdaptiveSampling:\n    def __init__(self, budget=10000, dim=10, initial_samples=100, adaptation_rate=0.1, exploration_prob=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.lb = -5.0\n        self.ub = 5.0\n        self.initial_samples = initial_samples\n        self.adaptation_rate = adaptation_rate\n        self.exploration_prob = exploration_prob\n        self.means = np.zeros(dim)\n        self.stddevs = np.ones(dim) * 2.5  # Initialize with a relatively large standard deviation\n\n    def initialize_sampling(self, func):\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.initial_samples, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.initial_samples\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n\n        self.means = np.mean(self.population, axis=0)\n        self.stddevs = np.std(self.population, axis=0)\n\n    def sample(self):\n        if np.random.rand() < self.exploration_prob:\n            return np.random.uniform(self.lb, self.ub, size=self.dim)\n        else:\n            sample = np.random.normal(self.means, self.stddevs, size=self.dim)\n            return np.clip(sample, self.lb, self.ub)\n\n    def update_distribution(self, x, f):\n        if f < self.f_opt:\n            self.f_opt = f\n            self.x_opt = x\n\n            # Update means and stddevs based on the new best point\n            new_means = (1 - self.adaptation_rate) * self.means + self.adaptation_rate * x\n            new_stddevs = (1 - self.adaptation_rate) * self.stddevs + self.adaptation_rate * np.abs(x - self.means)\n\n            self.means = new_means\n            self.stddevs = np.maximum(new_stddevs, 0.1)  # Ensure stddevs don't go to zero\n\n\n    def __call__(self, func):\n        self.initialize_sampling(func)\n\n        while self.evals < self.budget:\n            x = self.sample()\n            f = func(x)\n            self.evals += 1\n            self.update_distribution(x, f)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveSampling scored 0.304 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.11583428425829845, 0.18987702338390355, 0.3214500657592537, 0.23197001436082465, 0.22396486222747702, 0.25505069384615175, 0.24254488802503538, 0.25965566006265295, 0.22379016146828234, 0.18596307343899676, 0.21123916383620367, 0.9972985676277129, 0.26968432527496367, 0.23757410476533314, 0.6486582363114646, 0.2991804516440497, 0.24059285397221564, 0.3036995674132411, 0.15970810162273597, 0.4700376864445285]}, "task_prompt": ""}
