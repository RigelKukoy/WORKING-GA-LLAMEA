{"id": "88726173-ab72-4799-bddb-ecd6cfe3a10f", "fitness": "-inf", "name": "SobolNelderMead", "description": "No description provided.", "code": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass SobolNelderMead:\n    def __init__(self, budget=10000, dim=10, stagnation_tolerance=100):\n        self.budget = budget\n        self.dim = dim\n        self.stagnation_tolerance = stagnation_tolerance\n        try:\n            from sobol_seq import i4_sobol_generate\n            self.sobol_available = True\n            self.sobol_generator = i4_sobol_generate\n        except ImportError:\n            self.sobol_available = False\n            print(\"Warning: sobol_seq not found. Falling back to random initialization.\")\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        eval_count = 0\n        stagnation_counter = 0\n\n        while eval_count < self.budget:\n            if self.sobol_available:\n                remaining_evals = self.budget - eval_count\n                n_points = min(100, remaining_evals)\n                sobol_points = self.sobol_generator(self.dim, n_points)\n                initial_points = func.bounds.lb + sobol_points * (func.bounds.ub - func.bounds.lb)\n            else:\n                initial_points = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(100, self.dim))\n                n_points = 100\n\n            for i in range(n_points):\n                if eval_count >= self.budget:\n                    break\n                x0 = initial_points[i, :]\n\n                res = minimize(func, x0, method='Nelder-Mead', options={'maxfev': self.budget - eval_count, 'xatol': 1e-6, 'fatol': 1e-6})\n                \n                eval_count += res.nfev\n\n                if res.fun < self.f_opt:\n                    self.f_opt = res.fun\n                    self.x_opt = res.x\n                    stagnation_counter = 0\n                else:\n                    stagnation_counter += 1\n                \n                if stagnation_counter > self.stagnation_tolerance:\n                    stagnation_counter = 0\n                    break  # Restart from a new Sobol point\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "In the code, line 38, in __call__, the following error occurred:\nNameError: name 'minimize' is not defined\nOn line: res = minimize(func, x0, method='Nelder-Mead', options={'maxfev': self.budget - eval_count, 'xatol': 1e-6, 'fatol': 1e-6})", "error": "In the code, line 38, in __call__, the following error occurred:\nNameError: name 'minimize' is not defined\nOn line: res = minimize(func, x0, method='Nelder-Mead', options={'maxfev': self.budget - eval_count, 'xatol': 1e-6, 'fatol': 1e-6})", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "4523dc1f-5414-42af-8549-647e3cf5965a", "fitness": 0.3316231915015579, "name": "ADE", "description": "No description provided.", "code": "import numpy as np\n\nclass ADE:\n    def __init__(self, budget=10000, dim=10, pop_size=40, archive_size=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        \n        # Initialization\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n        \n        archive = []\n\n        # Main loop\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size + len(archive), 4, replace=False)\n                if idxs[0] >= self.pop_size:\n                  a = archive[idxs[0] - self.pop_size]\n                else:\n                  a = population[idxs[0]]\n                if idxs[1] >= self.pop_size:\n                  b = archive[idxs[1] - self.pop_size]\n                else:\n                  b = population[idxs[1]]\n                if idxs[2] >= self.pop_size:\n                  c = archive[idxs[2] - self.pop_size]\n                else:\n                  c = population[idxs[2]]                  \n                \n                F = np.random.uniform(0.1, 0.9)\n                mutant = population[i] + F * (a - b) + F * (population[i] - c) \n                \n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover (Orthogonal Crossover)\n                CR = np.random.uniform(0, 1)\n                trial = np.copy(population[i])\n                j_rand = np.random.randint(self.dim)\n\n                for j in range(self.dim):\n                  if np.random.rand() < CR or j == j_rand:\n                    trial[j] = mutant[j]\n\n                # Selection\n                f_trial = func(trial)\n                self.budget -= 1\n                \n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    population[i] = trial\n\n                    # Update archive\n                    if len(archive) < self.archive_size:\n                        archive.append(population[i].copy())\n                    else:\n                        idx_to_replace = np.random.randint(self.archive_size)\n                        archive[idx_to_replace] = population[i].copy()\n\n                # Update optimal solution\n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = trial.copy()\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm ADE scored 0.332 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.14149102520762225, 0.2274017805551679, 0.33054071558043996, 0.2944490620231188, 0.2524396647031938, 0.2928188051039443, 0.2658868418345466, 0.2667768171166207, 0.25046489814812967, 0.19764429743793466, 0.33061311399042315, 0.9888443545164902, 0.28856600339641714, 0.26381281218524744, 0.6618544100311825, 0.37134545615073145, 0.25673986724084086, 0.31599892012825037, 0.16752379263708406, 0.4672511920437725]}, "task_prompt": ""}
{"id": "1ea43e9c-ccfb-465d-8d12-4e825dabb849", "fitness": 0.6366799081871742, "name": "AdaptiveDE_LocalSearch", "description": "No description provided.", "code": "import numpy as np\n\nclass AdaptiveDE_LocalSearch:\n    def __init__(self, budget=10000, dim=10, pop_size=50, f=0.5, cr=0.7, local_search_iterations=5, local_search_radius=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.f = f\n        self.cr = cr\n        self.local_search_iterations = local_search_iterations\n        self.local_search_radius = local_search_radius\n        self.population = None\n        self.fitness = None\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.eval_count = 0\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.eval_count += self.pop_size\n\n        best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_index]\n        self.x_opt = self.population[best_index]\n\n    def mutate(self):\n        for i in range(self.pop_size):\n            idxs = [idx for idx in range(self.pop_size) if idx != i]\n            a, b, c = np.random.choice(idxs, 3, replace=False)\n            mutant = self.population[a] + self.f * (self.population[b] - self.population[c])\n            mutant = np.clip(mutant, -5.0, 5.0)\n            \n            cross_points = np.random.rand(self.dim) < self.cr\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n\n            trial = np.where(cross_points, mutant, self.population[i])\n            yield i, trial\n\n    def local_search(self, func):\n        for _ in range(self.local_search_iterations):\n            x_new = self.x_opt + np.random.uniform(-self.local_search_radius, self.local_search_radius, size=self.dim)\n            x_new = np.clip(x_new, -5.0, 5.0)\n            f_new = func(x_new)\n            self.eval_count += 1\n            if f_new < self.f_opt:\n                self.f_opt = f_new\n                self.x_opt = x_new\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        generation = 0\n        while self.eval_count < self.budget:\n            generation += 1\n            \n            successful_f = []\n            successful_cr = []\n            \n            for i, trial in self.mutate():\n                f_trial = func(trial)\n                self.eval_count += 1\n\n                if f_trial < self.fitness[i]:\n                    successful_f.append(self.f)\n                    successful_cr.append(self.cr)\n\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                \n                if self.eval_count >= self.budget:\n                    break\n\n            if len(successful_f) > 0:\n                self.f = np.mean(successful_f) if len(successful_f) > 0 else 0.5\n                self.cr = np.mean(successful_cr) if len(successful_cr) > 0 else 0.7\n            else:\n                self.f = np.clip(np.random.normal(0.5, 0.1), 0.1, 1.0)\n                self.cr = np.clip(np.random.normal(0.7, 0.1), 0.1, 1.0)\n                \n            if generation % 10 == 0:\n                self.local_search(func)\n        \n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveDE_LocalSearch scored 0.637 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.28238372236840437, 0.4685387556553736, 0.6121424228378084, 0.8273702410093784, 0.6915707591057985, 0.7486389659664221, 0.5670390257882015, 0.6121124932118418, 0.6926950818655752, 0.5701679548463909, 0.7774607808110572, 0.9912809527303997, 0.47788589678577076, 0.6467286538420094, 0.8840015901962665, 0.7730776480333732, 0.5322767813056515, 0.7973216973485222, 0.2628197286891266, 0.5180850113461137]}, "task_prompt": ""}
{"id": "464c0bdf-a140-42ba-b8aa-dd809722da31", "fitness": 0.4158723176281631, "name": "AdaptivePopulationSearch", "description": "No description provided.", "code": "import numpy as np\n\nclass AdaptivePopulationSearch:\n    def __init__(self, budget=10000, dim=10, pop_size=20, exploration_rate=0.5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.exploration_rate = exploration_rate\n        self.lb = -5.0\n        self.ub = 5.0\n        self.step_size = (self.ub - self.lb) / 10.0\n        self.success_rate = 0.0\n        self.success_memory = []\n        self.memory_size = 10\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        \n        # Initialize population\n        population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n        \n        # Find best individual\n        best_index = np.argmin(fitness)\n        best_x = population[best_index]\n        best_f = fitness[best_index]\n        \n        self.f_opt = best_f\n        self.x_opt = best_x\n\n        while self.budget > 0:\n            # Generate new individuals around the best\n            new_population = np.zeros_like(population)\n            \n            for i in range(self.pop_size):\n                if np.random.rand() < self.exploration_rate:\n                    # Exploration: Random search within bounds\n                    new_population[i] = np.random.uniform(self.lb, self.ub, size=self.dim)\n                else:\n                    # Exploitation: Gaussian perturbation around the best\n                    new_population[i] = np.clip(best_x + np.random.normal(0, self.step_size, size=self.dim), self.lb, self.ub)\n            \n            new_fitness = np.array([func(x) for x in new_population])\n            self.budget -= self.pop_size\n            \n            # Update success rate memory\n            successes = new_fitness < fitness\n            self.success_memory.append(np.mean(successes))\n            if len(self.success_memory) > self.memory_size:\n                self.success_memory.pop(0)\n            self.success_rate = np.mean(self.success_memory) if self.success_memory else 0.5\n\n            # Update population\n            for i in range(self.pop_size):\n                if new_fitness[i] < fitness[i]:\n                    population[i] = new_population[i]\n                    fitness[i] = new_fitness[i]\n\n            # Update best individual\n            best_index = np.argmin(fitness)\n            if fitness[best_index] < self.f_opt:\n                self.f_opt = fitness[best_index]\n                self.x_opt = population[best_index]\n                best_x = population[best_index]\n                best_f = fitness[best_index]\n            \n            # Adjust step size and exploration rate based on success rate\n            if self.success_rate > 0.3:\n                self.step_size *= 1.1\n                self.exploration_rate *= 0.95\n            elif self.success_rate < 0.2:\n                self.step_size *= 0.9\n                self.exploration_rate *= 1.05\n            \n            self.step_size = np.clip(self.step_size, (self.ub - self.lb) / 1000, (self.ub - self.lb) / 2)\n            self.exploration_rate = np.clip(self.exploration_rate, 0.1, 0.9)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptivePopulationSearch scored 0.416 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.1496436949101575, 0.21577571366669113, 0.4648949851152776, 0.7968476286491748, 0.24580407416565442, 0.5552087886455737, 0.3119418069652705, 0.45048949278945416, 0.2809724713584717, 0.1778536888581701, 0.3249165437843894, 0.9999124624909134, 0.23344453405347154, 0.34842896857310846, 0.7703365371480009, 0.32098706368702556, 0.28420441839005905, 0.6949211002840525, 0.2042405204723171, 0.48662185855602924]}, "task_prompt": ""}
{"id": "189e80f5-f662-424e-aab4-80a8f54a8109", "fitness": "-inf", "name": "LHCS_NM", "description": "No description provided.", "code": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass LHCS_NM:\n    def __init__(self, budget=10000, dim=10, n_initial_points=50):\n        self.budget = budget\n        self.dim = dim\n        self.n_initial_points = min(n_initial_points, budget)  # Cap initial points\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.eval_count = 0\n\n    def latin_hypercube_sampling(self, n_samples, bounds):\n        lb, ub = bounds.lb, bounds.ub\n        samples = np.zeros((n_samples, self.dim))\n        for i in range(self.dim):\n            samples[:, i] = np.random.uniform(low=lb[i], high=ub[i], size=n_samples)\n\n            indices = np.random.permutation(n_samples)\n            samples[:, i] = samples[indices, i]\n        return samples\n    \n    def __call__(self, func):\n        # Latin Hypercube Sampling for initial points\n        initial_points = self.latin_hypercube_sampling(self.n_initial_points, func.bounds)\n\n        for i in range(self.n_initial_points):\n            if self.eval_count >= self.budget:\n                return self.f_opt, self.x_opt\n\n            x = initial_points[i]\n            f = func(x)\n            self.eval_count += 1\n\n            if f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = x\n\n        # Local Search (Nelder-Mead) around the best point found so far\n        if self.eval_count < self.budget:\n\n            remaining_budget = self.budget - self.eval_count\n            options = {'maxfev': remaining_budget}\n\n            res = minimize(func, self.x_opt, method='Nelder-Mead', bounds=func.bounds, options=options)\n            \n            if res.fun < self.f_opt:\n                self.f_opt = res.fun\n                self.x_opt = res.x\n\n            self.eval_count = self.budget # Set eval count to budget since minimize already considers it\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "In the code, line 45, in __call__, the following error occurred:\nNameError: name 'minimize' is not defined\nOn line: res = minimize(func, self.x_opt, method='Nelder-Mead', bounds=func.bounds, options=options)", "error": "In the code, line 45, in __call__, the following error occurred:\nNameError: name 'minimize' is not defined\nOn line: res = minimize(func, self.x_opt, method='Nelder-Mead', bounds=func.bounds, options=options)", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "b4646b50-41e2-4e79-b31b-95d161b38a2d", "fitness": "-inf", "name": "DifferentialEvolutionLocalSearch", "description": "No description provided.", "code": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass DifferentialEvolutionLocalSearch:\n    def __init__(self, budget=10000, dim=10, pop_size=20, F=0.7, Cr=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.Cr = Cr\n        self.lb = -5.0\n        self.ub = 5.0\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        \n        # Initialize population\n        population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n        \n        best_idx = np.argmin(fitness)\n        self.f_opt = fitness[best_idx]\n        self.x_opt = population[best_idx]\n        \n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = population[idxs]\n                x_mutated = population[i] + self.F * (x_r2 - x_r3)\n                x_mutated = np.clip(x_mutated, self.lb, self.ub)\n                \n                # Crossover\n                x_trial = np.copy(population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < self.Cr or j == j_rand:\n                        x_trial[j] = x_mutated[j]\n                \n                # Selection\n                f_trial = func(x_trial)\n                self.budget -= 1\n                \n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    population[i] = x_trial\n                    \n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = x_trial\n            \n            # Local Search around best solution\n            if self.budget > self.dim*2:\n                res = minimize(func, self.x_opt, method='Nelder-Mead', \n                               bounds=[(self.lb, self.ub)]*self.dim,\n                               options={'maxfev': min(self.budget, 500)})\n                \n                if res.fun < self.f_opt:\n                     self.f_opt = res.fun\n                     self.x_opt = res.x\n\n                self.budget -= res.nfev\n            \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "In the code, line 56, in __call__, the following error occurred:\nNameError: name 'minimize' is not defined\nOn line: res = minimize(func, self.x_opt, method='Nelder-Mead',", "error": "In the code, line 56, in __call__, the following error occurred:\nNameError: name 'minimize' is not defined\nOn line: res = minimize(func, self.x_opt, method='Nelder-Mead',", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "8e09b93e-cc04-4176-b07e-b73210992141", "fitness": 0.48764627579529873, "name": "AdaptiveDifferentialEvolution", "description": "No description provided.", "code": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget=10000, dim=10, pop_size=20, F=0.5, CR=0.7):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F  # Mutation factor\n        self.CR = CR  # Crossover rate\n        self.lb = -5.0\n        self.ub = 5.0\n        self.pop = None\n        self.fitness = None\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.success_history = []\n\n    def initialize_population(self):\n        self.pop = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.zeros(self.pop_size)\n\n    def evaluate_population(self, func):\n        for i in range(self.pop_size):\n            self.fitness[i] = func(self.pop[i])\n            if self.fitness[i] < self.f_opt:\n                self.f_opt = self.fitness[i]\n                self.x_opt = self.pop[i]\n\n    def mutate(self):\n        for i in range(self.pop_size):\n            idxs = [idx for idx in range(self.pop_size) if idx != i]\n            a, b, c = self.pop[np.random.choice(idxs, 3, replace=False)]\n            mutant = a + self.F * (b - c)\n            mutant = np.clip(mutant, self.lb, self.ub)\n            \n            #Crossover and trial vector\n            trial = np.copy(self.pop[i])\n            j_rand = np.random.randint(self.dim)\n            for j in range(self.dim):\n                if np.random.rand() < self.CR or j == j_rand:\n                    trial[j] = mutant[j]\n            \n            yield i, trial\n\n    def __call__(self, func):\n        self.initialize_population()\n        self.evaluate_population(func)\n        eval_count = self.pop_size  # Initial population evaluation\n\n        while eval_count < self.budget:\n            successful_mutations = 0\n            for i, trial in self.mutate():\n                f_trial = func(trial)\n                eval_count += 1\n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.pop[i] = trial\n                    successful_mutations += 1\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n                if eval_count >= self.budget:\n                    break\n            #Adaptive parameter control based on mutation success ratio\n            if self.pop_size > 0:\n                success_ratio = successful_mutations / self.pop_size if self.pop_size > 0 else 0.0\n\n                #Update F and CR based on success ratio\n                if success_ratio > 0.5: #Exploitation\n                    self.F *= 0.95\n                    self.CR *= 1.05\n                elif success_ratio < 0.1: #Exploration\n                    self.F *= 1.05\n                    self.CR *= 0.95\n\n                self.F = np.clip(self.F, 0.1, 1.0)\n                self.CR = np.clip(self.CR, 0.1, 1.0)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveDifferentialEvolution scored 0.488 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.2141435437664504, 0.32245757867000624, 0.6568563803855838, 0.9182596281122568, 0.4078768258269133, 0.294541035651808, 0.31988635888636907, 0.36928343836014443, 0.5765614829631623, 0.6962682944856095, 0.2341844532673999, 0.9987294993324125, 0.3039844612197671, 0.34802776965498383, 0.7746200185312826, 0.390778815004832, 0.6906520347102931, 0.531269745885594, 0.21741152764288563, 0.48713262354822184]}, "task_prompt": ""}
{"id": "8cd8a716-ff50-424a-82d4-99d6e78573ab", "fitness": 0.0, "name": "AdaptiveDE", "description": "No description provided.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=None, F=0.5, CR=0.7):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size_init = pop_size if pop_size is not None else 10*dim\n        self.F = F  # Mutation factor\n        self.CR = CR  # Crossover rate\n        self.lb = -5.0\n        self.ub = 5.0\n        self.history_size = 10\n        self.fitness_history = []\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.pop_size = self.pop_size_init  # Initialize population size\n\n        # Initialize population within bounds\n        population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.nevals = self.pop_size\n        \n        # Update optimal solution\n        best_index = np.argmin(fitness)\n        if fitness[best_index] < self.f_opt:\n                self.f_opt = fitness[best_index]\n                self.x_opt = population[best_index]\n\n        while self.nevals < self.budget:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + self.F * (b - c), self.lb, self.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                f = func(trial)\n                self.nevals += 1\n                \n                if f < fitness[i]:\n                    fitness[i] = f\n                    population[i] = trial\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = trial\n            \n            #Adaptive Population Size\n            self.fitness_history.append(self.f_opt)\n            if len(self.fitness_history) > self.history_size:\n              self.fitness_history.pop(0)\n              improvement = self.fitness_history[0] - self.fitness_history[-1]\n              if improvement > 0:\n                  self.pop_size = int(self.pop_size * 1.1)  # Increase population size if improving\n              else:\n                  self.pop_size = int(self.pop_size * 0.9)  # Decrease population size if stagnating\n\n              self.pop_size = max(self.dim + 1, min(self.pop_size, self.budget // 2))\n              \n              if self.pop_size != population.shape[0]: #Re-initialize population if needed\n                 new_pop_size = self.pop_size\n                 new_population = np.random.uniform(self.lb, self.ub, size=(new_pop_size, self.dim))\n                 new_fitness = np.array([func(x) for x in new_population])\n                 self.nevals += new_pop_size\n                 \n                 best_indices = np.argsort(fitness)[:min(self.pop_size, population.shape[0])]\n                 \n                 new_population[:len(best_indices)] = population[best_indices]\n                 new_fitness[:len(best_indices)] = fitness[best_indices]\n\n                 population = new_population\n                 fitness = new_fitness\n                 \n                 best_index = np.argmin(fitness)\n                 if fitness[best_index] < self.f_opt:\n                        self.f_opt = fitness[best_index]\n                        self.x_opt = population[best_index]\n                        \n            if self.nevals >= self.budget:\n                break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveDE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0]}, "task_prompt": ""}
{"id": "f48b7b4c-333e-4eb7-ac10-55bc3bc7eafb", "fitness": "-inf", "name": "GaussianProcessOptimization", "description": "No description provided.", "code": "import numpy as np\nfrom sklearn.gaussian_process import GaussianProcessRegressor\nfrom sklearn.gaussian_process.kernels import RBF, ConstantKernel as C\nfrom scipy.stats import norm\nfrom scipy.optimize import minimize\n\nclass GaussianProcessOptimization:\n    def __init__(self, budget=10000, dim=10, n_initial_samples=10, xi=0.01):\n        self.budget = budget\n        self.dim = dim\n        self.n_initial_samples = n_initial_samples\n        self.lb = -5.0\n        self.ub = 5.0\n        self.xi = xi  # Exploration-exploitation trade-off parameter\n        self.X_samples = []\n        self.Y_samples = []\n        self.gpr = None\n\n    def acquisition_function(self, x):\n        mu, sigma = self.gpr.predict(x.reshape(1, -1), return_std=True)\n        mu = mu[0]\n        sigma = sigma[0]\n\n        if sigma == 0:\n          return 0\n\n        Z = (mu - self.best_f - self.xi) / sigma\n        return - (mu - self.best_f - self.xi) * norm.cdf(Z) - sigma * norm.pdf(Z)\n        \n    def optimize_acquisition_function(self):\n        bounds = [(self.lb, self.ub) for _ in range(self.dim)]\n        x0 = np.random.uniform(self.lb, self.ub, size=self.dim)\n        \n        res = minimize(self.acquisition_function, x0, method='L-BFGS-B', bounds=bounds)\n        return res.x\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        \n        # Initial sampling\n        X_initial = np.random.uniform(self.lb, self.ub, size=(self.n_initial_samples, self.dim))\n        Y_initial = np.array([func(x) for x in X_initial])\n        self.X_samples = X_initial.tolist()\n        self.Y_samples = Y_initial.tolist()\n        self.best_f = np.min(Y_initial)\n        self.x_opt = X_initial[np.argmin(Y_initial)]\n        self.f_opt = self.best_f\n        eval_count = self.n_initial_samples\n        \n        # Gaussian process setup\n        kernel = C(1.0, (1e-3, 1e3)) * RBF(1.0, (1e-2, 1e2))\n        self.gpr = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=10)\n        \n        # Optimization loop\n        while eval_count < self.budget:\n            # Fit GP model\n            self.gpr.fit(self.X_samples, self.Y_samples)\n            \n            # Find next sample point by maximizing acquisition function\n            x_new = self.optimize_acquisition_function()\n\n            # Evaluate function at new point\n            f_new = func(x_new)\n            eval_count += 1\n            \n            # Update samples\n            self.X_samples.append(x_new)\n            self.Y_samples.append(f_new)\n            \n            # Update best\n            if f_new < self.f_opt:\n                self.f_opt = f_new\n                self.x_opt = x_new\n            self.best_f = np.min(self.Y_samples)\n        \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "In the code, line 52, in __call__, the following error occurred:\nNameError: name 'C' is not defined\nOn line: kernel = C(1.0, (1e-3, 1e3)) * RBF(1.0, (1e-2, 1e2))", "error": "In the code, line 52, in __call__, the following error occurred:\nNameError: name 'C' is not defined\nOn line: kernel = C(1.0, (1e-3, 1e3)) * RBF(1.0, (1e-2, 1e2))", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "feee2299-f93c-40dd-911d-1723ae7ce213", "fitness": 0.19703069637941079, "name": "AdaptiveSimulatedAnnealing", "description": "No description provided.", "code": "import numpy as np\n\nclass AdaptiveSimulatedAnnealing:\n    def __init__(self, budget=10000, dim=10, initial_temp=1.0, temp_min=0.0001, alpha=0.99, step_size=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.initial_temp = initial_temp\n        self.temp_min = temp_min\n        self.alpha = alpha\n        self.step_size = step_size\n        self.temp = initial_temp\n        self.eval_count = 0\n        self.acceptance_rate = 0.0\n        self.acceptance_history = []\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        x = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        f = func(x)\n        self.eval_count += 1\n        self.f_opt = f\n        self.x_opt = x\n\n        while self.eval_count < self.budget and self.temp > self.temp_min:\n            x_new = x + np.random.uniform(-self.step_size, self.step_size, size=self.dim)\n            x_new = np.clip(x_new, func.bounds.lb, func.bounds.ub)\n            f_new = func(x_new)\n            self.eval_count += 1\n\n            delta_e = f_new - f\n\n            if delta_e < 0 or np.random.rand() < np.exp(-delta_e / self.temp):\n                x = x_new\n                f = f_new\n                self.acceptance_rate += 1\n                if f < self.f_opt:\n                    self.f_opt = f\n                    self.x_opt = x\n            \n\n            self.temp *= self.alpha\n            \n            if self.eval_count % 100 == 0:\n                self.acceptance_rate /= 100\n                self.acceptance_history.append(self.acceptance_rate)\n                if self.acceptance_rate > 0.6:\n                    self.step_size *= 1.1\n                elif self.acceptance_rate < 0.4:\n                    self.step_size *= 0.9\n                self.step_size = np.clip(self.step_size, 0.01, 1.0)\n                self.acceptance_rate = 0.0\n            \n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveSimulatedAnnealing scored 0.197 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.0, 0.07896262886746697, 0.18393006917521448, 0.17855619260135513, 0.06552384240712794, 0.09296062937195004, 0.15126045953846168, 0.4007151818326492, 0.14603311278196418, 0.11218313187488127, 0.19829421234523603, 0.15872793763350723, 0.24002506098953036, 0.14569782995201752, 0.5494143452174616, 0.2886758355105882, 0.19571210108165993, 0.509142131357978, 0.1265535519545662, 0.11824567309459921]}, "task_prompt": ""}
{"id": "9ee5f744-19b8-4e77-b0fb-6012c91ba6cd", "fitness": "-inf", "name": "NelderMeadOptimization", "description": "No description provided.", "code": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass NelderMeadOptimization:\n    def __init__(self, budget=10000, dim=10, initial_simplex=None):\n        self.budget = budget\n        self.dim = dim\n        self.lb = -5.0\n        self.ub = 5.0\n        self.initial_simplex = initial_simplex\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        # Initial guess (center of the search space)\n        x0 = np.random.uniform(self.lb, self.ub, size=self.dim)\n\n        # Define the bounds for the optimization\n        bounds = [(self.lb, self.ub)] * self.dim\n\n        # Perform Nelder-Mead optimization\n        result = minimize(func, x0, method='Nelder-Mead', bounds=bounds,\n                            options={'maxfev': self.budget, 'xatol': 1e-4, 'fatol': 1e-4})\n\n        self.f_opt = result.fun\n        self.x_opt = result.x\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "In the code, line 23, in __call__, the following error occurred:\nNameError: name 'minimize' is not defined\nOn line: result = minimize(func, x0, method='Nelder-Mead', bounds=bounds,", "error": "In the code, line 23, in __call__, the following error occurred:\nNameError: name 'minimize' is not defined\nOn line: result = minimize(func, x0, method='Nelder-Mead', bounds=bounds,", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "e25ba407-d21f-4893-9d03-60973c8dd75e", "fitness": "-inf", "name": "AdaptiveSimulatedAnnealing", "description": "No description provided.", "code": "import numpy as np\n\nclass AdaptiveSimulatedAnnealing:\n    def __init__(self, budget=10000, dim=10, initial_temp=100.0, cooling_rate=0.95, temp_decay_factor=0.01):\n        self.budget = budget\n        self.dim = dim\n        self.initial_temp = initial_temp\n        self.cooling_rate = cooling_rate\n        self.temp_decay_factor = temp_decay_factor\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.eval_count = 0\n\n    def __call__(self, func):\n        x = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        f = func(x)\n        self.eval_count += 1\n        self.f_opt = f\n        self.x_opt = x\n        temp = self.initial_temp\n\n        while self.eval_count < self.budget:\n            x_new = x + np.random.normal(0, 0.1, size=self.dim)\n            x_new = np.clip(x_new, func.bounds.lb, func.bounds.ub)\n            f_new = func(x_new)\n            self.eval_count += 1\n\n            delta_f = f_new - f\n\n            if delta_f < 0:\n                x = x_new\n                f = f_new\n                if f < self.f_opt:\n                    self.f_opt = f\n                    self.x_opt = x\n            else:\n                acceptance_probability = np.exp(-delta_f / temp)\n                if np.random.rand() < acceptance_probability:\n                    x = x_new\n                    f = f_new\n            \n            # Adaptive temperature decay\n            if acceptance_probability > 0.5:\n                temp *= (1 - self.temp_decay_factor)\n            else:\n                temp *= self.cooling_rate\n            \n            if temp < 1e-5:\n                temp = 1e-5\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "In the code, line 43, in __call__, the following error occurred:\nUnboundLocalError: cannot access local variable 'acceptance_probability' where it is not associated with a value\nOn line: if acceptance_probability > 0.5:", "error": "In the code, line 43, in __call__, the following error occurred:\nUnboundLocalError: cannot access local variable 'acceptance_probability' where it is not associated with a value\nOn line: if acceptance_probability > 0.5:", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "19ab4193-2ad7-4e9e-b0e4-fcfb6a649be5", "fitness": 0.33429861951907297, "name": "StochasticPerturbation", "description": "No description provided.", "code": "import numpy as np\n\nclass StochasticPerturbation:\n    def __init__(self, budget=10000, dim=10, pop_size=20, perturbation_factor=0.1, reinit_rate=0.05):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.perturbation_factor = perturbation_factor\n        self.reinit_rate = reinit_rate\n        self.lb = -5.0\n        self.ub = 5.0\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        # Initialize population\n        population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        # Find initial best\n        best_idx = np.argmin(fitness)\n        self.f_opt = fitness[best_idx]\n        self.x_opt = population[best_idx].copy()\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Perturb the best solution\n                perturbation = np.random.normal(0, self.perturbation_factor, size=self.dim)\n                trial = self.x_opt + perturbation\n                trial = np.clip(trial, self.lb, self.ub)\n\n                f_trial = func(trial)\n                self.budget -= 1\n\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    population[i] = trial.copy()\n\n                    # Update optimal solution\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial.copy()\n\n                # Periodically re-initialize a portion of the population\n                if np.random.rand() < self.reinit_rate:\n                    population[i] = np.random.uniform(self.lb, self.ub, size=self.dim)\n                    fitness[i] = func(population[i])\n                    self.budget -= 1\n                    if fitness[i] < self.f_opt:\n                        self.f_opt = fitness[i]\n                        self.x_opt = population[i].copy()\n                \n                if self.budget <= 0:\n                    break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm StochasticPerturbation scored 0.334 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.1519364411514451, 0.1595004982979915, 0.4765155340330771, 0.24961451073674157, 0.20242546694564634, 0.38606433624470515, 0.23621573547182273, 0.21037877494160406, 0.35778471392574107, 0.16622854798044495, 0.9303888591461346, 0.9996081630797112, 0.2703317221447795, 0.21748598868625024, 0]}, "task_prompt": ""}
{"id": "b666200c-4a67-4363-83ae-c23fb6b4c036", "fitness": 0.0, "name": "EllipsoidalEvolution", "description": "No description provided.", "code": "import numpy as np\n\nclass EllipsoidalEvolution:\n    def __init__(self, budget=10000, dim=10, pop_size=15, initial_volume=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lb = -5.0\n        self.ub = 5.0\n        self.initial_volume = initial_volume\n        self.ellipsoids = []\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def initialize_ellipsoids(self):\n        # Initial ellipsoid: center, covariance matrix\n        center = np.random.uniform(self.lb, self.ub, size=self.dim)\n        initial_covariance = np.eye(self.dim) * (self.initial_volume**(1/self.dim))\n        self.ellipsoids = [(center, initial_covariance)]\n\n    def sample_ellipsoid(self, center, covariance, num_samples=1):\n        A = np.linalg.cholesky(covariance)\n        z = np.random.normal(size=(num_samples, self.dim))\n        samples = center + z @ A.T\n        samples = np.clip(samples, self.lb, self.ub)\n        return samples\n\n    def update_ellipsoid(self, ellipsoid, successful_points):\n        center, covariance = ellipsoid\n        if len(successful_points) > self.dim + 1:\n            new_center = np.mean(successful_points, axis=0)\n            centered_points = successful_points - new_center\n            new_covariance = np.cov(centered_points, rowvar=False)\n            # Regularize covariance matrix to avoid singularity\n            new_covariance += np.eye(self.dim) * 1e-6\n            return new_center, new_covariance\n        else:\n            return center, covariance  # No significant update\n\n    def __call__(self, func):\n        self.initialize_ellipsoids()\n        eval_count = 0\n\n        while eval_count < self.budget:\n            new_ellipsoids = []\n            for center, covariance in self.ellipsoids:\n                # Sample points from ellipsoid\n                samples = self.sample_ellipsoid(center, covariance, num_samples=self.pop_size)\n                fitness_values = np.array([func(x) for x in samples])\n                eval_count += self.pop_size\n                \n                #Update best solution\n                for i in range(self.pop_size):\n                    if fitness_values[i] < self.f_opt:\n                        self.f_opt = fitness_values[i]\n                        self.x_opt = samples[i]\n\n                # Identify successful points\n                threshold = np.mean(fitness_values)\n                successful_points = samples[fitness_values < threshold]\n\n                # Update ellipsoid parameters\n                new_center, new_covariance = self.update_ellipsoid((center, covariance), successful_points)\n                new_ellipsoids.append((new_center, new_covariance))\n\n            self.ellipsoids = new_ellipsoids\n\n            # Adaptive ellipsoid management: split or merge (simplified)\n            if len(self.ellipsoids) < 5 and eval_count < self.budget * 0.8 :  # Limit ellipsoid count\n                center = np.random.uniform(self.lb, self.ub, size=self.dim)\n                initial_covariance = np.eye(self.dim) * (self.initial_volume**(1/self.dim))\n                self.ellipsoids.append((center, initial_covariance))\n\n            if eval_count >= self.budget:\n                break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm EllipsoidalEvolution scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0]}, "task_prompt": ""}
{"id": "0fb5b0e0-1a17-450e-a583-2560e13d7f25", "fitness": 0.16618555562706194, "name": "AdaptiveGaussianOptimization", "description": "No description provided.", "code": "import numpy as np\n\nclass AdaptiveGaussianOptimization:\n    def __init__(self, budget=10000, dim=10, pop_size=50, learning_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.learning_rate = learning_rate\n        self.mean = None\n        self.std = None\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        # Initialization\n        self.mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        self.std = np.ones(self.dim) * 0.5  # Initial standard deviation\n\n        population = np.random.normal(self.mean, self.std, size=(self.pop_size, self.dim))\n        population = np.clip(population, func.bounds.lb, func.bounds.ub)\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        while self.budget > 0:\n            # Generate new population\n            new_population = np.random.normal(self.mean, self.std, size=(self.pop_size, self.dim))\n            new_population = np.clip(new_population, func.bounds.lb, func.bounds.ub)\n            new_fitness = np.array([func(x) for x in new_population])\n            self.budget -= self.pop_size\n            if self.budget <= 0:\n                new_fitness = new_fitness[:self.budget + self.pop_size]\n                new_population = new_population[:self.budget + self.pop_size]\n\n\n            # Selection: Combine old and new populations, select best\n            combined_population = np.concatenate((population, new_population))\n            combined_fitness = np.concatenate((fitness, new_fitness))\n            \n            \n            sorted_indices = np.argsort(combined_fitness)\n            best_indices = sorted_indices[:self.pop_size]\n            \n            population = combined_population[best_indices]\n            fitness = combined_fitness[best_indices]\n\n            # Update mean and std\n            self.mean = np.mean(population, axis=0)\n            self.std = np.std(population, axis=0)\n            self.std = np.maximum(self.std, 1e-6) # Ensure standard deviation is not zero\n\n            # Adapt step size (std) based on success\n            if fitness[0] < self.f_opt:\n                self.f_opt = fitness[0]\n                self.x_opt = population[0]\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveGaussianOptimization scored 0.166 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.05824568268483343, 0.07572016658266278, 0.2054716715375411, 0.096368775143436, 0.10444788942113958, 0.14943660070919684, 0.08687304545652563, 0.08401870923645915, 0.354537891912054, 0.1306686966219941, 0.11843189602058424, 0.6111112494583235, 0.20700244335972706, 0.07668156292241124, 0.10134291126322736, 0.2321716515829677, 0.09668075452537983, 0.14430111487979813, 0.21407879745990455, 0.1761196017630724]}, "task_prompt": ""}
{"id": "aaba1548-94c1-4881-bb7d-8747a7456aef", "fitness": 0.0, "name": "GradientGuidedSearch", "description": "No description provided.", "code": "import numpy as np\n\nclass GradientGuidedSearch:\n    def __init__(self, budget=10000, dim=10, exploration_prob=0.2, step_size=0.1, num_gradient_samples=5):\n        self.budget = budget\n        self.dim = dim\n        self.exploration_prob = exploration_prob\n        self.step_size = step_size\n        self.num_gradient_samples = num_gradient_samples\n        self.lb = -5.0\n        self.ub = 5.0\n\n    def estimate_gradient(self, func, x):\n        gradient = np.zeros(self.dim)\n        for i in range(self.dim):\n            delta = np.zeros(self.dim)\n            delta[i] = self.step_size\n            \n            x_plus = np.clip(x + delta, self.lb, self.ub)\n            x_minus = np.clip(x - delta, self.lb, self.ub)\n            \n            gradient[i] = (func(x_plus) - func(x_minus)) / (2 * self.step_size)\n            self.budget -= 2\n            if self.budget <= 0:\n              break\n        return gradient\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        \n        # Initial guess\n        x = np.random.uniform(self.lb, self.ub, size=self.dim)\n        f = func(x)\n        self.budget -= 1\n        \n        self.f_opt = f\n        self.x_opt = x\n\n        while self.budget > 0:\n            if np.random.rand() < self.exploration_prob:\n                # Exploration: Random search\n                x_new = np.random.uniform(self.lb, self.ub, size=self.dim)\n                f_new = func(x_new)\n                self.budget -= 1\n\n                if f_new < self.f_opt:\n                    self.f_opt = f_new\n                    self.x_opt = x_new\n                    x = x_new\n                    f = f_new\n\n            else:\n                # Exploitation: Gradient-guided search\n                gradient = self.estimate_gradient(func, x)\n                if self.budget <= 0:\n                  break\n                \n                # Normalize gradient\n                norm = np.linalg.norm(gradient)\n                if norm > 0:\n                    gradient = gradient / norm\n                \n                x_new = np.clip(x - self.step_size * gradient, self.lb, self.ub)\n                f_new = func(x_new)\n                self.budget -= 1\n\n                if f_new < self.f_opt:\n                    self.f_opt = f_new\n                    self.x_opt = x_new\n                    x = x_new\n                    f = f_new\n                else:\n                    # If the gradient step doesn't improve, try a smaller random step\n                    x_new = np.clip(x + np.random.normal(0, self.step_size/2, size=self.dim), self.lb, self.ub)\n                    f_new = func(x_new)\n                    self.budget -= 1\n                    if f_new < self.f_opt:\n                        self.f_opt = f_new\n                        self.x_opt = x_new\n                        x = x_new\n                        f = f_new\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm GradientGuidedSearch scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0]}, "task_prompt": ""}
{"id": "59407d1b-e552-4fb9-8d41-cc5d9e5bf81a", "fitness": 0.5171545740567121, "name": "AdaptiveCauchySearch", "description": "No description provided.", "code": "import numpy as np\n\nclass AdaptiveCauchySearch:\n    def __init__(self, budget=10000, dim=10, pop_size=20, initial_step_size=1.0, local_search_iterations=5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lb = -5.0\n        self.ub = 5.0\n        self.step_size = initial_step_size\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.local_search_iterations = local_search_iterations\n\n    def cauchy_mutation(self, x):\n        return x + self.step_size * np.random.standard_cauchy(size=self.dim)\n\n    def local_search(self, func, x):\n        best_x = x.copy()\n        best_f = func(x)\n        self.budget -= 1\n\n        for _ in range(self.local_search_iterations):\n            if self.budget <= 0:\n                break\n            new_x = np.clip(x + np.random.normal(0, self.step_size/2, size=self.dim), self.lb, self.ub)\n            new_f = func(new_x)\n            self.budget -= 1\n\n            if new_f < best_f:\n                best_f = new_f\n                best_x = new_x.copy()\n        \n        return best_f, best_x\n\n    def __call__(self, func):\n        # Initialize population\n        population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        # Find best initial individual\n        best_index = np.argmin(fitness)\n        self.x_opt = population[best_index].copy()\n        self.f_opt = fitness[best_index]\n\n        while self.budget > 0:\n            # Generate new individuals using Cauchy mutation\n            new_population = np.array([np.clip(self.cauchy_mutation(self.x_opt), self.lb, self.ub) for _ in range(self.pop_size)])\n            new_fitness = np.array([func(x) for x in new_population])\n            self.budget -= self.pop_size\n\n            # Update best individual\n            for i in range(self.pop_size):\n                if new_fitness[i] < self.f_opt:\n                    self.f_opt = new_fitness[i]\n                    self.x_opt = new_population[i].copy()\n            \n            # Local search around the best solution\n            local_f, local_x = self.local_search(func, self.x_opt.copy())\n            if local_f < self.f_opt:\n                 self.f_opt = local_f\n                 self.x_opt = local_x.copy()\n\n            # Adjust step size\n            self.step_size *= 0.95\n\n            self.step_size = np.clip(self.step_size, (self.ub - self.lb) / 1000, (self.ub - self.lb) / 2)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveCauchySearch scored 0.517 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.12189723812391806, 0.1868717289408749, 0.5445439965116492, 0.9254061486063957, 0.33917984169536153, 0.6255751068333828, 0.3684605772862386, 0.47048223888029295, 0.5657087586706017, 0.2149144302549031, 0.9113007004359837, 0.9979545134408324, 0.24170623051451245, 0.39727354188978903, 0.8757858018002957, 0.6211244595794922, 0.49197308861532796, 0.7408235757029873, 0.20441376282202073, 0.4976957405293796]}, "task_prompt": ""}
{"id": "b2ebad56-663c-413d-b10c-0cbd3005390a", "fitness": 0.3438534532857211, "name": "AdaptiveES", "description": "No description provided.", "code": "import numpy as np\n\nclass AdaptiveES:\n    def __init__(self, budget=10000, dim=10, pop_size=20, tau=None, tau_prime=None, initial_step_size=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lb = -5.0\n        self.ub = 5.0\n        self.tau = tau or (1 / np.sqrt(2 * dim))\n        self.tau_prime = tau_prime or (1 / np.sqrt(2 * np.sqrt(dim)))\n        self.initial_step_size = initial_step_size\n\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        # Initialize population\n        population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        step_sizes = np.full((self.pop_size, self.dim), self.initial_step_size)\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        # Find initial best\n        best_idx = np.argmin(fitness)\n        self.f_opt = fitness[best_idx]\n        self.x_opt = population[best_idx].copy()\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutate step sizes\n                xi = np.random.normal(0, 1, size=self.dim)\n                global_mutation = self.tau_prime * np.random.normal(0, 1)\n                new_step_sizes = step_sizes[i] * np.exp(global_mutation + self.tau * xi)\n                new_step_sizes = np.clip(new_step_sizes, 1e-10, 1) # Avoid zero step sizes\n\n                # Mutate solution\n                mutation = np.random.normal(0, new_step_sizes, size=self.dim)\n                trial = population[i] + mutation\n                trial = np.clip(trial, self.lb, self.ub)\n\n                f_trial = func(trial)\n                self.budget -= 1\n\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    population[i] = trial.copy()\n                    step_sizes[i] = new_step_sizes.copy()\n\n\n                    # Update optimal solution\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial.copy()\n\n                if self.budget <= 0:\n                    break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveES scored 0.344 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.11609502020170515, 0.1828589473375989, 0.37622704929110473, 0.2631154654400387, 0.25321016497763793, 0.334977711311768, 0.28277449750270134, 0.25517238647699814, 0.27096783579632033, 0.16989008976844, 0.31044466605221954, 0.9805417782437532, 0.26425946975490655, 0.31951967284352667, 0.7404330170526617, 0.34432039802856884, 0.2720653968070401, 0.47553379134467355, 0.19461790380741473, 0.4700438036753434]}, "task_prompt": ""}
{"id": "a0ef87ec-b8f5-4704-943d-fcaa79f5a11a", "fitness": 0.16906201820179162, "name": "AdaptiveSimulatedAnnealing", "description": "No description provided.", "code": "import numpy as np\n\nclass AdaptiveSimulatedAnnealing:\n    def __init__(self, budget=10000, dim=10, initial_temp=100.0, cooling_rate=0.95, min_temp=1e-5, reanneal_factor=1.1):\n        self.budget = budget\n        self.dim = dim\n        self.initial_temp = initial_temp\n        self.cooling_rate = cooling_rate\n        self.min_temp = min_temp\n        self.reanneal_factor = reanneal_factor\n        self.eval_count = 0\n\n    def __call__(self, func):\n        x = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        self.f_opt = func(x)\n        self.x_opt = x\n        self.eval_count += 1\n\n        temp = self.initial_temp\n\n        while self.eval_count < self.budget and temp > self.min_temp:\n            x_new = x + np.random.uniform(-0.1, 0.1, size=self.dim) * temp  # Perturbation scaled by temperature\n            x_new = np.clip(x_new, -5.0, 5.0)\n            f_new = func(x_new)\n            self.eval_count += 1\n\n            delta_e = f_new - self.f_opt\n\n            if delta_e < 0:\n                self.f_opt = f_new\n                self.x_opt = x_new\n                x = x_new\n            else:\n                acceptance_probability = np.exp(-delta_e / temp)\n                if np.random.rand() < acceptance_probability:\n                    x = x_new\n\n            temp *= self.cooling_rate\n            \n            # Re-annealing strategy: If no improvement after a certain number of iterations, re-anneal\n            if self.eval_count % (self.budget // 10) == 0:\n                if func(self.x_opt) > self.f_opt:  # Check if x_opt is still valid\n                    temp *= self.reanneal_factor # Increase the temperature\n        \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveSimulatedAnnealing scored 0.169 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.07360658028173761, 0.1319899484701187, 0.23510023561412596, 0.15561328306713762, 0.12315050771302971, 0.15595927897214223, 0.1944132274045688, 0.1373589154758189, 0.14366838887872113, 0.12434890093058348, 0.16047099039226265, 0.20761414997212535, 0.20547749448887465, 0.1295287709329359, 0.1349281968497562, 0.20800405647072007, 0.17658078088882811, 0.17253130395854754, 0.10431512172923452, 0.4065802315445629]}, "task_prompt": ""}
{"id": "f9655dcc-9252-40ea-b32a-2e2a67c8e6a2", "fitness": 0.6386856024173647, "name": "AdaptiveDifferentialEvolution", "description": "No description provided.", "code": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget=10000, dim=10, pop_size=20, F=0.5, CR=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F  # Mutation factor\n        self.CR = CR  # Crossover rate\n        self.lb = -5.0\n        self.ub = 5.0\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        # Initialize population\n        population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        # Find initial best\n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index]\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                \n                # Self-adaptive F\n                F_adaptive = np.random.normal(self.F, 0.1)\n                F_adaptive = np.clip(F_adaptive, 0.1, 1.0)\n\n                mutant = a + F_adaptive * (b - c)\n                mutant = np.clip(mutant, self.lb, self.ub)\n\n                # Crossover\n                trial = np.copy(population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR or j == j_rand:\n                        trial[j] = mutant[j]\n\n                # Evaluation\n                f_trial = func(trial)\n                self.budget -= 1\n\n                # Selection\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n\n                    # Update best\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n            \n            best_index = np.argmin(fitness)\n            if fitness[best_index] < self.f_opt:\n                self.f_opt = fitness[best_index]\n                self.x_opt = population[best_index]\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveDifferentialEvolution scored 0.639 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.25162169518971644, 0.40228838643342346, 0.44095651469321895, 0.9464962564612532, 0.890189633557396, 0.9141116353789536, 0.8544164088681412, 0.5276198688909433, 0.45204058594803653, 0.8599443445755431, 0.3329514745552473, 0.9987493180154472, 0.32137485631864016, 0.8594207084945515, 0.7594852371565903, 0.9179896397206454, 0.39113081538511474, 0.9428720954674072, 0.2239812600430796, 0.4860713131939446]}, "task_prompt": ""}
{"id": "374501e0-726b-4440-a37c-2555fce6eb13", "fitness": 0.23537459388310417, "name": "GaussianDE_SA", "description": "No description provided.", "code": "import numpy as np\n\nclass GaussianDE_SA:\n    def __init__(self, budget=10000, dim=10, pop_size=50, mutation_rate=0.1, crossover_rate=0.5, initial_temp=1.0, cooling_rate=0.99):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.mutation_rate = mutation_rate\n        self.crossover_rate = crossover_rate\n        self.initial_temp = initial_temp\n        self.cooling_rate = cooling_rate\n        self.population = None\n        self.fitness = None\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.temperature = self.initial_temp\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.eval_count += self.pop_size\n\n        best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_index]\n        self.x_opt = self.population[best_index]\n\n    def mutate(self, func):\n        for i in range(self.pop_size):\n            mutant = self.population[i] + self.mutation_rate * np.random.normal(0, 1, self.dim)\n            mutant = np.clip(mutant, -5.0, 5.0)\n\n            cross_points = np.random.rand(self.dim) < self.crossover_rate\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n\n            trial = np.where(cross_points, mutant, self.population[i])\n            f_trial = func(trial)\n            self.eval_count += 1\n\n            delta_e = f_trial - self.fitness[i]\n            if delta_e < 0:\n                self.fitness[i] = f_trial\n                self.population[i] = trial\n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = trial\n            else:\n                acceptance_prob = np.exp(-delta_e / self.temperature)\n                if np.random.rand() < acceptance_prob:\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.eval_count < self.budget:\n            self.mutate(func)\n            self.temperature *= self.cooling_rate\n            if self.temperature < 0.0001:\n                self.temperature = 0.0001\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm GaussianDE_SA scored 0.235 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.09888275972234606, 0.2149897787194912, 0.26292429622720404, 0.1734060766120321, 0.17144861646224685, 0.1854838729981021, 0.22549875519714402, 0.17934125350725594, 0.16962732373278633, 0.13542997035096183, 0.2076561340814702, 0.46738337696239085, 0.2799942567334982, 0.18312958033480942, 0.48870667294295667, 0.24873762032204394, 0.19295152422281825, 0.19510812679787426, 0.17897005167426228, 0.4478218300603889]}, "task_prompt": ""}
{"id": "109b2f6a-9eba-45f0-9c0a-1b79f9f0d46f", "fitness": "-inf", "name": "AdaptiveDE_NM", "description": "No description provided.", "code": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveDE_NM:\n    def __init__(self, budget=10000, dim=10, pop_size=20, de_mutation_factor=0.5, de_crossover_rate=0.7, nm_iterations=5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lb = -5.0\n        self.ub = 5.0\n        self.de_mutation_factor = de_mutation_factor\n        self.de_crossover_rate = de_crossover_rate\n        self.nm_iterations = nm_iterations\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.stagnation_counter = 0\n        self.max_stagnation = 50\n        self.using_de = True\n\n    def differential_evolution(self, func, population):\n        new_population = np.zeros_like(population)\n        fitness = np.zeros(self.pop_size)\n\n        for i in range(self.pop_size):\n            if self.budget <= 0:\n                break\n\n            # Mutation\n            indices = np.random.choice(self.pop_size, 4, replace=False)\n            x_r1, x_r2, x_r3, x_r4 = population[indices]\n            v_mutation = x_r1 + self.de_mutation_factor * (x_r2 - x_r3)\n\n            # Crossover\n            u_crossover = np.zeros(self.dim)\n            for j in range(self.dim):\n                if np.random.rand() < self.de_crossover_rate or j == np.random.randint(0, self.dim):\n                    u_crossover[j] = v_mutation[j]\n                else:\n                    u_crossover[j] = population[i][j]\n            u_crossover = np.clip(u_crossover, self.lb, self.ub)\n\n            # Selection\n            f = func(u_crossover)\n            self.budget -= 1\n\n            fitness[i] = f\n            new_population[i] = u_crossover\n\n        return new_population, fitness\n\n    def nelder_mead(self, func, x0):\n        if self.budget <= 0:\n            return self.f_opt, self.x_opt\n        \n        result = minimize(func, x0, method='Nelder-Mead', bounds=[(self.lb, self.ub)] * self.dim, options={'maxiter': self.nm_iterations, 'maxfev': self.budget})\n        \n        if result.success:\n          self.budget -= result.nfev\n          return result.fun, result.x\n        else:\n          return self.f_opt, self.x_opt\n\n    def __call__(self, func):\n        # Initialize population\n        population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        # Find best initial individual\n        best_index = np.argmin(fitness)\n        self.x_opt = population[best_index].copy()\n        self.f_opt = fitness[best_index]\n\n        while self.budget > 0:\n            if self.using_de:\n                # Differential Evolution\n                new_population, new_fitness = self.differential_evolution(func, population)\n\n                # Update best individual\n                for i in range(self.pop_size):\n                    if new_fitness[i] < self.f_opt:\n                        self.f_opt = new_fitness[i]\n                        self.x_opt = new_population[i].copy()\n                \n                population = new_population.copy()\n                \n                # Stagnation check\n                if np.min(new_fitness) >= self.f_opt:\n                    self.stagnation_counter +=1\n                else:\n                    self.stagnation_counter = 0\n                    \n                if self.stagnation_counter > self.max_stagnation:\n                    self.using_de = False\n                    self.stagnation_counter = 0\n\n            else:\n                # Nelder-Mead\n                local_f, local_x = self.nelder_mead(func, self.x_opt.copy())\n                if local_f < self.f_opt:\n                    self.f_opt = local_f\n                    self.x_opt = local_x.copy()\n                \n                self.using_de = True # switch back to DE\n                self.stagnation_counter = 0 # reset stagnation\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "In the code, line 55, in nelder_mead, the following error occurred:\nNameError: name 'minimize' is not defined\nOn line: result = minimize(func, x0, method='Nelder-Mead', bounds=[(self.lb, self.ub)] * self.dim, options={'maxiter': self.nm_iterations, 'maxfev': self.budget})", "error": "In the code, line 55, in nelder_mead, the following error occurred:\nNameError: name 'minimize' is not defined\nOn line: result = minimize(func, x0, method='Nelder-Mead', bounds=[(self.lb, self.ub)] * self.dim, options={'maxiter': self.nm_iterations, 'maxfev': self.budget})", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "d7b2cf2b-a5ea-4d94-9343-0491989b181f", "fitness": 0.6074630745935419, "name": "AdaptiveCMAES", "description": "No description provided.", "code": "import numpy as np\n\nclass AdaptiveCMAES:\n    def __init__(self, budget=10000, dim=10, pop_size=None, sigma=0.5, cs=0.8, c_cov=0.05, restart_trigger=100):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size if pop_size is not None else 4 + int(3 * np.log(self.dim))\n        self.sigma = sigma\n        self.cs = cs\n        self.c_cov = c_cov\n        self.lb = -5.0\n        self.ub = 5.0\n        self.mean = np.random.uniform(self.lb, self.ub, size=self.dim)\n        self.C = np.eye(self.dim)\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.path_success = np.zeros(self.dim)\n        self.restart_trigger = restart_trigger\n        self.no_improvement_count = 0\n        self.best_f_history = []\n\n    def sample_population(self):\n        z = np.random.multivariate_normal(np.zeros(self.dim), np.eye(self.dim), size=self.pop_size)\n        x = self.mean + self.sigma * np.dot(z, np.linalg.cholesky(self.C).T)\n        x = np.clip(x, self.lb, self.ub)\n        return x\n\n    def update_parameters(self, x, fitness):\n        idx = np.argsort(fitness)\n        x_best = x[idx[0]]\n        f_best = fitness[idx[0]]\n\n        if f_best < self.f_opt:\n            self.f_opt = f_best\n            self.x_opt = x_best\n            self.no_improvement_count = 0\n        else:\n            self.no_improvement_count += 1\n\n        y = (x_best - self.mean) / self.sigma\n        self.path_success = (1 - self.cs) * self.path_success + np.sqrt(self.cs * (2 - self.cs)) * y\n        self.mean = x_best\n\n        self.C = (1 - self.c_cov) * self.C + self.c_cov * np.outer(self.path_success, self.path_success)\n        try:\n            np.linalg.cholesky(self.C)\n        except np.linalg.LinAlgError:\n            self.C += 1e-6 * np.eye(self.dim)\n\n        if self.no_improvement_count > self.restart_trigger:\n            self.mean = np.random.uniform(self.lb, self.ub, size=self.dim)\n            self.C = np.eye(self.dim)\n            self.sigma = 0.5\n            self.no_improvement_count = 0\n\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.no_improvement_count = 0\n        self.best_f_history = []\n        self.mean = np.random.uniform(self.lb, self.ub, size=self.dim)\n        self.C = np.eye(self.dim)\n        self.path_success = np.zeros(self.dim)\n\n        while self.eval_count < self.budget:\n            x = self.sample_population()\n            fitness = np.array([func(xi) for xi in x])\n            self.eval_count += self.pop_size\n\n            self.update_parameters(x, fitness)\n\n            if self.eval_count >= self.budget:\n                break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveCMAES scored 0.607 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.20368972065724456, 0.7120371323402501, 0.6922664418990856, 0.5181237641025026, 0.7335704846535945, 0.7684101085809298, 0.33668321679653423, 0.6197269183474018, 0.5018933450625729, 0.22475819002538078, 0.8957852911484159, 0.9777236552659937, 0.3775491965683292, 0.7165873474261331, 0.6985259280205095, 0.7679456399168556, 0.4471957898438216, 0.8632144939632002, 0.590578959116077, 0.5029958681360038]}, "task_prompt": ""}
{"id": "07efee04-c59f-4b23-811a-23e4b512cc65", "fitness": 0.0, "name": "DynamicParticleSwarm", "description": "No description provided.", "code": "import numpy as np\n\nclass DynamicParticleSwarm:\n    def __init__(self, budget=10000, dim=10, pop_size=30, w_max=0.9, w_min=0.4, c1=2, c2=2, v_max=1.0):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.w_max = w_max  # Inertia weight maximum\n        self.w_min = w_min  # Inertia weight minimum\n        self.c1 = c1  # Cognitive coefficient\n        self.c2 = c2  # Social coefficient\n        self.v_max = v_max # Maximum velocity\n        self.lb = -5.0\n        self.ub = 5.0\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        # Initialize particles and velocities\n        particles = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        velocities = np.random.uniform(-self.v_max, self.v_max, size=(self.pop_size, self.dim))\n        \n        # Initialize personal best positions and fitnesses\n        pbest_positions = np.copy(particles)\n        pbest_fitness = np.array([func(x) for x in particles])\n        self.budget -= self.pop_size\n\n        # Find initial global best\n        best_index = np.argmin(pbest_fitness)\n        self.f_opt = pbest_fitness[best_index]\n        self.x_opt = pbest_positions[best_index]\n        \n        # PSO iterations\n        while self.budget > 0:\n            # Dynamic inertia weight adjustment\n            w = self.w_max - (self.w_max - self.w_min) * (self.budget / self.budget)\n            \n            for i in range(self.pop_size):\n                # Update velocity\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                \n                velocities[i] = w * velocities[i] + \\\n                                self.c1 * r1 * (pbest_positions[i] - particles[i]) + \\\n                                self.c2 * r2 * (self.x_opt - particles[i])\n                \n                # Velocity clamping\n                velocities[i] = np.clip(velocities[i], -self.v_max, self.v_max)\n\n                # Update particle position\n                particles[i] = particles[i] + velocities[i]\n                \n                # Boundary handling\n                particles[i] = np.clip(particles[i], self.lb, self.ub)\n                \n                # Evaluate fitness\n                fitness = func(particles[i])\n                self.budget -= 1\n\n                # Update personal best\n                if fitness < pbest_fitness[i]:\n                    pbest_fitness[i] = fitness\n                    pbest_positions[i] = np.copy(particles[i])\n                    \n                    # Update global best\n                    if fitness < self.f_opt:\n                        self.f_opt = fitness\n                        self.x_opt = particles[i]\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm DynamicParticleSwarm scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0]}, "task_prompt": ""}
{"id": "5aac9ea4-7eb6-4cb3-89d5-39b302dcb9a0", "fitness": "-inf", "name": "CMAES", "description": "No description provided.", "code": "import numpy as np\n\nclass CMAES:\n    def __init__(self, budget=10000, dim=10, pop_size=None, sigma=0.5, cs=0.3, dsigma=0.2, c_cov=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size if pop_size is not None else 4 + int(3 * np.log(dim))\n        self.sigma = sigma\n        self.mean = None\n        self.C = None\n        self.lb = -5.0\n        self.ub = 5.0\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.cs = cs\n        self.dsigma = dsigma\n        self.c_cov = c_cov\n        self.p_sigma = np.zeros(dim)\n        self.p_c = np.zeros(dim)\n        self.eigenspace = None\n        self.eval_count = 0\n\n    def initialize(self):\n        self.mean = np.random.uniform(self.lb, self.ub, size=self.dim)\n        self.C = np.eye(self.dim)\n        eigenvalues, eigenvectors = np.linalg.eigh(self.C)\n        self.eigenspace = eigenvectors\n\n    def sample_population(self):\n        z = np.random.normal(0, 1, size=(self.pop_size, self.dim))\n        samples = self.mean + self.sigma * self.eigenspace @ (np.sqrt(np.diag(self.C)) * z.T).T\n        samples = np.clip(samples, self.lb, self.ub)\n        return samples\n\n    def update_distribution(self, population, fitness):\n        best_indices = np.argsort(fitness)\n        selected_indices = best_indices[:self.pop_size // 2]\n        selected_samples = population[selected_indices]\n\n        old_mean = self.mean.copy()\n        self.mean = np.mean(selected_samples, axis=0)\n        \n        z = (self.mean - old_mean) / self.sigma\n        self.p_sigma = (1 - self.cs) * self.p_sigma + np.sqrt(self.cs * (2 - self.cs)) * z\n        self.sigma *= np.exp((self.dsigma / self.dim) * (np.linalg.norm(self.p_sigma) / np.sqrt(self.dim) - 1))\n        self.sigma = np.clip(self.sigma, 1e-10, 10)\n\n        rank_one = np.outer(z, z)\n        self.C = (1 - self.c_cov) * self.C + self.c_cov * rank_one\n\n        try:\n            eigenvalues, eigenvectors = np.linalg.eigh(self.C)\n            self.eigenspace = eigenvectors\n            self.C = eigenvectors @ np.diag(eigenvalues) @ eigenvectors.T\n        except np.linalg.LinAlgError:\n            self.C = np.eye(self.dim)\n            eigenvalues, eigenvectors = np.linalg.eigh(self.C)\n            self.eigenspace = eigenvectors\n\n    def __call__(self, func):\n        self.initialize()\n\n        while self.eval_count < self.budget:\n            population = self.sample_population()\n            fitness = np.array([func(x) for x in population])\n            self.eval_count += self.pop_size\n\n            best_index = np.argmin(fitness)\n            if fitness[best_index] < self.f_opt:\n                self.f_opt = fitness[best_index]\n                self.x_opt = population[best_index]\n\n            self.update_distribution(population, fitness)\n        \n            if self.eval_count >= self.budget:\n                break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "In the code, line 31, in sample_population, the following error occurred:\nValueError: operands could not be broadcast together with shapes (2,) (2,6) \nOn line: samples = self.mean + self.sigma * self.eigenspace @ (np.sqrt(np.diag(self.C)) * z.T).T", "error": "In the code, line 31, in sample_population, the following error occurred:\nValueError: operands could not be broadcast together with shapes (2,) (2,6) \nOn line: samples = self.mean + self.sigma * self.eigenspace @ (np.sqrt(np.diag(self.C)) * z.T).T", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "27f711f2-d879-4ef7-bdfd-82bd4b4b08f3", "fitness": "-inf", "name": "GaussianProcessOptimization", "description": "No description provided.", "code": "import numpy as np\nfrom sklearn.gaussian_process import GaussianProcessRegressor\nfrom sklearn.gaussian_process.kernels import RBF, ConstantKernel as C\n\nclass GaussianProcessOptimization:\n    def __init__(self, budget=10000, dim=10, n_initial=10, kernel=None, alpha=1e-5):\n        self.budget = budget\n        self.dim = dim\n        self.n_initial = n_initial\n        self.lb = -5.0\n        self.ub = 5.0\n        self.f_opt = np.inf\n        self.x_opt = None\n        if kernel is None:\n            self.kernel = C(1.0, constant_value_bounds=\"fixed\") * RBF(1.0, length_scale_bounds=\"fixed\")\n        else:\n            self.kernel = kernel\n        self.alpha = alpha\n        self.X = None\n        self.y = None\n        self.gp = None\n\n    def acquisition_function(self, x, gp):\n        mu, sigma = gp.predict(x.reshape(1, -1), return_std=True)\n        return -mu + 2 * sigma  # Upper confidence bound\n\n    def __call__(self, func):\n        # Initial sampling\n        self.X = np.random.uniform(self.lb, self.ub, size=(self.n_initial, self.dim))\n        self.y = np.array([func(x) for x in self.X])\n        self.budget -= self.n_initial\n\n        best_index = np.argmin(self.y)\n        self.f_opt = self.y[best_index]\n        self.x_opt = self.X[best_index]\n\n        # Gaussian process regression\n        self.gp = GaussianProcessRegressor(kernel=self.kernel, n_restarts_optimizer=10, alpha=self.alpha)\n        self.gp.fit(self.X, self.y)\n\n        while self.budget > 0:\n            # Find next point to evaluate using acquisition function\n            x_next = None\n            best_acq = -np.inf\n            for _ in range(1000): # Sample multiple points and pick best\n                x_sample = np.random.uniform(self.lb, self.ub, size=self.dim)\n                acq = self.acquisition_function(x_sample, self.gp)\n                if acq > best_acq:\n                    best_acq = acq\n                    x_next = x_sample\n\n            # Evaluate function\n            f_next = func(x_next)\n            self.budget -= 1\n\n            # Update data\n            self.X = np.vstack((self.X, x_next))\n            self.y = np.append(self.y, f_next)\n\n            # Update best\n            if f_next < self.f_opt:\n                self.f_opt = f_next\n                self.x_opt = x_next\n\n            # Update Gaussian process\n            self.gp.fit(self.X, self.y)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "In the code, line 15, in __init__, the following error occurred:\nNameError: name 'C' is not defined\nOn line: self.kernel = C(1.0, constant_value_bounds=\"fixed\") * RBF(1.0, length_scale_bounds=\"fixed\")", "error": "In the code, line 15, in __init__, the following error occurred:\nNameError: name 'C' is not defined\nOn line: self.kernel = C(1.0, constant_value_bounds=\"fixed\") * RBF(1.0, length_scale_bounds=\"fixed\")", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "4ba5ad6f-c926-4d1c-9ba0-71a6ffb643da", "fitness": 0.29275538569317094, "name": "AdaptiveSimulatedAnnealing", "description": "No description provided.", "code": "import numpy as np\n\nclass AdaptiveSimulatedAnnealing:\n    def __init__(self, budget=10000, dim=10, initial_temp=1.0, cooling_rate=0.99, temp_decay_factor=0.01):\n        self.budget = budget\n        self.dim = dim\n        self.initial_temp = initial_temp\n        self.cooling_rate = cooling_rate\n        self.temp_decay_factor = temp_decay_factor\n        self.lb = -5.0\n        self.ub = 5.0\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.eval_count = 0\n\n    def __call__(self, func):\n        x = np.random.uniform(self.lb, self.ub, size=self.dim)\n        f = func(x)\n        self.eval_count += 1\n        self.f_opt = f\n        self.x_opt = x\n        temp = self.initial_temp\n        \n        acceptance_rate = 0.0\n\n        while self.eval_count < self.budget:\n            x_new = x + np.random.normal(0, temp, size=self.dim)\n            x_new = np.clip(x_new, self.lb, self.ub)\n            f_new = func(x_new)\n            self.eval_count += 1\n\n            delta_f = f_new - f\n\n            if delta_f < 0 or np.random.rand() < np.exp(-delta_f / temp):\n                x = x_new\n                f = f_new\n                if f < self.f_opt:\n                    self.f_opt = f\n                    self.x_opt = x\n                acceptance_rate += 1\n            \n            if self.eval_count % 100 == 0:\n                if acceptance_rate / 100 > 0.5:\n                    temp *= (1 + self.temp_decay_factor)\n                else:\n                    temp *= (1 - self.temp_decay_factor)\n                acceptance_rate = 0.0\n            \n            temp *= self.cooling_rate\n            temp = max(temp, 1e-6)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveSimulatedAnnealing scored 0.293 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.11561050329147382, 0.16343084793201812, 0.21835081792699418, 0.3735263634707964, 0.25091284501189903, 0.2246176849886179, 0.2512668074279406, 0.3405162836790876, 0.2917501727134043, 0.14797249214277453, 0.17548234925791817, 0.9985353662177509, 0.21615810507697186, 0.2509093367693347, 0.574181739369965, 0.24159150298058352, 0.20948034903889645, 0.2938821170085696, 0.32747893138217044, 0.18945309817625022]}, "task_prompt": ""}
{"id": "889e695b-57b2-4118-a820-b294fc476685", "fitness": "-inf", "name": "HybridDE", "description": "No description provided.", "code": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.7, Cr=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.Cr = Cr\n        self.lb = -5.0\n        self.ub = 5.0\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.zeros(self.pop_size)\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.eval_count = 0\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.eval_count = 0\n\n        # Initial evaluation\n        for i in range(self.pop_size):\n            self.fitness[i] = func(self.population[i])\n            self.eval_count += 1\n\n            if self.fitness[i] < self.f_opt:\n                self.f_opt = self.fitness[i]\n                self.x_opt = self.population[i]\n\n        # DE iterations\n        while self.eval_count < self.budget:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = self.population[idxs]\n                x_mutated = x_r1 + self.F * (x_r2 - x_r3)\n                x_mutated = np.clip(x_mutated, self.lb, self.ub)\n\n                # Crossover\n                x_trial = np.copy(self.population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < self.Cr or j == j_rand:\n                        x_trial[j] = x_mutated[j]\n\n                # Evaluation\n                f_trial = func(x_trial)\n                self.eval_count += 1\n\n                # Selection\n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.population[i] = x_trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = x_trial\n\n            # Local search on best\n            res = minimize(func, self.x_opt, method='Nelder-Mead', bounds=[(self.lb, self.ub)] * self.dim, options={'maxfev': max(1, self.budget - self.eval_count)})\n            if res.success:\n                  self.x_opt = res.x\n                  self.f_opt = res.fun\n            self.eval_count = min(self.budget, self.eval_count + res.nfev) #Ensuring evaluation budget constraint\n            if self.eval_count >= self.budget:\n                  break\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "In the code, line 63, in __call__, the following error occurred:\nNameError: name 'minimize' is not defined\nOn line: res = minimize(func, self.x_opt, method='Nelder-Mead', bounds=[(self.lb, self.ub)] * self.dim, options={'maxfev': max(1, self.budget - self.eval_count)})", "error": "In the code, line 63, in __call__, the following error occurred:\nNameError: name 'minimize' is not defined\nOn line: res = minimize(func, self.x_opt, method='Nelder-Mead', bounds=[(self.lb, self.ub)] * self.dim, options={'maxfev': max(1, self.budget - self.eval_count)})", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "1926f74c-1cee-4dc8-9a56-0d36201323fb", "fitness": 0.21496700618793066, "name": "AdaptiveSimulatedAnnealing", "description": "No description provided.", "code": "import numpy as np\n\nclass AdaptiveSimulatedAnnealing:\n    def __init__(self, budget=10000, dim=10, initial_temp=1.0, temp_min=0.0001, alpha=0.99):\n        self.budget = budget\n        self.dim = dim\n        self.initial_temp = initial_temp\n        self.temp_min = temp_min\n        self.alpha = alpha\n        self.temp = initial_temp\n        self.x_opt = None\n        self.f_opt = np.inf\n        self.eval_count = 0\n\n    def __call__(self, func):\n        x = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        f = func(x)\n        self.eval_count += 1\n\n        self.x_opt = x\n        self.f_opt = f\n\n        while self.eval_count < self.budget and self.temp > self.temp_min:\n            x_new = x + np.random.uniform(-0.1, 0.1, size=self.dim)\n            x_new = np.clip(x_new, func.bounds.lb, func.bounds.ub)\n            f_new = func(x_new)\n            self.eval_count += 1\n\n            delta_f = f_new - f\n\n            if delta_f < 0 or np.random.rand() < np.exp(-delta_f / self.temp):\n                x = x_new\n                f = f_new\n\n                if f < self.f_opt:\n                    self.f_opt = f\n                    self.x_opt = x\n                    \n            self.temp *= self.alpha\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveSimulatedAnnealing scored 0.215 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.0, 0.05010076358354254, 0.1904081876984416, 0.9417024777440435, 0.0838136802983317, 0.1423695157894732, 0.1606526476213529, 0.1308632475890238, 0.1415197089335647, 0.10956126950385747, 0.449156780612179, 0.1428128509984805, 0.22602457215208127, 0.1472949146841358, 0.17407421526030942, 0.25275193696998066, 0.24001128720694476, 0.16794848783674443, 0.13312569912940575, 0.41514788014672044]}, "task_prompt": ""}
{"id": "29bcfa00-1a63-465e-8a12-60a4279d8a00", "fitness": 0.5284084010959178, "name": "ParticleSwarmRestart", "description": "No description provided.", "code": "import numpy as np\n\nclass ParticleSwarmRestart:\n    def __init__(self, budget=10000, dim=10, pop_size=20, inertia=0.7, cognitive_coeff=1.4, social_coeff=1.4, stagnation_threshold=1000):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.inertia = inertia\n        self.cognitive_coeff = cognitive_coeff\n        self.social_coeff = social_coeff\n        self.lb = -5.0\n        self.ub = 5.0\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.stagnation_threshold = stagnation_threshold\n        self.stagnation_counter = 0\n        self.best_fitness_history = []\n\n    def initialize_swarm(self, func):\n        self.swarm = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.swarm])\n        self.pbest_swarm = self.swarm.copy()\n        self.pbest_fitness = self.fitness.copy()\n        \n        best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_index]\n        self.x_opt = self.swarm[best_index]\n        self.budget -= self.pop_size\n    \n    def update_swarm(self, func):\n        for i in range(self.pop_size):\n            r1 = np.random.rand(self.dim)\n            r2 = np.random.rand(self.dim)\n            \n            self.velocities[i] = (self.inertia * self.velocities[i] +\n                                self.cognitive_coeff * r1 * (self.pbest_swarm[i] - self.swarm[i]) +\n                                self.social_coeff * r2 * (self.x_opt - self.swarm[i]))\n            \n            self.swarm[i] = self.swarm[i] + self.velocities[i]\n            self.swarm[i] = np.clip(self.swarm[i], self.lb, self.ub)\n            \n            fitness = func(self.swarm[i])\n            self.budget -= 1\n            \n            if fitness < self.pbest_fitness[i]:\n                self.pbest_fitness[i] = fitness\n                self.pbest_swarm[i] = self.swarm[i].copy()\n                \n                if fitness < self.f_opt:\n                    self.f_opt = fitness\n                    self.x_opt = self.swarm[i].copy()\n\n    def __call__(self, func):\n        self.initialize_swarm(func)\n        \n        while self.budget > 0:\n            self.update_swarm(func)\n            \n            self.best_fitness_history.append(self.f_opt)\n            if len(self.best_fitness_history) > self.stagnation_threshold:\n                if np.std(self.best_fitness_history[-self.stagnation_threshold:]) < 1e-6:\n                    self.stagnation_counter += 1\n                else:\n                    self.stagnation_counter = 0\n\n                if self.stagnation_counter > 5:\n                    self.initialize_swarm(func)  # Restart\n                    self.stagnation_counter = 0\n                    self.best_fitness_history = []\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm ParticleSwarmRestart scored 0.528 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.2316736938466335, 0.21331796549426174, 0.8721477496461707, 0.2472819485303508, 0.27356879486513386, 0.28492731732515264, 0.33573179969671485, 0.7412581635925284, 0.8979928576405061, 0.20801651078371308, 0.9377464503532784, 0.9977117968138369, 0.2808100650127987, 0.5781197108369229, 0.5878188536372922, 0.9126108953712091, 0.7475080076730747, 0.3809691202299237, 0.35659610343484305, 0.4823602171340109]}, "task_prompt": ""}
{"id": "646a329d-d00c-4800-9aac-de91f0a0720a", "fitness": 0.28185556770173115, "name": "SocialLearningOptimization", "description": "No description provided.", "code": "import numpy as np\n\nclass SocialLearningOptimization:\n    def __init__(self, budget=10000, dim=10, pop_size=20, learning_rate=0.1, exploration_prob=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.learning_rate = learning_rate\n        self.exploration_prob = exploration_prob\n        self.lb = -5.0\n        self.ub = 5.0\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        # Initialize population\n        population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        # Find initial best\n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index]\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                if np.random.rand() < self.exploration_prob:\n                    # Exploration: Randomly move the individual\n                    new_x = np.random.uniform(self.lb, self.ub, size=self.dim)\n                else:\n                    # Social Learning: Move towards a better individual\n                    better_indices = np.where(fitness < fitness[i])[0]\n                    if len(better_indices) > 0:\n                        chosen_index = np.random.choice(better_indices)\n                        chosen_x = population[chosen_index]\n                        new_x = population[i] + self.learning_rate * (chosen_x - population[i])\n                        new_x = np.clip(new_x, self.lb, self.ub)\n                    else:\n                        new_x = np.random.uniform(self.lb, self.ub, size=self.dim)\n\n                # Evaluation\n                f_new = func(new_x)\n                self.budget -= 1\n\n                # Selection: Replace if better\n                if f_new < fitness[i]:\n                    population[i] = new_x\n                    fitness[i] = f_new\n\n                    # Update best\n                    if f_new < self.f_opt:\n                        self.f_opt = f_new\n                        self.x_opt = new_x\n            \n            best_index = np.argmin(fitness)\n            if fitness[best_index] < self.f_opt:\n                self.f_opt = fitness[best_index]\n                self.x_opt = population[best_index]\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm SocialLearningOptimization scored 0.282 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.10021791341503872, 0.19112841079423326, 0.2683923627139644, 0.2024296390793222, 0.19836563908020743, 0.2938303055092766, 0.24554920165647787, 0.2145220260172429, 0.2148242009306257, 0.17500817769198362, 0.2090887044512334, 0.9970192209889907, 0.2623139364792624, 0.19124935347482852, 0.5651172253760888, 0.2639213726411813, 0.22338500781044446, 0.2183892067997787, 0.15425337300467867, 0.4481060761197636]}, "task_prompt": ""}
{"id": "dc1a11d1-40ea-4f2e-8fd9-61fa750c15b4", "fitness": "-inf", "name": "CMAES_Restart_NelderMead", "description": "No description provided.", "code": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass CMAES_Restart_NelderMead:\n    def __init__(self, budget=10000, dim=10, initial_sigma=0.5, restart_threshold=1e-12, nelder_mead_iterations=50):\n        self.budget = budget\n        self.dim = dim\n        self.lb = -5.0\n        self.ub = 5.0\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.initial_sigma = initial_sigma\n        self.restart_threshold = restart_threshold\n        self.nelder_mead_iterations = nelder_mead_iterations\n\n    def __call__(self, func):\n        mean = np.random.uniform(self.lb, self.ub, size=self.dim)\n        sigma = self.initial_sigma\n        C = np.eye(self.dim)  # Covariance matrix\n\n        while self.budget > 0:\n            # Sample a new point from the multivariate normal distribution\n            z = np.random.normal(0, 1, size=self.dim)\n            x = mean + sigma * np.dot(np.linalg.cholesky(C), z)\n            x = np.clip(x, self.lb, self.ub)\n\n            f = func(x)\n            self.budget -= 1\n\n            if f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = x.copy()\n\n            # Update the mean and covariance matrix using a simplified CMA-ES update rule\n            mean = 0.9 * mean + 0.1 * x\n            C = 0.9 * C + 0.1 * np.outer(x - mean, x - mean)\n\n            # Restart strategy: If the function value hasn't improved significantly, restart with a new mean\n            if self.f_opt - f < self.restart_threshold or self.budget % 1000 == 0:\n                mean = np.random.uniform(self.lb, self.ub, size=self.dim)\n                sigma = self.initial_sigma\n                C = np.eye(self.dim)\n\n            # Local search with Nelder-Mead\n            if self.budget > self.nelder_mead_iterations:\n                res = minimize(func, self.x_opt, method='Nelder-Mead', \n                               bounds=[(self.lb, self.ub)] * self.dim, \n                               options={'maxiter': self.nelder_mead_iterations, 'maxfev': self.nelder_mead_iterations})\n                \n                self.budget -= res.nfev\n                if res.fun < self.f_opt:\n                    self.f_opt = res.fun\n                    self.x_opt = res.x\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "In the code, line 46, in __call__, the following error occurred:\nNameError: name 'minimize' is not defined\nOn line: res = minimize(func, self.x_opt, method='Nelder-Mead',", "error": "In the code, line 46, in __call__, the following error occurred:\nNameError: name 'minimize' is not defined\nOn line: res = minimize(func, self.x_opt, method='Nelder-Mead',", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "dd65d975-199b-46ce-a8ce-0bf7570d970a", "fitness": 0.600762081520377, "name": "ClampedPSO", "description": "No description provided.", "code": "import numpy as np\n\nclass ClampedPSO:\n    def __init__(self, budget=10000, dim=10, pop_size=20, w=0.7, c1=1.5, c2=1.5, v_max_ratio=0.2):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.w = w  # Inertia weight\n        self.c1 = c1  # Cognitive coefficient\n        self.c2 = c2  # Social coefficient\n        self.lb = -5.0\n        self.ub = 5.0\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.v_max = v_max_ratio * (self.ub - self.lb)\n        self.K = 2 / abs(2 - (self.c1 + self.c2) - np.sqrt((self.c1 + self.c2)**2 - 4*(self.c1 + self.c2))) if (self.c1 + self.c2) > 4 else 1\n    def __call__(self, func):\n        # Initialize particles and velocities\n        particles = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        velocities = np.random.uniform(-self.v_max, self.v_max, size=(self.pop_size, self.dim))\n        \n        # Initialize personal best positions and fitnesses\n        personal_best_positions = np.copy(particles)\n        fitness = np.array([func(x) for x in particles])\n        personal_best_fitness = np.copy(fitness)\n        self.budget -= self.pop_size\n\n        # Find initial global best\n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = particles[best_index]\n\n        while self.budget > 0:\n            # Update velocities and positions\n            r1 = np.random.rand(self.pop_size, self.dim)\n            r2 = np.random.rand(self.pop_size, self.dim)\n\n            global_best_position = np.tile(self.x_opt, (self.pop_size, 1))  # Repeat global best\n\n            velocities = self.K * (self.w * velocities +\n                               self.c1 * r1 * (personal_best_positions - particles) +\n                               self.c2 * r2 * (global_best_position - particles))\n\n            velocities = np.clip(velocities, -self.v_max, self.v_max)\n            particles = particles + velocities\n            particles = np.clip(particles, self.lb, self.ub)\n\n            # Evaluate new positions\n            new_fitness = np.array([func(x) for x in particles])\n            self.budget -= self.pop_size\n\n            # Update personal bests\n            for i in range(self.pop_size):\n                if new_fitness[i] < personal_best_fitness[i]:\n                    personal_best_fitness[i] = new_fitness[i]\n                    personal_best_positions[i] = np.copy(particles[i])\n\n            # Update global best\n            best_index = np.argmin(new_fitness)\n            if new_fitness[best_index] < self.f_opt:\n                self.f_opt = new_fitness[best_index]\n                self.x_opt = np.copy(particles[best_index])\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm ClampedPSO scored 0.601 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.15388851345799137, 0.3553645082778968, 0.8486178167709681, 0.956680326241636, 0.742307223391169, 0.9020721131724598, 0.32801106089046184, 0.6657894102028724, 0.8682691018481927, 0.2369728505573021, 0.9178590755982659, 0.994391282743029, 0.31990304692291516, 0.2591230964670821, 0.7376952061106832, 0.880521226948621, 0.7679042971305781, 0.38220171754178733, 0.19378748121750333, 0.5038822749161254]}, "task_prompt": ""}
{"id": "86fb65ef-11b2-45b8-81fb-8bd1d37906a5", "fitness": 0.36820126558981403, "name": "AdaptiveDifferentialEvolutionRestart", "description": "No description provided.", "code": "import numpy as np\n\nclass AdaptiveDifferentialEvolutionRestart:\n    def __init__(self, budget=10000, dim=10, pop_size=20, F=0.5, CR=0.9, stagnation_threshold=50):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F  # Mutation factor\n        self.CR = CR  # Initial Crossover rate\n        self.lb = -5.0\n        self.ub = 5.0\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.stagnation_threshold = stagnation_threshold\n        self.stagnation_counter = 0\n        self.previous_best = np.inf\n        self.success_history = []\n\n    def __call__(self, func):\n        # Initialize population\n        population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        # Find initial best\n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index]\n        self.previous_best = self.f_opt\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                \n                # Self-adaptive F\n                F_adaptive = np.random.normal(self.F, 0.1)\n                F_adaptive = np.clip(F_adaptive, 0.1, 1.0)\n\n                mutant = a + F_adaptive * (b - c)\n                mutant = np.clip(mutant, self.lb, self.ub)\n\n                # Crossover\n                trial = np.copy(population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR or j == j_rand:\n                        trial[j] = mutant[j]\n\n                # Evaluation\n                f_trial = func(trial)\n                self.budget -= 1\n\n                # Selection\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n\n                    # Update best\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                        self.success_history.append(1)\n                    else:\n                        self.success_history.append(0)\n                else:\n                    self.success_history.append(0)\n            \n            best_index = np.argmin(fitness)\n            if fitness[best_index] < self.f_opt:\n                self.f_opt = fitness[best_index]\n                self.x_opt = population[best_index]\n                self.stagnation_counter = 0\n            else:\n                self.stagnation_counter += 1\n            \n            # Stagnation Check and Restart\n            if self.stagnation_counter > self.stagnation_threshold:\n                population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n                fitness = np.array([func(x) for x in population])\n                self.budget -= self.pop_size # Account for new evaluations\n                best_index = np.argmin(fitness)\n                self.f_opt = fitness[best_index]\n                self.x_opt = population[best_index]\n                self.stagnation_counter = 0\n                self.CR = 0.9  # Reset CR\n                self.previous_best = self.f_opt\n\n            # Adjust Crossover Rate (CR) based on success\n            if len(self.success_history) > self.pop_size:\n                 success_rate = np.mean(self.success_history[-self.pop_size:])\n                 self.CR = np.clip(success_rate, 0.1, 0.9)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveDifferentialEvolutionRestart scored 0.368 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.14360378135387986, 0.23269915799119234, 0.3687449165444404, 0.28191189856895005, 0.2877825014966412, 0.395495064029475, 0.29908342320650916, 0.3331738146684188, 0.33476436556197453, 0.20017428062859222, 0.3208301974471067, 0.9884440377067093, 0.2710328072823145, 0.31415445329057545, 0.7295500176774965, 0.40082092729480734, 0.29558013104546266, 0.46494712429152396, 0.20577745564792804, 0.49545495606228196]}, "task_prompt": ""}
{"id": "2f6bb1cb-586d-4de2-adb4-b6043535e709", "fitness": 0.7468116103481907, "name": "AdaptiveCMAES_DE", "description": "No description provided.", "code": "import numpy as np\n\nclass AdaptiveCMAES_DE:\n    def __init__(self, budget=10000, dim=10, pop_size=20, F=0.5, CR=0.9, sigma=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F  # Mutation factor\n        self.CR = CR  # Crossover rate\n        self.sigma = sigma # Step size\n        self.lb = -5.0\n        self.ub = 5.0\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.C = np.eye(dim)  # Covariance matrix\n\n    def __call__(self, func):\n        # Initialize population\n        population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        # Find initial best\n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index]\n\n        while self.budget > 0:\n            # Sort population by fitness\n            sorted_indices = np.argsort(fitness)\n            population = population[sorted_indices]\n            fitness = fitness[sorted_indices]\n\n            # Update covariance matrix (simplified CMA-ES update)\n            weights = np.log(self.pop_size + 0.5) - np.log(np.arange(1, self.pop_size + 1))\n            weights /= np.sum(weights)\n            \n            mean = np.sum(population * weights[:, None], axis=0)\n            \n            C_update = np.zeros_like(self.C)\n            for i in range(self.pop_size):\n                diff = population[i] - mean\n                C_update += weights[i] * np.outer(diff, diff)\n            \n            self.C = (1 - 0.1) * self.C + 0.1 * C_update  # Learning rate\n\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n\n                # Sample mutation vector from multivariate normal distribution\n                z = np.random.multivariate_normal(np.zeros(self.dim), self.C)\n                mutant = a + self.F * (b - c) + self.sigma * z\n                mutant = np.clip(mutant, self.lb, self.ub)\n                \n\n                # Crossover\n                trial = np.copy(population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR or j == j_rand:\n                        trial[j] = mutant[j]\n\n                # Evaluation\n                f_trial = func(trial)\n                self.budget -= 1\n\n                # Selection\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n\n                    # Update best\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n            best_index = np.argmin(fitness)\n            if fitness[best_index] < self.f_opt:\n                self.f_opt = fitness[best_index]\n                self.x_opt = population[best_index]\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveCMAES_DE scored 0.747 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.5322428941663363, 0.23523991344696438, 0.8043736559034721, 0.9005754577775097, 0.8498613718254031, 0.8536180306841434, 0.712247852225755, 0.7785088206137283, 0.8397676023051481, 0.79976023880356, 0.9261048136917914, 0.9955057769133334, 0.6883186152485676, 0.8385361362323398, 0.8953891513239157, 0.8557195920574592, 0.797714154825165, 0.9125951676507923, 0.2128548573722946, 0.5072981038961333]}, "task_prompt": ""}
{"id": "498aa00d-6ccb-4337-8b6f-ce01f6d4d6fe", "fitness": 0.7231379327131535, "name": "AdaptiveDifferentialEvolution", "description": "No description provided.", "code": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget=10000, dim=10, pop_size=20, F=0.5, Cr=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F  # Mutation factor\n        self.Cr = Cr  # Crossover rate\n        self.lb = -5.0\n        self.ub = 5.0\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n\n        best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_index]\n        self.x_opt = self.population[best_index]\n\n    def evolve(self, func):\n        for i in range(self.pop_size):\n            # Mutation\n            idxs = [idx for idx in range(self.pop_size) if idx != i]\n            a, b, c = np.random.choice(idxs, 3, replace=False)\n            mutant = self.population[a] + self.F * (self.population[b] - self.population[c])\n            mutant = np.clip(mutant, self.lb, self.ub)\n\n            # Crossover\n            trial = self.population[i].copy()\n            j_rand = np.random.randint(self.dim)\n            for j in range(self.dim):\n                if np.random.rand() < self.Cr or j == j_rand:\n                    trial[j] = mutant[j]\n\n            # Selection\n            f = func(trial)\n            self.budget -= 1\n            if f < self.fitness[i]:\n                self.fitness[i] = f\n                self.population[i] = trial\n\n                if f < self.f_opt:\n                    self.f_opt = f\n                    self.x_opt = trial.copy()\n                    \n            # Adaptive F and Cr (optional, but can improve performance)\n            if np.random.rand() < 0.1:\n                self.F = np.random.uniform(0.1, 0.9)\n                self.Cr = np.random.uniform(0.1, 0.9)\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        while self.budget > 0:\n            self.evolve(func)\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveDifferentialEvolution scored 0.723 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.3656424332809334, 0.2073862602781995, 0.7586860147893433, 0.9362095904266288, 0.857292487175318, 0.8911702437392897, 0.8190466259994033, 0.8249700823222426, 0.8622902576937361, 0.7581884042917246, 0.8895792931193663, 0.9970968094174301, 0.35955391817064986, 0.7428503150902908, 0.9045016043477151, 0.884323344937605, 0.6368808036587379, 0.8780248206833328, 0.3555568144797382, 0.533508530361386]}, "task_prompt": ""}
{"id": "6068ad46-9562-4407-b938-ff9aa4884eeb", "fitness": "-inf", "name": "CMAES_Restart", "description": "No description provided.", "code": "import numpy as np\n\nclass CMAES_Restart:\n    def __init__(self, budget=10000, dim=10, pop_size=None, sigma0=0.2, restarts=3):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size if pop_size is not None else 4 + int(3 * np.log(dim))\n        self.sigma0 = sigma0\n        self.restarts = restarts\n        self.eval_count = 0\n        self.x_mean = None\n        self.sigma = None\n        self.C = None\n        self.pc = None\n        self.ps = None\n        self.eigenspace_ready = False\n        self.B = None\n        self.D = None\n        self.mueff = None\n        self.mu = self.pop_size // 2\n        self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.cc = (4 + self.mueff / self.dim) / (self.dim + 4 + 2 * self.mueff / self.dim)\n        self.cs = (self.mueff + 2) / (self.dim + self.mueff + 5)\n        self.c1 = 2 / ((self.dim + 1.3)**2 + self.mueff)\n        self.cmu = min(1 - self.c1, 2 * (self.mueff - 2 + 1 / self.mueff) / ((self.dim + 2)**2 + self.mueff))\n        self.damps = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1) + self.cs\n\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.restart_count = 0\n\n    def initialize(self, func):\n        self.x_mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        self.sigma = self.sigma0\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.eigenspace_ready = False\n\n    def sample_population(self):\n        z = np.random.normal(0, 1, size=(self.pop_size, self.dim))\n        if self.eigenspace_ready:\n            y = self.B @ (self.D * z.T)\n            x = self.x_mean + self.sigma * y.T\n        else:\n            x = self.x_mean + self.sigma * z\n        x = np.clip(x, -5.0, 5.0)\n        return x\n\n    def update_distribution(self, x, fitness):\n        x_sorted = x[np.argsort(fitness)]\n        y = (x_sorted[:self.mu] - self.x_mean) / self.sigma\n        \n        self.x_mean = np.sum(self.weights[:, None] * x_sorted[:self.mu], axis=0)\n        \n        ps_tmp = np.sqrt(self.cs * (2 - self.cs) * self.mueff) * np.linalg.solve(self.B @ np.diag(self.D**2) @ self.B.T, (x_sorted[:self.mu] - self.x_mean).sum(axis=0))\n        self.ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs) * self.mueff) * (x_sorted[:self.mu] - self.x_mean).sum(axis=0) / self.sigma\n        \n        hsig = np.linalg.norm(self.ps) / np.sqrt(1 - (1 - self.cs)**(2 * (self.eval_count // self.pop_size))) < (1.4 + 2 / (self.dim + 1)) * np.sqrt(self.dim)\n        self.pc = (1 - self.cc) * self.pc + hsig * np.sqrt(self.cc * (2 - self.cc) * self.mueff) * y.sum(axis=0)\n\n        self.C = (1 - self.c1 - self.cmu) * self.C + self.c1 * (self.pc[:, None] @ self.pc[None, :]) + self.cmu * (y.T @ np.diag(self.weights) @ y)\n        \n        self.sigma *= np.exp((self.cs / self.damps) * (np.linalg.norm(self.ps) / np.sqrt(self.dim) - 1))\n\n    def prepare_eigenspace(self):\n        try:\n            self.D, self.B = np.linalg.eigh(self.C)\n            self.D = np.sqrt(np.maximum(self.D, 1e-16))\n            self.eigenspace_ready = True\n        except np.linalg.LinAlgError:\n            self.C += 1e-6 * np.eye(self.dim)\n            self.D, self.B = np.linalg.eigh(self.C)\n            self.D = np.sqrt(np.maximum(self.D, 1e-16))\n            self.eigenspace_ready = True\n\n    def restart(self, func):\n        self.restart_count += 1\n        self.x_mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        self.sigma = self.sigma0\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.eigenspace_ready = False\n\n    def __call__(self, func):\n        self.initialize(func)\n\n        while self.eval_count < self.budget:\n            x = self.sample_population()\n            fitness = np.array([func(xi) for xi in x])\n            self.eval_count += self.pop_size\n\n            best_index = np.argmin(fitness)\n            if fitness[best_index] < self.f_opt:\n                self.f_opt = fitness[best_index]\n                self.x_opt = x[best_index]\n\n            self.update_distribution(x, fitness)\n            \n            if self.eval_count // self.pop_size % (self.budget // (self.pop_size * self.restarts)) == 0 and self.restart_count < self.restarts - 1:\n                self.restart(func)\n            \n            if not self.eigenspace_ready and self.eval_count > self.dim * 5:\n                self.prepare_eigenspace()\n            \n            if self.eval_count >= self.budget:\n                break\n        \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "In the code, line 23, in __init__, the following error occurred:\nTypeError: unsupported operand type(s) for /: 'NoneType' and 'int'\nOn line: self.cc = (4 + self.mueff / self.dim) / (self.dim + 4 + 2 * self.mueff / self.dim)", "error": "In the code, line 23, in __init__, the following error occurred:\nTypeError: unsupported operand type(s) for /: 'NoneType' and 'int'\nOn line: self.cc = (4 + self.mueff / self.dim) / (self.dim + 4 + 2 * self.mueff / self.dim)", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "39ca8cd5-b843-4e85-b9aa-55e0a1b118ca", "fitness": 0.5637817637238409, "name": "ConstrictionPSO", "description": "No description provided.", "code": "import numpy as np\n\nclass ConstrictionPSO:\n    def __init__(self, budget=10000, dim=10, pop_size=20, omega=0.729, phi_p=2.05, phi_g=2.05):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.omega = omega  # Inertia weight\n        self.phi_p = phi_p  # Personal learning coefficient\n        self.phi_g = phi_g  # Global learning coefficient\n        self.lb = -5.0\n        self.ub = 5.0\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.k = 2.0 / abs(2.0 - (self.phi_p + self.phi_g) - np.sqrt((self.phi_p + self.phi_g)**2 - 4 * (self.phi_p + self.phi_g))) if (self.phi_p + self.phi_g) > 4 else 1\n\n    def initialize_swarm(self, func):\n        self.swarm = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, size=(self.pop_size, self.dim)) * (self.ub - self.lb) * 0.1  # Initialize velocities\n        self.fitness = np.array([func(x) for x in self.swarm])\n        self.budget -= self.pop_size\n        self.personal_best_positions = self.swarm.copy()\n        self.personal_best_fitness = self.fitness.copy()\n\n        best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_index]\n        self.x_opt = self.swarm[best_index]\n        self.global_best_position = self.x_opt.copy()\n\n    def update_swarm(self, func):\n        for i in range(self.pop_size):\n            r_p = np.random.rand(self.dim)\n            r_g = np.random.rand(self.dim)\n\n            # Velocity update with clamping\n            self.velocities[i] = self.k * (self.omega * self.velocities[i] +\n                                       self.phi_p * r_p * (self.personal_best_positions[i] - self.swarm[i]) +\n                                       self.phi_g * r_g * (self.global_best_position - self.swarm[i]))\n\n            v_max = (self.ub - self.lb) * 0.1\n            self.velocities[i] = np.clip(self.velocities[i], -v_max, v_max) #velocity clamping\n\n            # Position update\n            self.swarm[i] += self.velocities[i]\n            self.swarm[i] = np.clip(self.swarm[i], self.lb, self.ub)\n\n            # Evaluate fitness\n            f = func(self.swarm[i])\n            self.budget -= 1\n\n            # Update personal best\n            if f < self.personal_best_fitness[i]:\n                self.personal_best_fitness[i] = f\n                self.personal_best_positions[i] = self.swarm[i].copy()\n\n            # Update global best\n            if f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = self.swarm[i].copy()\n                self.global_best_position = self.x_opt.copy()\n\n    def __call__(self, func):\n        self.initialize_swarm(func)\n        while self.budget > 0:\n            self.update_swarm(func)\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm ConstrictionPSO scored 0.564 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.18612469517363528, 0.48964071166898293, 0.9295146375167535, 0.18041337053789064, 0.9349249275050332, 0.9453089946789796, 0.2626718236123262, 0.6779821768894694, 0.1778227035489064, 0.18604533793456435, 0.9419638422097011, 0.9971888901912892, 0.23619778020808346, 0.5116076228567662, 0.7384219450946627, 0.9376717541862666, 0.29432591536148134, 0.9519625777892202, 0.19904326896063163, 0.4968022985521722]}, "task_prompt": ""}
{"id": "03d16e61-c7f3-48b5-bb14-d2c6469d0f7a", "fitness": "-inf", "name": "CMAES_Restart", "description": "No description provided.", "code": "import numpy as np\n\nclass CMAES_Restart:\n    def __init__(self, budget=10000, dim=10, pop_size=None, sigma=0.5, mu_percentage=0.5, restart_threshold=1e-12):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size if pop_size is not None else 4 + int(3 * np.log(dim))\n        self.sigma = sigma\n        self.mu = int(self.pop_size * mu_percentage)\n        self.weights = np.log(self.pop_size + 1) - np.log(np.arange(1, self.pop_size + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        self.cs = (self.mueff + 2) / (self.dim + self.mueff + 5)\n        self.damps = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1) + self.cs\n        self.ccov1 = (1 / self.mueff) * min(1, (2 / ((self.dim + 1.3)**2 + self.mueff)))\n        self.ccovmu = min(1 - self.ccov1, (2 * (self.mueff - 2 + 1/self.mueff)) / ((self.dim + 2)**2 + self.mueff))\n        self.chiN = self.dim**0.5 * (1 - (1/(4*self.dim)) + 1/(21*self.dim**2))\n        self.ps = np.zeros(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.C = np.eye(self.dim)\n        self.B = np.eye(self.dim)\n        self.D = np.ones(self.dim)\n        self.invsqrtC = np.eye(self.dim)\n        self.m = None\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.restart_threshold = restart_threshold\n        self.last_f_opt = np.inf\n        self.no_improvement_count = 0\n\n    def update_decomposition(self):\n        self.C = np.triu(self.C) + np.triu(self.C, 1).T\n        self.D, self.B = np.linalg.eigh(self.C)\n        self.D = np.sqrt(np.abs(self.D))\n        self.invsqrtC = np.dot(self.B, np.dot(np.diag(self.D**-1), self.B.T))\n\n    def sample_population(self):\n        z = np.random.normal(0, 1, size=(self.pop_size, self.dim))\n        x = self.m + self.sigma * np.dot(z, self.B * self.D)\n        return x, z\n\n    def __call__(self, func):\n        self.m = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.last_f_opt = np.inf\n        self.no_improvement_count = 0\n        self.ps = np.zeros(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.C = np.eye(self.dim)\n        self.B = np.eye(self.dim)\n        self.D = np.ones(self.dim)\n        self.invsqrtC = np.eye(self.dim)\n\n        while self.eval_count < self.budget:\n            x, z = self.sample_population()\n            x = np.clip(x, func.bounds.lb, func.bounds.ub)\n            f = np.array([func(xi) for xi in x])\n            self.eval_count += self.pop_size\n\n            if np.min(f) < self.f_opt:\n                self.f_opt = np.min(f)\n                self.x_opt = x[np.argmin(f)]\n\n            idx = np.argsort(f)\n            x_sorted = x[idx]\n            z_sorted = z[idx]\n\n            y = x_sorted[:self.mu] - self.m\n            z_mu = z_sorted[:self.mu]\n\n            y_w = np.sum((y.T * self.weights).T, axis=0)\n            z_w = np.sum((z_mu.T * self.weights).T, axis=0)\n\n            self.ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs) * self.mueff) * np.dot(self.invsqrtC, z_w)\n            hsig = (np.linalg.norm(self.ps) / np.sqrt(1 - (1 - self.cs)**(2 * (self.eval_count / self.pop_size))) / self.chiN) < (1.4 + 2 / (self.dim + 1))\n            self.pc = (1 - self.damps * self.cs) * self.pc + hsig * np.sqrt(self.damps * (2 - self.damps) * self.mueff) * y_w\n\n            artmp = (1 / self.sigma) * y_mu\n            self.C = (1 - self.ccov1 - self.ccovmu + self.ccov1 * (self.ccovmu > 0) * (2 / (self.sigma**2)) * np.sum((artmp.T * self.weights).T * artmp, axis=0)) * self.C + self.ccov1 * self.pc[:, None] @ self.pc[None, :] + self.ccovmu * np.sum((artmp.T * self.weights).T[:, :, None] * artmp[:, :, None].transpose(0, 2, 1), axis=0)\n\n            self.sigma *= np.exp((self.cs / self.damps) * (np.linalg.norm(self.ps) / self.chiN - 1))\n\n            self.m = x_sorted[0] + self.pc\n\n            self.update_decomposition()\n\n            if self.f_opt < self.last_f_opt - self.restart_threshold:\n                self.no_improvement_count = 0\n                self.last_f_opt = self.f_opt\n            else:\n                self.no_improvement_count += self.pop_size\n\n            if self.no_improvement_count > self.budget / 5: #dynamic restart condition\n               self.m = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n               self.sigma *= 2\n               self.ps = np.zeros(self.dim)\n               self.pc = np.zeros(self.dim)\n               self.C = np.eye(self.dim)\n               self.B = np.eye(self.dim)\n               self.D = np.ones(self.dim)\n               self.invsqrtC = np.eye(self.dim)\n               self.no_improvement_count = 0\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "In the code, line 74, in __call__, the following error occurred:\nValueError: operands could not be broadcast together with shapes (2,3) (6,) \nOn line: y_w = np.sum((y.T * self.weights).T, axis=0)", "error": "In the code, line 74, in __call__, the following error occurred:\nValueError: operands could not be broadcast together with shapes (2,3) (6,) \nOn line: y_w = np.sum((y.T * self.weights).T, axis=0)", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "4da1bc43-84d9-4a0b-b1e0-6247811158dd", "fitness": 0.18352632283982395, "name": "AdaptiveSimulatedAnnealing", "description": "No description provided.", "code": "import numpy as np\n\nclass AdaptiveSimulatedAnnealing:\n    def __init__(self, budget=10000, dim=10, initial_temp=100, cooling_rate=0.95, restart_prob=0.05):\n        self.budget = budget\n        self.dim = dim\n        self.initial_temp = initial_temp\n        self.cooling_rate = cooling_rate\n        self.restart_prob = restart_prob\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.eval_count = 0\n\n    def __call__(self, func):\n        x = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        f = func(x)\n        self.eval_count += 1\n        self.f_opt = f\n        self.x_opt = x\n        temp = self.initial_temp\n\n        while self.eval_count < self.budget:\n            x_new = x + np.random.normal(0, temp/10, size=self.dim)\n            x_new = np.clip(x_new, func.bounds.lb, func.bounds.ub)\n            f_new = func(x_new)\n            self.eval_count += 1\n\n            delta_f = f_new - f\n            if delta_f < 0 or np.random.rand() < np.exp(-delta_f / temp):\n                x = x_new\n                f = f_new\n                if f < self.f_opt:\n                    self.f_opt = f\n                    self.x_opt = x\n\n            temp *= self.cooling_rate\n            if np.random.rand() < self.restart_prob:\n                x = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                f = func(x)\n                self.eval_count += 1\n                if f < self.f_opt:\n                    self.f_opt = f\n                    self.x_opt = x\n                temp = self.initial_temp\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveSimulatedAnnealing scored 0.184 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.12801510864960564, 0.1932592065248946, 0.24777029413383234, 0.21288006997272035, 0.2014213935153013, 0.222990101101185, 0.23332290575180825, 0.2120778259090682, 0]}, "task_prompt": ""}
{"id": "68994a63-acf4-4358-b71f-4d91cf1a1ee8", "fitness": 0.6479638554501794, "name": "ParticleSwarmOptimizer", "description": "No description provided.", "code": "import numpy as np\n\nclass ParticleSwarmOptimizer:\n    def __init__(self, budget=10000, dim=10, pop_size=20, inertia=0.7, c1=1.5, c2=1.5, v_max=1.0):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.inertia = inertia\n        self.c1 = c1\n        self.c2 = c2\n        self.v_max = v_max\n        self.lb = -5.0\n        self.ub = 5.0\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        # Initialize particles and velocities\n        particles = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        velocities = np.random.uniform(-self.v_max, self.v_max, size=(self.pop_size, self.dim))\n        \n        # Initialize personal best positions and fitnesses\n        personal_best_positions = particles.copy()\n        personal_best_fitnesses = np.array([func(x) for x in particles])\n        self.budget -= self.pop_size\n\n        # Find initial global best\n        best_index = np.argmin(personal_best_fitnesses)\n        self.f_opt = personal_best_fitnesses[best_index]\n        self.x_opt = personal_best_positions[best_index].copy()\n        \n        global_best_position = self.x_opt.copy()\n\n        while self.budget > 0:\n            # Update inertia weight (linearly decreasing)\n            inertia = self.inertia * (self.budget / (self.budget + self.pop_size)) #+ 0.4\n\n            for i in range(self.pop_size):\n                # Update velocity\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                \n                cognitive_velocity = self.c1 * r1 * (personal_best_positions[i] - particles[i])\n                social_velocity = self.c2 * r2 * (global_best_position - particles[i])\n                \n                velocities[i] = inertia * velocities[i] + cognitive_velocity + social_velocity\n                velocities[i] = np.clip(velocities[i], -self.v_max, self.v_max)  # Velocity clamping\n                \n                # Update position\n                particles[i] = particles[i] + velocities[i]\n                particles[i] = np.clip(particles[i], self.lb, self.ub)\n                \n                # Evaluate fitness\n                fitness = func(particles[i])\n                self.budget -= 1\n                \n                # Update personal best\n                if fitness < personal_best_fitnesses[i]:\n                    personal_best_fitnesses[i] = fitness\n                    personal_best_positions[i] = particles[i].copy()\n                    \n                    # Update global best\n                    if fitness < self.f_opt:\n                        self.f_opt = fitness\n                        self.x_opt = particles[i].copy()\n                        global_best_position = self.x_opt.copy()\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm ParticleSwarmOptimizer scored 0.648 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.13774238739466382, 0.3879676538868354, 0.873596874678767, 0.9622906261680667, 0.8840649222799943, 0.9129756300455585, 0.33687589085890857, 0.7120633255124551, 0.8993983317754126, 0.18076725158499918, 0.9422562542333928, 0.9982481580493464, 0.24323992639915437, 0.22529496883657707, 0.9419502563485044, 0.9015126279854138, 0.7665064601855333, 0.9376303311008463, 0.21181627851856688, 0.5030789531605916]}, "task_prompt": ""}
{"id": "e7ed7f0b-9b65-4445-b11c-ca8929482590", "fitness": 0.5410059365435969, "name": "AdaptivePSO", "description": "No description provided.", "code": "import numpy as np\n\nclass AdaptivePSO:\n    def __init__(self, budget=10000, dim=10, pop_size=20, inertia=0.7, cognitive_coeff=1.5, social_coeff=1.5, neighborhood_size=5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.inertia = inertia\n        self.cognitive_coeff = cognitive_coeff\n        self.social_coeff = social_coeff\n        self.neighborhood_size = neighborhood_size\n        self.lb = -5.0\n        self.ub = 5.0\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.particles = None\n        self.velocities = None\n        self.personal_best_positions = None\n        self.personal_best_fitness = None\n\n    def initialize_swarm(self, func):\n        self.particles = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, size=(self.pop_size, self.dim))  # Initialize velocities\n        self.personal_best_positions = np.copy(self.particles)\n        self.personal_best_fitness = np.array([func(x) for x in self.particles])\n        self.budget -= self.pop_size\n\n        best_index = np.argmin(self.personal_best_fitness)\n        self.f_opt = self.personal_best_fitness[best_index]\n        self.x_opt = self.personal_best_positions[best_index]\n\n    def update_particle(self, func, i):\n        # Find neighborhood best\n        neighborhood_indices = np.arange(max(0, i - self.neighborhood_size // 2), min(self.pop_size, i + self.neighborhood_size // 2 + 1))\n        best_neighbor_index = neighborhood_indices[np.argmin(self.personal_best_fitness[neighborhood_indices])]\n        \n        # Adaptive inertia weight\n        inertia_adaptive = self.inertia * (0.5 + 0.5 * np.exp(-10 * (self.personal_best_fitness[i] - self.f_opt) / (np.max(self.personal_best_fitness) - self.f_opt + 1e-8)))\n\n        # Update velocity\n        r1 = np.random.rand(self.dim)\n        r2 = np.random.rand(self.dim)\n        self.velocities[i] = inertia_adaptive * self.velocities[i] + \\\n                            self.cognitive_coeff * r1 * (self.personal_best_positions[i] - self.particles[i]) + \\\n                            self.social_coeff * r2 * (self.personal_best_positions[best_neighbor_index] - self.particles[i])\n        \n        # Velocity clamping\n        v_max = (self.ub - self.lb) * 0.1  # Clamp to 10% of the search range\n        self.velocities[i] = np.clip(self.velocities[i], -v_max, v_max)\n\n        # Update position\n        self.particles[i] = self.particles[i] + self.velocities[i]\n        self.particles[i] = np.clip(self.particles[i], self.lb, self.ub)\n\n        # Evaluate new position\n        f_new = func(self.particles[i])\n        self.budget -= 1\n\n        # Update personal best\n        if f_new < self.personal_best_fitness[i]:\n            self.personal_best_fitness[i] = f_new\n            self.personal_best_positions[i] = np.copy(self.particles[i])\n\n            # Update global best\n            if f_new < self.f_opt:\n                self.f_opt = f_new\n                self.x_opt = np.copy(self.particles[i])\n\n    def __call__(self, func):\n        self.initialize_swarm(func)\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                self.update_particle(func, i)\n                if self.budget <= 0:\n                    break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptivePSO scored 0.541 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.16289044655232354, 0.19705826888605893, 0.904497268947559, 0.1986878214397736, 0.5264429777185129, 0.9167873187654764, 0.37031056302750465, 0.42223330159730554, 0.8775351380944129, 0.19796905960188182, 0.6485395216257214, 0.9986484387228058, 0.24731132369745046, 0.34960420921172275, 0.9483514369684691, 0.9378227908561985, 0.26753558742514083, 0.9433183099801157, 0.20594256161562297, 0.4986323861378792]}, "task_prompt": ""}
{"id": "e8627605-c9c1-4c61-a8fd-70f9533551e4", "fitness": 0.0, "name": "AdaptiveSimulatedAnnealing", "description": "No description provided.", "code": "import numpy as np\n\nclass AdaptiveSimulatedAnnealing:\n    def __init__(self, budget=10000, dim=10, initial_temp=100.0, cooling_rate=0.95, local_search_radius=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.initial_temp = initial_temp\n        self.cooling_rate = cooling_rate\n        self.local_search_radius = local_search_radius\n        self.lb = -5.0\n        self.ub = 5.0\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.temp = initial_temp\n\n    def __call__(self, func):\n        x = np.random.uniform(self.lb, self.ub, size=self.dim)\n        self.f_opt = func(x)\n        self.x_opt = x\n        self.eval_count += 1\n\n        while self.eval_count < self.budget:\n            x_new = x + np.random.uniform(-self.local_search_radius, self.local_search_radius, size=self.dim)\n            x_new = np.clip(x_new, self.lb, self.ub)\n            f_new = func(x_new)\n            self.eval_count += 1\n\n            delta_e = f_new - self.f_opt\n\n            if delta_e < 0:\n                x = x_new\n                self.f_opt = f_new\n                self.x_opt = x\n            else:\n                acceptance_prob = np.exp(-delta_e / self.temp)\n                if np.random.rand() < acceptance_prob:\n                    x = x_new\n\n            self.temp *= self.cooling_rate\n\n            # Adaptive temperature adjustment based on acceptance rate\n            if self.eval_count % 100 == 0:\n                # Estimate acceptance rate\n                num_accepted = 0\n                for _ in range(50):\n                    x_new = x + np.random.uniform(-self.local_search_radius, self.local_search_radius, size=self.dim)\n                    x_new = np.clip(x_new, self.lb, self.ub)\n                    f_new = func(x_new)\n\n                    delta_e = f_new - self.f_opt\n                    if delta_e < 0 or np.random.rand() < np.exp(-delta_e / self.temp):\n                        num_accepted += 1\n                acceptance_rate = num_accepted / 50.0\n                if acceptance_rate > 0.7:\n                    self.temp *= 1.1  # Increase temperature if too many accepted\n                elif acceptance_rate < 0.3:\n                    self.temp *= 0.9 # Decrease temperature if too few accepted\n                    \n                self.temp = np.clip(self.temp, 1e-6, self.initial_temp) # keep temp within bounds\n\n            if self.eval_count >= self.budget:\n                break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveSimulatedAnnealing scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0]}, "task_prompt": ""}
{"id": "bf923f37-e36c-4a80-a8a1-b81feec81ed2", "fitness": "-inf", "name": "GaussianProcessOptimization", "description": "No description provided.", "code": "import numpy as np\nfrom sklearn.gaussian_process import GaussianProcessRegressor\nfrom sklearn.gaussian_process.kernels import RBF, ConstantKernel as C\n\nclass GaussianProcessOptimization:\n    def __init__(self, budget=10000, dim=10, n_initial=10, kernel=None, lb=-5.0, ub=5.0):\n        self.budget = budget\n        self.dim = dim\n        self.n_initial = n_initial\n        self.lb = lb\n        self.ub = ub\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        if kernel is None:\n            self.kernel = C(1.0, (1e-3, 1e3)) * RBF(1.0, (1e-2, 1e2))\n        else:\n            self.kernel = kernel\n        self.gp = GaussianProcessRegressor(kernel=self.kernel, n_restarts_optimizer=10)\n\n        self.X = None\n        self.y = None\n\n    def initialize(self, func):\n        self.X = np.random.uniform(self.lb, self.ub, size=(self.n_initial, self.dim))\n        self.y = np.array([func(x) for x in self.X])\n        self.budget -= self.n_initial\n\n        best_index = np.argmin(self.y)\n        self.f_opt = self.y[best_index]\n        self.x_opt = self.X[best_index]\n    \n    def acquisition(self, x, gp):\n        mu, sigma = gp.predict(x.reshape(1, -1), return_std=True)\n        return mu - 2 * sigma\n    \n    def find_next_point(self, gp):\n        best_x = None\n        best_acq = np.inf\n        for i in range(1000):\n            x = np.random.uniform(self.lb, self.ub, size=self.dim)\n            acq = self.acquisition(x, gp)\n            if acq < best_acq:\n                best_acq = acq\n                best_x = x\n        return best_x\n\n    def __call__(self, func):\n        self.initialize(func)\n\n        while self.budget > 0:\n            self.gp.fit(self.X, self.y)\n            \n            x_next = self.find_next_point(self.gp)\n\n            f_next = func(x_next)\n            self.budget -= 1\n\n            self.X = np.vstack((self.X, x_next))\n            self.y = np.append(self.y, f_next)\n\n            if f_next < self.f_opt:\n                self.f_opt = f_next\n                self.x_opt = x_next.copy()\n        \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "In the code, line 16, in __init__, the following error occurred:\nNameError: name 'C' is not defined\nOn line: self.kernel = C(1.0, (1e-3, 1e3)) * RBF(1.0, (1e-2, 1e2))", "error": "In the code, line 16, in __init__, the following error occurred:\nNameError: name 'C' is not defined\nOn line: self.kernel = C(1.0, (1e-3, 1e3)) * RBF(1.0, (1e-2, 1e2))", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "dc75fc92-5676-4799-bcf2-da757b4efc73", "fitness": "-inf", "name": "GaussianProcessOptimization", "description": "No description provided.", "code": "import numpy as np\nfrom sklearn.gaussian_process import GaussianProcessRegressor\nfrom sklearn.gaussian_process.kernels import RBF, ConstantKernel as C\n\nclass GaussianProcessOptimization:\n    def __init__(self, budget=10000, dim=10, n_initial=10):\n        self.budget = budget\n        self.dim = dim\n        self.lb = -5.0\n        self.ub = 5.0\n        self.n_initial = n_initial\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.X = None\n        self.y = None\n        self.gpr = None\n\n    def initialize(self, func):\n        self.X = np.random.uniform(self.lb, self.ub, size=(self.n_initial, self.dim))\n        self.y = np.array([func(x) for x in self.X])\n        self.budget -= self.n_initial\n        best_index = np.argmin(self.y)\n        self.f_opt = self.y[best_index]\n        self.x_opt = self.X[best_index]\n\n    def acquisition(self, x):\n        mu, sigma = self.gpr.predict(x.reshape(1, -1), return_std=True)\n        return mu - 2 * sigma  # Lower Confidence Bound\n\n    def __call__(self, func):\n        self.initialize(func)\n        kernel = C(1.0, (1e-3, 1e3)) * RBF(1.0, (1e-3, 1e3))\n        self.gpr = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=9)\n\n        while self.budget > 0:\n            self.gpr.fit(self.X, self.y)\n            \n            # Find next point to evaluate using acquisition function\n            x_next = None\n            best_acq = np.inf\n            for _ in range(100):\n                x_candidate = np.random.uniform(self.lb, self.ub, size=self.dim)\n                acq = self.acquisition(x_candidate)\n                if acq < best_acq:\n                    best_acq = acq\n                    x_next = x_candidate\n            \n            f_next = func(x_next)\n            self.budget -= 1\n\n            self.X = np.vstack((self.X, x_next))\n            self.y = np.append(self.y, f_next)\n\n            if f_next < self.f_opt:\n                self.f_opt = f_next\n                self.x_opt = x_next\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "In the code, line 32, in __call__, the following error occurred:\nNameError: name 'C' is not defined\nOn line: kernel = C(1.0, (1e-3, 1e3)) * RBF(1.0, (1e-3, 1e3))", "error": "In the code, line 32, in __call__, the following error occurred:\nNameError: name 'C' is not defined\nOn line: kernel = C(1.0, (1e-3, 1e3)) * RBF(1.0, (1e-3, 1e3))", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "dca8cb4c-531c-41b4-83e9-6467d33be613", "fitness": 0.5366423847288834, "name": "ProbabilisticSwarm", "description": "No description provided.", "code": "import numpy as np\n\nclass ProbabilisticSwarm:\n    def __init__(self, budget=10000, dim=10, swarm_size=20, inertia=0.7, cognitive_coeff=1.4, social_coeff=1.4, velocity_clamp=0.5):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = swarm_size\n        self.inertia = inertia\n        self.cognitive_coeff = cognitive_coeff\n        self.social_coeff = social_coeff\n        self.velocity_clamp = velocity_clamp\n        self.lb = -5.0\n        self.ub = 5.0\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        # Initialize swarm\n        swarm = np.random.uniform(self.lb, self.ub, size=(self.swarm_size, self.dim))\n        velocities = np.random.uniform(-self.velocity_clamp, self.velocity_clamp, size=(self.swarm_size, self.dim))\n        personal_best_positions = swarm.copy()\n        personal_best_fitnesses = np.array([func(x) for x in swarm])\n        self.budget -= self.swarm_size\n\n        # Find initial global best\n        best_index = np.argmin(personal_best_fitnesses)\n        global_best_position = personal_best_positions[best_index].copy()\n        self.f_opt = personal_best_fitnesses[best_index]\n        self.x_opt = global_best_position.copy()\n\n        while self.budget > 0:\n            for i in range(self.swarm_size):\n                # Update velocity\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                cognitive_component = self.cognitive_coeff * r1 * (personal_best_positions[i] - swarm[i])\n                social_component = self.social_coeff * r2 * (global_best_position - swarm[i])\n                velocities[i] = self.inertia * velocities[i] + cognitive_component + social_component\n                velocities[i] = np.clip(velocities[i], -self.velocity_clamp * (self.ub - self.lb), self.velocity_clamp * (self.ub - self.lb))\n\n                # Update position\n                new_position = swarm[i] + velocities[i]\n                new_position = np.clip(new_position, self.lb, self.ub)\n\n                # Evaluation\n                f = func(new_position)\n                self.budget -= 1\n\n                # Update personal best\n                if f < personal_best_fitnesses[i]:\n                    personal_best_fitnesses[i] = f\n                    personal_best_positions[i] = new_position.copy()\n\n                    # Update global best\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = new_position.copy()\n                        global_best_position = new_position.copy()\n\n                swarm[i] = new_position\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm ProbabilisticSwarm scored 0.537 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.2256263818332267, 0.20836702650856187, 0.8687823592345677, 0.24883794113800461, 0.8927787256445019, 0.2848421854191593, 0.3079380123718848, 0.8450327241989775, 0.874313085578447, 0.23738376934669547, 0.923326606945015, 0.9999115042095256, 0.23530453570243315, 0.2914756495223957, 0.6493170187294176, 0.9041831217854162, 0.673010269399456, 0.38112380953734903, 0.2002083963203517, 0.481084571152281]}, "task_prompt": ""}
{"id": "6dfecf85-8ff8-497e-9d00-d4f1a61e2f14", "fitness": "-inf", "name": "GaussianProcessOptimization", "description": "No description provided.", "code": "import numpy as np\nfrom sklearn.gaussian_process import GaussianProcessRegressor\nfrom sklearn.gaussian_process.kernels import RBF, ConstantKernel as C\n\nclass GaussianProcessOptimization:\n    def __init__(self, budget=10000, dim=10, n_initial_samples=10):\n        self.budget = budget\n        self.dim = dim\n        self.lb = -5.0\n        self.ub = 5.0\n        self.n_initial_samples = n_initial_samples\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.X = None\n        self.y = None\n        self.kernel = C(1.0, (1e-3, 1e3)) * RBF(1.0, (1e-3, 1e3))\n        self.gp = GaussianProcessRegressor(kernel=self.kernel, n_restarts_optimizer=10)\n\n    def initialize(self, func):\n        self.X = np.random.uniform(self.lb, self.ub, size=(self.n_initial_samples, self.dim))\n        self.y = np.array([func(x) for x in self.X])\n        self.budget -= self.n_initial_samples\n\n        best_index = np.argmin(self.y)\n        self.f_opt = self.y[best_index]\n        self.x_opt = self.X[best_index]\n        self.gp.fit(self.X, self.y)\n\n    def acquisition(self, x):\n        mu, sigma = self.gp.predict(x.reshape(1, -1), return_std=True)\n        return mu - 2 * sigma  # Lower confidence bound\n\n    def __call__(self, func):\n        self.initialize(func)\n\n        while self.budget > 0:\n            # Find next point to evaluate using acquisition function\n            x_next = None\n            best_acq = np.inf\n            for _ in range(100): # Sample multiple candidate points\n                x_candidate = np.random.uniform(self.lb, self.ub, size=self.dim)\n                acq = self.acquisition(x_candidate)\n                if acq < best_acq:\n                    best_acq = acq\n                    x_next = x_candidate\n            \n            f_next = func(x_next)\n            self.budget -= 1\n\n            self.X = np.vstack((self.X, x_next))\n            self.y = np.append(self.y, f_next)\n            self.gp.fit(self.X, self.y)\n\n            if f_next < self.f_opt:\n                self.f_opt = f_next\n                self.x_opt = x_next.copy()\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "In the code, line 16, in __init__, the following error occurred:\nNameError: name 'C' is not defined\nOn line: self.kernel = C(1.0, (1e-3, 1e3)) * RBF(1.0, (1e-3, 1e3))", "error": "In the code, line 16, in __init__, the following error occurred:\nNameError: name 'C' is not defined\nOn line: self.kernel = C(1.0, (1e-3, 1e3)) * RBF(1.0, (1e-3, 1e3))", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "5655b66e-3736-4ef3-8cd5-a7018e871539", "fitness": 0.49898379200083776, "name": "SingleSolutionNeighborhoodSearch", "description": "No description provided.", "code": "import numpy as np\n\nclass SingleSolutionNeighborhoodSearch:\n    def __init__(self, budget=10000, dim=10, initial_step_size=1.0):\n        self.budget = budget\n        self.dim = dim\n        self.lb = -5.0\n        self.ub = 5.0\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.step_size = initial_step_size\n\n    def __call__(self, func):\n        # Initialize solution\n        x = np.random.uniform(self.lb, self.ub, size=self.dim)\n        f = func(x)\n        self.budget -= 1\n\n        self.f_opt = f\n        self.x_opt = x\n\n        while self.budget > 0:\n            # Generate a neighbor\n            neighbor = x + np.random.normal(0, self.step_size, size=self.dim)\n            neighbor = np.clip(neighbor, self.lb, self.ub)\n\n            # Evaluate the neighbor\n            f_neighbor = func(neighbor)\n            self.budget -= 1\n\n            # Accept if better\n            if f_neighbor < self.f_opt:\n                self.f_opt = f_neighbor\n                self.x_opt = neighbor\n                x = neighbor  # Move to the new solution\n\n            # Reduce step size (simulated annealing-like)\n            self.step_size *= 0.999\n\n            if self.step_size < 1e-6:\n                self.step_size = 1e-6\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm SingleSolutionNeighborhoodSearch scored 0.499 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.15129803488801985, 0.25575409385765624, 0.5540238405387945, 0.1664025883538598, 0.5690441194939282, 0.6256142274173508, 0.3016868010313185, 0.4952710357228757, 0.5732123252568566, 0.20011946003552072, 0.8596037000971446, 0.9989859151936055, 0.2616823130669551, 0.3294750029527592, 0.9154605907490447, 0.6222922027033034, 0.5009607365590767, 0.7502793014959219, 0.3534630925557868, 0.4950464580469769]}, "task_prompt": ""}
{"id": "65739592-782d-4ef6-b1f1-89044b93c272", "fitness": 0.6818857521239462, "name": "HybridDE_PSO", "description": "No description provided.", "code": "import numpy as np\n\nclass HybridDE_PSO:\n    def __init__(self, budget=10000, dim=10, pop_size=20, F=0.5, CR=0.9, w=0.7, c1=1.5, c2=1.5, de_ratio=0.5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F  # Mutation factor for DE\n        self.CR = CR  # Crossover rate for DE\n        self.w = w  # Inertia weight for PSO\n        self.c1 = c1  # Cognitive coefficient for PSO\n        self.c2 = c2  # Social coefficient for PSO\n        self.de_ratio = de_ratio  # Ratio of DE updates vs. PSO updates\n        self.lb = -5.0\n        self.ub = 5.0\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.population = None\n        self.fitness = None\n        self.velocities = None\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_index]\n        self.x_opt = self.population[best_index]\n        self.velocities = np.zeros((self.pop_size, self.dim))\n\n\n    def differential_evolution(self, i, func):\n        idxs = [idx for idx in range(self.pop_size) if idx != i]\n        a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n        mutant = a + self.F * (b - c)\n        mutant = np.clip(mutant, self.lb, self.ub)\n\n        trial = np.copy(self.population[i])\n        j_rand = np.random.randint(self.dim)\n        for j in range(self.dim):\n            if np.random.rand() < self.CR or j == j_rand:\n                trial[j] = mutant[j]\n\n        f_trial = func(trial)\n        self.budget -= 1\n\n        if f_trial < self.fitness[i]:\n            self.population[i] = trial\n            self.fitness[i] = f_trial\n            if f_trial < self.f_opt:\n                self.f_opt = f_trial\n                self.x_opt = trial\n\n    def particle_swarm_optimization(self, i, func):\n        personal_best = self.population[i]\n        personal_best_fitness = self.fitness[i]\n\n        r1 = np.random.rand(self.dim)\n        r2 = np.random.rand(self.dim)\n        \n        self.velocities[i] = self.w * self.velocities[i] + \\\n                             self.c1 * r1 * (personal_best - self.population[i]) + \\\n                             self.c2 * r2 * (self.x_opt - self.population[i])\n        \n        new_position = self.population[i] + self.velocities[i]\n        new_position = np.clip(new_position, self.lb, self.ub)\n        \n        f_new = func(new_position)\n        self.budget -= 1\n        \n        if f_new < self.fitness[i]:\n            self.population[i] = new_position\n            self.fitness[i] = f_new\n\n            if f_new < self.f_opt:\n                self.f_opt = f_new\n                self.x_opt = new_position\n\n\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                if np.random.rand() < self.de_ratio:\n                    self.differential_evolution(i, func)\n                else:\n                    self.particle_swarm_optimization(i, func)\n                    \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDE_PSO scored 0.682 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.44149000007083594, 0.5340055555495611, 0.8236952373690651, 0.936823569302459, 0.8571756267174668, 0.8539693600720006, 0.761825660774382, 0.8102028462359534, 0.8343948004312803, 0.3892068950639225, 0.9274640410975958, 0.9981283529758362, 0.30089007782175625, 0.8163471829574923, 0.6472496363659389, 0.8669156050480591, 0.737298906015421, 0.37921056290770194, 0.21330724086918318, 0.5081138848330136]}, "task_prompt": ""}
{"id": "5e3b962d-5033-4895-a626-6c5ccc4e054d", "fitness": 0.6960285451059247, "name": "SelfAdaptiveDifferentialEvolution", "description": "No description provided.", "code": "import numpy as np\n\nclass SelfAdaptiveDifferentialEvolution:\n    def __init__(self, budget=10000, dim=10, pop_size=20, F=0.5, CR=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.lb = -5.0\n        self.ub = 5.0\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        # Initialize population\n        population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n        \n        # Find initial best\n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index].copy()\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = np.random.choice(idxs, 3, replace=False)\n                mutant = population[a] + self.F * (population[b] - population[c])\n                \n                # Repair\n                mutant = np.clip(mutant, self.lb, self.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                f = func(trial)\n                self.budget -= 1\n                \n                if f < fitness[i]:\n                    fitness[i] = f\n                    population[i] = trial\n                    \n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = trial.copy()\n                        \n                        \n\n            # Self-adaptive F and CR (optional)\n            self.F = 0.5 #np.random.normal(0.5, 0.1)\n            self.CR = 0.9 #np.random.normal(0.9, 0.1)\n            self.F = np.clip(self.F, 0.1, 0.9)\n            self.CR = np.clip(self.CR, 0.1, 1.0)\n                        \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm SelfAdaptiveDifferentialEvolution scored 0.696 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.4090807500960386, 0.4096147403052731, 0.45227741864336646, 0.9308489474430968, 0.8998191174419, 0.9279485890810846, 0.8767508765457858, 0.8581400980968625, 0.9031767427201447, 0.8361591806160584, 0.3758778373620928, 0.9964376875253818, 0.4808739855953492, 0.8793909929428178, 0.5883593712976487, 0.8869633738846918, 0.5510189613578174, 0.938007405721627, 0.22887458610367684, 0.4909502393377795]}, "task_prompt": ""}
{"id": "33fa0f35-a3a4-4776-a703-f0becf1f06bb", "fitness": 0.0, "name": "HybridDE", "description": "No description provided.", "code": "import numpy as np\n\nclass HybridDE:\n    def __init__(self, budget=10000, dim=10, pop_size=20, F=0.5, CR=0.9, local_search_prob=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.lb = -5.0\n        self.ub = 5.0\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.local_search_prob = local_search_prob\n\n    def local_search(self, func, x, step_size=0.1):\n        \"\"\"Performs a simple gradient-based local search.\"\"\"\n        x_new = np.copy(x)\n        for i in range(self.dim):\n            # Estimate gradient\n            x_plus = np.copy(x)\n            x_minus = np.copy(x)\n            x_plus[i] += step_size\n            x_minus[i] -= step_size\n            x_plus[i] = np.clip(x_plus[i], self.lb, self.ub)\n            x_minus[i] = np.clip(x_minus[i], self.lb, self.ub)\n            \n            if self.budget >=2:\n              f_plus = func(x_plus)\n              f_minus = func(x_minus)\n              self.budget -= 2\n\n\n              # Update x if improvement\n              if f_plus < func(x_new):\n                  x_new = x_plus\n              elif f_minus < func(x_new):\n                  x_new = x_minus\n            else:\n              break\n        return x_new\n\n    def __call__(self, func):\n        # Initialize population\n        population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        # Find initial best\n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index]\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = a + self.F * (b - c)\n                mutant = np.clip(mutant, self.lb, self.ub)\n\n                # Crossover\n                trial = np.copy(population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR or j == j_rand:\n                        trial[j] = mutant[j]\n\n                # Evaluation\n                f_trial = func(trial)\n                self.budget -= 1\n\n                # Selection\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n\n                    # Update best\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n            # Local Search\n            for i in range(self.pop_size):\n                if np.random.rand() < self.local_search_prob and self.budget > 0:\n                    population[i] = self.local_search(func, population[i])\n                    fitness[i] = func(population[i])\n                    self.budget -= 1\n\n                    if fitness[i] < self.f_opt:\n                        self.f_opt = fitness[i]\n                        self.x_opt = population[i]\n\n            best_index = np.argmin(fitness)\n            if fitness[best_index] < self.f_opt:\n                self.f_opt = fitness[best_index]\n                self.x_opt = population[best_index]\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0]}, "task_prompt": ""}
{"id": "350e4cd3-60ab-4e27-9efb-bc2077548ed9", "fitness": "-inf", "name": "HybridDifferentialEvolution", "description": "No description provided.", "code": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridDifferentialEvolution:\n    def __init__(self, budget=10000, dim=10, pop_size=20, F=0.5, Cr=0.9, local_search_freq=100):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.Cr = Cr\n        self.lb = -5.0\n        self.ub = 5.0\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.local_search_freq = local_search_freq\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n\n        best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_index]\n        self.x_opt = self.population[best_index]\n\n    def evolve(self, func):\n        for i in range(self.pop_size):\n            # Mutation\n            idxs = [idx for idx in range(self.pop_size) if idx != i]\n            a, b, c = np.random.choice(idxs, 3, replace=False)\n            mutant = self.population[a] + self.F * (self.population[b] - self.population[c])\n            mutant = np.clip(mutant, self.lb, self.ub)\n\n            # Crossover\n            trial = self.population[i].copy()\n            j_rand = np.random.randint(self.dim)\n            for j in range(self.dim):\n                if np.random.rand() < self.Cr or j == j_rand:\n                    trial[j] = mutant[j]\n\n            # Selection\n            f = func(trial)\n            self.budget -= 1\n            if f < self.fitness[i]:\n                self.fitness[i] = f\n                self.population[i] = trial\n\n                if f < self.f_opt:\n                    self.f_opt = f\n                    self.x_opt = trial.copy()\n\n            # Local Search (Nelder-Mead) on the best solution\n            if self.budget > 0 and np.random.rand() < 1.0 / self.local_search_freq:\n                \n                res = minimize(func, self.x_opt, method='Nelder-Mead',\n                               bounds=np.array([(self.lb, self.ub)] * self.dim),\n                               options={'maxfev': min(self.budget, self.local_search_freq)})  # Limit FE\n                \n                if res.fun < self.f_opt:\n                  \n                  self.f_opt = res.fun\n                  self.x_opt = res.x.copy()\n                  \n                self.budget -= res.nfev # account function evaluations\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        while self.budget > 0:\n            self.evolve(func)\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "In the code, line 55, in evolve, the following error occurred:\nNameError: name 'minimize' is not defined\nOn line: res = minimize(func, self.x_opt, method='Nelder-Mead',", "error": "In the code, line 55, in evolve, the following error occurred:\nNameError: name 'minimize' is not defined\nOn line: res = minimize(func, self.x_opt, method='Nelder-Mead',", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "da0ce253-08bd-4c34-8d49-bc9864e7889b", "fitness": "-inf", "name": "SelfAdaptiveDE", "description": "No description provided.", "code": "import numpy as np\n\nclass SelfAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, initial_pop_size=50, F=0.5, CR=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = initial_pop_size\n        self.F = F\n        self.CR = CR\n        self.lb = -5.0\n        self.ub = 5.0\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.population = None\n        self.fitness = None\n\n    def initialize_population(self):\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n\n        best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_index]\n        self.x_opt = self.population[best_index]\n\n    def adjust_population_size(self):\n        if self.budget > 0:\n            reduction_factor = 0.1\n            new_pop_size = max(10, int(self.pop_size * (1 - reduction_factor)))\n            \n            if new_pop_size < self.pop_size:\n                self.pop_size = new_pop_size\n                sorted_indices = np.argsort(self.fitness)\n                self.population = self.population[sorted_indices[:self.pop_size]]\n                self.fitness = self.fitness[sorted_indices[:self.pop_size]]\n                \n\n    def __call__(self, func):\n        self.initialize_population()\n\n        while self.budget > 0:\n            \n            self.adjust_population_size()\n            \n            for i in range(self.pop_size):\n                # Mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n                mutant = a + self.F * (b - c)\n                mutant = np.clip(mutant, self.lb, self.ub)\n\n                # Crossover\n                trial = np.copy(self.population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR or j == j_rand:\n                        trial[j] = mutant[j]\n\n                # Evaluation\n                f_trial = func(trial)\n                self.budget -= 1\n\n                # Selection\n                if f_trial < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = f_trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "In the code, line 19, in initialize_population, the following error occurred:\nNameError: name 'func' is not defined\nOn line: self.fitness = np.array([func(x) for x in self.population])", "error": "In the code, line 19, in initialize_population, the following error occurred:\nNameError: name 'func' is not defined\nOn line: self.fitness = np.array([func(x) for x in self.population])", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "ae651b5f-3104-4a0e-ab26-310400275c8c", "fitness": 0.4419052481161058, "name": "ParticleSwarmOptimizer", "description": "No description provided.", "code": "import numpy as np\n\nclass ParticleSwarmOptimizer:\n    def __init__(self, budget=10000, dim=10, pop_size=20, inertia=0.7, c1=1.4, c2=1.4, v_max_ratio=0.2):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.inertia = inertia\n        self.c1 = c1\n        self.c2 = c2\n        self.lb = -5.0\n        self.ub = 5.0\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.v_max = v_max_ratio * (self.ub - self.lb)\n        self.constriction_factor = 0.729  # Standard value for constriction factor\n\n    def __call__(self, func):\n        # Initialize population and velocities\n        population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        velocities = np.random.uniform(-self.v_max, self.v_max, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n        \n        # Initialize personal best positions and fitnesses\n        personal_best_positions = population.copy()\n        personal_best_fitnesses = fitness.copy()\n\n        # Find initial global best\n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index].copy()\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Update velocities\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                \n                cognitive_component = self.c1 * r1 * (personal_best_positions[i] - population[i])\n                social_component = self.c2 * r2 * (self.x_opt - population[i])\n                \n                velocities[i] = self.constriction_factor * (self.inertia * velocities[i] + cognitive_component + social_component)\n\n                # Velocity clamping\n                velocities[i] = np.clip(velocities[i], -self.v_max, self.v_max)\n\n                # Update positions\n                population[i] = population[i] + velocities[i]\n\n                # Boundary handling (clip)\n                population[i] = np.clip(population[i], self.lb, self.ub)\n\n                # Evaluate fitness\n                f = func(population[i])\n                self.budget -= 1\n\n                # Update personal best\n                if f < personal_best_fitnesses[i]:\n                    personal_best_fitnesses[i] = f\n                    personal_best_positions[i] = population[i].copy()\n\n                # Update global best\n                if f < self.f_opt:\n                    self.f_opt = f\n                    self.x_opt = population[i].copy()\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm ParticleSwarmOptimizer scored 0.442 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.1254968378611211, 0.24144561793775543, 0.6138213342995837, 0.32645469397709415, 0.31226832740814026, 0.3093194774661231, 0.3487416324561007, 0.4920609281077596, 0.2803286427276296, 0.18062049834550908, 0.20278642815518266, 0.9989665385617503, 0.22555599155239459, 0.2794471560520706, 0.9725048768645779, 0.9240039238577565, 0.4035348787161408, 0.9667120496178837, 0.18960344509996419, 0.4444316832575791]}, "task_prompt": ""}
{"id": "a9f3fd70-d128-4b24-9948-e235dcf3b9a3", "fitness": 0.7669611198627566, "name": "SelfAdaptiveDE", "description": "No description provided.", "code": "import numpy as np\n\nclass SelfAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lb = -5.0\n        self.ub = 5.0\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.num_strategies = 4\n        self.probabilities = np.ones(self.num_strategies) / self.num_strategies\n        self.memory_size = 10\n        self.success_memory = [[] for _ in range(self.num_strategies)]\n\n    def __call__(self, func):\n        population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index]\n\n        while self.budget > 0:\n            new_population = np.copy(population)\n            new_fitness = np.copy(fitness)\n\n            for i in range(self.pop_size):\n                strategy_index = np.random.choice(self.num_strategies, p=self.probabilities)\n\n                if strategy_index == 0: # DE/rand/1\n                    idxs = [idx for idx in range(self.pop_size) if idx != i]\n                    a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                    mutant = population[np.random.choice(idxs)] + 0.5 * (b - c)\n                elif strategy_index == 1: # DE/best/1\n                    best = population[np.argmin(fitness)]\n                    idxs = [idx for idx in range(self.pop_size) if idx != i]\n                    b, c = population[np.random.choice(idxs, 2, replace=False)]\n                    mutant = best + 0.5 * (b - c)\n                elif strategy_index == 2: # DE/rand/2\n                    idxs = [idx for idx in range(self.pop_size) if idx != i]\n                    a, b, c, d, e = population[np.random.choice(idxs, 5, replace=False)]\n                    mutant = population[np.random.choice(idxs)] + 0.5 * (b - c) + 0.5 * (d - e)\n                else: # DE/current-to-best/1\n                    best = population[np.argmin(fitness)]\n                    idxs = [idx for idx in range(self.pop_size) if idx != i]\n                    b, c = population[np.random.choice(idxs, 2, replace=False)]\n                    mutant = population[i] + 0.5 * (best - population[i]) + 0.5 * (b - c)\n\n                mutant = np.clip(mutant, self.lb, self.ub)\n                \n                trial = np.copy(population[i])\n                j_rand = np.random.randint(self.dim)\n                CR = 0.9\n                for j in range(self.dim):\n                    if np.random.rand() < CR or j == j_rand:\n                        trial[j] = mutant[j]\n\n                f_trial = func(trial)\n                self.budget -= 1\n\n                if f_trial < fitness[i]:\n                    new_population[i] = trial\n                    new_fitness[i] = f_trial\n                    self.success_memory[strategy_index].append(1)\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                else:\n                    self.success_memory[strategy_index].append(0)\n                    new_population[i] = population[i]\n                    new_fitness[i] = fitness[i]\n\n                if len(self.success_memory[strategy_index]) > self.memory_size:\n                    self.success_memory[strategy_index].pop(0)\n            \n            population = new_population\n            fitness = new_fitness\n\n            # Update probabilities\n            success_rates = [np.mean(s) if s else 0 for s in self.success_memory]\n            sum_success = np.sum(success_rates)\n            if sum_success > 0:\n                self.probabilities = success_rates / sum_success\n            else:\n                self.probabilities = np.ones(self.num_strategies) / self.num_strategies\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm SelfAdaptiveDE scored 0.767 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.531644209696251, 0.8712179797935956, 0.8383874047726996, 0.9336765311582474, 0.8533671586119765, 0.892057190196945, 0.8071459241761929, 0.8542477051369419, 0.8609626066273393, 0.8069941503799023, 0.9241510442949445, 0.9988064262696634, 0.35912625941380794, 0.8606174239419804, 0.9506615223434227, 0.8854835021120717, 0.44876723556559517, 0.9144682193720731, 0.24078301167575966, 0.5066568917157217]}, "task_prompt": ""}
{"id": "54cae8ab-169f-4aa1-86e4-f5bec54df377", "fitness": 0.4197947147718886, "name": "SelfAdaptiveDE", "description": "No description provided.", "code": "import numpy as np\n\nclass SelfAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=20, F=0.5, CR=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.lb = -5.0\n        self.ub = 5.0\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.population = None\n        self.fitness = None\n        self.mutation_strategies = [self.mutation_1, self.mutation_2]\n        self.crossover_rates = [0.5, 0.9]\n        self.success_counts = np.zeros(len(self.mutation_strategies))\n        self.strategy_probs = np.ones(len(self.mutation_strategies)) / len(self.mutation_strategies)\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_index]\n        self.x_opt = self.population[best_index]\n\n    def mutation_1(self, i):\n        idxs = [idx for idx in range(self.pop_size) if idx != i]\n        a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n        mutant = a + self.F * (b - c)\n        mutant = np.clip(mutant, self.lb, self.ub)\n        return mutant\n\n    def mutation_2(self, i):\n        idxs = [idx for idx in range(self.pop_size) if idx != i]\n        a, b = self.population[np.random.choice(idxs, 2, replace=False)]\n        mutant = self.x_opt + self.F * (a - b)\n        mutant = np.clip(mutant, self.lb, self.ub)\n        return mutant\n\n    def crossover(self, individual, mutant, cr):\n        trial = np.copy(individual)\n        j_rand = np.random.randint(self.dim)\n        for j in range(self.dim):\n            if np.random.rand() < cr or j == j_rand:\n                trial[j] = mutant[j]\n        return trial\n\n    def update_strategy_probs(self):\n        total_success = np.sum(self.success_counts)\n        if total_success > 0:\n            self.strategy_probs = self.success_counts / total_success\n        else:\n            self.strategy_probs = np.ones(len(self.mutation_strategies)) / len(self.mutation_strategies)\n        self.strategy_probs = 0.9 * self.strategy_probs + 0.1 / len(self.mutation_strategies)\n        self.strategy_probs /= np.sum(self.strategy_probs)\n        self.success_counts[:] = 0  # Reset success counts\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        generation = 0\n        while self.budget > 0:\n            generation += 1\n            for i in range(self.pop_size):\n                strategy_index = np.random.choice(len(self.mutation_strategies), p=self.strategy_probs)\n                mutation_strategy = self.mutation_strategies[strategy_index]\n                mutant = mutation_strategy(i)\n                trial = self.crossover(self.population[i], mutant, self.CR)\n\n                f_trial = func(trial)\n                self.budget -= 1\n\n                if f_trial < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = f_trial\n                    self.success_counts[strategy_index] += 1\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n            if generation % 10 == 0:\n                self.update_strategy_probs()\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm SelfAdaptiveDE scored 0.420 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.19023261163601468, 0.23569188170789523, 0.2571568698234924, 0.23389370461720005, 0.26732020264114686, 0.4458958112936864, 0.3500188615242871, 0.29869292639864375, 0.3486273810872814, 0.23394412900914763, 0.33372136135382147, 0.9972104061030537, 0.395386574653547, 0.3059749528005148, 0.5851787842778501, 0.9378666025694656, 0.28719354071776615, 0.9667658441257759, 0.213386823212387, 0.5117350258847957]}, "task_prompt": ""}
{"id": "46fd2ed2-5c9b-49cc-9c1d-2054d56ca84f", "fitness": "-inf", "name": "GaussianProcessUCB", "description": "No description provided.", "code": "import numpy as np\nfrom sklearn.gaussian_process import GaussianProcessRegressor\nfrom sklearn.gaussian_process.kernels import RBF, ConstantKernel as C\n\nclass GaussianProcessUCB:\n    def __init__(self, budget=10000, dim=10, n_initial_samples=10):\n        self.budget = budget\n        self.dim = dim\n        self.lb = -5.0\n        self.ub = 5.0\n        self.n_initial_samples = n_initial_samples\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.X = None\n        self.Y = None\n        self.gp = None\n\n    def _acquisition(self, x, kappa=2.576):\n        \"\"\"Upper Confidence Bound acquisition function.\"\"\"\n        mu, sigma = self.gp.predict(x.reshape(1, -1), return_std=True)\n        return mu + kappa * sigma\n\n    def __call__(self, func):\n        # Initial sampling\n        X_init = np.random.uniform(self.lb, self.ub, size=(self.n_initial_samples, self.dim))\n        Y_init = np.array([func(x) for x in X_init])\n        self.budget -= self.n_initial_samples\n\n        best_index = np.argmin(Y_init)\n        self.f_opt = Y_init[best_index]\n        self.x_opt = X_init[best_index]\n        self.X = X_init\n        self.Y = Y_init\n\n        # Gaussian process model\n        kernel = C(1.0, (1e-3, 1e3)) * RBF(1.0, (1e-2, 1e2))\n        self.gp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=10, alpha=1e-5)\n\n        while self.budget > 0:\n            self.gp.fit(self.X, self.Y)\n\n            # Select next point to evaluate\n            x_next = None\n            best_acq = -np.inf\n            n_candidates = min(1000, self.budget)  # Reduce candidate size when budget is low\n            X_candidates = np.random.uniform(self.lb, self.ub, size=(n_candidates, self.dim))\n            for x in X_candidates:\n                acq = self._acquisition(x)\n                if acq > best_acq:\n                    best_acq = acq\n                    x_next = x\n\n            f_next = func(x_next)\n            self.budget -= 1\n\n            self.X = np.vstack((self.X, x_next))\n            self.Y = np.append(self.Y, f_next)\n\n            if f_next < self.f_opt:\n                self.f_opt = f_next\n                self.x_opt = x_next\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "In the code, line 36, in __call__, the following error occurred:\nNameError: name 'C' is not defined\nOn line: kernel = C(1.0, (1e-3, 1e3)) * RBF(1.0, (1e-2, 1e2))", "error": "In the code, line 36, in __call__, the following error occurred:\nNameError: name 'C' is not defined\nOn line: kernel = C(1.0, (1e-3, 1e3)) * RBF(1.0, (1e-2, 1e2))", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "4a1bad4c-6683-4497-8e20-efd9944290f9", "fitness": "-inf", "name": "GaussianProcessOptimization", "description": "No description provided.", "code": "import numpy as np\nfrom sklearn.gaussian_process import GaussianProcessRegressor\nfrom sklearn.gaussian_process.kernels import RBF, ConstantKernel\n\nclass GaussianProcessOptimization:\n    def __init__(self, budget=10000, dim=10, n_initial=10, kernel=None):\n        self.budget = budget\n        self.dim = dim\n        self.n_initial = n_initial\n        self.lb = -5.0\n        self.ub = 5.0\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        if kernel is None:\n            self.kernel = ConstantKernel(1.0, constant_value_bounds=\"fixed\") * RBF(length_scale=1.0, length_scale_bounds=\"fixed\")\n        else:\n            self.kernel = kernel\n        self.gp = GaussianProcessRegressor(kernel=self.kernel, n_restarts_optimizer=10)\n        self.X = None\n        self.y = None\n\n    def acquisition_function(self, x, gp):\n        mu, sigma = gp.predict(x.reshape(1, -1), return_std=True)\n        return mu - 2*sigma  # Lower Confidence Bound\n\n    def __call__(self, func):\n        # Initial random sampling\n        self.X = np.random.uniform(self.lb, self.ub, size=(self.n_initial, self.dim))\n        self.y = np.array([func(x) for x in self.X])\n        self.budget -= self.n_initial\n\n        best_index = np.argmin(self.y)\n        self.f_opt = self.y[best_index]\n        self.x_opt = self.X[best_index]\n        \n        # Optimization loop\n        while self.budget > 0:\n            self.gp.fit(self.X, self.y)\n            \n            # Find next point to evaluate\n            x_next = None\n            best_acq = np.inf\n            n_candidates = 100\n            candidates = np.random.uniform(self.lb, self.ub, size=(n_candidates, self.dim))\n\n            for x in candidates:\n                acq = self.acquisition_function(x, self.gp)\n                if acq < best_acq:\n                    best_acq = acq\n                    x_next = x\n\n            # Evaluate the function\n            f_next = func(x_next)\n            self.budget -= 1\n\n            # Update the data\n            self.X = np.vstack((self.X, x_next))\n            self.y = np.append(self.y, f_next)\n\n            # Update the best solution\n            if f_next < self.f_opt:\n                self.f_opt = f_next\n                self.x_opt = x_next\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "In the code, line 16, in __init__, the following error occurred:\nNameError: name 'ConstantKernel' is not defined\nOn line: self.kernel = ConstantKernel(1.0, constant_value_bounds=\"fixed\") * RBF(length_scale=1.0, length_scale_bounds=\"fixed\")", "error": "In the code, line 16, in __init__, the following error occurred:\nNameError: name 'ConstantKernel' is not defined\nOn line: self.kernel = ConstantKernel(1.0, constant_value_bounds=\"fixed\") * RBF(length_scale=1.0, length_scale_bounds=\"fixed\")", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "3af87624-5ea2-4dcc-9a08-f6b60193e9a5", "fitness": "-inf", "name": "GaussianProcessOptimization", "description": "No description provided.", "code": "import numpy as np\nfrom sklearn.gaussian_process import GaussianProcessRegressor\nfrom sklearn.gaussian_process.kernels import RBF, ConstantKernel as C\n\nclass GaussianProcessOptimization:\n    def __init__(self, budget=10000, dim=10, n_initial_samples=10):\n        self.budget = budget\n        self.dim = dim\n        self.lb = -5.0\n        self.ub = 5.0\n        self.n_initial_samples = n_initial_samples\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.X = None\n        self.y = None\n        self.kernel = C(1.0, (1e-3, 1e3)) * RBF(1.0, (1e-2, 1e2))\n        self.gp = GaussianProcessRegressor(kernel=self.kernel, n_restarts_optimizer=10)\n        self.evaluated = 0\n\n    def acquisition_function(self, x, gp):\n        mu, sigma = gp.predict(x.reshape(1, -1), return_std=True)\n        return mu - 2*sigma  # Optimizing for minimum value\n\n    def __call__(self, func):\n        # Initial sampling\n        initial_X = np.random.uniform(self.lb, self.ub, size=(self.n_initial_samples, self.dim))\n        initial_y = np.array([func(x) for x in initial_X])\n        self.budget -= self.n_initial_samples\n        self.evaluated += self.n_initial_samples\n\n        self.X = initial_X\n        self.y = initial_y\n\n        best_index = np.argmin(initial_y)\n        self.f_opt = initial_y[best_index]\n        self.x_opt = initial_X[best_index]\n\n        while self.budget > 0:\n            # Fit GP model\n            self.gp.fit(self.X, self.y)\n\n            # Find next point to evaluate using acquisition function\n            best_x = None\n            best_acq = np.inf\n            for _ in range(100):  # Sample candidate points\n                x_candidate = np.random.uniform(self.lb, self.ub, size=self.dim)\n                acq_value = self.acquisition_function(x_candidate, self.gp)\n                if acq_value < best_acq:\n                    best_acq = acq_value\n                    best_x = x_candidate\n            \n            # Evaluate function at new point\n            f_new = func(best_x)\n            self.budget -= 1\n            self.evaluated += 1\n\n            # Update data\n            self.X = np.vstack((self.X, best_x))\n            self.y = np.append(self.y, f_new)\n\n            # Update best\n            if f_new < self.f_opt:\n                self.f_opt = f_new\n                self.x_opt = best_x\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "In the code, line 16, in __init__, the following error occurred:\nNameError: name 'C' is not defined\nOn line: self.kernel = C(1.0, (1e-3, 1e3)) * RBF(1.0, (1e-2, 1e2))", "error": "In the code, line 16, in __init__, the following error occurred:\nNameError: name 'C' is not defined\nOn line: self.kernel = C(1.0, (1e-3, 1e3)) * RBF(1.0, (1e-2, 1e2))", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "08a3e7af-4574-4b7c-91c0-6ee94843665a", "fitness": 0.1732718979896618, "name": "KrillHerd", "description": "No description provided.", "code": "import numpy as np\n\nclass KrillHerd:\n    def __init__(self, budget=10000, dim=10, pop_size=50, Nmax=0.01, Vf=0.02, Dmax=0.005):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lb = -5.0\n        self.ub = 5.0\n        self.Nmax = Nmax  # Maximum induced speed\n        self.Vf = Vf  # Foraging speed\n        self.Dmax = Dmax  # Maximum diffusion speed\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.krill = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.zeros(self.pop_size)\n        self.delta_t = 1\n\n    def calculate_fitness(self, func):\n        for i in range(self.pop_size):\n            if self.budget > 0:\n                self.fitness[i] = func(self.krill[i])\n                self.budget -= 1\n            else:\n                self.fitness[i] = np.inf\n        return self.fitness\n    \n    def calculate_induced_motion(self, krill_index):\n        Si = np.zeros(self.dim)\n        for j in range(self.pop_size):\n            if j != krill_index:\n                dist = np.linalg.norm(self.krill[krill_index] - self.krill[j])\n                alpha = np.exp(-dist / (2 * self.sigma))\n                Si += (self.fitness[j] - self.fitness[krill_index]) * alpha * (self.krill[j] - self.krill[krill_index])\n        return self.Nmax * Si\n\n    def calculate_foraging_motion(self, krill_index):\n        Fi = np.zeros(self.dim)\n        best_index = np.argmin(self.fitness)\n        dist = np.linalg.norm(self.krill[krill_index] - self.krill[best_index])\n        beta = np.exp(-dist / (2 * self.sigma))\n        Fi = self.Vf * beta * (self.krill[best_index] - self.krill[krill_index])\n        return Fi\n    \n    def calculate_diffusion_motion(self):\n        Di = np.random.uniform(-1, 1, size=(self.pop_size, self.dim)) * self.Dmax\n        return Di\n\n    def __call__(self, func):\n        self.sigma = (self.ub - self.lb) / 2 \n        self.fitness = self.calculate_fitness(func)\n        best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_index]\n        self.x_opt = self.krill[best_index]\n\n        while self.budget > 0:\n            new_krill = np.copy(self.krill)\n            for i in range(self.pop_size):\n                induced_motion = self.calculate_induced_motion(i)\n                foraging_motion = self.calculate_foraging_motion(i)\n                diffusion_motion = self.calculate_diffusion_motion()[i]\n\n                new_krill[i] = self.krill[i] + self.delta_t * (induced_motion + foraging_motion + diffusion_motion)\n                new_krill[i] = np.clip(new_krill[i], self.lb, self.ub)\n\n            self.krill = new_krill\n            self.fitness = self.calculate_fitness(func)\n            \n            best_index = np.argmin(self.fitness)\n            if self.fitness[best_index] < self.f_opt:\n                self.f_opt = self.fitness[best_index]\n                self.x_opt = self.krill[best_index]\n            \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm KrillHerd scored 0.173 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.05247644377357774, 0.12114225955565994, 0.2725838269107329, 0.1508981919519139, 0.13704038597902268, 0.16473695080095563, 0.20285929888067034, 0.16852780869385775, 0.14676594284872224, 0.13229979234367273, 0.1432584094717455, 0.2713943285699769, 0.09071063526776035, 0.17146187511287103, 0.13615615382258872, 0.2294712873970034, 0.16152398449306515, 0.1631307622447834, 0.13008598662395054, 0.41891363505070467]}, "task_prompt": ""}
{"id": "452ceed9-5ff5-4615-bfaa-95a339ff0b61", "fitness": 0.2777075442020756, "name": "AdaptiveSimulatedAnnealing", "description": "No description provided.", "code": "import numpy as np\n\nclass AdaptiveSimulatedAnnealing:\n    def __init__(self, budget=10000, dim=10, temp_init=1.0, alpha=0.99, step_size=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.temp = temp_init\n        self.alpha = alpha\n        self.step_size = step_size\n        self.lb = -5.0\n        self.ub = 5.0\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.acceptance_rate = 0.0\n        self.acceptance_window = 100\n        self.accepted_moves = 0\n\n    def __call__(self, func):\n        x = np.random.uniform(self.lb, self.ub, size=self.dim)\n        f = func(x)\n        self.budget -= 1\n        self.f_opt = f\n        self.x_opt = x\n\n        while self.budget > 0:\n            # Generate neighbor\n            x_new = x + np.random.uniform(-self.step_size, self.step_size, size=self.dim)\n            x_new = np.clip(x_new, self.lb, self.ub)\n            \n            f_new = func(x_new)\n            self.budget -= 1\n            \n            # Acceptance criterion\n            delta_f = f_new - f\n            if delta_f < 0 or np.random.rand() < np.exp(-delta_f / self.temp):\n                x = x_new\n                f = f_new\n                self.accepted_moves += 1\n\n                if f < self.f_opt:\n                    self.f_opt = f\n                    self.x_opt = x\n            \n            # Temperature update\n            if self.budget % self.acceptance_window == 0:\n                self.acceptance_rate = self.accepted_moves / self.acceptance_window\n                self.accepted_moves = 0\n                \n                if self.acceptance_rate > 0.6:\n                    self.step_size *= 1.1 #Explore wider\n                elif self.acceptance_rate < 0.4:\n                    self.step_size *= 0.9 #Exploit narrower\n                \n                self.temp *= self.alpha #Cooling schedule\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveSimulatedAnnealing scored 0.278 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.04553727325199286, 0.13424811912866952, 0.26598016496479204, 0.31833536486751524, 0.23669219205996517, 0.2625242172979122, 0.24035979618526293, 0.24922488475185178, 0.23265374982250764, 0.17467612182180037, 0.27485442100969104, 0.6623232723601364, 0.3020134487812862, 0.25120815502842153, 0.5801281045857827, 0.2655734520885288, 0.24481793705626131, 0.3080419123116074, 0.08590425272887858, 0.4190540439386471]}, "task_prompt": ""}
{"id": "be24b7bb-144b-423a-88bc-021c33ae71e4", "fitness": 0.45806823483322495, "name": "PSO_SA", "description": "No description provided.", "code": "import numpy as np\n\nclass PSO_SA:\n    def __init__(self, budget=10000, dim=10, pop_size=20, w=0.7, c1=1.5, c2=1.5, temp_init=1.0, temp_decay=0.99):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.w = w  # Inertia weight\n        self.c1 = c1  # Cognitive coefficient\n        self.c2 = c2  # Social coefficient\n        self.lb = -5.0\n        self.ub = 5.0\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.temp = temp_init  # Initial temperature for SA\n        self.temp_decay = temp_decay # Temperature decay rate\n\n    def __call__(self, func):\n        # Initialize population and velocities\n        population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        velocities = np.random.uniform(-1, 1, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        # Initialize personal best positions and fitness\n        personal_best_positions = population.copy()\n        personal_best_fitness = fitness.copy()\n\n        # Find initial global best\n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index].copy()\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Update velocities\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                velocities[i] = (self.w * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - population[i]) +\n                                 self.c2 * r2 * (self.x_opt - population[i]))\n\n                # Update positions\n                population[i] = population[i] + velocities[i]\n                population[i] = np.clip(population[i], self.lb, self.ub)\n\n                # Evaluate fitness\n                f = func(population[i])\n                self.budget -= 1\n\n                # Update personal best\n                if f < personal_best_fitness[i]:\n                    personal_best_fitness[i] = f\n                    personal_best_positions[i] = population[i].copy()\n\n                    # Update global best\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = population[i].copy()\n                else:\n                    # Simulated Annealing\n                    delta_e = f - personal_best_fitness[i]\n                    if delta_e > 0:\n                        p = np.exp(-delta_e / self.temp)\n                        if np.random.rand() < p:\n                            personal_best_fitness[i] = f\n                            personal_best_positions[i] = population[i].copy()\n\n                            # Update global best\n                            if f < self.f_opt:\n                                self.f_opt = f\n                                self.x_opt = population[i].copy()\n\n            # Decay temperature\n            self.temp *= self.temp_decay\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm PSO_SA scored 0.458 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.2564620352090077, 0.19091423120066964, 0.8584672337055017, 0.2272843762954918, 0.2653782537753345, 0.284343001721443, 0.32959019583863625, 0.8118292213455284, 0.21217997134462008, 0.22845815831022942, 0.94879882509376, 0.9995765470273891, 0.3202743946531914, 0.2571525661284242, 0.6514686924586566, 0.9086537001012441, 0.3304596245468472, 0.290699639465122, 0.2948543654620397, 0.49451966298136163]}, "task_prompt": ""}
{"id": "e89c02f9-9f16-4781-ad84-d4ae1b302211", "fitness": 0.2567558040152146, "name": "AdaptivePopulationSearch", "description": "No description provided.", "code": "import numpy as np\n\nclass AdaptivePopulationSearch:\n    def __init__(self, budget=10000, dim=10, pop_size=50, learning_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lb = -5.0\n        self.ub = 5.0\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.learning_rate = learning_rate\n\n    def __call__(self, func):\n        population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index]\n\n        while self.budget > 0:\n            mean_position = np.mean(population, axis=0)\n\n            for i in range(self.pop_size):\n                # Local gradient estimation (simplified)\n                perturbation = np.random.normal(0, 0.1, size=self.dim)\n                x_perturbed = np.clip(population[i] + perturbation, self.lb, self.ub)\n                f_perturbed = func(x_perturbed)\n                self.budget -= 1\n\n                if f_perturbed < fitness[i]:\n                  gradient_estimate = x_perturbed - population[i]\n                  step_size = self.learning_rate\n                else:\n                  gradient_estimate = population[i] - x_perturbed\n                  step_size = -self.learning_rate\n                \n                # Move towards population mean with some randomness\n                new_position = population[i] + step_size * gradient_estimate + 0.1 * np.random.normal(0, 1, self.dim) + 0.01*(mean_position - population[i])\n                new_position = np.clip(new_position, self.lb, self.ub)\n\n                f_new = func(new_position)\n                self.budget -= 1\n\n                if f_new < fitness[i]:\n                    population[i] = new_position\n                    fitness[i] = f_new\n\n                    if f_new < self.f_opt:\n                        self.f_opt = f_new\n                        self.x_opt = new_position\n\n                if self.budget <= 0:\n                  break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptivePopulationSearch scored 0.257 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.0962887863486831, 0.17826283076517901, 0.3033310853152966, 0.15935472834799935, 0.14446450019891033, 0.16582149886462916, 0.2085546169198701, 0.1929554591037771, 0.19613854791668395, 0.15164460951067427, 0.1993882648870291, 0.7712141595103996, 0.2977834474675347, 0.14702614453583052, 0.4875108451521546, 0.2967590637937886, 0.2446489670491243, 0.2642292569886481, 0.16221499369098824, 0.4675242739370913]}, "task_prompt": ""}
{"id": "d146518f-41f9-4662-bbd1-42810fd04fa1", "fitness": 0.21152304437935482, "name": "AdaptiveStepSize", "description": "No description provided.", "code": "import numpy as np\n\nclass AdaptiveStepSize:\n    def __init__(self, budget=10000, dim=10, initial_step_size=1.0):\n        self.budget = budget\n        self.dim = dim\n        self.lb = -5.0\n        self.ub = 5.0\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.step_size = np.full(dim, initial_step_size)\n        self.success_rate = np.zeros(dim)\n        self.memory_size = 10\n        self.success_history = [[] for _ in range(dim)]\n        self.learning_rate = 0.1\n\n    def __call__(self, func):\n        x = np.random.uniform(self.lb, self.ub, size=self.dim)\n        self.f_opt = func(x)\n        self.x_opt = x\n        self.budget -= 1\n\n        while self.budget > 0:\n            dim_index = np.random.randint(self.dim)\n            original_x = np.copy(x)\n            step = self.step_size[dim_index] * np.random.normal()\n            x[dim_index] += step\n\n            x = np.clip(x, self.lb, self.ub)\n            f = func(x)\n            self.budget -= 1\n\n            if f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = x\n                self.success_history[dim_index].append(1)\n                self.step_size[dim_index] *= (1 + self.learning_rate)\n            else:\n                x = original_x\n                f = self.f_opt # keep the best\n                self.success_history[dim_index].append(0)\n                self.step_size[dim_index] *= (1 - self.learning_rate)\n\n            if len(self.success_history[dim_index]) > self.memory_size:\n                self.success_history[dim_index].pop(0)\n            \n            self.success_rate[dim_index] = np.mean(self.success_history[dim_index]) if self.success_history[dim_index] else 0\n\n            self.step_size = np.clip(self.step_size, 1e-6, self.ub - self.lb)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveStepSize scored 0.212 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.047512474644862746, 0.07101587748028015, 0.2513594186514636, 0.19838225064425785, 0.304243106032028, 0.17569068885960082, 0.26059081190277433, 0.18091472850074986, 0.15769310783037904, 0.09867675018196531, 0.18988750099082863, 0.149673543398135, 0.21974280641409483, 0.16048952984877884, 0.5858529691539133, 0.3059364333775727, 0.13247709007946162, 0.15419429526583495, 0.08656883849754671, 0.4995586658325689]}, "task_prompt": ""}
{"id": "f32c40f1-94a1-4151-88f5-65193e8db7b9", "fitness": "-inf", "name": "ShrinkingDifferentialEvolution", "description": "No description provided.", "code": "import numpy as np\n\nclass ShrinkingDifferentialEvolution:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, Cr=0.9, shrink_factor=0.95):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F  # Mutation factor\n        self.Cr = Cr  # Crossover rate\n        self.lb = -5.0\n        self.ub = 5.0\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.shrink_factor = shrink_factor\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n\n        best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_index]\n        self.x_opt = self.population[best_index]\n\n    def evolve(self, func):\n        new_population = []\n        new_fitness = []\n        \n        for i in range(self.pop_size):\n            # Mutation\n            idxs = [idx for idx in range(self.pop_size) if idx != i]\n            a, b, c = np.random.choice(idxs, 3, replace=False)\n            mutant = self.population[a] + self.F * (self.population[b] - self.population[c])\n            mutant = np.clip(mutant, self.lb, self.ub)\n\n            # Crossover\n            trial = self.population[i].copy()\n            j_rand = np.random.randint(self.dim)\n            for j in range(self.dim):\n                if np.random.rand() < self.Cr or j == j_rand:\n                    trial[j] = mutant[j]\n\n            # Selection\n            f = func(trial)\n            self.budget -= 1\n            if f < self.fitness[i]:\n                new_fitness.append(f)\n                new_population.append(trial)\n                \n                if f < self.f_opt:\n                    self.f_opt = f\n                    self.x_opt = trial.copy()\n            else:\n                new_fitness.append(self.fitness[i])\n                new_population.append(self.population[i])\n\n            if self.budget <= 0:\n                break\n                \n        self.population = np.array(new_population)\n        self.fitness = np.array(new_fitness)\n        \n        # Shrink population and intensify local search\n        if len(self.population) > 1 and np.random.rand() < 0.2:\n            num_to_keep = int(self.pop_size * self.shrink_factor)\n            indices = np.argsort(self.fitness)[:num_to_keep]\n            self.population = self.population[indices]\n            self.fitness = self.fitness[indices]\n            self.pop_size = len(self.population)\n            \n            # Local search around best solution\n            for i in range(min(5, self.budget)):\n                x = self.x_opt + np.random.normal(0, 0.1, size=self.dim)\n                x = np.clip(x, self.lb, self.ub)\n                f = func(x)\n                self.budget -= 1\n                if f < self.f_opt:\n                    self.f_opt = f\n                    self.x_opt = x.copy()\n                if self.budget <= 0:\n                  break\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        while self.budget > 0:\n            self.evolve(func)\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "In the code, line 1025, in numpy.random.mtrand.RandomState.choice, the following error occurred:\nValueError: Cannot take a larger sample than population when 'replace=False'", "error": "In the code, line 1025, in numpy.random.mtrand.RandomState.choice, the following error occurred:\nValueError: Cannot take a larger sample than population when 'replace=False'", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "faa6449a-0ce8-43e4-a7f8-36662fd941ff", "fitness": 0.3886492758781682, "name": "HybridPSO", "description": "No description provided.", "code": "import numpy as np\n\nclass HybridPSO:\n    def __init__(self, budget=10000, dim=10, pop_size=50, inertia=0.7, cognitive_coeff=1.4, social_coeff=1.4, mutation_rate=0.05):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lb = -5.0\n        self.ub = 5.0\n        self.inertia = inertia\n        self.cognitive_coeff = cognitive_coeff\n        self.social_coeff = social_coeff\n        self.mutation_rate = mutation_rate\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        velocities = np.random.uniform(-1, 1, size=(self.pop_size, self.dim))  # Initialize velocities\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        personal_best_positions = np.copy(population)\n        personal_best_fitness = np.copy(fitness)\n        global_best_index = np.argmin(fitness)\n        global_best_position = population[global_best_index]\n        self.f_opt = fitness[global_best_index]\n        self.x_opt = global_best_position\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Update velocity with PSO components\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                velocities[i] = (self.inertia * velocities[i] +\n                                 self.cognitive_coeff * r1 * (personal_best_positions[i] - population[i]) +\n                                 self.social_coeff * r2 * (global_best_position - population[i]))\n                \n                # Apply mutation using DE inspired strategy\n                if np.random.rand() < self.mutation_rate:\n                    idxs = [idx for idx in range(self.pop_size) if idx != i]\n                    a, b = population[np.random.choice(idxs, 2, replace=False)]\n                    mutation_vector = a - b\n                    velocities[i] += 0.5 * mutation_vector  # Incorporate mutation into velocity\n\n                # Update position\n                new_position = population[i] + velocities[i]\n                new_position = np.clip(new_position, self.lb, self.ub) # Clip to bounds\n\n\n                f_trial = func(new_position)\n                self.budget -= 1\n                \n                if f_trial < fitness[i]:\n                    population[i] = new_position\n                    fitness[i] = f_trial\n                    \n                    if f_trial < personal_best_fitness[i]:\n                        personal_best_fitness[i] = f_trial\n                        personal_best_positions[i] = new_position\n                        \n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = new_position\n                        global_best_position = new_position\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm HybridPSO scored 0.389 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.1486846525855644, 0.2841239760248806, 0.47286015594556463, 0.22628816735718082, 0.24355964723258317, 0.6238450194707039, 0.3137986501876334, 0.441700269324968, 0.5056210668493204, 0.22439599242382713, 0.3139866646005326, 0.9969167502261266, 0.2804340842004718, 0.25028932330253717, 0.6780384025029418, 0.5806323087922133, 0.3304031884382218, 0.21446438253147937, 0.15843859720909936, 0.4845042183575141]}, "task_prompt": ""}
{"id": "8d33d1e8-9113-4dcd-ae9a-4390581f8964", "fitness": 0.5241558558658901, "name": "AdaptivePSO", "description": "No description provided.", "code": "import numpy as np\n\nclass AdaptivePSO:\n    def __init__(self, budget=10000, dim=10, pop_size=50):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lb = -5.0\n        self.ub = 5.0\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.particles = None\n        self.velocities = None\n        self.personal_best_positions = None\n        self.personal_best_fitness = None\n        self.global_best_position = None\n        self.global_best_fitness = np.inf\n        self.learning_rate_personal = 0.7\n        self.learning_rate_global = 0.7\n\n    def initialize_population(self, func):\n        self.particles = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, size=(self.pop_size, self.dim))\n        self.personal_best_positions = np.copy(self.particles)\n        self.personal_best_fitness = np.array([func(x) for x in self.particles])\n        self.budget -= self.pop_size\n        \n        best_index = np.argmin(self.personal_best_fitness)\n        self.global_best_position = self.personal_best_positions[best_index]\n        self.global_best_fitness = self.personal_best_fitness[best_index]\n        self.f_opt = self.global_best_fitness\n        self.x_opt = self.global_best_position\n\n    def update_velocities(self):\n        inertia_weight = 0.5\n        cognitive_component = self.learning_rate_personal * np.random.rand(self.pop_size, self.dim) * (self.personal_best_positions - self.particles)\n        social_component = self.learning_rate_global * np.random.rand(self.pop_size, self.dim) * (self.global_best_position - self.particles)\n        self.velocities = inertia_weight * self.velocities + cognitive_component + social_component\n\n    def update_positions(self):\n        self.particles = self.particles + self.velocities\n        self.particles = np.clip(self.particles, self.lb, self.ub)\n\n    def update_personal_and_global_best(self, func):\n        fitness = np.array([func(x) for x in self.particles])\n        self.budget -= self.pop_size\n\n        for i in range(self.pop_size):\n            if fitness[i] < self.personal_best_fitness[i]:\n                self.personal_best_fitness[i] = fitness[i]\n                self.personal_best_positions[i] = self.particles[i]\n\n        best_index = np.argmin(self.personal_best_fitness)\n        if self.personal_best_fitness[best_index] < self.global_best_fitness:\n            self.global_best_fitness = self.personal_best_fitness[best_index]\n            self.global_best_position = self.personal_best_positions[best_index]\n            self.f_opt = self.global_best_fitness\n            self.x_opt = self.global_best_position\n\n    def adjust_learning_rates(self):\n        if np.random.rand() < 0.1:\n            self.learning_rate_personal = np.random.uniform(0.1, 0.9)\n            self.learning_rate_global = np.random.uniform(0.1, 0.9)\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.budget > self.pop_size:\n            self.update_velocities()\n            self.update_positions()\n            self.update_personal_and_global_best(func)\n            self.adjust_learning_rates()\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptivePSO scored 0.524 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.12301213011240908, 0.1712665601361516, 0.3617069897947057, 0.9545417188835869, 0.8907756230414512, 0.7311514151483663, 0.3506740836411606, 0.38601393618543567, 0.6244451426070374, 0.2372957057788755, 0.5454127399673705, 0.999824934627465, 0.2525296704470724, 0.6264066686395717, 0.8770501085624612, 0.3850101390097952, 0.3384519780219646, 0.9191758617542604, 0.20278423216001962, 0.5055874787986415]}, "task_prompt": ""}
{"id": "de6418ca-0050-41ba-b251-fea14ea4cf4a", "fitness": 0.36054087818213143, "name": "AdaptivePSO", "description": "No description provided.", "code": "import numpy as np\n\nclass AdaptivePSO:\n    def __init__(self, budget=10000, dim=10, pop_size=50):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lb = -5.0\n        self.ub = 5.0\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.inertia_weight = 0.7\n        self.c1 = 1.5\n        self.c2 = 1.5\n        self.velocity = None\n        self.personal_best_position = None\n        self.personal_best_fitness = None\n        self.global_best_position = None\n        self.global_best_fitness = np.inf\n        self.success_rate = 0.0\n        self.success_history = []\n        self.history_length = 10\n\n    def __call__(self, func):\n        population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.velocity = np.random.uniform(-1, 1, size=(self.pop_size, self.dim))\n        self.personal_best_position = np.copy(population)\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n        self.personal_best_fitness = np.copy(fitness)\n        best_index = np.argmin(fitness)\n        self.global_best_position = population[best_index]\n        self.global_best_fitness = fitness[best_index]\n        self.f_opt = self.global_best_fitness\n        self.x_opt = self.global_best_position\n\n        while self.budget > 0:\n            r1 = np.random.rand(self.pop_size, self.dim)\n            r2 = np.random.rand(self.pop_size, self.dim)\n\n            self.velocity = (self.inertia_weight * self.velocity +\n                             self.c1 * r1 * (self.personal_best_position - population) +\n                             self.c2 * r2 * (self.global_best_position - population))\n\n            population += self.velocity\n            population = np.clip(population, self.lb, self.ub)\n            new_fitness = np.array([func(x) for x in population])\n            self.budget -= self.pop_size\n\n            improved = new_fitness < self.personal_best_fitness\n            self.success_history.extend(improved)\n            self.personal_best_position[improved] = population[improved]\n            self.personal_best_fitness[improved] = new_fitness[improved]\n\n            best_index = np.argmin(self.personal_best_fitness)\n            if self.personal_best_fitness[best_index] < self.global_best_fitness:\n                self.global_best_position = self.personal_best_position[best_index]\n                self.global_best_fitness = self.personal_best_fitness[best_index]\n                self.f_opt = self.global_best_fitness\n                self.x_opt = self.global_best_position\n            \n            #Adaptive Inertia Weight and Acceleration Coefficients\n            if len(self.success_history) > self.history_length:\n                self.success_rate = np.mean(self.success_history[-self.history_length:])\n            else:\n                self.success_rate = np.mean(self.success_history) if self.success_history else 0\n\n            self.inertia_weight = 0.7 + 0.2 * (1-self.success_rate)  # Adjust inertia weight\n\n            self.c1 = 1.5 + 0.5 * self.success_rate       # Adjust c1\n            self.c2 = 1.5 + 0.5 * self.success_rate       # Adjust c2\n                \n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptivePSO scored 0.361 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.16454939056544937, 0.18892207317203258, 0.39017510476739925, 0.45939714026424583, 0.24840187602162522, 0.25157364744254396, 0.29283321130412865, 0.3152537459815268, 0.29884798160158665, 0.2685200567074998, 0.410654510339726, 0.996154505685713, 0.2603324893156932, 0.24821504224040447, 0.7035479499124402, 0.3484417738973614, 0.3027247555528474, 0.3720701459045085, 0.20136784407419261, 0.4888343188917039]}, "task_prompt": ""}
{"id": "da994b79-5b7c-4951-962e-9210d1f620c7", "fitness": 0.5381009539766229, "name": "SimplifiedPSO", "description": "No description provided.", "code": "import numpy as np\n\nclass SimplifiedPSO:\n    def __init__(self, budget=10000, dim=10, pop_size=40, inertia_max=0.9, inertia_min=0.4, cognitive_coeff=2.0, social_coeff=2.0, velocity_clamp=1.0):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lb = -5.0\n        self.ub = 5.0\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.inertia_max = inertia_max\n        self.inertia_min = inertia_min\n        self.cognitive_coeff = cognitive_coeff\n        self.social_coeff = social_coeff\n        self.velocity_clamp = velocity_clamp\n\n    def __call__(self, func):\n        population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        velocities = np.random.uniform(-self.velocity_clamp, self.velocity_clamp, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        personal_best_positions = np.copy(population)\n        personal_best_fitness = np.copy(fitness)\n\n        global_best_index = np.argmin(fitness)\n        self.f_opt = fitness[global_best_index]\n        self.x_opt = population[global_best_index]\n        global_best_position = np.copy(population[global_best_index])\n\n        iteration = 0\n        while self.budget > 0:\n            inertia = self.inertia_max - (self.inertia_max - self.inertia_min) * iteration / (self.budget // self.pop_size + iteration + 1)\n            \n            for i in range(self.pop_size):\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n\n                velocities[i] = (inertia * velocities[i]\n                                 + self.cognitive_coeff * r1 * (personal_best_positions[i] - population[i])\n                                 + self.social_coeff * r2 * (global_best_position - population[i]))\n                \n                velocities[i] = np.clip(velocities[i], -self.velocity_clamp, self.velocity_clamp)\n                \n                population[i] = population[i] + velocities[i]\n                population[i] = np.clip(population[i], self.lb, self.ub)\n\n                f_current = func(population[i])\n                self.budget -= 1\n\n                if f_current < personal_best_fitness[i]:\n                    personal_best_fitness[i] = f_current\n                    personal_best_positions[i] = np.copy(population[i])\n\n                    if f_current < self.f_opt:\n                        self.f_opt = f_current\n                        self.x_opt = np.copy(population[i])\n                        global_best_position = np.copy(population[i])\n\n            iteration += 1\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm SimplifiedPSO scored 0.538 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.29460129746752695, 0.4838156162473298, 0.5429558021509726, 0.7688393830377798, 0.5745979315547982, 0.6518606911339773, 0.334523176166859, 0.48673850683792774, 0.5564154751922673, 0.243155242520544, 0.773442302018222, 0.9965293746873144, 0.34035419274961454, 0.4932413238757697, 0.72528749823876, 0.6619322833350121, 0.40150347596677005, 0.719898016659319, 0.197538692497348, 0.5147887971943493]}, "task_prompt": ""}
{"id": "13d9a40e-b0f6-4849-bfcd-0de0ce1395ef", "fitness": 0.7693111948837684, "name": "SelfAdaptiveDE", "description": "No description provided.", "code": "import numpy as np\n\nclass SelfAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=20, lb=-5.0, ub=5.0,\n                 num_strategies=4):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lb = lb\n        self.ub = ub\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.num_strategies = num_strategies\n        self.mutation_factors = np.random.uniform(0.5, 1.0, size=num_strategies)\n        self.crossover_rates = np.random.uniform(0.5, 1.0, size=num_strategies)\n        self.success_rates_F = np.ones(num_strategies) / num_strategies\n        self.success_rates_CR = np.ones(num_strategies) / num_strategies\n        self.memory_F = np.zeros(num_strategies)\n        self.memory_CR = np.zeros(num_strategies)\n        self.memory_p = 0.1\n        self.archive_size = int(self.pop_size / 2)\n        self.archive = []\n    \n    def __call__(self, func):\n        # Initialize population\n        population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        # Find initial best\n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index]\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Strategy selection\n                strategy_index = np.random.choice(self.num_strategies, p=self.success_rates_F / np.sum(self.success_rates_F))\n\n                # Mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = a + self.mutation_factors[strategy_index] * (b - c)\n                mutant = np.clip(mutant, self.lb, self.ub)\n\n                # Crossover\n                trial = np.copy(population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < self.crossover_rates[strategy_index] or j == j_rand:\n                        trial[j] = mutant[j]\n\n                # Evaluation\n                f_trial = func(trial)\n                self.budget -= 1\n\n                # Selection\n                if f_trial < fitness[i]:\n                    # Update success rates\n                    self.success_rates_F[strategy_index] = (1 - self.memory_p) * self.success_rates_F[strategy_index] + self.memory_p\n                    self.success_rates_CR[strategy_index] = (1 - self.memory_p) * self.success_rates_CR[strategy_index] + self.memory_p\n\n                    population[i] = trial\n                    fitness[i] = f_trial\n\n                    # Update best\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                else:\n                     self.success_rates_F[strategy_index] = (1 - self.memory_p) * self.success_rates_F[strategy_index]\n\n            best_index = np.argmin(fitness)\n            if fitness[best_index] < self.f_opt:\n                self.f_opt = fitness[best_index]\n                self.x_opt = population[best_index]\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm SelfAdaptiveDE scored 0.769 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.530778376231021, 0.7917018694354774, 0.6960812910149533, 0.9094146072136416, 0.8241403150919828, 0.824323930667701, 0.657251258802455, 0.7569237274725836, 0.8071376346751096, 0.8164842712287459, 0.7808392960993833, 0.9909723062773867, 0.45417324592264674, 0.8274915996196541, 0.9535270848471233, 0.8216565419054922, 0.7195178038237333, 0.8909113927218995, 0.6163061828320113, 0.7165911617923675]}, "task_prompt": ""}
{"id": "09c5eb6e-01c2-4e9b-868a-5f08747f0e2d", "fitness": 0.35605341218655795, "name": "SelfAdaptiveDE", "description": "No description provided.", "code": "import numpy as np\n\nclass SelfAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, CR=0.9, shrink_factor=0.95):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F  # Mutation factor\n        self.CR = CR  # Crossover rate\n        self.lb = -5.0\n        self.ub = 5.0\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.shrink_factor = shrink_factor\n\n    def __call__(self, func):\n        # Initialize population\n        population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        # Find initial best\n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index]\n\n        while self.budget > 0 and self.pop_size > 3:\n            # Sort population by fitness\n            sorted_indices = np.argsort(fitness)\n            population = population[sorted_indices]\n            fitness = fitness[sorted_indices]\n\n            # Dynamic adjustment of F and CR\n            success_indices = fitness < np.mean(fitness)\n            if np.sum(success_indices) > 0:\n                self.F = np.mean(np.random.uniform(0.4, 0.9, size=np.sum(success_indices)))\n                self.CR = np.mean(np.random.uniform(0.7, 1.0, size=np.sum(success_indices)))\n            else:\n                self.F = 0.5  # Reset if no improvement\n                self.CR = 0.9\n\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = a + self.F * (b - c)\n                mutant = np.clip(mutant, self.lb, self.ub)\n\n                # Crossover\n                trial = np.copy(population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR or j == j_rand:\n                        trial[j] = mutant[j]\n\n                # Evaluation\n                f_trial = func(trial)\n                self.budget -= 1\n\n                # Selection\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n\n                    # Update best\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n            # Shrink the population\n            self.pop_size = int(self.pop_size * self.shrink_factor)\n            population = population[:self.pop_size]\n            fitness = fitness[:self.pop_size]\n\n            best_index = np.argmin(fitness)\n            if fitness[best_index] < self.f_opt:\n                self.f_opt = fitness[best_index]\n                self.x_opt = population[best_index]\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm SelfAdaptiveDE scored 0.356 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.14871042034176551, 0.21214207658142503, 0.3602058378903482, 0.41396195282557113, 0.2676145587598786, 0.32059029538167794, 0.27705906072387565, 0.3467269253001124, 0.267441700115883, 0.1803224291433857, 0.37112428222311844, 0.9991632045718417, 0.28212106907057843, 0.29741095927871153, 0.6634296440362976, 0.34878339539315983, 0.3012386678676251, 0.41023385354338815, 0.1770864178395143, 0.47570149284300156]}, "task_prompt": ""}
{"id": "80dfc276-5787-4872-ab2b-ab2eb6543514", "fitness": 0.5448705030187831, "name": "EnhancedSelfAdaptiveDE", "description": "No description provided.", "code": "import numpy as np\n\nclass EnhancedSelfAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=100):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lb = -5.0\n        self.ub = 5.0\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.num_strategies = 4\n        self.probabilities = np.ones(self.num_strategies) / self.num_strategies\n        self.memory_size = 10\n        self.success_memory = [[] for _ in range(self.num_strategies)]\n        self.mutation_factor = 0.7\n        self.crossover_rate = 0.7\n\n    def __call__(self, func):\n        population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index]\n\n        while self.budget > 0:\n            new_population = np.copy(population)\n            new_fitness = np.copy(fitness)\n\n            for i in range(self.pop_size):\n                strategy_index = np.random.choice(self.num_strategies, p=self.probabilities)\n\n                if strategy_index == 0: # DE/rand/1\n                    idxs = [idx for idx in range(self.pop_size) if idx != i]\n                    a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                    mutant = population[np.random.choice(idxs)] + self.mutation_factor * (b - c)\n                elif strategy_index == 1: # DE/best/1\n                    best = population[np.argmin(fitness)]\n                    idxs = [idx for idx in range(self.pop_size) if idx != i]\n                    b, c = population[np.random.choice(idxs, 2, replace=False)]\n                    mutant = best + self.mutation_factor * (b - c)\n                elif strategy_index == 2: # DE/rand/2\n                    idxs = [idx for idx in range(self.pop_size) if idx != i]\n                    a, b, c, d, e = population[np.random.choice(idxs, 5, replace=False)]\n                    mutant = population[np.random.choice(idxs)] + self.mutation_factor * (b - c) + self.mutation_factor * (d - e)\n                else: # DE/current-to-best/1\n                    best = population[np.argmin(fitness)]\n                    idxs = [idx for idx in range(self.pop_size) if idx != i]\n                    b, c = population[np.random.choice(idxs, 2, replace=False)]\n                    mutant = population[i] + self.mutation_factor * (best - population[i]) + self.mutation_factor * (b - c)\n\n                mutant = np.clip(mutant, self.lb, self.ub)\n                \n                trial = np.copy(population[i])\n                j_rand = np.random.randint(self.dim)\n                \n                for j in range(self.dim):\n                    if np.random.rand() < self.crossover_rate or j == j_rand:\n                        trial[j] = mutant[j]\n\n                f_trial = func(trial)\n                self.budget -= 1\n\n                if f_trial < fitness[i]:\n                    new_population[i] = trial\n                    new_fitness[i] = f_trial\n                    self.success_memory[strategy_index].append(1)\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                else:\n                    self.success_memory[strategy_index].append(0)\n                    new_population[i] = population[i]\n                    new_fitness[i] = fitness[i]\n\n                if len(self.success_memory[strategy_index]) > self.memory_size:\n                    self.success_memory[strategy_index].pop(0)\n            \n            population = new_population\n            fitness = new_fitness\n\n            # Update probabilities\n            success_rates = [np.mean(s) if s else 0 for s in self.success_memory]\n            sum_success = np.sum(success_rates)\n            if sum_success > 0:\n                self.probabilities = success_rates / sum_success\n            else:\n                self.probabilities = np.ones(self.num_strategies) / self.num_strategies\n            self.mutation_factor = np.clip(np.random.normal(0.7, 0.1), 0.1, 1.0)\n            self.crossover_rate = np.clip(np.random.normal(0.7, 0.1), 0.1, 1.0)\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm EnhancedSelfAdaptiveDE scored 0.545 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.19417932554029949, 0.3345688087443426, 0.5140100392797877, 0.7735964555280628, 0.5833012370722683, 0.6185972621016222, 0.4001960839059251, 0.47350850057161264, 0.5232569754109428, 0.5206257351232304, 0.7514610444565558, 0.9996570061902403, 0.35242763067588556, 0.5036215765533066, 0.8677548486736725, 0.631923485944911, 0.4054599882608786, 0.7324825020635963, 0.21321524450073004, 0.5035663097777905]}, "task_prompt": ""}
{"id": "4f5dcd62-f2d3-4ffa-a8a0-cc55e1df55f2", "fitness": "-inf", "name": "GaussianProcessOptimization", "description": "No description provided.", "code": "import numpy as np\nfrom sklearn.gaussian_process import GaussianProcessRegressor\nfrom sklearn.gaussian_process.kernels import RBF, ConstantKernel as C\n\nclass GaussianProcessOptimization:\n    def __init__(self, budget=10000, dim=10, n_initial_samples=10):\n        self.budget = budget\n        self.dim = dim\n        self.lb = -5.0\n        self.ub = 5.0\n        self.n_initial_samples = n_initial_samples\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.X = None\n        self.y = None\n        self.gp = None\n\n    def acquisition_function(self, x, gp):\n        mu, sigma = gp.predict(x.reshape(1, -1), return_std=True)\n        return mu - 2 * sigma  # Lower Confidence Bound\n\n    def __call__(self, func):\n        # Initial sampling\n        self.X = np.random.uniform(self.lb, self.ub, size=(self.n_initial_samples, self.dim))\n        self.y = np.array([func(x) for x in self.X])\n        self.budget -= self.n_initial_samples\n\n        best_index = np.argmin(self.y)\n        self.f_opt = self.y[best_index]\n        self.x_opt = self.X[best_index]\n\n        # Gaussian process regression\n        kernel = C(1.0, constant_value_bounds=\"fixed\") * RBF(1.0, length_scale_bounds=\"fixed\")\n        self.gp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=10)\n\n        while self.budget > 0:\n            self.gp.fit(self.X, self.y)\n\n            # Find next point to evaluate using acquisition function\n            x_next = None\n            best_acq = np.inf\n            for _ in range(100):  # Sample multiple points and choose the best one\n                x_candidate = np.random.uniform(self.lb, self.ub, size=self.dim)\n                acq = self.acquisition_function(x_candidate, self.gp)\n                if acq < best_acq:\n                    best_acq = acq\n                    x_next = x_candidate\n\n            f_next = func(x_next)\n            self.budget -= 1\n\n            self.X = np.vstack((self.X, x_next))\n            self.y = np.append(self.y, f_next)\n\n            if f_next < self.f_opt:\n                self.f_opt = f_next\n                self.x_opt = x_next\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "In the code, line 33, in __call__, the following error occurred:\nNameError: name 'C' is not defined\nOn line: kernel = C(1.0, constant_value_bounds=\"fixed\") * RBF(1.0, length_scale_bounds=\"fixed\")", "error": "In the code, line 33, in __call__, the following error occurred:\nNameError: name 'C' is not defined\nOn line: kernel = C(1.0, constant_value_bounds=\"fixed\") * RBF(1.0, length_scale_bounds=\"fixed\")", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "d088316d-f4bf-461c-930a-467c3dba74ad", "fitness": "-inf", "name": "GaussianProcessOptimization", "description": "No description provided.", "code": "import numpy as np\nfrom sklearn.gaussian_process import GaussianProcessRegressor\nfrom sklearn.gaussian_process.kernels import RBF, ConstantKernel as C\nfrom scipy.stats import norm\n\nclass GaussianProcessOptimization:\n    def __init__(self, budget=10000, dim=10, n_initial_samples=10):\n        self.budget = budget\n        self.dim = dim\n        self.lb = -5.0\n        self.ub = 5.0\n        self.n_initial_samples = n_initial_samples\n        self.X = None\n        self.y = None\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.kernel = C(1.0, (1e-3, 1e3)) * RBF(1.0, (1e-2, 1e2))\n        self.gp = GaussianProcessRegressor(kernel=self.kernel, n_restarts_optimizer=10, alpha=1e-10)\n\n    def acquisition_function(self, x, gp, xi=0.01):\n        mu, sigma = gp.predict(x.reshape(1, -1), return_std=True)\n        mu = mu[0]\n        sigma = sigma[0]\n\n        if sigma == 0:\n          return 0\n        \n        imp = self.f_opt - mu - xi\n        Z = imp / sigma\n        ei = imp * norm.cdf(Z) + sigma * norm.pdf(Z)\n        return ei\n\n    def __call__(self, func):\n        # Initial sampling\n        self.X = np.random.uniform(self.lb, self.ub, size=(self.n_initial_samples, self.dim))\n        self.y = np.array([func(x) for x in self.X])\n        self.budget -= self.n_initial_samples\n\n        best_index = np.argmin(self.y)\n        self.f_opt = self.y[best_index]\n        self.x_opt = self.X[best_index]\n\n        # Optimization loop\n        while self.budget > 0:\n            # Fit Gaussian process\n            self.gp.fit(self.X, self.y)\n\n            # Find next point to evaluate\n            best_x = None\n            best_acq = -np.inf\n            for _ in range(100): # Sample multiple times and pick the best\n              x_rand = np.random.uniform(self.lb, self.ub, size=self.dim)\n              acq_value = self.acquisition_function(x_rand, self.gp)\n              if acq_value > best_acq:\n                  best_acq = acq_value\n                  best_x = x_rand\n\n            \n            x_next = best_x\n            \n            # Evaluate the function\n            f_next = func(x_next)\n            self.budget -= 1\n            \n            # Update data\n            self.X = np.vstack((self.X, x_next))\n            self.y = np.append(self.y, f_next)\n            \n            # Update best\n            if f_next < self.f_opt:\n                self.f_opt = f_next\n                self.x_opt = x_next\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "In the code, line 17, in __init__, the following error occurred:\nNameError: name 'C' is not defined\nOn line: self.kernel = C(1.0, (1e-3, 1e3)) * RBF(1.0, (1e-2, 1e2))", "error": "In the code, line 17, in __init__, the following error occurred:\nNameError: name 'C' is not defined\nOn line: self.kernel = C(1.0, (1e-3, 1e3)) * RBF(1.0, (1e-2, 1e2))", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "17b1be07-c2ab-494f-96e3-131c5abf57ec", "fitness": "-inf", "name": "GaussianProcessOptimization", "description": "No description provided.", "code": "import numpy as np\nfrom sklearn.gaussian_process import GaussianProcessRegressor\nfrom sklearn.gaussian_process.kernels import RBF, ConstantKernel as C\n\nclass GaussianProcessOptimization:\n    def __init__(self, budget=10000, dim=10, n_initial_samples=10):\n        self.budget = budget\n        self.dim = dim\n        self.lb = -5.0\n        self.ub = 5.0\n        self.n_initial_samples = n_initial_samples\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.X = None\n        self.y = None\n        self.gpr = None\n\n    def initialize(self, func):\n        self.X = np.random.uniform(self.lb, self.ub, size=(self.n_initial_samples, self.dim))\n        self.y = np.array([func(x) for x in self.X])\n        self.budget -= self.n_initial_samples\n\n        best_index = np.argmin(self.y)\n        self.f_opt = self.y[best_index]\n        self.x_opt = self.X[best_index]\n        \n        kernel = C(1.0, constant_value_bounds=\"fixed\") * RBF(1.0, length_scale_bounds=\"fixed\")\n        self.gpr = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=0)\n        self.gpr.fit(self.X, self.y)\n\n    def acquisition_function(self, x, xi=0.01):\n        mu, sigma = self.gpr.predict(x.reshape(1, -1), return_std=True)\n        mu = mu[0]\n        sigma = sigma[0]\n        return (self.f_opt - mu - xi * sigma)\n\n    def optimize_acquisition(self):\n        best_x = None\n        best_acq = -np.inf\n        for _ in range(100):\n            x = np.random.uniform(self.lb, self.ub, size=self.dim)\n            acq = self.acquisition_function(x)\n            if acq > best_acq:\n                best_acq = acq\n                best_x = x\n        return best_x\n\n    def __call__(self, func):\n        self.initialize(func)\n        while self.budget > 0:\n            x_new = self.optimize_acquisition()\n            f_new = func(x_new)\n            self.budget -= 1\n\n            self.X = np.vstack((self.X, x_new))\n            self.y = np.append(self.y, f_new)\n            self.gpr.fit(self.X, self.y)\n\n            if f_new < self.f_opt:\n                self.f_opt = f_new\n                self.x_opt = x_new\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "In the code, line 27, in initialize, the following error occurred:\nNameError: name 'C' is not defined\nOn line: kernel = C(1.0, constant_value_bounds=\"fixed\") * RBF(1.0, length_scale_bounds=\"fixed\")", "error": "In the code, line 27, in initialize, the following error occurred:\nNameError: name 'C' is not defined\nOn line: kernel = C(1.0, constant_value_bounds=\"fixed\") * RBF(1.0, length_scale_bounds=\"fixed\")", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "35b0f93b-1898-484d-9e58-3fd7ad7f09b6", "fitness": "-inf", "name": "GaussianProcessOptimization", "description": "No description provided.", "code": "import numpy as np\nfrom sklearn.gaussian_process import GaussianProcessRegressor\nfrom sklearn.gaussian_process.kernels import RBF, ConstantKernel as C\n\nclass GaussianProcessOptimization:\n    def __init__(self, budget=10000, dim=10, n_initial_samples=10):\n        self.budget = budget\n        self.dim = dim\n        self.lb = -5.0\n        self.ub = 5.0\n        self.n_initial_samples = n_initial_samples\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.X = None\n        self.y = None\n        self.gpr = None\n\n    def initialize(self, func):\n        self.X = np.random.uniform(self.lb, self.ub, size=(self.n_initial_samples, self.dim))\n        self.y = np.array([func(x) for x in self.X])\n        self.budget -= self.n_initial_samples\n        \n        best_index = np.argmin(self.y)\n        self.f_opt = self.y[best_index]\n        self.x_opt = self.X[best_index]\n\n    def acquisition_function(self, x, gpr, xi=0.01):\n        mu, sigma = gpr.predict(x.reshape(1, -1), return_std=True)\n        return mu + xi * sigma\n\n    def find_next_point(self, gpr):\n        best_x = None\n        best_acq = -np.inf\n        for i in range(1000):  # Sample many points and pick the best\n            x = np.random.uniform(self.lb, self.ub, size=self.dim)\n            acq = self.acquisition_function(x, gpr)\n            if acq > best_acq:\n                best_acq = acq\n                best_x = x\n        return best_x\n\n    def __call__(self, func):\n        self.initialize(func)\n        \n        kernel = C(1.0, (1e-3, 1e3)) * RBF(10, (1e-2, 1e2))\n\n        while self.budget > 0:\n            self.gpr = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=10)\n            self.gpr.fit(self.X, self.y)\n            \n            x_next = self.find_next_point(self.gpr)\n            f_next = func(x_next)\n            self.budget -= 1\n\n            self.X = np.vstack((self.X, x_next))\n            self.y = np.append(self.y, f_next)\n\n            if f_next < self.f_opt:\n                self.f_opt = f_next\n                self.x_opt = x_next.copy()\n                \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "In the code, line 45, in __call__, the following error occurred:\nNameError: name 'C' is not defined\nOn line: kernel = C(1.0, (1e-3, 1e3)) * RBF(10, (1e-2, 1e2))", "error": "In the code, line 45, in __call__, the following error occurred:\nNameError: name 'C' is not defined\nOn line: kernel = C(1.0, (1e-3, 1e3)) * RBF(10, (1e-2, 1e2))", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "f3dd2ba1-a7e9-4dc1-a0e3-f2dfb1090cc5", "fitness": "-inf", "name": "KernelDensityEstimationOptimization", "description": "No description provided.", "code": "import numpy as np\nfrom scipy.stats import gaussian_kde\n\nclass KernelDensityEstimationOptimization:\n    def __init__(self, budget=10000, dim=10, pop_size=20, lb=-5.0, ub=5.0, top_fraction=0.25, bw_method=None):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lb = lb\n        self.ub = ub\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.top_fraction = top_fraction\n        self.bw_method = bw_method\n\n    def __call__(self, func):\n        # Initialize population\n        population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        # Find initial best\n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index]\n\n        while self.budget > 0:\n            # Select top individuals\n            num_top = int(self.pop_size * self.top_fraction)\n            top_indices = np.argsort(fitness)[:num_top]\n            top_individuals = population[top_indices]\n\n            # Kernel density estimation\n            try:\n                kde = gaussian_kde(top_individuals.T, bw_method=self.bw_method)\n            except np.linalg.LinAlgError:\n                # Fallback to a wider bandwidth if covariance is singular\n                kde = gaussian_kde(top_individuals.T, bw_method=0.5) # or some other value\n\n            # Generate new samples\n            new_samples = kde.resample(self.pop_size)\n            new_samples = np.clip(new_samples.T, self.lb, self.ub)\n\n            # Evaluate new samples\n            new_fitness = np.array([func(x) for x in new_samples])\n            self.budget -= self.pop_size\n            \n            # Update population\n            population = new_samples\n            fitness = new_fitness\n\n            # Update best\n            best_index = np.argmin(fitness)\n            if fitness[best_index] < self.f_opt:\n                self.f_opt = fitness[best_index]\n                self.x_opt = population[best_index]\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "In the code, line 35, in __call__, the following error occurred:\nNameError: name 'gaussian_kde' is not defined\nOn line: kde = gaussian_kde(top_individuals.T, bw_method=self.bw_method)", "error": "In the code, line 35, in __call__, the following error occurred:\nNameError: name 'gaussian_kde' is not defined\nOn line: kde = gaussian_kde(top_individuals.T, bw_method=self.bw_method)", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "9bd38c48-0074-45ca-a4d0-db5f0ce353ac", "fitness": "-inf", "name": "GMMOptimizer", "description": "No description provided.", "code": "import numpy as np\nfrom sklearn.mixture import GaussianMixture\n\nclass GMMOptimizer:\n    def __init__(self, budget=10000, dim=10, pop_size=20, lb=-5.0, ub=5.0, n_components=5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lb = lb\n        self.ub = ub\n        self.n_components = n_components\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.gmm = None\n\n    def __call__(self, func):\n        # Initialize population\n        population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        # Find initial best\n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index]\n\n        while self.budget > 0:\n            # Fit GMM to top individuals\n            top_indices = np.argsort(fitness)[:self.pop_size // 2]\n            top_population = population[top_indices]\n            \n            if len(top_population) > self.n_components:\n              self.gmm = GaussianMixture(n_components=self.n_components, random_state=0, covariance_type='full', max_iter=10) #full, tied, diag, spherical\n              self.gmm.fit(top_population)\n\n              # Sample new individuals from GMM\n              new_population, _ = self.gmm.sample(self.pop_size)\n              new_population = np.clip(new_population, self.lb, self.ub)\n            else:\n                new_population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n                \n            new_fitness = np.array([func(x) for x in new_population])\n            self.budget -= self.pop_size\n            \n            # Combine old and new populations\n            population = np.concatenate((population, new_population))\n            fitness = np.concatenate((fitness, new_fitness))\n\n            # Select best individuals for next iteration\n            indices = np.argsort(fitness)[:self.pop_size]\n            population = population[indices]\n            fitness = fitness[indices]\n\n            # Update best solution\n            best_index = np.argmin(fitness)\n            if fitness[best_index] < self.f_opt:\n                self.f_opt = fitness[best_index]\n                self.x_opt = population[best_index]\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "In the code, line 33, in __call__, the following error occurred:\nNameError: name 'GaussianMixture' is not defined\nOn line: self.gmm = GaussianMixture(n_components=self.n_components, random_state=0, covariance_type='full', max_iter=10) #full, tied, diag, spherical", "error": "In the code, line 33, in __call__, the following error occurred:\nNameError: name 'GaussianMixture' is not defined\nOn line: self.gmm = GaussianMixture(n_components=self.n_components, random_state=0, covariance_type='full', max_iter=10) #full, tied, diag, spherical", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "9255729d-0eed-471b-8d16-a29355ee03f9", "fitness": 0.21183692405721594, "name": "AdaptiveSimulatedAnnealing", "description": "No description provided.", "code": "import numpy as np\n\nclass AdaptiveSimulatedAnnealing:\n    def __init__(self, budget=10000, dim=10, temp_init=1.0, alpha=0.99, restart_prob=0.05):\n        self.budget = budget\n        self.dim = dim\n        self.temp = temp_init\n        self.alpha = alpha\n        self.lb = -5.0\n        self.ub = 5.0\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.restart_prob = restart_prob\n\n    def __call__(self, func):\n        x = np.random.uniform(self.lb, self.ub, size=self.dim)\n        f = func(x)\n        self.budget -= 1\n        self.f_opt = f\n        self.x_opt = x.copy()\n\n        while self.budget > 0:\n            x_new = x + np.random.normal(0, self.temp, size=self.dim)\n            x_new = np.clip(x_new, self.lb, self.ub)\n            f_new = func(x_new)\n            self.budget -= 1\n\n            delta_f = f_new - f\n\n            if delta_f < 0 or np.random.rand() < np.exp(-delta_f / self.temp):\n                x = x_new\n                f = f_new\n\n                if f < self.f_opt:\n                    self.f_opt = f\n                    self.x_opt = x.copy()\n\n            self.temp *= self.alpha  #Cooling\n\n            if np.random.rand() < self.restart_prob:\n                x = np.random.uniform(self.lb, self.ub, size=self.dim)\n                f = func(x)\n                self.budget -= 1\n                if f < self.f_opt:\n                  self.f_opt = f\n                  self.x_opt = x.copy()\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveSimulatedAnnealing scored 0.212 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.08748106701025604, 0.15333802468941937, 0.2699267724486627, 0.18112701679039744, 0.19896147613437498, 0.2055754776278098, 0.20744245728177213, 0.19926441283587537, 0.2058983037206683, 0.14615643338420126, 0.18652890925083276, 0.6914137524278736, 0.24309970834162353, 0.20134004891447155, 0]}, "task_prompt": ""}
{"id": "577278a6-823e-430f-8f72-8bfb9145d23e", "fitness": 0.4670519631181852, "name": "VelocityEnhancedLearning", "description": "No description provided.", "code": "import numpy as np\n\nclass VelocityEnhancedLearning:\n    def __init__(self, budget=10000, dim=10, pop_size=20, w=0.7, c1=1.5, c2=1.5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.w = w  # Inertia weight\n        self.c1 = c1 # Cognitive coefficient\n        self.c2 = c2 # Social coefficient\n        self.lb = -5.0\n        self.ub = 5.0\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.velocity = None\n\n    def __call__(self, func):\n        # Initialize population and velocity\n        population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.velocity = np.random.uniform(-1, 1, size=(self.pop_size, self.dim))\n\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        # Initialize personal best positions\n        personal_best_positions = population.copy()\n        personal_best_fitness = fitness.copy()\n\n        # Find initial global best\n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index]\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Update velocity\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n\n                # Learn from global best and a random individual\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                random_index = np.random.choice(idxs)\n\n                self.velocity[i] = (self.w * self.velocity[i] +\n                                self.c1 * r1 * (personal_best_positions[i] - population[i]) +\n                                self.c2 * r2 * (self.x_opt - population[i]))\n                \n                # Update position\n                new_position = population[i] + self.velocity[i]\n                new_position = np.clip(new_position, self.lb, self.ub)\n\n                # Evaluation\n                f_new = func(new_position)\n                self.budget -= 1\n\n                # Update personal best\n                if f_new < personal_best_fitness[i]:\n                    personal_best_fitness[i] = f_new\n                    personal_best_positions[i] = new_position\n\n                # Update global best\n                if f_new < self.f_opt:\n                    self.f_opt = f_new\n                    self.x_opt = new_position\n                \n                population[i] = new_position\n                fitness[i] = f_new\n                \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm VelocityEnhancedLearning scored 0.467 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.23793913228109687, 0.18578710600584902, 0.8665241998779232, 0.8758321358964076, 0.896833925033326, 0.2844668204002482, 0.34192574731872616, 0.5182479950781107, 0.24946005535956395, 0.21828419499202822, 0.23662644453255421, 0.999742587177531, 0.2674580268358523, 0.2602687994807156, 0.5878355859895885, 0.8913547269444735, 0.4085290990998216, 0.29804553031145264, 0.20702392810847936, 0.5088532216399568]}, "task_prompt": ""}
{"id": "5f860d16-237a-4100-bece-07011d438c9d", "fitness": 0.24917047554347413, "name": "CMAES_Restart", "description": "No description provided.", "code": "import numpy as np\n\nclass CMAES_Restart:\n    def __init__(self, budget=10000, dim=10, pop_size=None, initial_step_size=0.2):\n        self.budget = budget\n        self.dim = dim\n        self.lb = -5.0\n        self.ub = 5.0\n        if pop_size is None:\n            self.pop_size = 4 + int(3 * np.log(self.dim))\n        else:\n            self.pop_size = pop_size\n        self.mu = self.pop_size // 2\n        self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n        self.weights /= np.sum(self.weights)\n        self.m = np.random.uniform(self.lb, self.ub, size=self.dim)\n        self.sigma = initial_step_size\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.chiN = np.sqrt(self.dim) * (1 - 1/(4*self.dim) + 1/(21*self.dim**2))\n        self.c_sigma = (self.mu / (self.dim + self.mu))**0.5\n        self.d_sigma = 1 + 2*max(0, np.sqrt((self.mu-1)/(self.dim+1))-1) + self.c_sigma\n        self.c_c = (4 + self.mu/self.dim) / (self.dim + 4 + 2*self.mu/self.dim)\n        self.c_1 = 2 / ((self.dim + 1.3)**2 + self.mu)\n        self.c_mu = min(1 - self.c_1, 2 * (self.mu - 2 + 1/self.mu) / ((self.dim + 2)**2 + self.mu))\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.restart_trigger = 1000\n        self.eval_count = 0\n        self.restart_count = 0\n\n    def __call__(self, func):\n        while self.budget > 0:\n            z = np.random.normal(0, 1, size=(self.pop_size, self.dim))\n            C_sqrt = np.linalg.cholesky(self.C)\n            x = self.m + self.sigma * z @ C_sqrt.T\n            x = np.clip(x, self.lb, self.ub)\n            \n            fitness = np.array([func(xi) for xi in x])\n            self.eval_count += self.pop_size\n            self.budget -= self.pop_size\n\n            if np.min(fitness) < self.f_opt:\n                self.f_opt = np.min(fitness)\n                self.x_opt = x[np.argmin(fitness)]\n            \n            idx = np.argsort(fitness)\n            x_mu = x[idx[:self.mu]]\n            z_mu = z[idx[:self.mu]]\n            \n            y = x_mu - self.m\n            y_w = np.sum(self.weights[:, None] * y, axis=0)\n            z_w = np.sum(self.weights[:, None] * z_mu, axis=0)\n\n            self.ps = (1 - self.c_sigma) * self.ps + np.sqrt(self.c_sigma * (2 - self.c_sigma)) * z_w\n            self.pc = (1 - self.c_c) * self.pc + np.sqrt(self.c_c * (2 - self.c_c)) * np.sqrt(np.sum(self.weights**2)) * C_sqrt @ z_w\n            \n            norm_ps = np.linalg.norm(self.ps)\n            self.sigma *= np.exp((self.c_sigma / self.d_sigma) * (norm_ps / self.chiN - 1))\n            \n            self.C = (1 - self.c_1 - self.c_mu) * self.C + self.c_1 * np.outer(self.pc, self.pc)\n            self.C += self.c_mu * (z_mu.T @ np.diag(self.weights) @ z_mu)\n\n            self.m = x_mu.T @ self.weights\n            \n            if np.any(np.isnan(self.C)):\n                self.C = np.eye(self.dim)\n            \n            try:\n                self.C = np.triu(self.C) + np.triu(self.C, 1).T\n                np.linalg.cholesky(self.C)\n            except np.linalg.LinAlgError:\n                self.C = np.eye(self.dim)\n\n            if self.eval_count - self.restart_count * self.restart_trigger > self.restart_trigger:\n                self.m = np.random.uniform(self.lb, self.ub, size=self.dim)\n                self.sigma = 0.2\n                self.C = np.eye(self.dim)\n                self.pc = np.zeros(self.dim)\n                self.ps = np.zeros(self.dim)\n                self.restart_count += 1\n                \n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm CMAES_Restart scored 0.249 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.08010550853950149, 0.15176605634227636, 0.29945443264589344, 0.15240374447253013, 0.16166034683620345, 0.15375485405119182, 0.18225747266460435, 0.13234264995446632, 0.1471020758281677, 0.13186779137490268, 0.1935909872039756, 1.0, 0.31604415490594573, 0.15620466977893754, 0.6962330837957287, 0.28381204586580167, 0.2225722528497317, 0.23818345582399614, 0.14055671688295168, 0.14349721105267654]}, "task_prompt": ""}
{"id": "51cae60d-fb8a-48c2-91b3-4ffe04ba471d", "fitness": "-inf", "name": "HybridDE", "description": "No description provided.", "code": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridDE:\n    def __init__(self, budget=10000, dim=10, pop_size=20, lb=-5.0, ub=5.0, de_steps=5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lb = lb\n        self.ub = ub\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.de_steps = de_steps\n        self.F = 0.7\n        self.CR = 0.9\n\n\n    def __call__(self, func):\n        # Initialize population\n        population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        # Find initial best\n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index]\n\n        for _ in range(self.de_steps):\n            if self.budget <= 0:\n                break\n            for i in range(self.pop_size):\n                if self.budget <= 0:\n                    break\n                # Mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = a + self.F * (b - c)\n                mutant = np.clip(mutant, self.lb, self.ub)\n\n                # Crossover\n                trial = np.copy(population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR or j == j_rand:\n                        trial[j] = mutant[j]\n\n                # Evaluation\n                f_trial = func(trial)\n                self.budget -= 1\n\n                # Selection\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n\n                    # Update best\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n        # Local Search\n        if self.budget > self.dim * 10:\n            result = minimize(func, self.x_opt, method='Nelder-Mead', bounds=[(self.lb, self.ub)] * self.dim, options={'maxfev': self.budget})\n            if result.fun < self.f_opt:\n                self.f_opt = result.fun\n                self.x_opt = result.x\n            self.budget -= result.nfev\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "In the code, line 64, in __call__, the following error occurred:\nNameError: name 'minimize' is not defined\nOn line: result = minimize(func, self.x_opt, method='Nelder-Mead', bounds=[(self.lb, self.ub)] * self.dim, options={'maxfev': self.budget})", "error": "In the code, line 64, in __call__, the following error occurred:\nNameError: name 'minimize' is not defined\nOn line: result = minimize(func, self.x_opt, method='Nelder-Mead', bounds=[(self.lb, self.ub)] * self.dim, options={'maxfev': self.budget})", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "241b7a17-c2bc-42ab-aa20-93464412c4f3", "fitness": 0.22064468908621473, "name": "CMAES_PBSGD", "description": "No description provided.", "code": "import numpy as np\n\nclass CMAES_PBSGD:\n    def __init__(self, budget=10000, dim=10, pop_size=20, learning_rate=0.01, momentum=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.learning_rate = learning_rate\n        self.momentum = momentum\n        self.lb = -5.0\n        self.ub = 5.0\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.C = np.eye(dim)  # Covariance matrix\n        self.velocity = np.zeros((pop_size, dim))\n\n    def __call__(self, func):\n        # Initialize population\n        population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        # Find initial best\n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index]\n\n        while self.budget > 0:\n            # Sort population by fitness\n            sorted_indices = np.argsort(fitness)\n            population = population[sorted_indices]\n            fitness = fitness[sorted_indices]\n\n            # Update covariance matrix (simplified CMA-ES update)\n            weights = np.log(self.pop_size + 0.5) - np.log(np.arange(1, self.pop_size + 1))\n            weights /= np.sum(weights)\n            \n            mean = np.sum(population * weights[:, None], axis=0)\n            \n            C_update = np.zeros_like(self.C)\n            for i in range(self.pop_size):\n                diff = population[i] - mean\n                C_update += weights[i] * np.outer(diff, diff)\n            \n            self.C = (1 - 0.1) * self.C + 0.1 * C_update  # Learning rate\n\n            # Population-based Stochastic Gradient Descent\n            for i in range(self.pop_size):\n                # Sample search direction from multivariate normal distribution\n                direction = np.random.multivariate_normal(np.zeros(self.dim), self.C)\n\n                # Update velocity with momentum\n                self.velocity[i] = self.momentum * self.velocity[i] + self.learning_rate * direction\n\n                # Update individual position\n                trial = population[i] - self.velocity[i] # Gradient Descent\n                trial = np.clip(trial, self.lb, self.ub)\n\n                # Evaluation\n                f_trial = func(trial)\n                self.budget -= 1\n\n                # Selection\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n\n                    # Update best\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n            best_index = np.argmin(fitness)\n            if fitness[best_index] < self.f_opt:\n                self.f_opt = fitness[best_index]\n                self.x_opt = population[best_index]\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm CMAES_PBSGD scored 0.221 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.09743725056750385, 0.19891985840266801, 0.26157636786973815, 0.19251230131249442, 0.15112598590380277, 0.178111654670297, 0.22028612561607008, 0.20318748320825797, 0.1696705272018516, 0.1262911583764087, 0.39139926428306193, 0.22275247397634956, 0.2661211037968967, 0.16396868259283814, 0.34581662812477987, 0.2731455557943485, 0.19554230170521958, 0.3601646946704794, 0.14530987581660793, 0.2495544878346203]}, "task_prompt": ""}
{"id": "b735eea9-86c7-41da-a471-d54eff1c8bc9", "fitness": "-inf", "name": "SelfAdaptiveDE_LocalSearch", "description": "No description provided.", "code": "import numpy as np\n\nclass SelfAdaptiveDE_LocalSearch:\n    def __init__(self, budget=10000, dim=10, pop_size=20, F=0.5, CR=0.9, local_search_prob=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F  # Mutation factor\n        self.CR = CR  # Crossover rate\n        self.lb = -5.0\n        self.ub = 5.0\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.strategy_probs = np.array([0.5, 0.5])  # Probabilities for DE strategies\n        self.num_strategies = 2\n        self.local_search_prob = local_search_prob\n\n    def __call__(self, func):\n        # Initialize population\n        population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        # Find initial best\n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index]\n        \n        success_memory_fitness = np.zeros((self.pop_size,self.num_strategies))\n        success_memory_locations = np.zeros((self.pop_size,self.num_strategies,self.dim))\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Strategy selection\n                strategy_index = np.random.choice(self.num_strategies, p=self.strategy_probs)\n\n                # Mutation and Crossover\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                \n                if strategy_index == 0: # DE/rand/1\n                    mutant = population[np.random.choice(idxs, 1, replace=False)][0] + self.F * (b - c)\n                else: # DE/current-to-best/1\n                    mutant = population[i] + self.F * (self.x_opt - population[i]) + self.F * (b - c)\n\n                mutant = np.clip(mutant, self.lb, self.ub)\n\n                trial = np.copy(population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR or j == j_rand:\n                        trial[j] = mutant[j]\n                \n                # Local search with a probability\n                if np.random.rand() < self.local_search_prob:\n                    trial = self.local_search(trial, func) #changed from population[i] to trial because we are searching around trial\n                    trial = np.clip(trial, self.lb, self.ub)\n\n                # Evaluation\n                f_trial = func(trial)\n                self.budget -= 1\n\n                # Selection\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    \n                    success_memory_fitness[i,strategy_index] = f_trial\n                    success_memory_locations[i,strategy_index] = trial\n                    \n                    # Update best\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n            # Update strategy probabilities (simplified)\n            success_rate = np.mean(success_memory_fitness < fitness[:,None], axis=0) #this is not working\n            self.strategy_probs = (success_rate + 0.01) / np.sum(success_rate + 0.01) # avoid division by zero\n\n        return self.f_opt, self.x_opt\n\n    def local_search(self, x, func, radius=0.1, num_steps=5):\n        \"\"\"Performs a simple local search around the given solution.\"\"\"\n        best_x = x\n        best_f = func(x)\n        self.budget -= 1 #Important for not exceeding the budget\n        \n        for _ in range(num_steps):\n            # Generate a random neighbor within the radius\n            neighbor = x + np.random.uniform(-radius, radius, size=self.dim)\n            neighbor = np.clip(neighbor, self.lb, self.ub)\n            \n            f_neighbor = func(neighbor)\n            self.budget -= 1 #Important for not exceeding the budget\n            \n            if f_neighbor < best_f:\n                best_f = f_neighbor\n                best_x = neighbor\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "In the code, line 39, in _wrapit, the following error occurred:\nValueError: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part.\nOn line: a, b, c = population[np.random.choice(idxs, 3, replace=False)]", "error": "In the code, line 39, in _wrapit, the following error occurred:\nValueError: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part.\nOn line: a, b, c = population[np.random.choice(idxs, 3, replace=False)]", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "232a641a-e7da-4dba-a497-4cffaa3fc7ce", "fitness": "-inf", "name": "CMAES", "description": "No description provided.", "code": "import numpy as np\n\nclass CMAES:\n    def __init__(self, budget=10000, dim=10, pop_size=None, lb=-5.0, ub=5.0,\n                 sigma0=0.5, cs=0.3, dsigma=1.0, c_cov=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size if pop_size is not None else 4 + int(3 * np.log(dim))\n        self.lb = lb\n        self.ub = ub\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.sigma = sigma0\n        self.mean = np.random.uniform(lb, ub, size=dim)\n        self.C = np.eye(dim)\n        self.ps = np.zeros(dim)\n        self.pc = np.zeros(dim)\n        self.cs = cs\n        self.dsigma = dsigma\n        self.c_cov = c_cov\n        self.mu = self.pop_size // 2\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights /= np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        self.c_s = (self.mueff + 2) / (self.dim + self.mueff + 5)\n        self.c_cov = (1 / self.mueff) * ((self.mueff - 2 + 1/self.mueff) / (self.dim + 2)**2 + (2 - 1/self.mueff) / ((self.dim + 2) * np.sqrt(self.mueff + 2)))\n        self.damps = 1 + 2 * max(0, np.sqrt((self.mueff-1)/(self.dim+1)) - 1) + self.cs\n\n\n    def __call__(self, func):\n        while self.budget > 0:\n            # Generate population\n            Z = np.random.randn(self.pop_size, self.dim)\n            X = self.mean + self.sigma * Z @ np.linalg.cholesky(self.C).T\n            X = np.clip(X, self.lb, self.ub)\n\n            # Evaluate population\n            fitness = np.array([func(x) for x in X])\n            self.budget -= self.pop_size\n\n            # Sort by fitness\n            idx = np.argsort(fitness)\n            fitness = fitness[idx]\n            X = X[idx]\n\n            # Update best\n            if fitness[0] < self.f_opt:\n                self.f_opt = fitness[0]\n                self.x_opt = X[0]\n\n            # Update mean\n            xmean = np.sum(X[:self.mu].T * self.weights, axis=1)\n            y = (xmean - self.mean) / self.sigma\n            self.mean = xmean\n\n            # Update evolution path\n            self.ps = (1 - self.c_s) * self.ps + np.sqrt(self.c_s * (2 - self.c_s) * self.mueff) * y\n            hsig = np.linalg.norm(self.ps) / np.sqrt(1 - (1 - self.c_s)**2)**0.5 / np.sqrt(self.dim) < 1.4 + 2 / (self.dim + 1)\n\n            # Update covariance matrix\n            self.pc = (1 - self.c_cov) * self.pc + hsig * np.sqrt(self.c_cov * (2 - self.c_cov) * self.mueff) * y\n            artmp = (X[:self.mu] - np.tile(self.mean, (self.mu, 1))).T / self.sigma\n            self.C = (1 - self.c_cov) * self.C + self.c_cov * (1 / self.mueff * (self.pc[:, None] @ self.pc[None, :]) + np.sum(self.weights[:, None, None] * artmp[:, :, None] @ artmp[:, None, :], axis=0))\n\n            # Update step size\n            self.sigma *= np.exp((self.cs / self.damps) * (np.linalg.norm(self.ps) / (np.sqrt(self.dim) * (1 - (1 - self.cs)**(2 * (self.budget // self.pop_size)))**0.5) - 1))\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "In the code, line 63, in __call__, the following error occurred:\nValueError: operands could not be broadcast together with shapes (3,1,1) (2,3,1) \nOn line: self.C = (1 - self.c_cov) * self.C + self.c_cov * (1 / self.mueff * (self.pc[:, None] @ self.pc[None, :]) + np.sum(self.weights[:, None, None] * artmp[:, :, None] @ artmp[:, None, :], axis=0))", "error": "In the code, line 63, in __call__, the following error occurred:\nValueError: operands could not be broadcast together with shapes (3,1,1) (2,3,1) \nOn line: self.C = (1 - self.c_cov) * self.C + self.c_cov * (1 / self.mueff * (self.pc[:, None] @ self.pc[None, :]) + np.sum(self.weights[:, None, None] * artmp[:, :, None] @ artmp[:, None, :], axis=0))", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "b594e172-2a8b-4252-a033-82e7e30d2858", "fitness": 0.25136645817026115, "name": "SimplifiedCMAES", "description": "No description provided.", "code": "import numpy as np\n\nclass SimplifiedCMAES:\n    def __init__(self, budget=10000, dim=10, pop_size=None, lb=-5.0, ub=5.0, cs=0.3, c_cov=0.05):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size if pop_size is not None else 4 + int(3 * np.log(dim))\n        self.lb = lb\n        self.ub = ub\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.mean = np.random.uniform(lb, ub, size=dim)\n        self.sigma = 0.3 * (ub - lb)\n        self.cs = cs\n        self.c_cov = c_cov\n        self.p_sigma = np.zeros(dim)\n        self.C = np.eye(dim)\n\n    def __call__(self, func):\n        while self.budget > 0:\n            # Sample population\n            z = np.random.multivariate_normal(np.zeros(self.dim), self.C, size=self.pop_size)\n            population = self.mean + self.sigma * z\n            population = np.clip(population, self.lb, self.ub)\n\n            # Evaluate population\n            fitness = np.array([func(x) for x in population])\n            self.budget -= self.pop_size\n\n            # Find best individual\n            best_index = np.argmin(fitness)\n            if fitness[best_index] < self.f_opt:\n                self.f_opt = fitness[best_index]\n                self.x_opt = population[best_index]\n\n            # Update mean\n            x_best = population[best_index]\n            self.mean = x_best\n\n            # Update evolution path for sigma\n            z_best = (x_best - self.mean) / self.sigma\n            self.p_sigma = (1 - self.cs) * self.p_sigma + np.sqrt(self.cs * (2 - self.cs)) * z_best\n\n            # Update covariance matrix\n            self.C = (1 - self.c_cov) * self.C + self.c_cov * np.outer(self.p_sigma, self.p_sigma)\n\n            # Ensure C is positive definite\n            try:\n                np.linalg.cholesky(self.C)\n            except np.linalg.LinAlgError:\n                self.C = np.eye(self.dim)\n\n            # Update step size\n            self.sigma *= np.exp((self.cs / 0.8 ) * (np.linalg.norm(self.p_sigma) / np.sqrt(self.dim) - 1))\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm SimplifiedCMAES scored 0.251 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.11263066907907049, 0.1609019456210239, 0.27410321440061614, 0.1697184826944933, 0.16482289896085134, 0.2062531034435786, 0.2334660902156387, 0.18421736006568024, 0.1619904054943474, 0.17509554477281286, 0.16449118853069866, 0.9993156105574387, 0.25073947156900556, 0.2211334891079496, 0.5501977520850965, 0.29634976744600317, 0.15472277573231308, 0.18971673295915692, 0.14769665709086788, 0.20976600357857966]}, "task_prompt": ""}
{"id": "86b7f37a-9f4a-4f71-8548-e99beef1b99d", "fitness": 0.30465849825272595, "name": "SimplifiedCMAES", "description": "No description provided.", "code": "import numpy as np\n\nclass SimplifiedCMAES:\n    def __init__(self, budget=10000, dim=10, pop_size=None, lb=-5.0, ub=5.0, cs=0.3, damps=1.0):\n        self.budget = budget\n        self.dim = dim\n        self.lb = lb\n        self.ub = ub\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.pop_size = pop_size if pop_size is not None else 4 + int(3 * np.log(self.dim))\n        self.mu = self.pop_size // 2\n        self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n        self.weights /= np.sum(self.weights)\n        self.m = np.random.uniform(self.lb, self.ub, size=self.dim)\n        self.sigma = 0.3 * (self.ub - self.lb)\n        self.C = np.eye(self.dim)\n        self.cs = cs\n        self.damps = damps\n\n    def __call__(self, func):\n        while self.budget > 0:\n            Z = np.random.multivariate_normal(np.zeros(self.dim), self.C, size=self.pop_size)\n            X = self.m + self.sigma * Z\n            X = np.clip(X, self.lb, self.ub)\n            \n            fitness = np.array([func(x) for x in X])\n            self.budget -= self.pop_size\n            \n            idx = np.argsort(fitness)\n            best_indices = idx[:self.mu]\n            \n            x_best = X[best_indices]\n            f_best = fitness[best_indices]\n            \n            if np.min(f_best) < self.f_opt:\n                self.f_opt = np.min(f_best)\n                self.x_opt = x_best[np.argmin(f_best)]\n\n            m_old = self.m\n            self.m = np.sum(x_best * self.weights[:, np.newaxis], axis=0)\n\n            B = X[best_indices] - m_old\n            self.C = (1 - self.cs) * self.C + self.cs * np.cov(B.T, aweights=self.weights, bias=True)\n            \n            self.sigma *= np.exp((self.cs / self.damps) * (np.linalg.norm(self.m - m_old) / self.sigma - 1))\n            \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm SimplifiedCMAES scored 0.305 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.10203621642237382, 0.1624100800951852, 0.233027949800778, 0.1902508399065499, 0.22842580526692058, 0.39241372750059034, 0.2730048377052421, 0.19209375840746246, 0.17533350062850128, 0.14645894943174753, 0.5058318879513748, 0.9940833361258, 0.24696788478610343, 0.2666752462949964, 0.655169054716523, 0.3040150690737008, 0.21502309021092392, 0.21350999639909163, 0.14707652750225075, 0.44936220682840444]}, "task_prompt": ""}
{"id": "1201e336-678a-4f96-8ef8-e897aa6dc833", "fitness": 0.5582619723371726, "name": "ParticleSwarmOptimizer", "description": "No description provided.", "code": "import numpy as np\n\nclass ParticleSwarmOptimizer:\n    def __init__(self, budget=10000, dim=10, pop_size=20, w=0.7, c1=1.5, c2=1.5, v_max=1.0):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.w = w  # Inertia weight\n        self.c1 = c1  # Cognitive coefficient\n        self.c2 = c2  # Social coefficient\n        self.v_max = v_max  # Velocity clamping\n        self.lb = -5.0\n        self.ub = 5.0\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.constriction_factor = 0.729  # Constriction factor to improve convergence\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.velocities = np.random.uniform(-self.v_max, self.v_max, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.pbest_positions = self.population.copy()\n        self.pbest_fitness = self.fitness.copy()\n\n        best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_index]\n        self.x_opt = self.population[best_index]\n        self.budget -= self.pop_size\n\n    def evolve(self, func):\n        for i in range(self.pop_size):\n            # Update velocity\n            r1 = np.random.rand(self.dim)\n            r2 = np.random.rand(self.dim)\n            cognitive_component = self.c1 * r1 * (self.pbest_positions[i] - self.population[i])\n            social_component = self.c2 * r2 * (self.x_opt - self.population[i])\n            self.velocities[i] = self.constriction_factor * (self.w * self.velocities[i] + cognitive_component + social_component)\n            self.velocities[i] = np.clip(self.velocities[i], -self.v_max, self.v_max)\n\n            # Update position\n            self.population[i] += self.velocities[i]\n            self.population[i] = np.clip(self.population[i], self.lb, self.ub)\n\n            # Evaluate fitness\n            f = func(self.population[i])\n            self.budget -= 1\n\n            # Update personal best\n            if f < self.pbest_fitness[i]:\n                self.pbest_fitness[i] = f\n                self.pbest_positions[i] = self.population[i].copy()\n\n            # Update global best\n            if f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = self.population[i].copy()\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        while self.budget > 0:\n            self.evolve(func)\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm ParticleSwarmOptimizer scored 0.558 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.12264168785956042, 0.16360050000851822, 0.9424975223337634, 0.24041136534371077, 0.23090896946492434, 0.9570214930911437, 0.25414058927993655, 0.7603729343079714, 0.9486449837635829, 0.21781021442770743, 0.9695330309235789, 0.9928743226648947, 0.25514074117344443, 0.22292018097880828, 0.7395200403461064, 0.8426668207865253, 0.5196859785843593, 0.9575394318112189, 0.3803546202807695, 0.4469540193129258]}, "task_prompt": ""}
{"id": "0bb4023e-70db-400e-adc4-a536b4008c3f", "fitness": 0.2640524044065398, "name": "CMAES", "description": "No description provided.", "code": "import numpy as np\n\nclass CMAES:\n    def __init__(self, budget=10000, dim=10, pop_size=None, sigma=0.5):\n        self.budget = budget\n        self.dim = dim\n        self.sigma = sigma\n        self.pop_size = pop_size if pop_size is not None else 4 + int(3 * np.log(self.dim))\n        self.mu = self.pop_size // 2\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.m = np.zeros(self.dim)\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.chiN = np.sqrt(self.dim) * (1 - 1/(4*self.dim) + 1/(21*self.dim**2))\n        self.c_sigma = (self.mu / (self.dim + self.mu**2))**0.5\n        self.d_sigma = 1 + 2*max(0, np.sqrt((self.mu-1)/(self.dim+1)) - 1) + self.c_sigma\n        self.c_c = (4 + self.mu/self.dim)/(self.dim + 4 + 2*self.mu/self.dim)\n        self.c_1 = 2 / ((self.dim + 1.3)**2 + self.mu)\n        self.c_mu = min(1-self.c_1, 2 * (self.mu - 2 + 1/self.mu) / ((self.dim + 2)**2 + self.mu))\n        self.lb = -5.0\n        self.ub = 5.0\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        while self.budget > 0:\n            z = np.random.multivariate_normal(np.zeros(self.dim), self.C, size=self.pop_size)\n            y = self.sigma * z\n            x = self.m + y\n            \n            # Clip to bounds\n            x = np.clip(x, self.lb, self.ub)\n\n            fitness = np.array([func(xi) for xi in x])\n            self.budget -= self.pop_size\n\n            if np.min(fitness) < self.f_opt:\n                self.f_opt = np.min(fitness)\n                self.x_opt = x[np.argmin(fitness)].copy()\n            \n            idx = np.argsort(fitness)\n            x_mu = x[idx[:self.mu]]\n            y_mu = y[idx[:self.mu]]\n\n            self.m = np.sum(x_mu * self.weights[:, None], axis=0)\n            z_mu = np.sum(y_mu * self.weights[:, None], axis=0) / self.sigma\n\n            self.ps = (1 - self.c_sigma) * self.ps + np.sqrt(self.c_sigma * (2 - self.c_sigma)) * z_mu\n            norm_ps = np.linalg.norm(self.ps)\n\n            self.sigma *= np.exp((self.c_sigma / self.d_sigma) * (norm_ps / self.chiN - 1))\n            self.sigma = min(max(self.sigma, 1e-10), 10)  # Clamp sigma\n            \n            self.pc = (1 - self.c_c) * self.pc + np.sqrt(self.c_c * (2 - self.c_c)) * y_mu[0] / self.sigma  # use only one y_mu to prevent rank deficient\n\n            delta = x_mu - self.m\n            self.C = (1 - self.c_1 - self.c_mu) * self.C \\\n                        + self.c_1 * np.outer(self.pc, self.pc) \\\n                        + self.c_mu * np.sum(self.weights[:, None, None] * delta[:, :, None] * delta[:, None, :], axis=0)\n            \n            self.C = np.triu(self.C) + np.triu(self.C, 1).T # enforce symmetry\n            \n            try:\n                L = np.linalg.cholesky(self.C) # check if pos definite\n            except np.linalg.LinAlgError:\n                self.C = self.C + 1e-6 * np.eye(self.dim)  #regularize the matrix if not positive definite\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm CMAES scored 0.264 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.0820466447044198, 0.1856338782825433, 0.37621809524980343, 0.15400895276290172, 0.2446612609629667, 0.14905503182801494, 0.2716849991820389, 0.3143488271284536, 0.42126297615549224, 0.1533971565203046, 0.15671514264747932, 0.21376807846061796, 0.30550022883473027, 0.21632869868873061, 0.6601401423041056, 0.27062306948608217, 0.22912982812635296, 0.24759779033564733, 0.14010980317417254, 0.48881748329593844]}, "task_prompt": ""}
{"id": "cb10625f-369b-475a-bab6-0d143e9a2dde", "fitness": 0.5882333965330497, "name": "SimplifiedPSO", "description": "No description provided.", "code": "import numpy as np\n\nclass SimplifiedPSO:\n    def __init__(self, budget=10000, dim=10, pop_size=20, w=0.7, c1=1.5, c2=1.5, v_max=1.0):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.w = w  # Inertia weight\n        self.c1 = c1  # Cognitive coefficient\n        self.c2 = c2  # Social coefficient\n        self.v_max = v_max # Velocity clamping\n        self.lb = -5.0\n        self.ub = 5.0\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.particles = None\n        self.velocities = None\n        self.personal_best_positions = None\n        self.personal_best_fitness = None\n        self.global_best_position = None\n        self.global_best_fitness = np.inf\n        self.K = 0.729  # Constriction factor\n\n    def __call__(self, func):\n        # Initialize particles and velocities\n        self.particles = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.velocities = np.random.uniform(-self.v_max, self.v_max, size=(self.pop_size, self.dim))\n        self.personal_best_positions = np.copy(self.particles)\n        self.personal_best_fitness = np.array([func(x) for x in self.particles])\n        self.budget -= self.pop_size\n\n        # Find initial global best\n        best_index = np.argmin(self.personal_best_fitness)\n        self.global_best_position = self.personal_best_positions[best_index]\n        self.global_best_fitness = self.personal_best_fitness[best_index]\n        self.f_opt = self.global_best_fitness\n        self.x_opt = self.global_best_position\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Update velocity\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                cognitive_component = self.c1 * r1 * (self.personal_best_positions[i] - self.particles[i])\n                social_component = self.c2 * r2 * (self.global_best_position - self.particles[i])\n                self.velocities[i] = self.K * (self.w * self.velocities[i] + cognitive_component + social_component)\n                self.velocities[i] = np.clip(self.velocities[i], -self.v_max, self.v_max)\n\n                # Update position\n                self.particles[i] += self.velocities[i]\n                self.particles[i] = np.clip(self.particles[i], self.lb, self.ub)\n\n                # Evaluate fitness\n                fitness = func(self.particles[i])\n                self.budget -= 1\n\n                # Update personal best\n                if fitness < self.personal_best_fitness[i]:\n                    self.personal_best_fitness[i] = fitness\n                    self.personal_best_positions[i] = np.copy(self.particles[i])\n\n                    # Update global best\n                    if fitness < self.global_best_fitness:\n                        self.global_best_fitness = fitness\n                        self.global_best_position = np.copy(self.particles[i])\n                        self.f_opt = self.global_best_fitness\n                        self.x_opt = self.global_best_position\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm SimplifiedPSO scored 0.588 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.16496339116679648, 0.22196259640293425, 0.5136977207376583, 0.9755845215702077, 0.7889458084776194, 0.955467961285922, 0.29096024322413994, 0.4616948421299878, 0.9537215323448437, 0.2421223542206844, 0.9749262015112266, 0.9903214167599271, 0.2444663299208948, 0.2398985297466698, 0.7091398042309731, 0.6696358176740057, 0.6270205343192303, 0.9705006905294334, 0.29043083881480747, 0.4792067955930337]}, "task_prompt": ""}
{"id": "c943c15d-ef59-4734-ae72-37525dee8454", "fitness": "-inf", "name": "BayesianOptimization", "description": "No description provided.", "code": "import numpy as np\nfrom sklearn.gaussian_process import GaussianProcessRegressor\nfrom sklearn.gaussian_process.kernels import RBF, ConstantKernel as C\nfrom scipy.stats import norm\n\nclass BayesianOptimization:\n    def __init__(self, budget=10000, dim=10, n_initial_samples=10):\n        self.budget = budget\n        self.dim = dim\n        self.lb = -5.0\n        self.ub = 5.0\n        self.n_initial_samples = n_initial_samples\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.X = None\n        self.y = None\n        self.gpr = None\n\n    def acquisition_function(self, x, xi=0.01):\n        mu, sigma = self.gpr.predict(x.reshape(1, -1), return_std=True)\n        mu = mu[0]\n        sigma = sigma[0]\n        \n        if sigma == 0:\n            return 0\n            \n        imp = (mu - self.f_opt - xi)\n        Z = imp / sigma\n        ei = imp * norm.cdf(Z) + sigma * norm.pdf(Z)\n        return ei\n\n    def __call__(self, func):\n        # Initial sampling\n        self.X = np.random.uniform(self.lb, self.ub, size=(self.n_initial_samples, self.dim))\n        self.y = np.array([func(x) for x in self.X])\n        self.budget -= self.n_initial_samples\n        best_index = np.argmin(self.y)\n        self.f_opt = self.y[best_index]\n        self.x_opt = self.X[best_index]\n\n        # Gaussian process regression\n        kernel = C(1.0, (1e-3, 1e3)) * RBF(1.0, (1e-2, 1e2))\n        self.gpr = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=10, random_state=42)\n\n        while self.budget > 0:\n            self.gpr.fit(self.X, self.y)\n\n            # Find the next point to evaluate using acquisition function\n            from scipy.optimize import minimize\n            def objective(x):\n                return -self.acquisition_function(x)\n\n            bounds = [(self.lb, self.ub)] * self.dim\n            \n            x0 = np.random.uniform(self.lb, self.ub, size=self.dim)\n            res = minimize(objective, x0, method='L-BFGS-B', bounds=bounds)\n            x_next = res.x\n\n            # Evaluate the function\n            f_next = func(x_next)\n            self.budget -= 1\n\n            # Update the data\n            self.X = np.vstack((self.X, x_next))\n            self.y = np.append(self.y, f_next)\n\n            # Update the best solution\n            if f_next < self.f_opt:\n                self.f_opt = f_next\n                self.x_opt = x_next\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "In the code, line 42, in __call__, the following error occurred:\nNameError: name 'C' is not defined\nOn line: kernel = C(1.0, (1e-3, 1e3)) * RBF(1.0, (1e-2, 1e2))", "error": "In the code, line 42, in __call__, the following error occurred:\nNameError: name 'C' is not defined\nOn line: kernel = C(1.0, (1e-3, 1e3)) * RBF(1.0, (1e-2, 1e2))", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "e5e96e80-ded6-4111-8894-1eb5d330b1ff", "fitness": "-inf", "name": "CMAES", "description": "No description provided.", "code": "import numpy as np\n\nclass CMAES:\n    def __init__(self, budget=10000, dim=10, pop_size=None, lb=-5.0, ub=5.0, sigma=0.5, cs=0.3, damps=1.0, c_cov_mean=None, c_cov_rank_one=None, c_cov_rank_mu=None, mu_percentage=0.5):\n        self.budget = budget\n        self.dim = dim\n        self.lb = lb\n        self.ub = ub\n        self.sigma = sigma\n        self.pop_size = pop_size if pop_size else 4 + int(3 * np.log(self.dim))\n        self.mu = int(self.pop_size * mu_percentage)\n        self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n        self.weights /= np.sum(self.weights)\n        self.m = (ub + lb) / 2 * np.ones(self.dim)\n        self.P_sigma = np.zeros(self.dim)\n        self.P_c = np.zeros(self.dim)\n        self.C = np.eye(self.dim)\n        self.cs = cs\n        self.damps = damps + 2 * max(0, np.sqrt((self.mu - 1) / (self.dim + 1)) - 1) * damps\n        self.c_cov_mean = c_cov_mean if c_cov_mean else self.mu / (self.dim + np.square(self.mu / self.dim))\n        self.c_cov_rank_one = 2 / ((self.dim + 1.3)**2 + self.mu)\n        self.c_cov_rank_mu = 2 * (self.mu - 1 + 1e-8) / ((self.dim + 2)**2 + self.mu)\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        while self.budget > 0:\n            # Generate samples\n            z = np.random.randn(self.dim, self.pop_size)\n            x = self.m[:, np.newaxis] + self.sigma * np.dot(np.linalg.cholesky(self.C), z)\n            x = np.clip(x, self.lb, self.ub)\n            \n            # Evaluate samples\n            fitness = np.array([func(x[:, i]) for i in range(self.pop_size)])\n            self.budget -= self.pop_size\n            \n            # Sort by fitness\n            idxs = np.argsort(fitness)\n            x = x[:, idxs]\n            fitness = fitness[idxs]\n\n            # Update best\n            if fitness[0] < self.f_opt:\n                self.f_opt = fitness[0]\n                self.x_opt = x[:, 0]\n\n            # Update mean\n            m_old = self.m.copy()\n            self.m = np.dot(x[:, :self.mu], self.weights)\n            \n            # Update evolution path for sigma\n            B = np.linalg.cholesky(self.C)\n            z_mean = np.dot(z[:, idxs[:self.mu]], self.weights)\n            self.P_sigma = (1 - self.cs) * self.P_sigma + np.sqrt(self.cs * (2 - self.cs)) * np.dot(B, z_mean)\n\n            # Update covariance matrix\n            h_sigma = np.linalg.norm(self.P_sigma) / np.sqrt(1 - (1 - self.cs)**(2 * (self.budget / self.pop_size))) < (1.4 + 2 / (self.dim + 1)) * self.dim**(0.5)\n            d_h_sigma = 1 if h_sigma else 0\n            \n            self.P_c = (1 - self.c_cov_mean) * self.P_c + d_h_sigma * np.sqrt(self.c_cov_mean * (2 - self.c_cov_mean)) * (self.m - m_old) / self.sigma\n            self.C = (1 - self.c_cov_rank_one - self.c_cov_rank_mu) * self.C + self.c_cov_rank_one * np.outer(self.P_c, self.P_c)\n            \n            for i in range(self.mu):\n              self.C += self.c_cov_rank_mu * self.weights[i] * np.outer((x[:,i]-m_old)/self.sigma, (x[:,i]-m_old)/self.sigma)\n\n            # Ensure positive definiteness\n            self.C = np.triu(self.C) + np.triu(self.C, 1).T\n            self.C = np.linalg.eigvalsh(self.C).min()*np.eye(self.dim)*0.00000000001 + self.C\n            \n            # Update step size\n            self.sigma *= np.exp((self.cs / self.damps) * (np.linalg.norm(self.P_sigma) / np.sqrt(self.dim) - 1))\n        \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "In the code, line 148, in _raise_linalgerror_nonposdef, the following error occurred:\nLinAlgError: Matrix is not positive definite", "error": "In the code, line 148, in _raise_linalgerror_nonposdef, the following error occurred:\nLinAlgError: Matrix is not positive definite", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "0030b32f-296f-4404-a6b2-75b4f95fff1d", "fitness": "-inf", "name": "CMAES", "description": "No description provided.", "code": "import numpy as np\n\nclass CMAES:\n    def __init__(self, budget=10000, dim=10, pop_size=None, lb=-5.0, ub=5.0, initial_sigma=0.2):\n        self.budget = budget\n        self.dim = dim\n        self.lb = lb\n        self.ub = ub\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.pop_size = pop_size if pop_size is not None else 4 + int(3 * np.log(dim))\n        self.mean = np.random.uniform(lb, ub, size=dim)\n        self.sigma = initial_sigma\n        self.C = np.eye(dim)  # Covariance matrix\n        self.pc = np.zeros(dim)  # Evolution path for C\n        self.ps = np.zeros(dim)  # Evolution path for sigma\n        self.chiN = np.sqrt(dim) * (1 - (1 / (4 * dim)) + (1 / (100 * dim**2)))\n        self.c_sigma = (self.pop_size + 2) / (dim + self.pop_size + 5)\n        self.d_sigma = 1 + 2 * max(0, np.sqrt((self.c_sigma * (2 - self.c_sigma)) * dim**2) - 1) + self.c_sigma\n        self.c_c = 4 / (dim + 4)\n        self.mu = self.pop_size // 2\n        self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n        self.weights /= np.sum(self.weights)\n        self.c_1 = 2 / ((dim + 1.3)**2 + self.mu)\n        self.c_mu = min(1 - self.c_1, 2 * (self.mu - 1 + 1 / self.mu) / ((dim + 2)**2 + self.mu))\n        self.B = None\n        self.D = None\n        self.update_BD()\n\n    def update_BD(self):\n        self.D, self.B = np.linalg.eig(self.C)\n        self.D = np.sqrt(np.diag(self.D))\n        self.B = np.real(self.B)\n\n    def __call__(self, func):\n        while self.budget > 0:\n            # Sample population\n            z = np.random.normal(0, 1, size=(self.pop_size, self.dim))\n            y = self.B @ self.D @ z.T\n            population = self.mean + self.sigma * y.T\n            population = np.clip(population, self.lb, self.ub)\n            \n            # Evaluate population\n            fitness = np.array([func(x) for x in population])\n            self.budget -= self.pop_size\n            if self.budget <= 0:\n                fitness = fitness[:self.pop_size+self.budget]\n                population = population[:self.pop_size+self.budget]\n            \n            # Find best\n            best_index = np.argmin(fitness)\n            if fitness[best_index] < self.f_opt:\n                self.f_opt = fitness[best_index]\n                self.x_opt = population[best_index]\n\n            # Selection and Recombination\n            fitness_sorted_indices = np.argsort(fitness)\n            y_k = y[:, fitness_sorted_indices[:self.mu]]\n            x_diff = y_k @ self.weights\n\n            # Update Evolution Paths\n            self.ps = (1 - self.c_sigma) * self.ps + np.sqrt(self.c_sigma * (2 - self.c_sigma)) * (self.B @ x_diff)\n            if np.linalg.norm(self.ps) / np.sqrt(1 - (1 - self.c_sigma)**(self.budget / self.pop_size)) < self.chiN * (1.4 + 2 / (self.dim + 1)):\n                hsig = 1\n            else:\n                hsig = 0\n            self.pc = (1 - self.c_c) * self.pc + hsig * np.sqrt(self.c_c * (2 - self.c_c)) * x_diff\n            \n            # Update Covariance Matrix\n            self.C = (1 - self.c_1 - self.c_mu) * self.C + self.c_1 * (self.pc[:, None] @ self.pc[None, :])\n            for k in range(self.mu):\n                self.C += self.c_mu * self.weights[k] * (y_k[:, k:k+1] @ y_k[:, k:k+1].T)\n\n            # Update Step Size\n            self.sigma *= np.exp((self.c_sigma / self.d_sigma) * (np.linalg.norm(self.ps) / self.chiN - 1))\n            self.mean += self.sigma * self.B @ self.D @ np.sum(y_k * self.weights, axis=1)\n\n            # Keep C positive definite\n            try:\n                self.C = np.triu(self.C) + np.triu(self.C, 1).T\n                self.C = self.C / np.linalg.norm(self.C)\n            except:\n                self.C = np.eye(self.dim)\n            self.update_BD()\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "In the code, line 76, in __call__, the following error occurred:\n_UFuncOutputCastingError: Cannot cast ufunc 'add' output from dtype('complex128') to dtype('float64') with casting rule 'same_kind'\nOn line: self.mean += self.sigma * self.B @ self.D @ np.sum(y_k * self.weights, axis=1)", "error": "In the code, line 76, in __call__, the following error occurred:\n_UFuncOutputCastingError: Cannot cast ufunc 'add' output from dtype('complex128') to dtype('float64') with casting rule 'same_kind'\nOn line: self.mean += self.sigma * self.B @ self.D @ np.sum(y_k * self.weights, axis=1)", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "2ab1c8ed-ef2a-4169-b77a-06a37d58bd09", "fitness": 0.2593284543425008, "name": "AdaptiveSimulatedAnnealing", "description": "No description provided.", "code": "import numpy as np\n\nclass AdaptiveSimulatedAnnealing:\n    def __init__(self, budget=10000, dim=10, initial_temp=1.0, alpha=0.99, min_temp=1e-5, step_size=0.1, stagnation_threshold=500):\n        self.budget = budget\n        self.dim = dim\n        self.initial_temp = initial_temp\n        self.alpha = alpha\n        self.min_temp = min_temp\n        self.step_size = step_size\n        self.lb = -5.0\n        self.ub = 5.0\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.stagnation_threshold = stagnation_threshold\n        self.stagnation_counter = 0\n\n    def __call__(self, func):\n        x = np.random.uniform(self.lb, self.ub, size=self.dim)\n        f = func(x)\n        self.budget -= 1\n        self.f_opt = f\n        self.x_opt = x\n        temp = self.initial_temp\n\n        while self.budget > 0 and temp > self.min_temp:\n            x_new = x + np.random.normal(0, self.step_size, size=self.dim)\n            x_new = np.clip(x_new, self.lb, self.ub)\n            f_new = func(x_new)\n            self.budget -= 1\n\n            delta_f = f_new - f\n            if delta_f < 0 or np.random.rand() < np.exp(-delta_f / temp):\n                x = x_new\n                f = f_new\n\n                if f < self.f_opt:\n                    self.f_opt = f\n                    self.x_opt = x\n                    self.stagnation_counter = 0  # Reset stagnation counter\n                else:\n                    self.stagnation_counter += 1\n\n            else:\n                self.stagnation_counter += 1\n\n            # Adaptive temperature schedule and step size\n            if self.stagnation_counter > self.stagnation_threshold:\n                # Restart mechanism: Re-initialize x if stagnated\n                x = np.random.uniform(self.lb, self.ub, size=self.dim)\n                f = func(x)\n                self.budget -= 1\n                temp = self.initial_temp # Reinitialize temperature\n\n                self.stagnation_counter = 0\n\n            else:\n                temp *= self.alpha\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveSimulatedAnnealing scored 0.259 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.010785222700760655, 0.1653662732669745, 0.2447587096034345, 0.11715334124518262, 0.22282769199288377, 0.45670589469344314, 0.23387560942592878, 0.20663979958450285, 0.24400983414169708, 0.1325892413214822, 0.5084328856075091, 0.26334747032577377, 0.2436749227661852, 0.20303409662308813, 0.6504486274968511, 0.3108719730455509, 0.18456183282818162, 0.17329215649915675, 0.16867379840119723, 0.44551970528023166]}, "task_prompt": ""}
{"id": "ab377cc4-b909-4e20-a461-5f3b06ef35da", "fitness": 0.3422939133686997, "name": "GradientAdaptiveSearch", "description": "No description provided.", "code": "import numpy as np\n\nclass GradientAdaptiveSearch:\n    def __init__(self, budget=10000, dim=10, pop_size=20, step_size=0.1, grad_samples=5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.step_size = step_size\n        self.grad_samples = grad_samples\n        self.lb = -5.0\n        self.ub = 5.0\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        # Initialize population\n        population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        # Find initial best\n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index]\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Gradient estimation\n                grad_est = np.zeros(self.dim)\n                for _ in range(self.grad_samples):\n                    perturbation = np.random.normal(0, self.step_size, size=self.dim)\n                    x_perturbed = np.clip(population[i] + perturbation, self.lb, self.ub)\n                    f_perturbed = func(x_perturbed)\n                    self.budget -= 1\n                    if self.budget <= 0:\n                        return self.f_opt, self.x_opt\n                    grad_est += (f_perturbed - fitness[i]) * perturbation\n                grad_est /= self.grad_samples * self.step_size**2\n\n                # Gradient-based move\n                x_new = np.clip(population[i] - self.step_size * grad_est, self.lb, self.ub)\n                f_new = func(x_new)\n                self.budget -= 1\n                if self.budget <= 0:\n                    return self.f_opt, self.x_opt\n                \n                # Adaptive step size\n                if f_new < fitness[i]:\n                    population[i] = x_new\n                    fitness[i] = f_new\n                    self.step_size *= 1.1  # Increase step size\n                else:\n                    self.step_size *= 0.9  # Decrease step size\n\n                # Random exploration\n                if np.random.rand() < 0.1:\n                    x_rand = np.random.uniform(self.lb, self.ub, size=self.dim)\n                    f_rand = func(x_rand)\n                    self.budget -= 1\n                    if self.budget <= 0:\n                        return self.f_opt, self.x_opt\n\n                    if f_rand < fitness[i]:\n                        population[i] = x_rand\n                        fitness[i] = f_rand\n\n                # Update best\n                if fitness[i] < self.f_opt:\n                    self.f_opt = fitness[i]\n                    self.x_opt = population[i]\n\n            best_index = np.argmin(fitness)\n            if fitness[best_index] < self.f_opt:\n                self.f_opt = fitness[best_index]\n                self.x_opt = population[best_index]\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm GradientAdaptiveSearch scored 0.342 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.14274530197905244, 0.21188373793947413, 0.3379577061810427, 0.23081070078319044, 0.2795505941727412, 0.38671328125930626, 0.25307864971629457, 0.3137567298997529, 0.2655099516244178, 0.161106161124151, 0.27705367131668934, 0.9997408051454965, 0.2867503978600334, 0.296339410346124, 0.71574007395175, 0.3542831330444589, 0.2768649610912979, 0.42278151003246556, 0.1680757333877586, 0.46513575651849726]}, "task_prompt": ""}
{"id": "118467ba-e264-47c2-902f-29e02f6ac3b0", "fitness": 0.6236042089612424, "name": "AdaptiveCMAES", "description": "No description provided.", "code": "import numpy as np\n\nclass AdaptiveCMAES:\n    def __init__(self, budget=10000, dim=10, pop_size=None, lb=-5.0, ub=5.0,\n                 sigma0=0.5, mu_ratio=0.25, cs=0.3, damps=1.0, ccov1=0.0, ccovmu=0.0):\n        self.budget = budget\n        self.dim = dim\n        self.lb = lb\n        self.ub = ub\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.sigma0 = sigma0\n        self.mu_ratio = mu_ratio\n\n        self.pop_size = pop_size if pop_size is not None else 4 + int(3 * np.log(self.dim))\n        self.mu = int(self.pop_size * self.mu_ratio)\n\n        self.weights = np.log(self.pop_size + 1) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n\n        self.cs = cs\n        self.damps = damps + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1)\n        self.ccov1 = ccov1\n        self.ccovmu = ccovmu\n        self.chiN = self.dim**0.5 * (1 - (1/(4*self.dim)) + 1/(21*self.dim**2))\n        self.p_th = 0.1\n\n        self.C = np.eye(self.dim)\n        self.sigma = self.sigma0\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.B = None\n        self.D = None\n\n        self.ccov1 = 2 / ((self.dim + 1.3)**2 + self.mueff)\n        self.ccovmu = min(1 - self.ccov1, 2 * (self.mueff - 2 + 1/self.mueff) / ((self.dim + 2.0)**2 + self.mueff))\n        self.alpha = 1.5\n        self.iteration = 0\n        self.restart_iter = int(self.budget / (10*self.pop_size))\n\n    def __call__(self, func):\n        mean = np.random.uniform(self.lb, self.ub, size=self.dim)\n        while self.budget > 0:\n            self.iteration += 1\n            if self.iteration % self.restart_iter == 0:\n                mean = np.random.uniform(self.lb, self.ub, size=self.dim)\n                self.C = np.eye(self.dim)\n                self.sigma = self.sigma0\n                self.pc = np.zeros(self.dim)\n                self.ps = np.zeros(self.dim)\n                self.B = None\n                self.D = None\n\n            if self.B is None or self.D is None:\n                self.D, self.B = np.linalg.eigh(self.C)\n                self.D = np.sqrt(self.D)\n\n            z = np.random.randn(self.dim, self.pop_size)\n            x = mean[:, np.newaxis] + self.sigma * (self.B @ (self.D[:, np.newaxis] * z))\n            x = np.clip(x, self.lb, self.ub)\n\n            fitness = np.array([func(xi) for xi in x.T])\n            self.budget -= self.pop_size\n\n            if np.min(fitness) < self.f_opt:\n                self.f_opt = np.min(fitness)\n                self.x_opt = x[:, np.argmin(fitness)]\n\n            idx = np.argsort(fitness)\n            x_mu = x[:, idx[:self.mu]]\n            z_mu = z[:, idx[:self.mu]]\n\n            mean_new = np.sum(self.weights[np.newaxis, :] * x_mu, axis=1)\n            zmean = np.sum(self.weights * z_mu, axis=1)\n\n            ps_temp = self.ps\n            self.ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs) * self.mueff) * (self.B @ zmean)\n            norm_ps = np.linalg.norm(self.ps)\n\n            if norm_ps / np.sqrt(1 - (1 - self.cs)**(2*self.budget/self.pop_size)) < self.alpha * self.chiN:\n                self.C = (1-self.ccov1-self.ccovmu) * self.C + self.ccov1 * np.outer(self.pc, self.pc) \\\n                    + self.ccovmu * (self.weights[None, :] * z_mu) @ z_mu.T\n            else:\n                self.ps = ps_temp\n            \n            self.pc = (1 - 1) * self.pc + np.sqrt(1 * (2 - 1) * self.mueff) * (mean_new - mean) / self.sigma\n            self.C = (1-self.ccov1-self.ccovmu) * self.C + self.ccov1 * np.outer(self.pc, self.pc) \\\n                    + self.ccovmu * (self.weights[None, :] * z_mu) @ z_mu.T\n            mean = mean_new\n            self.sigma *= np.exp((self.cs/self.damps) * (norm_ps/self.chiN -1))\n                \n            self.C = np.triu(self.C) + np.triu(self.C, 1).T\n            self.C = self.C / np.linalg.norm(self.C)\n\n            min_eig = np.min(np.linalg.eigvalsh(self.C))\n            if min_eig < 1e-10:\n                self.C += (1e-10 - min_eig) * np.eye(self.dim)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveCMAES scored 0.624 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.24364301314241044, 0.38384385676105126, 0.6864855555135159, 0.9658875132316728, 0.9256190338345411, 0.7709355062262475, 0.32028494505050475, 0.7236837007345317, 0.8618755487849807, 0.24012524630514487, 0.946053521250809, 0.9974872340272865, 0.2598397541824198, 0.6945450267959943, 0.6361881938266396, 0.6664371475626027, 0.5001246997315035, 0.9228592572394902, 0.20681518950835587, 0.5193502355151418]}, "task_prompt": ""}
{"id": "7f50a4b0-a5a3-4769-a9d8-3fce2599b9e4", "fitness": "-inf", "name": "SimplifiedCMAES", "description": "No description provided.", "code": "import numpy as np\n\nclass SimplifiedCMAES:\n    def __init__(self, budget=10000, dim=10, pop_size=None, lb=-5.0, ub=5.0, cs=0.3, mu_fact=0.2):\n        self.budget = budget\n        self.dim = dim\n        self.lb = lb\n        self.ub = ub\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.pop_size = pop_size if pop_size is not None else 4 + int(3 * np.log(self.dim))\n        self.mean = np.random.uniform(self.lb, self.ub, size=self.dim)\n        self.sigma = 0.5 * (self.ub - self.lb)\n        self.C = np.eye(self.dim)\n        self.cs = cs\n        self.mu = max(1, int(self.pop_size * mu_fact))\n        self.restart_trigger = 100 * self.dim \n        self.restart_count = 0\n\n    def __call__(self, func):\n        while self.budget > 0:\n            # Sample population\n            z = np.random.randn(self.pop_size, self.dim)\n            population = self.mean + self.sigma * z @ np.linalg.cholesky(self.C).T\n            population = np.clip(population, self.lb, self.ub)\n\n            # Evaluate population\n            fitness = np.array([func(x) for x in population])\n            self.budget -= self.pop_size\n            \n            # Sort by fitness\n            indices = np.argsort(fitness)\n            fitness = fitness[indices]\n            population = population[indices]\n\n            # Update best solution\n            if fitness[0] < self.f_opt:\n                self.f_opt = fitness[0]\n                self.x_opt = population[0]\n\n            # Update mean\n            old_mean = self.mean.copy()\n            self.mean = np.mean(population[:self.mu], axis=0)\n\n            # Update covariance matrix\n            diff = (self.mean - old_mean) / self.sigma\n            self.C = (1 - self.cs) * self.C + self.cs * np.outer(diff, diff)\n\n            # Update step size\n            self.sigma *= np.exp(self.cs / 2 * (np.linalg.norm(diff)**2 - 1))\n\n            # Restart if covariance matrix collapses\n            if np.any(np.isnan(self.C)):\n                self.restart_count +=1\n                self.mean = np.random.uniform(self.lb, self.ub, size=self.dim)\n                self.sigma = 0.5 * (self.ub - self.lb)\n                self.C = np.eye(self.dim)\n            \n            if self.restart_count > self.restart_trigger:\n                self.restart_count = 0\n                self.mean = np.random.uniform(self.lb, self.ub, size=self.dim)\n                self.sigma = 0.5 * (self.ub - self.lb)\n                self.C = np.eye(self.dim)\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "In the code, line 148, in _raise_linalgerror_nonposdef, the following error occurred:\nLinAlgError: Matrix is not positive definite", "error": "In the code, line 148, in _raise_linalgerror_nonposdef, the following error occurred:\nLinAlgError: Matrix is not positive definite", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "debb473e-526f-45d8-9316-37ee6240f2fe", "fitness": 0.5645692171714531, "name": "HybridDEPSO", "description": "No description provided.", "code": "import numpy as np\n\nclass HybridDEPSO:\n    def __init__(self, budget=10000, dim=10, pop_size=20, lb=-5.0, ub=5.0):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lb = lb\n        self.ub = ub\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.zeros(self.pop_size)\n        self.velocity = np.zeros_like(self.population)\n        self.personal_best_positions = np.copy(self.population)\n        self.personal_best_fitness = np.full(self.pop_size, np.inf)\n\n        self.inertia_weight = 0.7\n        self.cognitive_coeff = 1.5\n        self.social_coeff = 1.5\n        self.mutation_factor = 0.8\n        self.crossover_rate = 0.7\n        self.inertia_decay = 0.995\n        self.mutation_decay = 0.995\n        self.crossover_decay = 0.995\n\n    def __call__(self, func):\n        # Initialize population and evaluate fitness\n        for i in range(self.pop_size):\n            self.fitness[i] = func(self.population[i])\n            self.budget -= 1\n            if self.fitness[i] < self.personal_best_fitness[i]:\n                self.personal_best_fitness[i] = self.fitness[i]\n                self.personal_best_positions[i] = self.population[i]\n            if self.fitness[i] < self.f_opt:\n                self.f_opt = self.fitness[i]\n                self.x_opt = self.population[i]\n\n        while self.budget > 0:\n            # Find global best\n            global_best_index = np.argmin(self.personal_best_fitness)\n            global_best_position = self.personal_best_positions[global_best_index]\n\n            for i in range(self.pop_size):\n                # Update velocity\n                self.velocity[i] = (self.inertia_weight * self.velocity[i] +\n                                    self.cognitive_coeff * np.random.rand() * (self.personal_best_positions[i] - self.population[i]) +\n                                    self.social_coeff * np.random.rand() * (global_best_position - self.population[i]))\n\n                # Mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n                mutant = a + self.mutation_factor * (b - c)\n                mutant = np.clip(mutant, self.lb, self.ub)\n\n                # Crossover\n                trial = np.copy(self.population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < self.crossover_rate or j == j_rand:\n                        trial[j] = mutant[j]\n\n                # Update position based on velocity and crossover\n                trial = np.clip(trial + self.velocity[i], self.lb, self.ub)\n\n                # Evaluate fitness\n                f_trial = func(trial)\n                self.budget -= 1\n\n                # Selection\n                if f_trial < self.fitness[i]:\n                    self.population[i] = trial\n                    self.fitness[i] = f_trial\n                    if f_trial < self.personal_best_fitness[i]:\n                        self.personal_best_fitness[i] = f_trial\n                        self.personal_best_positions[i] = trial\n                        if f_trial < self.f_opt:\n                            self.f_opt = f_trial\n                            self.x_opt = trial\n\n            # Update parameters\n            self.inertia_weight *= self.inertia_decay\n            self.mutation_factor *= self.mutation_decay\n            self.crossover_rate *= self.crossover_decay\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDEPSO scored 0.565 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.16497547573078553, 0.7189668048483997, 0.7682456476559063, 0.8866429187510128, 0.29228358378198704, 0.8329289732094282, 0.354941703044632, 0.7620286588406642, 0.8149457734222953, 0.20872891966368212, 0.8221097426061128, 0.9967371320552099, 0.31885854921662327, 0.257011844163607, 0.7318024219379897, 0.8428179950568008, 0.4266990117439956, 0.37252650560971146, 0.21251473025166856, 0.5056179518385542]}, "task_prompt": ""}
{"id": "5f006632-af50-4cee-81b3-74fe658e9710", "fitness": 0.6451853621517957, "name": "SelfAdaptiveDE", "description": "No description provided.", "code": "import numpy as np\n\nclass SelfAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size_init=50, F_init=0.5, CR_init=0.7, restart_freq=500):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size_init\n        self.F = F_init  # Mutation factor\n        self.CR = CR_init  # Crossover rate\n        self.lb = -5.0\n        self.ub = 5.0\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.restart_freq = restart_freq\n        self.eval_count = 0\n\n    def __call__(self, func):\n        # Initialize population\n        population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.eval_count += self.pop_size\n        self.budget -= self.pop_size\n\n        # Find initial best\n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index]\n\n        generation = 0\n        while self.budget > 0:\n            generation += 1\n\n            # Adapt population size\n            if generation % self.restart_freq == 0:\n                self.pop_size = int(self.pop_size * 0.9) + 50  # Reduce and add baseline\n                self.pop_size = min(self.pop_size, 200) # Cap population size\n                population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n                fitness = np.array([func(x) for x in population])\n                self.eval_count += self.pop_size\n                self.budget -= self.pop_size\n\n                best_index = np.argmin(fitness)\n                if fitness[best_index] < self.f_opt:\n                    self.f_opt = fitness[best_index]\n                    self.x_opt = population[best_index]\n\n\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n\n                # Adaptive F and CR\n                F_i = np.random.normal(self.F, 0.1)\n                F_i = np.clip(F_i, 0.1, 1.0)\n                CR_i = np.random.normal(self.CR, 0.1)\n                CR_i = np.clip(CR_i, 0.1, 1.0)\n                \n                mutant = a + F_i * (b - c)\n                mutant = np.clip(mutant, self.lb, self.ub)\n\n                # Crossover\n                trial = np.copy(population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < CR_i or j == j_rand:\n                        trial[j] = mutant[j]\n\n                # Evaluation\n                f_trial = func(trial)\n                self.eval_count += 1\n                self.budget -= 1\n\n                # Selection\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n\n                    # Update best\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n                if self.budget <= 0:\n                    break\n\n            best_index = np.argmin(fitness)\n            if fitness[best_index] < self.f_opt:\n                self.f_opt = fitness[best_index]\n                self.x_opt = population[best_index]\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm SelfAdaptiveDE scored 0.645 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.25429847033389996, 0.49647740588882583, 0.6280290522174969, 0.850794101361077, 0.7146690535749588, 0.7444523465231808, 0.598408556068192, 0.6376486036773097, 0.7023881532117671, 0.5504484644643552, 0.812942066930509, 0.9979824036942104, 0.40661032946202735, 0.6849287155213281, 0.8925656976732144, 0.7527707853883104, 0.5539457149674969, 0.8347014547261337, 0.2531455734299747, 0.5365002939216443]}, "task_prompt": ""}
{"id": "2b45a521-c291-47cc-b7f6-bdbe3140a6a7", "fitness": 0.712215002458658, "name": "SelfAdaptiveDE_LocalSearch", "description": "No description provided.", "code": "import numpy as np\n\nclass SelfAdaptiveDE_LocalSearch:\n    def __init__(self, budget=10000, dim=10, pop_size_init=20, F_init=0.5, CR_init=0.9, local_search_prob=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size_init = pop_size_init\n        self.F_init = F_init  # Initial Mutation factor\n        self.CR_init = CR_init  # Initial Crossover rate\n        self.lb = -5.0\n        self.ub = 5.0\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.pop_size = pop_size_init\n        self.F = F_init\n        self.CR = CR_init\n        self.local_search_prob = local_search_prob\n\n    def __call__(self, func):\n        # Initialize population\n        population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        # Find initial best\n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index]\n\n        generation = 0\n        while self.budget > 0:\n            generation += 1\n            # Self-adaptive parameters\n            self.F = np.random.normal(self.F_init, 0.1, self.pop_size)\n            self.F = np.clip(self.F, 0.1, 1.0)\n            self.CR = np.random.normal(self.CR_init, 0.1, self.pop_size)\n            self.CR = np.clip(self.CR, 0.1, 1.0)\n            \n            # Adjust population size (example, can be more sophisticated)\n            if generation % 100 == 0:\n                if np.std(fitness) < 1e-6:\n                    self.pop_size = max(10, int(self.pop_size * 0.8))  # Reduce if converging\n                else:\n                    self.pop_size = min(self.pop_size_init, int(self.pop_size * 1.2)) # Increase exploration\n\n                if self.pop_size != population.shape[0]:\n                    #resize population\n                    if self.pop_size < population.shape[0]:\n                      population = population[:self.pop_size]\n                      fitness = fitness[:self.pop_size]\n                    else:\n                      num_new = self.pop_size-population.shape[0]\n                      new_pop = np.random.uniform(self.lb, self.ub, size=(num_new, self.dim))\n                      population = np.vstack((population, new_pop))\n                      new_fitness = np.array([func(x) for x in new_pop])\n                      fitness = np.concatenate((fitness,new_fitness))\n                      self.budget -= num_new\n        \n\n            for i in range(population.shape[0]):\n                # Mutation\n                idxs = [idx for idx in range(population.shape[0]) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n\n                mutant = a + self.F[i] * (b - c)\n                mutant = np.clip(mutant, self.lb, self.ub)\n\n                # Crossover\n                trial = np.copy(population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR[i] or j == j_rand:\n                        trial[j] = mutant[j]\n\n                # Evaluation\n                f_trial = func(trial)\n                self.budget -= 1\n                if self.budget <= 0:\n                    break\n\n                # Selection\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n\n                    # Update best\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n            # Local search around best solution\n            if np.random.rand() < self.local_search_prob and self.budget > 0:\n                x_local = self.x_opt + np.random.normal(0, 0.01, self.dim)\n                x_local = np.clip(x_local, self.lb, self.ub)\n                f_local = func(x_local)\n                self.budget -= 1\n                if f_local < self.f_opt:\n                    self.f_opt = f_local\n                    self.x_opt = x_local\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm SelfAdaptiveDE_LocalSearch scored 0.712 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.25698742161073396, 0.22336842200688212, 0.5394642502915612, 0.5710188344737825, 0.8831967434868013, 0.804197256891431, 0.8866130247895302, 0.8545206780197641, 0.9048204535633939, 0.4559856245083722, 0.9314404706835478, 0.9989006990818383, 0.4879065202792079, 0.8386938111246607, 0.8425711645262666, 0.9152262001768274, 0.6100866204841501, 0.939871384993269, 0.7914772955445062, 0.5079531726366304]}, "task_prompt": ""}
