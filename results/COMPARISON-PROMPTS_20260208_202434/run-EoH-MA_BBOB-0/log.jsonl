{"id": "50a51fcb-b62e-4284-913d-98b4d7679516", "fitness": 0.38197811623801214, "name": "AdaptiveDE", "description": "No description provided.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = 0.5  # Mutation factor\n        self.CR = 0.7 # Crossover rate\n        self.lb = -5.0\n        self.ub = 5.0\n\n    def __call__(self, func):\n        self.population = np.random.uniform(self.lb, self.ub, (self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n        evals = self.pop_size\n\n        success_F = []\n        success_CR = []\n        success_count = 0\n\n        while evals < self.budget:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n                mutant = self.population[i] + self.F * (b - c)\n                mutant = np.clip(mutant, self.lb, self.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, self.population[i])\n\n                # Selection\n                f = func(trial)\n                evals += 1\n                if f < self.fitness[i]:\n                    self.fitness[i] = f\n                    self.population[i] = trial\n\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = trial\n                    \n                    success_count +=1\n                    success_F.append(self.F)\n                    success_CR.append(self.CR)\n\n            # Adapt F and CR\n            if success_count > 0:\n                self.F = np.mean(success_F) if success_F else 0.5\n                self.CR = np.mean(success_CR) if success_CR else 0.7\n                \n                success_F = []\n                success_CR = []\n                success_count = 0\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveDE scored 0.382 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.16109411841702437, 0.2724793531994564, 0.39259185351103965, 0.3912324715083584, 0.29115389278909254, 0.39693973960062845, 0.2914891443934101, 0.33232695391628453, 0.2855634598166772, 0.19517407480915316, 0.43142741088872394, 0.9937105560140105, 0.2915811402952193, 0.2805466317546851, 0.7254913442551383, 0.4171575810602912, 0.32354623961029527, 0.4924102861709707, 0.18919527026315164, 0.4844508024866313]}, "task_prompt": ""}
{"id": "246e263f-0691-4a4c-8f90-9961f93a5ddc", "fitness": 0.15128825531550208, "name": "AdaptiveDE", "description": "No description provided.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, cr_init=0.5, f_init=0.7):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.cr_init = cr_init\n        self.f_init = f_init\n        self.lb = -5.0\n        self.ub = 5.0\n        self.stagnation_threshold = 200 # Number of iterations without improvement before restart\n        self.stagnation_counter = 0\n\n    def initialize_population(self):\n        return np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.population = self.initialize_population()\n        self.fitness = np.array([func(x) for x in self.population])\n        self.nevals = self.pop_size  # Initial population evaluation\n\n        best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_index]\n        self.x_opt = self.population[best_index]\n        \n        cr = self.cr_init\n        f = self.f_init\n\n        while self.nevals < self.budget:\n            for i in range(self.pop_size):\n                # Mutation\n                indices = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = self.population[indices]\n                x_mutated = self.population[i] + f * (x_r1 - x_r2)\n\n                # Crossover\n                x_trial = np.copy(self.population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < cr or j == j_rand:\n                        x_trial[j] = x_mutated[j]\n\n                # Ensure bounds\n                x_trial = np.clip(x_trial, self.lb, self.ub)\n\n                # Selection\n                f_trial = func(x_trial)\n                self.nevals += 1\n                \n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.population[i] = x_trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = x_trial\n                        self.stagnation_counter = 0 # Reset counter if improvement found\n                else:\n                    self.stagnation_counter += 1\n\n            # Adaptive parameter control (adjust CR and F)\n            cr = np.clip(np.random.normal(self.cr_init, 0.1), 0.0, 1.0)\n            f = np.clip(np.random.normal(self.f_init, 0.1), 0.1, 1.0)\n\n            # Restart strategy\n            if self.stagnation_counter > self.stagnation_threshold:\n                self.population = self.initialize_population() # Re-initialize population\n                self.fitness = np.array([func(x) for x in self.population])\n                self.nevals += self.pop_size\n                \n                best_index = np.argmin(self.fitness)\n                if self.fitness[best_index] < self.f_opt:\n                    self.f_opt = self.fitness[best_index]\n                    self.x_opt = self.population[best_index]\n                \n                self.stagnation_counter = 0  # Reset stagnation counter\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveDE scored 0.151 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.13580372360466952, 0.18608244391251916, 0.28326685374481964, 0]}, "task_prompt": ""}
{"id": "f5881e49-3ba2-4eef-8039-d9daeba8d64e", "fitness": "-inf", "name": "SimplexRestart", "description": "No description provided.", "code": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass SimplexRestart:\n    def __init__(self, budget=10000, dim=10, num_restarts=5, simplex_size=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.num_restarts = num_restarts\n        self.simplex_size = simplex_size\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        eval_count = 0\n\n        for i in range(self.num_restarts):\n            if eval_count >= self.budget:\n                break\n\n            # Initial guess: random point within bounds\n            x0 = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n\n            # Define a callback to track function evaluations\n            def callback(xk):\n                nonlocal eval_count\n                eval_count += 1\n                return eval_count >= self.budget\n\n            # Run Nelder-Mead simplex algorithm\n            res = minimize(func, x0, method='Nelder-Mead',\n                           options={'maxiter': (self.budget - eval_count) // self.num_restarts if self.num_restarts > 0 else self.budget - eval_count,\n                                    'maxfev': self.budget - eval_count, 'xatol': 1e-8, 'fatol': 1e-8},\n                           callback=callback)\n            \n            if res.fun < self.f_opt:\n                self.f_opt = res.fun\n                self.x_opt = res.x\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "In the code, line 30, in __call__, the following error occurred:\nNameError: name 'minimize' is not defined\nOn line: res = minimize(func, x0, method='Nelder-Mead',", "error": "In the code, line 30, in __call__, the following error occurred:\nNameError: name 'minimize' is not defined\nOn line: res = minimize(func, x0, method='Nelder-Mead',", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "ad89d770-c97c-467a-8583-b5c59aa92fea", "fitness": 0.0, "name": "AdaptiveSimulatedAnnealing", "description": "No description provided.", "code": "import numpy as np\n\nclass AdaptiveSimulatedAnnealing:\n    def __init__(self, budget=10000, dim=10, initial_temp=1.0, cooling_rate=0.95, perturbation_size=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.initial_temp = initial_temp\n        self.cooling_rate = cooling_rate\n        self.perturbation_size = perturbation_size\n        self.lb = -5.0\n        self.ub = 5.0\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        x = np.random.uniform(self.lb, self.ub, size=self.dim)\n        f = func(x)\n        self.f_opt = f\n        self.x_opt = x\n        temp = self.initial_temp\n        eval_count = 1\n\n        while eval_count < self.budget:\n            x_new = x + np.random.normal(0, self.perturbation_size, size=self.dim)\n            x_new = np.clip(x_new, self.lb, self.ub)\n            f_new = func(x_new)\n            eval_count += 1\n\n            delta_f = f_new - f\n            if delta_f < 0:\n                x = x_new\n                f = f_new\n                if f < self.f_opt:\n                    self.f_opt = f\n                    self.x_opt = x\n            else:\n                acceptance_probability = np.exp(-delta_f / temp)\n                if np.random.rand() < acceptance_probability:\n                    x = x_new\n                    f = f_new\n            \n            # Adaptive temperature and perturbation size\n            if eval_count % (self.dim * 5) == 0:\n                temp *= self.cooling_rate\n                self.perturbation_size *= self.cooling_rate\n\n            # Local search to escape local optima (gradient estimation)\n            if eval_count % (self.dim * 10) == 0:\n                gradient = np.zeros(self.dim)\n                for i in range(self.dim):\n                    x_plus = x.copy()\n                    x_minus = x.copy()\n                    delta = 0.001\n                    x_plus[i] += delta\n                    x_minus[i] -= delta\n                    x_plus = np.clip(x_plus, self.lb, self.ub)\n                    x_minus = np.clip(x_minus, self.lb, self.ub)\n                    f_plus = func(x_plus)\n                    eval_count += 1\n                    if eval_count >= self.budget:\n                        break\n                    f_minus = func(x_minus)\n                    eval_count += 1\n                    if eval_count >= self.budget:\n                        break\n\n                    gradient[i] = (f_plus - f_minus) / (2 * delta)\n                \n                if eval_count >= self.budget:\n                    break\n                \n                x_local = x - 0.01 * gradient # Small step against the gradient\n                x_local = np.clip(x_local, self.lb, self.ub)\n                f_local = func(x_local)\n                eval_count += 1\n\n                if f_local < f:\n                    x = x_local\n                    f = f_local\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = x\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveSimulatedAnnealing scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0]}, "task_prompt": ""}
{"id": "8d5ba875-20f9-4eb3-83d6-aa4c77d2d5e4", "fitness": "-inf", "name": "AdaptiveLocalSearch", "description": "No description provided.", "code": "import numpy as np\n\nclass AdaptiveLocalSearch:\n    def __init__(self, budget=10000, dim=10, initial_step_size=1.0, local_search_restarts=5):\n        self.budget = budget\n        self.dim = dim\n        self.step_size = initial_step_size\n        self.lb = -5.0\n        self.ub = 5.0\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.local_search_restarts = local_search_restarts\n\n    def local_search(self, func, x_center, num_evals):\n        f_best_local = np.inf\n        x_best_local = None\n        for _ in range(num_evals):\n            x = x_center + self.step_size * np.random.normal(0, 1, self.dim)\n            x = np.clip(x, self.lb, self.ub)\n\n            f = func(x)\n            self.eval_count += 1\n\n            if f < f_best_local:\n                f_best_local = f\n                x_best_local = x\n\n            if f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = x\n        \n        return f_best_local, x_best_local\n\n    def __call__(self, func):\n        # Initialization\n        x = np.random.uniform(self.lb, self.ub, self.dim)\n        f = func(x)\n        self.eval_count += 1\n        self.f_opt = f\n        self.x_opt = x\n\n        while self.eval_count < self.budget:\n            # Adaptive Step Size (simplified CMA-ES)\n            success_rate = 0.0\n            num_successes = 0\n\n            for _ in range(min(100, self.budget - self.eval_count)):\n              x_new = self.x_opt + self.step_size * np.random.normal(0, 1, self.dim)\n              x_new = np.clip(x_new, self.lb, self.ub)\n              f_new = func(x_new)\n              self.eval_count += 1\n              if f_new < self.f_opt:\n                self.f_opt = f_new\n                self.x_opt = x_new\n                num_successes +=1\n\n            success_rate = num_successes / min(100, self.budget - self.eval_count)\n            if success_rate > 0.2:\n                self.step_size *= 1.2\n            elif success_rate < 0.1:\n                self.step_size *= 0.8\n            self.step_size = min(self.step_size, (self.ub - self.lb) / 2) # prevent step size from becoming too large\n\n            # Local Search around best-so-far\n            num_local_evals = (self.budget - self.eval_count) // self.local_search_restarts if self.local_search_restarts > 0 else 0\n            \n            for _ in range(self.local_search_restarts):\n                if self.eval_count >= self.budget:\n                  break\n\n                f_local, x_local = self.local_search(func, self.x_opt, num_local_evals)\n\n                if f_local < self.f_opt:\n                    self.f_opt = f_local\n                    self.x_opt = x_local\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "In the code, line 58, in __call__, the following error occurred:\nZeroDivisionError: division by zero\nOn line: success_rate = num_successes / min(100, self.budget - self.eval_count)", "error": "In the code, line 58, in __call__, the following error occurred:\nZeroDivisionError: division by zero\nOn line: success_rate = num_successes / min(100, self.budget - self.eval_count)", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "cdb79e2a-33a1-4e44-bae8-7bfbbd495876", "fitness": 0.32109689864390123, "name": "HybridPSO_DE", "description": "No description provided.", "code": "import numpy as np\n\nclass HybridPSO_DE:\n    def __init__(self, budget=10000, dim=10, pop_size=20, w=0.7, c1=1.5, c2=1.5, cr=0.7, f=0.8):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.w = w\n        self.c1 = c1\n        self.c2 = c2\n        self.cr = cr\n        self.f = f\n        self.lb = -5.0\n        self.ub = 5.0\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        \n        # Initialization\n        population = np.random.uniform(self.lb, self.ub, (self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n        \n        personal_best_positions = population.copy()\n        personal_best_fitness = fitness.copy()\n        \n        global_best_index = np.argmin(fitness)\n        global_best_position = population[global_best_index].copy()\n        global_best_fitness = fitness[global_best_index]\n\n        velocity = np.zeros_like(population)\n        \n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # PSO update\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                velocity[i] = self.w * velocity[i] + self.c1 * r1 * (personal_best_positions[i] - population[i]) + self.c2 * r2 * (global_best_position - population[i])\n                \n                # DE mutation and crossover\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = np.random.choice(idxs, 3, replace=False)\n                \n                mutant = population[a] + self.f * (population[b] - population[c])\n                \n                trial = population[i].copy()\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < self.cr or j == j_rand:\n                        trial[j] = mutant[j]\n                \n                # Combined update\n                new_position = population[i] + velocity[i]\n                \n                # Boundary handling\n                new_position = np.clip(new_position, self.lb, self.ub)\n\n                f = func(new_position)\n                self.budget -= 1\n\n                if f < fitness[i]:\n                    population[i] = new_position\n                    fitness[i] = f\n                    \n                    if f < personal_best_fitness[i]:\n                        personal_best_fitness[i] = f\n                        personal_best_positions[i] = new_position.copy()\n                        \n                        if f < global_best_fitness:\n                            global_best_fitness = f\n                            global_best_position = new_position.copy()\n                            \n\n                if self.budget <= 0:\n                    break\n                    \n        self.f_opt = global_best_fitness\n        self.x_opt = global_best_position\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm HybridPSO_DE scored 0.321 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.11639573485028265, 0.1627228036187215, 0.43366058682428377, 0.20539315746707876, 0.24799615297124322, 0.27330917637917174, 0.2672509798784246, 0.19506638513355212, 0.22342342932538617, 0.19181298244059997, 0.2548477359075303, 0.9972289124135219, 0.21818127094212914, 0.2658214122087956, 0.5686452652490186, 0.6858016623132881, 0.20962763156038755, 0.2906820142460165, 0.16810306331477864, 0.4459676158338137]}, "task_prompt": ""}
{"id": "b098d4da-5110-4218-8d2f-48961abb944c", "fitness": 0.6989042777030108, "name": "AdaptiveDE", "description": "No description provided.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, CR=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F  # Mutation factor\n        self.CR = CR  # Crossover rate\n        self.lb = -5.0\n        self.ub = 5.0\n        self.pop = None\n        self.fitness = None\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.eval_count = 0\n\n    def initialize_population(self, func):\n        self.pop = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.eval_count += self.pop_size\n        best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_index]\n        self.x_opt = self.pop[best_index]\n\n    def repair(self, x):\n        return np.clip(x, self.lb, self.ub)\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.eval_count < self.budget:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = self.pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = self.repair(a + self.F * (b - c))\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, self.pop[i])\n\n                # Selection\n                f = func(trial)\n                self.eval_count += 1\n                if f < self.fitness[i]:\n                    self.fitness[i] = f\n                    self.pop[i] = trial\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = trial\n                \n                # Adaptive F and CR (optional, but can improve performance)\n                self.F = np.random.normal(0.5, 0.1)\n                self.F = np.clip(self.F, 0.1, 1.0)\n                self.CR = np.random.normal(0.9, 0.1)\n                self.CR = np.clip(self.CR, 0.1, 1.0)\n                \n                if self.eval_count >= self.budget:\n                  break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveDE scored 0.699 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.3088847201429462, 0.6540319096769922, 0.6824743048072819, 0.8595780011791465, 0.7211604384766859, 0.8029553877830078, 0.6690350200509281, 0.647401564370916, 0.7337449650075414, 0.717876887099301, 0.8551074981751433, 0.9960463030998306, 0.6523989796460898, 0.7020270006060587, 0.9150854784450185, 0.7848046313932722, 0.636523775526348, 0.8396717335012398, 0.278971368023044, 0.5203055870494226]}, "task_prompt": ""}
{"id": "022e2e7c-c2af-4a60-ad16-b6703c11d144", "fitness": 0.6292278102651216, "name": "AdaptiveDifferentialEvolution", "description": "No description provided.", "code": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, CR=0.7):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        \n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n        \n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index]\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = a + self.F * (b - c)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                f = func(trial)\n                self.budget -= 1\n                if f < fitness[i]:\n                    fitness[i] = f\n                    population[i] = trial\n\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = trial\n            \n            best_index = np.argmin(fitness)\n            if fitness[best_index] < self.f_opt:\n                self.f_opt = fitness[best_index]\n                self.x_opt = population[best_index]    \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveDifferentialEvolution scored 0.629 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.22095572455761825, 0.4553180970312074, 0.6069552113950909, 0.8353732275051421, 0.6897410410964697, 0.7574853788594296, 0.5226305343653828, 0.6245408493100431, 0.6948750865267865, 0.5741155632499193, 0.7706487321915927, 0.9962358870906335, 0.35383755599775835, 0.6795792191248903, 0.8986418406396504, 0.7766386870197046, 0.5096684242318752, 0.814204104089404, 0.2788725359770998, 0.5242385050427331]}, "task_prompt": ""}
{"id": "901cec52-b1d2-41a9-92c7-bcc44e783041", "fitness": "-inf", "name": "GaussianProcessOptimization", "description": "No description provided.", "code": "import numpy as np\nfrom sklearn.gaussian_process import GaussianProcessRegressor\nfrom sklearn.gaussian_process.kernels import RBF, ConstantKernel as C\nfrom scipy.stats import norm\nfrom scipy.optimize import minimize\n\nclass GaussianProcessOptimization:\n    def __init__(self, budget=10000, dim=10, n_initial=10):\n        self.budget = budget\n        self.dim = dim\n        self.n_initial = n_initial\n        self.lb = -5.0\n        self.ub = 5.0\n\n    def __call__(self, func):\n        # Initialize data\n        X = np.random.uniform(self.lb, self.ub, size=(self.n_initial, self.dim))\n        y = np.array([func(x) for x in X])\n        self.budget -= self.n_initial\n\n        # Gaussian process model\n        kernel = C(1.0, (1e-3, 1e3)) * RBF(1.0, (1e-3, 1e3))\n        gp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=10)\n\n        # Optimization loop\n        while self.budget > 0:\n            # Fit Gaussian process to data\n            gp.fit(X, y)\n\n            # Define acquisition function (Upper Confidence Bound)\n            def acquisition(x, xi=0.1):\n                x = x.reshape(1, -1)\n                mu, sigma = gp.predict(x, return_std=True)\n                return -mu[0] + xi * sigma[0]\n\n            # Optimize acquisition function\n            x0 = np.random.uniform(self.lb, self.ub, size=(1, self.dim))\n            bounds = [(self.lb, self.ub)] * self.dim\n            res = minimize(acquisition, x0, method='L-BFGS-B', bounds=bounds)\n            x_new = res.x\n\n            # Evaluate objective function\n            f_new = func(x_new)\n            self.budget -= 1\n\n            # Add new data to training set\n            X = np.vstack((X, x_new))\n            y = np.append(y, f_new)\n\n        # Best observed value\n        self.f_opt = np.min(y)\n        self.x_opt = X[np.argmin(y)]\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "In the code, line 22, in __call__, the following error occurred:\nNameError: name 'C' is not defined\nOn line: kernel = C(1.0, (1e-3, 1e3)) * RBF(1.0, (1e-3, 1e3))", "error": "In the code, line 22, in __call__, the following error occurred:\nNameError: name 'C' is not defined\nOn line: kernel = C(1.0, (1e-3, 1e3)) * RBF(1.0, (1e-3, 1e3))", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "661991b7-961a-4f57-9d78-170542e3afc8", "fitness": 0.2907444441734654, "name": "AdaptiveSimulatedAnnealing", "description": "No description provided.", "code": "import numpy as np\n\nclass AdaptiveSimulatedAnnealing:\n    def __init__(self, budget=10000, dim=10, initial_temp=1.0, cooling_rate=0.99, temp_min=1e-5):\n        self.budget = budget\n        self.dim = dim\n        self.initial_temp = initial_temp\n        self.cooling_rate = cooling_rate\n        self.temp_min = temp_min\n        self.lb = -5.0\n        self.ub = 5.0\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.eval_count = 0\n\n    def __call__(self, func):\n        x = np.random.uniform(self.lb, self.ub, size=self.dim)\n        f = func(x)\n        self.eval_count += 1\n        self.f_opt = f\n        self.x_opt = x\n        \n        temp = self.initial_temp\n\n        while self.eval_count < self.budget and temp > self.temp_min:\n            # Adaptive perturbation based on fitness landscape\n            std_dev = 0.1 * (self.ub - self.lb) #Base std dev\n            if self.f_opt != np.inf:\n                std_dev = min(std_dev, abs(self.f_opt)) #Scale down the std dev if the f_opt is small\n            x_new = x + np.random.normal(0, std_dev, size=self.dim)\n            x_new = np.clip(x_new, self.lb, self.ub)\n            \n            f_new = func(x_new)\n            self.eval_count += 1\n\n            delta_f = f_new - f\n\n            if delta_f < 0:\n                x = x_new\n                f = f_new\n                if f < self.f_opt:\n                    self.f_opt = f\n                    self.x_opt = x\n            else:\n                acceptance_probability = np.exp(-delta_f / temp)\n                if np.random.rand() < acceptance_probability:\n                    x = x_new\n                    f = f_new\n                    \n            #Adaptive temperature schedule based on the number of accepted solutions\n            if delta_f < 0:\n                temp *= self.cooling_rate #Cool down the temperature as usual\n            else: \n                temp *= (1 + 0.01) #Warm up the temperature slightly if a worse solution is accepted\n\n            if self.eval_count >= self.budget:\n                break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveSimulatedAnnealing scored 0.291 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.1522355249736025, 0.1721617928475675, 0.2606243985109199, 0.23389530220702026, 0.23742275003213453, 0.3042125556201345, 0.25429502753989797, 0.24177103968635183, 0.2257532768664834, 0.1642324807529647, 0.21631577096795496, 0.9976824862761133, 0.2598045905082812, 0.2482414253168569, 0.531494173424444, 0.2724932490103418, 0.22807460202782492, 0.2529259109895238, 0.15912061788161846, 0.40213190802927146]}, "task_prompt": ""}
{"id": "0554e32b-eb88-4f1a-94b7-8901457796cd", "fitness": "-inf", "name": "GaussianProcessUCB", "description": "No description provided.", "code": "import numpy as np\nfrom sklearn.gaussian_process import GaussianProcessRegressor\nfrom sklearn.gaussian_process.kernels import RBF, ConstantKernel as C\n\nclass GaussianProcessUCB:\n    def __init__(self, budget=10000, dim=10, n_initial_samples=10):\n        self.budget = budget\n        self.dim = dim\n        self.n_initial_samples = n_initial_samples\n        self.lb = -5.0\n        self.ub = 5.0\n        self.kernel = C(1.0, (1e-3, 1e3)) * RBF(1.0, (1e-3, 1e3))\n        self.gp = GaussianProcessRegressor(kernel=self.kernel, n_restarts_optimizer=10, alpha=1e-7)\n        self.X = None\n        self.y = None\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.beta = 2.0 #Exploration-exploitation trade-off parameter\n\n    def acquisition_function(self, x):\n        \"\"\"Upper Confidence Bound acquisition function.\"\"\"\n        mu, sigma = self.gp.predict(x.reshape(1, -1), return_std=True)\n        return mu - self.beta * sigma\n\n    def __call__(self, func):\n        # Initial sampling\n        X_init = np.random.uniform(self.lb, self.ub, size=(self.n_initial_samples, self.dim))\n        y_init = np.array([func(x) for x in X_init])\n        self.budget -= self.n_initial_samples\n        \n        self.X = X_init\n        self.y = y_init\n        \n        best_index = np.argmin(self.y)\n        self.f_opt = self.y[best_index]\n        self.x_opt = self.X[best_index]\n\n        self.gp.fit(self.X, self.y)\n\n        while self.budget > 0:\n            # Find the next point to evaluate by maximizing the acquisition function\n            x_next = self.find_next_point()\n\n            # Evaluate the objective function\n            f_next = func(x_next)\n            self.budget -= 1\n\n            # Update the data\n            self.X = np.vstack((self.X, x_next))\n            self.y = np.append(self.y, f_next)\n\n            # Update the best solution\n            if f_next < self.f_opt:\n                self.f_opt = f_next\n                self.x_opt = x_next\n\n            # Update the Gaussian process model\n            self.gp.fit(self.X, self.y)\n\n        return self.f_opt, self.x_opt\n\n    def find_next_point(self):\n        \"\"\"Find the next point to evaluate by maximizing the acquisition function.\"\"\"\n        # Simple grid search for demonstration, can be replaced with more sophisticated optimization\n        x_candidates = np.random.uniform(self.lb, self.ub, size=(100, self.dim))\n        acq_values = np.array([self.acquisition_function(x) for x in x_candidates])\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "In the code, line 12, in __init__, the following error occurred:\nNameError: name 'C' is not defined\nOn line: self.kernel = C(1.0, (1e-3, 1e3)) * RBF(1.0, (1e-3, 1e3))", "error": "In the code, line 12, in __init__, the following error occurred:\nNameError: name 'C' is not defined\nOn line: self.kernel = C(1.0, (1e-3, 1e3)) * RBF(1.0, (1e-3, 1e3))", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "3be6a7c6-cedd-497b-9e77-13966fc552af", "fitness": 0.2744300562463562, "name": "AdaptiveSimulatedAnnealing", "description": "No description provided.", "code": "import numpy as np\n\nclass AdaptiveSimulatedAnnealing:\n    def __init__(self, budget=10000, dim=10, initial_temp=100.0, cooling_rate=0.95, min_temp=1e-5):\n        self.budget = budget\n        self.dim = dim\n        self.initial_temp = initial_temp\n        self.cooling_rate = cooling_rate\n        self.min_temp = min_temp\n        self.lb = -5.0\n        self.ub = 5.0\n        self.x_opt = None\n        self.f_opt = np.inf\n        self.eval_count = 0\n        self.current_x = None\n        self.current_f = None\n        self.temp = initial_temp\n        self.acceptance_rate = 0.0\n        self.acceptance_history = []\n\n    def initialize(self, func):\n        self.current_x = np.random.uniform(self.lb, self.ub, size=self.dim)\n        self.current_f = func(self.current_x)\n        self.eval_count += 1\n        self.x_opt = self.current_x\n        self.f_opt = self.current_f\n\n    def __call__(self, func):\n        self.initialize(func)\n\n        while self.eval_count < self.budget:\n            # Generate a new solution by adding a small random displacement\n            new_x = self.current_x + np.random.normal(0, 0.1, size=self.dim)\n            new_x = np.clip(new_x, self.lb, self.ub)  # Keep within bounds\n\n            new_f = func(new_x)\n            self.eval_count += 1\n\n            # Calculate the change in energy\n            delta_f = new_f - self.current_f\n\n            # Acceptance probability\n            if delta_f < 0 or np.random.rand() < np.exp(-delta_f / self.temp):\n                self.current_x = new_x\n                self.current_f = new_f\n                self.acceptance_rate += 1\n\n                if new_f < self.f_opt:\n                    self.f_opt = new_f\n                    self.x_opt = new_x\n\n            # Adaptive Temperature Schedule\n            self.acceptance_history.append(self.acceptance_rate)\n            if len(self.acceptance_history) > 100:\n                self.acceptance_history.pop(0)\n            \n            # Adjust cooling rate dynamically based on acceptance rate\n            if len(self.acceptance_history) == 100:\n                avg_acceptance_rate = sum(self.acceptance_history) / 100\n                if avg_acceptance_rate > 50:\n                    self.cooling_rate = min(0.99, self.cooling_rate + 0.001)\n                else:\n                    self.cooling_rate = max(0.8, self.cooling_rate - 0.001)\n                self.acceptance_rate = 0.0 \n                \n            # Cool the temperature\n            self.temp *= self.cooling_rate\n            self.temp = max(self.temp, self.min_temp)\n            \n            if self.eval_count >= self.budget:\n                break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveSimulatedAnnealing scored 0.274 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.07447276350249066, 0.17022670063719525, 0.45199051165371296, 0.9512877394482538, 0.1779047791209123, 0.11287518786941131, 0.22312504209334483, 0.136007465875974, 0.20834467574690252, 0.16591057830545564, 0.8972273153449991, 0.16839905893361318, 0.21781783095059326, 0.19439660638123768, 0.1750117833772863, 0.3208479651861518, 0.3676652357068507, 0.15348289389542025, 0.1582474191835297, 0.16335957171378812]}, "task_prompt": ""}
{"id": "010915ee-ff67-4a78-8292-67982d412abe", "fitness": 0.3110373222318176, "name": "SimulatedAnnealing", "description": "No description provided.", "code": "import numpy as np\n\nclass SimulatedAnnealing:\n    def __init__(self, budget=10000, dim=10, temp_init=1.0, cooling_rate=0.99):\n        self.budget = budget\n        self.dim = dim\n        self.temp_init = temp_init\n        self.cooling_rate = cooling_rate\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        \n        # Initialize solution\n        x = np.random.uniform(func.bounds.lb, func.bounds.ub, self.dim)\n        f = func(x)\n        self.budget -= 1\n        \n        self.f_opt = f\n        self.x_opt = x\n\n        temp = self.temp_init\n        \n        while self.budget > 0:\n            # Generate neighbor\n            x_new = x + np.random.normal(0, temp, self.dim)\n            x_new = np.clip(x_new, func.bounds.lb, func.bounds.ub)\n            \n            f_new = func(x_new)\n            self.budget -= 1\n\n            # Metropolis acceptance criterion\n            if f_new < f or np.random.rand() < np.exp((f - f_new) / temp):\n                x = x_new\n                f = f_new\n\n                if f < self.f_opt:\n                    self.f_opt = f\n                    self.x_opt = x\n\n            # Cooling\n            temp *= self.cooling_rate\n            \n            if self.budget <= 0:\n                break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm SimulatedAnnealing scored 0.311 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.15558759358850804, 0.15900480207740053, 0.23340046925525193, 0.47276134268129166, 0.2592087343262478, 0.19691204842504328, 0.25034370917043036, 0.26367948750420545, 0.3722532301814203, 0.19329167879945897, 0.21787067829317053, 0.9941010597993457, 0.2003608286127021, 0.21380334821406588, 0.5728799072661024, 0.2300103162897773, 0.251503334005195, 0.3431630427497263, 0.1776318508196173, 0.4629789825773931]}, "task_prompt": ""}
{"id": "80e17ffc-b96d-4aaa-8ce9-7d5c67bf06cd", "fitness": 0.5011363725296846, "name": "AdaptivePSO", "description": "No description provided.", "code": "import numpy as np\n\nclass AdaptivePSO:\n    def __init__(self, budget=10000, dim=10, pop_size=50, w_max=0.9, w_min=0.4, c1=2, c2=2):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.w_max = w_max\n        self.w_min = w_min\n        self.c1 = c1\n        self.c2 = c2\n        self.lb = -5.0\n        self.ub = 5.0\n        self.v_max = 0.2 * (self.ub - self.lb)\n\n    def __call__(self, func):\n        # Initialize population and velocities\n        population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        velocities = np.random.uniform(-self.v_max, self.v_max, size=(self.pop_size, self.dim))\n        \n        # Evaluate initial population\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n        \n        # Initialize personal best positions and fitnesses\n        pbest_positions = population.copy()\n        pbest_fitness = fitness.copy()\n        \n        # Initialize global best position and fitness\n        global_best_index = np.argmin(fitness)\n        self.f_opt = fitness[global_best_index]\n        self.x_opt = population[global_best_index]\n\n        iteration = 0\n        while self.budget > 0:\n            # Adaptive inertia weight\n            w = self.w_max - (self.w_max - self.w_min) * (iteration / (self.budget / self.pop_size))\n            \n            for i in range(self.pop_size):\n                # Update velocity\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                \n                velocities[i] = w * velocities[i] + \\\n                                self.c1 * r1 * (pbest_positions[i] - population[i]) + \\\n                                self.c2 * r2 * (self.x_opt - population[i])\n                \n                # Velocity clamping\n                velocities[i] = np.clip(velocities[i], -self.v_max, self.v_max)\n                \n                # Update position\n                population[i] = population[i] + velocities[i]\n                \n                # Boundary handling\n                population[i] = np.clip(population[i], self.lb, self.ub)\n                \n                # Evaluate new position\n                f = func(population[i])\n                self.budget -= 1\n                \n                # Update personal best\n                if f < pbest_fitness[i]:\n                    pbest_fitness[i] = f\n                    pbest_positions[i] = population[i].copy()\n                \n                # Update global best\n                if f < self.f_opt:\n                    self.f_opt = f\n                    self.x_opt = population[i].copy()\n            \n            iteration += 1\n            \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptivePSO scored 0.501 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.21647082765882308, 0.5212954478865751, 0.5825859070978912, 0.7917107315588962, 0.25899050487839537, 0.6718323843525095, 0.3177904586674045, 0.5277414282809135, 0.6529330053654789, 0.214815019473643, 0.72963374264961, 0.9994870407836558, 0.32626443976941144, 0.2774683672449054, 0.7087440868705048, 0.673370073874356, 0.46587574309119884, 0.3769287566579109, 0.19892684830059137, 0.5098626361310185]}, "task_prompt": ""}
{"id": "04cfa9f5-7738-47ba-a314-75cbf88cfdca", "fitness": 0.0, "name": "CMAES_LocalSearch", "description": "No description provided.", "code": "import numpy as np\n\nclass CMAES_LocalSearch:\n    def __init__(self, budget=10000, dim=10, pop_size=None, sigma0=0.5, local_search_iterations=5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size if pop_size is not None else 4 + int(3 * np.log(dim))\n        self.sigma0 = sigma0\n        self.lb = -5.0\n        self.ub = 5.0\n        self.local_search_iterations = local_search_iterations\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        \n        # Initialization\n        mean = np.random.uniform(self.lb, self.ub, self.dim)\n        sigma = self.sigma0\n        C = np.eye(self.dim)  # Covariance matrix\n        \n        eval_count = 0\n        \n        while eval_count < self.budget:\n            # Sample population\n            z = np.random.multivariate_normal(np.zeros(self.dim), C, self.pop_size)\n            population = mean + sigma * z\n            \n            # Boundary handling\n            population = np.clip(population, self.lb, self.ub)\n            \n            # Evaluate population\n            fitness = np.array([func(x) for x in population])\n            eval_count += self.pop_size\n            \n            # Sort population\n            indices = np.argsort(fitness)\n            population = population[indices]\n            fitness = fitness[indices]\n            \n            # Update best solution\n            if fitness[0] < self.f_opt:\n                self.f_opt = fitness[0]\n                self.x_opt = population[0]\n                \n            # Update mean\n            mean = np.mean(population[:self.pop_size // 2], axis=0)\n\n            # Simplified CMA-ES update (no rank-mu update for simplicity)\n            C = np.cov(population.T)\n            \n            # Update step size\n            sigma *= np.exp(0.5/self.dim * (np.mean(fitness) - self.f_opt)/np.abs(self.f_opt+1e-8))\n\n            # Local search around the best solution\n            x_local = self.x_opt.copy()\n            f_local = self.f_opt\n            \n            for _ in range(self.local_search_iterations):\n                # Estimate gradient (simplified finite differences)\n                gradient = np.zeros(self.dim)\n                delta = 1e-3\n                for j in range(self.dim):\n                    x_plus = x_local.copy()\n                    x_plus[j] += delta\n                    x_plus = np.clip(x_plus, self.lb, self.ub)\n                    f_plus = func(x_plus)\n                    eval_count += 1\n                    \n                    x_minus = x_local.copy()\n                    x_minus[j] -= delta\n                    x_minus = np.clip(x_minus, self.lb, self.ub)\n                    f_minus = func(x_minus)\n                    eval_count += 1\n\n                    gradient[j] = (f_plus - f_minus) / (2 * delta)\n                    if eval_count >= self.budget:\n                        break\n\n                if eval_count >= self.budget:\n                    break\n\n                # Gradient descent step\n                step_size = 0.01\n                x_new = x_local - step_size * gradient\n                x_new = np.clip(x_new, self.lb, self.ub)\n                f_new = func(x_new)\n                eval_count += 1\n                \n                if f_new < f_local:\n                    f_local = f_new\n                    x_local = x_new.copy()\n                    \n                    if f_local < self.f_opt:\n                        self.f_opt = f_local\n                        self.x_opt = x_local.copy()\n                \n                if eval_count >= self.budget:\n                    break\n\n\n            if eval_count >= self.budget:\n                break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm CMAES_LocalSearch scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0]}, "task_prompt": ""}
{"id": "2882d662-a050-4d4f-b589-db9e90c6c014", "fitness": "-inf", "name": "SimplexRestart", "description": "No description provided.", "code": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass SimplexRestart:\n    def __init__(self, budget=10000, dim=10, simplex_size=1.0, restart_prob=0.05):\n        self.budget = budget\n        self.dim = dim\n        self.simplex_size = simplex_size\n        self.restart_prob = restart_prob\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        evals = 0\n\n        while evals < self.budget:\n            # Initialize simplex\n            initial_simplex = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.dim + 1, self.dim))\n            \n            # Nelder-Mead optimization\n            res = minimize(func, initial_simplex[0], method='Nelder-Mead',\n                           options={'maxfev': self.budget - evals, 'initial_simplex': initial_simplex})\n            \n            evals += res.nfev\n            \n            if res.fun < self.f_opt:\n                self.f_opt = res.fun\n                self.x_opt = res.x\n\n            # Probabilistic restart\n            if np.random.rand() < self.restart_prob:\n                continue\n            else:\n                break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "In the code, line 21, in __call__, the following error occurred:\nNameError: name 'minimize' is not defined\nOn line: res = minimize(func, initial_simplex[0], method='Nelder-Mead',", "error": "In the code, line 21, in __call__, the following error occurred:\nNameError: name 'minimize' is not defined\nOn line: res = minimize(func, initial_simplex[0], method='Nelder-Mead',", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "586d8852-cc81-4725-9913-275814f8acbf", "fitness": 0.0, "name": "ModifiedPSO", "description": "No description provided.", "code": "import numpy as np\n\nclass ModifiedPSO:\n    def __init__(self, budget=10000, dim=10, pop_size=50, w_max=0.9, w_min=0.4, c1=2, c2=2, mutation_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.w_max = w_max\n        self.w_min = w_min\n        self.c1 = c1\n        self.c2 = c2\n        self.mutation_rate = mutation_rate\n        self.lb = -5.0\n        self.ub = 5.0\n        self.v_max = 0.2 * (self.ub - self.lb)\n\n    def __call__(self, func):\n        # Initialize population and velocities\n        population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        velocities = np.random.uniform(-self.v_max, self.v_max, size=(self.pop_size, self.dim))\n        \n        # Evaluate initial population\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n        \n        # Initialize personal best positions and fitnesses\n        pbest_positions = population.copy()\n        pbest_fitness = fitness.copy()\n        \n        # Initialize global best position and fitness\n        global_best_index = np.argmin(fitness)\n        self.f_opt = fitness[global_best_index]\n        self.x_opt = population[global_best_index]\n\n        iteration = 0\n        while self.budget > 0:\n            # Linearly decreasing inertia weight\n            w = self.w_max - (self.w_max - self.w_min) * (iteration / (self.budget / self.pop_size))\n            \n            for i in range(self.pop_size):\n                # Update velocity\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                \n                velocities[i] = w * velocities[i] + \\\n                                self.c1 * r1 * (pbest_positions[i] - population[i]) + \\\n                                self.c2 * r2 * (self.x_opt - population[i])\n                \n                # Velocity clamping\n                velocities[i] = np.clip(velocities[i], -self.v_max, self.v_max)\n                \n                # Update position\n                population[i] = population[i] + velocities[i]\n                \n                # Boundary handling\n                population[i] = np.clip(population[i], self.lb, self.ub)\n\n                # Mutation\n                if np.random.rand() < self.mutation_rate:\n                    mutation_indices = np.random.choice(self.dim, size=int(self.dim * self.mutation_rate), replace=False)\n                    population[i, mutation_indices] = np.random.uniform(self.lb, self.ub, size=len(mutation_indices))\n                \n                # Evaluate new position\n                f = func(population[i])\n                self.budget -= 1\n                \n                # Update personal best\n                if f < pbest_fitness[i]:\n                    pbest_fitness[i] = f\n                    pbest_positions[i] = population[i].copy()\n                \n                # Update global best\n                if f < self.f_opt:\n                    self.f_opt = f\n                    self.x_opt = population[i].copy()\n            \n            # Selection: Keep the best half of the population\n            sorted_indices = np.argsort(fitness)\n            population = population[sorted_indices[:self.pop_size//2]]\n            pbest_positions = pbest_positions[sorted_indices[:self.pop_size//2]]\n            pbest_fitness = pbest_fitness[sorted_indices[:self.pop_size//2]]\n            \n            # Repopulate the other half randomly\n            new_population = np.random.uniform(self.lb, self.ub, size=(self.pop_size - self.pop_size//2, self.dim))\n            population = np.concatenate((population, new_population))\n            velocities = np.random.uniform(-self.v_max, self.v_max, size=(self.pop_size, self.dim)) # Reinitialize velocities\n\n            # Evaluate newly populated individuals\n            new_fitness = np.array([func(x) for x in population[self.pop_size//2:]])\n            self.budget -= len(new_fitness)\n            fitness = np.concatenate((fitness[sorted_indices[:self.pop_size//2]], new_fitness))\n            \n            new_pbest_positions = population[self.pop_size//2:].copy()\n            new_pbest_fitness = fitness[self.pop_size//2:].copy()\n            pbest_positions = np.concatenate((pbest_positions, new_pbest_positions))\n            pbest_fitness = np.concatenate((pbest_fitness, new_pbest_fitness))\n\n            # Update global best after repopulation\n            global_best_index = np.argmin(fitness)\n            self.f_opt = fitness[global_best_index]\n            self.x_opt = population[global_best_index]\n            \n            iteration += 1\n            \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm ModifiedPSO scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0]}, "task_prompt": ""}
{"id": "3d30a51c-7143-45fa-b2d4-d1231a0ae145", "fitness": 0.0, "name": "AdaptiveDEArchiveRestart", "description": "No description provided.", "code": "import numpy as np\n\nclass AdaptiveDEArchiveRestart:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.F = 0.5  # Mutation factor\n        self.CR = 0.7 # Crossover rate\n        self.lb = -5.0\n        self.ub = 5.0\n        self.archive = []\n        self.restart_interval = 500 # Function evaluations between restarts\n\n    def __call__(self, func):\n        self.population = np.random.uniform(self.lb, self.ub, (self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n        evals = self.pop_size\n        restart_counter = 0\n\n        success_F = []\n        success_CR = []\n        success_count = 0\n\n        while evals < self.budget:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                if len(self.archive) > 0 and np.random.rand() < 0.1:  # Use archive occasionally\n                    archive_idx = np.random.randint(0, len(self.archive))\n                    a = self.archive[archive_idx]\n                    idxs = [idx for idx in range(self.pop_size) if idx != i]\n                    b, c = self.population[np.random.choice(idxs, 2, replace=False)]\n                    mutant = self.population[i] + self.F * (a - b) + self.F * (c-self.population[i])\n                else:\n                    a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n                    mutant = self.population[i] + self.F * (b - c)\n\n                mutant = np.clip(mutant, self.lb, self.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, self.population[i])\n\n                # Selection\n                f = func(trial)\n                evals += 1\n                if f < self.fitness[i]:\n                    self.fitness[i] = f\n                    self.population[i] = trial\n\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = trial\n                    \n                    success_count +=1\n                    success_F.append(self.F)\n                    success_CR.append(self.CR)\n\n                    # Update archive\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(trial)\n                    else:\n                        self.archive[np.random.randint(0, self.archive_size)] = trial\n\n            # Adapt F and CR\n            if success_count > 0:\n                self.F = np.mean(success_F) if success_F else 0.5\n                self.CR = np.mean(success_CR) if success_CR else 0.7\n                \n                success_F = []\n                success_CR = []\n                success_count = 0\n\n            restart_counter += self.pop_size\n            if restart_counter >= self.restart_interval:\n                self.population = np.random.uniform(self.lb, self.ub, (self.pop_size, self.dim))\n                self.fitness = np.array([func(x) for x in self.population])\n                best_idx = np.argmin(self.fitness)\n                if self.fitness[best_idx] < self.f_opt:\n                    self.f_opt = self.fitness[best_idx]\n                    self.x_opt = self.population[best_idx]\n                restart_counter = 0 # reset counter\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveDEArchiveRestart scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0]}, "task_prompt": ""}
{"id": "c9ea73c0-a34c-4af1-82ae-acd7d097dff1", "fitness": 0.0, "name": "RankDE", "description": "No description provided.", "code": "import numpy as np\n\nclass RankDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, CR=0.9, local_search_prob=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.lb = -5.0\n        self.ub = 5.0\n        self.pop = None\n        self.fitness = None\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.local_search_prob = local_search_prob\n\n    def initialize_population(self, func):\n        self.pop = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.eval_count += self.pop_size\n        best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_index]\n        self.x_opt = self.pop[best_index]\n\n    def repair(self, x):\n        return np.clip(x, self.lb, self.ub)\n\n    def local_search(self, func, x):\n        # Cauchy-based local search\n        for _ in range(5):  # Perform a few local search steps\n            step = np.random.standard_cauchy(size=self.dim) * 0.01  # Smaller step size\n            new_x = self.repair(x + step)\n            f = func(new_x)\n            self.eval_count += 1\n            if f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = new_x\n            if f < func(x):\n                x = new_x  # Move to the better solution\n        return x\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.eval_count < self.budget:\n            # Rank the population\n            ranked_indices = np.argsort(self.fitness)\n            ranked_pop = self.pop[ranked_indices]\n\n            for i in range(self.pop_size):\n                # Mutation - Use rank-based mutation\n                pbest = ranked_pop[0] # Best individual\n                prand = ranked_pop[np.random.randint(1, self.pop_size)]  # Random individual from ranked population\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b = self.pop[np.random.choice(idxs, 2, replace=False)]\n\n                mutant = self.repair(self.pop[i] + self.F * (pbest - self.pop[i]) + self.F * (a - b))\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, self.pop[i])\n\n                # Selection\n                f = func(trial)\n                self.eval_count += 1\n\n                if f < self.fitness[i]:\n                    self.fitness[i] = f\n                    self.pop[i] = trial\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = trial\n\n                # Adaptive F\n                self.F = np.random.normal(0.5, 0.1)\n                self.F = np.clip(self.F, 0.1, 1.0)\n                \n                # Local Search (Cauchy)\n                if np.random.rand() < self.local_search_prob:\n                    self.pop[i] = self.local_search(func, self.pop[i].copy()) # ensure not changing x_opt accidentally\n                    self.fitness[i] = func(self.pop[i])\n                    self.eval_count += 1  # Account for local search eval\n                    if self.fitness[i] < self.f_opt:\n                        self.f_opt = self.fitness[i]\n                        self.x_opt = self.pop[i]\n\n                if self.eval_count >= self.budget:\n                    break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm RankDE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0]}, "task_prompt": ""}
{"id": "710c9817-bcf2-4419-b3b3-77bdd649b49e", "fitness": 0.5266318148937563, "name": "AdaptiveDE", "description": "No description provided.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F_max=1.2, F_min=0.2, cr=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F_max = F_max\n        self.F_min = F_min\n        self.cr = cr\n        self.lb = -5.0\n        self.ub = 5.0\n\n    def __call__(self, func):\n        # Initialize population\n        population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        \n        # Evaluate initial population\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n        \n        # Initialize best position and fitness\n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index]\n\n        generation = 0\n        while self.budget > 0:\n            # Adaptive mutation factor\n            F = self.F_max - (self.F_max - self.F_min) * (generation / (self.budget / self.pop_size))\n            \n            for i in range(self.pop_size):\n                # Choose three random indices, distinct from each other and i\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = np.random.choice(idxs, 3, replace=False)\n                \n                # Mutation\n                mutant = population[a] + F * (population[b] - population[c])\n                \n                # Crossover\n                trial = population[i].copy()\n                cross_points = np.random.rand(self.dim) < self.cr\n                trial[cross_points] = mutant[cross_points]\n                \n                # Boundary handling\n                trial = np.clip(trial, self.lb, self.ub)\n                \n                # Evaluate trial vector\n                f = func(trial)\n                self.budget -= 1\n                \n                # Selection\n                if f < fitness[i]:\n                    fitness[i] = f\n                    population[i] = trial.copy()\n                    \n                    # Update global best\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = trial.copy()\n            \n            generation += 1\n            \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveDE scored 0.527 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.19426865958188766, 0.39038755114444523, 0.4723235042988657, 0.7239716610821497, 0.5174240153347309, 0.5711210229411894, 0.43590262034111815, 0.43891790616195736, 0.4493175689359068, 0.4277042954511645, 0.6851656404733736, 0.9948962235116466, 0.35810007872815197, 0.5215420456279223, 0.8552556318922718, 0.6338635593498929, 0.4231089424977126, 0.6891735912538017, 0.21108429380879867, 0.5391074854581392]}, "task_prompt": ""}
{"id": "3bc028c9-34e3-45ca-ba75-6f12c647f52a", "fitness": 0.6019551266552764, "name": "AggressiveDifferentialEvolution", "description": "No description provided.", "code": "import numpy as np\n\nclass AggressiveDifferentialEvolution:\n    def __init__(self, budget=10000, dim=10, pop_size=20, F=0.9, CR=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        \n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n        \n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index]\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = a + self.F * (b - c)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover - Adaptive CR\n                cr = self.CR * np.random.rand() # Adaptive Crossover Rate\n                cross_points = np.random.rand(self.dim) < cr\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                f = func(trial)\n                self.budget -= 1\n                if f < fitness[i]:\n                    fitness[i] = f\n                    population[i] = trial\n\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = trial\n            \n            best_index = np.argmin(fitness)\n            if fitness[best_index] < self.f_opt:\n                self.f_opt = fitness[best_index]\n                self.x_opt = population[best_index]    \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm AggressiveDifferentialEvolution scored 0.602 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.19528934311917645, 0.25086458717221105, 0.6146349266577531, 0.8042399929747118, 0.7574277460524079, 0.7885772931180204, 0.3801865038169513, 0.5947427924323659, 0.7009662104455968, 0.6381116176788193, 0.7193086177190375, 0.9968105674473211, 0.3082873014061702, 0.5853889944743976, 0.8834447217811067, 0.803792663785402, 0.4774763241858645, 0.8252687096528732, 0.21565806678793442, 0.49862555239740414]}, "task_prompt": ""}
{"id": "274802ee-4e8d-4734-acd8-1a4c972d1e59", "fitness": 0.0, "name": "SelfAdaptiveDE", "description": "No description provided.", "code": "import numpy as np\n\nclass SelfAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F_init=0.5, cr_init=0.9, restart_trigger=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F_init * np.ones(pop_size)  # Initialize F for each individual\n        self.cr = cr_init * np.ones(pop_size)  # Initialize CR for each individual\n        self.lb = -5.0\n        self.ub = 5.0\n        self.restart_trigger = restart_trigger\n        self.stagnation_counter = 0\n        self.best_fitness_history = []\n\n    def __call__(self, func):\n        # Initialize population\n        population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        \n        # Evaluate initial population\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n        \n        # Initialize best position and fitness\n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index]\n        self.best_fitness_history.append(self.f_opt)\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Self-adaptive parameters\n                self.F[i] = np.clip(np.random.normal(0.5, 0.3), 0.0, 1.0)\n                self.cr[i] = np.clip(np.random.normal(0.9, 0.2), 0.0, 1.0)\n\n                # Choose three random indices, distinct from each other and i\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = np.random.choice(idxs, 3, replace=False)\n                \n                # Mutation with current-to-best/1\n                mutant = population[i] + self.F[i] * (self.x_opt - population[i]) + self.F[i] * (population[a] - population[b])\n                \n                # Crossover\n                trial = population[i].copy()\n                cross_points = np.random.rand(self.dim) < self.cr[i]\n                trial[cross_points] = mutant[cross_points]\n                \n                # Boundary handling\n                trial = np.clip(trial, self.lb, self.ub)\n                \n                # Evaluate trial vector\n                f = func(trial)\n                self.budget -= 1\n                \n                # Selection\n                if f < fitness[i]:\n                    fitness[i] = f\n                    population[i] = trial.copy()\n                    \n                    # Update global best\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = trial.copy()\n                        self.stagnation_counter = 0 # Reset counter\n                    else:\n                        self.stagnation_counter += 1\n\n            self.best_fitness_history.append(self.f_opt)\n            # Restart mechanism\n            if self.stagnation_counter > self.restart_trigger * self.budget / self.pop_size:\n                population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n                fitness = np.array([func(x) for x in population])\n                self.budget -= self.pop_size\n                best_index = np.argmin(fitness)\n                self.f_opt = fitness[best_index]\n                self.x_opt = population[best_index]\n                self.stagnation_counter = 0 # Reset counter\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm SelfAdaptiveDE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0]}, "task_prompt": ""}
{"id": "4f25fc07-f451-4ac2-9880-380716332a46", "fitness": 0.3028510336198375, "name": "AdaptiveDE", "description": "No description provided.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, CR=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.lb = -5.0\n        self.ub = 5.0\n\n    def __call__(self, func):\n        # Initialize population\n        population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        # Find initial best\n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index].copy()\n\n        while self.budget > 0:\n            # Calculate population diversity\n            diversity = np.std(population)\n\n            # Adjust mutation factor based on diversity\n            adaptive_F = self.F * (1 + diversity)\n\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = np.random.choice(idxs, 3, replace=False)\n                mutant = population[a] + adaptive_F * (population[b] - population[c])\n                mutant = np.clip(mutant, self.lb, self.ub)\n\n                # Crossover\n                trial = population[i].copy()\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial[cross_points] = mutant[cross_points]\n\n                # Evaluation\n                f = func(trial)\n                self.budget -= 1\n\n                # Selection\n                if f < fitness[i]:\n                    fitness[i] = f\n                    population[i] = trial.copy()\n\n                    # Update best\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = trial.copy()\n\n                if self.budget <= 0:\n                    break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveDE scored 0.303 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.12936146836541484, 0.17388206229116132, 0.2884324663958887, 0.24500412170814112, 0.24504687661920987, 0.2721475678136338, 0.25671177633233877, 0.26357717401069525, 0.2152334108531463, 0.17130107133879424, 0.22191420860752564, 0.9934443007116256, 0.23045679814419984, 0.24986563806342543, 0.6574837815874544, 0.3016809642241437, 0.22131609403279862, 0.2953955009447612, 0.16953882700645395, 0.45522656334593825]}, "task_prompt": ""}
{"id": "d4cbe173-6304-40ec-8e47-f0087d77d84b", "fitness": 0.6168163699302205, "name": "EnhancedDE", "description": "No description provided.", "code": "import numpy as np\n\nclass EnhancedDE:\n    def __init__(self, budget=10000, dim=10, pop_size=100, F=0.3, CR=0.95, rejuvenation_rate=0.05):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F  # Mutation factor\n        self.CR = CR  # Crossover rate\n        self.lb = -5.0\n        self.ub = 5.0\n        self.pop = None\n        self.fitness = None\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.rejuvenation_rate = rejuvenation_rate\n\n    def initialize_population(self, func):\n        self.pop = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.eval_count += self.pop_size\n        best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_index]\n        self.x_opt = self.pop[best_index]\n\n    def repair(self, x):\n        return np.clip(x, self.lb, self.ub)\n\n    def rejuvenate_population(self):\n      # Replace a fraction of the population with new random individuals\n      num_rejuvenated = int(self.rejuvenation_rate * self.pop_size)\n      indices_to_rejuvenate = np.random.choice(self.pop_size, num_rejuvenated, replace=False)\n      self.pop[indices_to_rejuvenate] = np.random.uniform(self.lb, self.ub, size=(num_rejuvenated, self.dim))\n      self.fitness[indices_to_rejuvenate] = np.array([np.inf]*num_rejuvenated) # Reset fitness of rejuvenated individuals\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.eval_count < self.budget:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = self.pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = self.repair(a + self.F * (b - c))\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, self.pop[i])\n\n                # Selection\n                f = func(trial)\n                self.eval_count += 1\n                if f < self.fitness[i]:\n                    self.fitness[i] = f\n                    self.pop[i] = trial\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = trial\n                \n                if self.eval_count >= self.budget:\n                  break\n            \n            # Rejuvenate the population periodically\n            if (self.eval_count / self.pop_size) % 10 == 0 and self.eval_count < self.budget:\n                self.rejuvenate_population()\n                #Evaluate the fitness for new individuals.\n                for i in range(self.pop_size):\n                  if self.fitness[i] == np.inf:\n                    self.fitness[i] = func(self.pop[i])\n                    self.eval_count +=1\n                    if self.fitness[i] < self.f_opt:\n                      self.f_opt = self.fitness[i]\n                      self.x_opt = self.pop[i]\n                    if self.eval_count >= self.budget:\n                      break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm EnhancedDE scored 0.617 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.3383248749958988, 0.6429365034953276, 0.516436752653957, 0.8447087039196639, 0.5957057837063193, 0.7109147931648205, 0.5096692063167316, 0.5702897043688618, 0.6379839585861085, 0.4603285245664389, 0.831684113533313, 0.9903336937661588, 0.4316875603969603, 0.6602404867896827, 0.7586393281316417, 0.7026936550666507, 0.5263847271004434, 0.7733098401864011, 0.3189098831468997, 0.5151453047121288]}, "task_prompt": ""}
{"id": "262088f5-47bb-4a23-a098-55f42c612a7f", "fitness": 0.0, "name": "AdaptiveSimulatedAnnealing", "description": "No description provided.", "code": "import numpy as np\n\nclass AdaptiveSimulatedAnnealing:\n    def __init__(self, budget=10000, dim=10, temp_init=1.0, alpha=0.99, restarts=5):\n        self.budget = budget\n        self.dim = dim\n        self.temp_init = temp_init\n        self.alpha = alpha\n        self.restarts = restarts\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        eval_count = 0\n\n        for restart in range(self.restarts):\n            temp = self.temp_init\n            x = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n            f = func(x)\n            eval_count += 1\n\n            if f < self.f_opt:\n                self.f_opt = f\n                self.x_opt = x\n\n            while eval_count < self.budget:\n                x_new = x + np.random.normal(0, 0.1, size=self.dim)  # Small random step\n                x_new = np.clip(x_new, func.bounds.lb, func.bounds.ub)\n                f_new = func(x_new)\n                eval_count += 1\n\n                delta_f = f_new - f\n                if delta_f < 0 or np.random.rand() < np.exp(-delta_f / temp):\n                    f = f_new\n                    x = x_new\n\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = x\n\n                temp *= self.alpha # Temperature decreases\n                if eval_count >= self.budget:\n                    break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveSimulatedAnnealing scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0]}, "task_prompt": ""}
{"id": "fd96987f-1078-4911-8cdf-c8ff9b89f885", "fitness": 0.0, "name": "AdaptiveSimulatedAnnealing", "description": "No description provided.", "code": "import numpy as np\n\nclass AdaptiveSimulatedAnnealing:\n    def __init__(self, budget=10000, dim=10, initial_temp=100.0, cooling_rate=0.95, reset_prob=0.05):\n        self.budget = budget\n        self.dim = dim\n        self.initial_temp = initial_temp\n        self.cooling_rate = cooling_rate\n        self.reset_prob = reset_prob\n        self.lb = -5.0\n        self.ub = 5.0\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        x = np.random.uniform(self.lb, self.ub, size=self.dim)\n        f = func(x)\n        self.f_opt = f\n        self.x_opt = x\n        temp = self.initial_temp\n        \n        for i in range(self.budget):\n            x_new = x + np.random.normal(0, temp**0.5, size=self.dim)\n            x_new = np.clip(x_new, self.lb, self.ub)  # Keep within bounds\n            f_new = func(x_new)\n\n            delta_f = f_new - f\n            if delta_f < 0 or np.random.rand() < np.exp(-delta_f / temp):\n                f = f_new\n                x = x_new\n\n                if f < self.f_opt:\n                    self.f_opt = f\n                    self.x_opt = x\n\n            temp *= self.cooling_rate\n\n            # Periodic reset to explore different regions\n            if np.random.rand() < self.reset_prob:\n                x = np.random.uniform(self.lb, self.ub, size=self.dim)\n                f = func(x)\n                if f < self.f_opt:\n                    self.f_opt = f\n                    self.x_opt = x\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveSimulatedAnnealing scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0]}, "task_prompt": ""}
{"id": "c78297ce-696b-460f-a291-0b7a804b9035", "fitness": 0.3124403093522735, "name": "AdaptiveSimulatedAnnealing", "description": "No description provided.", "code": "import numpy as np\n\nclass AdaptiveSimulatedAnnealing:\n    def __init__(self, budget=10000, dim=10, temp_init=1.0, alpha=0.95, step_size=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.temp = temp_init\n        self.alpha = alpha\n        self.step_size = step_size\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.acceptance_rate = 0.0\n        self.acceptance_history = []\n\n    def __call__(self, func):\n        x = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        self.f_opt = func(x)\n        self.x_opt = x\n        self.eval_count += 1\n\n        while self.eval_count < self.budget:\n            # Generate neighbor\n            x_new = x + np.random.normal(0, self.step_size, size=self.dim)\n            x_new = np.clip(x_new, func.bounds.lb, func.bounds.ub)\n            f_new = func(x_new)\n            self.eval_count += 1\n\n            # Acceptance criterion\n            delta_f = f_new - self.f_opt\n            if delta_f < 0:\n                x = x_new\n                self.f_opt = f_new\n                self.x_opt = x_new\n                self.acceptance_rate = 1.0\n            else:\n                prob = np.exp(-delta_f / self.temp)\n                if np.random.rand() < prob:\n                    x = x_new\n                    self.f_opt = f_new\n                    self.x_opt = x_new\n                    self.acceptance_rate = 1.0\n                else:\n                    self.acceptance_rate = 0.0\n\n            self.acceptance_history.append(self.acceptance_rate)\n\n            # Adaptive temperature schedule\n            if len(self.acceptance_history) > 100:\n                avg_acceptance = np.mean(self.acceptance_history[-100:])\n                if avg_acceptance > 0.8:\n                    self.step_size *= 1.05 #Exploration\n                elif avg_acceptance < 0.2:\n                    self.step_size *= 0.95 #Exploitation\n                self.temp *= self.alpha\n\n            if self.eval_count >= self.budget:\n                break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveSimulatedAnnealing scored 0.312 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.09282652527618207, 0.11721362910609934, 0.3737035713260214, 0.17064298276477186, 0.11747719117501387, 0.130523609744442, 0.2915271299883945, 0.1513197543010405, 0.13437520251185164, 0.17732089805613582, 0.31400385003982456, 0.9921127910519951, 0.21552627606974084, 0.3075038877750086, 0.8990465452828255, 0.3131868541515289, 0.2306422983323696, 0.9669916692043083, 0.1328117441497778, 0.12004977673813888]}, "task_prompt": ""}
{"id": "4bf601da-5e40-4aeb-bbea-9b801040dfd2", "fitness": "-inf", "name": "CMAES", "description": "No description provided.", "code": "import numpy as np\n\nclass CMAES:\n    def __init__(self, budget=10000, dim=10, pop_size=None, sigma=0.5, cs=0.3, damps=None, ccov1=None, ccovmu=None, restarts=5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size if pop_size is not None else 4 + int(3 * np.log(dim))\n        self.sigma = sigma\n        self.restarts = restarts\n        self.lb = -5.0\n        self.ub = 5.0\n\n        # Initialize strategy parameters\n        self.mu = (self.ub + self.lb) / 2 * np.ones(self.dim)  # Mean value\n        self.lambda_ = self.pop_size\n        self.weights = np.log(self.lambda_ + 1/2) - np.log(np.arange(1, self.lambda_ + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n\n        self.cs = cs\n        self.damps = damps if damps is not None else 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1) + self.cs\n\n        self.ccov1 = ccov1 if ccov1 is not None else 2 / ((self.dim + 1.3)**2 + self.mueff)\n        self.ccovmu = ccovmu if ccovmu is not None else 2 * (self.mueff - 2 + 1/self.mueff) / ((self.dim + 2.0)**2 + self.mueff)\n        self.ccov1sep = min(1, self.ccov1 * (self.dim + 1.5) / 3)\n        self.ccovmusep = min(1, self.ccovmu * (self.dim + 1.5) / 3)\n\n        self.B = np.eye(self.dim)\n        self.D = np.ones(self.dim)\n        self.C = self.B @ np.diag(self.D**2) @ self.B.T\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.chiN = np.sqrt(self.dim) * (1 - (1/(4*self.dim)) + 1/(21*self.dim**2))\n        self.eval_count = 0\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.hist = []\n        self.tolx = 1e-12 * self.sigma\n\n    def sample_population(self):\n        z = np.random.randn(self.dim, self.lambda_)\n        y = self.B @ np.diag(self.D) @ z\n        x = self.mu.reshape(-1, 1) + self.sigma * y\n        x = np.clip(x, self.lb, self.ub)\n        return x.T, y\n\n    def update_distribution(self, x, y, fitness):\n        xmean = np.sum(x * self.weights.reshape(-1, 1), axis=0)\n        y_mean = np.sum(y * self.weights.reshape(-1, 1), axis=0)\n\n        ps_norm = np.linalg.norm(self.ps) / self.chiN\n        self.sigma *= np.exp((self.cs / self.damps) * (ps_norm - 1))\n        self.sigma = min(self.sigma, 5)\n        \n        self.ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs) * self.mueff) * (self.B @ y_mean)\n        self.pc = (1 - self.ccov1) * self.pc + np.sqrt(self.ccov1 * (2 - self.ccov1) * self.mueff) * (xmean - self.mu) / self.sigma\n\n        artmp = (1/self.sigma) * (x - self.mu)\n        self.C = (1-self.ccov1-self.ccovmu) * self.C + self.ccov1 * (self.pc[:,None] @ self.pc[None,:]) + self.ccovmu * artmp.T @ np.diag(self.weights) @ artmp\n        \n        self.mu = xmean\n        self.C = np.triu(self.C) + np.triu(self.C, 1).T\n        self.D, self.B = np.linalg.eigh(self.C)\n        self.D = np.sqrt(self.D)\n    \n\n    def __call__(self, func):\n        for _ in range(self.restarts):\n            self.mu = (self.ub + self.lb) / 2 * np.ones(self.dim)\n            self.sigma = 0.5\n            self.C = np.eye(self.dim)\n            self.pc = np.zeros(self.dim)\n            self.ps = np.zeros(self.dim)\n            self.B = np.eye(self.dim)\n            self.D = np.ones(self.dim)\n            \n            while self.eval_count < self.budget:\n                x, y = self.sample_population()\n                fitness = np.array([func(xi) for xi in x])\n                self.eval_count += self.lambda_\n                \n                if np.min(fitness) < self.f_opt:\n                    self.f_opt = np.min(fitness)\n                    self.x_opt = x[np.argmin(fitness)]\n\n                idx = np.argsort(fitness)\n                x = x[idx]\n                y = y[idx]\n\n                self.update_distribution(x, y, fitness)\n\n                if self.sigma < self.tolx:\n                  break\n\n                if self.eval_count >= self.budget:\n                  break\n            if self.eval_count >= self.budget:\n                  break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "In the code, line 88, in __call__, the following error occurred:\nIndexError: index 5 is out of bounds for axis 0 with size 2\nOn line: y = y[idx]", "error": "In the code, line 88, in __call__, the following error occurred:\nIndexError: index 5 is out of bounds for axis 0 with size 2\nOn line: y = y[idx]", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "fe7b3d87-ed57-47d6-8bf5-892541a63d3a", "fitness": 0.375115108177404, "name": "FocusedPerturbation", "description": "No description provided.", "code": "import numpy as np\n\nclass FocusedPerturbation:\n    def __init__(self, budget=10000, dim=10, initial_radius=1.0):\n        self.budget = budget\n        self.dim = dim\n        self.initial_radius = initial_radius\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        \n        # Initial random solution\n        x = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        f = func(x)\n        self.budget -= 1\n        self.f_opt = f\n        self.x_opt = x\n        \n        radius = self.initial_radius\n\n        while self.budget > 0:\n            # Perturb the current best solution\n            perturbation = np.random.uniform(-radius, radius, size=self.dim)\n            x_new = self.x_opt + perturbation\n            x_new = np.clip(x_new, func.bounds.lb, func.bounds.ub)\n\n            f_new = func(x_new)\n            self.budget -= 1\n\n            if f_new < self.f_opt:\n                self.f_opt = f_new\n                self.x_opt = x_new\n            else:\n                # Reduce the radius if no improvement\n                radius *= 0.99  # Reduce radius gradually\n        \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm FocusedPerturbation scored 0.375 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.06545355954801269, 0.16448955300783674, 0.9435812863046671, 0.18255539517432506, 0.2062597543928617, 0.14519714636090753, 0.31304953936860647, 0.5124006988397718, 0.16761405510223015, 0.13996861813614947, 0.9783290455615967, 0.19065538858148134, 0.22342477220647772, 0.237857813725326, 0.7092007163003182, 0.49399560092252137, 0.26294881143207993, 0.9686301872574526, 0.11071738113367358, 0.4859728401917838]}, "task_prompt": ""}
{"id": "3ab85e7f-3c57-4931-bbae-798b7cc2bd75", "fitness": 0.618374944495429, "name": "ClampedPSO", "description": "No description provided.", "code": "import numpy as np\n\nclass ClampedPSO:\n    def __init__(self, budget=10000, dim=10, swarm_size=50, w=0.7, c1=1.5, c2=1.5, v_max_ratio=0.2):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = swarm_size\n        self.w = w  # Inertia weight\n        self.c1 = c1  # Cognitive coefficient\n        self.c2 = c2  # Social coefficient\n        self.lb = -5.0\n        self.ub = 5.0\n        self.swarm = None\n        self.velocities = None\n        self.personal_best_positions = None\n        self.personal_best_fitness = None\n        self.global_best_position = None\n        self.global_best_fitness = np.inf\n        self.eval_count = 0\n        self.v_max = v_max_ratio * (self.ub - self.lb)\n\n    def initialize_swarm(self, func):\n        self.swarm = np.random.uniform(self.lb, self.ub, size=(self.swarm_size, self.dim))\n        self.velocities = np.random.uniform(-self.v_max, self.v_max, size=(self.swarm_size, self.dim))\n        self.personal_best_positions = self.swarm.copy()\n        self.personal_best_fitness = np.array([func(x) for x in self.swarm])\n        self.eval_count += self.swarm_size\n        best_index = np.argmin(self.personal_best_fitness)\n        self.global_best_fitness = self.personal_best_fitness[best_index]\n        self.global_best_position = self.personal_best_positions[best_index].copy()\n\n    def __call__(self, func):\n        self.initialize_swarm(func)\n\n        while self.eval_count < self.budget:\n            for i in range(self.swarm_size):\n                # Update velocity\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                cognitive_component = self.c1 * r1 * (self.personal_best_positions[i] - self.swarm[i])\n                social_component = self.c2 * r2 * (self.global_best_position - self.swarm[i])\n                self.velocities[i] = self.w * self.velocities[i] + cognitive_component + social_component\n                self.velocities[i] = np.clip(self.velocities[i], -self.v_max, self.v_max)\n\n                # Update position\n                self.swarm[i] += self.velocities[i]\n                self.swarm[i] = np.clip(self.swarm[i], self.lb, self.ub)  # Ensure bounds\n\n                # Evaluate fitness\n                fitness = func(self.swarm[i])\n                self.eval_count += 1\n\n                # Update personal best\n                if fitness < self.personal_best_fitness[i]:\n                    self.personal_best_fitness[i] = fitness\n                    self.personal_best_positions[i] = self.swarm[i].copy()\n\n                    # Update global best\n                    if fitness < self.global_best_fitness:\n                        self.global_best_fitness = fitness\n                        self.global_best_position = self.swarm[i].copy()\n                if self.eval_count >= self.budget:\n                  break\n\n        self.f_opt = self.global_best_fitness\n        self.x_opt = self.global_best_position\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm ClampedPSO scored 0.618 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.24009706381116058, 0.6984231298871356, 0.7399489932362058, 0.9078927318395524, 0.2642775118079602, 0.804189775979023, 0.32494894307546107, 0.6307197479941109, 0.7443764767883561, 0.22952447112658014, 0.8864475649190208, 0.9927182019043522, 0.26371633406914174, 0.7403729135646615, 0.7331415983395304, 0.7600527270434442, 0.6503201235765144, 0.8053772786551484, 0.4595853776316664, 0.49136792465955037]}, "task_prompt": ""}
{"id": "8ffa9938-6c0e-475a-ab2d-dcb38bd3fab1", "fitness": 0.5014775359454131, "name": "AdaptiveRandomSearch", "description": "No description provided.", "code": "import numpy as np\n\nclass AdaptiveRandomSearch:\n    def __init__(self, budget=10000, dim=10, initial_step_size=1.0, success_rate_threshold=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.step_size = initial_step_size\n        self.success_rate_threshold = success_rate_threshold\n        self.lb = -5.0\n        self.ub = 5.0\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        x = np.random.uniform(self.lb, self.ub, size=self.dim)\n        self.f_opt = func(x)\n        self.x_opt = x\n        self.budget -= 1\n        \n        successes = 0\n        iterations = 0\n\n        while self.budget > 0:\n            iterations += 1\n            # Perturb the current solution\n            direction = np.random.normal(0, 1, size=self.dim)\n            x_new = x + self.step_size * direction\n            x_new = np.clip(x_new, self.lb, self.ub)  # Ensure bounds are respected\n\n            f_new = func(x_new)\n            self.budget -= 1\n\n            if f_new < self.f_opt:\n                self.f_opt = f_new\n                self.x_opt = x_new\n                x = x_new\n                successes += 1\n\n            # Adapt step size based on success rate\n            if iterations % 100 == 0:\n                success_rate = successes / iterations\n                if success_rate > self.success_rate_threshold:\n                    self.step_size *= 1.1  # Increase step size if successful\n                else:\n                    self.step_size *= 0.9  # Decrease step size if unsuccessful\n                successes = 0\n                iterations = 0\n            if self.budget <= 0:\n                break\n            \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveRandomSearch scored 0.501 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.1729214405266004, 0.16398309542924905, 0.5606085292902472, 0.8619527818855448, 0.3031983887614249, 0.6238781093303642, 0.28704491447982505, 0.5026965612644029, 0.5576284831724849, 0.2437473522524679, 0.8137191066883107, 0.9970034052716956, 0.3066989188721967, 0.3486703623317473, 0.9089508455188942, 0.37013384485498146, 0.5293402904563358, 0.755029211980671, 0.2092733906275136, 0.5130716859133029]}, "task_prompt": ""}
{"id": "fb84ea08-5155-4f57-9aca-a84ae2553a3a", "fitness": "-inf", "name": "GaussianProcessOptimization", "description": "No description provided.", "code": "import numpy as np\nfrom sklearn.gaussian_process import GaussianProcessRegressor\nfrom sklearn.gaussian_process.kernels import RBF, ConstantKernel as C\n\nclass GaussianProcessOptimization:\n    def __init__(self, budget=10000, dim=10, n_initial=10, exploration_weight=2.0):\n        self.budget = budget\n        self.dim = dim\n        self.n_initial = n_initial\n        self.exploration_weight = exploration_weight\n        self.lb = -5.0\n        self.ub = 5.0\n        self.X = None\n        self.y = None\n        self.gp = None\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.eval_count = 0\n\n    def initialize(self, func):\n        self.X = np.random.uniform(self.lb, self.ub, size=(self.n_initial, self.dim))\n        self.y = np.array([func(x) for x in self.X])\n        self.eval_count += self.n_initial\n        best_index = np.argmin(self.y)\n        self.f_opt = self.y[best_index]\n        self.x_opt = self.X[best_index]\n\n    def acquisition(self, x, gp):\n        mu, sigma = gp.predict(x.reshape(1, -1), return_std=True)\n        return mu - self.exploration_weight * sigma\n\n    def __call__(self, func):\n        self.initialize(func)\n\n        kernel = C(1.0, constant_value_bounds=\"fixed\") * RBF(1.0, length_scale_bounds=\"fixed\")\n        self.gp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=10)\n        self.gp.fit(self.X, self.y)\n\n        while self.eval_count < self.budget:\n            # Sample a new point using the acquisition function\n            x_new = None\n            best_acq = np.inf\n\n            for _ in range(100):\n                x_candidate = np.random.uniform(self.lb, self.ub, size=self.dim)\n                acq = self.acquisition(x_candidate, self.gp)\n                if acq < best_acq:\n                    best_acq = acq\n                    x_new = x_candidate\n\n            f_new = func(x_new)\n            self.eval_count += 1\n\n            self.X = np.vstack((self.X, x_new))\n            self.y = np.append(self.y, f_new)\n            \n            if f_new < self.f_opt:\n                self.f_opt = f_new\n                self.x_opt = x_new\n\n            self.gp.fit(self.X, self.y)\n            \n            if self.eval_count >= self.budget:\n              break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "In the code, line 35, in __call__, the following error occurred:\nNameError: name 'C' is not defined\nOn line: kernel = C(1.0, constant_value_bounds=\"fixed\") * RBF(1.0, length_scale_bounds=\"fixed\")", "error": "In the code, line 35, in __call__, the following error occurred:\nNameError: name 'C' is not defined\nOn line: kernel = C(1.0, constant_value_bounds=\"fixed\") * RBF(1.0, length_scale_bounds=\"fixed\")", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "be0ae2b5-93a7-436c-bb34-2bb31e08d083", "fitness": "-inf", "name": "CMAES", "description": "No description provided.", "code": "import numpy as np\n\nclass CMAES:\n    def __init__(self, budget=10000, dim=10, pop_size=None, sigma=0.5, mu_percentage=0.5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size if pop_size is not None else 4 + int(3 * np.log(dim))\n        self.sigma = sigma\n        self.mu = int(self.pop_size * mu_percentage)\n        self.weights = np.log(self.pop_size + 0.5) - np.log(np.arange(1, self.pop_size + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mu_eff = np.sum(self.weights[:self.mu])**2 / np.sum(self.weights[:self.mu]**2)\n        self.c_sigma = (self.mu_eff + 2) / (self.dim + self.mu_eff + 5)\n        self.c_c = (4 + self.mu_eff / self.dim) / (self.dim + 4 + 2 * self.mu_eff / self.dim)\n        self.c_cov = (1 / self.mu_eff) * ((self.mu_eff + 2) / (self.dim - 2 + (self.mu_eff+2)**2))\n        self.d_sigma = 1 + 2 * max(0, np.sqrt((self.mu_eff - 1) / (self.dim + 1)) - 1) + self.c_sigma\n        self.lb = -5.0\n        self.ub = 5.0\n        self.m = None\n        self.C = None\n        self.p_sigma = None\n        self.p_c = None\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.restart_trigger = False\n        self.restart_threshold = 1e-12\n\n    def initialize(self):\n        self.m = np.random.uniform(self.lb, self.ub, size=self.dim)\n        self.C = np.eye(self.dim)\n        self.p_sigma = np.zeros(self.dim)\n        self.p_c = np.zeros(self.dim)\n\n    def sample_population(self):\n        z = np.random.randn(self.pop_size, self.dim)\n        try:\n            A = np.linalg.cholesky(self.C)\n            x = self.m + self.sigma * z @ A.T\n        except np.linalg.LinAlgError:\n            # Handle non-positive definite covariance matrix\n            self.C = self.C + 1e-6 * np.eye(self.dim)\n            A = np.linalg.cholesky(self.C)\n            x = self.m + self.sigma * z @ A.T\n        return x\n\n    def repair(self, x):\n        return np.clip(x, self.lb, self.ub)\n    \n    def restart(self):\n        self.initialize()\n        self.sigma = 0.5\n        self.restart_trigger = False\n\n    def __call__(self, func):\n        self.initialize()\n\n        while self.eval_count < self.budget:\n            x = self.sample_population()\n            x = np.array([self.repair(xi) for xi in x])  # Apply boundary repair\n            \n            f = np.array([func(xi) for xi in x])\n            self.eval_count += self.pop_size\n\n            idx = np.argsort(f)\n            x = x[idx]\n            f = f[idx]\n\n            if f[0] < self.f_opt:\n                self.f_opt = f[0]\n                self.x_opt = x[0]\n\n            # Check for stagnation and trigger restart\n            if np.abs(self.f_opt) < self.restart_threshold:\n                self.restart_trigger = True\n\n            if self.restart_trigger:\n                self.restart()\n                continue\n\n            z = (x[:self.mu] - self.m) / self.sigma\n            \n            # Update CMA-ES parameters\n            self.p_sigma = (1 - self.c_sigma) * self.p_sigma + np.sqrt(self.c_sigma * (2 - self.c_sigma) * self.mu_eff) * (z[:self.mu] @ self.weights[:self.mu])\n            ps_norm = np.linalg.norm(self.p_sigma)\n            self.sigma *= np.exp((self.c_sigma / self.d_sigma) * (ps_norm / np.sqrt(self.dim) - 1))\n            \n            h_sigma = 1.0 if (ps_norm / np.sqrt(1 - (1 - self.c_sigma)**(2 * self.eval_count / self.pop_size)) < (1.4 + 2 / (self.dim + 1))) else 0.0\n\n            self.p_c = (1 - self.c_c) * self.p_c + h_sigma * np.sqrt(self.c_c * (2 - self.c_c) * self.mu_eff) * ((x[:self.mu] - self.m) @ self.weights[:self.mu]) / self.sigma\n            \n            self.m = (x[:self.mu] @ self.weights[:self.mu])\n            \n            self.C = (1 - self.c_cov) * self.C + self.c_cov * (1 / self.mu_eff) * (self.p_c[:, None] @ self.p_c[None, :]) + self.c_cov * (1 - h_sigma) * (z[:self.mu].T @ np.diag(self.weights[:self.mu]) @ z[:self.mu])\n\n            if self.eval_count >= self.budget:\n                break\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "In the code, line 84, in __call__, the following error occurred:\nValueError: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 3 is different from 2)\nOn line: self.p_sigma = (1 - self.c_sigma) * self.p_sigma + np.sqrt(self.c_sigma * (2 - self.c_sigma) * self.mu_eff) * (z[:self.mu] @ self.weights[:self.mu])", "error": "In the code, line 84, in __call__, the following error occurred:\nValueError: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 3 is different from 2)\nOn line: self.p_sigma = (1 - self.c_sigma) * self.p_sigma + np.sqrt(self.c_sigma * (2 - self.c_sigma) * self.mu_eff) * (z[:self.mu] @ self.weights[:self.mu])", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "98e8caf3-aa13-4867-95fd-ea2605a45010", "fitness": "-inf", "name": "BudgetAwareCMAES", "description": "No description provided.", "code": "import numpy as np\n\nclass BudgetAwareCMAES:\n    def __init__(self, budget=10000, dim=10, initial_pop_size=None):\n        self.budget = budget\n        self.dim = dim\n        self.lb = -5.0\n        self.ub = 5.0\n        self.x_mean = None\n        self.sigma = None\n        self.C = None\n        self.pop_size = None\n        self.mu = None\n        self.weights = None\n        self.mueff = None\n        self.c_sigma = None\n        self.c_c = None\n        self.c_cov = None\n        self.D = None\n        self.B = None\n        self.pc = None\n        self.ps = None\n        self.eval_count = 0\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.initial_pop_size = initial_pop_size if initial_pop_size is not None else 4 + int(3 * np.log(self.dim))\n        self.restart_trigger = 100  # Restart after this many iterations without improvement\n        self.no_improvement_count = 0\n        self.last_f_opt = np.inf\n        self.min_pop_size = 4 # Minimum allowed population size\n\n    def initialize(self):\n        self.x_mean = np.random.uniform(self.lb, self.ub, size=self.dim)\n        self.sigma = 0.5\n        self.C = np.eye(self.dim)\n        self.pop_size = self.initial_pop_size\n        self.mu = self.pop_size // 2\n        self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n\n        self.c_sigma = (self.mueff + 2) / (self.dim + self.mueff + 5)\n        self.c_c = (4 + self.mueff / self.dim) / (self.dim + 4 + 2 * self.mueff / self.dim)\n        self.c_cov = (1 / self.mueff) * (self.mueff + 1) / ((self.dim + 2) * (self.dim + 2) + self.mueff / 2)\n        self.damps = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1) + self.c_sigma\n\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.C = np.eye(self.dim)\n        self.D, self.B = np.linalg.eig(self.C)\n        self.D = np.sqrt(self.D)\n\n    def sample_population(self):\n        z = np.random.normal(0, 1, size=(self.pop_size, self.dim))\n        x = self.x_mean + self.sigma * (self.B @ (self.D * z.T)).T\n        return np.clip(x, self.lb, self.ub)\n\n    def update_parameters(self, x, fitness):\n        x_old = self.x_mean\n        x_sorted = x[np.argsort(fitness)]\n        self.x_mean = np.sum(self.weights[:, None] * x_sorted[:self.mu], axis=0)\n        \n        self.ps = (1 - self.c_sigma) * self.ps + np.sqrt(self.c_sigma * (2 - self.c_sigma) * self.mueff) * (self.x_mean - x_old) / self.sigma\n        self.hsig = np.linalg.norm(self.ps) / np.sqrt(1 - (1 - self.c_sigma)**(2 * (self.eval_count / self.pop_size))) < (1.4 + 2 / (self.dim + 1))\n\n        self.pc = (1 - self.c_c) * self.pc + self.hsig * np.sqrt(self.c_c * (2 - self.c_c) * self.mueff) * (self.x_mean - x_old) / self.sigma\n\n        artmp = (x_sorted[:self.mu] - x_old).T / self.sigma\n        self.C = (1 - self.c_cov) * self.C + self.c_cov * (self.pc[:, None] @ self.pc[None, :]) + self.c_cov * np.sum(self.weights[:, None, None] * (artmp[:, :, None] @ artmp[:, None, :]), axis=0)\n\n        self.sigma *= np.exp((self.c_sigma / self.damps) * (np.linalg.norm(self.ps) / np.sqrt(self.dim) - 1))\n        self.D, self.B = np.linalg.eig(self.C)\n        self.D = np.sqrt(self.D)\n\n    def __call__(self, func):\n        self.initialize()\n\n        while self.eval_count < self.budget:\n            x = self.sample_population()\n            fitness = np.array([func(xi) for xi in x])\n            self.eval_count += self.pop_size\n\n            best_index = np.argmin(fitness)\n            if fitness[best_index] < self.f_opt:\n                self.f_opt = fitness[best_index]\n                self.x_opt = x[best_index]\n                self.no_improvement_count = 0\n            else:\n                self.no_improvement_count += 1\n\n            self.update_parameters(x, fitness)\n\n            # Budget-aware population size adaptation\n            remaining_budget = self.budget - self.eval_count\n            \n            # Adjust pop_size based on remaining budget and performance\n            if remaining_budget > 10 * self.dim:\n                self.pop_size = min(self.initial_pop_size + int(np.log(remaining_budget)), 2 * self.initial_pop_size)  # Increase pop_size if budget allows\n            else:\n                self.pop_size = max(self.min_pop_size, int(self.initial_pop_size * remaining_budget / (10 * self.dim))) # Reduce pop_size if budget is tight\n            \n            self.mu = max(1, self.pop_size // 2)\n            self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n            self.weights = self.weights / np.sum(self.weights)\n            self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n\n            # Restart mechanism\n            if self.no_improvement_count > self.restart_trigger:\n                self.initialize()  # Restart CMA-ES\n                self.no_improvement_count = 0\n            \n            if self.eval_count >= self.budget:\n              break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "In the code, line 55, in sample_population, the following error occurred:\nValueError: operands could not be broadcast together with shapes (2,) (2,6) \nOn line: x = self.x_mean + self.sigma * (self.B @ (self.D * z.T)).T", "error": "In the code, line 55, in sample_population, the following error occurred:\nValueError: operands could not be broadcast together with shapes (2,) (2,6) \nOn line: x = self.x_mean + self.sigma * (self.B @ (self.D * z.T)).T", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "ba2159cf-4d68-42d9-a7b1-89efe7365139", "fitness": "-inf", "name": "EnhancedDE", "description": "No description provided.", "code": "import numpy as np\n\nclass EnhancedDE:\n    def __init__(self, budget=10000, dim=10, pop_size_init=50, F=0.5, CR=0.9, neighborhood_size=5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size_init = pop_size_init\n        self.pop_size = pop_size_init\n        self.F = F  # Mutation factor\n        self.CR = CR  # Crossover rate\n        self.lb = -5.0\n        self.ub = 5.0\n        self.pop = None\n        self.fitness = None\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.neighborhood_size = neighborhood_size\n\n    def initialize_population(self, func):\n        self.pop = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.eval_count += self.pop_size\n        best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_index]\n        self.x_opt = self.pop[best_index]\n\n    def repair(self, x):\n        return np.clip(x, self.lb, self.ub)\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.eval_count < self.budget:\n            for i in range(self.pop_size):\n                # Neighborhood-based Mutation\n                neighborhood_indices = np.random.choice(self.pop_size, self.neighborhood_size, replace=False)\n                a = self.pop[np.random.choice(neighborhood_indices)]\n                b = self.pop[np.random.choice(neighborhood_indices)]\n                c = self.pop[np.random.choice(neighborhood_indices)]\n                mutant = self.repair(a + self.F * (b - c))\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, self.pop[i])\n\n                # Selection\n                f = func(trial)\n                self.eval_count += 1\n                if f < self.fitness[i]:\n                    self.fitness[i] = f\n                    self.pop[i] = trial\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = trial\n\n                # Adaptive F and CR (optional, but can improve performance)\n                self.F = np.random.normal(0.5, 0.1)\n                self.F = np.clip(self.F, 0.1, 1.0)\n                self.CR = np.random.normal(0.9, 0.1)\n                self.CR = np.clip(self.CR, 0.1, 1.0)\n\n                # Adaptive Population Size (example: reduce if no improvement)\n                if self.eval_count % (self.pop_size * 2) == 0:\n                    if self.f_opt == np.min(self.fitness):\n                        self.pop_size = max(10, int(self.pop_size * 0.9))  # Reduce pop size\n                        self.pop = self.pop[np.argsort(self.fitness)[:self.pop_size]]\n                        self.fitness = self.fitness[np.argsort(self.fitness)[:self.pop_size]]\n                    else:\n                        self.pop_size = min(self.pop_size_init, int(self.pop_size * 1.1))\n\n                if self.eval_count >= self.budget:\n                    break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "In the code, line 47, in __call__, the following error occurred:\nIndexError: index 40 is out of bounds for axis 0 with size 40\nOn line: trial = np.where(cross_points, mutant, self.pop[i])", "error": "In the code, line 47, in __call__, the following error occurred:\nIndexError: index 40 is out of bounds for axis 0 with size 40\nOn line: trial = np.where(cross_points, mutant, self.pop[i])", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "5857aee2-f0eb-4358-9b53-c8d89f1287cb", "fitness": 0.3111929102098143, "name": "SelfAdaptiveDE", "description": "No description provided.", "code": "import numpy as np\n\nclass SelfAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F_initial=0.5, CR_initial=0.9, local_search_prob=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F_initial  # Initial mutation factor\n        self.CR = CR_initial  # Initial crossover rate\n        self.lb = -5.0\n        self.ub = 5.0\n        self.pop = None\n        self.fitness = None\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.local_search_prob = local_search_prob\n\n    def initialize_population(self, func):\n        self.pop = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.eval_count += self.pop_size\n        best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_index]\n        self.x_opt = self.pop[best_index]\n\n    def repair(self, x):\n        return np.clip(x, self.lb, self.ub)\n    \n    def local_search(self, func, x, radius=0.1):\n        # Perform a simple local search around x\n        best_x = x\n        best_f = func(x)\n        self.eval_count += 1\n\n        for _ in range(5):  # Limited local search evaluations\n            new_x = x + np.random.uniform(-radius, radius, size=self.dim)\n            new_x = self.repair(new_x)\n            new_f = func(new_x)\n            self.eval_count += 1\n\n            if new_f < best_f:\n                best_f = new_f\n                best_x = new_x\n        \n        return best_f, best_x\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.eval_count < self.budget:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = self.pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = self.repair(a + self.F * (b - c))\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, self.pop[i])\n\n                # Selection\n                f = func(trial)\n                self.eval_count += 1\n                if f < self.fitness[i]:\n                    self.fitness[i] = f\n                    self.pop[i] = trial\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = trial\n                \n                # Local Search\n                if np.random.rand() < self.local_search_prob:\n                    f_local, x_local = self.local_search(func, self.pop[i])\n                    if f_local < self.fitness[i]:\n                        self.fitness[i] = f_local\n                        self.pop[i] = x_local\n                        if f_local < self.f_opt:\n                            self.f_opt = f_local\n                            self.x_opt = x_local\n\n                # Adaptive F and CR (simple adaptation)\n                if self.eval_count % 100 == 0:\n                  self.F = np.clip(np.random.normal(0.5, 0.1), 0.1, 0.9)\n                  self.CR = np.clip(np.random.normal(0.9, 0.1), 0.1, 0.99)\n                \n                if self.eval_count >= self.budget:\n                  break\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm SelfAdaptiveDE scored 0.311 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.23582466836642701, 0.4481033566998365, 0.5608436157729939, 0]}, "task_prompt": ""}
{"id": "dc4e5e3c-193c-4d02-865d-cfead570ebf3", "fitness": "-inf", "name": "HybridDE_NM", "description": "No description provided.", "code": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridDE_NM:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, CR=0.7, nm_iterations=3):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.lb = -5.0\n        self.ub = 5.0\n        self.pop = None\n        self.fitness = None\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.nm_iterations = nm_iterations #Number of Nelder-Mead iterations\n\n    def initialize_population(self, func):\n        self.pop = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.eval_count += self.pop_size\n        best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_index]\n        self.x_opt = self.pop[best_index]\n\n    def repair(self, x):\n        return np.clip(x, self.lb, self.ub)\n\n    def nelder_mead_refinement(self, func, x):\n        bounds = [(self.lb, self.ub)] * self.dim\n        result = minimize(func, x, method='Nelder-Mead', bounds=bounds, options={'maxiter': self.nm_iterations, 'maxfev': self.budget - self.eval_count})\n        self.eval_count += result.nfev\n        return result.fun, result.x\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.eval_count < self.budget:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = self.pop[np.random.choice(idxs, 3, replace=False)]\n                mutant = self.repair(a + self.F * (b - c))\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, self.pop[i])\n\n                # Selection\n                f = func(trial)\n                self.eval_count += 1\n                if f < self.fitness[i]:\n                    self.fitness[i] = f\n                    self.pop[i] = trial\n\n                    #Refine with Nelder-Mead\n                    f_refined, x_refined = self.nelder_mead_refinement(func, trial)\n                    if f_refined < self.fitness[i]:\n                        self.fitness[i] = f_refined\n                        self.pop[i] = x_refined\n                    f = self.fitness[i] #Update f with refined fitness\n\n\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = trial\n                if self.eval_count >= self.budget:\n                    break\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "In the code, line 33, in nelder_mead_refinement, the following error occurred:\nNameError: name 'minimize' is not defined\nOn line: result = minimize(func, x, method='Nelder-Mead', bounds=bounds, options={'maxiter': self.nm_iterations, 'maxfev': self.budget - self.eval_count})", "error": "In the code, line 33, in nelder_mead_refinement, the following error occurred:\nNameError: name 'minimize' is not defined\nOn line: result = minimize(func, x, method='Nelder-Mead', bounds=bounds, options={'maxiter': self.nm_iterations, 'maxfev': self.budget - self.eval_count})", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "c2985eae-c2e9-480f-8c5b-6d26d94269d6", "fitness": 0.6217712952457635, "name": "SelfAdaptiveDifferentialEvolution", "description": "No description provided.", "code": "import numpy as np\n\nclass SelfAdaptiveDifferentialEvolution:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F_init=0.5, CR_init=0.7):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F_init * np.ones(pop_size)\n        self.CR = CR_init * np.ones(pop_size)\n        self.F_init = F_init\n        self.CR_init = CR_init\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        \n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n        \n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index]\n\n        success_F = []\n        success_CR = []\n        \n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = a + self.F[i] * (b - c)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR[i]\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                f = func(trial)\n                self.budget -= 1\n                if f < fitness[i]:\n                    success_F.append(self.F[i])\n                    success_CR.append(self.CR[i])\n\n                    fitness[i] = f\n                    population[i] = trial\n\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = trial\n            \n            best_index = np.argmin(fitness)\n            if fitness[best_index] < self.f_opt:\n                self.f_opt = fitness[best_index]\n                self.x_opt = population[best_index]\n\n            # Adapt F and CR\n            if success_F:\n                self.F = 0.9 * self.F + 0.1 * np.mean(success_F) * np.ones(self.pop_size)\n                self.CR = 0.9 * self.CR + 0.1 * np.mean(success_CR) * np.ones(self.pop_size)\n            else:\n                self.F = self.F_init * np.ones(self.pop_size)\n                self.CR = self.CR_init * np.ones(self.pop_size)\n\n            self.F = np.clip(self.F, 0.1, 1.0)\n            self.CR = np.clip(self.CR, 0.1, 1.0)\n            \n            success_F = []\n            success_CR = []\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm SelfAdaptiveDifferentialEvolution scored 0.622 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.21450730873841073, 0.3648484045024769, 0.5841269857223619, 0.8476440588423093, 0.6861994289280187, 0.7519988020133475, 0.5843632516679234, 0.5970583318192639, 0.7102265563837717, 0.6218529543950149, 0.7684843119094289, 0.9987483005000037, 0.35072494284293565, 0.6618983791601138, 0.8775320190977706, 0.7527877904238274, 0.5310785895707426, 0.7881966866672185, 0.2242400578894701, 0.5189087438408606]}, "task_prompt": ""}
{"id": "472586f9-a662-45ce-aa3b-2fb6b249f5a2", "fitness": "-inf", "name": "CMAES", "description": "No description provided.", "code": "import numpy as np\n\nclass CMAES:\n    def __init__(self, budget=10000, dim=10, pop_size=None, sigma=0.5, cs=0.3, damps=None, c_cov=None, c_1=None, c_mu=None):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size if pop_size is not None else 4 + int(3 * np.log(dim))\n        self.sigma = sigma\n        self.lb = -5.0\n        self.ub = 5.0\n        self.mean = np.random.uniform(self.lb, self.ub, size=dim)\n        self.C = np.eye(dim)\n        self.pc = np.zeros(dim)\n        self.ps = np.zeros(dim)\n        self.chiN = dim**0.5 * (1 - 1/(4*dim) + 1/(21*dim**2))\n\n        self.mu = self.pop_size // 2\n\n        if damps is None:\n          self.damps = 1 + 2*max(0, np.sqrt((self.mu-1)/(self.dim+1)) - 1) + cs\n\n        if c_cov is None:\n            self.c_cov = (1 / self.mu) * (2 / ((self.dim+1.3)**2 + self.mu)) + (1 - 1/self.mu) * (0.3 / ((self.dim+1.3)**2 + self.mu))\n        if c_1 is None:\n            self.c_1 = 2 / ((self.dim+1.3)**2 + self.mu)\n        if c_mu is None:\n            self.c_mu = (1 / self.mu) * (2 / ((self.dim+1.3)**2 + self.mu))\n\n        self.eval_count = 0\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.restart_trigger = False\n\n    def sample_population(self):\n        z = np.random.randn(self.pop_size, self.dim)\n        return self.mean + self.sigma * np.dot(z, np.linalg.cholesky(self.C).T)\n\n    def repair(self, x):\n        return np.clip(x, self.lb, self.ub)\n\n    def update_distribution(self, pop, fitness):\n        idx = np.argsort(fitness)\n        elite_indices = idx[:self.mu]\n        elite_pop = pop[elite_indices]\n\n        self.mean = np.mean(elite_pop, axis=0)\n\n        z = (elite_pop - self.mean) / self.sigma\n\n        self.ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs)) * np.dot(np.linalg.inv(np.linalg.cholesky(self.C)), (self.mean - self.mean) / self.sigma)\n        self.sigma *= np.exp((self.cs/self.damps)*(np.linalg.norm(self.ps)/self.chiN - 1))\n        \n        self.pc = (1 - self.c_1) * self.pc + np.sqrt(self.c_1 * (2 - self.c_1)) * (self.mean - self.mean)/self.sigma\n        \n        self.C = (1 - self.c_1 - self.c_mu) * self.C + self.c_1 * np.outer(self.pc, self.pc) + self.c_mu * np.mean([np.outer(z[i], z[i]) for i in range(self.mu)], axis=0)\n\n        if np.any(np.diag(self.C) <= 0):\n            self.C = np.eye(self.dim)\n            self.sigma = 0.5\n            self.pc = np.zeros(self.dim)\n\n    def check_restart(self):\n      if self.sigma < 1e-8 or self.sigma > 1e8:\n        self.restart_trigger = True\n      else:\n        self.restart_trigger = False\n\n    def __call__(self, func):\n        while self.eval_count < self.budget:\n            pop = self.sample_population()\n            pop = np.array([self.repair(x) for x in pop])\n            fitness = np.array([func(x) for x in pop])\n            self.eval_count += self.pop_size\n\n            best_index = np.argmin(fitness)\n            if fitness[best_index] < self.f_opt:\n                self.f_opt = fitness[best_index]\n                self.x_opt = pop[best_index]\n            \n            self.update_distribution(pop, fitness)\n            self.check_restart()\n\n            if self.restart_trigger and self.eval_count < self.budget:\n                self.mean = np.random.uniform(self.lb, self.ub, size=self.dim)\n                self.C = np.eye(self.dim)\n                self.pc = np.zeros(self.dim)\n                self.ps = np.zeros(self.dim)\n                self.sigma = 0.5\n\n            if self.eval_count >= self.budget:\n              break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "In the code, line 50, in update_distribution, the following error occurred:\nAttributeError: 'CMAES' object has no attribute 'cs'. Did you mean: 'ps'?\nOn line: self.ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs)) * np.dot(np.linalg.inv(np.linalg.cholesky(self.C)), (self.mean - self.mean) / self.sigma)", "error": "In the code, line 50, in update_distribution, the following error occurred:\nAttributeError: 'CMAES' object has no attribute 'cs'. Did you mean: 'ps'?\nOn line: self.ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs)) * np.dot(np.linalg.inv(np.linalg.cholesky(self.C)), (self.mean - self.mean) / self.sigma)", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "52d29164-45b6-4025-934b-868dc6073df3", "fitness": "-inf", "name": "CMAES", "description": "No description provided.", "code": "import numpy as np\n\nclass CMAES:\n    def __init__(self, budget=10000, dim=10, pop_size=None, sigma=0.5, cs=0.3, damps=None, ccov1=None, ccovmu=None):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size if pop_size is not None else 4 + int(3 * np.log(self.dim))\n        self.sigma = sigma\n        self.lb = -5.0\n        self.ub = 5.0\n        self.eval_count = 0\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        # Strategy parameter setting: Selection\n        self.mu = self.pop_size // 2\n        self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n\n        # Strategy parameter setting: Adaptation\n        self.cs = cs\n        self.damps = damps if damps is not None else 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1) + self.cs\n        self.ccov1 = ccov1 if ccov1 is not None else 2 / ((self.dim + 1.3)**2 + self.mueff)\n        self.ccovmu = ccovmu if ccovmu is not None else 2 * (self.mueff - 2 + 1 / self.mueff) / ((self.dim + 2.0)**2 + self.mueff)\n        self.ccov1 = min(1, self.ccov1 * (self.dim + 1.3)**2 / (self.mueff + 2))\n        self.ccovmu = min(1, self.ccovmu * (self.dim + 2)**2 / (self.mueff))\n\n        # Initialize dynamic (internal) strategy parameters and constants\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.B = np.eye(self.dim)\n        self.D = np.ones(self.dim)\n        self.C = self.B @ np.diag(self.D**2) @ self.B.T\n        self.invsqrtC = self.B @ np.diag(self.D**-1) @ self.B.T\n        self.mean = np.random.uniform(self.lb, self.ub, self.dim)\n\n    def sample_population(self):\n        z = np.random.randn(self.dim, self.pop_size)\n        y = self.B @ (self.D * z)\n        x = self.mean[:, np.newaxis] + self.sigma * y\n        x = np.clip(x, self.lb, self.ub)\n        return x.T\n\n    def __call__(self, func):\n        while self.eval_count < self.budget:\n            # Generate and evaluate lambda offspring\n            x = self.sample_population()\n            f = np.array([func(xi) for xi in x])\n            self.eval_count += self.pop_size\n\n            # Sort by fitness and update mean\n            idx = np.argsort(f)\n            x = x[idx]\n            f = f[idx]\n            xmean = np.sum(self.weights[:, np.newaxis] * x[:self.mu], axis=0)\n\n            # Update optimal value\n            if f[0] < self.f_opt:\n                self.f_opt = f[0]\n                self.x_opt = x[0]\n                \n            if self.eval_count >= self.budget:\n              break\n\n            # Cumulation: Update evolution paths\n            y = (xmean - self.mean) / self.sigma\n            self.ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs) * self.mueff) * self.invsqrtC @ y\n            hsig = (np.linalg.norm(self.ps) / np.sqrt(1 - (1 - self.cs)**(2 * (self.eval_count / self.pop_size))) / np.sqrt(self.dim + (self.dim))) < 1.4 + 2 / (self.dim + 1)\n            self.pc = (1 - 1) * self.pc + hsig * np.sqrt(1 - 1) * y\n            # Adapt covariance matrix C\n            z = (x[:self.mu] - self.mean) / self.sigma\n            self.C = (1-self.ccov1-self.ccovmu) * self.C + self.ccov1 * (self.pc[:, np.newaxis] @ self.pc[np.newaxis,:]) + self.ccovmu * z.T @ np.diag(self.weights) @ z\n            # Update step size sigma\n            self.mean = xmean\n            self.sigma *= np.exp((self.cs / self.damps) * (np.linalg.norm(self.ps) / np.sqrt(self.dim) - 1))\n\n            try:\n                self.C = np.triu(self.C) + np.triu(self.C, 1).T\n                self.D, self.B = np.linalg.eigh(self.C)\n                self.D = np.sqrt(self.D.real)\n                self.invsqrtC = self.B @ np.diag(self.D**-1) @ self.B.T\n            except np.linalg.LinAlgError:\n                self.C = self.C + 1e-8 * np.eye(self.dim)\n                self.D, self.B = np.linalg.eigh(self.C)\n                self.D = np.sqrt(self.D.real)\n                self.invsqrtC = self.B @ np.diag(self.D**-1) @ self.B.T\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "In the code, line 40, in sample_population, the following error occurred:\nValueError: operands could not be broadcast together with shapes (2,) (2,6) \nOn line: y = self.B @ (self.D * z)", "error": "In the code, line 40, in sample_population, the following error occurred:\nValueError: operands could not be broadcast together with shapes (2,) (2,6) \nOn line: y = self.B @ (self.D * z)", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "8cded6cd-0895-4535-ad86-26ea9330e59c", "fitness": 0.4224768572031408, "name": "GravitationalParticleSwarm", "description": "No description provided.", "code": "import numpy as np\n\nclass GravitationalParticleSwarm:\n    def __init__(self, budget=10000, dim=10, swarm_size=50, w_max=0.9, w_min=0.2, c1=2, c2=2, G0=100):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = swarm_size\n        self.w_max = w_max\n        self.w_min = w_min\n        self.c1 = c1\n        self.c2 = c2\n        self.G0 = G0\n        self.lb = -5.0\n        self.ub = 5.0\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        \n        # Initialize swarm\n        swarm = np.random.uniform(self.lb, self.ub, size=(self.swarm_size, self.dim))\n        velocities = np.random.uniform(-1, 1, size=(self.swarm_size, self.dim))\n        fitness = np.array([func(x) for x in swarm])\n        self.budget -= self.swarm_size\n        \n        personal_best_positions = swarm.copy()\n        personal_best_fitness = fitness.copy()\n        \n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = swarm[best_index]\n        global_best_position = swarm[best_index].copy()\n        \n        iteration = 0\n        while self.budget > 0:\n            # Update inertia weight\n            w = self.w_max - (self.w_max - self.w_min) * iteration / (self.budget / self.swarm_size)\n            \n            # Calculate gravitational constant\n            G = self.G0 * np.exp(-20 * iteration / (self.budget / self.swarm_size))\n            \n            # Calculate total mass of each particle (proportional to fitness)\n            mass = np.exp(-fitness / np.mean(fitness))\n            mass = mass / np.sum(mass)\n            \n            for i in range(self.swarm_size):\n                # Calculate gravitational force\n                force = np.zeros(self.dim)\n                for j in range(self.swarm_size):\n                    if i != j:\n                        R = np.linalg.norm(swarm[j] - swarm[i])\n                        if R == 0:\n                            R = 1e-6 \n                        force += mass[j] * (swarm[j] - swarm[i]) / (R + 1e-6)\n                \n                # Update velocity\n                velocities[i] = w * velocities[i] + \\\n                                self.c1 * np.random.rand(self.dim) * (personal_best_positions[i] - swarm[i]) + \\\n                                self.c2 * np.random.rand(self.dim) * (global_best_position - swarm[i]) + \\\n                                G * force\n                \n                # Update position\n                swarm[i] = swarm[i] + velocities[i]\n                swarm[i] = np.clip(swarm[i], self.lb, self.ub)\n                \n                # Evaluate fitness\n                f = func(swarm[i])\n                self.budget -= 1\n                \n                if f < personal_best_fitness[i]:\n                    personal_best_fitness[i] = f\n                    personal_best_positions[i] = swarm[i].copy()\n                    \n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = swarm[i].copy()\n                        global_best_position = swarm[i].copy()\n            \n            iteration += 1\n            if self.budget <= 0:\n                break\n        \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm GravitationalParticleSwarm scored 0.422 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.15005735656473462, 0.2780483789042426, 0.5445641332844484, 0.6894320443835875, 0.6338015911037104, 0.6247115872483346, 0.3014208895468079, 0.21117092271261395, 0.6065380268436221, 0.22465041531784757, 0.21781328892732033, 0.999514795610102, 0.22327903177141784, 0.25391740519237027, 0.6838375577737109, 0.34665036540013017, 0.32471941043193386, 0.4919484453574099, 0.19992389177805758, 0.4435376059104129]}, "task_prompt": ""}
{"id": "c081679f-182a-499f-9afc-87c71397b7dc", "fitness": 0.31882946893354863, "name": "AdaptiveSimulatedAnnealing", "description": "No description provided.", "code": "import numpy as np\n\nclass AdaptiveSimulatedAnnealing:\n    def __init__(self, budget=10000, dim=10, initial_temp=1.0, cooling_rate=0.9, temp_min=1e-5):\n        self.budget = budget\n        self.dim = dim\n        self.initial_temp = initial_temp\n        self.cooling_rate = cooling_rate\n        self.temp_min = temp_min\n        self.lb = -5.0\n        self.ub = 5.0\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        x = np.random.uniform(self.lb, self.ub, size=self.dim)\n        f = func(x)\n        self.budget -= 1\n        self.f_opt = f\n        self.x_opt = x\n\n        temp = self.initial_temp\n        \n        while self.budget > 0 and temp > self.temp_min:\n            \n            # Generate neighbor\n            x_new = x + np.random.normal(0, temp, size=self.dim)\n            x_new = np.clip(x_new, self.lb, self.ub)\n\n            # Evaluate neighbor\n            f_new = func(x_new)\n            self.budget -= 1\n            \n            delta_e = f_new - f\n\n            # Acceptance probability\n            if delta_e < 0 or np.random.rand() < np.exp(-delta_e / temp):\n                x = x_new\n                f = f_new\n\n                if f < self.f_opt:\n                    self.f_opt = f\n                    self.x_opt = x\n            \n            # Adaptive temperature schedule\n            if delta_e > 0:\n                temp *= self.cooling_rate\n            else:\n                temp = self.initial_temp\n        \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveSimulatedAnnealing scored 0.319 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.09822835977273026, 0.19861551570835567, 0.27842093192134443, 0.3034147291464787, 0.23791945998402697, 0.30738753790159823, 0.2515514239834049, 0.2692379210696012, 0.23359056203061335, 0.2096179220871478, 0.31869630642819835, 0.9954270472237132, 0.2450958573612746, 0.2533633328513174, 0.6822295310921667, 0.2965266934385259, 0.2653704562815751, 0.3242588325227237, 0.1389050006873347, 0.46873195717884153]}, "task_prompt": ""}
{"id": "569de716-989e-423e-909a-093df7feee7f", "fitness": 0.5969608669548772, "name": "AdaptiveNeighborhoodSearch", "description": "No description provided.", "code": "import numpy as np\n\nclass AdaptiveNeighborhoodSearch:\n    def __init__(self, budget=10000, dim=10, pop_size=20, initial_radius=0.5, shrink_factor=0.9, expand_factor=1.1, success_threshold=0.2):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.radius = initial_radius\n        self.shrink_factor = shrink_factor\n        self.expand_factor = expand_factor\n        self.success_threshold = success_threshold\n        self.lb = -5.0\n        self.ub = 5.0\n        self.pop = None\n        self.fitness = None\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.eval_count = 0\n\n    def initialize_population(self, func):\n        self.pop = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.eval_count += self.pop_size\n        best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_index]\n        self.x_opt = self.pop[best_index]\n\n    def repair(self, x):\n        return np.clip(x, self.lb, self.ub)\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        success_count = 0\n\n        while self.eval_count < self.budget:\n            best_index = np.argmin(self.fitness)\n            best_x = self.pop[best_index]\n\n            for i in range(self.pop_size):\n                # Sample new point from neighborhood of best solution\n                new_x = best_x + np.random.normal(0, self.radius, self.dim)\n                new_x = self.repair(new_x)\n\n                f = func(new_x)\n                self.eval_count += 1\n\n                if f < self.fitness[i]:\n                    self.fitness[i] = f\n                    self.pop[i] = new_x\n                    success_count += 1\n\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = new_x\n\n                if self.eval_count >= self.budget:\n                    break\n\n            # Adjust radius based on success rate\n            success_rate = success_count / self.pop_size\n            if success_rate > self.success_threshold:\n                self.radius *= self.expand_factor\n            else:\n                self.radius *= self.shrink_factor\n            \n            self.radius = np.clip(self.radius, 1e-6, (self.ub - self.lb)/2) #prevent radius from becoming too small or too big\n            success_count = 0\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveNeighborhoodSearch scored 0.597 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.2837778480764861, 0.1912320807038781, 0.8043045991246921, 0.9219486441175779, 0.8047627540004711, 0.8410223484031776, 0.3449728989801468, 0.6929885205460318, 0.8056157876533915, 0.23004751717361616, 0.9191205353373253, 0.998263866276354, 0.2145654182822211, 0.7658780333514705, 0.9627513759964657, 0.36992503298485757, 0.25684343431470924, 0.8906187277913584, 0.13704690929433072, 0.503531006688984]}, "task_prompt": ""}
{"id": "51f69463-01dd-445f-9cde-69b376f19897", "fitness": 0.17770864675724493, "name": "AdaptiveSimulatedAnnealing", "description": "No description provided.", "code": "import numpy as np\n\nclass AdaptiveSimulatedAnnealing:\n    def __init__(self, budget=10000, dim=10, initial_temp=100.0, cooling_rate=0.95):\n        self.budget = budget\n        self.dim = dim\n        self.initial_temp = initial_temp\n        self.cooling_rate = cooling_rate\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        \n        # Initialize solution\n        current_x = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        current_f = func(current_x)\n        self.budget -= 1\n        \n        self.f_opt = current_f\n        self.x_opt = current_x\n        \n        temp = self.initial_temp\n\n        while self.budget > 0:\n            # Generate neighbor\n            neighbor_x = current_x + np.random.normal(0, temp/self.initial_temp, size=self.dim)\n            neighbor_x = np.clip(neighbor_x, func.bounds.lb, func.bounds.ub)\n            \n            neighbor_f = func(neighbor_x)\n            self.budget -= 1\n\n            # Acceptance probability\n            delta_f = neighbor_f - current_f\n            if delta_f < 0 or np.random.rand() < np.exp(-delta_f / temp):\n                current_x = neighbor_x\n                current_f = neighbor_f\n                \n                if current_f < self.f_opt:\n                    self.f_opt = current_f\n                    self.x_opt = current_x\n\n            # Cool down\n            temp *= self.cooling_rate\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveSimulatedAnnealing scored 0.178 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.08486951415768362, 0.12321200478931582, 0.2212130428960476, 0.2508455971839729, 0.09223243572091255, 0.11736825131964002, 0.10054834143772717, 0.07471591050127335, 0.11747621255852969, 0.07923339488132264, 0.12011756821789965, 1.0, 0.2376842450432477, 0.10386322150513516, 0.18597967716533936, 0.18257706848480526, 0.1388289548156154, 0.14381167278179185, 0.06762385640326396, 0.11197196528137465]}, "task_prompt": ""}
{"id": "49f1220c-2374-4f72-a071-ebabf7657c83", "fitness": 0.45461638875170207, "name": "HybridEvolutionarySwarm", "description": "No description provided.", "code": "import numpy as np\n\nclass HybridEvolutionarySwarm:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, CR=0.7, c1=1.5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.c1 = c1\n        self.lb = -5.0\n        self.ub = 5.0\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        \n        # Initialize population\n        population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n        \n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index]\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = a + self.F * (b - c)\n                mutant = np.clip(mutant, self.lb, self.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Social Learning\n                r = np.random.rand(self.dim)\n                trial = trial + self.c1 * r * (self.x_opt - trial)\n                trial = np.clip(trial, self.lb, self.ub)\n                \n                # Selection\n                f = func(trial)\n                self.budget -= 1\n                if f < fitness[i]:\n                    fitness[i] = f\n                    population[i] = trial\n\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = trial\n            \n            best_index = np.argmin(fitness)\n            if fitness[best_index] < self.f_opt:\n                self.f_opt = fitness[best_index]\n                self.x_opt = population[best_index]    \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm HybridEvolutionarySwarm scored 0.455 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.10419166613293673, 0.16416105628601962, 0.3035815677636803, 0.22726220630914074, 0.2606031172159925, 0.9597834965952357, 0.32393282105331556, 0.3885743125866702, 0.9613115637215115, 0.16964778573581263, 0.32547557625305956, 0.9980943981229071, 0.24220273601343745, 0.5634478200763502, 0.7426174896007451, 0.3642632280843213, 0.2288098833529717, 0.974874043038259, 0.30085990888838554, 0.4886330982032887]}, "task_prompt": ""}
{"id": "9f9109f8-46bc-4a44-88f1-c86379e9673d", "fitness": 0.2508295612509141, "name": "AdaptiveSimulatedAnnealing", "description": "No description provided.", "code": "import numpy as np\n\nclass AdaptiveSimulatedAnnealing:\n    def __init__(self, budget=10000, dim=10, initial_temp=1.0, cooling_rate=0.95, memory_size=10):\n        self.budget = budget\n        self.dim = dim\n        self.initial_temp = initial_temp\n        self.cooling_rate = cooling_rate\n        self.lb = -5.0\n        self.ub = 5.0\n        self.temp = initial_temp\n        self.memory_size = memory_size\n        self.memory = []\n        self.memory_fitness = []\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.eval_count = 0\n\n    def acceptance_probability(self, old_cost, new_cost, temp):\n        if new_cost < old_cost:\n            return 1.0\n        else:\n            return np.exp((old_cost - new_cost) / temp)\n\n    def __call__(self, func):\n        x = np.random.uniform(self.lb, self.ub, size=self.dim)\n        current_cost = func(x)\n        self.eval_count += 1\n        self.f_opt = current_cost\n        self.x_opt = x\n\n        while self.eval_count < self.budget:\n            # Generate neighbor\n            neighbor = x + np.random.normal(0, 0.1, size=self.dim)\n            neighbor = np.clip(neighbor, self.lb, self.ub)\n\n            # Evaluate neighbor\n            new_cost = func(neighbor)\n            self.eval_count += 1\n\n            # Acceptance criterion\n            ap = self.acceptance_probability(current_cost, new_cost, self.temp)\n\n            if ap > np.random.rand():\n                x = neighbor\n                current_cost = new_cost\n\n                if current_cost < self.f_opt:\n                    self.f_opt = current_cost\n                    self.x_opt = x\n                    \n                    if len(self.memory) < self.memory_size:\n                        self.memory.append(x.copy())\n                        self.memory_fitness.append(current_cost)\n                    else:\n                        max_fit_idx = np.argmax(self.memory_fitness)\n                        if current_cost < self.memory_fitness[max_fit_idx]:\n                            self.memory[max_fit_idx] = x.copy()\n                            self.memory_fitness[max_fit_idx] = current_cost\n\n            #Adaptive Temperature Schedule\n            if self.eval_count % 100 == 0: # Adjust temperature every 100 evaluations\n                if len(self.memory) > 0:\n                    self.temp = self.initial_temp * np.mean(self.memory_fitness) / self.f_opt\n                else:\n                     self.temp *= self.cooling_rate\n            \n            if self.temp < 1e-6:\n                self.temp = 1e-6 # prevent temp from going to 0\n\n            if self.eval_count >= self.budget:\n                break\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveSimulatedAnnealing scored 0.251 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.17097727555512487, 0.24622343501871446, 0.29824016425736977, 0.20538662956100984, 0.19952669854707816, 0.20623153510863634, 0.23728359112231134, 0.22920042242208283, 0.1851010985280791, 0.15049140810806771, 0.17790957516038686, 0.5224945813154538, 0.27076558412885043, 0.21102656260917996, 0.5153445609543061, 0.23044267611698077, 0.1967805742150628, 0.21438205315129166, 0.17970140987451744, 0.36908138926377887]}, "task_prompt": ""}
{"id": "a546974a-aece-4fd8-965a-230eba90ed82", "fitness": "-inf", "name": "FitnessProbabilityRestartDE", "description": "No description provided.", "code": "import numpy as np\n\nclass FitnessProbabilityRestartDE:\n    def __init__(self, budget=10000, dim=10, pop_size=40, F=0.6, CR=0.8, restart_prob=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.restart_prob = restart_prob\n        self.lb = -5.0\n        self.ub = 5.0\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        \n        # Initialize population\n        population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n        \n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index]\n\n        while self.budget > 0:\n            # Calculate probabilities based on fitness\n            fitness_normalized = (fitness - np.min(fitness)) / (np.max(fitness) - np.min(fitness) + 1e-8)\n            probabilities = 1 - fitness_normalized\n            probabilities /= np.sum(probabilities)\n\n            for i in range(self.pop_size):\n                # Mutation with fitness-based modification\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False, p=probabilities[idxs])]\n                mutant = a + self.F * (b - c) + np.random.normal(0, 0.01, self.dim) # Adding noise\n                mutant = np.clip(mutant, self.lb, self.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                f = func(trial)\n                self.budget -= 1\n                if f < fitness[i]:\n                    fitness[i] = f\n                    population[i] = trial\n\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = trial\n                \n                # Restart mechanism\n                if np.random.rand() < self.restart_prob:\n                    population[i] = np.random.uniform(self.lb, self.ub, self.dim)\n                    fitness[i] = func(population[i])\n                    self.budget -=1\n                    if fitness[i] < self.f_opt:\n                        self.f_opt = fitness[i]\n                        self.x_opt = population[i]\n                        \n                if self.budget <= 0:\n                    break\n            \n            best_index = np.argmin(fitness)\n            if fitness[best_index] < self.f_opt:\n                self.f_opt = fitness[best_index]\n                self.x_opt = population[best_index]\n                \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "In the code, line 999, in numpy.random.mtrand.RandomState.choice, the following error occurred:\nValueError: probabilities do not sum to 1", "error": "In the code, line 999, in numpy.random.mtrand.RandomState.choice, the following error occurred:\nValueError: probabilities do not sum to 1", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "1768c574-79a5-4bb5-afee-07fe7cdf4177", "fitness": 0.25942384709139155, "name": "TopLeadersEnhancedLearning", "description": "No description provided.", "code": "import numpy as np\n\nclass TopLeadersEnhancedLearning:\n    def __init__(self, budget=10000, dim=10, pop_size=50, top_ratio=0.2, initial_lr=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.top_ratio = top_ratio\n        self.initial_lr = initial_lr\n        self.lb = -5.0\n        self.ub = 5.0\n        self.population = None\n        self.fitness = None\n        self.personal_best_positions = None\n        self.personal_best_fitness = None\n        self.learning_rates = None\n        self.eval_count = 0\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.personal_best_positions = self.population.copy()\n        self.personal_best_fitness = self.fitness.copy()\n        self.learning_rates = np.full(self.pop_size, self.initial_lr)\n        self.eval_count += self.pop_size\n        best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_index]\n        self.x_opt = self.population[best_index].copy()\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.eval_count < self.budget:\n            # Identify top individuals\n            num_top = int(self.pop_size * self.top_ratio)\n            top_indices = np.argsort(self.fitness)[:num_top]\n            top_individuals = self.population[top_indices]\n\n            for i in range(self.pop_size):\n                # Learn from personal best\n                to_personal_best = self.personal_best_positions[i] - self.population[i]\n                \n                # Learn from a randomly selected top individual\n                top_individual = top_individuals[np.random.randint(num_top)]\n                to_top_individual = top_individual - self.population[i]\n\n                # Update position\n                new_position = self.population[i] + self.learning_rates[i] * (to_personal_best + to_top_individual)\n                new_position = np.clip(new_position, self.lb, self.ub)\n\n                # Evaluate fitness\n                new_fitness = func(new_position)\n                self.eval_count += 1\n\n                # Update personal best and learning rate\n                if new_fitness < self.personal_best_fitness[i]:\n                    self.personal_best_fitness[i] = new_fitness\n                    self.personal_best_positions[i] = new_position.copy()\n                    self.learning_rates[i] *= 1.1  # Increase learning rate if successful\n                else:\n                    self.learning_rates[i] *= 0.9  # Decrease learning rate if unsuccessful\n\n                self.learning_rates[i] = np.clip(self.learning_rates[i], 0.01, 0.5)\n\n                # Update population and fitness\n                self.population[i] = new_position\n                self.fitness[i] = new_fitness\n\n                # Update global best\n                if new_fitness < self.f_opt:\n                    self.f_opt = new_fitness\n                    self.x_opt = new_position.copy()\n\n                if self.eval_count >= self.budget:\n                    break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm TopLeadersEnhancedLearning scored 0.259 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.11732550055648538, 0.2140536297588963, 0.33050586187099473, 0.18754708428877342, 0.17863860240612484, 0.17032805525245676, 0.24860255441919932, 0.23198512202587285, 0.20261396679520605, 0.16755523069195077, 0.1926187674760954, 0.9975515888272684, 0.27165648997836533, 0.16734734470073565, 0.15033588956163146, 0.29607675356421637, 0.23560055707170935, 0.20598063257535948, 0.17392770764071375, 0.4482256023657737]}, "task_prompt": ""}
{"id": "f2467a6f-0d19-4476-bd34-fada87f7155e", "fitness": "-inf", "name": "CMAES", "description": "No description provided.", "code": "import numpy as np\n\nclass CMAES:\n    def __init__(self, budget=10000, dim=10, pop_size=None, sigma=0.5, cs=0.3, damps=1.0, c_mu=0.1, c_cov=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size if pop_size is not None else 4 + int(3 * np.log(dim))\n        self.sigma = sigma\n        self.cs = cs\n        self.damps = damps\n        self.c_mu = c_mu\n        self.c_cov = c_cov\n        self.lb = -5.0\n        self.ub = 5.0\n        self.mean = None\n        self.C = None\n        self.eval_count = 0\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.p_sigma = np.zeros(dim)\n        self.p_c = np.zeros(dim)\n        self.eigenspace = None\n        self.eigenvalues = None\n        self.invsqrtC = None\n        self.mu_eff = None\n\n\n    def initialize(self):\n        self.mean = np.random.uniform(self.lb, self.ub, size=self.dim)\n        self.C = np.eye(self.dim)\n        self.mu_eff = self.pop_size / (np.sum(np.arange(1,self.pop_size+1)**-1))\n        \n        weights = np.log(self.pop_size+1/2) - np.log(np.arange(1, self.pop_size + 1))\n        self.weights = weights/np.sum(weights)\n        mueff = self.mu_eff\n        self.c_mu = min(1-self.c_cov, mueff/np.linalg.norm(self.C,\"fro\")**2)\n        self.c_cov = min(1 - self.c_mu, (2+(mueff/self.dim))/( (self.dim + (mueff/self.dim)) ))\n\n        self.damps = 1 + 2*max(0, np.sqrt((mueff - 1)/(self.dim + 1)) -1) + self.cs\n    \n    def update_distribution(self, x, fitness):\n        #Sort by fitness\n        idx = np.argsort(fitness)\n        x = x[idx]\n\n        # Weighted recombination\n        delta_mean = np.sum(self.weights[:,None] * (x[:self.pop_size] - self.mean), axis=0)\n        self.mean += self.c_mu * delta_mean\n\n        #Adaptation of stepsize sigma\n        self.p_sigma = (1-self.cs)*self.p_sigma + np.sqrt(self.cs*(2 - self.cs) * self.mu_eff) * (self.invsqrtC @ delta_mean / self.sigma)\n        self.sigma *= np.exp((self.cs/self.damps) * (np.linalg.norm(self.p_sigma)/ np.sqrt(self.dim) - 1))\n\n        #Adapt covariance matrix C\n        x_i = (x[:self.pop_size] - self.mean)/self.sigma\n        self.p_c = (1-self.c_cov)*self.p_c + np.sqrt(self.c_cov * (2-self.c_cov) * self.mu_eff) * delta_mean/self.sigma\n        self.C = (1-self.c_cov)*self.C + self.c_cov*(self.p_c[:,None] @ self.p_c[None,:])\n        \n        for i in range(self.pop_size):\n            self.C += self.c_mu * self.weights[i] * (x_i[i,:,None] @ x_i[i,None,:])\n\n    def repair(self, x):\n        return np.clip(x, self.lb, self.ub)\n\n    def sample(self):\n      z = np.random.normal(0,1, (self.pop_size, self.dim))\n      x = self.mean + self.sigma * (self.eigenspace @ (self.eigenvalues*z.T)).T\n      return x\n\n    def __call__(self, func):\n        self.initialize()\n\n        while self.eval_count < self.budget:\n            # Sample population\n            self.eigenvalues, self.eigenspace = np.linalg.eigh(self.C)\n            self.eigenvalues = np.sqrt(np.abs(self.eigenvalues))\n            self.invsqrtC = self.eigenspace @ np.diag(self.eigenvalues**-1) @ self.eigenspace.T\n\n            x = self.sample()\n\n            # Evaluate population, repair the boundary\n            fitness = np.zeros(self.pop_size)\n            for i in range(self.pop_size):\n              x[i] = self.repair(x[i])\n              fitness[i] = func(x[i])\n              self.eval_count += 1\n\n              if fitness[i] < self.f_opt:\n                self.f_opt = fitness[i]\n                self.x_opt = x[i]\n              if self.eval_count >= self.budget:\n                break\n            if self.eval_count >= self.budget:\n              break\n\n            self.update_distribution(x, fitness)\n            \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "In the code, line 31, in initialize, the following error occurred:\nValueError: Integers to negative integer powers are not allowed.\nOn line: self.mu_eff = self.pop_size / (np.sum(np.arange(1,self.pop_size+1)**-1))", "error": "In the code, line 31, in initialize, the following error occurred:\nValueError: Integers to negative integer powers are not allowed.\nOn line: self.mu_eff = self.pop_size / (np.sum(np.arange(1,self.pop_size+1)**-1))", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "ec5c7eff-17de-42fc-8a09-92499f3afdf0", "fitness": 0.24926615604584654, "name": "ModifiedDifferentialEvolution", "description": "No description provided.", "code": "import numpy as np\n\nclass ModifiedDifferentialEvolution:\n    def __init__(self, budget=10000, dim=10, pop_size=50, top_ratio=0.2):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.top_ratio = top_ratio\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index]\n\n        while self.budget > 0:\n            # Sort population by fitness\n            sorted_indices = np.argsort(fitness)\n            top_indices = sorted_indices[:int(self.pop_size * self.top_ratio)]\n            top_population = population[top_indices]\n\n            for i in range(self.pop_size):\n                # Weighted average of top individuals\n                weights = np.random.rand(len(top_indices))\n                weights /= np.sum(weights)\n                weighted_sum = np.sum(top_population * weights[:, np.newaxis], axis=0)\n\n                # Random selection from the population\n                random_index = np.random.randint(self.pop_size)\n                random_individual = population[random_index]\n\n                # Update individual's position\n                mutant = population[i] + 0.5 * (weighted_sum - population[i]) + 0.5 * (random_individual - population[i])\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Evaluate the mutant\n                f = func(mutant)\n                self.budget -= 1\n\n                # Selection\n                if f < fitness[i]:\n                    fitness[i] = f\n                    population[i] = mutant\n\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = mutant\n            \n            best_index = np.argmin(fitness)\n            if fitness[best_index] < self.f_opt:\n                self.f_opt = fitness[best_index]\n                self.x_opt = population[best_index]\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm ModifiedDifferentialEvolution scored 0.249 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.11236963923488652, 0.18298851993528975, 0.29329934876115304, 0.16568270897770798, 0.19090881863341114, 0.18029344549936088, 0.2339895284403326, 0.2116647272867127, 0.18581581507087763, 0.16539787013005913, 0.1769233199624085, 0.99835645056465, 0.2715118751164053, 0.20680341639858002, 0.13811734833712686, 0.2767962907334913, 0.20649469593104453, 0.19542155578375675, 0.14886528124452758, 0.4436224648751482]}, "task_prompt": ""}
{"id": "0f9c5ed9-651e-4d24-b6da-ad5e6eb4eec9", "fitness": 0.2930898152444428, "name": "AdaptiveSimulatedAnnealing", "description": "No description provided.", "code": "import numpy as np\n\nclass AdaptiveSimulatedAnnealing:\n    def __init__(self, budget=10000, dim=10, initial_temp=1.0, temp_min=0.0001, alpha=0.99):\n        self.budget = budget\n        self.dim = dim\n        self.initial_temp = initial_temp\n        self.temp_min = temp_min\n        self.alpha = alpha\n        self.lb = -5.0\n        self.ub = 5.0\n        self.current_x = None\n        self.current_f = np.inf\n        self.best_x = None\n        self.best_f = np.inf\n        self.eval_count = 0\n        self.temp = initial_temp\n\n    def __call__(self, func):\n        self.current_x = np.random.uniform(self.lb, self.ub, size=self.dim)\n        self.current_f = func(self.current_x)\n        self.eval_count += 1\n        self.best_x = self.current_x.copy()\n        self.best_f = self.current_f\n\n        while self.eval_count < self.budget:\n            # Generate neighbor\n            new_x = self.current_x + np.random.normal(0, 0.1, size=self.dim)\n            new_x = np.clip(new_x, self.lb, self.ub)\n\n            # Evaluate neighbor\n            new_f = func(new_x)\n            self.eval_count += 1\n\n            # Acceptance probability\n            delta_f = new_f - self.current_f\n            if delta_f < 0 or np.random.rand() < np.exp(-delta_f / self.temp):\n                self.current_x = new_x.copy()\n                self.current_f = new_f\n\n                if new_f < self.best_f:\n                    self.best_f = new_f\n                    self.best_x = new_x.copy()\n            \n            # Adaptive temperature schedule\n            if self.eval_count % 100 == 0:\n                if self.temp > self.temp_min:\n                  self.temp *= self.alpha\n                else:\n                  self.temp = self.temp_min\n\n            if self.eval_count >= self.budget:\n              break\n        \n\n        self.f_opt = self.best_f\n        self.x_opt = self.best_x\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveSimulatedAnnealing scored 0.293 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.07119600586484265, 0.26466221357095077, 0.2608177224656184, 0.31908195859979227, 0.23036450057942814, 0.2747097216978577, 0.23709179940488667, 0.27559353840259615, 0.21002544944814117, 0.17139284567310753, 0.3052344654686914, 0.950067803155246, 0.2568025827891345, 0.24973313745153303, 0.4137817656642241, 0.2370086060105372, 0.21986685367307357, 0.29610480051889865, 0.18651411959686037, 0.4317464148534351]}, "task_prompt": ""}
{"id": "7d93477a-c80a-43b5-95ca-a0fb5d7debf9", "fitness": 0.4957911423717157, "name": "AdaptiveSimulatedAnnealingDE", "description": "No description provided.", "code": "import numpy as np\n\nclass AdaptiveSimulatedAnnealingDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, initial_temp=1.0, cooling_rate=0.95, F=0.5, CR=0.7):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.initial_temp = initial_temp\n        self.cooling_rate = cooling_rate\n        self.F = F\n        self.CR = CR\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index]\n\n        temp = self.initial_temp\n        while self.budget > 0 and temp > 1e-5:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = a + self.F * (b - c)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection (Simulated Annealing inspired)\n                f = func(trial)\n                self.budget -= 1\n                delta_e = f - fitness[i]\n                if delta_e < 0:\n                    fitness[i] = f\n                    population[i] = trial\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = trial\n                else:\n                    acceptance_prob = np.exp(-delta_e / temp)\n                    if np.random.rand() < acceptance_prob:\n                        fitness[i] = f\n                        population[i] = trial\n\n            temp *= self.cooling_rate\n        \n        best_index = np.argmin(fitness)\n        if fitness[best_index] < self.f_opt:\n            self.f_opt = fitness[best_index]\n            self.x_opt = population[best_index]\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveSimulatedAnnealingDE scored 0.496 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.2159137329098053, 0.45083824940292183, 0.4265613792154118, 0.6810298324468952, 0.49775958235271056, 0.4879499470933871, 0.43066744895353315, 0.43869448414842405, 0.4674467845254049, 0.4536280243300326, 0.7093393058308557, 0.9998371229368568, 0.3571911501384881, 0.47744935955926315, 0.6505202865543243, 0.49113546128821095, 0.4174985462650248, 0.5336758090153971, 0.2574124165800503, 0.47127392388731426]}, "task_prompt": ""}
{"id": "79c17a0b-8928-439d-a36b-5e1063add131", "fitness": "-inf", "name": "CMAES", "description": "No description provided.", "code": "import numpy as np\n\nclass CMAES:\n    def __init__(self, budget=10000, dim=10, pop_size=None, sigma=0.5, cs=0.3, damps=None, ccov1=None, ccovmu=None, mu=None):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size if pop_size is not None else 4 + int(3 * np.log(self.dim))\n        self.sigma = sigma\n        self.lb = -5.0\n        self.ub = 5.0\n        self.m = None\n        self.C = None\n        self.pc = None\n        self.ps = None\n        self.eval_count = 0\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        self.mu = mu if mu is not None else self.pop_size // 2\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        \n        self.cs = cs\n        self.damps = damps if damps is not None else 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1) + self.cs\n        self.ccov1 = ccov1 if ccov1 is not None else 2 / ((self.dim + 1.3)**2 + self.mueff)\n        self.ccovmu = ccovmu if ccovmu is not None else 2 * (self.mueff - 2 + 1/self.mueff) / ((self.dim + 2)**2 + self.mueff)\n        self.ccov1 = min(1, self.ccov1 * (self.dim + 1.3)**2 / (self.mueff + 2))\n        \n        self.chiN = self.dim**0.5 * (1 - 1/(4*self.dim) + 1/(21*self.dim**2))\n        \n    def initialize(self):\n        self.m = np.random.uniform(self.lb, self.ub, size=self.dim)\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        \n    def repair(self, x):\n        return np.clip(x, self.lb, self.ub)\n\n    def __call__(self, func):\n        self.initialize()\n\n        while self.eval_count < self.budget:\n            Z = np.random.normal(0, 1, size=(self.dim, self.pop_size))\n            Y = np.dot(np.linalg.cholesky(self.C), Z)\n            X = self.m[:, np.newaxis] + self.sigma * Y\n            X = np.clip(X, self.lb, self.ub)\n\n            fitness = np.array([func(x) for x in X.T])\n            self.eval_count += self.pop_size\n\n            idx_sorted = np.argsort(fitness)\n            fitness = fitness[idx_sorted]\n            X = X[:, idx_sorted]\n\n            x_best = X[:, 0]\n            f_best = fitness[0]\n            \n            if f_best < self.f_opt:\n                self.f_opt = f_best\n                self.x_opt = x_best\n\n            m_old = self.m.copy()\n            self.m = np.dot(X[:, :self.mu], self.weights)\n\n            self.ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs) * self.mueff) * np.dot(np.linalg.inv(np.linalg.cholesky(self.C)), (self.m - m_old)) / self.sigma\n            hsig = (np.linalg.norm(self.ps) / np.sqrt(1 - (1 - self.cs)**(2 * (self.eval_count / self.pop_size))) / self.chiN) < (1.4 + 2 / (self.dim + 1))\n\n            self.pc = (1 - self.ccov1) * self.pc + hsig * np.sqrt(self.ccov1 * (2 - self.ccov1) * self.mueff) * (self.m - m_old) / self.sigma\n\n            artmp = (1 / self.sigma) * (X[:, :self.mu] - m_old[:, np.newaxis])\n            self.C = (1 - self.ccov1 - self.ccovmu * np.sum(self.weights**2)) * self.C + self.ccov1 * np.outer(self.pc, self.pc) + self.ccovmu * np.dot(artmp, np.dot(np.diag(self.weights), artmp.T))\n            \n            self.sigma *= np.exp((self.cs / self.damps) * (np.linalg.norm(self.ps) / self.chiN - 1))\n\n            self.C = np.triu(self.C) + np.triu(self.C, 1).T\n            \n            if np.min(np.diag(self.C)) <= 0:\n                self.C = np.eye(self.dim)\n            \n            if self.eval_count >= self.budget:\n                break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "In the code, line 148, in _raise_linalgerror_nonposdef, the following error occurred:\nLinAlgError: Matrix is not positive definite", "error": "In the code, line 148, in _raise_linalgerror_nonposdef, the following error occurred:\nLinAlgError: Matrix is not positive definite", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "1670eb65-d074-44a6-a45b-a6dfca80a870", "fitness": 0.30376465308760403, "name": "PopulationBasedSimulatedAnnealing", "description": "No description provided.", "code": "import numpy as np\n\nclass PopulationBasedSimulatedAnnealing:\n    def __init__(self, budget=10000, dim=10, pop_size=50, initial_temp=100.0, cooling_rate=0.95, mutation_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.initial_temp = initial_temp\n        self.cooling_rate = cooling_rate\n        self.mutation_rate = mutation_rate\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        \n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n        \n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index]\n\n        temperature = self.initial_temp\n        \n        while self.budget > 0 and temperature > 1e-5:\n            for i in range(self.pop_size):\n                # Mutation\n                mutation = np.random.normal(0, self.mutation_rate * (func.bounds.ub - func.bounds.lb), size=self.dim)\n                mutant = population[i] + mutation\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Evaluate mutant\n                f_mutant = func(mutant)\n                self.budget -= 1\n\n                # Acceptance probability\n                delta_e = f_mutant - fitness[i]\n                if delta_e < 0:\n                    population[i] = mutant\n                    fitness[i] = f_mutant\n\n                    if f_mutant < self.f_opt:\n                        self.f_opt = f_mutant\n                        self.x_opt = mutant\n                else:\n                    acceptance_probability = np.exp(-delta_e / temperature)\n                    if np.random.rand() < acceptance_probability:\n                        population[i] = mutant\n                        fitness[i] = f_mutant\n\n            # Cooling\n            temperature *= self.cooling_rate\n            \n            best_index = np.argmin(fitness)\n            if fitness[best_index] < self.f_opt:\n                self.f_opt = fitness[best_index]\n                self.x_opt = population[best_index]\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm PopulationBasedSimulatedAnnealing scored 0.304 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.13654946943617197, 0.24554397932135497, 0.2831458247482801, 0.24412534373832773, 0.22425011547850704, 0.2654081909051622, 0.2542259621463794, 0.23478531313862372, 0.2229589543093109, 0.17173572776208335, 0.2505666636501167, 0.9819147011681826, 0.2801745116297907, 0.23711958163029445, 0.5851574680628664, 0.28864711659758646, 0.24073922778054746, 0.2968180296582018, 0.16692617673057153, 0.4645007038597204]}, "task_prompt": ""}
{"id": "4b4e07bc-d13b-44e9-97a7-a0a0ba298ce2", "fitness": 0.10304885167285979, "name": "EnhancedCMAES", "description": "No description provided.", "code": "import numpy as np\n\nclass EnhancedCMAES:\n    def __init__(self, budget=10000, dim=10, pop_size=None, sigma0=0.5, cs=0.3, c_cov=0.05, restarts=3):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size if pop_size is not None else 4 + int(3 * np.log(dim))\n        self.sigma0 = sigma0\n        self.cs = cs\n        self.c_cov = c_cov\n        self.restarts = restarts\n        self.lb = -5.0\n        self.ub = 5.0\n        self.mean = None\n        self.sigma = None\n        self.C = None\n        self.D = None\n        self.B = None\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.restart_count = 0\n        self.stagnation_counter = 0\n        self.stagnation_threshold = 500\n\n    def initialize(self):\n        self.mean = np.random.uniform(self.lb, self.ub, size=self.dim)\n        self.sigma = self.sigma0\n        self.C = np.eye(self.dim)\n        self.D, self.B = np.linalg.eig(self.C)\n        self.D = np.sqrt(self.D)\n        self.stagnation_counter = 0\n\n    def sample_population(self):\n        z = np.random.randn(self.dim, self.pop_size)\n        x = self.mean[:, np.newaxis] + self.sigma * (self.B @ (self.D[:, np.newaxis] * z))\n        x = np.clip(x, self.lb, self.ub)\n        return x.T\n\n    def update_distribution(self, x, fitness):\n        idx = np.argsort(fitness)\n        x_sorted = x[idx]\n        weights = np.log(self.pop_size + 1) - np.log(np.arange(1, self.pop_size + 1))\n        weights = weights / np.sum(weights)\n\n        delta_mean = np.sum(weights[:, np.newaxis] * (x_sorted - self.mean), axis=0)\n        self.mean = self.mean + self.cs * delta_mean\n\n        z = (x_sorted - self.mean) / self.sigma\n        C_update = np.sum(weights[:, np.newaxis, np.newaxis] * z[:, :, np.newaxis] * z[:, np.newaxis, :], axis=0)\n        self.C = (1 - self.c_cov) * self.C + self.c_cov * C_update\n\n        try:\n            self.D, self.B = np.linalg.eig(self.C)\n            self.D = np.sqrt(self.D)\n        except np.linalg.LinAlgError:\n            self.C = np.eye(self.dim)\n            self.D = np.ones(self.dim)\n            self.B = np.eye(self.dim)\n\n        self.sigma = self.sigma * np.exp(self.cs / 0.414 * (np.linalg.norm(delta_mean / self.sigma) - np.sqrt(self.dim)))\n        self.sigma = min(self.sigma, 5)\n        self.sigma = max(self.sigma, 1e-6)\n        \n        if np.allclose(delta_mean, 0):\n            self.stagnation_counter += 1\n        else:\n            self.stagnation_counter = 0\n\n    def __call__(self, func):\n        self.initialize()\n\n        while self.eval_count < self.budget:\n            x = self.sample_population()\n            fitness = np.array([func(xi) for xi in x])\n            self.eval_count += self.pop_size\n\n            best_index = np.argmin(fitness)\n            if fitness[best_index] < self.f_opt:\n                self.f_opt = fitness[best_index]\n                self.x_opt = x[best_index]\n\n            self.update_distribution(x, fitness)\n\n            if self.stagnation_counter > self.stagnation_threshold and self.restart_count < self.restarts:\n                self.initialize()\n                self.restart_count += 1\n                self.stagnation_counter = 0\n                \n\n            if self.eval_count >= self.budget:\n                break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm EnhancedCMAES scored 0.103 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.001614573916902562, 0.057868219460980974, 0.18600156710176174, 0.09202173059941776, 0.019181292056799082, 0.1302694912942305, 0.07166495132216699, 0.041171795852053994, 0.12896924263907317, 0.07289315281043796, 0.0993159243445807, 0.12126634621791477, 9.999999999998899e-05, 0.020217527394156054, 0.126884030153538, 0.13114490364631803, 0.12912191349330715, 0.14558263267682992, 0.101267353758287, 0.38442038471843976]}, "task_prompt": ""}
{"id": "6de0532b-66b6-444a-9bd7-86b8a02ad77d", "fitness": 0.7001836323741794, "name": "SelfAdaptiveDifferentialEvolution", "description": "No description provided.", "code": "import numpy as np\n\nclass SelfAdaptiveDifferentialEvolution:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, Cr=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F  # Mutation factor\n        self.Cr = Cr  # Crossover rate\n        self.lb = -5.0\n        self.ub = 5.0\n        self.population = None\n        self.fitness = None\n        self.eval_count = 0\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.eval_count += self.pop_size\n        best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_index]\n        self.x_opt = self.population[best_index].copy()\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.eval_count < self.budget:\n            for i in range(self.pop_size):\n                # Mutation\n                indices = [j for j in range(self.pop_size) if j != i]\n                a, b, c = np.random.choice(indices, 3, replace=False)\n\n                # Self-adaptive F\n                F = np.random.normal(self.F, 0.1, 1)[0]\n                F = np.clip(F, 0.1, 1.0)\n\n                mutant = self.population[a] + F * (self.population[b] - self.population[c])\n                mutant = np.clip(mutant, self.lb, self.ub)\n\n                # Crossover\n                crossover_mask = np.random.rand(self.dim) < self.Cr\n                trial_vector = np.where(crossover_mask, mutant, self.population[i])\n\n                # Evaluation\n                f = func(trial_vector)\n                self.eval_count += 1\n\n                # Selection\n                if f < self.fitness[i]:\n                    self.fitness[i] = f\n                    self.population[i] = trial_vector.copy()\n\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = trial_vector.copy()\n                \n                if self.eval_count >= self.budget:\n                    break\n\n            # Population reduction (optional - makes algorithm more complex)\n            if self.pop_size > 10 and self.eval_count > self.budget * 0.75:\n                sorted_indices = np.argsort(self.fitness)[::-1]\n                reduce_amount = max(1, int(0.1 * self.pop_size))\n                self.population = self.population[sorted_indices[:-reduce_amount]]\n                self.fitness = self.fitness[sorted_indices[:-reduce_amount]]\n                self.pop_size = len(self.population)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm SelfAdaptiveDifferentialEvolution scored 0.700 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.3205870687494866, 0.6519525903447193, 0.6307690678528408, 0.8567866234961196, 0.7276125413871096, 0.7807668938271765, 0.6433108401407759, 0.6584438708902506, 0.7346665093198625, 0.6397000869961706, 0.8549675976699501, 0.9919299539202208, 0.6726051366419115, 0.7340529382920966, 0.9056594037032722, 0.7914209349093552, 0.6394823852151282, 0.8470252883788549, 0.40053763078500715, 0.5213952849632786]}, "task_prompt": ""}
{"id": "f696fed9-f8cc-4180-b2c8-a6000863eadf", "fitness": 0.41477371287536996, "name": "AdaptiveSimulatedAnnealing", "description": "No description provided.", "code": "import numpy as np\n\nclass AdaptiveSimulatedAnnealing:\n    def __init__(self, budget=10000, dim=10, initial_temp=100.0, step_size=1.0, cooling_rate=0.95):\n        self.budget = budget\n        self.dim = dim\n        self.initial_temp = initial_temp\n        self.step_size = step_size\n        self.cooling_rate = cooling_rate\n        self.lb = -5.0\n        self.ub = 5.0\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.temp = initial_temp\n        self.success_rate = 0.0\n        self.success_count = 0\n\n    def __call__(self, func):\n        self.x_opt = np.random.uniform(self.lb, self.ub, size=self.dim)\n        self.f_opt = func(self.x_opt)\n        self.eval_count += 1\n\n        while self.eval_count < self.budget:\n            # Generate neighbor\n            x_new = self.x_opt + np.random.normal(0, self.step_size, size=self.dim)\n            x_new = np.clip(x_new, self.lb, self.ub)\n\n            # Evaluate neighbor\n            f_new = func(x_new)\n            self.eval_count += 1\n\n            # Acceptance probability\n            delta_f = f_new - self.f_opt\n            if delta_f < 0 or np.random.rand() < np.exp(-delta_f / self.temp):\n                if delta_f < 0:\n                    self.success_count += 1\n                self.x_opt = x_new.copy()\n                self.f_opt = f_new\n\n            # Adaptive temperature and step size\n            self.temp *= self.cooling_rate\n            self.success_rate = self.success_count / self.eval_count if self.eval_count > 0 else 0\n            if self.success_rate > 0.15:\n                self.step_size *= 1.05  # Increase step size to explore further\n            elif self.success_rate < 0.01:\n                self.step_size *= 0.95  # Decrease step size to exploit more\n\n            if self.eval_count >= self.budget:\n                break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveSimulatedAnnealing scored 0.415 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.09561019932954551, 0.15459542800710502, 0.3779514547790762, 0.6042727026827052, 0.5352358140848124, 0.4727451381799669, 0.27443260334900477, 0.4003064801480102, 0.5683627592907334, 0.16731565097186518, 0.589169922635964, 0.9966422149645355, 0.297105307036988, 0.24861660337255131, 0.7603229285547363, 0.3026895843125921, 0.2849885335107104, 0.5199952321750779, 0.2082343822923831, 0.43688131782903483]}, "task_prompt": ""}
{"id": "b3917967-7394-4a34-bf79-78ba2315da41", "fitness": 0.23999403369584824, "name": "NoisyGradientDescent", "description": "No description provided.", "code": "import numpy as np\n\nclass NoisyGradientDescent:\n    def __init__(self, budget=10000, dim=10, step_size=0.1, noise_level=0.01):\n        self.budget = budget\n        self.dim = dim\n        self.step_size = step_size\n        self.noise_level = noise_level\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        x = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        self.x_opt = x\n        self.f_opt = func(x)\n        self.budget -= 1\n\n        while self.budget > 0:\n            # Estimate noisy gradient\n            grad = np.zeros(self.dim)\n            num_samples = min(self.dim * 2, self.budget)\n            for _ in range(num_samples):\n                delta = np.random.normal(0, self.noise_level, size=self.dim)\n                x_perturbed = x + delta\n                x_perturbed = np.clip(x_perturbed, func.bounds.lb, func.bounds.ub)\n\n                f_perturbed = func(x_perturbed)\n                self.budget -= 1\n                if self.budget <= 0:\n                    break\n\n                grad += (f_perturbed - self.f_opt) * delta\n            \n            if num_samples > 0:\n                grad /= num_samples * self.noise_level**2\n            \n            # Update position\n            x_new = x - self.step_size * grad + np.random.normal(0, self.noise_level, size=self.dim) # Add random perturbation\n            x_new = np.clip(x_new, func.bounds.lb, func.bounds.ub)\n            f_new = func(x_new)\n            self.budget -= 1\n            \n            if f_new < self.f_opt:\n                self.f_opt = f_new\n                self.x_opt = x_new\n                x = x_new\n                self.step_size *= 1.05 # Adaptive step size\n            else:\n                self.step_size *= 0.95 # Reduce step size\n        \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm NoisyGradientDescent scored 0.240 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.10901136773462405, 0.1751404178522169, 0.45855619418921856, 0.16606418082114593, 0.19358397783236625, 0.11488522586064986, 0.22713997242724826, 0.21234771870673064, 0.1378409681249475, 0.1375709636912441, 0.5664650797037185, 0.2913891920490075, 0.28443865382462674, 0.3014675272618722, 0.5638179702346071, 0.28650198954095474, 0.1580900219562381, 0.15391512966722476, 0.12832731474064274, 0.13332680769767935]}, "task_prompt": ""}
{"id": "d22e3b28-3de5-4352-bad0-720d62784873", "fitness": "-inf", "name": "CMAES", "description": "No description provided.", "code": "import numpy as np\n\nclass CMAES:\n    def __init__(self, budget=10000, dim=10, pop_size=None, sigma=0.5, cs=0.3, damps=1.0, c_cov_mean=None, c_cov_rank_one=None, mu_eff=None, restarts=5):\n        self.budget = budget\n        self.dim = dim\n        self.sigma = sigma\n        self.restarts = restarts\n        \n        if pop_size is None:\n            self.pop_size = 4 + int(3 * np.log(self.dim))\n        else:\n            self.pop_size = pop_size\n\n        self.mu = self.pop_size // 2\n        self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n        self.weights /= np.sum(self.weights)\n\n        self.mu_eff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n\n        self.c_sigma = (self.mu_eff + 2) / (self.dim + self.mu_eff + 5)\n        self.d_sigma = 1 + 2 * max(0, np.sqrt((self.mu_eff - 1) / (self.dim + 1)) - 1) + self.c_sigma\n\n        self.c_cov_mean = 1 / self.c_sigma / self.dim**(0.5) if c_cov_mean is None else c_cov_mean\n        self.c_cov_rank_one = 2 / ((self.dim + 1.3)**2 + self.mu_eff) if c_cov_rank_one is None else c_cov_rank_one\n        self.c_cov_rank_mu = min(1 - self.c_cov_rank_one, 2 * (self.mu_eff - 2 + 1 / self.mu_eff) / ((self.dim + 2)**2 + self.mu_eff))\n        self.damps = 1 + 2 * max(0, np.sqrt((self.mu_eff - 1) / (self.dim + 1)) - 1) + self.c_sigma if damps is None else damps\n\n        self.m = None\n        self.C = None\n        self.pc = None\n        self.ps = None\n        self.eval_count = 0\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def initialize(self, func):\n        self.m = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def sample_population(self, func):\n        z = np.random.randn(self.dim, self.pop_size)\n        x = self.m[:, np.newaxis] + self.sigma * np.dot(np.linalg.cholesky(self.C), z)\n        x = np.clip(x, func.bounds.lb, func.bounds.ub)\n        \n        fitness = np.array([func(xi) for xi in x.T])\n        self.eval_count += self.pop_size\n        return x, fitness\n\n    def update_distribution(self, x, fitness):\n        indices = np.argsort(fitness)\n        x_mu = x[:, indices[:self.mu]]\n        \n        m_old = self.m.copy()\n        self.m = np.dot(x_mu, self.weights)\n\n        self.ps = (1 - self.c_sigma) * self.ps + np.sqrt(self.c_sigma * (2 - self.c_sigma) * self.mu_eff) * np.dot(np.linalg.inv(np.linalg.cholesky(self.C)), (self.m - m_old)) / self.sigma\n        self.pc = (1 - self.c_cov_mean) * self.pc + np.sqrt(self.c_cov_mean * (2 - self.c_cov_mean) * self.mu_eff) * (self.m - m_old) / self.sigma\n        \n        hsig = (np.linalg.norm(self.ps) / np.sqrt(1 - (1 - self.c_sigma)**(2 * self.eval_count / self.pop_size)) / np.sqrt(self.dim) < 1.4 + 2 / (self.dim + 1))\n        self.C = (1 - self.c_cov_rank_one - self.c_cov_rank_mu) * self.C + self.c_cov_rank_one * np.outer(self.pc, self.pc) + self.c_cov_rank_mu * np.dot(x_mu - m_old[:, np.newaxis], np.dot(np.diag(self.weights), (x_mu - m_old[:, np.newaxis]).T)) / self.sigma**2\n\n        self.sigma *= np.exp((self.c_sigma / self.d_sigma) * (np.linalg.norm(self.ps) / np.sqrt(self.dim) - 1))\n\n    def __call__(self, func):\n        self.initialize(func)\n\n        for _ in range(self.restarts):\n            while self.eval_count < self.budget:\n                x, fitness = self.sample_population(func)\n\n                best_index = np.argmin(fitness)\n                if fitness[best_index] < self.f_opt:\n                    self.f_opt = fitness[best_index]\n                    self.x_opt = x[:,best_index].copy()\n                \n                self.update_distribution(x, fitness)\n                \n                if np.linalg.det(self.C) <= 0 or np.isnan(np.linalg.det(self.C)):\n                    self.initialize(func)\n                    break\n                \n                if self.eval_count > self.budget * 0.8 and self.pop_size > 10:\n                     self.pop_size = max(10, self.pop_size // 2)\n                     self.mu = self.pop_size // 2\n                     self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n                     self.weights /= np.sum(self.weights)\n                     self.mu_eff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "In the code, line 113, in _clip, the following error occurred:\nValueError: operands could not be broadcast together with shapes (2,6) (2,) (2,) ", "error": "In the code, line 113, in _clip, the following error occurred:\nValueError: operands could not be broadcast together with shapes (2,6) (2,) (2,) ", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "2a14556f-1bdb-4092-9389-6aeb05d0fb0e", "fitness": 0.3373100712012318, "name": "SimplifiedCMAES", "description": "No description provided.", "code": "import numpy as np\n\nclass SimplifiedCMAES:\n    def __init__(self, budget=10000, dim=10, pop_size=None, sigma0=1.0, cs=0.3, c_cov=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size if pop_size is not None else 4 + int(3 * np.log(dim))\n        self.sigma = sigma0\n        self.cs = cs\n        self.c_cov = c_cov\n        self.lb = -5.0\n        self.ub = 5.0\n        self.mean = np.random.uniform(self.lb, self.ub, size=self.dim)\n        self.C = np.eye(self.dim)\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.path_s = np.zeros(self.dim)\n\n    def sample_population(self):\n        z = np.random.normal(0, 1, size=(self.pop_size, self.dim))\n        return self.mean + self.sigma * np.dot(z, np.linalg.cholesky(self.C).T)\n\n    def repair(self, x):\n        return np.clip(x, self.lb, self.ub)\n\n    def __call__(self, func):\n        while self.eval_count < self.budget:\n            # Sample population\n            population = self.sample_population()\n            population = np.array([self.repair(x) for x in population])\n\n            # Evaluate population\n            fitness = np.array([func(x) for x in population])\n            self.eval_count += self.pop_size\n            \n            # Find best individual\n            best_index = np.argmin(fitness)\n            if fitness[best_index] < self.f_opt:\n                self.f_opt = fitness[best_index]\n                self.x_opt = population[best_index].copy()\n\n            # Update mean\n            weights = np.zeros(self.pop_size)\n            weights[best_index] = 1.0  # Only best individual influences the mean\n            delta_mean = np.sum((weights * (population - self.mean).T).T, axis=0)\n            self.mean = self.mean + delta_mean\n\n            # Update evolution path\n            self.path_s = (1 - self.cs) * self.path_s + np.sqrt(self.cs * (2 - self.cs)) * delta_mean / self.sigma\n\n            # Update covariance matrix\n            self.C = (1 - self.c_cov) * self.C + self.c_cov * np.outer(self.path_s, self.path_s)\n            \n            # Ensure C is positive definite (numerical stability)\n            try:\n                np.linalg.cholesky(self.C)\n            except np.linalg.LinAlgError:\n                self.C = self.C + 1e-6 * np.eye(self.dim)\n\n            # Update step size\n            self.sigma *= np.exp(self.cs/0.881 * (np.linalg.norm(self.path_s)/np.sqrt(self.dim) - 1))\n\n            if self.eval_count >= self.budget:\n                break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm SimplifiedCMAES scored 0.337 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.10298117177355648, 0.21023863820726496, 0.2533889512936447, 0.26115610966493785, 0.2732801492332708, 0.5292753676818227, 0.28386404325175607, 0.3919615238281774, 0.31503401089538907, 0.1273688808307757, 0.3371803703269758, 0.9974199525219419, 0.3049930323279708, 0.3142505063187686, 0.9853720497753676, 0.3352082858958142, 0.27001316985247026, 0.1746258953913673, 0.10247177690250175, 0.1761175380508625]}, "task_prompt": ""}
{"id": "ee2f8271-3aef-4dd4-bdff-0c92c54b489d", "fitness": 0.3775457881480267, "name": "AdaptiveSampling", "description": "No description provided.", "code": "import numpy as np\n\nclass AdaptiveSampling:\n    def __init__(self, budget=10000, dim=10, pop_size=20, initial_radius=1.0, success_threshold=0.2):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.initial_radius = initial_radius\n        self.success_threshold = success_threshold\n        self.lb = -5.0\n        self.ub = 5.0\n        self.population = None\n        self.fitness = None\n        self.radii = None\n        self.success_rates = None\n        self.eval_count = 0\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.eval_count += self.pop_size\n        best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_index]\n        self.x_opt = self.population[best_index].copy()\n        self.radii = np.full(self.pop_size, self.initial_radius)\n        self.success_rates = np.zeros(self.pop_size)\n\n    def sample_around(self, x, radius):\n        sample = x + np.random.normal(0, radius, self.dim)\n        return np.clip(sample, self.lb, self.ub)\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.eval_count < self.budget:\n            for i in range(self.pop_size):\n                # Sample around the current individual\n                new_x = self.sample_around(self.population[i], self.radii[i])\n                f = func(new_x)\n                self.eval_count += 1\n\n                if f < self.fitness[i]:\n                    self.success_rates[i] = min(1.0, self.success_rates[i] + 0.1)\n                    self.fitness[i] = f\n                    self.population[i] = new_x.copy()\n\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = new_x.copy()\n                else:\n                    self.success_rates[i] = max(0.0, self.success_rates[i] - 0.1)\n\n                # Adjust the radius based on success rate\n                if self.success_rates[i] > self.success_threshold:\n                    self.radii[i] *= 1.1  # Expand search\n                else:\n                    self.radii[i] *= 0.9  # Narrow search\n                self.radii[i] = np.clip(self.radii[i], 1e-6, self.ub - self.lb)\n\n                if self.eval_count >= self.budget:\n                    break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveSampling scored 0.378 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.10360437149528734, 0.2602566806471387, 0.3763848264555688, 0.3026494537076415, 0.2659037136950466, 0.3872391016312965, 0.3190923334073412, 0.3576440304560512, 0.21805199102292572, 0.15886931761365142, 0.5218236518137371, 0.9988392375692495, 0.31329380739602153, 0.2338472453407351, 0.7445165464686945, 0.39890101470124173, 0.26225737766548896, 0.7028378378753347, 0.1590527158861743, 0.46585050811190576]}, "task_prompt": ""}
{"id": "c56204c2-db8b-446e-bd3f-d27a88a555d2", "fitness": "-inf", "name": "ModifiedCMAES", "description": "No description provided.", "code": "import numpy as np\n\nclass ModifiedCMAES:\n    def __init__(self, budget=10000, dim=10, pop_size=None, sigma=0.5, cs=0.3, damps=None, c_cov=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size if pop_size is not None else 4 + int(3 * np.log(dim))\n        self.sigma = sigma\n        self.cs = cs\n        self.damps = damps if damps is not None else 1 + dim / 2\n        self.c_cov = c_cov\n        self.lb = -5.0\n        self.ub = 5.0\n        self.mean = None\n        self.C = None\n        self.pc = None\n        self.eval_count = 0\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def initialize(self):\n        self.mean = np.random.uniform(self.lb, self.ub, size=self.dim)\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n\n    def sample_population(self):\n        z = np.random.multivariate_normal(np.zeros(self.dim), self.C, size=self.pop_size)\n        x = self.mean + self.sigma * z\n        return np.clip(x, self.lb, self.ub)\n\n    def update_parameters(self, x, fitness):\n        best_index = np.argmin(fitness)\n        y = (x[best_index] - self.mean) / self.sigma\n        self.pc = (1 - self.cs) * self.pc + np.sqrt(self.cs * (2 - self.cs)) * y\n        self.mean = x[best_index]\n        self.C = (1 - self.c_cov) * self.C + self.c_cov * (np.outer(self.pc, self.pc))\n\n    def __call__(self, func):\n        self.initialize()\n\n        while self.eval_count < self.budget:\n            x = self.sample_population()\n            fitness = np.array([func(xi) for xi in x])\n            self.eval_count += self.pop_size\n\n            best_index = np.argmin(fitness)\n            if fitness[best_index] < self.f_opt:\n                self.f_opt = fitness[best_index]\n                self.x_opt = x[best_index]\n            \n            self.update_parameters(x, fitness)\n\n            if self.eval_count >= self.budget:\n                break\n        return self.f_opt, self.x_opt\n# Description: Adapt CMA-ES by simplifying covariance matrix adaptation, focusing on rank-one updates with a dynamically adjusted step size and simplified parameter updates to balance exploration and exploitation more efficiently.\n# Code:\n```python\nimport numpy as np\n\nclass ModifiedCMAES:\n    def __init__(self, budget=10000, dim=10, pop_size=None, sigma=0.5, cs=0.3, damps=None, c_cov=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size if pop_size is not None else 4 + int(3 * np.log(dim))\n        self.sigma = sigma\n        self.cs = cs\n        self.damps = damps if damps is not None else 1 + dim / 2\n        self.c_cov = c_cov\n        self.lb = -5.0\n        self.ub = 5.0\n        self.mean = None\n        self.C = None\n        self.pc = None\n        self.eval_count = 0\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def initialize(self):\n        self.mean = np.random.uniform(self.lb, self.ub, size=self.dim)\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n\n    def sample_population(self):\n        z = np.random.multivariate_normal(np.zeros(self.dim), self.C, size=self.pop_size)\n        x = self.mean + self.sigma * z\n        return np.clip(x, self.lb, self.ub)\n\n    def update_parameters(self, x, fitness):\n        best_index = np.argmin(fitness)\n        y = (x[best_index] - self.mean) / self.sigma\n        self.pc = (1 - self.cs) * self.pc + np.sqrt(self.cs * (2 - self.cs)) * y\n        self.mean = x[best_index]\n        self.C = (1 - self.c_cov) * self.C + self.c_cov * (np.outer(self.pc, self.pc))\n        self.sigma *= np.exp(self.cs/self.damps*(np.linalg.norm(self.pc)/np.sqrt(self.dim)-1))\n\n    def __call__(self, func):\n        self.initialize()\n\n        while self.eval_count < self.budget:\n            x = self.sample_population()\n            fitness = np.array([func(xi) for xi in x])\n            self.eval_count += self.pop_size\n\n            best_index = np.argmin(fitness)\n            if fitness[best_index] < self.f_opt:\n                self.f_opt = fitness[best_index]\n                self.x_opt = x[best_index]\n            \n            self.update_parameters(x, fitness)\n\n            if self.eval_count >= self.budget:\n                break\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "In the code, line 143, in evaluate, the following error occurred:\nSyntaxError: invalid syntax", "error": "In the code, line 143, in evaluate, the following error occurred:\nSyntaxError: invalid syntax", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "1f42f2c7-78dd-4793-be39-38a22156cc0d", "fitness": 0.0, "name": "AdaptiveSimulatedAnnealing", "description": "No description provided.", "code": "import numpy as np\n\nclass AdaptiveSimulatedAnnealing:\n    def __init__(self, budget=10000, dim=10, initial_temp=100.0, cooling_rate=0.95):\n        self.budget = budget\n        self.dim = dim\n        self.initial_temp = initial_temp\n        self.cooling_rate = cooling_rate\n        self.lb = -5.0\n        self.ub = 5.0\n        self.eval_count = 0\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.current_x = None\n        self.current_f = None\n        self.temp = initial_temp\n\n    def initialize(self, func):\n        self.current_x = np.random.uniform(self.lb, self.ub, size=self.dim)\n        self.current_f = func(self.current_x)\n        self.eval_count += 1\n        self.f_opt = self.current_f\n        self.x_opt = self.current_x.copy()\n\n    def __call__(self, func):\n        self.initialize(func)\n\n        while self.eval_count < self.budget:\n            # Generate neighbor\n            neighbor_x = self.current_x + np.random.normal(0, self.temp/10, size=self.dim) # Scale perturbation by temperature\n            neighbor_x = np.clip(neighbor_x, self.lb, self.ub)\n            neighbor_f = func(neighbor_x)\n            self.eval_count += 1\n\n            # Acceptance probability\n            delta_f = neighbor_f - self.current_f\n            if delta_f < 0 or np.random.rand() < np.exp(-delta_f / self.temp):\n                self.current_x = neighbor_x\n                self.current_f = neighbor_f\n\n                if neighbor_f < self.f_opt:\n                    self.f_opt = neighbor_f\n                    self.x_opt = neighbor_x.copy()\n\n            # Adaptive Temperature Schedule\n            self.temp *= self.cooling_rate\n\n            #Random Restarts\n            if self.eval_count % (self.budget // 10) == 0:\n                new_x = np.random.uniform(self.lb, self.ub, size=self.dim)\n                new_f = func(new_x)\n                self.eval_count += 1\n                if new_f < self.f_opt:\n                  self.f_opt = new_f\n                  self.x_opt = new_x.copy()\n                self.current_x = new_x\n                self.current_f = new_f\n            if self.eval_count >= self.budget:\n                break\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveSimulatedAnnealing scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0]}, "task_prompt": ""}
{"id": "1ad0d9e3-5210-4114-a7b0-d2dcccef6295", "fitness": 0.46876670663269115, "name": "AdaptiveSimulatedAnnealing", "description": "No description provided.", "code": "import numpy as np\n\nclass AdaptiveSimulatedAnnealing:\n    def __init__(self, budget=10000, dim=10, initial_temp=100.0, cooling_rate=0.95, step_size=1.0):\n        self.budget = budget\n        self.dim = dim\n        self.initial_temp = initial_temp\n        self.cooling_rate = cooling_rate\n        self.step_size = step_size\n        self.lb = -5.0\n        self.ub = 5.0\n        self.eval_count = 0\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.current_x = None\n        self.current_f = None\n        self.temp = initial_temp\n        self.success_count = 0\n        self.move_count = 0\n\n\n    def initialize(self, func):\n        self.current_x = np.random.uniform(self.lb, self.ub, size=self.dim)\n        self.current_f = func(self.current_x)\n        self.eval_count += 1\n        self.f_opt = self.current_f\n        self.x_opt = self.current_x.copy()\n\n    def __call__(self, func):\n        self.initialize(func)\n\n        while self.eval_count < self.budget:\n            # Generate a new candidate solution\n            new_x = self.current_x + np.random.uniform(-self.step_size, self.step_size, size=self.dim)\n            new_x = np.clip(new_x, self.lb, self.ub)\n            new_f = func(new_x)\n            self.eval_count += 1\n\n            # Acceptance probability\n            delta_f = new_f - self.current_f\n            if delta_f < 0 or np.random.rand() < np.exp(-delta_f / self.temp):\n                self.current_x = new_x.copy()\n                self.current_f = new_f\n                self.success_count += 1\n\n                if new_f < self.f_opt:\n                    self.f_opt = new_f\n                    self.x_opt = new_x.copy()\n\n            self.move_count += 1\n\n            # Adaptive temperature and step size\n            self.temp *= self.cooling_rate\n            if self.move_count % 100 == 0:\n                success_rate = self.success_count / self.move_count\n                if success_rate > 0.6:\n                    self.step_size *= 1.1\n                elif success_rate < 0.4:\n                    self.step_size *= 0.9\n                self.success_count = 0\n                self.move_count = 0\n                self.step_size = np.clip(self.step_size, 0.01, 2.0)\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveSimulatedAnnealing scored 0.469 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.149558722931883, 0.579950996271431, 0.5183417570606311, 0.19844424850448317, 0.4973990929098041, 0.554331552911634, 0.28596510438145595, 0.44105671252976264, 0.5054798272556459, 0.18645484349973995, 0.8503004567108423, 0.9907435388625544, 0.22377556354344208, 0.39237898430657125, 0.6332194506550445, 0.3503734374813363, 0.4201328427600556, 0.7054782331359345, 0.39193331442698254, 0.5000154525145903]}, "task_prompt": ""}
{"id": "6c4c48b1-54d5-4327-a80d-8f14e626eba0", "fitness": 0.4143133368589812, "name": "AdaptiveDifferentialEvolutionRepair", "description": "No description provided.", "code": "import numpy as np\n\nclass AdaptiveDifferentialEvolutionRepair:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F_init=0.5, CR=0.7, F_adapt=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F_init\n        self.CR = CR\n        self.F_adapt = F_adapt\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        \n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n        \n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index]\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = a + self.F * (b - c)\n                \n                # Repair mechanism\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                f = func(trial)\n                self.budget -= 1\n                if f < fitness[i]:\n                    fitness[i] = f\n                    population[i] = trial\n\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = trial\n                        self.F = max(0, self.F - self.F_adapt) # Adaptive F\n                else:\n                     self.F = min(1, self.F + self.F_adapt) # Adaptive F\n            \n            best_index = np.argmin(fitness)\n            if fitness[best_index] < self.f_opt:\n                self.f_opt = fitness[best_index]\n                self.x_opt = population[best_index]    \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveDifferentialEvolutionRepair scored 0.414 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.1585442865254838, 0.2236622721281858, 0.38487119432808825, 0.5954630690129039, 0.35642120332146854, 0.45973156214535216, 0.3261120416455051, 0.35712219129975464, 0.3557643404039116, 0.26241623274239956, 0.4062522957936022, 0.9960504286681212, 0.259590665520389, 0.3304512414320405, 0.7571778832605607, 0.4970964456657102, 0.316716313261851, 0.5621629978114189, 0.19278011851144605, 0.48787995370143056]}, "task_prompt": ""}
{"id": "3d11bf99-3e25-4ce1-8a6b-8d00a6b3eca4", "fitness": "-inf", "name": "HybridDE_NM", "description": "No description provided.", "code": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridDE_NM:\n    def __init__(self, budget=10000, dim=10, pop_size=40, F=0.7, Cr=0.9, local_search_freq=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.Cr = Cr\n        self.lb = -5.0\n        self.ub = 5.0\n        self.population = None\n        self.fitness = None\n        self.eval_count = 0\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.local_search_freq = local_search_freq\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.eval_count += self.pop_size\n        best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_index]\n        self.x_opt = self.population[best_index].copy()\n\n    def local_search(self, func, x0):\n        res = minimize(func, x0, method='Nelder-Mead',\n                       bounds=[(self.lb, self.ub)] * self.dim,\n                       options={'maxiter': 50})  # Reduced maxiter to control budget\n        return res.fun, res.x, res.nit #Number of iterations\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.eval_count < self.budget:\n            for i in range(self.pop_size):\n                # Mutation\n                indices = [j for j in range(self.pop_size) if j != i]\n                a, b, c = np.random.choice(indices, 3, replace=False)\n                mutant = self.population[a] + self.F * (self.population[b] - self.population[c])\n                mutant = np.clip(mutant, self.lb, self.ub)\n\n                # Crossover\n                crossover_mask = np.random.rand(self.dim) < self.Cr\n                trial_vector = np.where(crossover_mask, mutant, self.population[i])\n\n                # Evaluation\n                f = func(trial_vector)\n                self.eval_count += 1\n\n                # Selection\n                if f < self.fitness[i]:\n                    self.fitness[i] = f\n                    self.population[i] = trial_vector.copy()\n\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = trial_vector.copy()\n\n                # Local Search\n                if np.random.rand() < self.local_search_freq and self.eval_count < self.budget:\n                    f_local, x_local, niter = self.local_search(func, self.population[i])\n                    self.eval_count += niter\n                    if f_local < self.fitness[i]:\n                        self.fitness[i] = f_local\n                        self.population[i] = x_local.copy()\n\n                        if f_local < self.f_opt:\n                            self.f_opt = f_local\n                            self.x_opt = x_local.copy()\n                            \n\n                if self.eval_count >= self.budget:\n                    break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "In the code, line 29, in local_search, the following error occurred:\nNameError: name 'minimize' is not defined\nOn line: res = minimize(func, x0, method='Nelder-Mead',", "error": "In the code, line 29, in local_search, the following error occurred:\nNameError: name 'minimize' is not defined\nOn line: res = minimize(func, x0, method='Nelder-Mead',", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "44eeb03d-eca2-4c8f-8987-32a186889514", "fitness": "-inf", "name": "HybridPSO_DE", "description": "No description provided.", "code": "import numpy as np\n\nclass HybridPSO_DE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, w=0.7, c1=1.5, c2=1.5, F=0.8, Cr=0.7):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.w = w  # Inertia weight for PSO\n        self.c1 = c1  # Cognitive coefficient for PSO\n        self.c2 = c2  # Social coefficient for PSO\n        self.F = F  # Mutation factor for DE\n        self.Cr = Cr  # Crossover rate for DE\n        self.lb = -5.0\n        self.ub = 5.0\n        self.population = None\n        self.velocity = None\n        self.fitness = None\n        self.personal_best_positions = None\n        self.personal_best_fitness = None\n        self.global_best_position = None\n        self.global_best_fitness = np.inf\n        self.eval_count = 0\n        self.pso_phase = True  # Start with PSO\n        self.switch_threshold = 0.9  # Threshold for switching\n        self.success_rate = 0.0\n        self.success_history = []\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.velocity = np.random.uniform(-1, 1, size=(self.pop_size, self.dim))  # Initialize velocity\n        self.fitness = np.array([func(x) for x in self.population])\n        self.eval_count += self.pop_size\n        self.personal_best_positions = self.population.copy()\n        self.personal_best_fitness = self.fitness.copy()\n        best_index = np.argmin(self.fitness)\n        self.global_best_position = self.population[best_index].copy()\n        self.global_best_fitness = self.fitness[best_index]\n\n    def update_success_rate(self, success):\n        self.success_history.append(success)\n        if len(self.success_history) > 20:\n            self.success_history = self.success_history[-20:]  # Keep only the last 20 values\n        self.success_rate = np.mean(self.success_history)\n\n\n    def pso_step(self, func):\n        for i in range(self.pop_size):\n            r1 = np.random.rand(self.dim)\n            r2 = np.random.rand(self.dim)\n            self.velocity[i] = (self.w * self.velocity[i]\n                               + self.c1 * r1 * (self.personal_best_positions[i] - self.population[i])\n                               + self.c2 * r2 * (self.global_best_position - self.population[i]))\n            self.population[i] += self.velocity[i]\n            self.population[i] = np.clip(self.population[i], self.lb, self.ub)\n\n            f = func(self.population[i])\n            self.eval_count += 1\n\n            if f < self.fitness[i]:\n                self.fitness[i] = f\n                if f < self.personal_best_fitness[i]:\n                    self.personal_best_fitness[i] = f\n                    self.personal_best_positions[i] = self.population[i].copy()\n                    if f < self.global_best_fitness:\n                        self.global_best_fitness = f\n                        self.global_best_position = self.population[i].copy()\n                        self.update_success_rate(1) # Successful update\n            else:\n                self.update_success_rate(0) # Unsuccessful update\n\n    def de_step(self, func):\n        for i in range(self.pop_size):\n            indices = [j for j in range(self.pop_size) if j != i]\n            a, b, c = np.random.choice(indices, 3, replace=False)\n\n            mutant = self.population[a] + self.F * (self.population[b] - self.population[c])\n            mutant = np.clip(mutant, self.lb, self.ub)\n\n            crossover_mask = np.random.rand(self.dim) < self.Cr\n            trial_vector = np.where(crossover_mask, mutant, self.population[i])\n\n            f = func(trial_vector)\n            self.eval_count += 1\n\n            if f < self.fitness[i]:\n                self.fitness[i] = f\n                self.population[i] = trial_vector.copy()\n                if f < self.global_best_fitness:\n                    self.global_best_fitness = f\n                    self.global_best_position = trial_vector.copy()\n                    self.update_success_rate(1)  # Successful update\n            else:\n                self.update_success_rate(0)  # Unsuccessful update\n\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.eval_count < self.budget:\n            if self.pso_phase:\n                self.pso_step(func)\n            else:\n                self.de_step(func)\n\n            # Switching mechanism\n            if self.success_rate < self.switch_threshold:\n                self.pso_phase = not self.pso_phase  # Switch phase\n\n            if self.eval_count >= self.budget:\n                break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "In the code, line 112, in __call__, the following error occurred:\nAttributeError: 'HybridPSO_DE' object has no attribute 'f_opt'\nOn line: return self.f_opt, self.x_opt", "error": "In the code, line 112, in __call__, the following error occurred:\nAttributeError: 'HybridPSO_DE' object has no attribute 'f_opt'\nOn line: return self.f_opt, self.x_opt", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "2f9820d9-3dbc-47fd-a666-0b2d24a50ada", "fitness": "-inf", "name": "HybridDE_NM", "description": "No description provided.", "code": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridDE_NM:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, Cr=0.9, nm_iterations=3):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.Cr = Cr\n        self.lb = -5.0\n        self.ub = 5.0\n        self.population = None\n        self.fitness = None\n        self.eval_count = 0\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.nm_iterations = nm_iterations\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.eval_count += self.pop_size\n        best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_index]\n        self.x_opt = self.population[best_index].copy()\n\n    def nelder_mead_refinement(self, func, x):\n        bounds = [(self.lb, self.ub)] * self.dim\n        result = minimize(func, x, method='Nelder-Mead', bounds=bounds, options={'maxiter': self.nm_iterations, 'maxfev':self.budget - self.eval_count})\n        \n        return result.fun, result.x, result.nfev\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.eval_count < self.budget:\n            for i in range(self.pop_size):\n                # Mutation\n                indices = [j for j in range(self.pop_size) if j != i]\n                a, b, c = np.random.choice(indices, 3, replace=False)\n\n                mutant = self.population[a] + self.F * (self.population[b] - self.population[c])\n                mutant = np.clip(mutant, self.lb, self.ub)\n\n                # Crossover\n                crossover_mask = np.random.rand(self.dim) < self.Cr\n                trial_vector = np.where(crossover_mask, mutant, self.population[i])\n\n                # Evaluation\n                f = func(trial_vector)\n                self.eval_count += 1\n\n                # Selection\n                if f < self.fitness[i]:\n                    self.fitness[i] = f\n                    self.population[i] = trial_vector.copy()\n\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = trial_vector.copy()\n\n                if self.eval_count >= self.budget:\n                    break\n\n            # Nelder-Mead on best individual\n            best_index = np.argmin(self.fitness)\n            f_nm, x_nm, nfev = self.nelder_mead_refinement(func, self.population[best_index])\n            self.eval_count += nfev\n            if f_nm < self.f_opt:\n                self.f_opt = f_nm\n                self.x_opt = x_nm.copy()\n            self.fitness[best_index] = f_nm\n            self.population[best_index] = x_nm.copy()\n\n            if self.eval_count >= self.budget:\n                break\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "In the code, line 30, in nelder_mead_refinement, the following error occurred:\nNameError: name 'minimize' is not defined\nOn line: result = minimize(func, x, method='Nelder-Mead', bounds=bounds, options={'maxiter': self.nm_iterations, 'maxfev':self.budget - self.eval_count})", "error": "In the code, line 30, in nelder_mead_refinement, the following error occurred:\nNameError: name 'minimize' is not defined\nOn line: result = minimize(func, x, method='Nelder-Mead', bounds=bounds, options={'maxiter': self.nm_iterations, 'maxfev':self.budget - self.eval_count})", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "b4d422d2-6cbf-42e7-9b9a-fbc766bc2b1a", "fitness": 0.441901657339787, "name": "ImprovedDifferentialEvolution", "description": "No description provided.", "code": "import numpy as np\n\nclass ImprovedDifferentialEvolution:\n    def __init__(self, budget=10000, dim=10, pop_size=100, F=0.3, Cr=0.95):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.Cr = Cr\n        self.lb = -5.0\n        self.ub = 5.0\n        self.population = None\n        self.fitness = None\n        self.eval_count = 0\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        # Mirrored sampling to enhance coverage\n        mirrored_population = self.lb + self.ub - self.population\n        self.population = np.concatenate((self.population, mirrored_population), axis=0)\n        self.pop_size = len(self.population)\n        self.fitness = np.array([func(x) for x in self.population])\n        self.eval_count += self.pop_size\n        best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_index]\n        self.x_opt = self.population[best_index].copy()\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.eval_count < self.budget:\n            for i in range(self.pop_size):\n                indices = [j for j in range(self.pop_size) if j != i]\n                a, b, c = np.random.choice(indices, 3, replace=False)\n\n                # Mutation\n                mutant = self.population[a] + self.F * (self.population[b] - self.population[c])\n                mutant = np.clip(mutant, self.lb, self.ub)\n\n                # Crossover\n                crossover_mask = np.random.rand(self.dim) < self.Cr\n                trial_vector = np.where(crossover_mask, mutant, self.population[i])\n\n                # Evaluation\n                f = func(trial_vector)\n                self.eval_count += 1\n\n                # Selection\n                if f < self.fitness[i]:\n                    self.fitness[i] = f\n                    self.population[i] = trial_vector.copy()\n\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = trial_vector.copy()\n\n                if self.eval_count >= self.budget:\n                    break\n\n            # Population reduction triggered earlier\n            if self.pop_size > 10 and self.eval_count > self.budget * 0.5:\n                sorted_indices = np.argsort(self.fitness)[::-1]\n                reduce_amount = max(1, int(0.1 * self.pop_size))\n                self.population = self.population[sorted_indices[:-reduce_amount]]\n                self.fitness = self.fitness[sorted_indices[:-reduce_amount]]\n                self.pop_size = len(self.population)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm ImprovedDifferentialEvolution scored 0.442 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.2021434222676446, 0.301118345153503, 0.38114609090242546, 0.6226113018890448, 0.3470941147612411, 0.47811209295654866, 0.31893737652476273, 0.37418093283865195, 0.36325157601989433, 0.25533736417120567, 0.6771765393527152, 0.9911860748352611, 0.3929642577058313, 0.3480919009599849, 0.7189578422291554, 0.4983420023031676, 0.3567215022733182, 0.5315238960810992, 0.19100680222393251, 0.48812971134635164]}, "task_prompt": ""}
{"id": "ce97b9bd-e1bb-4dfc-a94f-4e2f4f9e6318", "fitness": 0.30288075757019545, "name": "CMAES", "description": "No description provided.", "code": "import numpy as np\n\nclass CMAES:\n    def __init__(self, budget=10000, dim=10, pop_size=None, sigma=0.5, cs=0.3, damps=None, c_cov_mu=0.1, c_cov_one=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size if pop_size is not None else 4 + int(3 * np.log(dim))\n        self.sigma = sigma\n        self.mu = None\n        self.C = None\n        self.lb = -5.0\n        self.ub = 5.0\n        self.eval_count = 0\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.cs = cs\n\n        if damps is None:\n          self.damps = 1 + 2 * max(0, np.sqrt((self.pop_size - 1) / (self.dim + 1)) - 1) + self.cs\n        else:\n          self.damps = damps\n\n\n        self.c_cov_mu = c_cov_mu\n        self.c_cov_one = c_cov_one\n        self.weights = np.log(self.pop_size + 1) - np.log(np.arange(1, self.pop_size + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        self.c_s = (self.mueff + 2) / (self.dim + self.mueff + 5)\n        self.c_cov_mu = min(1 - self.c_s, self.c_cov_mu * (self.mueff / (self.dim + 13)))\n        self.c_cov_one = min(1 - self.c_s, self.c_cov_one * ((1 - self.c_cov_mu) * 2 * (self.mueff - 2 + 1 / self.mueff)) / ((self.dim + 2)**2 + self.mueff))\n\n        self.B = None\n        self.D = None\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.chiN = self.dim**0.5 * (1 - 1/(4*self.dim) + 1/(21*self.dim**2))\n\n\n    def sample_population(self):\n        z = np.random.randn(self.pop_size, self.dim)\n        y = np.dot(z, np.diag(self.D))\n        y = np.dot(y, self.B.T)\n        x = self.mu + self.sigma * y\n        return x\n\n    def repair(self, x):\n        return np.clip(x, self.lb, self.ub)\n\n    def __call__(self, func):\n        self.mu = np.random.uniform(self.lb, self.ub, size=self.dim)\n        self.C = np.eye(self.dim)\n        self.B = np.eye(self.dim)\n        self.D = np.ones(self.dim)\n\n        while self.eval_count < self.budget:\n            x = self.sample_population()\n            x = np.array([self.repair(xi) for xi in x])\n            fitness = np.array([func(xi) for xi in x])\n            self.eval_count += self.pop_size\n            \n            if np.min(fitness) < self.f_opt:\n              self.f_opt = np.min(fitness)\n              self.x_opt = x[np.argmin(fitness)]\n\n\n            idx = np.argsort(fitness)\n            fitness = fitness[idx]\n            x = x[idx]\n            y = (x - self.mu) / self.sigma\n\n            mu_old = self.mu.copy()\n            self.mu = np.sum(x[:self.pop_size] * self.weights[:, np.newaxis], axis=0)\n\n            ps_temp = np.sqrt(self.c_s * (2 - self.c_s) * self.mueff) * np.dot(self.B, np.dot(np.diag(self.D), np.mean(y[:self.pop_size], axis=0)))\n            self.ps = (1 - self.c_s) * self.ps + ps_temp\n\n            norm_ps = np.linalg.norm(self.ps)\n            self.sigma *= np.exp((self.c_s / self.damps) * (norm_ps / self.chiN - 1))\n\n            pc_temp = np.sqrt(self.c_cov_mu * (2 - self.c_cov_mu) * self.mueff) * (self.mu - mu_old) / self.sigma\n            self.pc = (1 - self.c_cov_mu) * self.pc + pc_temp\n\n            artifical_weights = self.weights.copy()\n            idx_bad_weights = artifical_weights * np.sum(y[:self.pop_size]**2, axis=1) > self.dim\n            artifical_weights[idx_bad_weights] = 0\n            \n            dC = (1-self.c_cov_one-self.c_cov_mu) * self.C\n            dC += self.c_cov_one * (np.outer(self.pc, self.pc) + (1 - self.c_s) * self.C)\n\n            y_weighted = y[:self.pop_size] * artifical_weights[:, np.newaxis]\n\n            dC += self.c_cov_mu * np.dot(y_weighted.T, y[:self.pop_size])\n\n            self.C = dC\n            \n            try:\n                self.D, self.B = np.linalg.eigh(self.C)\n                self.D = np.sqrt(np.maximum(self.D, 1e-16))\n            except np.linalg.LinAlgError:\n                self.C = np.eye(self.dim)\n                self.B = np.eye(self.dim)\n                self.D = np.ones(self.dim)\n                \n            if self.eval_count >= self.budget:\n                break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm CMAES scored 0.303 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.13735845391290746, 0.21312019394653126, 0.33075311571679655, 0.1799140001564098, 0.2081109819895447, 0.3440248981267424, 0.2573061229127165, 0.19584949524381767, 0.20459492499917042, 0.1536187452981983, 0.26691438697584957, 0.9678947382274588, 0.25655395688810845, 0.2424263392719911, 0.5343500550281901, 0.27051359752249504, 0.38027344877292923, 0.24038407732396683, 0.18400874540074874, 0.48964487368933673]}, "task_prompt": ""}
{"id": "05fe22ab-afb8-4f90-ad2f-1318f9bbf62e", "fitness": 0.15011637854795848, "name": "CMAES", "description": "No description provided.", "code": "import numpy as np\n\nclass CMAES:\n    def __init__(self, budget=10000, dim=10, pop_size=None, sigma0=0.2):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size if pop_size is not None else 4 + int(3 * np.log(dim))\n        self.sigma0 = sigma0\n        self.lb = -5.0\n        self.ub = 5.0\n        self.mu = self.pop_size // 2\n        self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n        self.weights /= np.sum(self.weights)\n        self.m = (self.ub + self.lb) / 2 * np.ones(self.dim)\n        self.C = np.eye(self.dim)\n        self.sigma = self.sigma0\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.chiN = np.sqrt(self.dim) * (1 - (1 / (4 * self.dim)) + 1 / (21 * self.dim**2))\n        self.c_sigma = (self.mu / (self.dim + self.mu)) / 2\n        self.d_sigma = 1 + 2 * max(0, np.sqrt((self.mu / (self.dim + self.mu)) * (self.dim - 1)) - 1) + self.c_sigma\n        self.c_c = (4 + self.mu / self.dim) / (self.dim + 4 + 2 * self.mu / self.dim)\n        self.c_1 = 2 / ((self.dim + 1.3)**2 + self.mu)\n        self.c_mu = min(1 - self.c_1, 2 * (self.mu - 2 + 1 / self.mu) / ((self.dim + 2)**2 + self.mu))\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.eval_count = 0\n\n    def sample_population(self):\n        z = np.random.normal(0, 1, size=(self.pop_size, self.dim))\n        return self.m + self.sigma * z @ np.linalg.cholesky(self.C).T\n\n    def repair(self, x):\n        return np.clip(x, self.lb, self.ub)\n\n    def __call__(self, func):\n        while self.eval_count < self.budget:\n            # Sample population\n            population = self.sample_population()\n            population = np.array([self.repair(x) for x in population])\n\n            # Evaluate population\n            fitness = np.array([func(x) for x in population])\n            self.eval_count += self.pop_size\n            if self.eval_count > self.budget:\n                fitness = fitness[:self.budget - (self.eval_count - self.pop_size)]\n                population = population[:self.budget - (self.eval_count - self.pop_size)]\n                self.pop_size = len(population)\n\n            # Sort by fitness\n            indices = np.argsort(fitness)\n            fitness = fitness[indices]\n            population = population[indices]\n\n            # Update best solution\n            if fitness[0] < self.f_opt:\n                self.f_opt = fitness[0]\n                self.x_opt = population[0]\n\n            # Update CMA-ES parameters\n            z = (population[:self.mu] - self.m) / self.sigma\n            y = z @ np.linalg.cholesky(self.C).T\n            self.m = np.sum(self.weights[:, None] * population[:self.mu], axis=0)\n\n            self.ps = (1 - self.c_sigma) * self.ps + np.sqrt(self.c_sigma * (2 - self.c_sigma)) * np.sqrt(self.mu) * (self.m - self.m) / self.sigma  # Using (m - old_m)\n\n            norm_ps = np.linalg.norm(self.ps)\n            self.sigma *= np.exp((self.c_sigma / self.d_sigma) * (norm_ps / self.chiN - 1))\n\n            self.pc = (1 - self.c_c) * self.pc + np.sqrt(self.c_c * (2 - self.c_c)) * np.sqrt(self.mu) * (self.m - self.m) / self.sigma\n\n            delta = population[:self.mu] - self.m\n            self.C = (1 - self.c_1 - self.c_mu) * self.C + self.c_1 * (self.pc[:, None] @ self.pc[None, :])\n            for i in range(self.mu):\n                self.C += self.c_mu * self.weights[i] * (delta[i][:, None] @ delta[i][None, :])\n            \n            if np.min(np.diag(self.C)) <= 0:\n                self.C += np.eye(self.dim) * 1e-10\n            \n            self.C = np.triu(self.C) + np.triu(self.C, k=1).T  # Enforce symmetry\n            \n            try:\n                np.linalg.cholesky(self.C)\n            except np.linalg.LinAlgError:\n                self.C += np.eye(self.dim) * 1e-8\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm CMAES scored 0.150 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.050931636962120996, 0.12236456395199835, 0.282647949908864, 0.102829864834171, 0.10632323738270211, 0.15592994762329304, 0.16780982041894144, 0.13164495682001598, 0.14717788117759933, 0.1449899828281932, 0.1565929678611263, 0.19013734898105694, 0.24930153813370448, 0.09562478300103283, 0.11430433814286023, 0.22273091172885606, 0.13534704280941368, 0.14461505687958165, 0.12793176815013774, 0.15309197336350044]}, "task_prompt": ""}
{"id": "d460f097-2ad8-41c4-8325-9a70fb1ce9b2", "fitness": 0.5489690230436197, "name": "HybridPSO_DE", "description": "No description provided.", "code": "import numpy as np\n\nclass HybridPSO_DE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, w=0.7, c1=1.5, c2=1.5, F=0.6, Cr=0.8):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.w = w  # Inertia weight for PSO\n        self.c1 = c1  # Cognitive coefficient for PSO\n        self.c2 = c2  # Social coefficient for PSO\n        self.F = F  # Mutation factor for DE\n        self.Cr = Cr  # Crossover rate for DE\n        self.lb = -5.0\n        self.ub = 5.0\n        self.population = None\n        self.velocities = None\n        self.fitness = None\n        self.personal_best_positions = None\n        self.personal_best_fitness = None\n        self.global_best_position = None\n        self.global_best_fitness = np.inf\n        self.eval_count = 0\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.de_probability = 0.5 # Probability of applying DE in each iteration\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.personal_best_positions = self.population.copy()\n        self.personal_best_fitness = self.fitness.copy()\n        self.global_best_position = self.population[np.argmin(self.fitness)].copy()\n        self.global_best_fitness = np.min(self.fitness)\n        self.eval_count += self.pop_size\n        self.f_opt = self.global_best_fitness\n        self.x_opt = self.global_best_position.copy()\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.eval_count < self.budget:\n            for i in range(self.pop_size):\n                if np.random.rand() < self.de_probability:\n                    # Differential Evolution\n                    indices = [j for j in range(self.pop_size) if j != i]\n                    a, b, c = np.random.choice(indices, 3, replace=False)\n\n                    mutant = self.population[a] + self.F * (self.population[b] - self.population[c])\n                    mutant = np.clip(mutant, self.lb, self.ub)\n\n                    crossover_mask = np.random.rand(self.dim) < self.Cr\n                    trial_vector = np.where(crossover_mask, mutant, self.population[i])\n\n                    f = func(trial_vector)\n                    self.eval_count += 1\n\n                    if f < self.fitness[i]:\n                        self.fitness[i] = f\n                        self.population[i] = trial_vector.copy()\n\n                        if f < self.personal_best_fitness[i]:\n                            self.personal_best_fitness[i] = f\n                            self.personal_best_positions[i] = trial_vector.copy()\n\n                        if f < self.global_best_fitness:\n                            self.global_best_fitness = f\n                            self.global_best_position = trial_vector.copy()\n                            self.f_opt = self.global_best_fitness\n                            self.x_opt = self.global_best_position.copy()\n\n                else:\n                    # Particle Swarm Optimization\n                    r1 = np.random.rand(self.dim)\n                    r2 = np.random.rand(self.dim)\n\n                    new_velocity = (self.w * self.velocities[i]\n                                    + self.c1 * r1 * (self.personal_best_positions[i] - self.population[i])\n                                    + self.c2 * r2 * (self.global_best_position - self.population[i]))\n\n                    self.velocities[i] = new_velocity\n                    new_position = self.population[i] + self.velocities[i]\n                    new_position = np.clip(new_position, self.lb, self.ub)\n\n                    f = func(new_position)\n                    self.eval_count += 1\n\n                    if f < self.fitness[i]:\n                        self.fitness[i] = f\n                        self.population[i] = new_position.copy()\n                        if f < self.personal_best_fitness[i]:\n                            self.personal_best_fitness[i] = f\n                            self.personal_best_positions[i] = new_position.copy()\n                        if f < self.global_best_fitness:\n                            self.global_best_fitness = f\n                            self.global_best_position = new_position.copy()\n                            self.f_opt = self.global_best_fitness\n                            self.x_opt = self.global_best_position.copy()\n\n                if self.eval_count >= self.budget:\n                    break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm HybridPSO_DE scored 0.549 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.19621952101437623, 0.36675023978641386, 0.46057698022048255, 0.7991993596123498, 0.5554572928557013, 0.6731145631696838, 0.39293411383322063, 0.46800695350471033, 0.5090595433403742, 0.4822885273460359, 0.8438439025091989, 0.9999124801896789, 0.3791354063362893, 0.498111479370694, 0.8748715017503108, 0.6154500731227088, 0.4067394677239857, 0.7423628950952641, 0.2076653786760091, 0.5076807814149089]}, "task_prompt": ""}
{"id": "548da3ce-d639-41ad-8427-aa257fb046de", "fitness": 0.2667194748617383, "name": "AdaptiveSimulatedAnnealing", "description": "No description provided.", "code": "import numpy as np\n\nclass AdaptiveSimulatedAnnealing:\n    def __init__(self, budget=10000, dim=10, initial_temp=1.0, alpha=0.99, temp_min=1e-5):\n        self.budget = budget\n        self.dim = dim\n        self.initial_temp = initial_temp\n        self.alpha = alpha\n        self.temp_min = temp_min\n        self.lb = -5.0\n        self.ub = 5.0\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.current_x = None\n        self.current_f = None\n        self.temp = initial_temp\n        self.acceptance_rate = 0.0\n\n    def initialize(self, func):\n        self.current_x = np.random.uniform(self.lb, self.ub, size=self.dim)\n        self.current_f = func(self.current_x)\n        self.eval_count += 1\n        self.f_opt = self.current_f\n        self.x_opt = self.current_x\n\n    def neighbor(self):\n        x_new = self.current_x + np.random.normal(0, 0.1, size=self.dim)  # Small perturbation\n        x_new = np.clip(x_new, self.lb, self.ub)  # Keep within bounds\n        return x_new\n\n    def acceptance_probability(self, delta_e):\n        if delta_e < 0:\n            return 1.0\n        else:\n            return np.exp(-delta_e / self.temp)\n\n    def update_temperature(self):\n        if self.acceptance_rate > 0.96:\n            self.alpha = 0.5\n        elif self.acceptance_rate < 0.04:\n            self.alpha = 0.99\n        else:\n            self.alpha = 0.95\n\n        self.temp = max(self.temp * self.alpha, self.temp_min)\n\n    def __call__(self, func):\n        self.initialize(func)\n        accepted_moves = 0\n        total_moves = 0\n\n        while self.eval_count < self.budget:\n            x_new = self.neighbor()\n            f_new = func(x_new)\n            self.eval_count += 1\n\n            delta_e = f_new - self.current_f\n            if self.acceptance_probability(delta_e) > np.random.rand():\n                self.current_x = x_new\n                self.current_f = f_new\n                accepted_moves += 1\n\n                if f_new < self.f_opt:\n                    self.f_opt = f_new\n                    self.x_opt = x_new\n            \n            total_moves +=1\n            self.acceptance_rate = accepted_moves / total_moves\n            self.update_temperature()\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveSimulatedAnnealing scored 0.267 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.05638030819283235, 0.15498024966272883, 0.23667488087549515, 0.1976835494680621, 0.15005374327738596, 0.12878113327624874, 0.24661761740047594, 0.12621231703669944, 0.13606811778087813, 0.12932371171541868, 0.9035607557870747, 0.17356482539888096, 0.26944608822003635, 0.18453107109596478, 0.7634218529865987, 0.34776264308664584, 0.326179955165665, 0.5809339185807323, 0.06041320643502113, 0.16179955179192018]}, "task_prompt": ""}
{"id": "695c9441-86ff-415d-bb1c-a1b72fbd674e", "fitness": 0.17677586906646486, "name": "AdaptiveSimulatedAnnealing", "description": "No description provided.", "code": "import numpy as np\n\nclass AdaptiveSimulatedAnnealing:\n    def __init__(self, budget=10000, dim=10, initial_temp=1.0, cooling_rate=0.95, temp_adjust_freq=50):\n        self.budget = budget\n        self.dim = dim\n        self.initial_temp = initial_temp\n        self.cooling_rate = cooling_rate\n        self.temp_adjust_freq = temp_adjust_freq\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        \n        x = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        f = func(x)\n        self.budget -= 1\n        self.f_opt = f\n        self.x_opt = x\n        \n        temp = self.initial_temp\n        acceptance_history = []\n\n        while self.budget > 0:\n            x_new = x + np.random.normal(0, 0.1, size=self.dim)  # Gaussian perturbation\n            x_new = np.clip(x_new, func.bounds.lb, func.bounds.ub)\n            f_new = func(x_new)\n            self.budget -= 1\n\n            delta_f = f_new - f\n            \n            if delta_f < 0 or np.random.rand() < np.exp(-delta_f / temp):\n                x = x_new\n                f = f_new\n                acceptance_history.append(1)\n                if f < self.f_opt:\n                    self.f_opt = f\n                    self.x_opt = x\n            else:\n                acceptance_history.append(0)\n\n            # Temperature adjustment based on acceptance rate\n            if len(acceptance_history) >= self.temp_adjust_freq:\n                acceptance_rate = np.mean(acceptance_history[-self.temp_adjust_freq:])\n                if acceptance_rate > 0.6:\n                    temp *= 1.1  # Increase temperature if accepting too often\n                elif acceptance_rate < 0.4:\n                    temp *= 0.9  # Decrease temperature if accepting too rarely\n\n            temp *= self.cooling_rate #Gradually reduce temperature\n                \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveSimulatedAnnealing scored 0.177 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.09223862843758801, 0.1553353757680399, 0.2418797177397598, 0.11604549697586974, 0.14526144515985528, 0.1662685182221474, 0.22369866447875197, 0.14303581398520415, 0.2361182341606981, 0.11899948933252513, 0.17901884798533207, 0.1652320952699753, 0.2541928082306155, 0.1766668938143261, 0.10618579732658073, 0.3022956417040604, 0.2616623838412112, 0.14020762718083535, 0.1531400011491778, 0.15803390056674327]}, "task_prompt": ""}
{"id": "0b501111-f8b8-461d-9057-1301ee7f3d84", "fitness": 0.3002242952374362, "name": "ParticleSwarmOptimizer", "description": "No description provided.", "code": "import numpy as np\n\nclass ParticleSwarmOptimizer:\n    def __init__(self, budget=10000, dim=10, pop_size=50, w=0.7, c1=1.5, c2=1.5, v_max=2.0):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.w = w  # Inertia weight\n        self.c1 = c1  # Cognitive coefficient\n        self.c2 = c2  # Social coefficient\n        self.v_max = v_max # Velocity clamping\n        self.lb = -5.0\n        self.ub = 5.0\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        # Initialize population and velocities\n        population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        velocities = np.random.uniform(-self.v_max, self.v_max, size=(self.pop_size, self.dim))\n        personal_best_positions = population.copy()\n        personal_best_fitnesses = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        # Find initial global best\n        global_best_index = np.argmin(personal_best_fitnesses)\n        global_best_position = personal_best_positions[global_best_index]\n        self.f_opt = personal_best_fitnesses[global_best_index]\n        self.x_opt = global_best_position\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Update velocity\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                velocities[i] = self.w * velocities[i] + \\\n                                self.c1 * r1 * (personal_best_positions[i] - population[i]) + \\\n                                self.c2 * r2 * (global_best_position - population[i])\n                \n                # Velocity clamping\n                velocities[i] = np.clip(velocities[i], -self.v_max, self.v_max)\n\n                # Update position\n                population[i] = population[i] + velocities[i]\n                population[i] = np.clip(population[i], self.lb, self.ub)  # Keep within bounds\n\n                # Evaluate fitness\n                f = func(population[i])\n                self.budget -= 1\n\n                # Update personal best\n                if f < personal_best_fitnesses[i]:\n                    personal_best_fitnesses[i] = f\n                    personal_best_positions[i] = population[i].copy()\n\n                    # Update global best\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = population[i].copy()\n\n                if self.budget <= 0:\n                    break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm ParticleSwarmOptimizer scored 0.300 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.13832695625760227, 0.17094991466751674, 0.27929837596629314, 0.2448621608679682, 0.20619674322270676, 0.3210847135995758, 0.26282991160985425, 0.203708333225618, 0.2332442124155989, 0.21483497049157652, 0.20950730656450167, 0.9958520646664901, 0.23469621104987004, 0.21393614533922012, 0.5851987902733489, 0.32852128328949337, 0.28136223446734177, 0.23041349451372284, 0.16957217353222553, 0.4800899087281999]}, "task_prompt": ""}
{"id": "850bc643-027d-4f91-9b3c-bb55061b32d6", "fitness": "-inf", "name": "AdaptiveSamplingRBF", "description": "No description provided.", "code": "import numpy as np\nfrom scipy.interpolate import Rbf\n\nclass AdaptiveSamplingRBF:\n    def __init__(self, budget=10000, dim=10, num_init_samples=50, rbf_epsilon=1.0):\n        self.budget = budget\n        self.dim = dim\n        self.num_init_samples = num_init_samples\n        self.rbf_epsilon = rbf_epsilon\n        self.lb = -5.0\n        self.ub = 5.0\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        # Initial sampling\n        X = np.random.uniform(self.lb, self.ub, size=(self.num_init_samples, self.dim))\n        y = np.array([func(x) for x in X])\n        self.budget -= self.num_init_samples\n\n        best_index = np.argmin(y)\n        self.f_opt = y[best_index]\n        self.x_opt = X[best_index]\n\n        while self.budget > 0:\n            # Train RBF model\n            rbf = Rbf(X, y, function='gaussian', epsilon=self.rbf_epsilon)\n\n            # Sample new points based on RBF prediction\n            num_new_samples = min(50, self.budget)  # Limit the number of new samples\n            X_new = np.random.uniform(self.lb, self.ub, size=(num_new_samples, self.dim))\n            y_pred = rbf(*X_new.T)\n\n            # Evaluate function at new points\n            y_new = np.array([func(x) for x in X_new])\n            self.budget -= num_new_samples\n\n            # Update the sample set\n            X = np.vstack((X, X_new))\n            y = np.hstack((y, y_new))\n\n            # Update best solution\n            best_index = np.argmin(y)\n            self.f_opt = y[best_index]\n            self.x_opt = X[best_index]\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "In the code, line 28, in __call__, the following error occurred:\nNameError: name 'Rbf' is not defined. Did you mean: 'rbf'?\nOn line: rbf = Rbf(X, y, function='gaussian', epsilon=self.rbf_epsilon)", "error": "In the code, line 28, in __call__, the following error occurred:\nNameError: name 'Rbf' is not defined. Did you mean: 'rbf'?\nOn line: rbf = Rbf(X, y, function='gaussian', epsilon=self.rbf_epsilon)", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "5efc6eec-4a0e-4a05-925d-a1d6342ebe5b", "fitness": 0.32377439539912806, "name": "EstimationOfDistributionAlgorithm", "description": "No description provided.", "code": "import numpy as np\nfrom scipy.stats import multivariate_normal\n\nclass EstimationOfDistributionAlgorithm:\n    def __init__(self, budget=10000, dim=10, pop_size=50, elite_frac=0.2):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.elite_frac = elite_frac\n        self.lb = -5.0\n        self.ub = 5.0\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        \n        # Initialize population\n        population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n        \n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index]\n\n        while self.budget > 0:\n            # Select elite individuals\n            num_elites = int(self.elite_frac * self.pop_size)\n            elite_indices = np.argsort(fitness)[:num_elites]\n            elites = population[elite_indices]\n\n            # Estimate distribution from elites\n            mean = np.mean(elites, axis=0)\n            covariance = np.cov(elites, rowvar=False)\n            \n            # Add a small constant to the diagonal to ensure positive definiteness\n            covariance += np.eye(self.dim) * 1e-6  \n\n            # Sample new population from the estimated distribution\n            try:\n                new_population = np.random.multivariate_normal(mean, covariance, self.pop_size)\n            except np.linalg.LinAlgError:\n                # If covariance is not positive definite, sample from a uniform distribution\n                new_population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n            \n            new_population = np.clip(new_population, self.lb, self.ub)\n\n            # Evaluate new population\n            new_fitness = np.array([func(x) for x in new_population])\n            self.budget -= self.pop_size\n            \n            # Update population and fitness\n            population = new_population\n            fitness = new_fitness\n            \n            # Update best solution\n            best_index = np.argmin(fitness)\n            if fitness[best_index] < self.f_opt:\n                self.f_opt = fitness[best_index]\n                self.x_opt = population[best_index]\n            \n            if self.budget <= 0:\n                break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm EstimationOfDistributionAlgorithm scored 0.324 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.16021397938918658, 0.2744274546784775, 0.32696403200189983, 0.216652164207211, 0.18550643412886025, 0.2441801049025636, 0.24881107897206411, 0.27987543061874154, 0.2788633196549597, 0.22680552725978687, 0.9680248852658316, 0.9903156701505786, 0.3089956055286427, 0.2110553979040355, 0.1730148950912055, 0.26327813747018713, 0.25832402831782886, 0.21211505238984296, 0.19020554739229634, 0.45785916265836146]}, "task_prompt": ""}
{"id": "f8cfe45d-6ee6-4252-a4cd-452a5a607bb3", "fitness": 0.3298777126969782, "name": "RankPerturbation", "description": "No description provided.", "code": "import numpy as np\n\nclass RankPerturbation:\n    def __init__(self, budget=10000, dim=10, pop_size=50, perturbation_scale=0.5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.perturbation_scale = perturbation_scale\n        self.lb = -5.0\n        self.ub = 5.0\n        self.population = None\n        self.fitness = None\n        self.eval_count = 0\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.eval_count += self.pop_size\n        best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_index]\n        self.x_opt = self.population[best_index].copy()\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.eval_count < self.budget:\n            # Rank the population based on fitness\n            ranked_indices = np.argsort(self.fitness)\n            ranked_population = self.population[ranked_indices]\n\n            for i in range(self.pop_size):\n                # Perturb each solution based on its rank\n                rank = np.where(ranked_indices == i)[0][0]\n                \n                # Scale the perturbation based on rank (better rank, smaller perturbation)\n                scale = self.perturbation_scale * (1 - (rank / self.pop_size))\n                \n                # Generate perturbation\n                perturbation = np.random.normal(0, scale, size=self.dim)\n\n                # Apply perturbation\n                mutant = ranked_population[i] + perturbation\n                mutant = np.clip(mutant, self.lb, self.ub)\n\n                # Evaluate the mutant\n                f = func(mutant)\n                self.eval_count += 1\n\n                # Update the population if the mutant is better\n                if f < self.fitness[ranked_indices[i]]:\n                    self.fitness[ranked_indices[i]] = f\n                    self.population[ranked_indices[i]] = mutant.copy()\n\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = mutant.copy()\n                        \n                if self.eval_count >= self.budget:\n                    break\n            \n            # Reduce perturbation scale\n            self.perturbation_scale *= 0.99\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm RankPerturbation scored 0.330 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.0977271240839136, 0.22060291465878257, 0.36395189541513184, 0.46934640355468593, 0.2655304030756094, 0.3241659207070572, 0.23835447354713812, 0.20211877029238046, 0.20354326031299452, 0.1636712243640328, 0.2403216848414239, 0.9998277860594357, 0.3233873538781986, 0.19335742668637934, 0.6754765259351885, 0.31080108888400415, 0.23696100700794864, 0.4030943437516402, 0.20184435697111636, 0.46347028991250294]}, "task_prompt": ""}
{"id": "7e0cb72c-e4e0-4fed-8ec7-a10e37880a76", "fitness": 0.3127223189673632, "name": "AdaptiveSimulatedAnnealing", "description": "No description provided.", "code": "import numpy as np\n\nclass AdaptiveSimulatedAnnealing:\n    def __init__(self, budget=10000, dim=10, temp_init=1.0, alpha=0.95):\n        self.budget = budget\n        self.dim = dim\n        self.temp = temp_init\n        self.alpha = alpha\n        self.x_current = None\n        self.f_current = np.inf\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        # Initialize solution\n        self.x_current = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        self.f_current = func(self.x_current)\n        self.budget -= 1\n        self.f_opt = self.f_current\n        self.x_opt = self.x_current\n\n        improvements = 0\n        iterations = 0\n\n        while self.budget > 0:\n            # Generate neighbor solution\n            x_new = self.x_current + np.random.normal(0, 0.1, size=self.dim)\n            x_new = np.clip(x_new, func.bounds.lb, func.bounds.ub)\n\n            # Evaluate neighbor solution\n            f_new = func(x_new)\n            self.budget -= 1\n\n            # Acceptance probability\n            delta_f = f_new - self.f_current\n            if delta_f < 0:\n                # Accept better solution\n                self.x_current = x_new\n                self.f_current = f_new\n                improvements += 1\n\n                if f_new < self.f_opt:\n                    self.f_opt = f_new\n                    self.x_opt = x_new\n            else:\n                # Accept worse solution with probability\n                prob = np.exp(-delta_f / self.temp)\n                if np.random.rand() < prob:\n                    self.x_current = x_new\n                    self.f_current = f_new\n\n            iterations += 1\n\n            # Adaptive temperature schedule\n            if iterations % 100 == 0:\n                improvement_rate = improvements / 100\n                if improvement_rate > 0.1:\n                    self.temp *= self.alpha # cool down\n                else:\n                    self.temp /= self.alpha #heat up\n                self.temp = np.clip(self.temp, 0.0001, 100)\n                improvements = 0\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveSimulatedAnnealing scored 0.313 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.11806623065746402, 0.23774400059103817, 0.31307199808827335, 0.1886616350721615, 0.27879794705051664, 0.333646474426019, 0.24447843904924538, 0.2925072956084156, 0.29278373142922587, 0.14491335220969537, 0.3509638844303442, 0.6930521356845609, 0.2526904402858644, 0.2864301629448932, 0.6083906903968972, 0.29847385020324546, 0.29415339746277236, 0.37644341523391067, 0.19560918488972445, 0.453568113632996]}, "task_prompt": ""}
{"id": "5ca3f2a9-ac0c-4b7f-977d-4d2d15a990bc", "fitness": 0.42700697803519316, "name": "GaussianEstimationOfDistribution", "description": "No description provided.", "code": "import numpy as np\nfrom scipy.stats import norm\n\nclass GaussianEstimationOfDistribution:\n    def __init__(self, budget=10000, dim=10, pop_size=50, selection_threshold=0.2):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.selection_threshold = selection_threshold\n        self.lb = -5.0\n        self.ub = 5.0\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        \n        # Initialize population\n        population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n        \n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index]\n\n        while self.budget > 0:\n            # Select promising solutions\n            threshold = np.quantile(fitness, self.selection_threshold)\n            selected_indices = np.where(fitness <= threshold)[0]\n            selected_population = population[selected_indices]\n\n            # Estimate mean and standard deviation\n            if len(selected_population) > 0:\n                mean = np.mean(selected_population, axis=0)\n                std = np.std(selected_population, axis=0)\n            else:\n                mean = np.mean(population, axis=0)\n                std = np.std(population, axis=0)\n\n            # Handle zero standard deviation\n            std = np.where(std == 0, 1.0, std)\n\n            # Generate new samples from Gaussian distribution\n            new_population = np.random.normal(mean, std, size=(self.pop_size, self.dim))\n            new_population = np.clip(new_population, self.lb, self.ub)\n\n            # Evaluate new population\n            new_fitness = np.array([func(x) for x in new_population])\n            self.budget -= self.pop_size\n            \n            # Update population and fitness\n            population = new_population\n            fitness = new_fitness\n\n            # Update best solution\n            best_index = np.argmin(fitness)\n            if fitness[best_index] < self.f_opt:\n                self.f_opt = fitness[best_index]\n                self.x_opt = population[best_index]\n                \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm GaussianEstimationOfDistribution scored 0.427 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.2363169879426953, 0.35921273830928824, 0.311862891144878, 0.2653247445962078, 0.6023008458241195, 0.3178553019462472, 0.6279614337322418, 0.4334984645065535, 0.9383706378822662, 0.20934657059738315, 0.29888240184332326, 0.994434312779538, 0.24635608972638734, 0.29323290921536305, 0.702095208601297, 0.3189824289790286, 0.2757810245972574, 0.3847728355200297, 0.22068848454565526, 0.5028632484141031]}, "task_prompt": ""}
{"id": "d86cad9d-10cd-4a5a-a9cc-06fafdd92b86", "fitness": "-inf", "name": "BudgetAwareCMAES", "description": "No description provided.", "code": "import numpy as np\n\nclass BudgetAwareCMAES:\n    def __init__(self, budget=10000, dim=10, pop_size=None, sigma0=0.5, cs=0.3, dsigma=0.2):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size if pop_size is not None else 4 + int(3 * np.log(dim))\n        self.sigma = sigma0\n        self.cs = cs\n        self.dsigma = dsigma\n        self.lb = -5.0\n        self.ub = 5.0\n        self.eval_count = 0\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.mean = np.random.uniform(self.lb, self.ub, size=self.dim)\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.chiN = np.sqrt(self.dim) * (1 - (1 / (4 * self.dim)) + (1 / (21 * self.dim**2)))\n        self.mu = self.pop_size // 2\n        self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        self.cc = (4 + self.mueff / self.dim) / (self.dim + 4 + 2 * self.mueff / self.dim)\n        self.c1 = 2 / ((self.dim + 1.3)**2 + self.mueff)\n        self.cmu = min(1 - self.c1, 2 * (self.mueff - 2 + 1 / self.mueff) / ((self.dim + 2)**2 + self.mueff))\n        self.c_sigma = (self.mueff + 2) / (self.dim + self.mueff + 5)\n        self.restart_trigger = self.budget // 5\n\n    def sample_population(self):\n        z = np.random.randn(self.pop_size, self.dim)\n        return self.mean + self.sigma * np.dot(z, np.linalg.cholesky(self.C).T)\n\n    def repair(self, x):\n        return np.clip(x, self.lb, self.ub)\n\n    def __call__(self, func):\n        while self.eval_count < self.budget:\n            population = self.sample_population()\n            population = np.array([self.repair(x) for x in population])\n            fitness = np.array([func(x) for x in population])\n            self.eval_count += self.pop_size\n\n            if np.min(fitness) < self.f_opt:\n                self.f_opt = np.min(fitness)\n                self.x_opt = population[np.argmin(fitness)]\n            \n            idx = np.argsort(fitness)\n            best_individuals = population[idx[:self.mu]]\n            \n            y = best_individuals - self.mean\n            delta_mean = np.sum(self.weights[:, None] * y, axis=0)\n\n            self.pc = (1 - self.cc) * self.pc + np.sqrt(self.cc * (2 - self.cc) * self.mueff) * delta_mean / self.sigma\n            self.ps = (1 - self.c_sigma) * self.ps + np.sqrt(self.c_sigma * (2 - self.c_sigma) * self.mueff) * np.dot(np.random.randn(self.mu, self.dim).T, y).mean(axis=1) / self.sigma\n\n            self.mean += delta_mean\n            self.C = (1 - self.c1 - self.cmu) * self.C + self.c1 * np.outer(self.pc, self.pc) + self.cmu * np.dot((self.weights * y).T, y)\n\n            self.sigma *= np.exp((self.cs / self.dsigma) * (np.linalg.norm(self.ps) / self.chiN - 1))\n\n            if self.eval_count > self.restart_trigger:\n               self.mean = np.random.uniform(self.lb, self.ub, size=self.dim)\n               self.C = np.eye(self.dim)\n               self.pc = np.zeros(self.dim)\n               self.ps = np.zeros(self.dim)\n               self.sigma = 0.5\n               self.restart_trigger = self.eval_count + self.budget // 5\n\n            if self.eval_count >= self.budget:\n                break\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "In the code, line 59, in __call__, the following error occurred:\nValueError: operands could not be broadcast together with shapes (3,) (3,2) \nOn line: self.C = (1 - self.c1 - self.cmu) * self.C + self.c1 * np.outer(self.pc, self.pc) + self.cmu * np.dot((self.weights * y).T, y)", "error": "In the code, line 59, in __call__, the following error occurred:\nValueError: operands could not be broadcast together with shapes (3,) (3,2) \nOn line: self.C = (1 - self.c1 - self.cmu) * self.C + self.c1 * np.outer(self.pc, self.pc) + self.cmu * np.dot((self.weights * y).T, y)", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "fad5a97b-c9ca-4b46-8593-42c33cf1e87f", "fitness": 0.3570154465171468, "name": "AdaptiveCauchyDifferentialEvolution", "description": "No description provided.", "code": "import numpy as np\n\nclass AdaptiveCauchyDifferentialEvolution:\n    def __init__(self, budget=10000, dim=10, pop_size=50, Cr=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.Cr = Cr  # Crossover rate\n        self.lb = -5.0\n        self.ub = 5.0\n        self.population = None\n        self.fitness = None\n        self.eval_count = 0\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.restart_trigger = 0.9 # Trigger for restart mechanism\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.eval_count += self.pop_size\n        best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_index]\n        self.x_opt = self.population[best_index].copy()\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        stagnation_counter = 0\n        last_f_opt = self.f_opt\n\n        while self.eval_count < self.budget:\n            for i in range(self.pop_size):\n                # Mutation (Cauchy)\n                indices = [j for j in range(self.pop_size) if j != i]\n                a, b, c = np.random.choice(indices, 3, replace=False)\n\n                #Cauchy Mutation with self adaptive scale\n                scale = np.std(self.population[a] - self.population[b])\n                mutant = self.population[a] + scale * np.random.standard_cauchy(size=self.dim) \n                mutant = np.clip(mutant, self.lb, self.ub)\n\n                # Adaptive Crossover\n                Cr = np.random.uniform(0, self.Cr) # Adapt crossover rate\n                crossover_mask = np.random.rand(self.dim) < Cr\n                trial_vector = np.where(crossover_mask, mutant, self.population[i])\n\n                # Evaluation\n                f = func(trial_vector)\n                self.eval_count += 1\n\n                # Selection\n                if f < self.fitness[i]:\n                    self.fitness[i] = f\n                    self.population[i] = trial_vector.copy()\n\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = trial_vector.copy()\n                        stagnation_counter = 0 # Reset stagnation counter\n\n                if self.eval_count >= self.budget:\n                    break\n\n            # Stagnation Check and Restart Mechanism\n            if abs(self.f_opt - last_f_opt) < 1e-6:\n                stagnation_counter += 1\n            else:\n                stagnation_counter = 0\n            \n            last_f_opt = self.f_opt\n            \n            if stagnation_counter > int(self.budget * self.restart_trigger/self.pop_size):\n                self.initialize_population(func) # Restart population to escape local optima\n                stagnation_counter = 0 # Reset stagnation counter\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveCauchyDifferentialEvolution scored 0.357 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.15936057978587725, 0.18045251452862143, 0.3283487233426563, 0.35828806458765183, 0.31446684794682656, 0.3880644976668064, 0.2893674382803093, 0.3230563448898849, 0.2877830486252224, 0.22202957192106199, 0.2856514661423991, 0.9986437438980451, 0.23064446855553555, 0.31037319840009503, 0.6941950385163553, 0.3563691220999079, 0.3005986026942119, 0.4608607888465649, 0.16258362846083874, 0.48917124115406385]}, "task_prompt": ""}
{"id": "69fb0573-a376-4881-aeb6-3da4737d9310", "fitness": 0.5553013715985262, "name": "PSO_DE", "description": "No description provided.", "code": "import numpy as np\n\nclass PSO_DE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, w=0.7, c1=1.5, c2=1.5, F=0.5, CR=0.7):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.w = w  # Inertia weight\n        self.c1 = c1 # Cognitive coefficient\n        self.c2 = c2 # Social coefficient\n        self.F = F\n        self.CR = CR\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        # Initialize population and velocities\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        velocities = np.random.uniform(-1, 1, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        # Initialize personal best positions and values\n        pbest_positions = population.copy()\n        pbest_fitness = fitness.copy()\n\n        # Initialize global best position and value\n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index]\n        gbest_position = population[best_index]\n        \n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Update velocity\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                velocities[i] = self.w * velocities[i] + \\\n                                self.c1 * r1 * (pbest_positions[i] - population[i]) + \\\n                                self.c2 * r2 * (gbest_position - population[i])\n                \n                # Mutation using DE inspired strategy\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = a + self.F * (b - c)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n                \n                #Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                  cross_points[np.random.randint(0, self.dim)] = True\n                \n                trial = np.where(cross_points, mutant, population[i])\n\n                # Update position\n                trial = np.clip(population[i] + velocities[i], func.bounds.lb, func.bounds.ub)\n                \n                # Evaluate new position\n                f = func(trial)\n                self.budget -= 1\n\n                # Update personal best\n                if f < pbest_fitness[i]:\n                    pbest_fitness[i] = f\n                    pbest_positions[i] = trial.copy()\n\n                    # Update global best\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = trial.copy()\n                        gbest_position = trial.copy()\n                \n                population[i] = trial.copy()\n\n            \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm PSO_DE scored 0.555 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.2028229720439353, 0.7364870185393242, 0.7221858966070378, 0.24761822651684173, 0.5774154840942851, 0.8032046932810559, 0.3322889758666904, 0.6426484613320695, 0.7236078640133463, 0.6590262048728222, 0.7916433797681638, 0.9960837507372879, 0.2343373959540187, 0.2949573314383811, 0.7326228990248871, 0.7778938126357642, 0.55282420125411, 0.37867424977211017, 0.18054068260494094, 0.5191439316134534]}, "task_prompt": ""}
{"id": "c6572884-2ab9-4324-8282-e7a505c27968", "fitness": 0.3640157039318117, "name": "ShrinkingAdaptiveDifferentialEvolution", "description": "No description provided.", "code": "import numpy as np\n\nclass ShrinkingAdaptiveDifferentialEvolution:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, CR=0.7, shrink_factor=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.shrink_factor = shrink_factor\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        \n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n        \n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index]\n\n        while self.budget > 0 and self.pop_size > 3:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                \n                # Adaptive F\n                F_adaptive = self.F * np.random.uniform(0.5, 1.5)\n                mutant = a + F_adaptive * (b - c)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                f = func(trial)\n                self.budget -= 1\n                if f < fitness[i]:\n                    fitness[i] = f\n                    population[i] = trial\n\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = trial\n            \n            best_index = np.argmin(fitness)\n            if fitness[best_index] < self.f_opt:\n                self.f_opt = fitness[best_index]\n                self.x_opt = population[best_index]\n\n            # Shrink population\n            sorted_indices = np.argsort(fitness)\n            new_pop_size = int(self.pop_size * self.shrink_factor)\n            population = population[sorted_indices[:new_pop_size]]\n            fitness = fitness[sorted_indices[:new_pop_size]]\n            self.pop_size = new_pop_size\n        \n        # Final refinement with remaining budget using the best individual as the mean\n        if self.budget > 0:\n            std = 0.1 * (func.bounds.ub - func.bounds.lb)\n            for _ in range(self.budget):\n              x = np.random.normal(self.x_opt, std, size=self.dim)\n              x = np.clip(x, func.bounds.lb, func.bounds.ub)\n              f = func(x)\n              if f < self.f_opt:\n                  self.f_opt = f\n                  self.x_opt = x\n        \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm ShrinkingAdaptiveDifferentialEvolution scored 0.364 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.1600143126750483, 0.2555859877158053, 0.3321797977203518, 0.3921452950446236, 0.2848433696716357, 0.35073254725247005, 0.2785603970522452, 0.2979042185140025, 0.27256703484054723, 0.20520213223258432, 0.4041826878545234, 0.9939544901153763, 0.31793771429482076, 0.2894263556864993, 0.7005264014392474, 0.3427198206338732, 0.29403164050842034, 0.41924561978723074, 0.20520339477474459, 0.4833508608221845]}, "task_prompt": ""}
{"id": "a24c25fa-47b3-476d-8ef6-3e4af4107ade", "fitness": 0.2504027424318115, "name": "AdaptiveCMAES", "description": "No description provided.", "code": "import numpy as np\n\nclass AdaptiveCMAES:\n    def __init__(self, budget=10000, dim=10, pop_size=None, sigma0=0.5, cs=0.3, damps=1.0, c_cov_rank_one=None, c_cov_mu=None, mu_eff=None):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size if pop_size is not None else 4 + int(3 * np.log(self.dim))\n        self.sigma = sigma0\n        self.lb = -5.0\n        self.ub = 5.0\n        self.eval_count = 0\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.mean = np.random.uniform(self.lb, self.ub, size=self.dim)\n        \n        self.mu = self.pop_size // 2\n\n        if mu_eff is None:\n            self.mu_eff = self.mu\n        else:\n            self.mu_eff = mu_eff\n\n        self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n        self.weights /= np.sum(self.weights)\n\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n\n        self.cs = cs\n        self.damps = damps  #Damping for step-size\n\n        if c_cov_rank_one is None:\n            self.c_cov_rank_one = 2 / ((self.dim + 1.3)**2 + self.mu_eff)\n        else:\n            self.c_cov_rank_one = c_cov_rank_one\n            \n        if c_cov_mu is None:\n            self.c_cov_mu = min(1 - self.c_cov_rank_one, 2 * (self.mu_eff - 2 + 1 / self.mu_eff) / ((self.dim + 2)**2 + self.mu_eff))\n        else:\n            self.c_cov_mu = c_cov_mu\n\n        self.chiN = self.dim**0.5 * (1 - 1 / (4 * self.dim) + 1 / (21 * self.dim**2))\n\n    def __call__(self, func):\n        while self.eval_count < self.budget:\n            # Generate and evaluate offspring\n            z = np.random.multivariate_normal(np.zeros(self.dim), self.C, size=self.pop_size)\n            x = self.mean + self.sigma * z\n            x = np.clip(x, self.lb, self.ub)\n\n            fitness = np.array([func(xi) for xi in x])\n            self.eval_count += self.pop_size\n            \n            if np.min(fitness) < self.f_opt:\n                best_index = np.argmin(fitness)\n                self.f_opt = fitness[best_index]\n                self.x_opt = x[best_index].copy()\n\n            # Selection and Recombination\n            indices = np.argsort(fitness)\n            x_sorted = x[indices]\n            z_sorted = z[indices]\n\n            mean_new = np.sum(self.weights.reshape(-1, 1) * x_sorted[:self.mu], axis=0)\n            zmean = np.sum(self.weights.reshape(-1, 1) * z_sorted[:self.mu], axis=0)\n            \n            # Update evolution path\n            self.ps = (1 - self.cs) * self.ps + (self.cs * (2 - self.cs))**0.5 * zmean\n            norm_ps = np.linalg.norm(self.ps)\n            self.pc = (1 - self.c_cov_rank_one) * self.pc + (self.c_cov_rank_one * (2 - self.c_cov_rank_one))**0.5 * (mean_new - self.mean) / self.sigma\n\n            # Update covariance matrix\n            self.C = (1 - self.c_cov_rank_one - self.c_cov_mu) * self.C + self.c_cov_rank_one * np.outer(self.pc, self.pc) + self.c_cov_mu * np.sum(self.weights.reshape(-1, 1, 1) * np.array([np.outer(zi, zi) for zi in z_sorted[:self.mu]]), axis=0)\n\n            # Update step size\n            self.sigma *= np.exp((self.cs / self.damps) * (norm_ps / self.chiN - 1))\n\n            # Update mean\n            self.mean = mean_new\n\n            if self.eval_count >= self.budget:\n                break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveCMAES scored 0.250 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.05904132599616918, 0.15045287714943478, 0.25055562231921946, 0.1985311525678699, 0.2034249576846754, 0.40526853088460824, 0.24143834067158143, 0.2434958096002554, 0.23330901287688455, 0.160686414894892, 0.24616440227487701, 0.24712191216071255, 0.3203880424201391, 0.15743173065898008, 0.23935629112916468, 0.2869657744651376, 0.21325781400913724, 0.5352827335073206, 0.15491604939364023, 0.4609660539715307]}, "task_prompt": ""}
{"id": "b70ac6d1-614c-4652-a1d6-d849e31b43be", "fitness": 0.23067768938577396, "name": "AdaptiveCMAES", "description": "No description provided.", "code": "import numpy as np\n\nclass AdaptiveCMAES:\n    def __init__(self, budget=10000, dim=10, pop_size=None, initial_sigma=0.5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size if pop_size is not None else 4 + int(3 * np.log(dim))\n        self.initial_sigma = initial_sigma\n        self.lb = -5.0\n        self.ub = 5.0\n        self.eval_count = 0\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        self.mean = np.random.uniform(self.lb, self.ub, size=self.dim)\n        self.sigma = self.initial_sigma\n        self.C = np.eye(self.dim)  # Covariance matrix\n        self.pc = np.zeros(self.dim)  # Evolution path for C\n        self.ps = np.zeros(self.dim)  # Evolution path for sigma\n        self.chiN = np.sqrt(self.dim) * (1 - (1 / (4 * self.dim)) + (1 / (100 * self.dim**2)))\n\n        self.mu = self.pop_size // 2\n        self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n        self.weights /= np.sum(self.weights)\n\n        self.cs = (np.sqrt(self.mu / np.sum(self.weights**2)) / (self.dim + 5)) if (np.sqrt(self.mu / np.sum(self.weights**2)) / (self.dim + 5)) < 1 else 1\n        self.cc = (4 + (self.mu / self.dim)) / (self.dim + 4 + (2 * self.mu / self.dim))\n        self.c1 = 2 / ((self.dim + 1.3)**2 + self.mu)\n        self.cmu = min(1 - self.c1, 2 * (self.mu - 2 + (1 / self.mu)) / ((self.dim + 2)**2 + (self.mu / 2)))\n        self.damps = 1 + (2 * max(0, np.sqrt((self.mu - 1) / (self.dim + 1)) - 1)) + self.cs\n\n\n    def __call__(self, func):\n        while self.eval_count < self.budget:\n            # Generate and evaluate population\n            z = np.random.normal(0, 1, size=(self.pop_size, self.dim))\n            x = self.mean + self.sigma * np.dot(z, np.linalg.cholesky(self.C).T)\n            x = np.clip(x, self.lb, self.ub)\n            \n            fitness = np.array([func(xi) for xi in x])\n            self.eval_count += self.pop_size\n\n            # Sort population by fitness\n            idx = np.argsort(fitness)\n            fitness = fitness[idx]\n            x = x[idx]\n\n            # Update best solution\n            if fitness[0] < self.f_opt:\n                self.f_opt = fitness[0]\n                self.x_opt = x[0].copy()\n\n            # Update CMA-ES parameters\n            y = x[:self.mu] - self.mean\n            z = np.linalg.solve(np.linalg.cholesky(self.C), y.T).T / self.sigma #equivalent to np.dot(y, np.linalg.inv(np.linalg.cholesky(self.C)).T) / self.sigma\n            \n            self.pc = (1 - self.cc) * self.pc + np.sqrt(self.cc * (2 - self.cc) * self.mu) * np.mean(z, axis=0)\n            self.ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs) * self.mu) * (np.linalg.solve(np.linalg.cholesky(self.C), self.pc))\n            \n            delta_sigma = np.linalg.norm(self.ps) / self.chiN\n            self.sigma *= np.exp((self.cs / self.damps) * (delta_sigma - 1))\n            \n            C1update = self.c1 * (np.outer(self.pc, self.pc) + ((1-self.cc) if delta_sigma > (2 + np.sqrt(self.mu))/self.dim else 0) * self.C)\n            Cmuupdate = self.cmu * np.sum(self.weights[:, None, None] * y[:, :, None] * y[:, None, :], axis=0) / (self.sigma**2)\n            self.C = (1 - self.c1 - self.cmu) * self.C + C1update + Cmuupdate\n            \n            try:\n                np.linalg.cholesky(self.C)\n            except np.linalg.LinAlgError:\n                self.C = np.eye(self.dim)\n            \n            self.mean = np.sum(self.weights[:, None] * x[:self.mu], axis=0)\n            if self.eval_count >= self.budget:\n                break\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveCMAES scored 0.231 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.04929781172762471, 0.10078730411302617, 0.2009152591080846, 0.15125758232068942, 0.13944626426001816, 0.19063202764986498, 0.19799283963121783, 0.14780801241299302, 0.14256729701027837, 0.13815163822666332, 0.1540780842460201, 0.9952025234587125, 0.19868608803146448, 0.15510958058340252, 0.5414734328607633, 0.24919962804764695, 0.1568977892196135, 0.18749614357744393, 0.10409214815890122, 0.4124623330710504]}, "task_prompt": ""}
{"id": "9d31f1f6-3c22-430b-881b-c8d80352304d", "fitness": 0.2221656169182648, "name": "ModifiedCMAES", "description": "No description provided.", "code": "import numpy as np\n\nclass ModifiedCMAES:\n    def __init__(self, budget=10000, dim=10, pop_size=None, mu_fraction=0.25, cs=0.3, damps=1, c_cov=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.lb = -5.0\n        self.ub = 5.0\n        if pop_size is None:\n            self.pop_size = 4 + int(3 * np.log(self.dim)) # Reduced population size\n        else:\n            self.pop_size = pop_size\n        self.mu = int(self.pop_size * mu_fraction)\n        self.mu = max(1, self.mu) # Ensure mu is at least 1\n        self.xmean = np.random.uniform(self.lb, self.ub, self.dim)\n        self.sigma = 0.5  # Initial step size\n        self.C = np.eye(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.chiN = self.dim**0.5 * (1 - 1/(4*self.dim) + 1/(21*self.dim**2))\n        self.cs = cs\n        self.damps = damps\n        self.c_cov = c_cov\n\n        self.eval_count = 0\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        while self.eval_count < self.budget:\n            # Generate lambda offsprings\n            arz = np.random.normal(0, 1, size=(self.pop_size, self.dim))\n            arx = self.xmean + self.sigma * np.dot(arz, np.linalg.cholesky(self.C).T)\n            arx = np.clip(arx, self.lb, self.ub)\n\n            # Evaluate offsprings\n            arfitness = np.zeros(self.pop_size)\n            for k in range(self.pop_size):\n                arfitness[k] = func(arx[k])\n                self.eval_count += 1\n                if arfitness[k] < self.f_opt:\n                    self.f_opt = arfitness[k]\n                    self.x_opt = arx[k].copy()\n                if self.eval_count >= self.budget:\n                    break\n\n            if self.eval_count >= self.budget:\n                break\n\n            # Sort by fitness and compute weighted mean into xmean\n            arindex = np.argsort(arfitness)\n            arx = arx[arindex]\n            arfitness = arfitness[arindex]\n\n            xold = self.xmean.copy()\n            self.xmean = np.mean(arx[:self.mu], axis=0)\n\n            # Cumulation\n            self.ps = (1 - self.cs) * self.ps + (self.cs**0.5) * (self.xmean - xold) / self.sigma\n            self.pc = (1 - self.c_cov) * self.pc + (self.c_cov**0.5) * (self.xmean - xold) / self.sigma\n            \n            # Adapt covariance matrix C\n            self.C = (1 - self.c_cov) * self.C + self.c_cov * (np.outer(self.pc, self.pc) + self.c_cov/self.dim * np.eye(self.dim)) # Simplified rank-one update\n\n            # Adapt step size sigma\n            self.sigma *= np.exp((np.linalg.norm(self.ps) / self.chiN - 1) * self.damps)\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm ModifiedCMAES scored 0.222 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.11237512719472964, 0.20903186256519124, 0.2417193006213353, 0.18656662249534817, 0.12183460560950865, 0.1332165573716675, 0.22987626182143428, 0.19676778834045583, 0.15470164388973606, 0.13687937121896154, 0.15992464554394514, 0.12326515510283187, 0.2684835676609537, 0.28236223952957507, 0.7375904172332637, 0.24823656352262435, 0.21955044191308037, 0.48001329561917716, 0.08004172084490602, 0.12087515026657059]}, "task_prompt": ""}
{"id": "fa98f5a8-d894-4e95-8a5e-38b46e04c8b5", "fitness": 0.0, "name": "AdaptivePopulationSearch", "description": "No description provided.", "code": "import numpy as np\n\nclass AdaptivePopulationSearch:\n    def __init__(self, budget=10000, dim=10, pop_size=40, local_steps=5, stagnation_limit=50):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.local_steps = local_steps\n        self.stagnation_limit = stagnation_limit\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        \n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n        \n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index]\n\n        stagnation_counter = 0\n\n        while self.budget > 0:\n            # Sort population by fitness\n            sorted_indices = np.argsort(fitness)\n            population = population[sorted_indices]\n            fitness = fitness[sorted_indices]\n\n            # Local search around best solutions\n            for i in range(min(self.pop_size // 2, self.budget)):  # Limit local searches to avoid over-exploitation\n                x_local = population[i].copy()\n                for _ in range(self.local_steps):\n                    direction = np.random.uniform(-0.1, 0.1, size=self.dim)  # Smaller step size\n                    x_new = x_local + direction\n                    x_new = np.clip(x_new, func.bounds.lb, func.bounds.ub)\n                    f_new = func(x_new)\n                    self.budget -= 1\n                    if self.budget <= 0:\n                        break\n                    if f_new < fitness[i]:\n                        fitness[i] = f_new\n                        population[i] = x_new\n                        x_local = x_new\n\n                        if f_new < self.f_opt:\n                            self.f_opt = f_new\n                            self.x_opt = x_new\n            if self.budget <=0:\n                break\n\n            # Global search (diversity injection)\n            if fitness[0] == self.f_opt:\n              stagnation_counter +=1\n            else:\n              stagnation_counter = 0\n              self.f_opt = fitness[0]\n              self.x_opt = population[0]\n\n            if stagnation_counter > self.stagnation_limit:\n                # Reset the worst individuals to random positions to encourage exploration.\n                num_to_reset = self.pop_size // 2\n                population[-num_to_reset:] = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(num_to_reset, self.dim))\n                fitness[-num_to_reset:] = [func(x) for x in population[-num_to_reset:]]\n                self.budget -= num_to_reset\n                stagnation_counter = 0 #reset stagnation counter\n                \n                if self.budget <= 0:\n                  break\n\n            best_index = np.argmin(fitness)\n            if fitness[best_index] < self.f_opt:\n                self.f_opt = fitness[best_index]\n                self.x_opt = population[best_index]\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptivePopulationSearch scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0]}, "task_prompt": ""}
{"id": "146e3171-f2a4-48af-981f-0042be065666", "fitness": 0.2523538305642559, "name": "AdaptiveSimulatedAnnealing", "description": "No description provided.", "code": "import numpy as np\n\nclass AdaptiveSimulatedAnnealing:\n    def __init__(self, budget=10000, dim=10, temp_init=100.0, alpha=0.95):\n        self.budget = budget\n        self.dim = dim\n        self.temp = temp_init\n        self.alpha = alpha\n        self.lb = -5.0\n        self.ub = 5.0\n        self.x = None\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.acceptance_rate = 0.0\n\n    def initialize(self, func):\n        self.x = np.random.uniform(self.lb, self.ub, size=self.dim)\n        self.f_opt = func(self.x)\n        self.x_opt = self.x\n        self.eval_count += 1\n\n    def __call__(self, func):\n        self.initialize(func)\n        accepted_moves = 0\n\n        while self.eval_count < self.budget:\n            # Generate a new candidate solution\n            x_new = self.x + np.random.normal(0, self.temp, size=self.dim)\n            x_new = np.clip(x_new, self.lb, self.ub)  # Ensure bounds are respected\n            f_new = func(x_new)\n            self.eval_count += 1\n\n            # Acceptance criterion\n            delta_f = f_new - self.f_opt\n            if delta_f < 0 or np.random.rand() < np.exp(-delta_f / self.temp):\n                self.x = x_new\n                self.f_opt = f_new\n                accepted_moves +=1\n                if f_new < self.f_opt:\n                    self.x_opt = x_new\n\n            # Adaptive temperature decay\n            if self.eval_count % (self.dim * 10) == 0:\n                self.acceptance_rate = accepted_moves / (self.dim * 10)\n                if self.acceptance_rate > 0.5:\n                    self.alpha = 0.99\n                elif self.acceptance_rate < 0.1:\n                    self.alpha = 0.8\n                else:\n                    self.alpha = 0.95\n\n                self.temp *= self.alpha\n                accepted_moves = 0\n            \n            if self.eval_count >= self.budget:\n              break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveSimulatedAnnealing scored 0.252 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.20935738073876153, 0.18857880022429185, 0.20540021980396428, 0.15307490441641913, 0.18979041782994377, 0.16832675202119696, 0.22141669374651252, 0.15713827225993804, 0.16552432892464697, 0.12620750011194504, 0.18166151538985453, 0.9992261919261971, 0.21432526309041888, 0.21763803197745202, 0.5557493250105414, 0.2055733320060381, 0.16667900309213146, 0.18514337662376557, 0.11879863026954207, 0.41746667182155783]}, "task_prompt": ""}
{"id": "a4a9a873-7746-4de3-a8b6-32d57a8e4260", "fitness": 0.40923759923437686, "name": "AdaptiveStepSizeSearch", "description": "No description provided.", "code": "import numpy as np\n\nclass AdaptiveStepSizeSearch:\n    def __init__(self, budget=10000, dim=10, initial_step_size=0.1, success_rate_threshold=0.1, adaptation_factor=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.step_size = initial_step_size\n        self.success_rate_threshold = success_rate_threshold\n        self.adaptation_factor = adaptation_factor\n        self.success_count = 0\n        self.iteration = 0\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        x = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        self.f_opt = func(x)\n        self.x_opt = x\n        self.budget -= 1\n\n        while self.budget > 0:\n            self.iteration += 1\n            # Mutation (Cauchy for exploration, Gaussian for exploitation)\n            if np.random.rand() < 0.2:  # Probability of Cauchy mutation\n                mut = self.step_size * np.random.standard_cauchy(size=self.dim)\n            else:\n                mut = self.step_size * np.random.randn(self.dim)\n            x_new = x + mut\n            x_new = np.clip(x_new, func.bounds.lb, func.bounds.ub)\n\n            f_new = func(x_new)\n            self.budget -= 1\n\n            if f_new < self.f_opt:\n                self.f_opt = f_new\n                self.x_opt = x_new\n                x = x_new\n                self.success_count += 1\n\n            # Step size adaptation\n            if self.iteration % 100 == 0:\n                success_rate = self.success_count / 100\n                if success_rate < self.success_rate_threshold:\n                    self.step_size *= self.adaptation_factor\n                else:\n                    self.step_size /= self.adaptation_factor\n                self.success_count = 0\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveStepSizeSearch scored 0.409 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.10726180962810117, 0.17142028783650798, 0.6515328577896695, 0.14507573897482873, 0.2899166297991771, 0.24616029701246278, 0.3018888576706992, 0.5779914024300516, 0.21413449891652858, 0.1272755899696827, 0.897027067441951, 0.9444955946257761, 0.21827709505514103, 0.20565040798846024, 0.9537432120554847, 0.33805523852760944, 0.4034877868892326, 0.8395979620447815, 0.0902395293842535, 0.4615201206471369]}, "task_prompt": ""}
{"id": "5fc7ae21-30a7-468e-a500-017de0527582", "fitness": 0.1371835118723404, "name": "AdaptiveSimulatedAnnealing", "description": "No description provided.", "code": "import numpy as np\n\nclass AdaptiveSimulatedAnnealing:\n    def __init__(self, budget=10000, dim=10, temp_init=100.0, alpha=0.99, step_size_init=1.0):\n        self.budget = budget\n        self.dim = dim\n        self.temp = temp_init\n        self.alpha = alpha\n        self.step_size = step_size_init\n        self.lb = -5.0\n        self.ub = 5.0\n        self.x_opt = None\n        self.f_opt = np.inf\n        self.eval_count = 0\n        self.x_current = None\n        self.f_current = None\n\n    def initialize(self, func):\n        self.x_current = np.random.uniform(self.lb, self.ub, size=self.dim)\n        self.f_current = func(self.x_current)\n        self.eval_count += 1\n        self.x_opt = self.x_current\n        self.f_opt = self.f_current\n\n    def __call__(self, func):\n        self.initialize(func)\n        while self.eval_count < self.budget:\n            # Generate a new candidate solution\n            x_new = self.x_current + np.random.uniform(-self.step_size, self.step_size, size=self.dim)\n            x_new = np.clip(x_new, self.lb, self.ub)  # Keep within bounds\n\n            f_new = func(x_new)\n            self.eval_count += 1\n\n            delta_f = f_new - self.f_current\n\n            # Acceptance probability\n            if delta_f < 0 or np.random.rand() < np.exp(-delta_f / self.temp):\n                self.x_current = x_new\n                self.f_current = f_new\n\n                if f_new < self.f_opt:\n                    self.f_opt = f_new\n                    self.x_opt = x_new\n\n            # Update temperature and step size\n            self.temp *= self.alpha\n            self.step_size *= 0.99  # Reduce step size gradually\n\n            if self.eval_count >= self.budget:\n                break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveSimulatedAnnealing scored 0.137 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.032036995047634464, 0.14718865172830142, 0.1877501161151004, 0.05236456914804288, 0.1347284122589617, 0.15015665517594057, 0.1895933561518044, 0.08844888340531365, 0.1465597232688357, 0.14247870786183614, 0.1094401084629204, 0.1448649696953236, 0.22989861564377567, 0.0820027670011324, 0.1305220847132974, 0.20827123215016374, 0.15146577056012445, 0.15191527832913543, 0.13817639976409624, 0.12580694096506706]}, "task_prompt": ""}
{"id": "69c24ac9-2c50-42bb-98c5-5e967686c97c", "fitness": "-inf", "name": "GaussianProcessOptimization", "description": "No description provided.", "code": "import numpy as np\nfrom sklearn.gaussian_process import GaussianProcessRegressor\nfrom sklearn.gaussian_process.kernels import RBF, ConstantKernel\nfrom scipy.stats import norm\nfrom scipy.optimize import minimize\n\nclass GaussianProcessOptimization:\n    def __init__(self, budget=10000, dim=10, n_initial=10, xi=0.01, kernel=None):\n        self.budget = budget\n        self.dim = dim\n        self.n_initial = n_initial\n        self.xi = xi\n        self.lb = -5.0\n        self.ub = 5.0\n        self.X = None\n        self.y = None\n        self.gpr = None\n        self.eval_count = 0\n        self.f_opt = np.inf\n        self.x_opt = None\n        if kernel is None:\n             self.kernel = ConstantKernel(1.0, constant_value_bounds=\"fixed\") * RBF(length_scale=1.0, length_scale_bounds=\"fixed\")\n        else:\n            self.kernel = kernel\n\n    def initialize(self, func):\n        self.X = np.random.uniform(self.lb, self.ub, size=(self.n_initial, self.dim))\n        self.y = np.array([func(x) for x in self.X])\n        self.eval_count += self.n_initial\n        best_index = np.argmin(self.y)\n        self.f_opt = self.y[best_index]\n        self.x_opt = self.X[best_index].copy()\n        self.gpr = GaussianProcessRegressor(kernel=self.kernel, n_restarts_optimizer=10, alpha=1e-5)\n\n    def expected_improvement(self, x, gpr, y_max, xi=0.01):\n        mu, sigma = gpr.predict(x.reshape(1, -1), return_std=True)\n        sigma = np.maximum(sigma, 1e-9)\n        imp = mu - y_max - xi\n        Z = imp / sigma\n        ei = imp * norm.cdf(Z) + sigma * norm.pdf(Z)\n        return ei\n\n    def propose_location(self, gpr, y_max, lb, ub, xi, n_restarts=25):\n        def min_obj(x):\n            return -self.expected_improvement(x, gpr, y_max, xi=xi)\n\n        best_x = None\n        best_ei = 1e9\n        for _ in range(n_restarts):\n            x0 = np.random.uniform(lb, ub, size=self.dim)\n            res = minimize(min_obj, x0, bounds=[(lb, ub)] * self.dim, method='L-BFGS-B')\n            if res.fun < best_ei:\n                best_ei = res.fun\n                best_x = res.x\n        return best_x\n\n    def __call__(self, func):\n        self.initialize(func)\n\n        while self.eval_count < self.budget:\n            self.gpr.fit(self.X, self.y)\n            \n            # Find next point to evaluate using expected improvement\n            x_next = self.propose_location(self.gpr, self.y.max(), self.lb, self.ub, self.xi)\n            \n            # Evaluate the objective function\n            f_next = func(x_next)\n            self.eval_count += 1\n\n            # Append the new data to our existing data\n            self.X = np.vstack((self.X, x_next.reshape(1, -1)))\n            self.y = np.append(self.y, f_next)\n\n            # Update best\n            if f_next < self.f_opt:\n                self.f_opt = f_next\n                self.x_opt = x_next.copy()\n                \n            if self.eval_count >= self.budget:\n                break\n            \n            # Restart if stagnated\n            if self.eval_count > self.n_initial * 5 and np.std(self.y[-self.n_initial:]) < 1e-6:\n                self.X = np.random.uniform(self.lb, self.ub, size=(self.n_initial, self.dim))\n                self.y = np.array([func(x) for x in self.X])\n                self.eval_count += self.n_initial\n                best_index = np.argmin(self.y)\n                self.f_opt = self.y[best_index]\n                self.x_opt = self.X[best_index].copy()\n                self.gpr = GaussianProcessRegressor(kernel=self.kernel, n_restarts_optimizer=10, alpha=1e-5)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "In the code, line 22, in __init__, the following error occurred:\nNameError: name 'ConstantKernel' is not defined\nOn line: self.kernel = ConstantKernel(1.0, constant_value_bounds=\"fixed\") * RBF(length_scale=1.0, length_scale_bounds=\"fixed\")", "error": "In the code, line 22, in __init__, the following error occurred:\nNameError: name 'ConstantKernel' is not defined\nOn line: self.kernel = ConstantKernel(1.0, constant_value_bounds=\"fixed\") * RBF(length_scale=1.0, length_scale_bounds=\"fixed\")", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "134cde3e-5ff7-411e-b4c9-c5943dfe7f2d", "fitness": 0.576192171213611, "name": "ParticleSwarmOptimizer", "description": "No description provided.", "code": "import numpy as np\n\nclass ParticleSwarmOptimizer:\n    def __init__(self, budget=10000, dim=10, pop_size=50, inertia=0.7, c1=1.5, c2=1.5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.inertia = inertia\n        self.c1 = c1\n        self.c2 = c2\n        self.lb = -5.0\n        self.ub = 5.0\n        self.particles = None\n        self.velocities = None\n        self.fitness = None\n        self.personal_best_positions = None\n        self.personal_best_fitness = None\n        self.global_best_position = None\n        self.global_best_fitness = np.inf\n        self.eval_count = 0\n\n    def initialize_swarm(self, func):\n        self.particles = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.velocities = np.random.uniform(-1, 1, size=(self.pop_size, self.dim))  # Initialize velocities\n        self.fitness = np.array([func(x) for x in self.particles])\n        self.eval_count += self.pop_size\n        self.personal_best_positions = self.particles.copy()\n        self.personal_best_fitness = self.fitness.copy()\n\n        best_index = np.argmin(self.fitness)\n        self.global_best_fitness = self.fitness[best_index]\n        self.global_best_position = self.particles[best_index].copy()\n\n    def __call__(self, func):\n        self.initialize_swarm(func)\n\n        while self.eval_count < self.budget:\n            # Update inertia weight (linearly decrease)\n            inertia = self.inertia - (self.inertia - 0.4) * (self.eval_count / self.budget)\n\n            for i in range(self.pop_size):\n                # Update velocity\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                \n                cognitive_velocity = self.c1 * r1 * (self.personal_best_positions[i] - self.particles[i])\n                social_velocity = self.c2 * r2 * (self.global_best_position - self.particles[i])\n\n                self.velocities[i] = inertia * self.velocities[i] + cognitive_velocity + social_velocity\n\n                # Velocity clamping\n                v_max = 0.1 * (self.ub - self.lb)\n                self.velocities[i] = np.clip(self.velocities[i], -v_max, v_max)\n\n                # Update position\n                self.particles[i] = self.particles[i] + self.velocities[i]\n                self.particles[i] = np.clip(self.particles[i], self.lb, self.ub)\n\n                # Evaluate fitness\n                f = func(self.particles[i])\n                self.eval_count += 1\n\n                # Update personal best\n                if f < self.personal_best_fitness[i]:\n                    self.personal_best_fitness[i] = f\n                    self.personal_best_positions[i] = self.particles[i].copy()\n\n                # Update global best\n                if f < self.global_best_fitness:\n                    self.global_best_fitness = f\n                    self.global_best_position = self.particles[i].copy()\n                \n                if self.eval_count >= self.budget:\n                    break\n\n        self.f_opt = self.global_best_fitness\n        self.x_opt = self.global_best_position\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm ParticleSwarmOptimizer scored 0.576 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.2357777161908866, 0.2802057133057847, 0.804563571194758, 0.19799236649318486, 0.8281512311309503, 0.8424659568776544, 0.3678052136344502, 0.616714336834844, 0.8239061768662825, 0.18663202142775492, 0.9046845934619832, 0.999571102006065, 0.2741680584158809, 0.7826186155933665, 0.7329278197073366, 0.825487448309962, 0.7196018187759643, 0.378156677832169, 0.195218320518588, 0.5271946656943551]}, "task_prompt": ""}
{"id": "22b74223-480e-4d0b-8df6-26ee402eb43d", "fitness": 0.12377067166127223, "name": "SimulatedAnnealing", "description": "No description provided.", "code": "import numpy as np\n\nclass SimulatedAnnealing:\n    def __init__(self, budget=10000, dim=10, initial_temp=100, cooling_rate=0.95):\n        self.budget = budget\n        self.dim = dim\n        self.initial_temp = initial_temp\n        self.cooling_rate = cooling_rate\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        x = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        f = func(x)\n        self.budget -= 1\n        self.f_opt = f\n        self.x_opt = x\n        temp = self.initial_temp\n\n        while self.budget > 0:\n            x_new = x + np.random.normal(0, temp/self.initial_temp, size=self.dim)\n            x_new = np.clip(x_new, func.bounds.lb, func.bounds.ub)\n            f_new = func(x_new)\n            self.budget -= 1\n\n            if f_new < f:\n                f = f_new\n                x = x_new\n                if f < self.f_opt:\n                    self.f_opt = f\n                    self.x_opt = x\n            else:\n                delta = f_new - f\n                if np.random.rand() < np.exp(-delta / temp):\n                    f = f_new\n                    x = x_new\n            temp *= self.cooling_rate\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm SimulatedAnnealing scored 0.124 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.05333875288022971, 0.11029184920338797, 0.19608799096331886, 0.10893028334138022, 0.06003280794827737, 0.10819282350282777, 0.09950527840116286, 0.12495156076123715, 0.0389853975008827, 0.10486755864258934, 0.12222166484833707, 0.14507994453881312, 0.24496465387112898, 0.10747829988476443, 0.12037633620344801, 0.1309029976899213, 0.165150815943924, 0.11346112864057345, 0.16177299136900347, 0.15882029709023682]}, "task_prompt": ""}
{"id": "a743cdbc-9f3c-4434-b326-b3356cb7a7b8", "fitness": "-inf", "name": "CMAES", "description": "No description provided.", "code": "import numpy as np\n\nclass CMAES:\n    def __init__(self, budget=10000, dim=10, pop_size=None, sigma=0.5, cs=0.3, damps=None, ccov1=None, ccovmu=None):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size if pop_size is not None else 4 + int(3 * np.log(dim))\n        self.sigma = sigma\n        self.cs = cs\n        self.damps = damps if damps is not None else 1 + dim / 2\n        self.ccov1 = ccov1 if ccov1 is not None else 2 / (dim + np.sqrt(2))\n        self.ccovmu = ccovmu if ccovmu is not None else 2 / (dim + np.sqrt(2))\n        self.mean = None\n        self.C = None\n        self.pc = None\n        self.ps = None\n        self.eval_count = 0\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.lb = -5.0\n        self.ub = 5.0\n        self.mu = self.pop_size // 2\n\n    def initialize(self):\n        self.mean = np.random.uniform(self.lb, self.ub, self.dim)\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n\n    def sample_population(self):\n        z = np.random.normal(0, 1, size=(self.pop_size, self.dim))\n        return self.mean + self.sigma * np.dot(z, np.linalg.cholesky(self.C).T)\n\n    def update_distribution(self, population, fitness):\n        # Select best individuals\n        idx = np.argsort(fitness)\n        best_idx = idx[:self.mu]\n        best_individuals = population[best_idx]\n\n        # Update mean\n        old_mean = self.mean.copy()\n        self.mean = np.mean(best_individuals, axis=0)\n\n        # Update evolution path\n        self.ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs)) * np.linalg.solve(np.linalg.cholesky(self.C), (self.mean - old_mean) / self.sigma)\n\n        # Update covariance matrix\n        self.pc = (1 - self.ccov1) * self.pc + np.sqrt(self.ccov1 * (2 - self.ccov1)) * (self.mean - old_mean) / self.sigma\n\n        delta = best_individuals - old_mean\n        weights = np.array([np.log(self.mu+1) - np.log(i+1) for i in range(self.mu)])\n        weights = weights / np.sum(weights)\n\n        rank_one = np.outer(self.pc, self.pc)\n        rank_mu = np.sum([weights[i] * np.outer(delta[i], delta[i]) / self.sigma**2 for i in range(self.mu)], axis=0)\n\n        hsig = np.linalg.norm(self.ps) / np.sqrt(1 - (1 - self.cs)**2)**0.5 < (2 + 4 / (self.dim + 1))\n\n        self.C = (1 - self.ccov1 - self.ccovmu + self.ccovmu * np.sum(weights)) * self.C + self.ccov1 * rank_one + self.ccovmu * rank_mu\n\n        # Damp step size\n        self.sigma *= np.exp((self.cs / self.damps) * (np.linalg.norm(self.ps) / (self.dim**0.5) - 1))\n\n    def __call__(self, func):\n        self.initialize()\n\n        while self.eval_count < self.budget:\n            population = self.sample_population()\n            population = np.clip(population, self.lb, self.ub)\n            fitness = np.array([func(x) for x in population])\n            self.eval_count += self.pop_size\n            \n            best_index = np.argmin(fitness)\n            if fitness[best_index] < self.f_opt:\n                self.f_opt = fitness[best_index]\n                self.x_opt = population[best_index].copy()\n            \n            if self.eval_count >= self.budget:\n                break\n            \n            self.update_distribution(population, fitness)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "In the code, line 148, in _raise_linalgerror_nonposdef, the following error occurred:\nLinAlgError: Matrix is not positive definite", "error": "In the code, line 148, in _raise_linalgerror_nonposdef, the following error occurred:\nLinAlgError: Matrix is not positive definite", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "981e1e6f-00d3-43c8-8bcc-413f11b6ced4", "fitness": 0.0, "name": "AdaptiveSimulatedAnnealing", "description": "No description provided.", "code": "import numpy as np\n\nclass AdaptiveSimulatedAnnealing:\n    def __init__(self, budget=10000, dim=10, initial_temp=1.0, cooling_rate=0.95, restart_prob=0.01):\n        self.budget = budget\n        self.dim = dim\n        self.initial_temp = initial_temp\n        self.cooling_rate = cooling_rate\n        self.restart_prob = restart_prob\n        self.lb = -5.0\n        self.ub = 5.0\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.eval_count = 0\n        x = np.random.uniform(self.lb, self.ub, size=self.dim)\n        f = func(x)\n        self.eval_count += 1\n        self.f_opt = f\n        self.x_opt = x.copy()\n        \n        temp = self.initial_temp\n\n        while self.eval_count < self.budget:\n            x_new = x + np.random.normal(0, temp, size=self.dim)\n            x_new = np.clip(x_new, self.lb, self.ub)\n            f_new = func(x_new)\n            self.eval_count += 1\n\n            delta_f = f_new - f\n            if delta_f < 0 or np.random.rand() < np.exp(-delta_f / temp):\n                x = x_new.copy()\n                f = f_new\n\n                if f < self.f_opt:\n                    self.f_opt = f\n                    self.x_opt = x.copy()\n\n            temp *= self.cooling_rate\n\n            if np.random.rand() < self.restart_prob:\n                x = np.random.uniform(self.lb, self.ub, size=self.dim)\n                f = func(x)\n                self.eval_count += 1\n                if f < self.f_opt:\n                    self.f_opt = f\n                    self.x_opt = x.copy()\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveSimulatedAnnealing scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0]}, "task_prompt": ""}
{"id": "80ff39a6-d446-4f0a-87c7-81bb6cbf3764", "fitness": 0.5734458341742891, "name": "RankBasedDifferentialEvolution", "description": "No description provided.", "code": "import numpy as np\n\nclass RankBasedDifferentialEvolution:\n    def __init__(self, budget=10000, dim=10, pop_size=50, Cr=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.Cr = Cr  # Crossover rate\n        self.lb = -5.0\n        self.ub = 5.0\n        self.population = None\n        self.fitness = None\n        self.eval_count = 0\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.eval_count += self.pop_size\n        best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_index]\n        self.x_opt = self.population[best_index].copy()\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        F_initial = 1.0\n        F_final = 0.1\n\n        while self.eval_count < self.budget:\n            # Linearly decreasing mutation factor\n            F = F_initial - (F_initial - F_final) * (self.eval_count / self.budget)\n\n            for i in range(self.pop_size):\n                # Mutation\n                indices = [j for j in range(self.pop_size) if j != i]\n                a, b, c = np.random.choice(indices, 3, replace=False)\n\n                mutant = self.population[a] + F * (self.population[b] - self.population[c])\n                mutant = np.clip(mutant, self.lb, self.ub)\n\n                # Crossover\n                crossover_mask = np.random.rand(self.dim) < self.Cr\n                trial_vector = np.where(crossover_mask, mutant, self.population[i])\n\n                # Evaluation\n                f = func(trial_vector)\n                self.eval_count += 1\n\n                # Rank-based selection\n                if f < self.fitness[i]:\n                    self.fitness[i] = f\n                    self.population[i] = trial_vector.copy()\n\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = trial_vector.copy()\n                \n                if self.eval_count >= self.budget:\n                    break\n\n            # Rank individuals and sort\n            ranked_indices = np.argsort(self.fitness)\n            self.population = self.population[ranked_indices]\n            self.fitness = self.fitness[ranked_indices]\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm RankBasedDifferentialEvolution scored 0.573 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.2274690111475629, 0.4663467618088415, 0.5454653243136367, 0.717684156564788, 0.5807024921874593, 0.65024597576596, 0.49951850113556384, 0.5035831063974305, 0.5727887247830632, 0.5165319328227901, 0.7184758550388763, 0.992974688578509, 0.4990116696574871, 0.5809563145655225, 0.8607777760344342, 0.6578778404884454, 0.46103456376072394, 0.7057442056310286, 0.20778764981736608, 0.5039401329862945]}, "task_prompt": ""}
{"id": "7445cfb7-b6b3-4251-9e9b-6f51b086b6de", "fitness": "-inf", "name": "HybridDE", "description": "No description provided.", "code": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, Cr=0.9, local_search_frequency=500):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.Cr = Cr\n        self.lb = -5.0\n        self.ub = 5.0\n        self.population = None\n        self.fitness = None\n        self.eval_count = 0\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.local_search_frequency = local_search_frequency\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.eval_count += self.pop_size\n        best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_index]\n        self.x_opt = self.population[best_index].copy()\n\n    def local_search(self, func, x0):\n        bounds = [(self.lb, self.ub)] * self.dim\n        res = minimize(func, x0, method='Nelder-Mead', bounds=bounds, options={'maxfev': self.local_search_frequency})\n        return res.fun, res.x, res.nfev\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.eval_count < self.budget:\n            for i in range(self.pop_size):\n                # Mutation\n                indices = [j for j in range(self.pop_size) if j != i]\n                a, b, c = np.random.choice(indices, 3, replace=False)\n\n                mutant = self.population[a] + self.F * (self.population[b] - self.population[c])\n                mutant = np.clip(mutant, self.lb, self.ub)\n\n                # Crossover\n                crossover_mask = np.random.rand(self.dim) < self.Cr\n                trial_vector = np.where(crossover_mask, mutant, self.population[i])\n\n                # Evaluation\n                f = func(trial_vector)\n                self.eval_count += 1\n\n                # Selection\n                if f < self.fitness[i]:\n                    self.fitness[i] = f\n                    self.population[i] = trial_vector.copy()\n\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = trial_vector.copy()\n                \n                if self.eval_count >= self.budget:\n                    break\n\n            # Local Search\n            if self.eval_count % self.local_search_frequency == 0:\n                best_index = np.argmin(self.fitness)\n                x0 = self.population[best_index].copy()\n                f_local, x_local, nfev = self.local_search(func, x0)\n                self.eval_count += nfev\n\n                if f_local < self.f_opt:\n                    self.f_opt = f_local\n                    self.x_opt = x_local.copy()\n                    self.fitness[best_index] = f_local\n                    self.population[best_index] = x_local.copy()\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "In the code, line 30, in local_search, the following error occurred:\nNameError: name 'minimize' is not defined\nOn line: res = minimize(func, x0, method='Nelder-Mead', bounds=bounds, options={'maxfev': self.local_search_frequency})", "error": "In the code, line 30, in local_search, the following error occurred:\nNameError: name 'minimize' is not defined\nOn line: res = minimize(func, x0, method='Nelder-Mead', bounds=bounds, options={'maxfev': self.local_search_frequency})", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}
{"id": "383f0889-1b28-4893-9365-c21d60db188c", "fitness": 0.44610861648699907, "name": "SelfAdaptiveMirroredDE", "description": "No description provided.", "code": "import numpy as np\n\nclass SelfAdaptiveMirroredDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, CR=0.7):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.F_list = np.ones(pop_size) * F\n        self.CR_list = np.ones(pop_size) * CR\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        \n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n        \n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index]\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                \n                # Self-adaptive F and CR\n                F = self.F_list[i] + 0.1 * np.random.normal()\n                F = np.clip(F, 0.1, 0.9)\n                CR = self.CR_list[i] + 0.1 * np.random.normal()\n                CR = np.clip(CR, 0.1, 0.9)\n                \n                mutant = a + F * (b - c)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Mirrored sampling - create a mirrored individual\n                mirrored_mutant = 2 * func.bounds.ub - mutant\n                mirrored_mutant = np.clip(mirrored_mutant, func.bounds.lb, func.bounds.ub)\n                \n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n                mirrored_trial = np.where(cross_points, mirrored_mutant, population[i])\n                \n                # Selection\n                f = func(trial)\n                f_mirrored = func(mirrored_trial)\n                self.budget -= 2 #two evaluations per loop\n                \n                if f < f_mirrored:\n                    if f < fitness[i]:\n                        fitness[i] = f\n                        population[i] = trial\n                        self.F_list[i] = F\n                        self.CR_list[i] = CR\n\n                        if f < self.f_opt:\n                            self.f_opt = f\n                            self.x_opt = trial\n                else:\n                    if f_mirrored < fitness[i]:\n                        fitness[i] = f_mirrored\n                        population[i] = mirrored_trial\n                        self.F_list[i] = F\n                        self.CR_list[i] = CR\n\n                        if f_mirrored < self.f_opt:\n                            self.f_opt = f_mirrored\n                            self.x_opt = mirrored_trial\n                if self.budget <= 0:\n                    return self.f_opt, self.x_opt\n            \n            best_index = np.argmin(fitness)\n            if fitness[best_index] < self.f_opt:\n                self.f_opt = fitness[best_index]\n                self.x_opt = population[best_index]\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm SelfAdaptiveMirroredDE scored 0.446 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.15649125564301125, 0.24396141527358373, 0.41015280197024906, 0.6788534114455004, 0.3941567419374523, 0.5436548764813012, 0.3282134229035528, 0.39314137668165605, 0.40290618518461596, 0.2667557841961221, 0.4637447619486107, 0.9994170646883189, 0.29252367262480616, 0.4241284507396781, 0.7505971744687934, 0.5253736025667246, 0.3520169633573399, 0.610521119544862, 0.1920675550849893, 0.4934946929988143]}, "task_prompt": ""}
{"id": "758173cd-42fb-421c-b406-c52dbab46f3a", "fitness": 0.5668576474173227, "name": "SelfAdaptiveDE", "description": "No description provided.", "code": "import numpy as np\n\nclass SelfAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, CR=0.7, local_search_prob=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.local_search_prob = local_search_prob\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        \n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n        \n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index]\n\n        F = np.full(self.pop_size, self.F)\n        CR = np.full(self.pop_size, self.CR)\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Self-adaptive parameters\n                F_i = F[i] + 0.1 * np.random.normal(0, 1)\n                F_i = np.clip(F_i, 0.1, 1.0)\n                CR_i = CR[i] + 0.1 * np.random.normal(0, 1)\n                CR_i = np.clip(CR_i, 0.1, 1.0)\n\n                # Mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = a + F_i * (b - c)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR_i\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Local search\n                if np.random.rand() < self.local_search_prob:\n                    trial = self.x_opt + 0.01 * np.random.normal(0, 1, self.dim)\n                    trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Selection\n                f = func(trial)\n                self.budget -= 1\n                if f < fitness[i]:\n                    fitness[i] = f\n                    population[i] = trial\n                    F[i] = F_i\n                    CR[i] = CR_i\n\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = trial\n            \n            best_index = np.argmin(fitness)\n            if fitness[best_index] < self.f_opt:\n                self.f_opt = fitness[best_index]\n                self.x_opt = population[best_index]    \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm SelfAdaptiveDE scored 0.567 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.20785219368634922, 0.2214173346745727, 0.7232264396337715, 0.26234447130859684, 0.33719007936341383, 0.8551078776278335, 0.7129896019941262, 0.6383785992819766, 0.2554506018715783, 0.24564408934775994, 0.8408541034873777, 0.9880222149604795, 0.3587320338781185, 0.7872600905893068, 0.9093454230465788, 0.84470910698967, 0.5527435114418735, 0.9065645587669557, 0.1982099608888398, 0.4911106555072753]}, "task_prompt": ""}
