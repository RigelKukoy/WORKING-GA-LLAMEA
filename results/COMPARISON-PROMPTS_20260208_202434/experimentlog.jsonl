{"method_name": "LLaMEA-Prompt5", "problem_name": "MA_BBOB", "llm_name": "gemini-2.0-flash", "method": {"method_name": "LLaMEA-Prompt5", "budget": 100, "kwargs": {"mutation_prompts": ["Refine the strategy of the selected solution to improve it.", "Generate a new algorithm that is different from the algorithms you have tried before.", "Refine and simplify the selected algorithm to improve it."], "n_parents": 4, "n_offspring": 8, "elitism": true}}, "problem": {"name": "MA_BBOB", "dims": [5], "training_instances": "range(0, 20)", "test_instances": "range(20, 120)", "budget_factor": 2000}, "llm": {"model": "gemini-2.0-flash", "code_pattern": "```(?:python)?\\n(.*?)\\n```", "name_pattern": "class\\s*(\\w*)(?:\\(\\w*\\))?\\:", "desc_pattern": "#\\s*Description\\s*:\\s*(.*)", "cs_pattern": "space\\s*:\\s*\\n*```\\n*(?:python)?\\n(.*?)\\n```"}, "solution": {"id": "0b20a372-d5e6-4da8-ba2e-89b36eb59095", "fitness": 0.7680248364110716, "name": "AdaptiveDifferentialEvolution", "description": "Simplified Adaptive Differential Evolution with self-adaptive parameters and a focus on exploitation around the current best solution.", "code": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget=10000, dim=10, pop_size=30, F=0.5, CR=0.7):\n        \"\"\"\n        Args:\n            budget (int): The maximum number of function evaluations.\n            dim (int): The dimensionality of the search space.\n            pop_size (int): The population size.\n            F (float): Mutation factor.\n            CR (float): Crossover rate.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        # Initialize population within bounds\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        # Find the best individual in the initial population\n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index].copy()\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation: Exploit the best solution\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b = np.random.choice(idxs, 2, replace=False)\n\n                mutant = population[i] + self.F * (self.x_opt - population[a]) + self.F * (population[b] - population[i])\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Boundary handling (clip to bounds)\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Evaluation\n                f = func(trial)\n                self.budget -= 1\n\n                if f < fitness[i]:\n                    fitness[i] = f\n                    population[i] = trial\n\n                    # Update global best\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = trial.copy()\n\n                if self.budget <= 0:\n                    break\n\n            # Self-adaptation of F and CR (simplified)\n            self.F = np.clip(np.random.normal(0.5, 0.1), 0.1, 0.9)\n            self.CR = np.clip(np.random.normal(0.7, 0.1), 0.1, 0.9)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "The algorithm AdaptiveDifferentialEvolution scored 0.768 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["ca08a41a-418b-4ee4-ba18-aaa3d01359f5"], "operator": null, "metadata": {"aucs": [0.34625060343136693, 0.7999224575758979, 0.8635171244978219, 0.9452961305002022, 0.8765408093994155, 0.9124049134064466, 0.33896218724164384, 0.8542118292006768, 0.8900648787089324, 0.8496888275454967, 0.9416018768671305, 0.9908206522947915, 0.3828348118667779, 0.8779459999006002, 0.9471674383792202, 0.9093120825676401, 0.8519996827528526, 0.9303423795280442, 0.3453946044148427, 0.5062174381416298]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}, "log_dir": "run-LLaMEA-Prompt5-MA_BBOB-0", "seed": 0}
{"method_name": "LLaMEA-Prompt5", "problem_name": "MA_BBOB", "llm_name": "gemini-2.0-flash", "method": {"method_name": "LLaMEA-Prompt5", "budget": 100, "kwargs": {"mutation_prompts": ["Refine the strategy of the selected solution to improve it.", "Generate a new algorithm that is different from the algorithms you have tried before.", "Refine and simplify the selected algorithm to improve it."], "n_parents": 4, "n_offspring": 8, "elitism": true}}, "problem": {"name": "MA_BBOB", "dims": [5], "training_instances": "range(0, 20)", "test_instances": "range(20, 120)", "budget_factor": 2000}, "llm": {"model": "gemini-2.0-flash", "code_pattern": "```(?:python)?\\n(.*?)\\n```", "name_pattern": "class\\s*(\\w*)(?:\\(\\w*\\))?\\:", "desc_pattern": "#\\s*Description\\s*:\\s*(.*)", "cs_pattern": "space\\s*:\\s*\\n*```\\n*(?:python)?\\n(.*?)\\n```"}, "solution": {"id": "eaaf8b67-137d-438f-9c3b-652b03ffc80e", "fitness": 0.7665221442909763, "name": "HybridOptimizer", "description": "A hybrid optimizer blending differential evolution with a self-adjusting Gaussian mutation guided by individual success history and a population-wide covariance adaptation, dynamically adjusting exploration based on function landscape and optimization stage.", "code": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget=10000, dim=10, pop_size=20, de_rate=0.7, mutation_factor=0.5, learning_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lb = -5.0\n        self.ub = 5.0\n        self.de_rate = de_rate\n        self.mutation_factor = mutation_factor\n        self.learning_rate = learning_rate\n        self.success_rates = np.zeros(self.pop_size)\n        self.mutation_magnitudes = np.zeros(self.pop_size)\n        self.mutation_scale = 0.1 * (self.ub - self.lb)\n\n        # CMA-ES-like covariance matrix adaptation\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.damps = 1 + self.learning_rate\n        self.cs = 0.3\n        self.cc = 0.3\n        self.mu_eff = self.pop_size / 4  #Heuristic value\n        self.c_mu = 0.1\n        self.c_1 = 0.1\n        \n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget_used = self.pop_size\n\n        self.best_index = np.argmin(self.fitness)\n        self.best_x = self.population[self.best_index].copy()\n        self.best_fitness = self.fitness[self.best_index].copy()\n\n        self.f_opt = self.best_fitness\n        self.x_opt = self.best_x\n\n        while self.budget_used < self.budget:\n            new_population = np.zeros_like(self.population)\n            new_fitness = np.zeros(self.pop_size)\n            \n            # Adaptive Mutation scale based on progress\n            adaptive_mutation_scale = self.mutation_scale * (1 - self.budget_used / self.budget)\n\n            delta_xs = [] #Collect steps for CMA\n\n            for i in range(self.pop_size):\n                old_fitness = self.fitness[i]\n\n                if np.random.rand() < self.de_rate:\n                    # Differential Evolution\n                    indices = np.random.choice(self.pop_size, 3, replace=False)\n                    x_r1, x_r2, x_r3 = self.population[indices]\n                    mutation = x_r1 + self.mutation_factor * (x_r2 - x_r3)\n                    crossover = np.random.rand(self.dim) < 0.7  # Binomial Crossover\n                    new_population[i] = np.where(crossover, mutation, self.population[i])\n                else:\n                    # Self-Adjusting Gaussian Mutation with CMA\n                    z = np.random.normal(0, 1, self.dim)\n                    mutation_vector = adaptive_mutation_scale * np.dot(self.C**0.5, z)\n                    new_population[i] = self.population[i] + mutation_vector\n                    delta_xs.append(mutation_vector)\n\n\n                new_population[i] = np.clip(new_population[i], self.lb, self.ub)\n                new_fitness[i] = func(new_population[i])\n                self.budget_used += 1\n\n                if new_fitness[i] < old_fitness:\n                    # Success! Update individual\n                    self.population[i] = new_population[i]\n                    self.fitness[i] = new_fitness[i]\n                    self.success_rates[i] = (1 - self.learning_rate) * self.success_rates[i] + self.learning_rate\n                    self.mutation_magnitudes[i] = (1 - self.learning_rate) * self.mutation_magnitudes[i] + self.learning_rate * np.linalg.norm(new_population[i] - self.population[i])\n\n                    if new_fitness[i] < self.best_fitness:\n                        self.best_x = new_population[i].copy()\n                        self.best_fitness = new_fitness[i].copy()\n                        if self.best_fitness < self.f_opt:\n                            self.f_opt = self.best_fitness\n                            self.x_opt = self.best_x\n                else:\n                     # Failure, decrease mutation magnitude\n                    self.success_rates[i] = (1 - self.learning_rate) * self.success_rates[i]\n                    self.mutation_magnitudes[i] *= (1 - self.learning_rate)\n\n                if self.budget_used >= self.budget:\n                    break\n\n            #CMA-ES like update\n            delta_xs = np.array(delta_xs)\n            if len(delta_xs) > 0:\n                delta_x_mean = np.mean(delta_xs, axis=0)\n                self.ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs)) * delta_x_mean / adaptive_mutation_scale\n                self.pc = (1 - self.cc) * self.pc + np.sqrt(self.cc * (2 - self.cc)) * delta_x_mean\n                \n                self.C = (1 - self.c_1 - self.c_mu) * self.C + self.c_1 * np.outer(self.pc, self.pc) + self.c_mu * np.mean([np.outer(delta_xs[j], delta_xs[j]) for j in range(len(delta_xs))], axis=0)\n                self.C = np.triu(self.C) + np.triu(self.C, 1).T  # enforce symmetry\n            \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 12, "feedback": "The algorithm HybridOptimizer scored 0.767 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["85378eab-540e-488e-9005-2e7b547753b4"], "operator": null, "metadata": {"aucs": [0.2434654563025913, 0.7709774328909881, 0.8321199557949259, 0.953569881752841, 0.8651918994978469, 0.8998646976218805, 0.7929257724749139, 0.8373759651066008, 0.8714963035500984, 0.8305539995220151, 0.8875801725420387, 0.9935232739234856, 0.3367605340631522, 0.83914767067776, 0.9427705700745924, 0.8872546843884186, 0.7757749734518422, 0.9219734317429621, 0.3056242547255078, 0.5424919557150636]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}, "log_dir": "run-LLaMEA-Prompt5-MA_BBOB-1", "seed": 1}
{"method_name": "LLaMEA-Prompt5", "problem_name": "MA_BBOB", "llm_name": "gemini-2.0-flash", "method": {"method_name": "LLaMEA-Prompt5", "budget": 100, "kwargs": {"mutation_prompts": ["Refine the strategy of the selected solution to improve it.", "Generate a new algorithm that is different from the algorithms you have tried before.", "Refine and simplify the selected algorithm to improve it."], "n_parents": 4, "n_offspring": 8, "elitism": true}}, "problem": {"name": "MA_BBOB", "dims": [5], "training_instances": "range(0, 20)", "test_instances": "range(20, 120)", "budget_factor": 2000}, "llm": {"model": "gemini-2.0-flash", "code_pattern": "```(?:python)?\\n(.*?)\\n```", "name_pattern": "class\\s*(\\w*)(?:\\(\\w*\\))?\\:", "desc_pattern": "#\\s*Description\\s*:\\s*(.*)", "cs_pattern": "space\\s*:\\s*\\n*```\\n*(?:python)?\\n(.*?)\\n```"}, "solution": {"id": "ca85b7ad-0f37-47f9-9049-9648e4f1e39a", "fitness": 0.8025498787083164, "name": "EnhancedAdaptiveDE", "description": "Enhanced Adaptive Differential Evolution with dynamic population sizing, archive-based learning, and a novel mutation strategy incorporating information from top-performing solutions to accelerate convergence.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=10, stagnation_threshold=50):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.stagnation_threshold = stagnation_threshold\n        self.stagnation_counter = 0\n        self.best_fitness_history = []\n        self.pop_factor = 1.0\n        self.archive = []\n        self.memory_f = [0.5] * archive_size\n        self.memory_cr = [0.9] * archive_size\n        self.sf = []\n        self.scr = []\n        self.population = None\n        self.fitness = None\n\n    def initialize_population(self, func):\n        current_pop_size = int(self.pop_size * self.pop_factor)\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(current_pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= current_pop_size\n\n    def mutate(self, i):\n        current_pop_size = int(self.pop_size * self.pop_factor)\n        phi_idx = np.random.randint(0, self.archive_size)\n        F = np.clip(np.random.normal(self.memory_f[phi_idx], 0.1), 0.1, 1.0)\n        CR = np.clip(np.random.normal(self.memory_cr[phi_idx], 0.1), 0.0, 1.0)\n\n        idxs = np.random.choice(current_pop_size, 3, replace=False)\n        a, b, c = idxs[0], idxs[1], idxs[2]\n\n        # Incorporate information from the top 20% performing solutions\n        top_indices = np.argsort(self.fitness)[:max(1, int(0.2 * current_pop_size))]\n        p_best_idx = np.random.choice(top_indices)\n\n        x_mutated = self.population[i] + F * (self.population[p_best_idx] - self.population[i]) + F * (self.population[a] - self.population[b])\n        return x_mutated, F, CR\n\n    def crossover(self, x_mutated, i, cr):\n        x_trial = np.copy(self.population[i])\n        j_rand = np.random.randint(self.dim)\n        for j in range(self.dim):\n            if np.random.rand() < cr or j == j_rand:\n                x_trial[j] = x_mutated[j]\n        return x_trial\n\n    def update_memory(self):\n        if self.sf:\n            self.memory_f.append(sum(val**2 for val in self.sf) / sum(self.sf) if sum(self.sf) != 0 else self.memory_f[-1])\n            self.memory_f.pop(0)\n            self.memory_cr.append(np.mean(self.scr))\n            self.memory_cr.pop(0)\n        self.sf = []\n        self.scr = []\n\n    def check_stagnation(self):\n        if len(self.best_fitness_history) > self.stagnation_threshold:\n            if np.std(self.best_fitness_history[-self.stagnation_threshold:]) < 1e-6:\n                self.stagnation_counter += 1\n                return True\n            else:\n                self.stagnation_counter = 0\n                return False\n        return False\n\n    def restart_population(self, func):\n        self.pop_factor *= 0.75  # Reduce population size\n        self.pop_factor = max(0.2, self.pop_factor) # Prevent too small pop size\n        self.initialize_population(func)\n        self.best_fitness_history = []\n        self.memory_f = [0.5] * self.archive_size\n        self.memory_cr = [0.9] * self.archive_size\n        self.sf = []\n        self.scr = []\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)].copy()\n        self.stagnation_counter = 0\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)].copy()\n        self.sf = []\n        self.scr = []\n\n        while self.budget > 0:\n            current_pop_size = int(self.pop_size * self.pop_factor)\n            for i in range(current_pop_size):\n                x_mutated, F, CR = self.mutate(i)\n                x_trial = self.crossover(x_mutated, i, CR)\n                x_trial = np.clip(x_trial, func.bounds.lb, func.bounds.ub)\n                f_trial = func(x_trial)\n                self.budget -= 1\n                if self.budget <= 0:\n                    break\n\n                if f_trial < self.fitness[i]:\n                    self.sf.append(F)\n                    self.scr.append(CR)\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(self.population[i].copy())\n                    else:\n                        j = np.random.randint(0, self.archive_size)\n                        self.archive[j] = self.population[i].copy()\n                    self.population[i] = x_trial.copy()\n                    self.fitness[i] = f_trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = x_trial.copy()\n\n            if self.budget <= 0:\n                break\n\n            self.update_memory()\n            best_index = np.argmin(self.fitness)\n            if self.fitness[best_index] < self.f_opt:\n                self.f_opt = self.fitness[best_index]\n                self.x_opt = self.population[best_index].copy()\n\n            self.best_fitness_history.append(self.f_opt)\n\n            if self.check_stagnation() and self.budget > int(self.pop_size * self.pop_factor) * 2:\n                self.restart_population(func)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "The algorithm EnhancedAdaptiveDE scored 0.803 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["b1035a21-7f93-411a-8c32-f80b40512d34"], "operator": null, "metadata": {"aucs": [0.3116433221531254, 0.8207972787351255, 0.8305635780657511, 0.9286556487883807, 0.8668568979116084, 0.8785062374656882, 0.7587164065316929, 0.8098526238474882, 0.8469187082545884, 0.8036111322908566, 0.9328772755741994, 0.9888171212864405, 0.8343738563336566, 0.8436764984231312, 0.9432946066620125, 0.8873486305527587, 0.7457242290666846, 0.9152551086299031, 0.588100778873019, 0.5154076347202161]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}, "log_dir": "run-LLaMEA-Prompt5-MA_BBOB-2", "seed": 2}
{"method_name": "LLaMEA-Prompt5", "problem_name": "MA_BBOB", "llm_name": "gemini-2.0-flash", "method": {"method_name": "LLaMEA-Prompt5", "budget": 100, "kwargs": {"mutation_prompts": ["Refine the strategy of the selected solution to improve it.", "Generate a new algorithm that is different from the algorithms you have tried before.", "Refine and simplify the selected algorithm to improve it."], "n_parents": 4, "n_offspring": 8, "elitism": true}}, "problem": {"name": "MA_BBOB", "dims": [5], "training_instances": "range(0, 20)", "test_instances": "range(20, 120)", "budget_factor": 2000}, "llm": {"model": "gemini-2.0-flash", "code_pattern": "```(?:python)?\\n(.*?)\\n```", "name_pattern": "class\\s*(\\w*)(?:\\(\\w*\\))?\\:", "desc_pattern": "#\\s*Description\\s*:\\s*(.*)", "cs_pattern": "space\\s*:\\s*\\n*```\\n*(?:python)?\\n(.*?)\\n```"}, "solution": {"id": "d410c33a-d3f9-4527-86d4-116e96fd5667", "fitness": 0.8099054449011044, "name": "EnhancedAdaptiveDE", "description": "Adaptive DE with self-adaptive mutation probabilities based on recent successes, simplified parameter adaptation, and a more robust restart mechanism.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=20):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = 0.5\n        self.CR = 0.7\n        self.lb = -5.0\n        self.ub = 5.0\n        self.evals = 0\n        self.success_history_F = []\n        self.success_history_CR = []\n        self.history_length = 5\n        self.mutation_strategies = ['rand1', 'current_to_best', 'rand2']\n        self.mutation_success_rates = np.ones(len(self.mutation_strategies)) / len(self.mutation_strategies)\n\n    def __call__(self, func):\n        self.pop = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.evals = self.pop_size\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.pop[np.argmin(self.fitness)]\n        self.generation = 0\n\n        while self.evals < self.budget:\n            self.generation += 1\n\n            for i in range(self.pop_size):\n                # Mutation Strategy Selection\n                mutation_idx = np.random.choice(len(self.mutation_strategies), p=self.mutation_success_rates)\n                mutation_strategy = self.mutation_strategies[mutation_idx]\n\n                # Mutation\n                if mutation_strategy == 'rand1':\n                    r1, r2, r3 = np.random.choice(self.pop_size, 3, replace=False)\n                    mutant = self.pop[r1] + self.F * (self.pop[r2] - self.pop[r3])\n                elif mutation_strategy == 'current_to_best':\n                    r1, r2 = np.random.choice(self.pop_size, 2, replace=False)\n                    best_idx = np.argmin(self.fitness)\n                    mutant = self.pop[i] + self.F * (self.pop[best_idx] - self.pop[i]) + self.F * (self.pop[r1] - self.pop[r2])\n                elif mutation_strategy == 'rand2':\n                    r1, r2, r3, r4, r5 = np.random.choice(self.pop_size, 5, replace=False)\n                    mutant = self.pop[r1] + self.F * (self.pop[r2] - self.pop[r3]) + self.F * (self.pop[r4] - self.pop[r5])\n                else:\n                    raise ValueError(\"Invalid mutation strategy\")\n\n                mutant = np.clip(mutant, self.lb, self.ub)\n\n                # Crossover\n                crossover_mask = np.random.rand(self.dim) < self.CR\n                trial = np.where(crossover_mask, mutant, self.pop[i])\n\n                # Selection\n                f_trial = func(trial)\n                self.evals += 1\n\n                if f_trial < self.fitness[i]:\n                    # Success!\n                    self.pop[i] = trial\n                    self.fitness[i] = f_trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n                    # Update mutation success rates\n                    self.mutation_success_rates[mutation_idx] += 0.1 * (1 - self.mutation_success_rates[mutation_idx])\n                    self.mutation_success_rates /= np.sum(self.mutation_success_rates)\n\n\n                else:\n                    # Decrease success rate if mutation failed\n                    self.mutation_success_rates[mutation_idx] *= 0.9\n                    self.mutation_success_rates /= np.sum(self.mutation_success_rates)\n\n\n                if self.evals >= self.budget:\n                    break\n\n            # Adapt F and CR (simplified)\n            if self.generation % 10 == 0:\n                 self.F = 0.5 + 0.3 * np.random.randn()\n                 self.CR = 0.7 + 0.1 * np.random.randn()\n                 self.F = np.clip(self.F, 0.1, 0.9)\n                 self.CR = np.clip(self.CR, 0.1, 0.9)\n\n\n            # Population Rejuvenation based on fitness stagnation\n            if self.generation % 50 == 0:\n                if np.std(self.fitness) < 1e-6:\n                    indices = np.argsort(self.fitness)[self.pop_size // 2:]  # Replace worst half\n                    self.pop[indices] = np.random.uniform(self.lb, self.ub, size=(len(indices), self.dim))\n                    self.fitness[indices] = [func(x) for x in self.pop[indices]]\n                    self.evals += len(indices)\n                    if self.evals >= self.budget:\n                        break\n                    self.f_opt = np.min(self.fitness)\n                    self.x_opt = self.pop[np.argmin(self.fitness)]\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 12, "feedback": "The algorithm EnhancedAdaptiveDE scored 0.810 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["bd93e02e-d2f4-47f0-8c99-fa122fa42187"], "operator": null, "metadata": {"aucs": [0.31835478541753426, 0.8192051557035122, 0.7905675689015408, 0.9260964140627995, 0.8856702161522975, 0.9208197479166909, 0.8431154691516838, 0.8508405410602052, 0.8708677094886331, 0.8543332837690532, 0.9384798680005713, 0.9977233864503565, 0.490804151707115, 0.8710006253155421, 0.9576186951584398, 0.8878489168150989, 0.7484194250168008, 0.9413672737697929, 0.7503684651565387, 0.534607199007883]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}, "log_dir": "run-LLaMEA-Prompt5-MA_BBOB-3", "seed": 3}
{"method_name": "LLaMEA-Prompt5", "problem_name": "MA_BBOB", "llm_name": "gemini-2.0-flash", "method": {"method_name": "LLaMEA-Prompt5", "budget": 100, "kwargs": {"mutation_prompts": ["Refine the strategy of the selected solution to improve it.", "Generate a new algorithm that is different from the algorithms you have tried before.", "Refine and simplify the selected algorithm to improve it."], "n_parents": 4, "n_offspring": 8, "elitism": true}}, "problem": {"name": "MA_BBOB", "dims": [5], "training_instances": "range(0, 20)", "test_instances": "range(20, 120)", "budget_factor": 2000}, "llm": {"model": "gemini-2.0-flash", "code_pattern": "```(?:python)?\\n(.*?)\\n```", "name_pattern": "class\\s*(\\w*)(?:\\(\\w*\\))?\\:", "desc_pattern": "#\\s*Description\\s*:\\s*(.*)", "cs_pattern": "space\\s*:\\s*\\n*```\\n*(?:python)?\\n(.*?)\\n```"}, "solution": {"id": "baf81fef-da08-4bf1-a2b9-0d32ed61131d", "fitness": 0.7630542438722347, "name": "CooperativeAdaptiveDE", "description": "Cooperative Differential Evolution with a population split into explorers and exploiters, guided by a shared archive and self-adaptive parameter control.", "code": "import numpy as np\n\nclass CooperativeAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, explorer_ratio=0.5, archive_size=10, F=0.5, Cr=0.9, lr_F=0.1, lr_Cr=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.explorer_ratio = explorer_ratio  # Ratio of population dedicated to exploration\n        self.archive_size = archive_size\n        self.F = F\n        self.Cr = Cr\n        self.lr_F = lr_F\n        self.lr_Cr = lr_Cr\n        self.archive = []\n        self.archive_fitness = []\n        self.explorer_size = int(self.pop_size * self.explorer_ratio)\n        self.exploiter_size = self.pop_size - self.explorer_size\n\n    def __call__(self, func):\n        lb, ub = func.bounds.lb, func.bounds.ub\n        population = np.random.uniform(lb, ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n        self.f_opt = np.min(fitness)\n        self.x_opt = population[np.argmin(fitness)]\n\n        while self.budget > 0:\n            # Split population into explorers and exploiters\n            explorer_indices = np.argsort(fitness)[:self.explorer_size]  # Worst individuals become explorers\n            exploiter_indices = np.argsort(fitness)[self.explorer_size:] # Best individuals become exploiters\n            explorers = population[explorer_indices]\n            exploiters = population[exploiter_indices]\n            explorer_fitness = fitness[explorer_indices]\n            exploiter_fitness = fitness[exploiter_indices]\n            \n            # Explorer update\n            for i in range(self.explorer_size):\n                idxs = [idx for idx in range(self.pop_size) if idx != explorer_indices[i]]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = explorers[i] + self.F * (b - c) + np.random.normal(0, 0.1, size=self.dim) # Add noise for exploration\n                mutant = np.clip(mutant, lb, ub)\n                trial = self.crossover(explorers[i], mutant, self.Cr)\n                f_trial = func(trial)\n                self.budget -= 1\n                if f_trial < explorer_fitness[i]:\n                    explorer_fitness[i] = f_trial\n                    explorers[i] = trial\n                    population[explorer_indices[i]] = trial\n                    fitness[explorer_indices[i]] = f_trial\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                    self.update_archive(trial, f_trial)\n\n            # Exploiter update\n            for i in range(self.exploiter_size):\n                idxs = [idx for idx in range(self.pop_size) if idx != exploiter_indices[i]]\n                #Exploitation around best solution using archive\n                if len(self.archive) > 0:\n                    archive_idx = np.random.randint(len(self.archive))\n                    a = self.archive[archive_idx]\n                    b, c = population[np.random.choice(idxs, 2, replace=False)]\n                    mutant = np.clip(a + self.F * (b - c), lb, ub)\n                else:\n                    a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                    mutant = np.clip(a + self.F * (b - c), lb, ub)\n                \n                trial = self.crossover(exploiters[i], mutant, self.Cr)\n                f_trial = func(trial)\n                self.budget -= 1\n\n                if f_trial < exploiter_fitness[i]:\n                    exploiter_fitness[i] = f_trial\n                    exploiters[i] = trial\n                    population[exploiter_indices[i]] = trial\n                    fitness[exploiter_indices[i]] = f_trial\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                    self.update_archive(trial, f_trial)\n\n            # Adaptive F and Cr\n            self.F = np.clip(self.F + self.lr_F * np.random.normal(0, 0.1), 0.1, 1.0)\n            self.Cr = np.clip(self.Cr + self.lr_Cr * np.random.normal(0, 0.1), 0.1, 1.0)\n\n        return self.f_opt, self.x_opt\n\n    def crossover(self, x, mutant, cr):\n        trial = x.copy()\n        for j in range(self.dim):\n            if np.random.rand() < cr:\n                trial[j] = mutant[j]\n        return trial\n\n    def update_archive(self, x, fitness):\n        if len(self.archive) < self.archive_size:\n            self.archive.append(x.copy())\n            self.archive_fitness.append(fitness)\n        else:\n            max_archive_index = np.argmax(self.archive_fitness)\n            if fitness < self.archive_fitness[max_archive_index]:\n                self.archive[max_archive_index] = x.copy()\n                self.archive_fitness[max_archive_index] = fitness", "configspace": "", "generation": 10, "feedback": "The algorithm CooperativeAdaptiveDE scored 0.763 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["c644790d-f6f7-49f4-87d6-80d5786151e8"], "operator": null, "metadata": {"aucs": [0.47264145982465133, 0.8029778061066286, 0.8057528939251792, 0.9287438032465801, 0.8033285097426462, 0.833695522512675, 0.8093090569421292, 0.7321596310517002, 0.8127436990707841, 0.7688537322846603, 0.9194590042627068, 0.9920219215073159, 0.671616481770777, 0.8144466679454256, 0.9487135705450869, 0.8590532230015604, 0.6537551414792473, 0.8720829101757699, 0.2209675340544458, 0.5387623079947235]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}, "log_dir": "run-LLaMEA-Prompt5-MA_BBOB-4", "seed": 4}
{"method_name": "GA-LLAMEA-Baseline", "problem_name": "MA_BBOB", "llm_name": "gemini-2.0-flash", "method": {"method_name": "GA-LLAMEA-Baseline", "budget": 100, "n_parents": 4, "n_offspring": 8, "elitism": true, "discount": 0.9, "tau_max": 0.2, "method_type": "GA-LLAMEA", "bandit_state": {"mutation": {"count": 1.6663739820174017, "mean": 7.044604328634174e-05, "var": 1.758663642934036e-06, "std": 0.0013261461619799063, "theta": -0.0020999182805444007, "pulls": 29}, "crossover": {"count": 5.586284589504082, "mean": 0.0010063207798402852, "var": 5.883672560779844e-06, "std": 0.0024256282816581447, "theta": 0.005636254950023362, "pulls": 40}, "random_new": {"count": 2.746936590818291, "mean": 0.000581021873217663, "var": 3.980205593454058e-06, "std": 0.0019950452610038847, "theta": -0.0007394424498591296, "pulls": 27}}}, "problem": {"name": "MA_BBOB", "dims": [5], "training_instances": "range(0, 20)", "test_instances": "range(20, 120)", "budget_factor": 2000}, "llm": {"model": "gemini-2.0-flash", "code_pattern": "```(?:python)?\\n(.*?)\\n```", "name_pattern": "class\\s*(\\w*)(?:\\(\\w*\\))?\\:", "desc_pattern": "#\\s*Description\\s*:\\s*(.*)", "cs_pattern": "space\\s*:\\s*\\n*```\\n*(?:python)?\\n(.*?)\\n```"}, "solution": {"id": "5b77f9fb-407a-4344-aa0a-a95d9f7e8690", "fitness": 0.733614602664464, "name": "AdaptiveDEBirdSwarm", "description": "A hybrid algorithm combining adaptive Differential Evolution with Bird Swarm Optimization's neighborhood search and Cauchy mutation for enhanced exploration and exploitation.", "code": "import numpy as np\n\nclass AdaptiveDEBirdSwarm:\n    def __init__(self, budget=10000, dim=10, pop_size=40, F=0.5, CR=0.7, neighborhood_size=5, cauchy_probability=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F  # Mutation factor\n        self.CR = CR  # Crossover rate\n        self.lb = -5.0\n        self.ub = 5.0\n        self.stagnation_counter = 0\n        self.stagnation_threshold = budget // 10\n        self.best_fitness_history = []\n        self.neighborhood_size = neighborhood_size\n        self.cauchy_probability = cauchy_probability\n        self.cauchy_scale = 0.1\n        self.shrink_factor = 0.95\n\n    def __call__(self, func):\n        self.population = np.random.uniform(self.lb, self.ub, (self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n        self.eval_count = self.pop_size\n\n        while self.eval_count < self.budget:\n            for i in range(self.pop_size):\n                # Adaptive F and CR\n                adaptive_F = self.F * (1 + 0.1 * np.random.randn())\n                adaptive_CR = self.CR * (1 + 0.1 * np.random.randn())\n                adaptive_F = np.clip(adaptive_F, 0.1, 1.0)\n                adaptive_CR = np.clip(adaptive_CR, 0.1, 1.0)\n\n                # Mutation (DE part)\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = np.random.choice(idxs, 3, replace=False)\n                mutant = self.population[a] + adaptive_F * (self.population[b] - self.population[c])\n                mutant = np.clip(mutant, self.lb, self.ub)\n\n                # Crossover\n                trial = np.copy(self.population[i])\n                mask = np.random.rand(self.dim) < adaptive_CR\n                trial[mask] = mutant[mask]\n                trial = np.clip(trial, self.lb, self.ub)\n\n                # Bird Swarm Inspired Local Search and Cauchy Mutation\n                if np.random.rand() < 0.5: # Apply local search with 50% probability\n                    # Neighborhood Search\n                    neighbors_indices = np.random.choice(self.pop_size, size=self.neighborhood_size, replace=False)\n                    neighbors = self.population[neighbors_indices]\n                    neighbors_fitness = self.fitness[neighbors_indices]\n                    best_neighbor_index = neighbors_indices[np.argmin(neighbors_fitness)]\n                    best_neighbor = self.population[best_neighbor_index]\n\n                    # Move towards the best neighbor\n                    trial = trial + 0.1 * (best_neighbor - trial)\n                    trial = np.clip(trial, self.lb, self.ub)\n\n                    # Cauchy Mutation\n                    if np.random.rand() < self.cauchy_probability:\n                        mutation = trial + (self.ub - self.lb) * np.random.standard_cauchy(size=self.dim) * self.cauchy_scale\n                        mutation = np.clip(mutation, self.lb, self.ub)\n                        trial = mutation\n\n                # Selection\n                f_trial = func(trial)\n                self.eval_count += 1\n\n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n            # Stagnation Check and Restart\n            self.best_fitness_history.append(self.f_opt)\n            if len(self.best_fitness_history) > self.stagnation_threshold:\n                if self.best_fitness_history[-1] >= self.best_fitness_history[-self.stagnation_threshold]:\n                    self.stagnation_counter += 1\n                else:\n                    self.stagnation_counter = 0\n\n            if self.stagnation_counter >= self.stagnation_threshold:\n                #Restart the population\n                self.population = np.random.uniform(self.lb, self.ub, (self.pop_size, self.dim))\n                self.fitness = np.array([func(x) for x in self.population])\n                self.f_opt = np.min(self.fitness)\n                self.x_opt = self.population[np.argmin(self.fitness)]\n                self.eval_count += self.pop_size - (self.budget-self.eval_count) if self.eval_count + self.pop_size > self.budget else self.pop_size\n                self.stagnation_counter = 0\n                self.best_fitness_history = [self.f_opt]\n\n            if self.eval_count >= self.budget:\n                break\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 10, "feedback": "The algorithm AdaptiveDEBirdSwarm scored 0.734 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["7f39ad48-10bd-4693-bf48-a08aea65c982", "5ed072b3-d57e-4c1e-8fc4-1c2b4f7cf697"], "operator": "crossover", "metadata": {"llm_response": "# Description: A hybrid algorithm combining adaptive Differential Evolution with Bird Swarm Optimization's neighborhood search and Cauchy mutation for enhanced exploration and exploitation.\n# Code: \n```python\nimport numpy as np\n\nclass AdaptiveDEBirdSwarm:\n    def __init__(self, budget=10000, dim=10, pop_size=40, F=0.5, CR=0.7, neighborhood_size=5, cauchy_probability=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F  # Mutation factor\n        self.CR = CR  # Crossover rate\n        self.lb = -5.0\n        self.ub = 5.0\n        self.stagnation_counter = 0\n        self.stagnation_threshold = budget // 10\n        self.best_fitness_history = []\n        self.neighborhood_size = neighborhood_size\n        self.cauchy_probability = cauchy_probability\n        self.cauchy_scale = 0.1\n        self.shrink_factor = 0.95\n\n    def __call__(self, func):\n        self.population = np.random.uniform(self.lb, self.ub, (self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n        self.eval_count = self.pop_size\n\n        while self.eval_count < self.budget:\n            for i in range(self.pop_size):\n                # Adaptive F and CR\n                adaptive_F = self.F * (1 + 0.1 * np.random.randn())\n                adaptive_CR = self.CR * (1 + 0.1 * np.random.randn())\n                adaptive_F = np.clip(adaptive_F, 0.1, 1.0)\n                adaptive_CR = np.clip(adaptive_CR, 0.1, 1.0)\n\n                # Mutation (DE part)\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = np.random.choice(idxs, 3, replace=False)\n                mutant = self.population[a] + adaptive_F * (self.population[b] - self.population[c])\n                mutant = np.clip(mutant, self.lb, self.ub)\n\n                # Crossover\n                trial = np.copy(self.population[i])\n                mask = np.random.rand(self.dim) < adaptive_CR\n                trial[mask] = mutant[mask]\n                trial = np.clip(trial, self.lb, self.ub)\n\n                # Bird Swarm Inspired Local Search and Cauchy Mutation\n                if np.random.rand() < 0.5: # Apply local search with 50% probability\n                    # Neighborhood Search\n                    neighbors_indices = np.random.choice(self.pop_size, size=self.neighborhood_size, replace=False)\n                    neighbors = self.population[neighbors_indices]\n                    neighbors_fitness = self.fitness[neighbors_indices]\n                    best_neighbor_index = neighbors_indices[np.argmin(neighbors_fitness)]\n                    best_neighbor = self.population[best_neighbor_index]\n\n                    # Move towards the best neighbor\n                    trial = trial + 0.1 * (best_neighbor - trial)\n                    trial = np.clip(trial, self.lb, self.ub)\n\n                    # Cauchy Mutation\n                    if np.random.rand() < self.cauchy_probability:\n                        mutation = trial + (self.ub - self.lb) * np.random.standard_cauchy(size=self.dim) * self.cauchy_scale\n                        mutation = np.clip(mutation, self.lb, self.ub)\n                        trial = mutation\n\n                # Selection\n                f_trial = func(trial)\n                self.eval_count += 1\n\n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n            # Stagnation Check and Restart\n            self.best_fitness_history.append(self.f_opt)\n            if len(self.best_fitness_history) > self.stagnation_threshold:\n                if self.best_fitness_history[-1] >= self.best_fitness_history[-self.stagnation_threshold]:\n                    self.stagnation_counter += 1\n                else:\n                    self.stagnation_counter = 0\n\n            if self.stagnation_counter >= self.stagnation_threshold:\n                #Restart the population\n                self.population = np.random.uniform(self.lb, self.ub, (self.pop_size, self.dim))\n                self.fitness = np.array([func(x) for x in self.population])\n                self.f_opt = np.min(self.fitness)\n                self.x_opt = self.population[np.argmin(self.fitness)]\n                self.eval_count += self.pop_size - (self.budget-self.eval_count) if self.eval_count + self.pop_size > self.budget else self.pop_size\n                self.stagnation_counter = 0\n                self.best_fitness_history = [self.f_opt]\n\n            if self.eval_count >= self.budget:\n                break\n        return self.f_opt, self.x_opt\n```", "aucs": [0.2822259974612914, 0.6688890995207211, 0.7475605394162931, 0.8929455740115426, 0.8035827376425364, 0.8358699818258939, 0.740242633016669, 0.7389479263641497, 0.8010779517339284, 0.7766985286372584, 0.8463038304718614, 0.9891587422953577, 0.42561126795995174, 0.7774972058941574, 0.924635952046161, 0.8250373262940898, 0.6407770091478557, 0.8654570013869505, 0.5657686637463073, 0.5240040844163039], "operator": "crossover", "theta_sampled": 0.0033115248246760993, "reward": 0.026381888297424494, "generation": 10}, "task_prompt": ""}, "log_dir": "run-GA-LLAMEA-Baseline-MA_BBOB-0", "seed": 0}
{"method_name": "GA-LLAMEA-Baseline", "problem_name": "MA_BBOB", "llm_name": "gemini-2.0-flash", "method": {"method_name": "GA-LLAMEA-Baseline", "budget": 100, "n_parents": 4, "n_offspring": 8, "elitism": true, "discount": 0.9, "tau_max": 0.2, "method_type": "GA-LLAMEA", "bandit_state": {"mutation": {"count": 3.7023602636248794, "mean": 0.001697483175257153, "var": 2.5011701899224206e-06, "std": 0.0015815088333368298, "theta": 0.002270282541068369, "pulls": 28}, "crossover": {"count": 5.1108976238228605, "mean": 0.0, "var": 0.04000000000000001, "std": 0.2, "theta": 0.10347235325103613, "pulls": 37}, "random_new": {"count": 1.186337274892034, "mean": 0.023192970195075113, "var": 0.0017110626809054995, "std": 0.041364993423249806, "theta": -0.013919854808427137, "pulls": 31}}}, "problem": {"name": "MA_BBOB", "dims": [5], "training_instances": "range(0, 20)", "test_instances": "range(20, 120)", "budget_factor": 2000}, "llm": {"model": "gemini-2.0-flash", "code_pattern": "```(?:python)?\\n(.*?)\\n```", "name_pattern": "class\\s*(\\w*)(?:\\(\\w*\\))?\\:", "desc_pattern": "#\\s*Description\\s*:\\s*(.*)", "cs_pattern": "space\\s*:\\s*\\n*```\\n*(?:python)?\\n(.*?)\\n```"}, "solution": {"id": "d3078f05-c4f7-4f99-b633-78d901dbc431", "fitness": 0.7520742068295887, "name": "AdaptiveDifferentialEvolution", "description": "Adaptive Differential Evolution with momentum-based population update and self-adaptive crossover rate.", "code": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget=10000, dim=10, pop_size=40, F=0.5, CR=0.9, momentum=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F  # Initial mutation factor\n        self.CR = CR # Crossover rate\n        self.population = None\n        self.fitness = None\n        self.archive = [] # Archive of successful mutation factors and CR values\n        self.archive_size = 10\n        self.momentum = momentum\n        self.velocity = None # Momentum term for population update\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        self.velocity = np.zeros_like(self.population)\n\n    def mutate(self, x, func):\n        # Adaptive mutation factor based on past successes\n        if self.archive:\n            F_vals = [item[0] for item in self.archive]\n            F = np.random.choice(F_vals)\n        else:\n            F = self.F\n\n        idxs = np.random.choice(self.pop_size, 3, replace=False)\n        x1, x2, x3 = self.population[idxs]\n        v = x1 + F * (x2 - x3)\n        v = np.clip(v, func.bounds.lb, func.bounds.ub)\n        return v\n    \n    def crossover(self, x, v):\n        # Self-adaptive crossover rate\n        if self.archive:\n            CR_vals = [item[1] for item in self.archive]\n            CR = np.random.choice(CR_vals)\n        else:\n            CR = self.CR\n            \n        mask = np.random.rand(self.dim) < CR\n        u = np.where(mask, v, x)\n        return u\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        \n        self.initialize_population(func)\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                x = self.population[i]\n                v = self.mutate(x, func)\n                u = self.crossover(x, v)\n                \n                f_u = func(u)\n                self.budget -= 1\n                if self.budget <= 0:\n                    break\n\n                if f_u < self.fitness[i]:\n                    # Update population and fitness\n                    delta = u - self.population[i]\n                    self.velocity[i] = self.momentum * self.velocity[i] + (1 - self.momentum) * delta\n                    self.population[i] = self.population[i] + self.velocity[i]\n                    self.population[i] = np.clip(self.population[i], func.bounds.lb, func.bounds.ub)\n\n                    self.fitness[i] = f_u\n\n                    # Archive the successful mutation factor and CR\n                    self.archive.append((self.F, self.CR))  # Store both F and CR\n                    if len(self.archive) > self.archive_size:\n                        self.archive.pop(0)\n                \n                if f_u < self.f_opt:\n                    self.f_opt = f_u\n                    self.x_opt = u\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 12, "feedback": "The algorithm AdaptiveDifferentialEvolution scored 0.752 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["f3932941-63da-4e5e-9bec-058fdea3a7fd"], "operator": "mutation", "metadata": {"llm_response": "# Description: Adaptive Differential Evolution with momentum-based population update and self-adaptive crossover rate.\n# Code:\n```python\nimport numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget=10000, dim=10, pop_size=40, F=0.5, CR=0.9, momentum=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F  # Initial mutation factor\n        self.CR = CR # Crossover rate\n        self.population = None\n        self.fitness = None\n        self.archive = [] # Archive of successful mutation factors and CR values\n        self.archive_size = 10\n        self.momentum = momentum\n        self.velocity = None # Momentum term for population update\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        self.velocity = np.zeros_like(self.population)\n\n    def mutate(self, x, func):\n        # Adaptive mutation factor based on past successes\n        if self.archive:\n            F_vals = [item[0] for item in self.archive]\n            F = np.random.choice(F_vals)\n        else:\n            F = self.F\n\n        idxs = np.random.choice(self.pop_size, 3, replace=False)\n        x1, x2, x3 = self.population[idxs]\n        v = x1 + F * (x2 - x3)\n        v = np.clip(v, func.bounds.lb, func.bounds.ub)\n        return v\n    \n    def crossover(self, x, v):\n        # Self-adaptive crossover rate\n        if self.archive:\n            CR_vals = [item[1] for item in self.archive]\n            CR = np.random.choice(CR_vals)\n        else:\n            CR = self.CR\n            \n        mask = np.random.rand(self.dim) < CR\n        u = np.where(mask, v, x)\n        return u\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        \n        self.initialize_population(func)\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                x = self.population[i]\n                v = self.mutate(x, func)\n                u = self.crossover(x, v)\n                \n                f_u = func(u)\n                self.budget -= 1\n                if self.budget <= 0:\n                    break\n\n                if f_u < self.fitness[i]:\n                    # Update population and fitness\n                    delta = u - self.population[i]\n                    self.velocity[i] = self.momentum * self.velocity[i] + (1 - self.momentum) * delta\n                    self.population[i] = self.population[i] + self.velocity[i]\n                    self.population[i] = np.clip(self.population[i], func.bounds.lb, func.bounds.ub)\n\n                    self.fitness[i] = f_u\n\n                    # Archive the successful mutation factor and CR\n                    self.archive.append((self.F, self.CR))  # Store both F and CR\n                    if len(self.archive) > self.archive_size:\n                        self.archive.pop(0)\n                \n                if f_u < self.f_opt:\n                    self.f_opt = f_u\n                    self.x_opt = u\n\n        return self.f_opt, self.x_opt\n```", "aucs": [0.3732147045537516, 0.7521416401002037, 0.7370265421332944, 0.906162174655273, 0.8324243966906475, 0.8590757576729151, 0.7186861525990511, 0.7333547564615659, 0.8328038666160118, 0.7824620203871142, 0.8985798481362637, 0.9922324693675679, 0.7136101424869846, 0.7821534271718849, 0.9394216102575037, 0.822719747455078, 0.7290028222796688, 0.8825970895482876, 0.22499766484801087, 0.5288173031706924], "operator": "mutation", "theta_sampled": 0.04867092848399234, "reward": 0.007758901204164337, "generation": 12}, "task_prompt": ""}, "log_dir": "run-GA-LLAMEA-Baseline-MA_BBOB-1", "seed": 1}
{"method_name": "GA-LLAMEA-Baseline", "problem_name": "MA_BBOB", "llm_name": "gemini-2.0-flash", "method": {"method_name": "GA-LLAMEA-Baseline", "budget": 100, "n_parents": 4, "n_offspring": 8, "elitism": true, "discount": 0.9, "tau_max": 0.2, "method_type": "GA-LLAMEA", "bandit_state": {"mutation": {"count": 4.88276672403653, "mean": 0.0003928577770476522, "var": 3.667629480800603e-07, "std": 0.001, "theta": 0.0021419546911079983, "pulls": 34}, "crossover": {"count": 1.862454830752717, "mean": 3.769895240496419e-05, "var": 4.832329623406739e-07, "std": 0.001, "theta": -0.0010410116404809178, "pulls": 23}, "random_new": {"count": 3.2543736075505274, "mean": 0.0, "var": 0.04000000000000001, "std": 0.2, "theta": -0.11738639286924596, "pulls": 39}}}, "problem": {"name": "MA_BBOB", "dims": [5], "training_instances": "range(0, 20)", "test_instances": "range(20, 120)", "budget_factor": 2000}, "llm": {"model": "gemini-2.0-flash", "code_pattern": "```(?:python)?\\n(.*?)\\n```", "name_pattern": "class\\s*(\\w*)(?:\\(\\w*\\))?\\:", "desc_pattern": "#\\s*Description\\s*:\\s*(.*)", "cs_pattern": "space\\s*:\\s*\\n*```\\n*(?:python)?\\n(.*?)\\n```"}, "solution": {"id": "7a017b9b-efe5-465f-b385-bd84e62d53d7", "fitness": 0.7164082434844664, "name": "HybridAdaptiveDECMOImproved", "description": "A hybrid Differential Evolution and CMA-ES algorithm with improved archive handling, step size adaptation, and covariance matrix learning, focusing on better exploration and exploitation.", "code": "import numpy as np\n\nclass HybridAdaptiveDECMOImproved:\n    def __init__(self, budget=10000, dim=10, pop_size=None, archive_size=10, sigma0=0.3, adaptation_rate=0.1, c_cov0=0.1, F=0.5, CR=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size if pop_size is not None else 10 * dim\n        self.archive_size = archive_size\n        self.F = F  # Scaling factor\n        self.CR = CR  # Crossover rate\n        self.archive = []\n        self.archive_fitness = []\n        self.sigma0 = sigma0\n        self.adaptation_rate = adaptation_rate\n        self.success_rate = 0.5\n        self.success_history = []\n        self.c_cov = c_cov0  # CMA-ES learning rate for covariance matrix\n        self.d_sigma = 1.0\n        self.chiN = self.dim**0.5 * (1 - 1/(4*self.dim) + 1/(21*self.dim**2))\n\n        self.mu = self.pop_size // 2 # number of parents/elite individuals\n        self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n        self.weights /= np.sum(self.weights) # normalize\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n\n\n    def __call__(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)].copy()\n\n        self.m = self.x_opt.copy()  # Initialize mean with the best individual\n        self.C = np.eye(self.dim)  # Initialize covariance matrix\n        self.sigma = self.sigma0 #Init sigma\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.B = np.eye(self.dim) #eigenvectors of C\n        self.D = np.ones(self.dim) #eigenvalues of C\n\n        while self.budget > 0:\n            offspring = np.zeros((self.pop_size, self.dim))\n            offspring_fitness = np.zeros(self.pop_size)\n\n            for i in range(self.pop_size):\n                # Mutation (DE part)\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = self.population[idxs]\n\n                # Add archive member with probability\n                if np.random.rand() < 0.1 and len(self.archive) > 0:\n                    x_r3 = self.archive[np.random.randint(len(self.archive))]\n\n                x_mutated = x_r1 + self.F * (x_r2 - x_r3)\n                x_mutated = np.clip(x_mutated, func.bounds.lb, func.bounds.ub)\n\n                # Crossover (DE part)\n                x_trial = np.copy(self.population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR or j == j_rand:\n                        x_trial[j] = x_mutated[j]\n\n                # Selection\n                f_trial = func(x_trial)\n                self.budget -= 1\n                offspring[i] = x_trial\n                offspring_fitness[i] = f_trial\n\n            #Combine parent and offspring populations\n            combined_population = np.vstack((self.population, offspring))\n            combined_fitness = np.concatenate((self.fitness, offspring_fitness))\n\n            # Select the best individuals to form the new population\n            sorted_indices = np.argsort(combined_fitness)\n            self.population = combined_population[sorted_indices[:self.pop_size]]\n            self.fitness = combined_fitness[sorted_indices[:self.pop_size]]\n\n            best_index = np.argmin(self.fitness)\n            y = self.population[best_index] - self.m\n            self.ps = (1 - 0.1) * self.ps + np.sqrt(0.1 * (2 - 0.1) * self.mueff) * y / self.sigma\n\n            hsig = (np.linalg.norm(self.ps) / np.sqrt(1 - (1 - 0.1)**(2 * self.budget / self.pop_size)) / self.chiN < 1.4 + 2/(self.dim+1))\n\n            self.pc = (1 - 0.1) * self.pc + hsig * np.sqrt(0.1 * (2 - 0.1) * self.mueff) * y / self.sigma\n\n            # Weighted update of the covariance matrix\n            oldC = self.C.copy()\n            self.C = (1-self.c_cov) * self.C + self.c_cov * (self.pc[:, None] @ self.pc[None, :])\n\n            for k in range(self.mu):\n                 yk = self.population[k] - self.m\n                 self.C += (self.c_cov / self.mueff) * self.weights[k] * (yk[:,None] @ yk[None,:])\n\n            self.sigma *= np.exp((0.1 / self.d_sigma) * (np.linalg.norm(self.ps) / self.chiN - 1))\n            self.m = self.population[0].copy() # Using the best individual as the new mean\n\n            # Adaptive Covariance Matrix Adaptation (CMA-ES part)\n            self.success_history.append(1 if self.fitness[0] < self.f_opt else 0) # Only update success based on the best in the population\n            if len(self.success_history) > 10 * self.dim:\n                self.success_history.pop(0)\n            self.success_rate = np.mean(self.success_history)\n\n            if self.success_rate > 0.7:\n                self.c_cov *= (1 + self.adaptation_rate)\n            elif self.success_rate < 0.3:\n                self.c_cov *= (1 - self.adaptation_rate)\n            self.c_cov = np.clip(self.c_cov, 0.01, 1.0)\n\n\n            # Archive Update: Adding the best individuals to the archive\n            for i in range(min(self.pop_size, self.archive_size)): # Adding at most archive_size individuals\n                if len(self.archive) < self.archive_size:\n                    self.archive.append(self.population[i].copy())\n                    self.archive_fitness.append(self.fitness[i])\n                else:\n                    max_archive_index = np.argmax(self.archive_fitness)\n                    if self.fitness[i] < self.archive_fitness[max_archive_index]:\n                        self.archive[max_archive_index] = self.population[i].copy()\n                        self.archive_fitness[max_archive_index] = self.fitness[i]\n\n\n\n            if self.fitness[0] < self.f_opt:\n                self.f_opt = self.fitness[0]\n                self.x_opt = self.population[0].copy()\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 11, "feedback": "The algorithm HybridAdaptiveDECMOImproved scored 0.716 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["f2a6fd6d-7de4-4b23-8433-7b43980425ea"], "operator": "mutation", "metadata": {"llm_response": "# Description: A hybrid Differential Evolution and CMA-ES algorithm with improved archive handling, step size adaptation, and covariance matrix learning, focusing on better exploration and exploitation.\n# Code:\n```python\nimport numpy as np\n\nclass HybridAdaptiveDECMOImproved:\n    def __init__(self, budget=10000, dim=10, pop_size=None, archive_size=10, sigma0=0.3, adaptation_rate=0.1, c_cov0=0.1, F=0.5, CR=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size if pop_size is not None else 10 * dim\n        self.archive_size = archive_size\n        self.F = F  # Scaling factor\n        self.CR = CR  # Crossover rate\n        self.archive = []\n        self.archive_fitness = []\n        self.sigma0 = sigma0\n        self.adaptation_rate = adaptation_rate\n        self.success_rate = 0.5\n        self.success_history = []\n        self.c_cov = c_cov0  # CMA-ES learning rate for covariance matrix\n        self.d_sigma = 1.0\n        self.chiN = self.dim**0.5 * (1 - 1/(4*self.dim) + 1/(21*self.dim**2))\n\n        self.mu = self.pop_size // 2 # number of parents/elite individuals\n        self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n        self.weights /= np.sum(self.weights) # normalize\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n\n\n    def __call__(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)].copy()\n\n        self.m = self.x_opt.copy()  # Initialize mean with the best individual\n        self.C = np.eye(self.dim)  # Initialize covariance matrix\n        self.sigma = self.sigma0 #Init sigma\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.B = np.eye(self.dim) #eigenvectors of C\n        self.D = np.ones(self.dim) #eigenvalues of C\n\n        while self.budget > 0:\n            offspring = np.zeros((self.pop_size, self.dim))\n            offspring_fitness = np.zeros(self.pop_size)\n\n            for i in range(self.pop_size):\n                # Mutation (DE part)\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = self.population[idxs]\n\n                # Add archive member with probability\n                if np.random.rand() < 0.1 and len(self.archive) > 0:\n                    x_r3 = self.archive[np.random.randint(len(self.archive))]\n\n                x_mutated = x_r1 + self.F * (x_r2 - x_r3)\n                x_mutated = np.clip(x_mutated, func.bounds.lb, func.bounds.ub)\n\n                # Crossover (DE part)\n                x_trial = np.copy(self.population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR or j == j_rand:\n                        x_trial[j] = x_mutated[j]\n\n                # Selection\n                f_trial = func(x_trial)\n                self.budget -= 1\n                offspring[i] = x_trial\n                offspring_fitness[i] = f_trial\n\n            #Combine parent and offspring populations\n            combined_population = np.vstack((self.population, offspring))\n            combined_fitness = np.concatenate((self.fitness, offspring_fitness))\n\n            # Select the best individuals to form the new population\n            sorted_indices = np.argsort(combined_fitness)\n            self.population = combined_population[sorted_indices[:self.pop_size]]\n            self.fitness = combined_fitness[sorted_indices[:self.pop_size]]\n\n            best_index = np.argmin(self.fitness)\n            y = self.population[best_index] - self.m\n            self.ps = (1 - 0.1) * self.ps + np.sqrt(0.1 * (2 - 0.1) * self.mueff) * y / self.sigma\n\n            hsig = (np.linalg.norm(self.ps) / np.sqrt(1 - (1 - 0.1)**(2 * self.budget / self.pop_size)) / self.chiN < 1.4 + 2/(self.dim+1))\n\n            self.pc = (1 - 0.1) * self.pc + hsig * np.sqrt(0.1 * (2 - 0.1) * self.mueff) * y / self.sigma\n\n            # Weighted update of the covariance matrix\n            oldC = self.C.copy()\n            self.C = (1-self.c_cov) * self.C + self.c_cov * (self.pc[:, None] @ self.pc[None, :])\n\n            for k in range(self.mu):\n                 yk = self.population[k] - self.m\n                 self.C += (self.c_cov / self.mueff) * self.weights[k] * (yk[:,None] @ yk[None,:])\n\n            self.sigma *= np.exp((0.1 / self.d_sigma) * (np.linalg.norm(self.ps) / self.chiN - 1))\n            self.m = self.population[0].copy() # Using the best individual as the new mean\n\n            # Adaptive Covariance Matrix Adaptation (CMA-ES part)\n            self.success_history.append(1 if self.fitness[0] < self.f_opt else 0) # Only update success based on the best in the population\n            if len(self.success_history) > 10 * self.dim:\n                self.success_history.pop(0)\n            self.success_rate = np.mean(self.success_history)\n\n            if self.success_rate > 0.7:\n                self.c_cov *= (1 + self.adaptation_rate)\n            elif self.success_rate < 0.3:\n                self.c_cov *= (1 - self.adaptation_rate)\n            self.c_cov = np.clip(self.c_cov, 0.01, 1.0)\n\n\n            # Archive Update: Adding the best individuals to the archive\n            for i in range(min(self.pop_size, self.archive_size)): # Adding at most archive_size individuals\n                if len(self.archive) < self.archive_size:\n                    self.archive.append(self.population[i].copy())\n                    self.archive_fitness.append(self.fitness[i])\n                else:\n                    max_archive_index = np.argmax(self.archive_fitness)\n                    if self.fitness[i] < self.archive_fitness[max_archive_index]:\n                        self.archive[max_archive_index] = self.population[i].copy()\n                        self.archive_fitness[max_archive_index] = self.fitness[i]\n\n\n\n            if self.fitness[0] < self.f_opt:\n                self.f_opt = self.fitness[0]\n                self.x_opt = self.population[0].copy()\n\n        return self.f_opt, self.x_opt\n```", "aucs": [0.3438393363489025, 0.706350175289517, 0.6501413743252731, 0.8631240997393157, 0.7897090316997923, 0.7940201967489359, 0.6981672628697289, 0.7119741696831604, 0.7669794639960743, 0.7119555360585033, 0.8695593143372187, 0.9978912053565364, 0.7172636894983045, 0.7329231159614299, 0.8402880707277637, 0.7917346722693784, 0.651166702809469, 0.8501397115332677, 0.3207563230932525, 0.520181417343502], "operator": "mutation", "theta_sampled": 0.008768905297047073, "reward": 0.004951296173146491, "generation": 11}, "task_prompt": ""}, "log_dir": "run-GA-LLAMEA-Baseline-MA_BBOB-2", "seed": 2}
{"method_name": "GA-LLAMEA-Baseline", "problem_name": "MA_BBOB", "llm_name": "gemini-2.0-flash", "method": {"method_name": "GA-LLAMEA-Baseline", "budget": 100, "n_parents": 4, "n_offspring": 8, "elitism": true, "discount": 0.9, "tau_max": 0.2, "method_type": "GA-LLAMEA", "bandit_state": {"mutation": {"count": 3.2248507708992946, "mean": 0.00030987729751734216, "var": 2.1720614010469137e-06, "std": 0.0014737915052838763, "theta": -0.0007121980948916514, "pulls": 51}, "crossover": {"count": 1.8070028102690414, "mean": 9.034227475645057e-08, "var": 4.980620653324507e-07, "std": 0.001, "theta": 0.0003760477116421588, "pulls": 15}, "random_new": {"count": 4.967741581171438, "mean": 4.703638867188429e-07, "var": 2.0129867513469086e-07, "std": 0.001, "theta": 0.0015444982562699912, "pulls": 30}}}, "problem": {"name": "MA_BBOB", "dims": [5], "training_instances": "range(0, 20)", "test_instances": "range(20, 120)", "budget_factor": 2000}, "llm": {"model": "gemini-2.0-flash", "code_pattern": "```(?:python)?\\n(.*?)\\n```", "name_pattern": "class\\s*(\\w*)(?:\\(\\w*\\))?\\:", "desc_pattern": "#\\s*Description\\s*:\\s*(.*)", "cs_pattern": "space\\s*:\\s*\\n*```\\n*(?:python)?\\n(.*?)\\n```"}, "solution": {"id": "b560cb53-c04f-4c2b-a476-50b6c9483c25", "fitness": 0.6716055014906605, "name": "AdaptiveAgingDE", "description": "Adaptive Differential Evolution with aging, dynamically adjusted CR/F parameters and a repair mechanism to handle out-of-bound individuals, aiming for a better exploration-exploitation balance.", "code": "import numpy as np\n\nclass AdaptiveAgingDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, mutation_strategies=None, aging_rate=0.01):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.mutation_strategies = mutation_strategies if mutation_strategies is not None else [\n            \"rand1\", \"best2\", \"currtorand1\"\n        ]\n        self.mutation_probs = np.ones(len(self.mutation_strategies)) / len(self.mutation_strategies)\n        self.CR = 0.7\n        self.F = 0.8\n        self.CR_decay = 0.995  # Decay factor for CR\n        self.F_decay = 0.995  # Decay factor for F\n        self.aging_rate = aging_rate\n        self.age = np.zeros(pop_size)\n\n    def repair(self, x, lb, ub):\n        \"\"\"Repair an individual to lie within the bounds.\"\"\"\n        return np.clip(x, lb, ub)\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        self.population = np.random.uniform(lb, ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.pop_size\n\n        while self.evals < self.budget:\n            mutation_strategy_idx = np.random.choice(len(self.mutation_strategies), p=self.mutation_probs)\n            mutation_strategy = self.mutation_strategies[mutation_strategy_idx]\n\n            for i in range(self.pop_size):\n                # Mutation\n                if mutation_strategy == \"rand1\":\n                    idxs = np.random.choice(self.pop_size, 3, replace=False)\n                    x_1, x_2, x_3 = self.population[idxs]\n                    v = x_1 + self.F * (x_2 - x_3)\n                elif mutation_strategy == \"best2\":\n                    best_idx = np.argmin(self.fitness)\n                    idxs = np.random.choice(self.pop_size, 2, replace=False)\n                    x_1, x_2 = self.population[idxs]\n                    v = self.population[best_idx] + self.F * (x_1 - x_2)\n\n                elif mutation_strategy == \"currtorand1\":\n                    idxs = np.random.choice(self.pop_size, 3, replace=False)\n                    x_1, x_2, x_3 = self.population[idxs]\n                    v = self.population[i] + self.F * (x_1 - self.population[i]) + self.F * (x_2 - x_3)\n                else:\n                    raise ValueError(\"Unknown mutation strategy\")\n\n                # Crossover\n                u = np.copy(self.population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR or j == j_rand:\n                        u[j] = v[j]\n\n                u = self.repair(u, lb, ub)\n\n                # Selection\n                f_u = func(u)\n                self.evals += 1\n\n                if f_u < self.fitness[i]:\n                    # Strategy adaptation: increase probability of successful strategy\n                    self.mutation_probs[mutation_strategy_idx] *= 1.1\n                    self.mutation_probs /= np.sum(self.mutation_probs)  # Normalize probabilities\n\n                    self.fitness[i] = f_u\n                    self.population[i] = u\n                    self.age[i] = 0  # Reset age\n\n                    if f_u < self.f_opt:\n                        self.f_opt = f_u\n                        self.x_opt = u\n                else:\n                    # Strategy adaptation: decrease probability of unsuccessful strategy\n                    self.mutation_probs[mutation_strategy_idx] *= 0.9\n                    self.mutation_probs /= np.sum(self.mutation_probs)  # Normalize probabilities\n                    self.age[i] += 1\n\n                if self.evals >= self.budget:\n                    break\n            \n            # Aging mechanism: replace old individuals with new random ones\n            for i in range(self.pop_size):\n                if self.age[i] > 1.0 / self.aging_rate:\n                    self.population[i] = np.random.uniform(lb, ub)\n                    self.fitness[i] = func(self.population[i])\n                    self.evals += 1\n                    self.age[i] = 0\n                    if self.fitness[i] < self.f_opt:\n                        self.f_opt = self.fitness[i]\n                        self.x_opt = self.population[i]\n            \n            self.CR *= self.CR_decay  # Decay CR\n            self.F *= self.F_decay    # Decay F\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "The algorithm AdaptiveAgingDE scored 0.672 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["45a74d20-a070-43f7-a641-9696fe6cf414"], "operator": "mutation", "metadata": {"llm_response": "# Description: Adaptive Differential Evolution with aging, dynamically adjusted CR/F parameters and a repair mechanism to handle out-of-bound individuals, aiming for a better exploration-exploitation balance.\n# Code: \n```python\nimport numpy as np\n\nclass AdaptiveAgingDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, mutation_strategies=None, aging_rate=0.01):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.mutation_strategies = mutation_strategies if mutation_strategies is not None else [\n            \"rand1\", \"best2\", \"currtorand1\"\n        ]\n        self.mutation_probs = np.ones(len(self.mutation_strategies)) / len(self.mutation_strategies)\n        self.CR = 0.7\n        self.F = 0.8\n        self.CR_decay = 0.995  # Decay factor for CR\n        self.F_decay = 0.995  # Decay factor for F\n        self.aging_rate = aging_rate\n        self.age = np.zeros(pop_size)\n\n    def repair(self, x, lb, ub):\n        \"\"\"Repair an individual to lie within the bounds.\"\"\"\n        return np.clip(x, lb, ub)\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        self.population = np.random.uniform(lb, ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.pop_size\n\n        while self.evals < self.budget:\n            mutation_strategy_idx = np.random.choice(len(self.mutation_strategies), p=self.mutation_probs)\n            mutation_strategy = self.mutation_strategies[mutation_strategy_idx]\n\n            for i in range(self.pop_size):\n                # Mutation\n                if mutation_strategy == \"rand1\":\n                    idxs = np.random.choice(self.pop_size, 3, replace=False)\n                    x_1, x_2, x_3 = self.population[idxs]\n                    v = x_1 + self.F * (x_2 - x_3)\n                elif mutation_strategy == \"best2\":\n                    best_idx = np.argmin(self.fitness)\n                    idxs = np.random.choice(self.pop_size, 2, replace=False)\n                    x_1, x_2 = self.population[idxs]\n                    v = self.population[best_idx] + self.F * (x_1 - x_2)\n\n                elif mutation_strategy == \"currtorand1\":\n                    idxs = np.random.choice(self.pop_size, 3, replace=False)\n                    x_1, x_2, x_3 = self.population[idxs]\n                    v = self.population[i] + self.F * (x_1 - self.population[i]) + self.F * (x_2 - x_3)\n                else:\n                    raise ValueError(\"Unknown mutation strategy\")\n\n                # Crossover\n                u = np.copy(self.population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR or j == j_rand:\n                        u[j] = v[j]\n\n                u = self.repair(u, lb, ub)\n\n                # Selection\n                f_u = func(u)\n                self.evals += 1\n\n                if f_u < self.fitness[i]:\n                    # Strategy adaptation: increase probability of successful strategy\n                    self.mutation_probs[mutation_strategy_idx] *= 1.1\n                    self.mutation_probs /= np.sum(self.mutation_probs)  # Normalize probabilities\n\n                    self.fitness[i] = f_u\n                    self.population[i] = u\n                    self.age[i] = 0  # Reset age\n\n                    if f_u < self.f_opt:\n                        self.f_opt = f_u\n                        self.x_opt = u\n                else:\n                    # Strategy adaptation: decrease probability of unsuccessful strategy\n                    self.mutation_probs[mutation_strategy_idx] *= 0.9\n                    self.mutation_probs /= np.sum(self.mutation_probs)  # Normalize probabilities\n                    self.age[i] += 1\n\n                if self.evals >= self.budget:\n                    break\n            \n            # Aging mechanism: replace old individuals with new random ones\n            for i in range(self.pop_size):\n                if self.age[i] > 1.0 / self.aging_rate:\n                    self.population[i] = np.random.uniform(lb, ub)\n                    self.fitness[i] = func(self.population[i])\n                    self.evals += 1\n                    self.age[i] = 0\n                    if self.fitness[i] < self.f_opt:\n                        self.f_opt = self.fitness[i]\n                        self.x_opt = self.population[i]\n            \n            self.CR *= self.CR_decay  # Decay CR\n            self.F *= self.F_decay    # Decay F\n\n\n        return self.f_opt, self.x_opt\n```", "aucs": [0.1981453456831107, 0.464017318888992, 0.6630950574862853, 0.8368292076142992, 0.7606551089638532, 0.837373824206319, 0.635507455369301, 0.6685788149844429, 0.7637127134079693, 0.6980549326823895, 0.8187125618426727, 0.9992361814559579, 0.33000798769096584, 0.7161059480485212, 0.9028227115359397, 0.8114923000451418, 0.5063634738862176, 0.8425584996957343, 0.4459633429149803, 0.5328772434101149], "operator": "mutation", "theta_sampled": 0.0003093219976439617, "reward": 0.032436205742558455, "generation": 8}, "task_prompt": ""}, "log_dir": "run-GA-LLAMEA-Baseline-MA_BBOB-3", "seed": 3}
{"method_name": "GA-LLAMEA-Baseline", "problem_name": "MA_BBOB", "llm_name": "gemini-2.0-flash", "method": {"method_name": "GA-LLAMEA-Baseline", "budget": 100, "n_parents": 4, "n_offspring": 8, "elitism": true, "discount": 0.9, "tau_max": 0.2, "method_type": "GA-LLAMEA", "bandit_state": {"mutation": {"count": 2.7877722350816336, "mean": 4.1952827956355575e-06, "var": 3.2283832162463444e-07, "std": 0.001, "theta": -0.0007608907309022945, "pulls": 23}, "crossover": {"count": 1.894648744966481, "mean": 2.0880253126764236e-05, "var": 5.278020397472944e-07, "std": 0.001, "theta": 0.002001645601596138, "pulls": 26}, "random_new": {"count": 5.31717418229166, "mean": 0.0012497149007834392, "var": 2.318073959200478e-05, "std": 0.00481463805410176, "theta": -0.002931608058337143, "pulls": 47}}}, "problem": {"name": "MA_BBOB", "dims": [5], "training_instances": "range(0, 20)", "test_instances": "range(20, 120)", "budget_factor": 2000}, "llm": {"model": "gemini-2.0-flash", "code_pattern": "```(?:python)?\\n(.*?)\\n```", "name_pattern": "class\\s*(\\w*)(?:\\(\\w*\\))?\\:", "desc_pattern": "#\\s*Description\\s*:\\s*(.*)", "cs_pattern": "space\\s*:\\s*\\n*```\\n*(?:python)?\\n(.*?)\\n```"}, "solution": {"id": "c11b0a48-b338-4ace-9f74-68f3ec440e0a", "fitness": 0.6880798328575554, "name": "AdaptiveDE_NM", "description": "A population-based algorithm that uses a combination of global search with differential evolution and local search with a Nelder-Mead simplex, adaptively adjusting the balance between exploration and exploitation based on population diversity.", "code": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveDE_NM:\n    def __init__(self, budget=10000, dim=10, pop_size=40, de_cr=0.7, de_f=0.5, nm_freq=100):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.de_cr = de_cr\n        self.de_f = de_f\n        self.nm_freq = nm_freq  # Frequency of Nelder-Mead local search\n        self.pop = None\n        self.fitness = None\n        self.eval_count = 0\n\n    def initialize_population(self, func):\n        self.pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.eval_count = self.pop_size\n        self.best_idx = np.argmin(self.fitness)\n        self.f_opt = self.fitness[self.best_idx]\n        self.x_opt = self.pop[self.best_idx]\n\n    def differential_evolution(self, func):\n        for i in range(self.pop_size):\n            if self.eval_count >= self.budget:\n                break\n\n            idxs = np.random.choice(self.pop_size, 3, replace=False)\n            x_r1, x_r2, x_r3 = self.pop[idxs]\n\n            x_mutated = x_r1 + self.de_f * (x_r2 - x_r3)\n            x_mutated = np.clip(x_mutated, func.bounds.lb, func.bounds.ub)\n\n            j_rand = np.random.randint(self.dim)\n            x_trial = np.copy(self.pop[i])\n            for j in range(self.dim):\n                if np.random.rand() < self.de_cr or j == j_rand:\n                    x_trial[j] = x_mutated[j]\n\n            f_trial = func(x_trial)\n            self.eval_count += 1\n\n            if f_trial < self.fitness[i]:\n                self.fitness[i] = f_trial\n                self.pop[i] = x_trial\n\n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = x_trial\n                    self.best_idx = i\n\n    def nelder_mead_local_search(self, func):\n        for i in range(self.pop_size):\n            if self.eval_count >= self.budget:\n                break\n            \n            # Start Nelder-Mead from the current individual\n            res = minimize(func, self.pop[i], method='Nelder-Mead',\n                           options={'maxfev': self.budget - self.eval_count})  # Limit FE calls\n\n            if res.success:  # Check if optimization was successful\n                f_val = res.fun\n                x_val = res.x\n                n_fev = res.nfev  # Number of function evaluations used by Nelder-Mead\n                \n                self.eval_count += n_fev\n                if f_val < self.fitness[i]:\n                  self.fitness[i] = f_val\n                  self.pop[i] = x_val\n\n                  if f_val < self.f_opt:\n                      self.f_opt = f_val\n                      self.x_opt = x_val\n                      self.best_idx = i\n\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        \n        while self.eval_count < self.budget:\n            self.differential_evolution(func)\n            if self.eval_count >= self.budget:\n              break\n            if self.eval_count // self.nm_freq != (self.eval_count - self.pop_size // self.nm_freq ) // self.nm_freq : #self.eval_count % self.nm_freq == 0: # Perform NM every nm_freq evaluations\n                self.nelder_mead_local_search(func)\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 9, "feedback": "The algorithm AdaptiveDE_NM scored 0.688 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": "random_new", "metadata": {"llm_response": "# Description: A population-based algorithm that uses a combination of global search with differential evolution and local search with a Nelder-Mead simplex, adaptively adjusting the balance between exploration and exploitation based on population diversity.\n# Code: \n```python\nimport numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveDE_NM:\n    def __init__(self, budget=10000, dim=10, pop_size=40, de_cr=0.7, de_f=0.5, nm_freq=100):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.de_cr = de_cr\n        self.de_f = de_f\n        self.nm_freq = nm_freq  # Frequency of Nelder-Mead local search\n        self.pop = None\n        self.fitness = None\n        self.eval_count = 0\n\n    def initialize_population(self, func):\n        self.pop = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.eval_count = self.pop_size\n        self.best_idx = np.argmin(self.fitness)\n        self.f_opt = self.fitness[self.best_idx]\n        self.x_opt = self.pop[self.best_idx]\n\n    def differential_evolution(self, func):\n        for i in range(self.pop_size):\n            if self.eval_count >= self.budget:\n                break\n\n            idxs = np.random.choice(self.pop_size, 3, replace=False)\n            x_r1, x_r2, x_r3 = self.pop[idxs]\n\n            x_mutated = x_r1 + self.de_f * (x_r2 - x_r3)\n            x_mutated = np.clip(x_mutated, func.bounds.lb, func.bounds.ub)\n\n            j_rand = np.random.randint(self.dim)\n            x_trial = np.copy(self.pop[i])\n            for j in range(self.dim):\n                if np.random.rand() < self.de_cr or j == j_rand:\n                    x_trial[j] = x_mutated[j]\n\n            f_trial = func(x_trial)\n            self.eval_count += 1\n\n            if f_trial < self.fitness[i]:\n                self.fitness[i] = f_trial\n                self.pop[i] = x_trial\n\n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = x_trial\n                    self.best_idx = i\n\n    def nelder_mead_local_search(self, func):\n        for i in range(self.pop_size):\n            if self.eval_count >= self.budget:\n                break\n            \n            # Start Nelder-Mead from the current individual\n            res = minimize(func, self.pop[i], method='Nelder-Mead',\n                           options={'maxfev': self.budget - self.eval_count})  # Limit FE calls\n\n            if res.success:  # Check if optimization was successful\n                f_val = res.fun\n                x_val = res.x\n                n_fev = res.nfev  # Number of function evaluations used by Nelder-Mead\n                \n                self.eval_count += n_fev\n                if f_val < self.fitness[i]:\n                  self.fitness[i] = f_val\n                  self.pop[i] = x_val\n\n                  if f_val < self.f_opt:\n                      self.f_opt = f_val\n                      self.x_opt = x_val\n                      self.best_idx = i\n\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        \n        while self.eval_count < self.budget:\n            self.differential_evolution(func)\n            if self.eval_count >= self.budget:\n              break\n            if self.eval_count // self.nm_freq != (self.eval_count - self.pop_size // self.nm_freq ) // self.nm_freq : #self.eval_count % self.nm_freq == 0: # Perform NM every nm_freq evaluations\n                self.nelder_mead_local_search(func)\n\n\n        return self.f_opt, self.x_opt\n```", "aucs": [0.2351698986141485, 0.6233163630915002, 0.7006243095095397, 0.8373153910338667, 0.7904997971614298, 0.8133366668842795, 0.6680802957569111, 0.6800283312503201, 0.7710484999640195, 0.7298616680449175, 0.8572302389691695, 0.9889071896569459, 0.43214064870246427, 0.7052331115688055, 0.9046345151012454, 0.8159207929752833, 0.6159095649732265, 0.839029501500428, 0.22989873132521654, 0.5234111410673911], "operator": "random_new", "theta_sampled": 0.0020596810798100985, "reward": 0.10990515027755332, "generation": 9}, "task_prompt": ""}, "log_dir": "run-GA-LLAMEA-Baseline-MA_BBOB-4", "seed": 4}
{"method_name": "EoH", "problem_name": "MA_BBOB", "llm_name": "gemini-2.0-flash", "method": {"method_name": "EoH", "budget": 100, "kwargs": {"pop_size": 4}}, "problem": {"name": "MA_BBOB", "dims": [5], "training_instances": "range(0, 20)", "test_instances": "range(20, 120)", "budget_factor": 2000}, "llm": {"model": "gemini-2.0-flash", "code_pattern": "```(?:python)?\\n(.*?)\\n```", "name_pattern": "class\\s*(\\w*)(?:\\(\\w*\\))?\\:", "desc_pattern": "#\\s*Description\\s*:\\s*(.*)", "cs_pattern": "space\\s*:\\s*\\n*```\\n*(?:python)?\\n(.*?)\\n```"}, "solution": {"id": "5991fd9e-6cda-44aa-b9b6-a906227cb3f0", "fitness": 0.70018, "name": "SelfAdaptiveDifferentialEvolution", "description": "A differential evolution algorithm with a self-adaptive mutation strategy and a population reduction scheme to balance exploration and exploitation.", "code": "import numpy as np\n\nclass SelfAdaptiveDifferentialEvolution:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, Cr=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F  # Mutation factor\n        self.Cr = Cr  # Crossover rate\n        self.lb = -5.0\n        self.ub = 5.0\n        self.population = None\n        self.fitness = None\n        self.eval_count = 0\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.eval_count += self.pop_size\n        best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_index]\n        self.x_opt = self.population[best_index].copy()\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.eval_count < self.budget:\n            for i in range(self.pop_size):\n                # Mutation\n                indices = [j for j in range(self.pop_size) if j != i]\n                a, b, c = np.random.choice(indices, 3, replace=False)\n\n                # Self-adaptive F\n                F = np.random.normal(self.F, 0.1, 1)[0]\n                F = np.clip(F, 0.1, 1.0)\n\n                mutant = self.population[a] + F * (self.population[b] - self.population[c])\n                mutant = np.clip(mutant, self.lb, self.ub)\n\n                # Crossover\n                crossover_mask = np.random.rand(self.dim) < self.Cr\n                trial_vector = np.where(crossover_mask, mutant, self.population[i])\n\n                # Evaluation\n                f = func(trial_vector)\n                self.eval_count += 1\n\n                # Selection\n                if f < self.fitness[i]:\n                    self.fitness[i] = f\n                    self.population[i] = trial_vector.copy()\n\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = trial_vector.copy()\n                \n                if self.eval_count >= self.budget:\n                    break\n\n            # Population reduction (optional - makes algorithm more complex)\n            if self.pop_size > 10 and self.eval_count > self.budget * 0.75:\n                sorted_indices = np.argsort(self.fitness)[::-1]\n                reduce_amount = max(1, int(0.1 * self.pop_size))\n                self.population = self.population[sorted_indices[:-reduce_amount]]\n                self.fitness = self.fitness[sorted_indices[:-reduce_amount]]\n                self.pop_size = len(self.population)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "", "error": "", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}, "log_dir": "run-EoH-MA_BBOB-0", "seed": 0}
{"method_name": "EoH", "problem_name": "MA_BBOB", "llm_name": "gemini-2.0-flash", "method": {"method_name": "EoH", "budget": 100, "kwargs": {"pop_size": 4}}, "problem": {"name": "MA_BBOB", "dims": [5], "training_instances": "range(0, 20)", "test_instances": "range(20, 120)", "budget_factor": 2000}, "llm": {"model": "gemini-2.0-flash", "code_pattern": "```(?:python)?\\n(.*?)\\n```", "name_pattern": "class\\s*(\\w*)(?:\\(\\w*\\))?\\:", "desc_pattern": "#\\s*Description\\s*:\\s*(.*)", "cs_pattern": "space\\s*:\\s*\\n*```\\n*(?:python)?\\n(.*?)\\n```"}, "solution": {"id": "90c041ea-a6ab-4c08-9aa8-03a40c9c13ed", "fitness": 0.81402, "name": "AdaptiveDEwithCMAandRestart", "description": "This algorithm employs a self-adaptive Differential Evolution with a restart mechanism and covariance matrix adaptation (CMA) for enhanced exploration and exploitation, periodically re-initializing the population based on stagnation detection, and adapting the search distribution using CMA to improve convergence speed.", "code": "import numpy as np\n\nclass AdaptiveDEwithCMAandRestart:\n    def __init__(self, budget=10000, dim=10, pop_size=20, F=0.5, CR=0.7, restart_trigger=0.1, cma_learning_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.restart_trigger = restart_trigger\n        self.cma_learning_rate = cma_learning_rate\n        self.mean = None\n        self.covariance = None\n        self.success_history = []\n        self.stagnation_counter = 0\n        self.restart_threshold = budget // 100 # Number of iterations without improvement before restart\n\n    def initialize_population(self, lb, ub):\n        return np.random.uniform(lb, ub, size=(self.pop_size, self.dim))\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n\n        self.mean = np.random.uniform(lb, ub, size=self.dim)\n        self.covariance = np.eye(self.dim) * ((ub - lb) / 2)**2  # Initialize with a diagonal covariance matrix\n\n        population = self.initialize_population(lb, ub)\n        fitness = np.array([func(x) for x in population])\n        evals = self.pop_size\n\n        best_idx = np.argmin(fitness)\n        self.f_opt = fitness[best_idx]\n        self.x_opt = population[best_idx]\n\n        while evals < self.budget:\n            old_f_opt = self.f_opt\n\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = np.random.choice(idxs, 3, replace=False)\n                mutant = population[a] + self.F * (population[b] - population[c])\n                mutant = np.clip(mutant, lb, ub)\n\n                # Crossover\n                trial = np.copy(population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR or j == j_rand:\n                        trial[j] = mutant[j]\n                \n                trial = np.clip(trial, lb, ub)\n\n                # Selection\n                f_trial = func(trial)\n                evals += 1\n\n                if f_trial < fitness[i]:\n                    population[i] = trial\n                    fitness[i] = f_trial\n                    self.success_history.append(f_trial)\n\n                    # Update best solution\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n            \n            # CMA-ES adaptation of search distribution\n            if self.success_history:\n                self.mean = np.mean(population, axis=0)\n                \n                # Sample a direction from the covariance matrix\n                z = np.random.normal(0, 1, self.dim)\n                direction = np.dot(np.linalg.cholesky(self.covariance), z)\n\n                # Update covariance matrix (simplified)\n                self.covariance = (1 - self.cma_learning_rate) * self.covariance + self.cma_learning_rate * np.outer(direction, direction)\n\n                self.success_history = []\n\n            # Stagnation detection and restart\n            if self.f_opt >= old_f_opt:\n                self.stagnation_counter += 1\n            else:\n                self.stagnation_counter = 0\n\n            if self.stagnation_counter > self.restart_threshold:\n                population = self.initialize_population(lb, ub)\n                fitness = np.array([func(x) for x in population])\n                evals += self.pop_size\n                best_idx = np.argmin(fitness)\n                if fitness[best_idx] < self.f_opt:\n                    self.f_opt = fitness[best_idx]\n                    self.x_opt = population[best_idx]\n                self.mean = np.random.uniform(lb, ub, size=self.dim)\n                self.covariance = np.eye(self.dim) * ((ub - lb) / 2)**2\n                self.stagnation_counter = 0\n\n            # Adapt F and CR (simplified)\n            self.F = 0.5 + 0.3 * np.random.rand()\n            self.CR = 0.6 + 0.4 * np.random.rand()\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "", "error": "", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}, "log_dir": "run-EoH-MA_BBOB-1", "seed": 1}
{"method_name": "EoH", "problem_name": "MA_BBOB", "llm_name": "gemini-2.0-flash", "method": {"method_name": "EoH", "budget": 100, "kwargs": {"pop_size": 4}}, "problem": {"name": "MA_BBOB", "dims": [5], "training_instances": "range(0, 20)", "test_instances": "range(20, 120)", "budget_factor": 2000}, "llm": {"model": "gemini-2.0-flash", "code_pattern": "```(?:python)?\\n(.*?)\\n```", "name_pattern": "class\\s*(\\w*)(?:\\(\\w*\\))?\\:", "desc_pattern": "#\\s*Description\\s*:\\s*(.*)", "cs_pattern": "space\\s*:\\s*\\n*```\\n*(?:python)?\\n(.*?)\\n```"}, "solution": {"id": "aaae279d-b265-4be0-92a5-1bbfca4df927", "fitness": 0.7516, "name": "AdaptiveDifferentialEvolution", "description": "An adaptive differential evolution strategy with a population size adjusted based on function evaluation budget and a self-adaptive mutation factor to balance exploration and exploitation.", "code": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget=10000, dim=10, pop_multiplier=5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = min(budget // 10, dim * pop_multiplier)  # Adaptive population size\n        self.F = 0.5  # Initial mutation factor\n        self.CR = 0.9  # Crossover rate\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        \n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        \n        self.budget -= self.pop_size\n        \n        if np.min(fitness) < self.f_opt:\n            self.f_opt = np.min(fitness)\n            self.x_opt = population[np.argmin(fitness)]\n        \n        \n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = np.clip(a + self.F * (b - c), func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                f = func(trial)\n                self.budget -= 1\n\n                if f < fitness[i]:\n                    fitness[i] = f\n                    population[i] = trial\n                    \n                    # Update mutation factor\n                    if np.random.rand() < 0.1:\n                        self.F = np.random.uniform(0.4, 0.9)\n                    \n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = trial\n\n                if self.budget <= 0:\n                    break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "", "error": "", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}, "log_dir": "run-EoH-MA_BBOB-2", "seed": 2}
{"method_name": "EoH", "problem_name": "MA_BBOB", "llm_name": "gemini-2.0-flash", "method": {"method_name": "EoH", "budget": 100, "kwargs": {"pop_size": 4}}, "problem": {"name": "MA_BBOB", "dims": [5], "training_instances": "range(0, 20)", "test_instances": "range(20, 120)", "budget_factor": 2000}, "llm": {"model": "gemini-2.0-flash", "code_pattern": "```(?:python)?\\n(.*?)\\n```", "name_pattern": "class\\s*(\\w*)(?:\\(\\w*\\))?\\:", "desc_pattern": "#\\s*Description\\s*:\\s*(.*)", "cs_pattern": "space\\s*:\\s*\\n*```\\n*(?:python)?\\n(.*?)\\n```"}, "solution": {"id": "f1b4aede-0d11-4808-8f2d-56815008cc3a", "fitness": 0.76931, "name": "SelfAdaptiveDE", "description": "This algorithm uses a self-adaptive Differential Evolution (SaDE) strategy to dynamically adjust the mutation and crossover parameters based on the success rate of previous generations, promoting a balance between exploration and exploitation.", "code": "import numpy as np\n\nclass SelfAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=20, lb=-5.0, ub=5.0,\n                 num_strategies=4):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lb = lb\n        self.ub = ub\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.num_strategies = num_strategies\n        self.mutation_factors = np.random.uniform(0.5, 1.0, size=num_strategies)\n        self.crossover_rates = np.random.uniform(0.5, 1.0, size=num_strategies)\n        self.success_rates_F = np.ones(num_strategies) / num_strategies\n        self.success_rates_CR = np.ones(num_strategies) / num_strategies\n        self.memory_F = np.zeros(num_strategies)\n        self.memory_CR = np.zeros(num_strategies)\n        self.memory_p = 0.1\n        self.archive_size = int(self.pop_size / 2)\n        self.archive = []\n    \n    def __call__(self, func):\n        # Initialize population\n        population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        # Find initial best\n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index]\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Strategy selection\n                strategy_index = np.random.choice(self.num_strategies, p=self.success_rates_F / np.sum(self.success_rates_F))\n\n                # Mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = a + self.mutation_factors[strategy_index] * (b - c)\n                mutant = np.clip(mutant, self.lb, self.ub)\n\n                # Crossover\n                trial = np.copy(population[i])\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() < self.crossover_rates[strategy_index] or j == j_rand:\n                        trial[j] = mutant[j]\n\n                # Evaluation\n                f_trial = func(trial)\n                self.budget -= 1\n\n                # Selection\n                if f_trial < fitness[i]:\n                    # Update success rates\n                    self.success_rates_F[strategy_index] = (1 - self.memory_p) * self.success_rates_F[strategy_index] + self.memory_p\n                    self.success_rates_CR[strategy_index] = (1 - self.memory_p) * self.success_rates_CR[strategy_index] + self.memory_p\n\n                    population[i] = trial\n                    fitness[i] = f_trial\n\n                    # Update best\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                else:\n                     self.success_rates_F[strategy_index] = (1 - self.memory_p) * self.success_rates_F[strategy_index]\n\n            best_index = np.argmin(fitness)\n            if fitness[best_index] < self.f_opt:\n                self.f_opt = fitness[best_index]\n                self.x_opt = population[best_index]\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "", "error": "", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": ""}, "log_dir": "run-EoH-MA_BBOB-3", "seed": 3}
