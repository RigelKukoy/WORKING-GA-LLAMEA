{"id": "42996575-4eeb-4828-bdd6-e3fdb9d71835", "fitness": 0.3218696949234804, "name": "HybridDECMARestart", "description": "Population-based algorithm using a combination of differential evolution mutation and covariance matrix adaptation with restart strategy to balance exploration and exploitation.", "code": "import numpy as np\n\nclass HybridDECMARestart:\n    def __init__(self, budget=10000, dim=10, pop_size=None, mutation_factor=0.5, crossover_rate=0.7, restart_trigger=100):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size if pop_size is not None else 4 + int(3 * np.log(dim)) # Adaptive population size\n        self.mutation_factor = mutation_factor\n        self.crossover_rate = crossover_rate\n        self.restart_trigger = restart_trigger\n        self.rng = np.random.default_rng()\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.population = self.rng.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.eval_count += self.pop_size\n\n        self.mean = np.mean(self.population, axis=0)\n        self.C = np.eye(self.dim)  # Covariance matrix\n        self.pc = np.zeros(self.dim)  # Evolution path for C\n        self.ps = np.zeros(self.dim)  # Evolution path for step size\n        self.sigma = 0.3  # Overall step size\n        self.damps = 1 + (2 * max(0, np.log((self.dim + 1) / 3)))  # Damping for step size\n        self.chiN = np.sqrt(self.dim) * (1 - (1 / (4 * self.dim)) + (1 / (21 * self.dim**2)))\n\n        self.mu = self.pop_size // 2\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        self.cc = (4 + self.mueff/self.dim) / (self.dim + 4 + 2*self.mueff/self.dim)\n        self.cs = (self.mueff + 2) / (self.dim + self.mueff + 5)\n        self.c1 = 2 / ((self.dim + 1.3)**2 + self.mueff)\n        self.cmu = min(1 - self.c1, 2 * (self.mueff - 2 + 1/self.mueff) / ((self.dim + 2)**2 + self.mueff))\n        \n        self.no_improvement_counter = 0\n        self.best_fitness_ever = np.min(self.fitness)\n        \n        while self.eval_count < self.budget:\n            # Differential Evolution Mutation\n            for i in range(self.pop_size):\n                idxs = self.rng.choice(self.pop_size, 3, replace=False)\n                x_1, x_2, x_3 = self.population[idxs]\n                mutant = self.population[i] + self.mutation_factor * (x_1 - x_2)\n                \n                # Crossover\n                trial = self.population[i].copy()\n                j_rand = self.rng.integers(0, self.dim)\n                for j in range(self.dim):\n                    if self.rng.random() < self.crossover_rate or j == j_rand:\n                        trial[j] = mutant[j]\n                \n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n                f_trial = func(trial)\n                self.eval_count += 1\n\n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                        self.no_improvement_counter = 0\n                else:\n                    self.no_improvement_counter +=1\n\n\n            # CMA-ES Adaptation\n            indices = np.argsort(self.fitness)\n            xmean = np.sum(self.weights[:, None] * self.population[indices[:self.mu]], axis=0)\n            \n            y = (self.population[indices[:self.mu]] - self.mean) / self.sigma\n            self.ps = (1-self.cs) * self.ps + np.sqrt(self.cs*(2-self.cs)) * (np.linalg.solve(np.linalg.cholesky(self.C), (xmean - self.mean) / self.sigma))\n            hsig = np.linalg.norm(self.ps)/np.sqrt(1-(1-self.cs)**(2*(self.eval_count/self.pop_size))) < (1.4 + 2/(self.dim+1)) * self.chiN\n            self.pc = (1-self.cc) * self.pc + hsig * np.sqrt(self.cc*(2-self.cc)) * ((xmean - self.mean) / self.sigma)\n            \n            self.C = (1-self.c1-self.cmu) * self.C + self.c1 * (self.pc[:, None] @ self.pc[None, :]) + self.cmu * np.sum(self.weights[:, None, None] * (y[:, :, None] @ y[:, None, :]), axis=0)\n            self.sigma *= np.exp((self.cs/self.damps) * (np.linalg.norm(self.ps)/self.chiN - 1))\n\n            self.mean = xmean\n\n            self.C = np.triu(self.C) + np.triu(self.C, 1).T # enforce symmetry\n            \n            if np.min(self.fitness) < self.best_fitness_ever:\n                self.best_fitness_ever = np.min(self.fitness)\n                self.no_improvement_counter = 0\n            else:\n                self.no_improvement_counter += 1\n\n            if self.no_improvement_counter > self.restart_trigger or np.any(np.diag(self.C) <= 0):\n                self.population = self.rng.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                self.fitness = np.array([func(x) for x in self.population])\n                self.eval_count += self.pop_size\n                self.mean = np.mean(self.population, axis=0)\n                self.C = np.eye(self.dim)\n                self.pc = np.zeros(self.dim)\n                self.ps = np.zeros(self.dim)\n                self.sigma = 0.3\n                self.no_improvement_counter = 0\n                \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDECMARestart scored 0.322 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.13324052654876561, 0.22697717999362743, 0.3835139982137289, 0.9467951638254196, 0.24069130095934077, 0]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "0b543e23-371f-432d-a057-a2e5b307c05c", "fitness": 0.45171199921035116, "name": "AdaptiveDifferentialEvolution", "description": "A population-based algorithm that combines elements of differential evolution with a self-adaptive learning rate and a restart mechanism to escape local optima.", "code": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.7, CR=0.9, learning_rate=0.1, restart_patience=500):\n        \"\"\"\n        Args:\n            budget (int): The maximum number of function evaluations.\n            dim (int): The dimensionality of the search space.\n            pop_size (int): The population size.\n            F (float): The mutation factor.\n            CR (float): The crossover rate.\n            learning_rate (float): The learning rate for adaptive step size.\n            restart_patience (int): Number of iterations without improvement before restart.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.learning_rate = learning_rate\n        self.restart_patience = restart_patience\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        # Initialize population within bounds\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size # Update remaining budget\n\n        # Find the best individual in the initial population\n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index].copy()\n\n        iteration = 0\n        no_improvement_count = 0\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = np.random.choice(idxs, 3, replace=False)\n                mutant = population[a] + self.F * (population[b] - population[c])\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Boundary handling (clip to bounds)\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Evaluation\n                f = func(trial)\n                self.budget -= 1\n                if f < fitness[i]:\n                    fitness[i] = f\n                    population[i] = trial\n                    \n                    # Update global best\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = trial.copy()\n                        no_improvement_count = 0 # Reset counter\n                else:\n                    no_improvement_count += 1\n\n                if self.budget <= 0:\n                    break\n            \n            iteration += 1\n            \n            # Adaptive learning rate adjustment (example: shrinking F)\n            self.F *= (1 - self.learning_rate)  # Reduce F\n            self.F = np.clip(self.F, 0.1, 0.9)  # Ensure F stays within reasonable bounds\n            self.learning_rate *= 0.99 # Reduce Learning Rate gradually\n            \n            # Restart mechanism if no improvement for a while\n            if no_improvement_count > self.restart_patience:\n                population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                fitness = np.array([func(x) for x in population])\n                self.budget -= self.pop_size\n                best_index = np.argmin(fitness)\n                if fitness[best_index] < self.f_opt:\n                    self.f_opt = fitness[best_index]\n                    self.x_opt = population[best_index].copy()\n                no_improvement_count = 0 # Reset counter\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveDifferentialEvolution scored 0.452 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.20831611902100045, 0.36737303103083474, 0.3770630351563149, 0.9133258743735502, 0.4327027360226814, 0.4678536340230254, 0.3686686512700257, 0.3823700463789249, 0.322126978084743, 0.2493432400267498, 0.44712773007934337, 0.9969382610363432, 0.3284953937900045, 0.3726954823748757, 0.7262398996005219, 0.3693783633663885, 0.3696966670996621, 0.6004388110398671, 0.2294445070906984, 0.5046415233414681]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "65ca3433-814b-4d91-9fcf-c34d334aadd4", "fitness": 0.5617314357618451, "name": "AdaptiveGaussianSearch", "description": "A population-based algorithm that uses a Gaussian distribution centered around the best solution found so far to sample new candidate solutions, adapting the distribution's standard deviation based on success rate.", "code": "import numpy as np\n\nclass AdaptiveGaussianSearch:\n    def __init__(self, budget=10000, dim=10, population_size=20, initial_std=0.5, success_threshold=0.2):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = population_size\n        self.initial_std = initial_std\n        self.success_threshold = success_threshold\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.std = self.initial_std\n        eval_count = 0\n\n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.population_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        eval_count += self.population_size\n        \n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index]\n\n        while eval_count < self.budget:\n            # Generate new samples around the best solution\n            new_samples = np.random.normal(loc=self.x_opt, scale=self.std, size=(self.population_size, self.dim))\n            new_samples = np.clip(new_samples, func.bounds.lb, func.bounds.ub)  # Clip to bounds\n\n            new_fitness = np.array([func(x) for x in new_samples])\n            eval_count += self.population_size\n\n            # Update best solution\n            for i in range(self.population_size):\n                if new_fitness[i] < self.f_opt:\n                    self.f_opt = new_fitness[i]\n                    self.x_opt = new_samples[i]\n\n            # Adapt the standard deviation based on success rate\n            success_rate = np.sum(new_fitness < fitness) / self.population_size\n            if success_rate > self.success_threshold:\n                self.std *= 1.1  # Increase exploration if successful\n            else:\n                self.std *= 0.9  # Decrease exploration if unsuccessful\n            \n            fitness = np.minimum(fitness, new_fitness) #replace only if better\n            best_index = np.argmin(fitness)\n            \n            if eval_count >= self.budget:\n                break\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveGaussianSearch scored 0.562 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.20049606913262263, 0.2153468276271585, 0.8168354621303715, 0.9264848060282292, 0.8018578351980671, 0.8079259711767164, 0.2796513129599185, 0.390829537623556, 0.1668764334467302, 0.20921371233445718, 0.9319149162200335, 0.9952384513149439, 0.2925711121308695, 0.3719244154673672, 0.9443504275185233, 0.6999333893261253, 0.6578997934635358, 0.8779022780803489, 0.1356269419200129, 0.5117490221373175]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "9507e339-c0e6-4b5f-9f2b-ae44c93931fa", "fitness": 0.0, "name": "HybridDECMAS", "description": "A population-based algorithm that combines elements of differential evolution and covariance matrix adaptation evolution strategy (CMA-ES) with self-adaptive parameters for exploration and exploitation.", "code": "import numpy as np\n\nclass HybridDECMAS:\n    def __init__(self, budget=10000, dim=10, pop_size=None, F=0.5, CR=0.9, sigma=0.1, cs=0.3, damps=1.0, mu=None):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size if pop_size is not None else 4 + int(3 * np.log(dim))\n        self.F = F\n        self.CR = CR\n        self.sigma = sigma\n        self.cs = cs\n        self.damps = damps\n        self.mu = self.pop_size // 2 if mu is None else mu\n\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        \n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        evaluations = self.pop_size\n        \n        # Main loop\n        while evaluations < self.budget:\n            # Differential Evolution part\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = a + self.F * (b - c)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                f = func(trial)\n                evaluations += 1\n                if f < fitness[i]:\n                    fitness[i] = f\n                    population[i] = trial\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = trial.copy()\n\n                        \n                if evaluations >= self.budget:\n                    break\n\n            # CMA-ES Adaptation (Simplified)\n            fitness_sorted_indices = np.argsort(fitness)\n            best_individuals = population[fitness_sorted_indices[:self.mu]]\n            mean = np.mean(best_individuals, axis=0)\n\n            # Update step size (simplified)\n            self.sigma *= np.exp(self.cs / self.damps * (np.linalg.norm((mean - population[fitness_sorted_indices[0]]) / self.sigma ) - 1))\n            self.sigma = max(self.sigma, 1e-8)\n\n            # Sample new individuals using CMA-ES update (simplified)\n            new_individuals = np.random.normal(mean, self.sigma, size=(self.pop_size, self.dim))\n            new_individuals = np.clip(new_individuals, func.bounds.lb, func.bounds.ub)\n            new_fitness = np.array([func(x) for x in new_individuals])\n            evaluations += self.pop_size\n\n            # Replace worst individuals with new individuals\n            worst_indices = fitness_sorted_indices[self.mu:]\n            population[worst_indices] = new_individuals[worst_indices]\n            fitness[worst_indices] = new_fitness[worst_indices]\n\n\n            for i in range(self.pop_size):\n                if fitness[i] < self.f_opt:\n                    self.f_opt = fitness[i]\n                    self.x_opt = population[i].copy()\n                    \n            if evaluations >= self.budget:\n                break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm HybridDECMAS scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "c6340a68-f631-4186-96fc-8ea1228db941", "fitness": "-inf", "name": "LevyGaussianSearch", "description": "A population-based algorithm employing Lévy flights for exploration and Gaussian perturbation around the best solution for exploitation, adaptively adjusting flight length and perturbation strength.", "code": "import numpy as np\n\nclass LevyGaussianSearch:\n    def __init__(self, budget=10000, dim=10, population_size=20, initial_levy_exponent=1.5, initial_gaussian_std=0.1, levy_adapt_rate=0.01, gaussian_adapt_rate=0.01):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = population_size\n        self.initial_levy_exponent = initial_levy_exponent\n        self.initial_gaussian_std = initial_gaussian_std\n        self.levy_exponent = initial_levy_exponent\n        self.gaussian_std = initial_gaussian_std\n        self.levy_adapt_rate = levy_adapt_rate\n        self.gaussian_adapt_rate = gaussian_adapt_rate\n\n\n    def levy_flight(self, shape, exponent=1.5):\n        num = np.random.randn(*shape) * np.sqrt(np.math.gamma(1 + exponent) * np.sin(np.pi * exponent / 2) / (np.math.gamma((1 + exponent) / 2) * exponent * 2**((exponent - 1) / 2)))\n        den = np.abs(np.random.randn(*shape))**(1 / exponent)\n        return num / den\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        eval_count = 0\n\n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.population_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        eval_count += self.population_size\n\n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index]\n\n        while eval_count < self.budget:\n            new_population = np.zeros_like(population)\n            new_fitness = np.zeros_like(fitness)\n\n            for i in range(self.population_size):\n                # Exploration using Lévy flight\n                levy_step = self.levy_flight(self.dim, self.levy_exponent)\n                candidate_solution_levy = population[i] + levy_step * (func.bounds.ub - func.bounds.lb) * 0.01 # scale levy step\n                candidate_solution_levy = np.clip(candidate_solution_levy, func.bounds.lb, func.bounds.ub)\n\n\n                # Exploitation using Gaussian perturbation around best\n                gaussian_step = np.random.normal(loc=0, scale=self.gaussian_std, size=self.dim)\n                candidate_solution_gaussian = self.x_opt + gaussian_step * (func.bounds.ub - func.bounds.lb) * 0.01 # scale gaussian step\n                candidate_solution_gaussian = np.clip(candidate_solution_gaussian, func.bounds.lb, func.bounds.ub)\n\n                # Select between levy and gaussian based on random chance\n                if np.random.rand() < 0.5:\n                    candidate_solution = candidate_solution_levy\n                else:\n                    candidate_solution = candidate_solution_gaussian\n\n\n                new_population[i] = candidate_solution\n                new_fitness[i] = func(candidate_solution)\n                eval_count += 1\n\n                if new_fitness[i] < self.f_opt:\n                    self.f_opt = new_fitness[i]\n                    self.x_opt = candidate_solution\n                \n                if new_fitness[i] < fitness[i]:\n                    population[i] = candidate_solution\n                    fitness[i] = new_fitness[i]\n                    # Adapt levy exponent and gaussian std if improvement\n                    self.levy_exponent = max(0.5, self.levy_exponent * (1 - self.levy_adapt_rate))\n                    self.gaussian_std = max(0.001, self.gaussian_std * (1 - self.gaussian_adapt_rate))\n                else:\n                     # Adapt levy exponent and gaussian std if no improvement\n                    self.levy_exponent = min(3.0, self.levy_exponent * (1 + self.levy_adapt_rate))\n                    self.gaussian_std = self.gaussian_std * (1 + self.gaussian_adapt_rate)\n\n                if eval_count >= self.budget:\n                    break\n            \n            if eval_count >= self.budget:\n                break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "In the code, line 17, in levy_flight, the following error occurred:\nTypeError: numpy.random.RandomState.randn() argument after * must be an iterable, not int\nOn line: num = np.random.randn(*shape) * np.sqrt(np.math.gamma(1 + exponent) * np.sin(np.pi * exponent / 2) / (np.math.gamma((1 + exponent) / 2) * exponent * 2**((exponent - 1) / 2)))", "error": "In the code, line 17, in levy_flight, the following error occurred:\nTypeError: numpy.random.RandomState.randn() argument after * must be an iterable, not int\nOn line: num = np.random.randn(*shape) * np.sqrt(np.math.gamma(1 + exponent) * np.sin(np.pi * exponent / 2) / (np.math.gamma((1 + exponent) / 2) * exponent * 2**((exponent - 1) / 2)))", "parent_ids": ["65ca3433-814b-4d91-9fcf-c34d334aadd4"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "483c1b28-e275-422b-9c48-b2716d76e7fb", "fitness": 0.22979174810774974, "name": "AdaptiveCauchySearch", "description": "A population-based algorithm that uses a Cauchy distribution to sample new candidate solutions, adapting both location and scale parameters based on the improvement rate.", "code": "import numpy as np\n\nclass AdaptiveCauchySearch:\n    def __init__(self, budget=10000, dim=10, population_size=20, initial_scale=0.5, success_threshold=0.2, lr_loc=0.1, lr_scale=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = population_size\n        self.initial_scale = initial_scale\n        self.success_threshold = success_threshold\n        self.lr_loc = lr_loc\n        self.lr_scale = lr_scale\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.scale = self.initial_scale\n        self.location = np.zeros(self.dim)  # Initialize location parameter\n        eval_count = 0\n\n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.population_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        eval_count += self.population_size\n        \n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index]\n        self.location = self.x_opt.copy()  # Initialize location with the best solution\n\n\n        while eval_count < self.budget:\n            # Generate new samples around the current location using a Cauchy distribution\n            cauchy_samples = self.location + self.scale * np.random.standard_cauchy(size=(self.population_size, self.dim))\n            new_samples = np.clip(cauchy_samples, func.bounds.lb, func.bounds.ub)\n\n            new_fitness = np.array([func(x) for x in new_samples])\n            eval_count += self.population_size\n\n            # Update best solution and location\n            improved_indices = new_fitness < fitness\n            num_improvements = np.sum(improved_indices)\n            for i in range(self.population_size):\n                if new_fitness[i] < self.f_opt:\n                    self.f_opt = new_fitness[i]\n                    self.x_opt = new_samples[i]\n                    \n            fitness = np.minimum(fitness, new_fitness)\n            best_index = np.argmin(fitness)\n            self.location = population[best_index].copy()  # Move location towards the best member of population\n\n            # Adapt the location and scale parameters based on success rate\n            success_rate = num_improvements / self.population_size\n            if success_rate > self.success_threshold:\n                self.scale *= 1.1  # Increase exploration\n                self.location += self.lr_loc * (self.x_opt - self.location)  #Move location towards best\n            else:\n                self.scale *= 0.9  # Decrease exploration\n\n            if eval_count >= self.budget:\n                break\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "The algorithm AdaptiveCauchySearch scored 0.230 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["65ca3433-814b-4d91-9fcf-c34d334aadd4"], "operator": null, "metadata": {"aucs": [0.07396187728881976, 0.13316002639412783, 0.23478251987803356, 0.2262223889221534, 0.16382533564130441, 0.17258000743178759, 0.22700289532153528, 0.1541851558649896, 0.16370820442709777, 0.1318841268513533, 0.15980049132718233, 0.9992402941480836, 0.20802614843255796, 0.17974379335567625, 0.17539058226182047, 0.2567993555027934, 0.18773855012052343, 0.1906422536564586, 0.13192376038516618, 0.4252171949435307]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "3f4dbed7-615a-44be-9e4d-93b05dfe2215", "fitness": 0.5075510970079599, "name": "AdaptiveClampedPSO", "description": "Combines a simplified PSO with velocity clamping and adaptive exploration based on population diversity to find the optimum.", "code": "import numpy as np\n\nclass AdaptiveClampedPSO:\n    def __init__(self, budget=10000, dim=10, pop_size=None, w=0.7, c1=1.5, c2=1.5, clamp_factor=0.5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size if pop_size is not None else 20 # Increased pop_size for better exploration\n        self.w = w  # Inertia weight\n        self.c1 = c1  # Cognitive coefficient\n        self.c2 = c2  # Social coefficient\n        self.clamp_factor = clamp_factor  # Factor for clamping velocities\n        self.swarm = None\n        self.velocities = None\n        self.personal_best_positions = None\n        self.personal_best_fitness = None\n        self.global_best_position = None\n        self.global_best_fitness = np.inf\n        self.evaluations = 0\n\n    def initialize_swarm(self, func):\n        self.swarm = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.velocities = np.zeros((self.pop_size, self.dim))\n        self.personal_best_positions = self.swarm.copy()\n        self.personal_best_fitness = np.array([func(x) for x in self.swarm])\n        self.evaluations += self.pop_size\n\n        self.global_best_position = self.personal_best_positions[np.argmin(self.personal_best_fitness)].copy()\n        self.global_best_fitness = np.min(self.personal_best_fitness)\n\n    def update_velocities(self, func):\n        for i in range(self.pop_size):\n            r1 = np.random.rand(self.dim)\n            r2 = np.random.rand(self.dim)\n\n            cognitive_component = self.c1 * r1 * (self.personal_best_positions[i] - self.swarm[i])\n            social_component = self.c2 * r2 * (self.global_best_position - self.swarm[i])\n\n            self.velocities[i] = self.w * self.velocities[i] + cognitive_component + social_component\n\n            # Velocity clamping\n            max_velocity = self.clamp_factor * (func.bounds.ub - func.bounds.lb)\n            self.velocities[i] = np.clip(self.velocities[i], -max_velocity, max_velocity)\n\n    def update_positions(self, func):\n        self.swarm += self.velocities\n        self.swarm = np.clip(self.swarm, func.bounds.lb, func.bounds.ub)\n\n        fitness = np.array([func(x) for x in self.swarm])\n        self.evaluations += self.pop_size\n\n        for i in range(self.pop_size):\n            if fitness[i] < self.personal_best_fitness[i]:\n                self.personal_best_fitness[i] = fitness[i]\n                self.personal_best_positions[i] = self.swarm[i].copy()\n\n                if fitness[i] < self.global_best_fitness:\n                    self.global_best_fitness = fitness[i]\n                    self.global_best_position = self.swarm[i].copy()\n\n    def adapt_exploration(self):\n        # Adaptive inertia weight based on population diversity\n        diversity = np.mean(np.std(self.swarm, axis=0))\n        # Linearly decrease w based on diversity, but keep it within reasonable bounds\n        self.w = np.clip(0.7 - 0.5 * diversity, 0.2, 0.7)\n\n    def __call__(self, func):\n        self.initialize_swarm(func)\n\n        while self.evaluations < self.budget:\n            self.update_velocities(func)\n            self.update_positions(func)\n            self.adapt_exploration()  #Adapt exploration based on the swarm state\n            if self.evaluations >= self.budget:\n                break\n        return self.global_best_fitness, self.global_best_position", "configspace": "", "generation": 1, "feedback": "The algorithm AdaptiveClampedPSO scored 0.508 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["9507e339-c0e6-4b5f-9f2b-ae44c93931fa"], "operator": null, "metadata": {"aucs": [0.14867728621380116, 0.16356571229042927, 0.8414664811778231, 0.19461378154256836, 0.2550773361401264, 0.9280081444070977, 0.3626517107599374, 0.8426513371389133, 0.2670871060939024, 0.20079252014888627, 0.9700039608759425, 0.9996624348967544, 0.24218425319417924, 0.24149411708357071, 0.649714761869675, 0.8565113264738979, 0.3364716706197922, 0.9459746329102129, 0.23103448059652154, 0.4733788857251653]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "e2406384-6ad0-476e-b737-5d93617d1596", "fitness": 0.2685575441258563, "name": "AdaptiveDEwithLocalSearch", "description": "Differential Evolution with self-adaptive parameters (mutation factor and crossover rate), a local search operator using Gaussian perturbation around the best solution, and a restart strategy.", "code": "import numpy as np\n\nclass AdaptiveDEwithLocalSearch:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F_init=0.7, CR_init=0.9, learning_rate=0.1, restart_patience=500, local_search_prob=0.1, local_search_sigma=0.1):\n        \"\"\"\n        Args:\n            budget (int): The maximum number of function evaluations.\n            dim (int): The dimensionality of the search space.\n            pop_size (int): The population size.\n            F_init (float): The initial mutation factor.\n            CR_init (float): The initial crossover rate.\n            learning_rate (float): The learning rate for adaptive parameters.\n            restart_patience (int): Number of iterations without improvement before restart.\n            local_search_prob (float): Probability of applying local search.\n            local_search_sigma (float): Standard deviation for local search Gaussian perturbation.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F_init * np.ones(pop_size)\n        self.CR = CR_init * np.ones(pop_size)\n        self.learning_rate = learning_rate\n        self.restart_patience = restart_patience\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.local_search_prob = local_search_prob\n        self.local_search_sigma = local_search_sigma\n\n    def __call__(self, func):\n        # Initialize population within bounds\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size # Update remaining budget\n\n        # Find the best individual in the initial population\n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index].copy()\n\n        iteration = 0\n        no_improvement_count = 0\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Parameter adaptation for F and CR\n                self.F[i] = np.clip(self.F[i] + self.learning_rate * (np.random.rand() - 0.5), 0.1, 0.9)\n                self.CR[i] = np.clip(self.CR[i] + self.learning_rate * (np.random.rand() - 0.5), 0.1, 0.9)\n\n                # Mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = np.random.choice(idxs, 3, replace=False)\n                mutant = population[a] + self.F[i] * (population[b] - population[c])\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR[i]\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Boundary handling (clip to bounds)\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Local Search\n                if np.random.rand() < self.local_search_prob:\n                    trial = self.x_opt + self.local_search_sigma * np.random.normal(0, 1, self.dim)\n                    trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Evaluation\n                f = func(trial)\n                self.budget -= 1\n                if f < fitness[i]:\n                    fitness[i] = f\n                    population[i] = trial\n                    \n                    # Update global best\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = trial.copy()\n                        no_improvement_count = 0 # Reset counter\n                else:\n                    no_improvement_count += 1\n\n                if self.budget <= 0:\n                    break\n            \n            iteration += 1\n            \n            # Adaptive learning rate adjustment\n            self.learning_rate *= 0.99 # Reduce Learning Rate gradually\n            \n            # Restart mechanism if no improvement for a while\n            if no_improvement_count > self.restart_patience:\n                population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                fitness = np.array([func(x) for x in population])\n                self.budget -= self.pop_size\n                best_index = np.argmin(fitness)\n                if fitness[best_index] < self.f_opt:\n                    self.f_opt = fitness[best_index]\n                    self.x_opt = population[best_index].copy()\n                no_improvement_count = 0 # Reset counter\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "The algorithm AdaptiveDEwithLocalSearch scored 0.269 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["0b543e23-371f-432d-a057-a2e5b307c05c"], "operator": null, "metadata": {"aucs": [0.2130805825721691, 0.344242175261703, 0.5169074186695533, 0]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "d1517f29-4f3d-4230-baa6-eb81cbe45e7f", "fitness": "-inf", "name": "SelfAdjustingOrthogonalDE", "description": "A self-adjusting population-based algorithm that balances global exploration with local exploitation by dynamically adapting both mutation strength and crossover probability based on the population's diversity and success rate, incorporating orthogonal learning to enhance exploration and restart from orthogonal array based initial design.", "code": "import numpy as np\n\nclass SelfAdjustingOrthogonalDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F_init=0.7, CR_init=0.9, learning_rate=0.1, restart_patience=500, oa_size=10):\n        \"\"\"\n        Args:\n            budget (int): The maximum number of function evaluations.\n            dim (int): The dimensionality of the search space.\n            pop_size (int): The population size.\n            F_init (float): Initial mutation factor.\n            CR_init (float): Initial crossover rate.\n            learning_rate (float): The learning rate for adaptive parameters.\n            restart_patience (int): Number of iterations without improvement before restart.\n            oa_size (int): Size of the orthogonal array for initialization.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F_init\n        self.CR = CR_init\n        self.learning_rate = learning_rate\n        self.restart_patience = restart_patience\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.oa_size = oa_size # Size of Orthogonal Array\n        self.archive = []      # Archive to store discarded solutions\n\n    def generate_orthogonal_array(self, n, k, levels):\n        \"\"\"Generates an orthogonal array (OA) of size n with k factors each having levels levels.\"\"\"\n        # A simplified example; in practice, use dedicated OA generation tools/libraries.\n        oa = np.zeros((n, k))\n        for i in range(n):\n            for j in range(k):\n                oa[i, j] = np.random.randint(0, levels)\n        return oa\n\n    def initialize_population_with_oa(self, func):\n        \"\"\"Initializes a part of the population using an orthogonal array design.\"\"\"\n        oa = self.generate_orthogonal_array(self.oa_size, self.dim, 5) # Generating OA with 5 levels (arbitrary)\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        \n        # Replace the first oa_size individuals with OA-based designs\n        for i in range(min(self.oa_size, self.pop_size)):\n            for j in range(self.dim):\n                 population[i, j] = func.bounds.lb + (func.bounds.ub - func.bounds.lb) * oa[i % self.oa_size, j] / 4.0 #Scale OA to bounds\n                 \n        return population\n\n    def orthogonal_learning(self, x, func):\n        \"\"\"Performs orthogonal learning by perturbing the solution x along orthogonal directions.\"\"\"\n        direction = np.random.randn(self.dim)  # Generate a random direction\n        direction /= np.linalg.norm(direction) # Normalize\n        step_size = np.random.uniform(-0.1, 0.1) # Random step size\n        \n        trial = x + step_size * direction\n        trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n        return trial\n\n    def __call__(self, func):\n        # Initialize population using OA design\n        population = self.initialize_population_with_oa(func)\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        # Find the best individual in the initial population\n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index].copy()\n\n        iteration = 0\n        no_improvement_count = 0\n        while self.budget > 0:\n            # Calculate population diversity (standard deviation)\n            diversity = np.std(population)\n\n            for i in range(self.pop_size):\n                # Adaptive F and CR based on population diversity\n                F = self.F * (1 + self.learning_rate * (diversity - 0.5))  #Example adaptive F\n                CR = self.CR * (1 - self.learning_rate * (diversity - 0.5))  #Example adaptive CR\n                F = np.clip(F, 0.1, 0.9)\n                CR = np.clip(CR, 0.1, 0.9)\n\n                # Mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = np.random.choice(idxs, 3, replace=False)\n                mutant = population[a] + F * (population[b] - population[c])\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Boundary handling (clip to bounds)\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n                \n                # Orthogonal Learning with small probability\n                if np.random.rand() < 0.1: \n                    trial = self.orthogonal_learning(trial, func)\n\n                # Evaluation\n                f = func(trial)\n                self.budget -= 1\n                if f < fitness[i]:\n                    # Store discarded solution in archive for later use\n                    self.archive.append(population[i].copy())\n                    \n                    fitness[i] = f\n                    population[i] = trial\n                    \n                    # Update global best\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = trial.copy()\n                        no_improvement_count = 0 # Reset counter\n                else:\n                    no_improvement_count += 1\n\n                if self.budget <= 0:\n                    break\n            \n            iteration += 1\n            \n            # Adaptive learning rate adjustment\n            self.F *= (1 - self.learning_rate)  # Reduce F\n            self.CR *= (1 + self.learning_rate)  # Increase CR\n            self.F = np.clip(self.F, 0.1, 0.9)  # Ensure F stays within reasonable bounds\n            self.CR = np.clip(self.CR, 0.1, 0.9)  # Ensure CR stays within reasonable bounds\n            self.learning_rate *= 0.99 # Reduce Learning Rate gradually\n            \n            # Restart mechanism if no improvement for a while\n            if no_improvement_count > self.restart_patience:\n                # Restart from OA-based design and archive information\n                new_population = self.initialize_population_with_oa(func)\n                \n                # Incorporate some archived solutions if available\n                num_archived = min(len(self.archive), self.pop_size - self.oa_size)\n                if num_archived > 0:\n                    archived_indices = np.random.choice(len(self.archive), num_archived, replace=False)\n                    for j, idx in enumerate(archived_indices):\n                        new_population[self.oa_size + j] = self.archive[idx]\n                \n                population = new_population\n                fitness = np.array([func(x) for x in population])\n                self.budget -= self.pop_size\n\n                best_index = np.argmin(fitness)\n                if fitness[best_index] < self.f_opt:\n                    self.f_opt = fitness[best_index]\n                    self.x_opt = population[best_index].copy()\n                no_improvement_count = 0 # Reset counter\n                self.archive = []  # Clear archive after restart\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "In the code, line 45, in initialize_population_with_oa, the following error occurred:\nValueError: setting an array element with a sequence.\nOn line: population[i, j] = func.bounds.lb + (func.bounds.ub - func.bounds.lb) * oa[i % self.oa_size, j] / 4.0 #Scale OA to bounds", "error": "In the code, line 45, in initialize_population_with_oa, the following error occurred:\nValueError: setting an array element with a sequence.\nOn line: population[i, j] = func.bounds.lb + (func.bounds.ub - func.bounds.lb) * oa[i % self.oa_size, j] / 4.0 #Scale OA to bounds", "parent_ids": ["0b543e23-371f-432d-a057-a2e5b307c05c"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "f7a92b53-a6e8-4fb0-948a-54335f3461b9", "fitness": 0.15444751514785598, "name": "SimplifiedHybridDE", "description": "A simplified hybrid algorithm that combines a simplified differential evolution with a self-adaptive step size and a restart mechanism based on stagnation.", "code": "import numpy as np\n\nclass SimplifiedHybridDE:\n    def __init__(self, budget=10000, dim=10, pop_size=None, mutation_factor=0.5, crossover_rate=0.7, restart_trigger=50):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size if pop_size is not None else 5 * dim\n        self.mutation_factor = mutation_factor\n        self.crossover_rate = crossover_rate\n        self.restart_trigger = restart_trigger\n        self.rng = np.random.default_rng()\n        self.step_size = 0.1  # Initial step size\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.population = self.rng.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.eval_count += self.pop_size\n\n        self.no_improvement_counter = 0\n        self.best_fitness_ever = np.min(self.fitness)\n\n        while self.eval_count < self.budget:\n            for i in range(self.pop_size):\n                # Simplified Differential Evolution Mutation\n                idxs = self.rng.choice(self.pop_size, 3, replace=False)\n                x_1, x_2, x_3 = self.population[idxs]\n                mutant = self.population[i] + self.mutation_factor * (x_1 - x_2) * self.step_size # Step-size scaling\n\n                # Crossover\n                trial = self.population[i].copy()\n                j_rand = self.rng.integers(0, self.dim)\n                for j in range(self.dim):\n                    if self.rng.random() < self.crossover_rate or j == j_rand:\n                        trial[j] = mutant[j]\n\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n                f_trial = func(trial)\n                self.eval_count += 1\n\n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                        self.no_improvement_counter = 0\n                else:\n                    self.no_improvement_counter += 1\n\n            # Self-Adaptive Step Size\n            if np.min(self.fitness) < self.best_fitness_ever:\n                self.best_fitness_ever = np.min(self.fitness)\n                self.no_improvement_counter = 0\n                self.step_size *= 1.1  # Increase step size if improving\n            else:\n                self.step_size *= 0.9  # Decrease step size if stagnating\n                self.no_improvement_counter += 1\n\n            self.step_size = np.clip(self.step_size, 0.001, 0.5)  # Limit step size\n\n            # Restart mechanism\n            if self.no_improvement_counter > self.restart_trigger:\n                self.population = self.rng.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                self.fitness = np.array([func(x) for x in self.population])\n                self.eval_count += self.pop_size\n                self.no_improvement_counter = 0\n                self.step_size = 0.1 # Reset step size\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "The algorithm SimplifiedHybridDE scored 0.154 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["42996575-4eeb-4828-bdd6-e3fdb9d71835"], "operator": null, "metadata": {"aucs": [0.10116004319215866, 0.17603443622462178, 0.2712368445468125, 0.22380625177568703, 0]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "a02d75a9-32ac-408d-a92c-617efd78dab3", "fitness": 0.0, "name": "AdaptiveHybridDECMASA", "description": "Adaptive Hybrid algorithm combining DE, CMA-ES, and Simulated Annealing for enhanced exploration and exploitation.", "code": "import numpy as np\n\nclass AdaptiveHybridDECMASA:\n    def __init__(self, budget=10000, dim=10, pop_size=None, F=0.5, CR=0.9, sigma=0.1, cs=0.3, damps=1.0, mu=None, sa_prob=0.1, sa_temp=1.0):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size if pop_size is not None else 4 + int(3 * np.log(dim))\n        self.F = F\n        self.CR = CR\n        self.sigma = sigma\n        self.cs = cs\n        self.damps = damps\n        self.mu = self.pop_size // 2 if mu is None else mu\n        self.sa_prob = sa_prob  # Probability of applying SA\n        self.sa_temp = sa_temp  # Initial temperature for SA\n        self.sa_cooling_rate = 0.999  # Cooling rate for SA\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        \n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        evaluations = self.pop_size\n        \n        # Main loop\n        while evaluations < self.budget:\n            # Differential Evolution part\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = a + self.F * (b - c)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Selection\n                f = func(trial)\n                evaluations += 1\n                if f < fitness[i]:\n                    fitness[i] = f\n                    population[i] = trial\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = trial.copy()\n                else:\n                    # Simulated Annealing: accept worse solution with probability\n                    delta = f - fitness[i]\n                    if np.random.rand() < self.sa_prob and np.random.rand() < np.exp(-delta / self.sa_temp):\n                        fitness[i] = f\n                        population[i] = trial\n                \n                if evaluations >= self.budget:\n                    break\n\n            # CMA-ES Adaptation (Simplified)\n            fitness_sorted_indices = np.argsort(fitness)\n            best_individuals = population[fitness_sorted_indices[:self.mu]]\n            mean = np.mean(best_individuals, axis=0)\n\n            # Update step size (simplified)\n            self.sigma *= np.exp(self.cs / self.damps * (np.linalg.norm((mean - population[fitness_sorted_indices[0]]) / self.sigma ) - 1))\n            self.sigma = max(self.sigma, 1e-8)\n\n            # Sample new individuals using CMA-ES update (simplified)\n            new_individuals = np.random.normal(mean, self.sigma, size=(self.pop_size, self.dim))\n            new_individuals = np.clip(new_individuals, func.bounds.lb, func.bounds.ub)\n            new_fitness = np.array([func(x) for x in new_individuals])\n            evaluations += self.pop_size\n\n            # Replace worst individuals with new individuals\n            worst_indices = fitness_sorted_indices[self.mu:]\n            population[worst_indices] = new_individuals[worst_indices]\n            fitness[worst_indices] = new_fitness[worst_indices]\n\n\n            for i in range(self.pop_size):\n                if fitness[i] < self.f_opt:\n                    self.f_opt = fitness[i]\n                    self.x_opt = population[i].copy()\n                    \n            if evaluations >= self.budget:\n                break\n                \n            # Anneal the temperature\n            self.sa_temp *= self.sa_cooling_rate\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "The algorithm AdaptiveHybridDECMASA scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["9507e339-c0e6-4b5f-9f2b-ae44c93931fa"], "operator": null, "metadata": {"aucs": [0]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "05a3229c-be5a-4b35-aee6-4703d2f7bf99", "fitness": "-inf", "name": "AdaptiveEnsembleDECMARestart", "description": "Integrates a dynamically adjusted ensemble of mutation strategies within Differential Evolution, coupled with a CMA-ES-inspired adaptation of the covariance matrix for enhanced exploration and exploitation, and a restart mechanism based on stagnation detection using an exponentially weighted moving average of the function values.", "code": "import numpy as np\n\nclass AdaptiveEnsembleDECMARestart:\n    def __init__(self, budget=10000, dim=10, pop_size=None, mutation_factors=(0.5, 0.7, 0.9), crossover_rate=0.7, restart_trigger=100, ensemble_probs=(0.33, 0.33, 0.34), ewma_alpha=0.05):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size if pop_size is not None else 4 + int(3 * np.log(dim))\n        self.mutation_factors = mutation_factors\n        self.crossover_rate = crossover_rate\n        self.restart_trigger = restart_trigger\n        self.ensemble_probs = list(ensemble_probs)  # Convert to list for modification\n        self.rng = np.random.default_rng()\n        self.ewma_alpha = ewma_alpha  # Alpha for EWMA calculation\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.population = self.rng.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.eval_count += self.pop_size\n\n        self.mean = np.mean(self.population, axis=0)\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.sigma = 0.3\n        self.damps = 1 + (2 * max(0, np.log((self.dim + 1) / 3)))\n        self.chiN = np.sqrt(self.dim) * (1 - (1 / (4 * self.dim)) + (1 / (21 * self.dim**2)))\n\n        self.mu = self.pop_size // 2\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        self.cc = (4 + self.mueff/self.dim) / (self.dim + 4 + 2*self.mueff/self.dim)\n        self.cs = (self.mueff + 2) / (self.dim + self.mueff + 5)\n        self.c1 = 2 / ((self.dim + 1.3)**2 + self.mueff)\n        self.cmu = min(1 - self.c1, 2 * (self.mueff - 2 + 1/self.mueff) / ((self.dim + 2)**2 + self.mueff))\n\n        self.no_improvement_counter = 0\n        self.best_fitness_ever = np.min(self.fitness)\n        self.ewma_fitness = self.best_fitness_ever  # Initialize EWMA\n        self.ensemble_success_counts = [0] * len(self.mutation_factors)\n        self.ensemble_trial_counts = [0] * len(self.mutation_factors)\n\n        while self.eval_count < self.budget:\n            # Differential Evolution Mutation with Ensemble\n            for i in range(self.pop_size):\n                # Choose mutation strategy from ensemble\n                mutation_index = self.rng.choice(len(self.mutation_factors), p=self.ensemble_probs)\n                self.ensemble_trial_counts[mutation_index] += 1\n                mutation_factor = self.mutation_factors[mutation_index]\n\n                idxs = self.rng.choice(self.pop_size, 3, replace=False)\n                x_1, x_2, x_3 = self.population[idxs]\n                mutant = self.population[i] + mutation_factor * (x_1 - x_2)\n\n                # Crossover\n                trial = self.population[i].copy()\n                j_rand = self.rng.integers(0, self.dim)\n                for j in range(self.dim):\n                    if self.rng.random() < self.crossover_rate or j == j_rand:\n                        trial[j] = mutant[j]\n\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n                f_trial = func(trial)\n                self.eval_count += 1\n\n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n                    self.ensemble_success_counts[mutation_index] += 1\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                        self.no_improvement_counter = 0\n                else:\n                    self.no_improvement_counter += 1\n\n\n            # CMA-ES Adaptation\n            indices = np.argsort(self.fitness)\n            xmean = np.sum(self.weights[:, None] * self.population[indices[:self.mu]], axis=0)\n\n            y = (self.population[indices[:self.mu]] - self.mean) / self.sigma\n            self.ps = (1-self.cs) * self.ps + np.sqrt(self.cs*(2-self.cs)) * (np.linalg.solve(np.linalg.cholesky(self.C), (xmean - self.mean) / self.sigma))\n            hsig = np.linalg.norm(self.ps)/np.sqrt(1-(1-self.cs)**(2*(self.eval_count/self.pop_size))) < (1.4 + 2/(self.dim+1)) * self.chiN\n            self.pc = (1-self.cc) * self.pc + hsig * np.sqrt(self.cc*(2-self.cc)) * ((xmean - self.mean) / self.sigma)\n\n            self.C = (1-self.c1-self.cmu) * self.C + self.c1 * (self.pc[:, None] @ self.pc[None, :]) + self.cmu * np.sum(self.weights[:, None, None] * (y[:, :, None] @ y[:, None, :]), axis=0)\n            self.sigma *= np.exp((self.cs/self.damps) * (np.linalg.norm(self.ps)/self.chiN - 1))\n\n            self.mean = xmean\n\n            self.C = np.triu(self.C) + np.triu(self.C, 1).T\n\n            # Update EWMA of best fitness\n            current_best_fitness = np.min(self.fitness)\n            self.ewma_fitness = self.ewma_alpha * current_best_fitness + (1 - self.ewma_alpha) * self.ewma_fitness\n\n            if current_best_fitness < self.best_fitness_ever:\n                self.best_fitness_ever = current_best_fitness\n                self.no_improvement_counter = 0\n            else:\n                self.no_improvement_counter += 1\n\n            # Adjust ensemble probabilities\n            for k in range(len(self.mutation_factors)):\n                if self.ensemble_trial_counts[k] > 0:\n                    success_rate = self.ensemble_success_counts[k] / self.ensemble_trial_counts[k]\n                    self.ensemble_probs[k] *= (1 + 0.1 * (success_rate - 0.5))  # Bias towards successful strategies\n\n            # Normalize probabilities\n            sum_probs = sum(self.ensemble_probs)\n            self.ensemble_probs = [p / sum_probs for p in self.ensemble_probs]\n\n            if self.no_improvement_counter > self.restart_trigger or np.any(np.diag(self.C) <= 0):\n                self.population = self.rng.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                self.fitness = np.array([func(x) for x in self.population])\n                self.eval_count += self.pop_size\n                self.mean = np.mean(self.population, axis=0)\n                self.C = np.eye(self.dim)\n                self.pc = np.zeros(self.dim)\n                self.ps = np.zeros(self.dim)\n                self.sigma = 0.3\n                self.no_improvement_counter = 0\n                self.ewma_fitness = np.min(self.fitness)  # Reset EWMA\n                self.ensemble_success_counts = [0] * len(self.mutation_factors)\n                self.ensemble_trial_counts = [0] * len(self.mutation_factors)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "Evaluation timed out after 300 seconds.", "error": "Evaluation timed out after 300 seconds.", "parent_ids": ["42996575-4eeb-4828-bdd6-e3fdb9d71835"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "9e4ac537-5cff-4ded-a447-9a395f9a363f", "fitness": "-inf", "name": "HybridDEwithCMAES", "description": "A novel hybrid algorithm combining Differential Evolution with a Covariance Matrix Adaptation strategy (CMA-ES) and a self-adaptive population size to dynamically adjust exploration and exploitation.", "code": "import numpy as np\n\nclass HybridDEwithCMAES:\n    def __init__(self, budget=10000, dim=10, pop_size_initial=50, F=0.7, CR=0.9, learning_rate=0.1, restart_patience=500, cma_sigma=0.1):\n        \"\"\"\n        Args:\n            budget (int): The maximum number of function evaluations.\n            dim (int): The dimensionality of the search space.\n            pop_size_initial (int): The initial population size.\n            F (float): The mutation factor for DE.\n            CR (float): The crossover rate for DE.\n            learning_rate (float): The learning rate for adaptive step size adjustment.\n            restart_patience (int): Number of iterations without improvement before restart.\n            cma_sigma (float): Initial sigma value for CMA-ES.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size_initial\n        self.F = F\n        self.CR = CR\n        self.learning_rate = learning_rate\n        self.restart_patience = restart_patience\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.cma_sigma = cma_sigma\n        self.mean = None\n        self.C = None  # Covariance matrix\n        self.population = None # Keep population as object variable\n        self.fitness = None # Keep fitness as object variable\n\n    def __call__(self, func):\n        # Initialize population within bounds\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size # Update remaining budget\n\n        # Find the best individual in the initial population\n        best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_index]\n        self.x_opt = self.population[best_index].copy()\n\n        self.mean = self.x_opt.copy()\n        self.C = np.eye(self.dim) * self.cma_sigma**2\n\n        iteration = 0\n        no_improvement_count = 0\n        success_history = [] # for adaptive population size\n        while self.budget > 0:\n            # DE step\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = np.random.choice(idxs, 3, replace=False)\n                mutant = self.population[a] + self.F * (self.population[b] - self.population[c])\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, self.population[i])\n\n                # Boundary handling (clip to bounds)\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Evaluation\n                f = func(trial)\n                self.budget -= 1\n                if f < self.fitness[i]:\n                    self.fitness[i] = f\n                    self.population[i] = trial\n                    \n                    # Update global best\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = trial.copy()\n                        self.mean = trial.copy() # Update CMA-ES mean\n                        no_improvement_count = 0 # Reset counter\n                        success_history.append(True)\n                    else:\n                        success_history.append(False)\n\n\n                else:\n                    no_improvement_count += 1\n                    success_history.append(False)\n\n                if self.budget <= 0:\n                    break\n\n            # CMA-ES step: Sample new points around the best using CMA\n            if self.budget > self.dim*2: # only if enough budget for CMA\n                z = np.random.multivariate_normal(np.zeros(self.dim), self.C, size=2)\n                new_points = self.mean + self.cma_sigma * z\n                new_points = np.clip(new_points, func.bounds.lb, func.bounds.ub)\n                new_fitness = np.array([func(x) for x in new_points])\n                self.budget -= 2\n\n                # Replace worst individuals in population with CMA samples\n                worst_indices = self.fitness.argsort()[-2:] # indices of two worst\n                self.fitness[worst_indices] = new_fitness\n                self.population[worst_indices] = new_points\n                \n                best_index = np.argmin(self.fitness)\n                if self.fitness[best_index] < self.f_opt:\n                        self.f_opt = self.fitness[best_index]\n                        self.x_opt = self.population[best_index].copy()\n                        self.mean = self.population[best_index].copy() # Update CMA-ES mean\n                        no_improvement_count = 0\n\n            iteration += 1\n\n            # Adaptive population size adjustment\n            if len(success_history) > 10:\n                success_rate = np.mean(success_history[-10:])\n                if success_rate > 0.6 and self.pop_size < 100:\n                    self.pop_size += 5\n                    self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim)) # reinitialize\n                    self.fitness = np.array([func(x) for x in self.population])\n                    self.budget -= (self.pop_size - len(self.fitness))\n                elif success_rate < 0.2 and self.pop_size > 10:\n                    self.pop_size -= 5\n                    self.population = self.population[:self.pop_size] # truncate\n                    self.fitness = self.fitness[:self.pop_size]\n\n            # Adaptive learning rate adjustment (example: shrinking F)\n            self.F *= (1 - self.learning_rate)  # Reduce F\n            self.F = np.clip(self.F, 0.1, 0.9)  # Ensure F stays within reasonable bounds\n            self.learning_rate *= 0.99 # Reduce Learning Rate gradually\n\n            # Update CMA-ES covariance matrix (simplified)\n            if iteration % 10 == 0:\n                diff = self.population - self.mean\n                self.C = np.cov(diff.T) + np.eye(self.dim) * (self.cma_sigma * 0.01)**2 # Add regularization\n\n            # Restart mechanism if no improvement for a while\n            if no_improvement_count > self.restart_patience:\n                self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                self.fitness = np.array([func(x) for x in self.population])\n                self.budget -= self.pop_size\n                best_index = np.argmin(self.fitness)\n                if self.fitness[best_index] < self.f_opt:\n                    self.f_opt = self.fitness[best_index]\n                    self.x_opt = self.population[best_index].copy()\n                    self.mean = self.x_opt.copy() # Update CMA-ES mean\n                no_improvement_count = 0 # Reset counter\n                self.C = np.eye(self.dim) * self.cma_sigma**2  # Reset covariance matrix\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "Evaluation timed out after 300 seconds.", "error": "Evaluation timed out after 300 seconds.", "parent_ids": ["0b543e23-371f-432d-a057-a2e5b307c05c"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "fbabe5e7-ab11-4247-852d-fd54685961f0", "fitness": "-inf", "name": "AdaptiveOrthogonalPSO", "description": "Integrates a modified PSO with adaptive parameters, orthogonal learning, and a local search operator to enhance exploration and exploitation.", "code": "import numpy as np\n\nclass AdaptiveOrthogonalPSO:\n    def __init__(self, budget=10000, dim=10, pop_size=None, w_max=0.9, w_min=0.4, c1=2.0, c2=2.0, clamp_factor=0.5, orthogonal_learning_rate=0.1, local_search_probability=0.05):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size if pop_size is not None else 20\n        self.w_max = w_max  # Max inertia weight\n        self.w_min = w_min  # Min inertia weight\n        self.c1 = c1  # Cognitive coefficient\n        self.c2 = c2  # Social coefficient\n        self.clamp_factor = clamp_factor\n        self.orthogonal_learning_rate = orthogonal_learning_rate\n        self.local_search_probability = local_search_probability\n        self.swarm = None\n        self.velocities = None\n        self.personal_best_positions = None\n        self.personal_best_fitness = None\n        self.global_best_position = None\n        self.global_best_fitness = np.inf\n        self.evaluations = 0\n        self.iteration = 0\n\n    def initialize_swarm(self, func):\n        self.swarm = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.velocities = np.zeros((self.pop_size, self.dim))\n        self.personal_best_positions = self.swarm.copy()\n        self.personal_best_fitness = np.array([func(x) for x in self.swarm])\n        self.evaluations += self.pop_size\n\n        self.global_best_position = self.personal_best_positions[np.argmin(self.personal_best_fitness)].copy()\n        self.global_best_fitness = np.min(self.personal_best_fitness)\n\n    def update_velocities(self, func):\n        w = self.w_max - (self.w_max - self.w_min) * (self.iteration / self.budget)  # Linearly decreasing inertia weight\n\n        for i in range(self.pop_size):\n            r1 = np.random.rand(self.dim)\n            r2 = np.random.rand(self.dim)\n\n            cognitive_component = self.c1 * r1 * (self.personal_best_positions[i] - self.swarm[i])\n            social_component = self.c2 * r2 * (self.global_best_position - self.swarm[i])\n\n            self.velocities[i] = w * self.velocities[i] + cognitive_component + social_component\n\n            # Velocity clamping\n            max_velocity = self.clamp_factor * (func.bounds.ub - func.bounds.lb)\n            self.velocities[i] = np.clip(self.velocities[i], -max_velocity, max_velocity)\n\n    def update_positions(self, func):\n        self.swarm += self.velocities\n        self.swarm = np.clip(self.swarm, func.bounds.lb, func.bounds.ub)\n\n        fitness = np.array([func(x) for x in self.swarm])\n        self.evaluations += self.pop_size\n\n        for i in range(self.pop_size):\n            if fitness[i] < self.personal_best_fitness[i]:\n                self.personal_best_fitness[i] = fitness[i]\n                self.personal_best_positions[i] = self.swarm[i].copy()\n\n                if fitness[i] < self.global_best_fitness:\n                    self.global_best_fitness = fitness[i]\n                    self.global_best_position = self.swarm[i].copy()\n\n    def orthogonal_learning(self, func):\n        # Select a particle for orthogonal learning (e.g., the worst performing particle)\n        worst_index = np.argmax(self.personal_best_fitness)\n        \n        # Generate an orthogonal array (simplified for demonstration)\n        orthogonal_array = np.array([[-1, -1], [-1, 1], [1, -1], [1, 1]]) # Example L4 array\n\n        # Map the array values to the search space around the particle's current position\n        levels = np.linspace(self.swarm[worst_index] - self.orthogonal_learning_rate * (func.bounds.ub - func.bounds.lb),\n                             self.swarm[worst_index] + self.orthogonal_learning_rate * (func.bounds.ub - func.bounds.lb),\n                             5) # 5 levels for better spread\n        \n        candidates = []\n        for row in orthogonal_array:\n            candidate = self.swarm[worst_index].copy()\n            for j in range(self.dim):\n                #Ensure the dim is less than the orthogonal array size\n                level_index = int((row[j%2] + 1) * 2)\n                candidate[j] = levels[level_index % 5][j]  #Correctly index the levels array and wrap around for higher dims\n            candidate = np.clip(candidate, func.bounds.lb, func.bounds.ub) #clamp bounds\n            candidates.append(candidate)\n        \n        # Evaluate the candidates\n        candidate_fitnesses = [func(x) for x in candidates]\n        self.evaluations += len(candidates)\n        \n        # Select the best candidate\n        best_candidate_index = np.argmin(candidate_fitnesses)\n        \n        # Update the particle's position and personal best if the candidate is better\n        if candidate_fitnesses[best_candidate_index] < self.personal_best_fitness[worst_index]:\n            self.swarm[worst_index] = candidates[best_candidate_index].copy()\n            self.personal_best_fitness[worst_index] = candidate_fitnesses[best_candidate_index]\n            self.personal_best_positions[worst_index] = candidates[best_candidate_index].copy()\n            \n            if candidate_fitnesses[best_candidate_index] < self.global_best_fitness:\n                self.global_best_fitness = candidate_fitnesses[best_candidate_index]\n                self.global_best_position = candidates[best_candidate_index].copy()\n\n    def local_search(self, func):\n        for i in range(self.pop_size):\n            if np.random.rand() < self.local_search_probability:\n                # Apply a small perturbation to the particle's position\n                perturbation = np.random.uniform(-self.orthogonal_learning_rate * (func.bounds.ub - func.bounds.lb),\n                                                 self.orthogonal_learning_rate * (func.bounds.ub - func.bounds.lb),\n                                                 size=self.dim)\n                new_position = self.swarm[i] + perturbation\n                new_position = np.clip(new_position, func.bounds.lb, func.bounds.ub)\n\n                new_fitness = func(new_position)\n                self.evaluations += 1\n\n                if new_fitness < self.personal_best_fitness[i]:\n                    self.swarm[i] = new_position.copy()\n                    self.personal_best_fitness[i] = new_fitness\n                    self.personal_best_positions[i] = new_position.copy()\n\n                    if new_fitness < self.global_best_fitness:\n                        self.global_best_fitness = new_fitness\n                        self.global_best_position = new_position.copy()\n\n    def __call__(self, func):\n        self.initialize_swarm(func)\n\n        while self.evaluations < self.budget:\n            self.iteration += 1\n            self.update_velocities(func)\n            self.update_positions(func)\n            \n            # Apply orthogonal learning periodically\n            if self.iteration % 100 == 0:\n                self.orthogonal_learning(func)\n            \n            # Apply local search probabilistically\n            self.local_search(func)\n\n            if self.evaluations >= self.budget:\n                break\n        return self.global_best_fitness, self.global_best_position", "configspace": "", "generation": 2, "feedback": "Evaluation timed out after 300 seconds.", "error": "Evaluation timed out after 300 seconds.", "parent_ids": ["3f4dbed7-615a-44be-9e4d-93b05dfe2215"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "9cc2a885-ec96-4e63-987b-01cfedd7e9f6", "fitness": "-inf", "name": "SHADE_LSearch", "description": "A self-adjusting Differential Evolution variant employing a success-history based parameter adaptation, stochastic ranking for constraint handling, and a local search operator triggered with low probability to refine promising solutions.", "code": "import numpy as np\n\nclass SHADE_LSearch:\n    def __init__(self, budget=10000, dim=10, pop_size=50, memory_size=5, local_search_prob=0.05):\n        \"\"\"\n        Args:\n            budget (int): The maximum number of function evaluations.\n            dim (int): The dimensionality of the search space.\n            pop_size (int): The population size.\n            memory_size (int): The size of the memory for historical success rates.\n            local_search_prob (float): Probability of applying a local search.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.memory_size = memory_size\n        self.memory_CR = np.full(self.memory_size, 0.5)\n        self.memory_F = np.full(self.memory_size, 0.5)\n        self.p_values = np.full(self.memory_size, 0.1) / self.memory_size\n        self.archive = []\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.local_search_prob = local_search_prob\n\n    def __call__(self, func):\n        # Initialize population within bounds\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        # Find the best individual in the initial population\n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index].copy()\n\n        archive = []\n\n        while self.budget > 0:\n            rand_indices = np.random.choice(self.memory_size, size=self.pop_size, p=self.p_values)\n            CRs = self.memory_CR[rand_indices]\n            Fs = self.memory_F[rand_indices]\n\n            new_population = np.zeros_like(population)\n            new_fitness = np.zeros_like(fitness)\n\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b = np.random.choice(idxs, 2, replace=False)\n\n                pbest_index = np.argmin(fitness)\n                mutant = population[i] + Fs[i] * (population[pbest_index] - population[i]) + Fs[i] * (population[a] - population[b])\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CRs[i]\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Boundary handling (clip to bounds)\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Evaluation\n                f = func(trial)\n                self.budget -= 1\n\n                # Local Search\n                if np.random.rand() < self.local_search_prob:\n                    trial_ls = self.local_search(trial, func, func.bounds.lb, func.bounds.ub)\n                    f_ls = func(trial_ls)\n                    self.budget -= 1\n\n                    if f_ls < f:\n                        f = f_ls\n                        trial = trial_ls\n\n                if f < fitness[i]:\n                    new_fitness[i] = f\n                    new_population[i] = trial\n                    archive.append(population[i].copy())\n\n                    # Update global best\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = trial.copy()\n                else:\n                    new_fitness[i] = fitness[i]\n                    new_population[i] = population[i]\n\n                if self.budget <= 0:\n                    break\n\n            # Update population\n            population = new_population\n            fitness = new_fitness\n\n            # Update memory\n            successful_CRs = CRs[new_fitness < fitness]\n            successful_Fs = Fs[new_fitness < fitness]\n\n            if len(successful_CRs) > 0:\n                self.update_memory(successful_CRs, successful_Fs)\n\n        return self.f_opt, self.x_opt\n    \n    def local_search(self, x, func, lower_bound, upper_bound, num_iterations=5, step_size=0.1):\n        \"\"\"Performs a simple local search around a given solution.\"\"\"\n        x_current = x.copy()\n        f_current = func(x_current)\n        \n        for _ in range(num_iterations):\n            # Generate a random perturbation\n            delta = np.random.uniform(-step_size, step_size, size=self.dim)\n\n            # Create a new solution\n            x_new = x_current + delta\n\n            # Clip to bounds\n            x_new = np.clip(x_new, lower_bound, upper_bound)\n            \n            # Evaluate the new solution\n            f_new = func(x_new)\n\n            # Update if the new solution is better\n            if f_new < f_current:\n                x_current = x_new\n                f_current = f_new\n        \n        return x_current\n\n    def update_memory(self, successful_CRs, successful_Fs):\n        # Update CR memory\n        if len(successful_CRs) > 0:\n            mean_CR = np.mean(successful_CRs)\n            self.memory_CR[np.argmin(self.p_values)] = mean_CR\n\n        # Update F memory (using Lehmer mean)\n        if len(successful_Fs) > 0:\n            lehmer_mean_F = np.sum(successful_Fs**2) / np.sum(successful_Fs) if np.sum(successful_Fs) > 0 else 0.5\n            self.memory_F[np.argmin(self.p_values)] = lehmer_mean_F\n\n        # Update p-values based on success\n        if len(successful_CRs) > 0:\n            delta_fitness = np.abs(successful_Fs)  # Assuming F values are related to improvement magnitude. This is a simplistic choice\n            self.p_values = delta_fitness / np.sum(delta_fitness) if np.sum(delta_fitness) > 0 else np.full(self.memory_size, 1/self.memory_size)\n        else:\n            self.p_values = np.full(self.memory_size, 1/self.memory_size) # If no improvement, equally likely", "configspace": "", "generation": 2, "feedback": "In the code, line 999, in numpy.random.mtrand.RandomState.choice, the following error occurred:\nValueError: probabilities do not sum to 1", "error": "In the code, line 999, in numpy.random.mtrand.RandomState.choice, the following error occurred:\nValueError: probabilities do not sum to 1", "parent_ids": ["0b543e23-371f-432d-a057-a2e5b307c05c"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "a92a17bd-8d40-4bb7-9ddc-6772667da90e", "fitness": "-inf", "name": "AdaptiveGaussianSearch", "description": "Adaptive Gaussian Search with Momentum and Dynamic Population Size, leveraging past search direction and dynamically adjusting population size for enhanced exploration/exploitation.", "code": "import numpy as np\n\nclass AdaptiveGaussianSearch:\n    def __init__(self, budget=10000, dim=10, initial_population_size=20, initial_std=0.5, success_threshold=0.2, momentum_factor=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = initial_population_size\n        self.population_size = initial_population_size  # Dynamic population size\n        self.initial_std = initial_std\n        self.success_threshold = success_threshold\n        self.momentum_factor = momentum_factor\n        self.previous_direction = np.zeros(dim)\n\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.std = self.initial_std\n        eval_count = 0\n\n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.population_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        eval_count += self.population_size\n        \n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index]\n\n        while eval_count < self.budget:\n            # Generate new samples around the best solution with momentum\n            direction = self.x_opt - population[best_index]\n            self.previous_direction = self.momentum_factor * self.previous_direction + (1 - self.momentum_factor) * direction\n\n            new_samples = np.random.normal(loc=self.x_opt + self.previous_direction, scale=self.std, size=(self.population_size, self.dim))\n            new_samples = np.clip(new_samples, func.bounds.lb, func.bounds.ub)  # Clip to bounds\n\n            new_fitness = np.array([func(x) for x in new_samples])\n            eval_count += self.population_size\n\n            # Update best solution\n            for i in range(self.population_size):\n                if new_fitness[i] < self.f_opt:\n                    self.f_opt = new_fitness[i]\n                    self.x_opt = new_samples[i]\n\n            # Adapt the standard deviation based on success rate\n            success_rate = np.sum(new_fitness < fitness) / self.population_size\n            if success_rate > self.success_threshold:\n                self.std *= 1.1  # Increase exploration if successful\n                self.population_size = min(self.initial_population_size * 2, int(self.budget / 10)) #increase pop size\n            else:\n                self.std *= 0.9  # Decrease exploration if unsuccessful\n                self.population_size = max(int(self.initial_population_size / 2), 5) #decrease pop size\n                \n\n            fitness = np.minimum(fitness, new_fitness) #replace only if better\n            best_index = np.argmin(fitness)\n            population = np.concatenate((population, new_samples), axis=0)[:self.population_size] # Update population\n            fitness = np.concatenate((fitness, new_fitness))[:self.population_size]\n\n            if eval_count >= self.budget:\n                break\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "In the code, line 32, in __call__, the following error occurred:\nIndexError: index 16 is out of bounds for axis 0 with size 10\nOn line: direction = self.x_opt - population[best_index]", "error": "In the code, line 32, in __call__, the following error occurred:\nIndexError: index 16 is out of bounds for axis 0 with size 10\nOn line: direction = self.x_opt - population[best_index]", "parent_ids": ["65ca3433-814b-4d91-9fcf-c34d334aadd4"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "3c9c1e2c-65fd-4c74-9202-f6ed55545a0f", "fitness": 0.2121834168643278, "name": "AdaptiveProbabilisticDE", "description": "Adaptive population-based algorithm combining differential evolution with covariance matrix adaptation, dynamically adjusting parameters and focusing search using a probabilistic model built from successful solutions.", "code": "import numpy as np\n\nclass AdaptiveProbabilisticDE:\n    def __init__(self, budget=10000, dim=10, pop_size=None, mutation_factor=0.5, crossover_rate=0.7, selection_pressure=0.3):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size if pop_size is not None else 4 + int(3 * np.log(dim))\n        self.mutation_factor = mutation_factor\n        self.crossover_rate = crossover_rate\n        self.selection_pressure = selection_pressure  # Percentage of best individuals to use for adaptation\n        self.rng = np.random.default_rng()\n        self.mean = None\n        self.C = None\n        self.population = None\n        self.fitness = None\n        self.eval_count = 0\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def initialize_population(self, func):\n        self.population = self.rng.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.eval_count += self.pop_size\n        \n        best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_index]\n        self.x_opt = self.population[best_index].copy() # Important: copy the array\n    \n    def adapt_covariance_matrix(self):\n        num_selected = int(self.pop_size * self.selection_pressure)\n        indices = np.argsort(self.fitness)[:num_selected]\n        selected_population = self.population[indices]\n\n        self.mean = np.mean(selected_population, axis=0)\n        if num_selected > 1:\n            self.C = np.cov(selected_population.T)\n            # Regularize covariance matrix\n            self.C += np.eye(self.dim) * 1e-6  # Adding a small value to the diagonal\n        else:\n            self.C = np.eye(self.dim)\n        \n        \n    def generate_offspring(self, func):\n        new_population = np.zeros_like(self.population)\n        new_fitness = np.zeros_like(self.fitness)\n\n        for i in range(self.pop_size):\n            # Differential Evolution Mutation\n            idxs = self.rng.choice(self.pop_size, 3, replace=False)\n            x_1, x_2, x_3 = self.population[idxs]\n            mutant = self.population[i] + self.mutation_factor * (x_1 - x_2)\n\n            # Crossover\n            trial = self.population[i].copy()\n            j_rand = self.rng.integers(0, self.dim)\n            for j in range(self.dim):\n                if self.rng.random() < self.crossover_rate or j == j_rand:\n                    trial[j] = mutant[j]\n                    \n            # Probabilistic sampling based on CMA\n            if self.mean is not None and self.C is not None:\n                try:\n                    trial = self.rng.multivariate_normal(self.mean, self.C)\n                except np.linalg.LinAlgError:\n                    # If CMA fails, revert to DE\n                    pass # trial is already set to DE mutation.\n\n            trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n            f_trial = func(trial)\n            self.eval_count += 1\n\n            new_population[i] = trial\n            new_fitness[i] = f_trial\n            \n        return new_population, new_fitness\n\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        \n        while self.eval_count < self.budget:\n            self.adapt_covariance_matrix()\n            new_population, new_fitness = self.generate_offspring(func)\n                \n            for i in range(self.pop_size):\n                if new_fitness[i] < self.fitness[i]:\n                    self.fitness[i] = new_fitness[i]\n                    self.population[i] = new_population[i]\n                    if new_fitness[i] < self.f_opt:\n                        self.f_opt = new_fitness[i]\n                        self.x_opt = new_population[i].copy()\n                        \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "The algorithm AdaptiveProbabilisticDE scored 0.212 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["42996575-4eeb-4828-bdd6-e3fdb9d71835"], "operator": null, "metadata": {"aucs": [0.07344627737561171, 0.07376494564470792, 0.20182809236823107, 0.13882466290869688, 0.17135122463922947, 0.17212843043493742, 0.16761781593862413, 0.14905771693304104, 0.1422066903662591, 0.1495715159329788, 0.14483996659008935, 0.9993998733314424, 0.20122999213596326, 0.15034580294407673, 0.13906807088481687, 0.21416042086585219, 0.24300159694412948, 0.20063499503832782, 0.11855476019930189, 0.39263548581023866]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "06f1ca4c-7460-43de-bec6-389ed901f274", "fitness": 0.0, "name": "AdaptiveClampedPSO", "description": "A self-adaptive PSO with velocity clamping, dynamic inertia weight adjustment based on stagnation detection, and a local search operator to enhance exploitation.", "code": "import numpy as np\n\nclass AdaptiveClampedPSO:\n    def __init__(self, budget=10000, dim=10, pop_size=None, w_max=0.9, w_min=0.2, c1=2.0, c2=2.0, clamp_factor=0.5, stagnation_threshold=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size if pop_size is not None else 20\n        self.w_max = w_max\n        self.w_min = w_min\n        self.c1 = c1\n        self.c2 = c2\n        self.clamp_factor = clamp_factor\n        self.stagnation_threshold = stagnation_threshold # Number of iterations without improvement before stagnation is detected\n        self.swarm = None\n        self.velocities = None\n        self.personal_best_positions = None\n        self.personal_best_fitness = None\n        self.global_best_position = None\n        self.global_best_fitness = np.inf\n        self.evaluations = 0\n        self.stagnation_counter = 0  # Counter to track stagnation\n        self.previous_best_fitness = np.inf # Stores previous best fitness to detect stagnation\n        self.w = w_max  # Initialize inertia weight\n\n    def initialize_swarm(self, func):\n        self.swarm = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.velocities = np.zeros((self.pop_size, self.dim))\n        self.personal_best_positions = self.swarm.copy()\n        self.personal_best_fitness = np.array([func(x) for x in self.swarm])\n        self.evaluations += self.pop_size\n\n        self.global_best_position = self.personal_best_positions[np.argmin(self.personal_best_fitness)].copy()\n        self.global_best_fitness = np.min(self.personal_best_fitness)\n        self.previous_best_fitness = self.global_best_fitness  # Initialize previous best fitness\n\n    def update_velocities(self, func):\n        for i in range(self.pop_size):\n            r1 = np.random.rand(self.dim)\n            r2 = np.random.rand(self.dim)\n\n            cognitive_component = self.c1 * r1 * (self.personal_best_positions[i] - self.swarm[i])\n            social_component = self.c2 * r2 * (self.global_best_position - self.swarm[i])\n\n            self.velocities[i] = self.w * self.velocities[i] + cognitive_component + social_component\n\n            # Velocity clamping\n            max_velocity = self.clamp_factor * (func.bounds.ub - func.bounds.lb)\n            self.velocities[i] = np.clip(self.velocities[i], -max_velocity, max_velocity)\n\n    def update_positions(self, func):\n        self.swarm += self.velocities\n        self.swarm = np.clip(self.swarm, func.bounds.lb, func.bounds.ub)\n\n        fitness = np.array([func(x) for x in self.swarm])\n        self.evaluations += self.pop_size\n\n        for i in range(self.pop_size):\n            if fitness[i] < self.personal_best_fitness[i]:\n                self.personal_best_fitness[i] = fitness[i]\n                self.personal_best_positions[i] = self.swarm[i].copy()\n\n                if fitness[i] < self.global_best_fitness:\n                    self.global_best_fitness = fitness[i]\n                    self.global_best_position = self.swarm[i].copy()\n\n    def adapt_inertia_weight(self):\n        # Dynamic inertia weight adjustment based on stagnation detection\n        if self.global_best_fitness < self.previous_best_fitness:\n            self.stagnation_counter = 0\n            self.previous_best_fitness = self.global_best_fitness\n            self.w = self.w_max  # Reset inertia to explore more\n        else:\n            self.stagnation_counter += 1\n            if self.stagnation_counter > self.stagnation_threshold:\n                self.w = self.w_min  # Reduce inertia to exploit more\n\n    def local_search(self, func):\n        # Apply a local search around the best solution found so far\n        num_neighbors = 5  # Number of neighbors to sample\n        step_size = 0.05 * (func.bounds.ub - func.bounds.lb)  # Step size for local search\n\n        for _ in range(num_neighbors):\n            neighbor = self.global_best_position + np.random.uniform(-step_size, step_size, size=self.dim)\n            neighbor = np.clip(neighbor, func.bounds.lb, func.bounds.ub)  # Ensure bounds\n\n            fitness = func(neighbor)\n            self.evaluations += 1\n\n            if fitness < self.global_best_fitness:\n                self.global_best_fitness = fitness\n                self.global_best_position = neighbor.copy()\n                self.stagnation_counter = 0  # Reset stagnation counter\n\n    def __call__(self, func):\n        self.initialize_swarm(func)\n\n        while self.evaluations < self.budget:\n            self.update_velocities(func)\n            self.update_positions(func)\n            self.adapt_inertia_weight()\n            if self.stagnation_counter > self.stagnation_threshold:\n                self.local_search(func)\n            if self.evaluations >= self.budget:\n                break\n        return self.global_best_fitness, self.global_best_position", "configspace": "", "generation": 2, "feedback": "The algorithm AdaptiveClampedPSO scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["3f4dbed7-615a-44be-9e4d-93b05dfe2215"], "operator": null, "metadata": {"aucs": [0]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "4ffd3c30-0755-4418-9748-0bd8bbe1fde7", "fitness": 0.35955489467667867, "name": "AdaptiveDifferentialEvolution", "description": "Self-adaptive Differential Evolution with a distance-based mutation strategy and population archiving to enhance exploration and exploitation.", "code": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget=10000, dim=10, pop_size=40, F=0.5, CR=0.7, archive_size=10):\n        \"\"\"\n        Args:\n            budget (int): The maximum number of function evaluations.\n            dim (int): The dimensionality of the search space.\n            pop_size (int): The population size.  Reduced from 50.\n            F (float): Initial mutation factor.\n            CR (float): Initial crossover rate.\n            archive_size (int): The size of the archive.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.archive_size = archive_size\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.archive = []\n\n    def __call__(self, func):\n        # Initialize population within bounds\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size # Update remaining budget\n\n        # Find the best individual in the initial population\n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index].copy()\n\n        # Initialize Archive\n        archive = population.copy()\n\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Self-adaptive parameters\n                F_i = np.random.normal(self.F, 0.1)\n                F_i = np.clip(F_i, 0.1, 1.0)\n                CR_i = np.random.normal(self.CR, 0.1)\n                CR_i = np.clip(CR_i, 0.1, 1.0)\n\n                # Mutation: Distance-based selection from population and archive\n                candidates = np.vstack((population, archive))\n                idxs = np.arange(len(candidates))\n                np.random.shuffle(idxs)\n                a, b = idxs[:2]\n\n                # Calculate distances to current individual\n                distances = np.linalg.norm(candidates - population[i], axis=1)\n                probabilities = np.exp(-distances / np.mean(distances))  # Softmax-like weighting\n                probabilities /= np.sum(probabilities)\n\n\n                c_indices = np.random.choice(len(candidates), size=1, replace=False, p=probabilities)[0]\n                c = candidates[c_indices]\n\n\n                mutant = population[i] + F_i * (candidates[a] - candidates[b])\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR_i\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Boundary handling (clip to bounds)\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Evaluation\n                f = func(trial)\n                self.budget -= 1\n                if f < fitness[i]:\n                    # Success\n                    fitness[i] = f\n                    population[i] = trial\n\n                    # Update global best\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = trial.copy()\n\n                    # Update Archive (replace a random element)\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(trial)\n                    else:\n                        replace_index = np.random.randint(0, len(self.archive))\n                        self.archive[replace_index] = trial\n                else:\n                   # Failure, do not update archive\n                   pass\n\n\n                if self.budget <= 0:\n                    break\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "The algorithm AdaptiveDifferentialEvolution scored 0.360 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["0b543e23-371f-432d-a057-a2e5b307c05c"], "operator": null, "metadata": {"aucs": [0.14640715672004012, 0.2429882265983112, 0.33435842394559734, 0.3693670473434145, 0.2783676328490494, 0.36123637127098795, 0.28558672928843487, 0.30950044191797577, 0.27885713961394387, 0.1969930656520693, 0.3557271476941781, 0.9957393982208486, 0.2939404540277991, 0.2801216761652279, 0.6756492536285816, 0.35518378492455027, 0.30130476982523957, 0.45463587338869904, 0.18936235425010284, 0.48577094620852157]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "7653ee91-b0dc-4e39-b4b7-fa3d9fa28ab4", "fitness": 0.10749287103561063, "name": "SimplifiedHybridDECMARestart", "description": "A simplified hybrid DE-CMA-ES algorithm with adaptive parameters and a focused restart strategy based on stagnation detection and population diversity, aiming to balance exploration and exploitation more efficiently.", "code": "import numpy as np\n\nclass SimplifiedHybridDECMARestart:\n    def __init__(self, budget=10000, dim=10, pop_size=None, mutation_factor=0.5, crossover_rate=0.7, restart_trigger=50):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size if pop_size is not None else 4 + int(3 * np.log(dim))\n        self.mutation_factor = mutation_factor\n        self.crossover_rate = crossover_rate\n        self.restart_trigger = restart_trigger\n        self.rng = np.random.default_rng()\n        self.lb = -5.0\n        self.ub = 5.0\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.population = self.rng.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.eval_count += self.pop_size\n\n        self.mean = np.mean(self.population, axis=0)\n        self.C = np.eye(self.dim)\n        self.sigma = 0.3\n\n        self.mu = self.pop_size // 2\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n\n        self.c1 = 2 / ((self.dim + 1.3)**2 + self.mueff)\n        self.cmu = min(1 - self.c1, 2 * (self.mueff - 2 + 1/self.mueff) / ((self.dim + 2)**2 + self.mueff))\n        \n        self.no_improvement_counter = 0\n        self.best_fitness_ever = np.min(self.fitness)\n        self.diversity_threshold = 0.01\n        \n        while self.eval_count < self.budget:\n            # Differential Evolution Mutation\n            for i in range(self.pop_size):\n                idxs = self.rng.choice(self.pop_size, 3, replace=False)\n                x_1, x_2, x_3 = self.population[idxs]\n                mutant = self.population[i] + self.mutation_factor * (x_1 - x_2)\n                \n                # Crossover\n                trial = self.population[i].copy()\n                j_rand = self.rng.integers(0, self.dim)\n                for j in range(self.dim):\n                    if self.rng.random() < self.crossover_rate or j == j_rand:\n                        trial[j] = mutant[j]\n                \n                trial = np.clip(trial, self.lb, self.ub)\n                f_trial = func(trial)\n                self.eval_count += 1\n\n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                        self.no_improvement_counter = 0\n                else:\n                    self.no_improvement_counter +=1\n\n            # CMA-ES Adaptation (Simplified)\n            indices = np.argsort(self.fitness)\n            xmean = np.sum(self.weights[:, None] * self.population[indices[:self.mu]], axis=0)\n            y = self.population[indices[:self.mu]] - self.mean\n            self.C = (1-self.c1-self.cmu) * self.C + self.c1 * (y[0][:, None] @ y[0][None, :])  + self.cmu * np.sum(self.weights[:, None, None] * (y[:, :, None] @ y[:, None, :]), axis=0)\n            self.mean = xmean\n\n            if np.min(self.fitness) < self.best_fitness_ever:\n                self.best_fitness_ever = np.min(self.fitness)\n                self.no_improvement_counter = 0\n            else:\n                self.no_improvement_counter += 1\n\n            # Diversity Check\n            diversity = np.std(self.population)\n            \n            if self.no_improvement_counter > self.restart_trigger or diversity < self.diversity_threshold:\n                self.population = self.rng.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n                self.fitness = np.array([func(x) for x in self.population])\n                self.eval_count += self.pop_size\n                self.mean = np.mean(self.population, axis=0)\n                self.C = np.eye(self.dim)\n                self.no_improvement_counter = 0\n                \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "The algorithm SimplifiedHybridDECMARestart scored 0.107 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["42996575-4eeb-4828-bdd6-e3fdb9d71835"], "operator": null, "metadata": {"aucs": [0.12565731984076467, 0.19682129326606723, 0]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "67e55d66-5cd5-4500-ada3-51bc62f2a4ab", "fitness": "-inf", "name": "EnhancedAdaptiveDifferentialEvolution", "description": "Enhanced Self-Adaptive Differential Evolution with orthogonal design for parameter adaptation and a tournament-based archive update strategy for improved exploration and exploitation.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDifferentialEvolution:\n    def __init__(self, budget=10000, dim=10, pop_size=40, F=0.5, CR=0.7, archive_size=10, ortho_levels=3):\n        \"\"\"\n        Args:\n            budget (int): The maximum number of function evaluations.\n            dim (int): The dimensionality of the search space.\n            pop_size (int): The population size.\n            F (float): Initial mutation factor.\n            CR (float): Initial crossover rate.\n            archive_size (int): The size of the archive.\n            ortho_levels (int): Number of levels for orthogonal design.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.archive_size = archive_size\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.archive = []\n        self.ortho_levels = ortho_levels\n        self.ortho_matrix = self._generate_orthogonal_array()\n\n    def _generate_orthogonal_array(self):\n        \"\"\"Generates an orthogonal array for parameter tuning.\"\"\"\n        # A simple L9 orthogonal array (3 levels, 4 factors) - hardcoded for simplicity\n        # In a real application, consider using a library like pyDOE to generate arrays of various sizes.\n        if self.ortho_levels == 3:\n            return np.array([\n                [0, 0, 0, 0],\n                [0, 1, 1, 1],\n                [0, 2, 2, 2],\n                [1, 0, 1, 2],\n                [1, 1, 2, 0],\n                [1, 2, 0, 1],\n                [2, 0, 2, 1],\n                [2, 1, 0, 2],\n                [2, 2, 1, 0]\n            ])\n        else:\n            raise ValueError(\"Only ortho_levels=3 is currently supported.\")\n\n    def __call__(self, func):\n        # Initialize population within bounds\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size # Update remaining budget\n\n        # Find the best individual in the initial population\n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index].copy()\n\n        # Initialize Archive\n        archive = population.copy()\n\n        generation = 0 # Counter for orthogonal design application\n\n        while self.budget > 0:\n            # Apply orthogonal design every few generations\n            if generation % (self.ortho_levels * 2) == 0 and generation > 0:\n                self._apply_orthogonal_design(population, fitness, func)\n\n            for i in range(self.pop_size):\n                # Self-adaptive parameters\n                F_i = np.random.normal(self.F, 0.1)\n                F_i = np.clip(F_i, 0.1, 1.0)\n                CR_i = np.random.normal(self.CR, 0.1)\n                CR_i = np.clip(CR_i, 0.1, 1.0)\n\n                # Mutation: Distance-based selection from population and archive\n                candidates = np.vstack((population, archive))\n                idxs = np.arange(len(candidates))\n                np.random.shuffle(idxs)\n                a, b = idxs[:2]\n\n                # Calculate distances to current individual\n                distances = np.linalg.norm(candidates - population[i], axis=1)\n                probabilities = np.exp(-distances / np.mean(distances))  # Softmax-like weighting\n                probabilities /= np.sum(probabilities)\n\n\n                c_indices = np.random.choice(len(candidates), size=1, replace=False, p=probabilities)[0]\n                c = candidates[c_indices]\n\n\n                mutant = population[i] + F_i * (candidates[a] - candidates[b])\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR_i\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Boundary handling (clip to bounds)\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Evaluation\n                f = func(trial)\n                self.budget -= 1\n                if f < fitness[i]:\n                    # Success\n                    fitness[i] = f\n                    population[i] = trial\n\n                    # Update global best\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = trial.copy()\n\n                    # Tournament-based archive update\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(trial)\n                    else:\n                        # Tournament selection: compare the new solution with a random archive member\n                        archive_index = np.random.randint(0, len(self.archive))\n                        if f < func(self.archive[archive_index]):  # Evaluate archive member\n                            self.budget -= 1 # account function evaluation\n                            self.archive[archive_index] = trial # replace if trial is better\n                else:\n                   # Failure, do not update archive\n                   pass\n\n\n                if self.budget <= 0:\n                    break\n\n            generation += 1\n        return self.f_opt, self.x_opt\n\n    def _apply_orthogonal_design(self, population, fitness, func):\n        \"\"\"Applies orthogonal design to tune F and CR.\"\"\"\n        best_index = np.argmin(fitness)\n        best_F = self.F\n        best_CR = self.CR\n        best_fitness = fitness[best_index]\n\n\n        for i in range(self.ortho_matrix.shape[0]): # Iterate over rows of the orthogonal array\n            # Map orthogonal array levels to F and CR values\n            F_trial = 0.1 + (self.ortho_matrix[i, 0] / (self.ortho_levels - 1)) * 0.9 # Map to [0.1, 1.0]\n            CR_trial = 0.1 + (self.ortho_matrix[i, 1] / (self.ortho_levels - 1)) * 0.9 # Map to [0.1, 1.0]\n            # Create a temporary population with the new F and CR\n            temp_pop = population.copy()\n            temp_fitness = fitness.copy()\n\n            for j in range(self.pop_size):\n                # Apply DE with the trial F and CR\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                a, b, c = temp_pop[idxs]\n\n                mutant = a + F_trial * (b - c)\n                cross_points = np.random.rand(self.dim) < CR_trial\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, temp_pop[j])\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                f = func(trial)\n                self.budget -= 1\n                if self.budget <= 0:\n                  return\n\n                if f < temp_fitness[j]:\n                    temp_fitness[j] = f\n                    temp_pop[j] = trial\n\n            # Evaluate the best fitness with these parameters\n            best_trial_index = np.argmin(temp_fitness)\n            if temp_fitness[best_trial_index] < best_fitness:\n                best_fitness = temp_fitness[best_trial_index]\n                best_F = F_trial\n                best_CR = CR_trial\n                population = temp_pop\n                fitness = temp_fitness\n\n\n        self.F = best_F\n        self.CR = best_CR\n        self.f_opt = min(self.f_opt, best_fitness)\n        self.x_opt = population[np.argmin(fitness)].copy() # update best solution found during ortho", "configspace": "", "generation": 3, "feedback": "Evaluation timed out after 300 seconds.", "error": "Evaluation timed out after 300 seconds.", "parent_ids": ["4ffd3c30-0755-4418-9748-0bd8bbe1fde7"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "e8e50dd2-9a31-4649-899f-108d26dfefc6", "fitness": "-inf", "name": "AdaptiveMutatingPSO", "description": "An adaptive PSO variant that uses a decaying inertia weight and a mutation operator to enhance exploration and exploitation, while also employing a restart mechanism to escape local optima.", "code": "import numpy as np\n\nclass AdaptiveMutatingPSO:\n    def __init__(self, budget=10000, dim=10, pop_size=None, w_max=0.9, w_min=0.4, c1=2.0, c2=2.0, mutation_rate=0.05, restart_trigger=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size if pop_size is not None else 20\n        self.w_max = w_max\n        self.w_min = w_min\n        self.c1 = c1\n        self.c2 = c2\n        self.mutation_rate = mutation_rate\n        self.restart_trigger = restart_trigger  #Percentage of budget used before checking for restart\n        self.swarm = None\n        self.velocities = None\n        self.personal_best_positions = None\n        self.personal_best_fitness = None\n        self.global_best_position = None\n        self.global_best_fitness = np.inf\n        self.evaluations = 0\n        self.initial_global_best_fitness = None\n\n    def initialize_swarm(self, func):\n        self.swarm = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.velocities = np.random.uniform(-0.1 * (func.bounds.ub - func.bounds.lb), 0.1 * (func.bounds.ub - func.bounds.lb), size=(self.pop_size, self.dim))\n        self.personal_best_positions = self.swarm.copy()\n        self.personal_best_fitness = np.array([func(x) for x in self.swarm])\n        self.evaluations += self.pop_size\n\n        self.global_best_position = self.personal_best_positions[np.argmin(self.personal_best_fitness)].copy()\n        self.global_best_fitness = np.min(self.personal_best_fitness)\n        self.initial_global_best_fitness = self.global_best_fitness\n\n\n    def update_velocities(self, func, iteration, max_iterations):\n        w = self.w_max - (self.w_max - self.w_min) * (iteration / max_iterations)  # Decaying inertia weight\n\n        for i in range(self.pop_size):\n            r1 = np.random.rand(self.dim)\n            r2 = np.random.rand(self.dim)\n\n            cognitive_component = self.c1 * r1 * (self.personal_best_positions[i] - self.swarm[i])\n            social_component = self.c2 * r2 * (self.global_best_position - self.swarm[i])\n\n            self.velocities[i] = w * self.velocities[i] + cognitive_component + social_component\n\n\n    def mutate_particles(self, func):\n        for i in range(self.pop_size):\n            if np.random.rand() < self.mutation_rate:\n                # Apply mutation to a random dimension\n                mutation_dim = np.random.randint(0, self.dim)\n                self.swarm[i, mutation_dim] = np.random.uniform(func.bounds.lb, func.bounds.ub)\n\n    def update_positions(self, func):\n        self.swarm += self.velocities\n        self.swarm = np.clip(self.swarm, func.bounds.lb, func.bounds.ub)\n\n        fitness = np.array([func(x) for x in self.swarm])\n        self.evaluations += self.pop_size\n\n        for i in range(self.pop_size):\n            if fitness[i] < self.personal_best_fitness[i]:\n                self.personal_best_fitness[i] = fitness[i]\n                self.personal_best_positions[i] = self.swarm[i].copy()\n\n                if fitness[i] < self.global_best_fitness:\n                    self.global_best_fitness = fitness[i]\n                    self.global_best_position = self.swarm[i].copy()\n\n    def should_restart(self):\n         # Check if the improvement is below a threshold after a certain percentage of the budget\n        if self.evaluations > self.restart_trigger * self.budget:\n            improvement = (self.initial_global_best_fitness - self.global_best_fitness) / self.initial_global_best_fitness\n            if improvement < 0.0001:  # If improvement is less than 0.01%\n                return True\n        return False\n\n    def restart_swarm(self, func):\n        self.swarm = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.velocities = np.random.uniform(-0.1 * (func.bounds.ub - func.bounds.lb), 0.1 * (func.bounds.ub - func.bounds.lb), size=(self.pop_size, self.dim))\n        self.personal_best_positions = self.swarm.copy()\n        self.personal_best_fitness = np.array([func(x) for x in self.swarm])\n        self.evaluations += self.pop_size  # Account for evaluations during restart.\n\n        self.global_best_position = self.personal_best_positions[np.argmin(self.personal_best_fitness)].copy()\n        self.global_best_fitness = np.min(self.personal_best_fitness)\n\n\n    def __call__(self, func):\n        self.initialize_swarm(func)\n        max_iterations = self.budget // self.pop_size  #Rough estimate of iterations\n        iteration = 0\n\n        while self.evaluations < self.budget:\n            self.update_velocities(func, iteration, max_iterations)\n            self.mutate_particles(func)\n            self.update_positions(func)\n\n            if self.should_restart():\n                self.restart_swarm(func)\n                self.initial_global_best_fitness = self.global_best_fitness # Reset initial best after restart\n\n            iteration += 1\n\n            if self.evaluations >= self.budget:\n                break\n\n        return self.global_best_fitness, self.global_best_position", "configspace": "", "generation": 3, "feedback": "In the code, line 53, in mutate_particles, the following error occurred:\nValueError: setting an array element with a sequence.\nOn line: self.swarm[i, mutation_dim] = np.random.uniform(func.bounds.lb, func.bounds.ub)", "error": "In the code, line 53, in mutate_particles, the following error occurred:\nValueError: setting an array element with a sequence.\nOn line: self.swarm[i, mutation_dim] = np.random.uniform(func.bounds.lb, func.bounds.ub)", "parent_ids": ["3f4dbed7-615a-44be-9e4d-93b05dfe2215"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "a14f93f6-6b33-4bc6-a919-a90ba5c36a3d", "fitness": 0.22097767665865148, "name": "AdaptiveGaussianSearch", "description": "An adaptive Gaussian search that adjusts its mean and standard deviation based on the success history of previous steps, incorporating a simplified momentum term to smooth the search trajectory.", "code": "import numpy as np\n\nclass AdaptiveGaussianSearch:\n    def __init__(self, budget=10000, dim=10, initial_std=0.5, learning_rate=0.1, momentum=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.initial_std = initial_std\n        self.learning_rate = learning_rate\n        self.momentum = momentum\n        self.x_mean = None\n        self.std = None\n        self.velocity = np.zeros(dim)\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        eval_count = 0\n\n        # Initialization\n        self.x_mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        self.std = self.initial_std\n\n        # Evaluate initial solution\n        f_current = func(self.x_mean)\n        eval_count += 1\n        \n        if f_current < self.f_opt:\n            self.f_opt = f_current\n            self.x_opt = self.x_mean.copy()\n\n        while eval_count < self.budget:\n            # Generate new sample around the current mean\n            x_new = np.random.normal(loc=self.x_mean, scale=self.std, size=self.dim)\n            x_new = np.clip(x_new, func.bounds.lb, func.bounds.ub)\n\n            # Evaluate new sample\n            f_new = func(x_new)\n            eval_count += 1\n\n            # Update the mean and standard deviation based on success\n            if f_new < f_current:\n                # Update velocity with momentum\n                self.velocity = self.momentum * self.velocity + (1 - self.momentum) * (x_new - self.x_mean)\n\n                # Update mean with momentum\n                self.x_mean = self.x_mean + self.learning_rate * self.velocity\n                self.x_mean = np.clip(self.x_mean, func.bounds.lb, func.bounds.ub)\n                f_current = f_new\n\n\n                # Reduce standard deviation if successful\n                self.std *= 0.95\n            else:\n                # Increase standard deviation if unsuccessful\n                self.std *= 1.05\n\n            self.std = np.clip(self.std, 1e-6, 1.0) # Ensure std stays within reasonable bounds\n            \n            if f_current < self.f_opt:\n                self.f_opt = f_current\n                self.x_opt = self.x_mean.copy()\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "The algorithm AdaptiveGaussianSearch scored 0.221 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["65ca3433-814b-4d91-9fcf-c34d334aadd4"], "operator": null, "metadata": {"aucs": [0.1633542078575403, 0.18618582459746402, 0.26585072429100187, 0.16190830402432788, 0.2086215791862498, 0.2568681381748902, 0.22608953074687532, 0.24213651336370445, 0.23814487476689072, 0.15249315534545926, 0.2833680541937339, 0.16497390177582305, 0.27566217837469087, 0.12169617960184664, 0.2714536019335976, 0.18403910513577293, 0.263454637130497, 0.1788437999661031, 0.13735893709857339, 0.4370502856079874]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "4e6f6197-1a0b-4885-a8dc-56b9fc2a9809", "fitness": 0.426399737308546, "name": "AdaptiveGaussianSearch", "description": "Exploits promising regions by iteratively shrinking a Gaussian distribution centered on the best solution and probabilistically restarting when progress stalls.", "code": "import numpy as np\n\nclass AdaptiveGaussianSearch:\n    def __init__(self, budget=10000, dim=10, initial_std=0.5, stagnation_tolerance=100):\n        self.budget = budget\n        self.dim = dim\n        self.initial_std = initial_std\n        self.stagnation_tolerance = stagnation_tolerance\n        self.std = self.initial_std\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.stagnation_counter = 0\n\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n\n        # Initialization\n        x = np.random.uniform(lb, ub, size=self.dim)\n        f = func(x)\n        self.eval_count += 1\n\n        self.f_opt = f\n        self.x_opt = x\n\n        while self.eval_count < self.budget:\n            # Generate a new sample from a Gaussian distribution\n            new_x = np.random.normal(loc=self.x_opt, scale=self.std, size=self.dim)\n            new_x = np.clip(new_x, lb, ub)\n            new_f = func(new_x)\n            self.eval_count += 1\n\n            # Update the best solution if the new sample is better\n            if new_f < self.f_opt:\n                self.f_opt = new_f\n                self.x_opt = new_x\n                self.stagnation_counter = 0  # Reset stagnation counter\n            else:\n                self.stagnation_counter += 1\n\n            # Adapt the standard deviation\n            if self.stagnation_counter > self.stagnation_tolerance:\n                self.std *= 0.8  # Reduce exploration if stagnating\n                self.stagnation_counter = 0\n\n                # Probabilistic Restart\n                if np.random.rand() < 0.2:\n                    self.x_opt = np.random.uniform(lb, ub, size=self.dim)\n                    self.f_opt = func(self.x_opt)\n                    self.eval_count += 1\n                    self.std = self.initial_std  # Reset std after restart\n\n\n            self.std = np.clip(self.std, 1e-6, self.initial_std) #avoid std becoming zero.\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "The algorithm AdaptiveGaussianSearch scored 0.426 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["65ca3433-814b-4d91-9fcf-c34d334aadd4"], "operator": null, "metadata": {"aucs": [0.16171214672499867, 0.27660703216064286, 0.46689326787911445, 0.4921426101575255, 0.4250200406625002, 0.5261521866072332, 0.2857628440724853, 0.40682523031290374, 0.30104660940509886, 0.18205680673971203, 0.6963937156646858, 0.8756202959498662, 0.24689847990518976, 0.3865788844817616, 0.8163672696056816, 0.3988796979134539, 0.3658487771294522, 0.4932653952236923, 0.21857329840034911, 0.5053501571745715]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "9fb791a7-9d8a-4f8e-bc75-5fa9ae7db1b9", "fitness": "-inf", "name": "EnhancedAdaptiveDifferentialEvolution", "description": "Enhanced Adaptive Differential Evolution with orthogonal design for crossover and covariance matrix adaptation for mutation.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDifferentialEvolution:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.7, CR=0.9, learning_rate=0.1, restart_patience=500, orthogonal_design_size=7):\n        \"\"\"\n        Args:\n            budget (int): The maximum number of function evaluations.\n            dim (int): The dimensionality of the search space.\n            pop_size (int): The population size.\n            F (float): The initial mutation factor.\n            CR (float): The initial crossover rate.\n            learning_rate (float): The learning rate for adaptive step size.\n            restart_patience (int): Number of iterations without improvement before restart.\n            orthogonal_design_size (int): Number of dimensions to vary using orthogonal design in crossover.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.learning_rate = learning_rate\n        self.restart_patience = restart_patience\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.orthogonal_design_size = min(orthogonal_design_size, dim) # Ensure it doesn't exceed the dimension\n        self.covariance_matrix = np.eye(dim) * 0.1 # Initialize covariance matrix for CMA-ES-like mutation\n\n    def generate_orthogonal_array(self, n, k):\n        \"\"\"\n        Generates an orthogonal array of size n with k factors (dimensions).\n        Uses a simple approach for power-of-2 levels.\n        \"\"\"\n        if n not in [4, 8, 16, 32]:\n            raise ValueError(\"Orthogonal array size must be a power of 2 (4, 8, 16, 32).\")\n\n        if n == 4:\n            oa = np.array([[0, 0, 0],\n                           [0, 1, 1],\n                           [1, 0, 1],\n                           [1, 1, 0]])\n            if k > 3:\n                oa = np.pad(oa, ((0,0),(0, k-3)), 'constant')\n        elif n == 8:\n             oa = np.array([[0, 0, 0, 0, 0, 0, 0],\n                            [0, 0, 0, 1, 1, 1, 1],\n                            [0, 1, 1, 0, 0, 1, 1],\n                            [0, 1, 1, 1, 1, 0, 0],\n                            [1, 0, 1, 0, 1, 0, 1],\n                            [1, 0, 1, 1, 0, 1, 0],\n                            [1, 1, 0, 0, 1, 1, 0],\n                            [1, 1, 0, 1, 0, 0, 1]])\n             if k > 7:\n                 oa = np.pad(oa, ((0,0),(0, k-7)), 'constant')\n        elif n == 16:\n            oa = np.array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n                           [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1],\n                           [0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1],\n                           [0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1],\n                           [0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1],\n                           [1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1],\n                           [1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1],\n                           [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n                           [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n                           [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n                           [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n                           [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n                           [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n                           [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n                           [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n            if k > 15:\n                oa = np.pad(oa, ((0,0),(0, k-15)), 'constant')\n\n\n        return oa\n\n    def __call__(self, func):\n        # Initialize population within bounds\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size # Update remaining budget\n\n        # Find the best individual in the initial population\n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index].copy()\n\n        iteration = 0\n        no_improvement_count = 0\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation (CMA-ES-like)\n                mutant = population[i] + self.F * np.random.multivariate_normal(np.zeros(self.dim), self.covariance_matrix)\n\n                # Orthogonal Crossover\n                if self.orthogonal_design_size > 0:\n                    oa_size = 4 # Smallest working OA size\n                    orthogonal_array = self.generate_orthogonal_array(oa_size, self.orthogonal_design_size)\n                    selected_dims = np.random.choice(self.dim, self.orthogonal_design_size, replace=False)\n                    trial = population[i].copy()\n\n                    for row in orthogonal_array:\n                        temp_trial = population[i].copy()\n                        for j, dim_index in enumerate(selected_dims):\n                            if row[j] == 1:\n                                temp_trial[dim_index] = mutant[dim_index]\n                        \n                        # Boundary handling (clip to bounds)\n                        temp_trial = np.clip(temp_trial, func.bounds.lb, func.bounds.ub)\n                        f_temp = func(temp_trial)\n                        self.budget -= 1\n                        if f_temp < fitness[i]:\n                            fitness[i] = f_temp\n                            population[i] = temp_trial\n                            \n                            # Update global best\n                            if f_temp < self.f_opt:\n                                self.f_opt = f_temp\n                                self.x_opt = temp_trial.copy()\n                                no_improvement_count = 0 # Reset counter\n\n                        if self.budget <= 0:\n                            break\n\n                    if self.budget <= 0:\n                        break\n                else:\n                    # Standard DE Crossover\n                    cross_points = np.random.rand(self.dim) < self.CR\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n                    trial = np.where(cross_points, mutant, population[i])\n\n                    # Boundary handling (clip to bounds)\n                    trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                    # Evaluation\n                    f = func(trial)\n                    self.budget -= 1\n                    if f < fitness[i]:\n                        fitness[i] = f\n                        population[i] = trial\n                        \n                        # Update global best\n                        if f < self.f_opt:\n                            self.f_opt = f\n                            self.x_opt = trial.copy()\n                            no_improvement_count = 0 # Reset counter\n                    else:\n                        no_improvement_count += 1\n\n                    if self.budget <= 0:\n                        break\n\n            iteration += 1\n            \n            # Adaptive learning rate adjustment (example: shrinking F and CR)\n            self.F *= (1 - self.learning_rate)  # Reduce F\n            self.F = np.clip(self.F, 0.1, 0.9)  # Ensure F stays within reasonable bounds\n            self.CR *= (1 - self.learning_rate)  # Reduce CR\n            self.CR = np.clip(self.CR, 0.1, 0.9)  # Ensure CR stays within reasonable bounds\n            self.learning_rate *= 0.99 # Reduce Learning Rate gradually\n             # Update covariance matrix (simplified)\n            best_individual = population[np.argmin(fitness)]\n            diff = population - best_individual\n            self.covariance_matrix = (1 - self.learning_rate) * self.covariance_matrix + self.learning_rate * np.cov(diff.T)\n            self.covariance_matrix = np.clip(self.covariance_matrix, 0.0001, 1) #Bound the covariance\n\n            # Restart mechanism if no improvement for a while\n            if no_improvement_count > self.restart_patience:\n                population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                fitness = np.array([func(x) for x in population])\n                self.budget -= self.pop_size\n                best_index = np.argmin(fitness)\n                if fitness[best_index] < self.f_opt:\n                    self.f_opt = fitness[best_index]\n                    self.x_opt = population[best_index].copy()\n                no_improvement_count = 0 # Reset counter\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "Evaluation timed out after 300 seconds.", "error": "Evaluation timed out after 300 seconds.", "parent_ids": ["0b543e23-371f-432d-a057-a2e5b307c05c"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "64ebeab6-6d09-40b6-abf2-4dee181b4837", "fitness": "-inf", "name": "AdaptiveGaussianSearch", "description": "Adaptive Gaussian Search with momentum, adaptive population size, and a Cauchy mutation operator for enhanced exploration.", "code": "import numpy as np\n\nclass AdaptiveGaussianSearch:\n    def __init__(self, budget=10000, dim=10, initial_population_size=20, initial_std=0.5, success_threshold=0.2, momentum=0.1, min_population_size=5, max_population_size=100):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = initial_population_size\n        self.population_size = initial_population_size\n        self.initial_std = initial_std\n        self.success_threshold = success_threshold\n        self.momentum = momentum\n        self.min_population_size = min_population_size\n        self.max_population_size = max_population_size\n        self.velocity = np.zeros(dim)  # Initialize velocity for momentum\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.std = self.initial_std\n        eval_count = 0\n\n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.population_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        eval_count += self.population_size\n        \n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index]\n\n        while eval_count < self.budget:\n            # Generate new samples around the best solution with momentum and Cauchy mutation\n            new_samples = np.random.normal(loc=self.x_opt, scale=self.std, size=(self.population_size, self.dim))\n            cauchy_noise = np.random.standard_cauchy(size=(self.population_size, self.dim)) * self.std * 0.1  # Cauchy noise\n            new_samples += cauchy_noise\n            \n            # Apply momentum\n            self.velocity = self.momentum * self.velocity + (1 - self.momentum) * (new_samples - self.x_opt)\n            new_samples = new_samples + self.velocity\n            \n\n            new_samples = np.clip(new_samples, func.bounds.lb, func.bounds.ub)  # Clip to bounds\n\n            new_fitness = np.array([func(x) for x in new_samples])\n            eval_count += self.population_size\n\n            # Update best solution\n            for i in range(self.population_size):\n                if new_fitness[i] < self.f_opt:\n                    self.f_opt = new_fitness[i]\n                    self.x_opt = new_samples[i]\n\n            # Adapt the standard deviation based on success rate\n            success_rate = np.sum(new_fitness < fitness) / self.population_size\n            if success_rate > self.success_threshold:\n                self.std *= 1.1  # Increase exploration if successful\n                self.population_size = min(self.population_size + 1, self.max_population_size) #increase population size\n            else:\n                self.std *= 0.9  # Decrease exploration if unsuccessful\n                self.population_size = max(self.population_size - 1, self.min_population_size) #decrease population size\n                \n\n            fitness = np.minimum(fitness, new_fitness) #replace only if better\n            best_index = np.argmin(fitness)\n            \n            #reinitialize population if stuck\n            if np.std(fitness) < 1e-8:\n                 population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.population_size, self.dim))\n                 fitness = np.array([func(x) for x in population])\n                 eval_count += self.population_size\n                 best_index = np.argmin(fitness)\n                 self.f_opt = fitness[best_index]\n                 self.x_opt = population[best_index]\n\n            if eval_count >= self.budget:\n                break\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "In the code, line 38, in __call__, the following error occurred:\nValueError: operands could not be broadcast together with shapes (20,2) (21,2) \nOn line: self.velocity = self.momentum * self.velocity + (1 - self.momentum) * (new_samples - self.x_opt)", "error": "In the code, line 38, in __call__, the following error occurred:\nValueError: operands could not be broadcast together with shapes (20,2) (21,2) \nOn line: self.velocity = self.momentum * self.velocity + (1 - self.momentum) * (new_samples - self.x_opt)", "parent_ids": ["65ca3433-814b-4d91-9fcf-c34d334aadd4"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "ea87e7e8-65d9-469b-97da-a9feec0e09a1", "fitness": "-inf", "name": "CMA_DE", "description": "An adaptive population-based algorithm combining aspects of covariance matrix adaptation evolution strategy (CMA-ES) with differential evolution (DE) to adapt the search distribution and mutation strategy during optimization.", "code": "import numpy as np\n\nclass CMA_DE:\n    def __init__(self, budget=10000, dim=10, pop_size=20, initial_sigma=0.5, cs=0.3, c1=0.1, cmu=0.1, restart_patience=500):\n        \"\"\"\n        Args:\n            budget (int): The maximum number of function evaluations.\n            dim (int): The dimensionality of the search space.\n            pop_size (int): The population size.\n            initial_sigma (float): Initial step size.\n            cs (float): Learning rate for step size adaptation.\n            c1 (float): Learning rate for rank-one update of covariance matrix.\n            cmu (float): Learning rate for rank-mu update of covariance matrix.\n            restart_patience (int): Number of iterations without improvement before restart.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.sigma = initial_sigma\n        self.cs = cs\n        self.c1 = c1\n        self.cmu = cmu\n        self.restart_patience = restart_patience\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.mean = None  # Initialize mean here\n\n    def __call__(self, func):\n        # Initialize population and covariance matrix\n        self.mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)  # Initialize mean\n        C = np.eye(self.dim)  # Covariance matrix\n        population = np.random.multivariate_normal(self.mean, self.sigma**2 * C, size=self.pop_size) # create pop\n        population = np.clip(population, func.bounds.lb, func.bounds.ub)\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        # Find the best individual in the initial population\n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index].copy()\n\n        p_sigma = np.zeros(self.dim) # Evolution path for step size\n        weights = np.log(self.pop_size + 1) - np.log(np.arange(1, self.pop_size + 1))\n        weights /= np.sum(weights)  # Normalize weights\n\n        iteration = 0\n        no_improvement_count = 0\n        while self.budget > 0:\n            # Generate and evaluate offspring\n            offspring = np.zeros((self.pop_size, self.dim))\n            fitness_offspring = np.zeros(self.pop_size)\n\n            for i in range(self.pop_size):\n                # Sample from multivariate normal distribution, then perform DE mutation\n                z = np.random.multivariate_normal(np.zeros(self.dim), C)\n                y = self.mean + self.sigma * z\n                y = np.clip(y, func.bounds.lb, func.bounds.ub)\n                \n                # Differential Evolution Mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = np.random.choice(idxs, 3, replace=False)\n                mutant = population[a] + 0.8 * (population[b] - population[c])\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub) # Keep it inside the bounds.\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < 0.9\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, y)\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                offspring[i] = trial\n                fitness_offspring[i] = func(trial)\n                self.budget -= 1\n\n                if fitness_offspring[i] < self.f_opt:\n                    self.f_opt = fitness_offspring[i]\n                    self.x_opt = offspring[i].copy()\n                    no_improvement_count = 0\n                else:\n                    no_improvement_count += 1\n\n                if self.budget <= 0:\n                    break\n\n            # Sort offspring based on fitness\n            sorted_indices = np.argsort(fitness_offspring)\n            offspring = offspring[sorted_indices]\n            fitness_offspring = fitness_offspring[sorted_indices]\n            \n            # Update mean\n            delta_mean = np.sum(weights[:, None] * (offspring[:self.pop_size] - self.mean), axis=0)\n            self.mean += delta_mean\n\n            # Update evolution path for step size\n            p_sigma = (1 - self.cs) * p_sigma + np.sqrt(self.cs * (2 - self.cs)) * delta_mean / self.sigma\n            self.sigma *= np.exp(self.cs / 0.6 * (np.linalg.norm(p_sigma) / np.sqrt(self.dim) - 1))\n            self.sigma = max(self.sigma, 1e-6) # Prevent sigma from becoming too small\n\n            # Update covariance matrix\n            C = (1 - self.c1 - self.cmu) * C + self.c1 * np.outer(p_sigma, p_sigma)\n            C += self.cmu * np.sum(weights[i] * np.outer((offspring[i] - self.mean) / self.sigma, (offspring[i] - self.mean) / self.sigma) for i in range(self.pop_size))\n\n            # Ensure covariance matrix is positive definite\n            try:\n                np.linalg.cholesky(C)\n            except np.linalg.LinAlgError:\n                C = np.eye(self.dim)\n\n            population = offspring.copy()\n            fitness = fitness_offspring.copy()\n            \n            iteration += 1\n\n            # Restart mechanism\n            if no_improvement_count > self.restart_patience:\n                self.mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                C = np.eye(self.dim)\n                population = np.random.multivariate_normal(self.mean, self.sigma**2 * C, size=self.pop_size)\n                population = np.clip(population, func.bounds.lb, func.bounds.ub)\n\n                fitness = np.array([func(x) for x in population])\n                self.budget -= self.pop_size\n                best_index = np.argmin(fitness)\n                if fitness[best_index] < self.f_opt:\n                    self.f_opt = fitness[best_index]\n                    self.x_opt = population[best_index].copy()\n                no_improvement_count = 0\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "In the code, line 2419, in sum, the following error occurred:\nTypeError: Calling np.sum(generator) is deprecated.Use np.sum(np.fromiter(generator)) or the python sum builtin instead.", "error": "In the code, line 2419, in sum, the following error occurred:\nTypeError: Calling np.sum(generator) is deprecated.Use np.sum(np.fromiter(generator)) or the python sum builtin instead.", "parent_ids": ["0b543e23-371f-432d-a057-a2e5b307c05c"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "d5ed18c1-dca7-4a82-9f58-847acf44c488", "fitness": 0.5056444280602058, "name": "AdaptiveDifferentialEvolution", "description": "Adaptive Differential Evolution with a simplified parameter adaptation strategy and a focus on population diversity maintenance through periodic re-initialization of poorly performing individuals.", "code": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget=10000, dim=10, pop_size=40, F=0.5, CR=0.7, restart_patience=300, lr_F=0.05, lr_CR=0.05, diversity_threshold=0.1):\n        \"\"\"\n        Args:\n            budget (int): The maximum number of function evaluations.\n            dim (int): The dimensionality of the search space.\n            pop_size (int): The population size.\n            F (float): The mutation factor.\n            CR (float): The crossover rate.\n            restart_patience (int): Number of iterations without improvement before restart.\n            lr_F (float): Learning rate for F adaptation.\n            lr_CR (float): Learning rate for CR adaptation.\n            diversity_threshold (float): Threshold for diversity check.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.restart_patience = restart_patience\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.lr_F = lr_F\n        self.lr_CR = lr_CR\n        self.diversity_threshold = diversity_threshold\n\n    def __call__(self, func):\n        # Initialize population within bounds\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        # Find the best individual in the initial population\n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index].copy()\n\n        iteration = 0\n        no_improvement_count = 0\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = np.random.choice(idxs, 3, replace=False)\n                mutant = population[a] + self.F * (population[b] - population[c])\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Boundary handling\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Evaluation\n                f = func(trial)\n                self.budget -= 1\n                if f < fitness[i]:\n                    # Success: Adapt parameters\n                    fitness[i] = f\n                    population[i] = trial\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = trial.copy()\n                        no_improvement_count = 0\n                        self.F += self.lr_F * (1 - self.F) # Increase F upon success to diversify search\n                        self.CR += self.lr_CR * (1 - self.CR)  # Increase CR upon success to refine search\n\n                else:\n                    # Failure: Adapt parameters\n                    no_improvement_count += 1\n                    self.F -= self.lr_F * self.F # Decrease F upon failure to focus search\n                    self.CR -= self.lr_CR * self.CR # Decrease CR upon failure to broaden search\n\n\n                self.F = np.clip(self.F, 0.1, 0.9)\n                self.CR = np.clip(self.CR, 0.1, 0.9)\n                if self.budget <= 0:\n                    break\n\n            iteration += 1\n\n            # Restart mechanism (simplified - re-initialize only the worst individuals)\n            if no_improvement_count > self.restart_patience:\n                worst_indices = np.argsort(fitness)[-self.pop_size // 4:] # Re-initialize 25% worst individuals\n                population[worst_indices] = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size // 4, self.dim))\n                fitness[worst_indices] = np.array([func(x) for x in population[worst_indices]])\n                self.budget -= self.pop_size // 4\n\n                best_index = np.argmin(fitness)\n                if fitness[best_index] < self.f_opt:\n                    self.f_opt = fitness[best_index]\n                    self.x_opt = population[best_index].copy()\n                no_improvement_count = 0\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "The algorithm AdaptiveDifferentialEvolution scored 0.506 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["0b543e23-371f-432d-a057-a2e5b307c05c"], "operator": null, "metadata": {"aucs": [0.17340937849539384, 0.2887571361503195, 0.4762535511141214, 0.6922921659031614, 0.5351345915523769, 0.7329752339767399, 0.401171229088334, 0.44024711247050996, 0.46041548287818446, 0.3635872057114493, 0.3601338790834321, 0.999744929017229, 0.27650981027573285, 0.4001629498385594, 0.817127236500866, 0.7593522333598788, 0.46388459205503063, 0.7041139532381155, 0.23424804739505412, 0.5333678430996257]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "b6e0d13e-186e-44cc-a9a7-fe46dd0daaef", "fitness": 0.16040005358396137, "name": "AdaptiveODE", "description": "Adaptive Differential Evolution with orthogonal learning, covariance matrix adaptation, and a simplified restart strategy.", "code": "import numpy as np\n\nclass AdaptiveODE:\n    def __init__(self, budget=10000, dim=10, pop_size=40, F=0.5, CR=0.7, restart_patience=300):\n        \"\"\"\n        Args:\n            budget (int): The maximum number of function evaluations.\n            dim (int): The dimensionality of the search space.\n            pop_size (int): The population size.\n            F (float): The mutation factor.\n            CR (float): The crossover rate.\n            restart_patience (int): Number of iterations without improvement before restart.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.restart_patience = restart_patience\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.cov = np.eye(dim)  # Covariance matrix for orthogonal learning\n        self.mean = None\n\n    def __call__(self, func):\n        # Initialize population within bounds\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.mean = np.mean(population, axis=0)\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        # Find the best individual in the initial population\n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index].copy()\n\n        iteration = 0\n        no_improvement_count = 0\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = np.random.choice(idxs, 3, replace=False)\n                mutant = population[a] + self.F * (population[b] - population[c])\n\n                # Orthogonal Crossover\n                orthogonal_sample = self.mean + np.random.multivariate_normal(np.zeros(self.dim), self.cov)\n                \n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, orthogonal_sample)\n\n\n                # Boundary handling\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Evaluation\n                f = func(trial)\n                self.budget -= 1\n                if f < fitness[i]:\n                    fitness[i] = f\n                    population[i] = trial\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = trial.copy()\n                        no_improvement_count = 0\n                else:\n                    no_improvement_count += 1\n\n                if self.budget <= 0:\n                    break\n\n            iteration += 1\n            self.mean = np.mean(population, axis=0)\n\n            # CMA-like update of covariance matrix (simplified)\n            diff = population - self.mean\n            self.cov = np.cov(diff.T) + 1e-6 * np.eye(self.dim)  # Add a small value for numerical stability\n\n            # Restart mechanism (simplified - re-initialize only the worst individual if stuck)\n            if no_improvement_count > self.restart_patience:\n                worst_index = np.argmax(fitness)\n                population[worst_index] = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                fitness[worst_index] = func(population[worst_index])\n                self.budget -= 1\n\n                if fitness[worst_index] < self.f_opt:\n                    self.f_opt = fitness[worst_index]\n                    self.x_opt = population[worst_index].copy()\n                no_improvement_count = 0\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "The algorithm AdaptiveODE scored 0.160 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["d5ed18c1-dca7-4a82-9f58-847acf44c488"], "operator": null, "metadata": {"aucs": [0.32080010716792273, 0]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "e5816bba-69aa-4bd8-a268-23326ae2ca01", "fitness": 0.27611348162537797, "name": "AdaptiveGaussianSearch", "description": "An adaptive Gaussian search algorithm that dynamically adjusts its step size based on the success history and introduces a momentum term to guide the search direction.", "code": "import numpy as np\n\nclass AdaptiveGaussianSearch:\n    def __init__(self, budget=10000, dim=10, initial_std=0.5, success_threshold=0.2, momentum_factor=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.initial_std = initial_std\n        self.success_threshold = success_threshold\n        self.momentum_factor = momentum_factor\n        self.x_prev = None\n        self.std_history = [] # Track std history\n        \n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.std = self.initial_std\n        eval_count = 0\n\n        # Initialize solution\n        x = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        f = func(x)\n        eval_count += 1\n\n        self.x_opt = x\n        self.f_opt = f\n        self.x_prev = x.copy() #initialize previous solution\n\n        while eval_count < self.budget:\n            # Generate new sample around the best solution with momentum\n            if self.x_prev is not None:\n                x_new = np.random.normal(loc=self.x_opt + self.momentum_factor * (self.x_opt - self.x_prev), scale=self.std, size=self.dim)\n            else:\n                x_new = np.random.normal(loc=self.x_opt, scale=self.std, size=self.dim)\n                \n            x_new = np.clip(x_new, func.bounds.lb, func.bounds.ub)  # Clip to bounds\n\n            f_new = func(x_new)\n            eval_count += 1\n\n            # Update best solution\n            if f_new < self.f_opt:\n                self.f_opt = f_new\n                self.x_opt = x_new\n                success = True\n            else:\n                success = False\n\n            # Adapt the standard deviation based on success\n            if success:\n                self.std *= 1.1  # Increase exploration if successful\n            else:\n                self.std *= 0.9  # Decrease exploration if unsuccessful\n            \n            self.std = np.clip(self.std, self.initial_std/10, self.initial_std*10) #prevent std from vanishing or exploding\n            self.std_history.append(self.std)\n            \n            self.x_prev = x.copy()\n            x = x_new #Update the current solution for the next iteration\n\n            if eval_count >= self.budget:\n                break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "The algorithm AdaptiveGaussianSearch scored 0.276 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["65ca3433-814b-4d91-9fcf-c34d334aadd4"], "operator": null, "metadata": {"aucs": [0.024017215853281648, 0.10871557205541271, 0.4835961565673029, 0.16477172798737116, 0.18124540205009798, 0.12204050796782517, 0.2519288306408254, 0.103000219256423, 0.15914739046219273, 0.1032870324768117, 0.9651166806169782, 0.13611284480836783, 0.23805373559199317, 0.1813530875514402, 0.8556674375416305, 0.33590701170898496, 0.1721460187203857, 0.6623452342493787, 0.09366591798131996, 0.1801516084195346]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "ca08a41a-418b-4ee4-ba18-aaa3d01359f5", "fitness": 0.617270231475303, "name": "AdaptiveDifferentialEvolution", "description": "Simplified Adaptive Differential Evolution with rank-based selection and adaptive CR/F parameters based on recent success.", "code": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget=10000, dim=10, pop_size=40, F_base=0.5, CR_base=0.7, learning_rate=0.1, success_history_size=10):\n        \"\"\"\n        Args:\n            budget (int): The maximum number of function evaluations.\n            dim (int): The dimensionality of the search space.\n            pop_size (int): The population size.\n            F_base (float): Base mutation factor.\n            CR_base (float): Base crossover rate.\n            learning_rate (float): The learning rate for adaptive CR/F.\n            success_history_size (int): Size of the success history for adapting CR/F.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F_base = F_base\n        self.CR_base = CR_base\n        self.learning_rate = learning_rate\n        self.success_history_size = success_history_size\n        self.success_F = np.ones(self.success_history_size) * self.F_base\n        self.success_CR = np.ones(self.success_history_size) * self.CR_base\n        self.success_idx = 0\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        # Initialize population within bounds\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size # Update remaining budget\n\n        # Find the best individual in the initial population\n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index].copy()\n\n        while self.budget > 0:\n            # Rank population\n            ranked_indices = np.argsort(fitness)\n            \n            for i in range(self.pop_size):\n                # Mutation: Rank-based selection of parents\n                pbest_idx = ranked_indices[np.random.randint(0, max(1, int(0.2 * self.pop_size)))] # Select best 20%\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b = np.random.choice(idxs, 2, replace=False)\n                \n                # Adaptive F and CR\n                F = np.random.normal(self.F_base, 0.1)\n                F = np.clip(F, 0.1, 0.9)\n                CR = np.random.normal(self.CR_base, 0.1)\n                CR = np.clip(CR, 0.1, 0.9)\n                \n                mutant = population[i] + F * (population[pbest_idx] - population[a]) + F * (population[b] - population[i])\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Boundary handling (clip to bounds)\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Evaluation\n                f = func(trial)\n                self.budget -= 1\n                \n                if f < fitness[i]:\n                    delta = fitness[i] - f\n                    fitness[i] = f\n                    population[i] = trial\n                    \n                    # Update global best\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = trial.copy()\n\n                    # Update success history for F and CR\n                    self.success_F[self.success_idx] = F\n                    self.success_CR[self.success_idx] = CR\n                    self.success_idx = (self.success_idx + 1) % self.success_history_size\n\n                    # Adjust base values based on recent success\n                    self.F_base = np.mean(self.success_F)\n                    self.CR_base = np.mean(self.success_CR)\n                    self.F_base = np.clip(self.F_base, 0.1, 0.9)\n                    self.CR_base = np.clip(self.CR_base, 0.1, 0.9)\n\n                if self.budget <= 0:\n                    break\n            \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "The algorithm AdaptiveDifferentialEvolution scored 0.617 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["0b543e23-371f-432d-a057-a2e5b307c05c"], "operator": null, "metadata": {"aucs": [0.24103898146050273, 0.7350997209351331, 0.5861709650471683, 0.8422658512489773, 0.8048015236164302, 0.8510471413376336, 0.455748512998682, 0.594728547568217, 0.8078335099206726, 0.1874927806016803, 0.5370039728209981, 0.9900382160367851, 0.3144890164673446, 0.7807347239484549, 0.8400330525811348, 0.817788945097123, 0.3252396121582275, 0.8844394906167979, 0.2299270791269622, 0.5194829859171349]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "dd524f86-fb83-498a-89b8-95b03d054dd3", "fitness": 0.5068273274898856, "name": "AdaptiveGaussianCauchySearch", "description": "An adaptive population-based algorithm that combines Gaussian mutation around the best solution with a Cauchy mutation for wider exploration, dynamically adjusting mutation strengths based on success.", "code": "import numpy as np\n\nclass AdaptiveGaussianCauchySearch:\n    def __init__(self, budget=10000, dim=10, population_size=20, initial_gaussian_std=0.5, initial_cauchy_std=1.0, success_threshold=0.25, cauchy_prob=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = population_size\n        self.initial_gaussian_std = initial_gaussian_std\n        self.initial_cauchy_std = initial_cauchy_std\n        self.success_threshold = success_threshold\n        self.cauchy_prob = cauchy_prob\n        self.gaussian_std = initial_gaussian_std\n        self.cauchy_std = initial_cauchy_std\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        eval_count = 0\n\n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.population_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        eval_count += self.population_size\n\n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index]\n\n        while eval_count < self.budget:\n            new_samples = np.zeros((self.population_size, self.dim))\n            for i in range(self.population_size):\n                if np.random.rand() < self.cauchy_prob:\n                    # Cauchy mutation for exploration\n                    mutation = np.random.standard_cauchy(size=self.dim) * self.cauchy_std\n                    new_sample = self.x_opt + mutation\n                else:\n                    # Gaussian mutation around the best\n                    new_sample = np.random.normal(loc=self.x_opt, scale=self.gaussian_std, size=self.dim)\n\n                new_sample = np.clip(new_sample, func.bounds.lb, func.bounds.ub)\n                new_samples[i] = new_sample\n\n\n            new_fitness = np.array([func(x) for x in new_samples])\n            eval_count += self.population_size\n\n            # Update best solution\n            improved = new_fitness < fitness\n            num_improved = np.sum(improved)\n\n            for i in range(self.population_size):\n                if new_fitness[i] < self.f_opt:\n                    self.f_opt = new_fitness[i]\n                    self.x_opt = new_samples[i]\n\n            # Adapt standard deviations\n            success_rate = num_improved / self.population_size\n            if success_rate > self.success_threshold:\n                self.gaussian_std *= 1.1\n                self.cauchy_std *= 1.05 #slightly increase cauchy too\n            else:\n                self.gaussian_std *= 0.9\n                self.cauchy_std *= 0.95\n\n\n            fitness = np.minimum(fitness, new_fitness)\n            population = np.where(improved[:, None], new_samples, population) #update population\n\n            best_index = np.argmin(fitness)\n\n            if eval_count >= self.budget:\n                break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "The algorithm AdaptiveGaussianCauchySearch scored 0.507 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["65ca3433-814b-4d91-9fcf-c34d334aadd4"], "operator": null, "metadata": {"aucs": [0.27349442141529257, 0.173540276927719, 0.829280460449688, 0.19863186372760755, 0.26442580611518507, 0.8485683495489282, 0.38345356513214024, 0.6305561879029757, 0.8379321976717414, 0.23195796754040388, 0.9380916079404362, 0.9989195550055566, 0.22598017503389078, 0.28243524168914014, 0.5870921513067697, 0.32635520248356864, 0.4520897665276151, 0.9040596807168543, 0.2505427795255315, 0.4991392931366673]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "fa2a7d54-4061-4ddb-a3a3-055a3a086e82", "fitness": 0.0, "name": "SelfAdjustingDE", "description": "Self-adjusting Differential Evolution with ensemble mutation strategies and a simplified adaptation mechanism focusing on exploration and exploitation balance.", "code": "import numpy as np\n\nclass SelfAdjustingDE:\n    def __init__(self, budget=10000, dim=10, pop_size=40):\n        \"\"\"\n        Args:\n            budget (int): The maximum number of function evaluations.\n            dim (int): The dimensionality of the search space.\n            pop_size (int): The population size.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.F = 0.5  # Initialize F\n        self.CR = 0.7 # Initialize CR\n        self.mutation_strategies = [self._mutation_1, self._mutation_2]\n        self.success_F = []\n        self.success_CR = []\n        self.archive = []\n\n    def _mutation_1(self, population, i):\n        idxs = np.random.choice(len(population), 3, replace=False)\n        a, b, c = population[idxs]\n        return a + self.F * (b - c)\n\n    def _mutation_2(self, population, i):\n        idxs = np.random.choice(len(population), 5, replace=False)\n        a, b, c, d, e = population[idxs]\n        return a + self.F * (b - c) + self.F * (d - e)\n\n    def __call__(self, func):\n        # Initialize population within bounds\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        # Find the best individual in the initial population\n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index].copy()\n\n        while self.budget > 0:\n            new_population = np.zeros_like(population)\n            new_fitness = np.zeros_like(fitness)\n\n            for i in range(self.pop_size):\n                # Mutation Strategy Selection (Ensemble)\n                mutation_func = np.random.choice(self.mutation_strategies)\n                mutant = mutation_func(population, i)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Boundary handling (clip to bounds)\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Evaluation\n                f = func(trial)\n                self.budget -= 1\n                if f < fitness[i]:\n                    new_fitness[i] = f\n                    new_population[i] = trial\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = trial.copy()\n                    self.success_F.append(self.F)\n                    self.success_CR.append(self.CR)\n                    self.archive.append(population[i])\n                else:\n                    new_fitness[i] = fitness[i]\n                    new_population[i] = population[i]\n\n                if self.budget <= 0:\n                    break\n\n            population = new_population\n            fitness = new_fitness\n            \n            # Simplified Adaptation\n            if self.success_F:\n                self.F = np.mean(self.success_F)\n                self.CR = np.mean(self.success_CR)\n                self.F = np.clip(self.F, 0.1, 0.9)\n                self.CR = np.clip(self.CR, 0.1, 0.9)\n                self.success_F = []\n                self.success_CR = []\n\n            # Archive handling\n            if len(self.archive) > self.pop_size:\n                self.archive = self.archive[-self.pop_size:]\n\n            # Exploration: Inject random individuals to maintain diversity\n            num_random_injections = int(0.05 * self.pop_size) # inject 5% of the population\n            random_indices = np.random.choice(self.pop_size, num_random_injections, replace=False)\n            population[random_indices] = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(num_random_injections, self.dim))\n            fitness[random_indices] = np.array([func(x) for x in population[random_indices]])\n            self.budget -= num_random_injections\n            \n            best_index = np.argmin(fitness)\n            if fitness[best_index] < self.f_opt:\n                self.f_opt = fitness[best_index]\n                self.x_opt = population[best_index].copy()\n                \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "The algorithm SelfAdjustingDE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["0b543e23-371f-432d-a057-a2e5b307c05c"], "operator": null, "metadata": {"aucs": [0]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "d2611c97-91c2-4b10-9d00-15fc08e3af90", "fitness": "-inf", "name": "HybridPSO_CMAES", "description": "A novel hybrid algorithm combining aspects of PSO and CMA-ES, utilizing PSO for initial exploration and CMA-ES for fine-tuning around promising regions, with adaptive switching based on stagnation detection.", "code": "import numpy as np\nimport cma\n\nclass HybridPSO_CMAES:\n    def __init__(self, budget=10000, dim=10, pop_size=None, w=0.7, c1=1.5, c2=1.5, clamp_factor=0.5, stagnation_threshold=1000):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size if pop_size is not None else 20\n        self.w = w\n        self.c1 = c1\n        self.c2 = c2\n        self.clamp_factor = clamp_factor\n        self.stagnation_threshold = stagnation_threshold\n        self.swarm = None\n        self.velocities = None\n        self.personal_best_positions = None\n        self.personal_best_fitness = None\n        self.global_best_position = None\n        self.global_best_fitness = np.inf\n        self.evaluations = 0\n        self.stagnation_counter = 0\n        self.using_pso = True # Start with PSO\n\n    def initialize_swarm(self, func):\n        self.swarm = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.velocities = np.zeros((self.pop_size, self.dim))\n        self.personal_best_positions = self.swarm.copy()\n        self.personal_best_fitness = np.array([func(x) for x in self.swarm])\n        self.evaluations += self.pop_size\n\n        self.global_best_position = self.personal_best_positions[np.argmin(self.personal_best_fitness)].copy()\n        self.global_best_fitness = np.min(self.personal_best_fitness)\n    \n    def update_velocities(self, func):\n        for i in range(self.pop_size):\n            r1 = np.random.rand(self.dim)\n            r2 = np.random.rand(self.dim)\n\n            cognitive_component = self.c1 * r1 * (self.personal_best_positions[i] - self.swarm[i])\n            social_component = self.c2 * r2 * (self.global_best_position - self.swarm[i])\n\n            self.velocities[i] = self.w * self.velocities[i] + cognitive_component + social_component\n\n            # Velocity clamping\n            max_velocity = self.clamp_factor * (func.bounds.ub - func.bounds.lb)\n            self.velocities[i] = np.clip(self.velocities[i], -max_velocity, max_velocity)\n\n    def update_positions(self, func):\n        self.swarm += self.velocities\n        self.swarm = np.clip(self.swarm, func.bounds.lb, func.bounds.ub)\n\n        fitness = np.array([func(x) for x in self.swarm])\n        self.evaluations += self.pop_size\n\n        for i in range(self.pop_size):\n            if fitness[i] < self.personal_best_fitness[i]:\n                self.personal_best_fitness[i] = fitness[i]\n                self.personal_best_positions[i] = self.swarm[i].copy()\n\n                if fitness[i] < self.global_best_fitness:\n                    self.global_best_fitness = fitness[i]\n                    self.global_best_position = self.swarm[i].copy()\n    \n    def run_cmaes(self, func):\n        es = cma.purecma.CMAES(self.global_best_position, 0.5, \n                             {'bounds': [func.bounds.lb, func.bounds.ub], \n                              'verbose': -9,\n                              'popsize': self.pop_size})\n\n        while self.evaluations < self.budget and not es.stop():\n            solutions = []\n            for _ in range(es.popsize):\n                x = es.ask()\n                solutions.append((x, func(x)))\n                self.evaluations += 1\n                if self.evaluations >= self.budget:\n                    break\n\n            es.tell(solutions)\n            es.disp()\n\n            if es.result.fbest < self.global_best_fitness:\n                self.global_best_fitness = es.result.fbest\n                self.global_best_position = es.result.xbest\n    \n    def check_stagnation(self):\n        if self.using_pso:\n          # Simple stagnation check: if global best hasn't improved significantly in a while\n          # Define a tolerance for \"significant\" improvement\n          tolerance = 1e-5\n          if self.evaluations > self.pop_size:\n              previous_best_fitness = self.global_best_fitness_history[-1]\n              if abs(self.global_best_fitness - previous_best_fitness) < tolerance:\n                  self.stagnation_counter += self.pop_size\n              else:\n                  self.stagnation_counter = 0\n          else:\n              self.stagnation_counter = 0\n\n          self.global_best_fitness_history.append(self.global_best_fitness)\n          \n          if self.stagnation_counter > self.stagnation_threshold:\n              return True\n          else:\n              return False\n        else:\n            return False # CMA-ES has its own stopping criteria\n\n    def __call__(self, func):\n        self.global_best_fitness_history = []\n        self.initialize_swarm(func)\n        \n        while self.evaluations < self.budget:\n            if self.using_pso:\n                self.update_velocities(func)\n                self.update_positions(func)\n\n                if self.check_stagnation():\n                    self.using_pso = False\n                    print(\"Switching to CMA-ES\")\n                    self.run_cmaes(func)\n                    break # Exit PSO loop and let CMA-ES finish\n            else:\n                break # CMA-ES is running, let it finish\n\n        return self.global_best_fitness, self.global_best_position", "configspace": "", "generation": 4, "feedback": "In the code, line 2, in <module>, the following error occurred:\nModuleNotFoundError: No module named 'cma'\nOn line: import cma", "error": "In the code, line 2, in <module>, the following error occurred:\nModuleNotFoundError: No module named 'cma'\nOn line: import cma", "parent_ids": ["3f4dbed7-615a-44be-9e4d-93b05dfe2215"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "a1393d5d-1050-428f-8b6f-176671efc7aa", "fitness": "-inf", "name": "SelfAdaptivePSO", "description": "Self-Adaptive PSO with Velocity Mutation and a Restart Mechanism to enhance exploration and exploitation capabilities.", "code": "import numpy as np\n\nclass SelfAdaptivePSO:\n    def __init__(self, budget=10000, dim=10, pop_size=None, w_init=0.9, w_end=0.4, c1=2.0, c2=2.0, v_max_ratio=0.2, stagnation_threshold=50):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size if pop_size is not None else 20\n        self.w_init = w_init\n        self.w_end = w_end\n        self.c1 = c1\n        self.c2 = c2\n        self.v_max_ratio = v_max_ratio\n        self.stagnation_threshold = stagnation_threshold\n        self.swarm = None\n        self.velocities = None\n        self.personal_best_positions = None\n        self.personal_best_fitness = None\n        self.global_best_position = None\n        self.global_best_fitness = np.inf\n        self.evaluations = 0\n        self.stagnation_counter = 0\n        self.best_fitness_history = []\n\n    def initialize_swarm(self, func):\n        self.swarm = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        v_max = self.v_max_ratio * (func.bounds.ub - func.bounds.lb)\n        self.velocities = np.random.uniform(-v_max, v_max, size=(self.pop_size, self.dim))\n        self.personal_best_positions = self.swarm.copy()\n        self.personal_best_fitness = np.array([func(x) for x in self.swarm])\n        self.evaluations += self.pop_size\n\n        self.global_best_position = self.personal_best_positions[np.argmin(self.personal_best_fitness)].copy()\n        self.global_best_fitness = np.min(self.personal_best_fitness)\n        self.best_fitness_history.append(self.global_best_fitness)\n\n    def update_velocities(self, func, iteration):\n        w = self.w_init - (self.w_init - self.w_end) * (iteration / self.budget)\n        v_max = self.v_max_ratio * (func.bounds.ub - func.bounds.lb)\n\n\n        for i in range(self.pop_size):\n            r1 = np.random.rand(self.dim)\n            r2 = np.random.rand(self.dim)\n\n            cognitive_component = self.c1 * r1 * (self.personal_best_positions[i] - self.swarm[i])\n            social_component = self.c2 * r2 * (self.global_best_position - self.swarm[i])\n\n            self.velocities[i] = w * self.velocities[i] + cognitive_component + social_component\n            self.velocities[i] = np.clip(self.velocities[i], -v_max, v_max)\n\n            # Velocity Mutation: Randomly mutate velocities with a small probability\n            mutation_prob = 0.05\n            for j in range(self.dim):\n                if np.random.rand() < mutation_prob:\n                    self.velocities[i][j] = np.random.uniform(-v_max, v_max)\n\n    def update_positions(self, func):\n        self.swarm += self.velocities\n        self.swarm = np.clip(self.swarm, func.bounds.lb, func.bounds.ub)\n\n        fitness = np.array([func(x) for x in self.swarm])\n        self.evaluations += self.pop_size\n\n        for i in range(self.pop_size):\n            if fitness[i] < self.personal_best_fitness[i]:\n                self.personal_best_fitness[i] = fitness[i]\n                self.personal_best_positions[i] = self.swarm[i].copy()\n\n                if fitness[i] < self.global_best_fitness:\n                    self.global_best_fitness = fitness[i]\n                    self.global_best_position = self.swarm[i].copy()\n\n    def check_stagnation(self):\n        if len(self.best_fitness_history) > self.stagnation_threshold:\n            if abs(self.best_fitness_history[-1] - np.mean(self.best_fitness_history[-self.stagnation_threshold:])) < 1e-6:\n                self.stagnation_counter += 1\n            else:\n                self.stagnation_counter = 0\n\n            if self.stagnation_counter >= self.stagnation_threshold:\n                return True\n        return False\n    \n    def restart_swarm(self, func):\n        # Re-initialize a portion of the swarm (e.g., 50%)\n        num_to_restart = int(self.pop_size * 0.5)\n        indices_to_restart = np.random.choice(self.pop_size, num_to_restart, replace=False)\n        \n        v_max = self.v_max_ratio * (func.bounds.ub - func.bounds.lb)\n\n        for i in indices_to_restart:\n            self.swarm[i] = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n            self.velocities[i] = np.random.uniform(-v_max, v_max, size=self.dim)\n            \n            fitness = func(self.swarm[i])\n            self.evaluations += 1\n            \n            if fitness < self.personal_best_fitness[i]:\n                self.personal_best_fitness[i] = fitness\n                self.personal_best_positions[i] = self.swarm[i].copy()\n                \n                if fitness < self.global_best_fitness:\n                    self.global_best_fitness = fitness\n                    self.global_best_position = self.swarm[i].copy()\n\n\n    def __call__(self, func):\n        self.initialize_swarm(func)\n        iteration = 0\n        while self.evaluations < self.budget:\n            self.update_velocities(func, iteration)\n            self.update_positions(func)\n            self.best_fitness_history.append(self.global_best_fitness)\n\n            if self.check_stagnation():\n                self.restart_swarm(func)\n                self.stagnation_counter = 0\n                self.best_fitness_history = self.best_fitness_history[-self.stagnation_threshold:] #keep history relevant\n\n            iteration += 1\n\n            if self.evaluations >= self.budget:\n                break\n\n        return self.global_best_fitness, self.global_best_position", "configspace": "", "generation": 4, "feedback": "In the code, line 55, in update_velocities, the following error occurred:\nValueError: setting an array element with a sequence.\nOn line: self.velocities[i][j] = np.random.uniform(-v_max, v_max)", "error": "In the code, line 55, in update_velocities, the following error occurred:\nValueError: setting an array element with a sequence.\nOn line: self.velocities[i][j] = np.random.uniform(-v_max, v_max)", "parent_ids": ["3f4dbed7-615a-44be-9e4d-93b05dfe2215"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "4cd8ed7e-546e-4ac2-99b3-c61692579161", "fitness": 0.37135683047933743, "name": "HybridPESA", "description": "A novel hybrid algorithm combining aspects of PSO, Differential Evolution, and Simulated Annealing, focusing on diversification and intensification through adaptive parameter control and probabilistic acceptance.", "code": "import numpy as np\n\nclass HybridPESA:\n    def __init__(self, budget=10000, dim=10, pop_size=None, w=0.7, c1=1.5, c2=1.5, de_rate=0.3, sa_temp=1.0, sa_cooling_rate=0.99, clamp_factor=0.5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size if pop_size is not None else 20\n        self.w = w\n        self.c1 = c1\n        self.c2 = c2\n        self.de_rate = de_rate  # Differential Evolution mutation rate\n        self.sa_temp = sa_temp  # Simulated Annealing initial temperature\n        self.sa_cooling_rate = sa_cooling_rate\n        self.clamp_factor = clamp_factor\n        self.swarm = None\n        self.velocities = None\n        self.personal_best_positions = None\n        self.personal_best_fitness = None\n        self.global_best_position = None\n        self.global_best_fitness = np.inf\n        self.evaluations = 0\n\n    def initialize_swarm(self, func):\n        self.swarm = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.velocities = np.zeros((self.pop_size, self.dim))\n        self.personal_best_positions = self.swarm.copy()\n        self.personal_best_fitness = np.array([func(x) for x in self.swarm])\n        self.evaluations += self.pop_size\n\n        self.global_best_position = self.personal_best_positions[np.argmin(self.personal_best_fitness)].copy()\n        self.global_best_fitness = np.min(self.personal_best_fitness)\n\n    def update_velocities(self, func):\n        for i in range(self.pop_size):\n            r1 = np.random.rand(self.dim)\n            r2 = np.random.rand(self.dim)\n\n            cognitive_component = self.c1 * r1 * (self.personal_best_positions[i] - self.swarm[i])\n            social_component = self.c2 * r2 * (self.global_best_position - self.swarm[i])\n\n            self.velocities[i] = self.w * self.velocities[i] + cognitive_component + social_component\n\n            # Velocity clamping\n            max_velocity = self.clamp_factor * (func.bounds.ub - func.bounds.lb)\n            self.velocities[i] = np.clip(self.velocities[i], -max_velocity, max_velocity)\n\n    def differential_evolution_mutation(self):\n        for i in range(self.pop_size):\n            if np.random.rand() < self.de_rate:\n                indices = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = self.swarm[indices]\n                diff_vector = x_r2 - x_r3\n                mutated_vector = x_r1 + diff_vector\n                mutated_vector = np.clip(mutated_vector, -5.0, 5.0)\n                self.swarm[i] = mutated_vector\n\n    def update_positions(self, func):\n        original_swarm = self.swarm.copy() #Store original swarm before DE mutation.\n\n        self.differential_evolution_mutation()  # Apply DE mutation to increase exploration\n        self.swarm += self.velocities\n        self.swarm = np.clip(self.swarm, func.bounds.lb, func.bounds.ub)\n\n\n        fitness = np.array([func(x) for x in self.swarm])\n        self.evaluations += self.pop_size\n\n        for i in range(self.pop_size):\n            #Simulated Annealing acceptance criterion.\n            delta_fitness = fitness[i] - self.personal_best_fitness[i]\n            if delta_fitness < 0:\n                self.personal_best_fitness[i] = fitness[i]\n                self.personal_best_positions[i] = self.swarm[i].copy()\n                if fitness[i] < self.global_best_fitness:\n                    self.global_best_fitness = fitness[i]\n                    self.global_best_position = self.swarm[i].copy()\n            else:\n                acceptance_probability = np.exp(-delta_fitness / self.sa_temp)\n                if np.random.rand() < acceptance_probability:\n                    self.personal_best_fitness[i] = fitness[i]\n                    self.personal_best_positions[i] = self.swarm[i].copy()\n                    if fitness[i] < self.global_best_fitness:\n                        self.global_best_fitness = fitness[i]\n                        self.global_best_position = self.swarm[i].copy()\n                else:\n                    self.swarm[i] = original_swarm[i].copy()\n\n    def adapt_exploration(self):\n        # Adaptive inertia weight and DE rate based on temperature\n        self.w = 0.2 + 0.5 * (self.sa_temp / 1.0) #Linearly decrease w as temp decreases\n        self.de_rate = 0.1 + 0.4 * (self.sa_temp / 1.0) #Linearly decrease DE rate as temp decreases\n        self.sa_temp *= self.sa_cooling_rate  # Cool the temperature\n\n    def __call__(self, func):\n        self.initialize_swarm(func)\n\n        while self.evaluations < self.budget:\n            self.update_velocities(func)\n            self.update_positions(func)\n            self.adapt_exploration()\n            if self.evaluations >= self.budget:\n                break\n        return self.global_best_fitness, self.global_best_position", "configspace": "", "generation": 4, "feedback": "The algorithm HybridPESA scored 0.371 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["3f4dbed7-615a-44be-9e4d-93b05dfe2215"], "operator": null, "metadata": {"aucs": [0.16948363033414993, 0.3232875005541177, 0.5565512650494886, 0.20430456431947663, 0.2603873583080365, 0.7417005814275319, 0.24779082013904097, 0.5209427003145599, 0.31274028681352606, 0.23436185902359763, 0.18786502822251305, 0.9979638794924963, 0.2810370953923971, 0.2511801257599344, 0.6481040170045487, 0.385582561744525, 0.211190441368881, 0.22262792031993628, 0.1822256379669439, 0.4878093360310479]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "10cedc67-1c94-4404-a040-afc38ac2cd37", "fitness": "-inf", "name": "SelfOrganizingPSO", "description": "Self-organizing PSO with dynamic sub-swarms and adaptive resource allocation, enhancing exploration and exploitation based on sub-swarm performance.", "code": "import numpy as np\n\nclass SelfOrganizingPSO:\n    def __init__(self, budget=10000, dim=10, num_subswarms=5, w=0.7, c1=1.5, c2=1.5, initial_subswarm_size=None):\n        self.budget = budget\n        self.dim = dim\n        self.num_subswarms = num_subswarms\n        self.w = w\n        self.c1 = c1\n        self.c2 = c2\n        self.initial_subswarm_size = initial_subswarm_size if initial_subswarm_size is not None else int(20 / num_subswarms)\n        self.subswarms = []\n        self.evaluations = 0\n\n    def initialize_subswarms(self, func):\n        for i in range(self.num_subswarms):\n            size = self.initial_subswarm_size\n            swarm = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(size, self.dim))\n            velocities = np.zeros((size, self.dim))\n            personal_best_positions = swarm.copy()\n            personal_best_fitness = np.array([func(x) for x in swarm])\n            self.evaluations += size\n            global_best_index = np.argmin(personal_best_fitness)\n            global_best_position = personal_best_positions[global_best_index].copy()\n            global_best_fitness = personal_best_fitness[global_best_index]\n            self.subswarms.append({\n                'swarm': swarm,\n                'velocities': velocities,\n                'personal_best_positions': personal_best_positions,\n                'personal_best_fitness': personal_best_fitness,\n                'global_best_position': global_best_position,\n                'global_best_fitness': global_best_fitness,\n                'size': size,\n                'success_rate': 0.0,\n                'evaluations': 0\n            })\n\n    def update_velocities(self, subswarm):\n        for i in range(subswarm['size']):\n            r1 = np.random.rand(self.dim)\n            r2 = np.random.rand(self.dim)\n\n            cognitive_component = self.c1 * r1 * (subswarm['personal_best_positions'][i] - subswarm['swarm'][i])\n            social_component = self.c2 * r2 * (subswarm['global_best_position'] - subswarm['swarm'][i])\n\n            subswarm['velocities'][i] = self.w * subswarm['velocities'][i] + cognitive_component + social_component\n\n            # Velocity clamping\n            max_velocity = 0.5 * (func.bounds.ub - func.bounds.lb)\n            subswarm['velocities'][i] = np.clip(subswarm['velocities'][i], -max_velocity, max_velocity)\n\n    def update_positions(self, func, subswarm):\n        subswarm['swarm'] += subswarm['velocities']\n        subswarm['swarm'] = np.clip(subswarm['swarm'], func.bounds.lb, func.bounds.ub)\n\n        fitness = np.array([func(x) for x in subswarm['swarm']])\n        self.evaluations += subswarm['size']\n        subswarm['evaluations'] += subswarm['size']\n\n        improved = 0\n        for i in range(subswarm['size']):\n            if fitness[i] < subswarm['personal_best_fitness'][i]:\n                subswarm['personal_best_fitness'][i] = fitness[i]\n                subswarm['personal_best_positions'][i] = subswarm['swarm'][i].copy()\n                improved += 1\n\n                if fitness[i] < subswarm['global_best_fitness']:\n                    subswarm['global_best_fitness'] = fitness[i]\n                    subswarm['global_best_position'] = subswarm['swarm'][i].copy()\n\n        subswarm['success_rate'] = 0.9 * subswarm['success_rate'] + 0.1 * (improved / subswarm['size'])\n\n\n    def adapt_resource_allocation(self):\n        # Sort subswarms by success rate\n        sorted_subswarms = sorted(self.subswarms, key=lambda x: x['success_rate'], reverse=True)\n\n        # Allocate more particles to successful subswarms\n        total_size = sum(s['size'] for s in self.subswarms)\n        target_size = 20 # Target population size.\n        for i, subswarm in enumerate(sorted_subswarms):\n            # Proportional allocation based on success rate.  More successful swarms get more particles.\n            new_size = int(target_size * (subswarm['success_rate'] / sum(s['success_rate'] for s in self.subswarms) if sum(s['success_rate'] for s in self.subswarms) > 0 else target_size / self.num_subswarms) ) + 1\n            new_size = min(new_size, int(self.budget / (self.num_subswarms*2)))  #Prevent a swarm from hogging resources.\n            if new_size != subswarm['size']:\n                delta = new_size - subswarm['size']\n                if delta > 0: # Add new particles\n                    new_particles = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(delta, self.dim))\n                    new_velocities = np.zeros((delta, self.dim))\n                    new_personal_best_positions = new_particles.copy()\n                    new_personal_best_fitness = np.array([func(x) for x in new_particles])\n                    self.evaluations += delta\n\n                    subswarm['swarm'] = np.vstack((subswarm['swarm'], new_particles))\n                    subswarm['velocities'] = np.vstack((subswarm['velocities'], new_velocities))\n                    subswarm['personal_best_positions'] = np.vstack((subswarm['personal_best_positions'], new_personal_best_positions))\n                    subswarm['personal_best_fitness'] = np.concatenate((subswarm['personal_best_fitness'], new_personal_best_fitness))\n                    subswarm['size'] = new_size\n\n                    # Update the global best.\n                    for i in range(delta):\n                        if new_personal_best_fitness[i] < subswarm['global_best_fitness']:\n                            subswarm['global_best_fitness'] = new_personal_best_fitness[i]\n                            subswarm['global_best_position'] = new_personal_best_positions[i].copy()\n\n                else: # Remove particles (least fit ones)\n                    indices_to_remove = np.argsort(subswarm['personal_best_fitness'])[-delta:]\n                    subswarm['swarm'] = np.delete(subswarm['swarm'], indices_to_remove, axis=0)\n                    subswarm['velocities'] = np.delete(subswarm['velocities'], indices_to_remove, axis=0)\n                    subswarm['personal_best_positions'] = np.delete(subswarm['personal_best_positions'], indices_to_remove, axis=0)\n                    subswarm['personal_best_fitness'] = np.delete(subswarm['personal_best_fitness'], indices_to_remove)\n                    subswarm['size'] = new_size\n\n\n    def get_global_best(self):\n        best_fitness = np.inf\n        best_position = None\n        for subswarm in self.subswarms:\n            if subswarm['global_best_fitness'] < best_fitness:\n                best_fitness = subswarm['global_best_fitness']\n                best_position = subswarm['global_best_position']\n        return best_fitness, best_position\n\n    def __call__(self, func):\n        self.initialize_subswarms(func)\n\n        while self.evaluations < self.budget:\n            for subswarm in self.subswarms:\n                self.update_velocities(subswarm)\n                self.update_positions(func, subswarm)\n            self.adapt_resource_allocation()\n\n            if self.evaluations >= self.budget:\n                break\n\n        return self.get_global_best()", "configspace": "", "generation": 5, "feedback": "In the code, line 49, in update_velocities, the following error occurred:\nNameError: name 'func' is not defined\nOn line: max_velocity = 0.5 * (func.bounds.ub - func.bounds.lb)", "error": "In the code, line 49, in update_velocities, the following error occurred:\nNameError: name 'func' is not defined\nOn line: max_velocity = 0.5 * (func.bounds.ub - func.bounds.lb)", "parent_ids": ["3f4dbed7-615a-44be-9e4d-93b05dfe2215"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "9b2ba85d-5900-472a-92c9-295427f2c7ea", "fitness": 0.3164494762247706, "name": "AdaptiveGaussianSearch", "description": "Adaptive Gaussian Search with dynamic step size adjustment and a memory of past successful steps to guide future exploration.", "code": "import numpy as np\n\nclass AdaptiveGaussianSearch:\n    def __init__(self, budget=10000, dim=10, initial_std=0.5, success_threshold=0.2, memory_size=10):\n        self.budget = budget\n        self.dim = dim\n        self.initial_std = initial_std\n        self.success_threshold = success_threshold\n        self.memory_size = memory_size\n        self.memory = []  # Store successful steps\n        self.std = self.initial_std\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        self.f_opt = func(self.x_opt)\n        eval_count = 1\n        \n        while eval_count < self.budget:\n            # Generate new sample around the best solution\n            noise = np.random.normal(loc=0, scale=self.std, size=self.dim)\n\n            # Add memory influence if available\n            if self.memory:\n                memory_influence = np.mean(self.memory, axis=0)\n                noise += memory_influence * 0.5 # Reduced influence\n            \n            x_new = self.x_opt + noise\n            x_new = np.clip(x_new, func.bounds.lb, func.bounds.ub)  # Clip to bounds\n\n            f_new = func(x_new)\n            eval_count += 1\n\n            if f_new < self.f_opt:\n                # Step was successful:\n                step = x_new - self.x_opt\n                self.f_opt = f_new\n                self.x_opt = x_new\n\n                # Update memory\n                self.memory.append(step)\n                if len(self.memory) > self.memory_size:\n                    self.memory.pop(0)\n\n                #Increase step size (exploration)\n                self.std *= 1.1\n            else:\n                #Reduce step size (exploitation)\n                self.std *= 0.9\n            \n            self.std = np.clip(self.std, self.initial_std/10, 1) # Clamp to reasonable values\n\n            if eval_count >= self.budget:\n                break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "The algorithm AdaptiveGaussianSearch scored 0.316 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["65ca3433-814b-4d91-9fcf-c34d334aadd4"], "operator": null, "metadata": {"aucs": [0.11485740663790989, 0.17807488354648127, 0.19932666046854153, 0.18520241295772943, 0.24969405678011158, 0.10181603606764733, 0.19200332859543012, 0.25818169050784323, 0.1896001360703814, 0.11781606696567992, 0.8587875449681546, 0.999280688235453, 0.26787007737706603, 0.20821900162645857, 0.9504812565231706, 0.28371781446827493, 0.15564761049820808, 0.28485145837538994, 0.09636014132790782, 0.43720125249757325]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "3d4f373a-1b2f-41c7-90b5-6c403b832f47", "fitness": 0.0, "name": "EnhancedAdaptivePSO", "description": "An enhanced PSO with adaptive parameters, a restart mechanism based on stagnation detection, and a local search phase using a shrinking hypersphere around the best particle.", "code": "import numpy as np\n\nclass EnhancedAdaptivePSO:\n    def __init__(self, budget=10000, dim=10, pop_size=None, w_init=0.9, w_end=0.4, c1=2.0, c2=2.0, clamp_factor=0.5, stagnation_limit=50):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size if pop_size is not None else 30 # Increased population size\n        self.w_init = w_init\n        self.w_end = w_end\n        self.c1 = c1\n        self.c2 = c2\n        self.clamp_factor = clamp_factor\n        self.stagnation_limit = stagnation_limit\n        self.swarm = None\n        self.velocities = None\n        self.personal_best_positions = None\n        self.personal_best_fitness = None\n        self.global_best_position = None\n        self.global_best_fitness = np.inf\n        self.evaluations = 0\n        self.stagnation_counter = 0\n        self.best_fitness_history = []\n        self.radius = 0.1  # Initial radius for local search\n\n    def initialize_swarm(self, func):\n        self.swarm = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.velocities = np.zeros((self.pop_size, self.dim))\n        self.personal_best_positions = self.swarm.copy()\n        self.personal_best_fitness = np.array([func(x) for x in self.swarm])\n        self.evaluations += self.pop_size\n\n        self.global_best_position = self.personal_best_positions[np.argmin(self.personal_best_fitness)].copy()\n        self.global_best_fitness = np.min(self.personal_best_fitness)\n        self.best_fitness_history.append(self.global_best_fitness)\n\n    def update_velocities(self, func, iteration):\n        w = self.w_init - (self.w_init - self.w_end) * (iteration / self.budget)  # Linearly decreasing inertia weight\n\n        for i in range(self.pop_size):\n            r1 = np.random.rand(self.dim)\n            r2 = np.random.rand(self.dim)\n\n            cognitive_component = self.c1 * r1 * (self.personal_best_positions[i] - self.swarm[i])\n            social_component = self.c2 * r2 * (self.global_best_position - self.swarm[i])\n\n            self.velocities[i] = w * self.velocities[i] + cognitive_component + social_component\n\n            # Velocity clamping\n            max_velocity = self.clamp_factor * (func.bounds.ub - func.bounds.lb)\n            self.velocities[i] = np.clip(self.velocities[i], -max_velocity, max_velocity)\n\n    def update_positions(self, func):\n        self.swarm += self.velocities\n        self.swarm = np.clip(self.swarm, func.bounds.lb, func.bounds.ub)\n\n        fitness = np.array([func(x) for x in self.swarm])\n        self.evaluations += self.pop_size\n\n        for i in range(self.pop_size):\n            if fitness[i] < self.personal_best_fitness[i]:\n                self.personal_best_fitness[i] = fitness[i]\n                self.personal_best_positions[i] = self.swarm[i].copy()\n\n                if fitness[i] < self.global_best_fitness:\n                    self.global_best_fitness = fitness[i]\n                    self.global_best_position = self.swarm[i].copy()\n\n    def check_stagnation(self):\n        if len(self.best_fitness_history) > self.stagnation_limit:\n            if np.abs(self.best_fitness_history[-1] - np.mean(self.best_fitness_history[-self.stagnation_limit:])) < 1e-6:\n                self.stagnation_counter += 1\n            else:\n                self.stagnation_counter = 0\n        else:\n            self.stagnation_counter = 0\n\n    def restart_swarm(self, func):\n        # Option 1: Re-initialize the entire swarm\n        self.swarm = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.velocities = np.zeros((self.pop_size, self.dim))\n        self.personal_best_positions = self.swarm.copy()\n        self.personal_best_fitness = np.array([func(x) for x in self.swarm])\n        self.evaluations += self.pop_size\n\n        # OR Option 2: Keep the best particle and re-initialize the rest\n        # for i in range(1, self.pop_size):\n        #     self.swarm[i] = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        #     self.velocities[i] = np.zeros(self.dim)\n        #     fitness = func(self.swarm[i])\n        #     self.evaluations +=1\n        #     self.personal_best_positions[i] = self.swarm[i].copy()\n        #     self.personal_best_fitness[i] = fitness\n\n        # Update global best\n        best_index = np.argmin(self.personal_best_fitness)\n        if self.personal_best_fitness[best_index] < self.global_best_fitness:\n            self.global_best_fitness = self.personal_best_fitness[best_index]\n            self.global_best_position = self.personal_best_positions[best_index].copy()\n        self.stagnation_counter = 0\n        self.best_fitness_history.append(self.global_best_fitness)\n\n\n    def local_search(self, func, num_samples=20):\n        # Sample points within a shrinking hypersphere around the global best\n        for _ in range(num_samples):\n            sample = self.global_best_position + np.random.normal(0, self.radius, self.dim)\n            sample = np.clip(sample, func.bounds.lb, func.bounds.ub)  # Keep within bounds\n\n            fitness = func(sample)\n            self.evaluations += 1\n\n            if fitness < self.global_best_fitness:\n                self.global_best_fitness = fitness\n                self.global_best_position = sample.copy()\n\n        self.radius *= 0.95  # Shrink the radius after each local search\n\n    def __call__(self, func):\n        self.initialize_swarm(func)\n        iteration = 0\n        while self.evaluations < self.budget:\n            self.update_velocities(func, iteration)\n            self.update_positions(func)\n            self.best_fitness_history.append(self.global_best_fitness)\n            self.check_stagnation()\n\n            if self.stagnation_counter > self.stagnation_limit // 2: #Slightly less strict stagnation before local search.\n                self.local_search(func) #Perform local search before restart\n\n            if self.stagnation_counter > self.stagnation_limit:\n                self.restart_swarm(func) #Restart swarm if stagnation persists\n                self.radius = 0.1   #Reset local search radius\n                self.stagnation_counter = 0\n\n            if self.evaluations >= self.budget:\n                break\n            iteration += 1\n\n        return self.global_best_fitness, self.global_best_position", "configspace": "", "generation": 5, "feedback": "The algorithm EnhancedAdaptivePSO scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["3f4dbed7-615a-44be-9e4d-93b05dfe2215"], "operator": null, "metadata": {"aucs": [0]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "7be430de-0b05-42d2-82ea-481ef5779881", "fitness": "-inf", "name": "AdaptiveDifferentialEvolution", "description": "Simplified Adaptive Differential Evolution with self-adaptive population size, F/CR adaptation based on successful mutations, and rank-based parent selection.", "code": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget=10000, dim=10, pop_size_base=40, F_base=0.5, CR_base=0.7, learning_rate=0.1, success_history_size=5):\n        \"\"\"\n        Args:\n            budget (int): The maximum number of function evaluations.\n            dim (int): The dimensionality of the search space.\n            pop_size_base (int): The base population size.\n            F_base (float): Base mutation factor.\n            CR_base (float): Base crossover rate.\n            learning_rate (float): The learning rate for adaptive CR/F.\n            success_history_size (int): Size of the success history for adapting CR/F.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.pop_size_base = pop_size_base\n        self.pop_size = pop_size_base\n        self.F_base = F_base\n        self.CR_base = CR_base\n        self.learning_rate = learning_rate\n        self.success_history_size = success_history_size\n        self.success_F = np.ones(self.success_history_size) * self.F_base\n        self.success_CR = np.ones(self.success_history_size) * self.CR_base\n        self.success_idx = 0\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.lb = None\n        self.ub = None\n\n    def __call__(self, func):\n        self.lb = func.bounds.lb\n        self.ub = func.bounds.ub\n        \n        # Initialize population within bounds\n        population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size # Update remaining budget\n\n        # Find the best individual in the initial population\n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index].copy()\n\n        generation = 0\n        while self.budget > 0:\n            generation += 1\n            # Rank population\n            ranked_indices = np.argsort(fitness)\n            \n            # Adjust population size (simplified)\n            if generation % 10 == 0:\n                if np.std(fitness) < 1e-3:\n                    self.pop_size = max(10, int(self.pop_size_base * 0.5))  # Reduce pop size if converging\n                else:\n                    self.pop_size = min(self.pop_size_base, int(self.pop_size_base * 1.2))  # Increase pop size if diverse\n                \n                if self.pop_size != population.shape[0]:\n                    population = population[ranked_indices[:self.pop_size]]  # Truncate population\n                    fitness = fitness[ranked_indices[:self.pop_size]]\n\n            for i in range(self.pop_size):\n                # Mutation: Rank-based selection of parents\n                pbest_idx = ranked_indices[np.random.randint(0, max(1, int(0.2 * self.pop_size)))] # Select best 20%\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b = np.random.choice(idxs, 2, replace=False)\n                \n                # Adaptive F and CR\n                F = np.random.normal(self.F_base, 0.1)\n                F = np.clip(F, 0.1, 0.9)\n                CR = np.random.normal(self.CR_base, 0.1)\n                CR = np.clip(CR, 0.1, 0.9)\n                \n                mutant = population[i] + F * (population[pbest_idx] - population[a]) + F * (population[b] - population[i])\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Boundary handling (clip to bounds)\n                trial = np.clip(trial, self.lb, self.ub)\n\n                # Evaluation\n                f = func(trial)\n                self.budget -= 1\n                \n                if f < fitness[i]:\n                    fitness[i] = f\n                    population[i] = trial\n                    \n                    # Update global best\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = trial.copy()\n\n                    # Update success history for F and CR\n                    self.success_F[self.success_idx] = F\n                    self.success_CR[self.success_idx] = CR\n                    self.success_idx = (self.success_idx + 1) % self.success_history_size\n\n                    # Adjust base values based on recent success\n                    self.F_base = np.mean(self.success_F)\n                    self.CR_base = np.mean(self.success_CR)\n                    self.F_base = np.clip(self.F_base, 0.1, 0.9)\n                    self.CR_base = np.clip(self.CR_base, 0.1, 0.9)\n\n                if self.budget <= 0:\n                    break\n            \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "In the code, line 74, in __call__, the following error occurred:\nIndexError: index 29 is out of bounds for axis 0 with size 20\nOn line: mutant = population[i] + F * (population[pbest_idx] - population[a]) + F * (population[b] - population[i])", "error": "In the code, line 74, in __call__, the following error occurred:\nIndexError: index 29 is out of bounds for axis 0 with size 20\nOn line: mutant = population[i] + F * (population[pbest_idx] - population[a]) + F * (population[b] - population[i])", "parent_ids": ["ca08a41a-418b-4ee4-ba18-aaa3d01359f5"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "75818724-7b64-4563-9fc7-cd12b05e88a3", "fitness": 0.49763188652196133, "name": "AdaptivePSO", "description": "Adaptive PSO with dynamically adjusted cognitive and social coefficients based on individual and swarm performance.", "code": "import numpy as np\n\nclass AdaptivePSO:\n    def __init__(self, budget=10000, dim=10, pop_size=None, w=0.7, c1_initial=1.5, c2_initial=1.5, clamp_factor=0.5, stagnation_threshold=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size if pop_size is not None else 20\n        self.w = w  # Inertia weight\n        self.c1_initial = c1_initial  # Initial cognitive coefficient\n        self.c2_initial = c2_initial  # Initial social coefficient\n        self.c1 = c1_initial\n        self.c2 = c2_initial\n        self.clamp_factor = clamp_factor  # Factor for clamping velocities\n        self.swarm = None\n        self.velocities = None\n        self.personal_best_positions = None\n        self.personal_best_fitness = None\n        self.global_best_position = None\n        self.global_best_fitness = np.inf\n        self.evaluations = 0\n        self.stagnation_counter = 0\n        self.previous_best_fitness = np.inf\n        self.stagnation_threshold = stagnation_threshold  #Number of iterations before stagnation detected\n\n    def initialize_swarm(self, func):\n        self.swarm = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.velocities = np.zeros((self.pop_size, self.dim))\n        self.personal_best_positions = self.swarm.copy()\n        self.personal_best_fitness = np.array([func(x) for x in self.swarm])\n        self.evaluations += self.pop_size\n\n        self.global_best_position = self.personal_best_positions[np.argmin(self.personal_best_fitness)].copy()\n        self.global_best_fitness = np.min(self.personal_best_fitness)\n\n    def update_velocities(self, func):\n        for i in range(self.pop_size):\n            r1 = np.random.rand(self.dim)\n            r2 = np.random.rand(self.dim)\n\n            cognitive_component = self.c1 * r1 * (self.personal_best_positions[i] - self.swarm[i])\n            social_component = self.c2 * r2 * (self.global_best_position - self.swarm[i])\n\n            self.velocities[i] = self.w * self.velocities[i] + cognitive_component + social_component\n\n            # Velocity clamping\n            max_velocity = self.clamp_factor * (func.bounds.ub - func.bounds.lb)\n            self.velocities[i] = np.clip(self.velocities[i], -max_velocity, max_velocity)\n\n    def update_positions(self, func):\n        self.swarm += self.velocities\n        self.swarm = np.clip(self.swarm, func.bounds.lb, func.bounds.ub)\n\n        fitness = np.array([func(x) for x in self.swarm])\n        self.evaluations += self.pop_size\n\n        for i in range(self.pop_size):\n            if fitness[i] < self.personal_best_fitness[i]:\n                self.personal_best_fitness[i] = fitness[i]\n                self.personal_best_positions[i] = self.swarm[i].copy()\n\n                if fitness[i] < self.global_best_fitness:\n                    self.global_best_fitness = fitness[i]\n                    self.global_best_position = self.swarm[i].copy()\n\n    def adapt_coefficients(self):\n        # Dynamically adjust c1 and c2 based on swarm and individual performance\n        if self.global_best_fitness < self.previous_best_fitness:\n            self.stagnation_counter = 0\n            self.previous_best_fitness = self.global_best_fitness\n            # If improvement, favor social component to converge faster\n            self.c1 = max(0.5, self.c1 * 0.95)\n            self.c2 = min(2.5, self.c2 * 1.05)\n        else:\n            self.stagnation_counter += 1\n            # If stagnation, favor cognitive component to explore more\n            self.c1 = min(2.5, self.c1 * 1.05)\n            self.c2 = max(0.5, self.c2 * 0.95)\n\n        #Widen search in case of stagnation\n        if self.stagnation_counter > self.stagnation_threshold:\n            self.w = min(0.9, self.w * 1.05)\n        else:\n            self.w = max(0.4, self.w * 0.95)\n\n\n    def __call__(self, func):\n        self.initialize_swarm(func)\n\n        while self.evaluations < self.budget:\n            self.update_velocities(func)\n            self.update_positions(func)\n            self.adapt_coefficients()  # Adapt coefficients based on the swarm state\n            if self.evaluations >= self.budget:\n                break\n        return self.global_best_fitness, self.global_best_position", "configspace": "", "generation": 5, "feedback": "The algorithm AdaptivePSO scored 0.498 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["3f4dbed7-615a-44be-9e4d-93b05dfe2215"], "operator": null, "metadata": {"aucs": [0.19935566808647398, 0.19032960814775401, 0.9072502227649354, 0.9452720748762256, 0.2667323987207707, 0.2547413197773879, 0.3182170816225144, 0.45383151072425265, 0.7217568363610731, 0.18462698327840543, 0.9460119484299356, 0.9968645892642922, 0.24653821229559236, 0.2907496527369796, 0.5880548473918121, 0.9273943398763982, 0.41725105856308586, 0.382030077035283, 0.23898512180337317, 0.47664417868268205]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "a48e5255-9ec5-4747-b092-a871508c6179", "fitness": "-inf", "name": "AdaptiveConstrictionPSO", "description": "A PSO variant with a dynamically adjusted constriction factor to control convergence speed and prevent premature convergence, alongside an adaptive mutation operator applied to the global best particle.", "code": "import numpy as np\n\nclass AdaptiveConstrictionPSO:\n    def __init__(self, budget=10000, dim=10, pop_size=None, w_start=0.9, w_end=0.4, c1=2.0, c2=2.0, constriction_factor=0.72984):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size if pop_size is not None else 20\n        self.w_start = w_start  # Initial inertia weight\n        self.w_end = w_end  # Final inertia weight\n        self.c1 = c1  # Cognitive coefficient\n        self.c2 = c2  # Social coefficient\n        self.constriction_factor = constriction_factor # Constriction factor\n        self.swarm = None\n        self.velocities = None\n        self.personal_best_positions = None\n        self.personal_best_fitness = None\n        self.global_best_position = None\n        self.global_best_fitness = np.inf\n        self.evaluations = 0\n        self.iteration = 0\n\n    def initialize_swarm(self, func):\n        self.swarm = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.velocities = np.random.uniform(-0.1 * (func.bounds.ub - func.bounds.lb), 0.1 * (func.bounds.ub - func.bounds.lb), size=(self.pop_size, self.dim))\n        self.personal_best_positions = self.swarm.copy()\n        self.personal_best_fitness = np.array([func(x) for x in self.swarm])\n        self.evaluations += self.pop_size\n\n        self.global_best_position = self.personal_best_positions[np.argmin(self.personal_best_fitness)].copy()\n        self.global_best_fitness = np.min(self.personal_best_fitness)\n\n    def update_velocities(self):\n        w = self.w_start - (self.w_start - self.w_end) * (self.iteration / self.budget)\n        for i in range(self.pop_size):\n            r1 = np.random.rand(self.dim)\n            r2 = np.random.rand(self.dim)\n\n            cognitive_component = self.c1 * r1 * (self.personal_best_positions[i] - self.swarm[i])\n            social_component = self.c2 * r2 * (self.global_best_position - self.swarm[i])\n\n            self.velocities[i] = self.constriction_factor * (w * self.velocities[i] + cognitive_component + social_component)\n\n    def update_positions(self, func):\n        self.swarm += self.velocities\n        self.swarm = np.clip(self.swarm, func.bounds.lb, func.bounds.ub)\n\n        fitness = np.array([func(x) for x in self.swarm])\n        self.evaluations += self.pop_size\n\n        for i in range(self.pop_size):\n            if fitness[i] < self.personal_best_fitness[i]:\n                self.personal_best_fitness[i] = fitness[i]\n                self.personal_best_positions[i] = self.swarm[i].copy()\n\n                if fitness[i] < self.global_best_fitness:\n                    self.global_best_fitness = fitness[i]\n                    self.global_best_position = self.swarm[i].copy()\n\n    def adapt_exploration(self):\n        # Mutate the global best position with a small probability\n        mutation_rate = 0.05\n        if np.random.rand() < mutation_rate:\n            mutation = np.random.normal(0, 0.01 * (func.bounds.ub - func.bounds.lb), size=self.dim) # Reduced mutation strength\n            self.global_best_position += mutation\n            self.global_best_position = np.clip(self.global_best_position, func.bounds.lb, func.bounds.ub)\n\n    def __call__(self, func):\n        self.initialize_swarm(func)\n\n        while self.evaluations < self.budget:\n            self.update_velocities()\n            self.update_positions(func)\n            self.adapt_exploration()\n            self.iteration += self.pop_size  # Increment iteration by pop_size, not 1\n\n            # Re-evaluate the global best fitness after mutation\n            if self.evaluations < self.budget:\n                fitness = func(self.global_best_position)\n                self.evaluations += 1\n                if fitness < self.global_best_fitness:\n                    self.global_best_fitness = fitness\n            else:\n                break\n        return self.global_best_fitness, self.global_best_position", "configspace": "", "generation": 5, "feedback": "In the code, line 63, in adapt_exploration, the following error occurred:\nNameError: name 'func' is not defined\nOn line: mutation = np.random.normal(0, 0.01 * (func.bounds.ub - func.bounds.lb), size=self.dim) # Reduced mutation strength", "error": "In the code, line 63, in adapt_exploration, the following error occurred:\nNameError: name 'func' is not defined\nOn line: mutation = np.random.normal(0, 0.01 * (func.bounds.ub - func.bounds.lb), size=self.dim) # Reduced mutation strength", "parent_ids": ["3f4dbed7-615a-44be-9e4d-93b05dfe2215"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "503b8c13-659a-4deb-993e-ee2ffe71f92c", "fitness": 0.0, "name": "AdaptiveGaussianSearch", "description": "Adaptive Gaussian Search with dynamic population size adjustment and a Cauchy mutation component for enhanced exploration, adjusting parameters based on success rate and stagnation detection.", "code": "import numpy as np\n\nclass AdaptiveGaussianSearch:\n    def __init__(self, budget=10000, dim=10, initial_population_size=20, initial_std=0.5, success_threshold=0.2, stagnation_threshold=10):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = initial_population_size\n        self.population_size = initial_population_size  # Dynamic population size\n        self.initial_std = initial_std\n        self.std = initial_std  #Dynamic std\n        self.success_threshold = success_threshold\n        self.stagnation_threshold = stagnation_threshold\n        self.stagnation_counter = 0\n        self.best_fitness_history = []\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        eval_count = 0\n\n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.population_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        eval_count += self.population_size\n        \n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index]\n        self.best_fitness_history.append(self.f_opt)\n\n        while eval_count < self.budget:\n            # Generate new samples around the best solution (Gaussian)\n            new_samples_gaussian = np.random.normal(loc=self.x_opt, scale=self.std, size=(self.population_size, self.dim))\n            new_samples_gaussian = np.clip(new_samples_gaussian, func.bounds.lb, func.bounds.ub)  # Clip to bounds\n            \n            # Generate new samples with Cauchy mutation for exploration\n            new_samples_cauchy = self.x_opt + self.std * np.random.standard_cauchy(size=(self.population_size, self.dim))\n            new_samples_cauchy = np.clip(new_samples_cauchy, func.bounds.lb, func.bounds.ub)\n\n            # Combine Gaussian and Cauchy samples (e.g., 50/50)\n            mask = np.random.rand(self.population_size, self.dim) < 0.5\n            new_samples = np.where(mask, new_samples_gaussian, new_samples_cauchy)\n\n\n            new_fitness = np.array([func(x) for x in new_samples])\n            eval_count += self.population_size\n\n            # Update best solution\n            for i in range(self.population_size):\n                if new_fitness[i] < self.f_opt:\n                    self.f_opt = new_fitness[i]\n                    self.x_opt = new_samples[i]\n                    self.stagnation_counter = 0 #reset stagnation\n            \n            self.best_fitness_history.append(self.f_opt)\n\n            # Adapt the standard deviation based on success rate\n            success_rate = np.sum(new_fitness < fitness) / self.population_size\n            if success_rate > self.success_threshold:\n                self.std *= 1.1  # Increase exploration if successful\n            else:\n                self.std *= 0.9  # Decrease exploration if unsuccessful\n            \n            fitness = np.minimum(fitness, new_fitness) #replace only if better\n            best_index = np.argmin(fitness)\n            \n            #Stagnation Detection and Population Size Adjustment\n            self.stagnation_counter +=1\n            if self.stagnation_counter > self.stagnation_threshold:\n                #if stagnant, increase population to introduce more diversity and reset std\n                self.population_size = min(2 * self.population_size, self.budget // 10) #avoid exploding pop size\n                population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.population_size, self.dim))\n                fitness = np.array([func(x) for x in population]) #re-evaluate fitness\n                eval_count += self.population_size - len(fitness) # account for the increase in population\n                best_index = np.argmin(fitness)\n                if fitness[best_index] < self.f_opt:\n                  self.f_opt = fitness[best_index]\n                  self.x_opt = population[best_index]\n                self.std = self.initial_std # reset std\n                self.stagnation_counter = 0 #reset stagnation counter\n\n\n            if eval_count >= self.budget:\n                break\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "The algorithm AdaptiveGaussianSearch scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["65ca3433-814b-4d91-9fcf-c34d334aadd4"], "operator": null, "metadata": {"aucs": [0]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "274d5ead-53c4-4052-91dd-ef9cf9c1177b", "fitness": 0.4401488862616617, "name": "EnhancedAdaptiveClampedPSO", "description": "An enhanced Adaptive Clamped PSO with a Cauchy mutation operator to promote exploration and a more sophisticated adaptation of inertia weight based on success history.", "code": "import numpy as np\n\nclass EnhancedAdaptiveClampedPSO:\n    def __init__(self, budget=10000, dim=10, pop_size=None, w_init=0.7, c1=1.5, c2=1.5, clamp_factor=0.5, cauchy_scale=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size if pop_size is not None else 20\n        self.w_init = w_init # Initial inertia weight\n        self.w = w_init\n        self.c1 = c1\n        self.c2 = c2\n        self.clamp_factor = clamp_factor\n        self.cauchy_scale = cauchy_scale # Scale parameter for Cauchy mutation\n        self.swarm = None\n        self.velocities = None\n        self.personal_best_positions = None\n        self.personal_best_fitness = None\n        self.global_best_position = None\n        self.global_best_fitness = np.inf\n        self.evaluations = 0\n        self.success_history = [] # Stores recent success (improvement) rates\n\n    def initialize_swarm(self, func):\n        self.swarm = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.velocities = np.zeros((self.pop_size, self.dim))\n        self.personal_best_positions = self.swarm.copy()\n        self.personal_best_fitness = np.array([func(x) for x in self.swarm])\n        self.evaluations += self.pop_size\n\n        self.global_best_position = self.personal_best_positions[np.argmin(self.personal_best_fitness)].copy()\n        self.global_best_fitness = np.min(self.personal_best_fitness)\n\n    def update_velocities(self, func):\n        for i in range(self.pop_size):\n            r1 = np.random.rand(self.dim)\n            r2 = np.random.rand(self.dim)\n\n            cognitive_component = self.c1 * r1 * (self.personal_best_positions[i] - self.swarm[i])\n            social_component = self.c2 * r2 * (self.global_best_position - self.swarm[i])\n\n            self.velocities[i] = self.w * self.velocities[i] + cognitive_component + social_component\n\n            # Velocity clamping\n            max_velocity = self.clamp_factor * (func.bounds.ub - func.bounds.lb)\n            self.velocities[i] = np.clip(self.velocities[i], -max_velocity, max_velocity)\n\n    def update_positions(self, func):\n        self.swarm += self.velocities\n\n        # Cauchy mutation for exploration\n        for i in range(self.pop_size):\n             if np.random.rand() < 0.1:  # Apply Cauchy mutation with a probability of 0.1\n                 mutation = self.cauchy_scale * np.random.standard_cauchy(size=self.dim)\n                 self.swarm[i] += mutation\n                 \n        self.swarm = np.clip(self.swarm, func.bounds.lb, func.bounds.ub)\n\n        fitness = np.array([func(x) for x in self.swarm])\n        self.evaluations += self.pop_size\n\n        improved_count = 0\n        for i in range(self.pop_size):\n            if fitness[i] < self.personal_best_fitness[i]:\n                improved_count += 1\n                self.personal_best_fitness[i] = fitness[i]\n                self.personal_best_positions[i] = self.swarm[i].copy()\n\n                if fitness[i] < self.global_best_fitness:\n                    self.global_best_fitness = fitness[i]\n                    self.global_best_position = self.swarm[i].copy()\n\n        # Update success history\n        self.success_history.append(improved_count / self.pop_size)\n        if len(self.success_history) > 10:\n            self.success_history.pop(0) # Keep a window of 10 recent success rates\n\n    def adapt_exploration(self):\n        # Adaptive inertia weight based on success history\n        if self.success_history:\n            avg_success = np.mean(self.success_history)\n            # Adjust w based on success rate. If success rate is high, reduce w for exploitation, else increase for exploration.\n            self.w = np.clip(self.w_init + 0.2 * (0.5 - avg_success), 0.2, 0.9)\n        else:\n            self.w = self.w_init # Keep initial value if no history yet.\n\n    def __call__(self, func):\n        self.initialize_swarm(func)\n\n        while self.evaluations < self.budget:\n            self.update_velocities(func)\n            self.update_positions(func)\n            self.adapt_exploration()\n            if self.evaluations >= self.budget:\n                break\n        return self.global_best_fitness, self.global_best_position", "configspace": "", "generation": 5, "feedback": "The algorithm EnhancedAdaptiveClampedPSO scored 0.440 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["3f4dbed7-615a-44be-9e4d-93b05dfe2215"], "operator": null, "metadata": {"aucs": [0.1872423152877407, 0.2167524263384797, 0.4506528679930353, 0.7536341125910002, 0.41291812923810167, 0.46679529034516487, 0.3190331624741155, 0.4108385509153376, 0.39431628034027344, 0.18615592436004114, 0.5769329799237374, 0.9962556754271346, 0.28322252519498314, 0.4158397672252536, 0.7481650933227444, 0.432086172976781, 0.2574896852554911, 0.5753759989708276, 0.22612829366595866, 0.49314247338703265]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "017d9407-e513-4be3-ab26-fa4a861b402d", "fitness": 0.35360669714034154, "name": "AdaptiveRegionSearch", "description": "A population-based algorithm that adapts both the search space and the step size based on the success rate of recent moves, focusing on promising regions while maintaining exploration.", "code": "import numpy as np\n\nclass AdaptiveRegionSearch:\n    def __init__(self, budget=10000, dim=10, pop_size=20, initial_radius=1.0, shrink_factor=0.9, expand_factor=1.1, success_threshold=0.2, history_size=10):\n        \"\"\"\n        Args:\n            budget (int): The maximum number of function evaluations.\n            dim (int): The dimensionality of the search space.\n            pop_size (int): The population size.\n            initial_radius (float): The initial radius of the search region.\n            shrink_factor (float): Factor to shrink the radius when unsuccessful.\n            expand_factor (float): Factor to expand the radius when successful.\n            success_threshold (float): The success rate threshold to adjust the radius.\n            history_size (int): The size of the success history.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.radius = initial_radius\n        self.shrink_factor = shrink_factor\n        self.expand_factor = expand_factor\n        self.success_threshold = success_threshold\n        self.history_size = history_size\n        self.success_history = np.zeros(history_size)\n        self.success_idx = 0\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.center = None  # Initialize center\n\n    def __call__(self, func):\n        # Initialize population within bounds\n        if self.center is None:\n            self.center = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim) # initial center\n        \n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        # Find the best individual in the initial population\n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index].copy()\n        self.center = self.x_opt.copy()\n\n        while self.budget > 0:\n            # Generate new candidate solutions around the current center\n            new_population = np.random.normal(loc=self.center, scale=self.radius, size=(self.pop_size, self.dim))\n\n            # Clip to bounds\n            new_population = np.clip(new_population, func.bounds.lb, func.bounds.ub)\n\n            new_fitness = np.array([func(x) for x in new_population])\n            self.budget -= self.pop_size\n\n            # Update success history\n            num_success = np.sum(new_fitness < fitness)\n            success_rate = num_success / self.pop_size\n            self.success_history[self.success_idx] = success_rate\n            self.success_idx = (self.success_idx + 1) % self.history_size\n            avg_success_rate = np.mean(self.success_history)\n\n            # Update population and fitness\n            for i in range(self.pop_size):\n                if new_fitness[i] < fitness[i]:\n                    fitness[i] = new_fitness[i]\n                    population[i] = new_population[i]\n\n                    # Update global best\n                    if new_fitness[i] < self.f_opt:\n                        self.f_opt = new_fitness[i]\n                        self.x_opt = new_population[i].copy()\n\n            # Adjust the search region radius based on success rate\n            if avg_success_rate > self.success_threshold:\n                self.radius *= self.expand_factor\n                self.center = self.x_opt.copy() # Move center to the best\n            else:\n                self.radius *= self.shrink_factor\n\n            self.radius = np.clip(self.radius, 1e-6, abs(func.bounds.ub - func.bounds.lb).max()) # clip radius\n            \n            if self.budget <= 0:\n                break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "The algorithm AdaptiveRegionSearch scored 0.354 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["ca08a41a-418b-4ee4-ba18-aaa3d01359f5"], "operator": null, "metadata": {"aucs": [0.12082599761952384, 0.1947286127335084, 0.4632106335022007, 0.16641324517756506, 0.2665803036965255, 0.35318320492929545, 0.27186727455972093, 0.25435546692459554, 0.25482380480948663, 0.1989009431732326, 0.5687190847558591, 0.9912101306973459, 0.323142518397479, 0.2910890403868953, 0.6775048060406732, 0.32373376461251, 0.23709371244107302, 0.4738257228071452, 0.15526996383175884, 0.48565571171043687]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "8edf5a7c-04f8-4330-a600-b0e9d9276c26", "fitness": 0.2672880645636153, "name": "AdaptiveGaussianCauchySearch2", "description": "An adaptive algorithm that combines Gaussian mutation around the best solution with a Cauchy mutation for wider exploration, dynamically adjusting mutation strengths based on success and also uses a learning rate to dampen changes in the best solution.", "code": "import numpy as np\n\nclass AdaptiveGaussianCauchySearch2:\n    def __init__(self, budget=10000, dim=10, population_size=20, initial_std=0.5, success_threshold=0.2, cauchy_factor=0.1, learning_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = population_size\n        self.initial_std = initial_std\n        self.success_threshold = success_threshold\n        self.cauchy_factor = cauchy_factor\n        self.learning_rate = learning_rate\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.std = self.initial_std\n        eval_count = 0\n\n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.population_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        eval_count += self.population_size\n        \n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index]\n\n        while eval_count < self.budget:\n            # Generate new samples around the best solution - combined Gaussian and Cauchy\n            gaussian_samples = np.random.normal(loc=self.x_opt, scale=self.std, size=(self.population_size, self.dim))\n            cauchy_samples = self.x_opt + self.cauchy_factor * self.std * np.random.standard_cauchy(size=(self.population_size, self.dim))\n            new_samples = np.clip(0.5 * (gaussian_samples + cauchy_samples), func.bounds.lb, func.bounds.ub)  # Clip to bounds\n\n            new_fitness = np.array([func(x) for x in new_samples])\n            eval_count += self.population_size\n\n            # Update best solution with learning rate\n            for i in range(self.population_size):\n                if new_fitness[i] < self.f_opt:\n                    self.f_opt = new_fitness[i]\n                    self.x_opt = (1 - self.learning_rate) * self.x_opt + self.learning_rate * new_samples[i]\n\n            # Adapt the standard deviation based on success rate\n            success_rate = np.sum(new_fitness < fitness) / self.population_size\n            if success_rate > self.success_threshold:\n                self.std *= 1.1  # Increase exploration if successful\n            else:\n                self.std *= 0.9  # Decrease exploration if unsuccessful\n            \n            fitness = np.minimum(fitness, new_fitness) #replace only if better\n            best_index = np.argmin(fitness)\n\n            if eval_count >= self.budget:\n                break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "The algorithm AdaptiveGaussianCauchySearch2 scored 0.267 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["65ca3433-814b-4d91-9fcf-c34d334aadd4"], "operator": null, "metadata": {"aucs": [0.08659663102382165, 0.16303408171801093, 0.2719754591375566, 0.15943853484346893, 0.1816311475909541, 0.20437423207230654, 0.2111948656666709, 0.22696424644721913, 0.1865230093936492, 0.14470464964835106, 0.3793507943562284, 0.9936975280206864, 0.24576971552585258, 0.18168035567083685, 0.5457634123929253, 0.24362612355369284, 0.18241955112199104, 0.19341558405083692, 0.1290816321289252, 0.41451973690832156]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "33b5be3c-7f19-4736-9969-c6982b3d7bd8", "fitness": 0.4090106162734317, "name": "AdaptiveVelocityPSO", "description": "Adaptive Velocity PSO: Dynamically adjusts inertia and acceleration coefficients based on swarm stagnation and success rate, focusing on velocity updates for refined exploration and exploitation.", "code": "import numpy as np\n\nclass AdaptiveVelocityPSO:\n    def __init__(self, budget=10000, dim=10, pop_size=None, w_init=0.9, c1_init=2.0, c2_init=2.0):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size if pop_size is not None else 20\n        self.w = w_init  # Initial inertia weight\n        self.c1 = c1_init  # Initial cognitive coefficient\n        self.c2 = c2_init  # Initial social coefficient\n        self.swarm = None\n        self.velocities = None\n        self.personal_best_positions = None\n        self.personal_best_fitness = None\n        self.global_best_position = None\n        self.global_best_fitness = np.inf\n        self.evaluations = 0\n        self.success_rate = 0.0\n        self.stagnation_counter = 0\n\n    def initialize_swarm(self, func):\n        self.swarm = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.velocities = np.random.uniform(-0.1 * (func.bounds.ub - func.bounds.lb), 0.1 * (func.bounds.ub - func.bounds.lb), size=(self.pop_size, self.dim))\n        self.personal_best_positions = self.swarm.copy()\n        self.personal_best_fitness = np.array([func(x) for x in self.swarm])\n        self.evaluations += self.pop_size\n        self.global_best_position = self.personal_best_positions[np.argmin(self.personal_best_fitness)].copy()\n        self.global_best_fitness = np.min(self.personal_best_fitness)\n\n    def update_velocities(self):\n        r1 = np.random.rand(self.pop_size, self.dim)\n        r2 = np.random.rand(self.pop_size, self.dim)\n        cognitive_component = self.c1 * r1 * (self.personal_best_positions - self.swarm)\n        social_component = self.c2 * r2 * (self.global_best_position - self.swarm)\n        self.velocities = self.w * self.velocities + cognitive_component + social_component\n        # Velocity clamping - removed max_velocity calculation for simplicity and focus on adaptivity\n        # self.velocities = np.clip(self.velocities, -max_velocity, max_velocity)\n\n    def update_positions(self, func):\n        self.swarm += self.velocities\n        self.swarm = np.clip(self.swarm, func.bounds.lb, func.bounds.ub)\n        fitness = np.array([func(x) for x in self.swarm])\n        self.evaluations += self.pop_size\n\n        improved_count = 0\n        for i in range(self.pop_size):\n            if fitness[i] < self.personal_best_fitness[i]:\n                self.personal_best_fitness[i] = fitness[i]\n                self.personal_best_positions[i] = self.swarm[i].copy()\n                improved_count += 1\n\n                if fitness[i] < self.global_best_fitness:\n                    self.global_best_fitness = fitness[i]\n                    self.global_best_position = self.swarm[i].copy()\n\n        self.success_rate = improved_count / self.pop_size\n\n\n    def adapt_parameters(self):\n        # Dynamically adjust inertia and acceleration coefficients based on success\n        if self.success_rate > 0.2:\n            self.w *= 0.95  # Reduce inertia if doing well\n            self.c1 *= 1.05 # Increase cognitive\n            self.c2 *= 1.05 # Increase social\n            self.stagnation_counter = 0\n        else:\n            self.w *= 1.05  # Increase inertia if stagnating\n            self.c1 *= 0.95 # Decrease cognitive\n            self.c2 *= 0.95 # Decrease social\n            self.stagnation_counter += 1\n\n        # Keep parameters within reasonable bounds\n        self.w = np.clip(self.w, 0.4, 0.9)\n        self.c1 = np.clip(self.c1, 1.0, 3.0)\n        self.c2 = np.clip(self.c2, 1.0, 3.0)\n        \n        if self.stagnation_counter > 20:\n           # If stagnating for long, increase exploration\n           self.w = 0.9\n           self.c1 = 1.5\n           self.c2 = 1.5\n           self.stagnation_counter = 0\n\n\n    def __call__(self, func):\n        self.initialize_swarm(func)\n        while self.evaluations < self.budget:\n            self.update_velocities()\n            self.update_positions(func)\n            self.adapt_parameters()\n            if self.evaluations >= self.budget:\n                break\n\n        return self.global_best_fitness, self.global_best_position", "configspace": "", "generation": 6, "feedback": "The algorithm AdaptiveVelocityPSO scored 0.409 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["3f4dbed7-615a-44be-9e4d-93b05dfe2215"], "operator": null, "metadata": {"aucs": [0.17655881984908595, 0.17383426606016616, 0.44585702560333473, 0.8593149485554701, 0.2587132389341782, 0.5329893394776509, 0.30741283782618345, 0.45920203725114606, 0.46378910376297555, 0.22905757986759367, 0.3366218809754191, 0.9963096455191687, 0.2099628552872942, 0.25087859296098547, 0.6485918510731368, 0.5229616524218214, 0.372184073379206, 0.2892402687566028, 0.20047693067633188, 0.4462553772308846]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "520eb34a-3e7b-4531-bdec-f93eae7ec3c7", "fitness": 0.582057196467912, "name": "AdaptiveConstrictionPSO", "description": "Adaptive PSO with dynamically adjusted constriction factor and velocity mutation for enhanced exploration and exploitation.", "code": "import numpy as np\n\nclass AdaptiveConstrictionPSO:\n    def __init__(self, budget=10000, dim=10, pop_size=None, w_init=0.9, w_final=0.4, c1=2.0, c2=2.0, constriction_factor=0.72984):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size if pop_size is not None else 20\n        self.w_init = w_init  # Initial inertia weight\n        self.w_final = w_final # Final inertia weight\n        self.c1 = c1  # Cognitive coefficient\n        self.c2 = c2  # Social coefficient\n        self.constriction_factor = constriction_factor # Default constriction factor\n        self.swarm = None\n        self.velocities = None\n        self.personal_best_positions = None\n        self.personal_best_fitness = None\n        self.global_best_position = None\n        self.global_best_fitness = np.inf\n        self.evaluations = 0\n        self.iteration = 0\n\n    def initialize_swarm(self, func):\n        self.swarm = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.velocities = np.random.uniform(-0.1 * (func.bounds.ub - func.bounds.lb), 0.1 * (func.bounds.ub - func.bounds.lb), size=(self.pop_size, self.dim))\n        self.personal_best_positions = self.swarm.copy()\n        self.personal_best_fitness = np.array([func(x) for x in self.swarm])\n        self.evaluations += self.pop_size\n\n        self.global_best_position = self.personal_best_positions[np.argmin(self.personal_best_fitness)].copy()\n        self.global_best_fitness = np.min(self.personal_best_fitness)\n\n    def update_velocities(self, func):\n        # Linearly decreasing inertia weight\n        w = self.w_init - (self.w_init - self.w_final) * (self.iteration / self.budget)\n\n        for i in range(self.pop_size):\n            r1 = np.random.rand(self.dim)\n            r2 = np.random.rand(self.dim)\n\n            cognitive_component = self.c1 * r1 * (self.personal_best_positions[i] - self.swarm[i])\n            social_component = self.c2 * r2 * (self.global_best_position - self.swarm[i])\n\n            self.velocities[i] = self.constriction_factor * (w * self.velocities[i] + cognitive_component + social_component)\n\n            # Velocity Mutation: Add small random velocity component for exploration\n            if np.random.rand() < 0.05:  # 5% probability\n                 self.velocities[i] += np.random.uniform(-0.01, 0.01, self.dim)\n                 self.velocities[i] = np.clip(self.velocities[i], -0.2 * (func.bounds.ub - func.bounds.lb), 0.2 * (func.bounds.ub - func.bounds.lb))\n\n\n    def update_positions(self, func):\n        self.swarm += self.velocities\n        self.swarm = np.clip(self.swarm, func.bounds.lb, func.bounds.ub)\n\n        fitness = np.array([func(x) for x in self.swarm])\n        self.evaluations += self.pop_size\n\n        for i in range(self.pop_size):\n            if fitness[i] < self.personal_best_fitness[i]:\n                self.personal_best_fitness[i] = fitness[i]\n                self.personal_best_positions[i] = self.swarm[i].copy()\n\n                if fitness[i] < self.global_best_fitness:\n                    self.global_best_fitness = fitness[i]\n                    self.global_best_position = self.swarm[i].copy()\n\n    def adapt_constriction(self):\n         #Dynamically change constriction factor\n         if np.random.rand() < 0.1: #10% chance to adapt the constriction factor\n            self.constriction_factor = np.random.uniform(0.6, 0.8)\n\n    def __call__(self, func):\n        self.initialize_swarm(func)\n\n        while self.evaluations < self.budget:\n            self.update_velocities(func)\n            self.update_positions(func)\n            self.adapt_constriction() # Adapt the constriction factor\n            self.iteration += self.pop_size\n            if self.evaluations >= self.budget:\n                break\n        return self.global_best_fitness, self.global_best_position", "configspace": "", "generation": 6, "feedback": "The algorithm AdaptiveConstrictionPSO scored 0.582 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["3f4dbed7-615a-44be-9e4d-93b05dfe2215"], "operator": null, "metadata": {"aucs": [0.20126105060981603, 0.22999232321143892, 0.8542105714410573, 0.9572634855894645, 0.26533155331849734, 0.25815453809783084, 0.8911072229367388, 0.6057009870312009, 0.9035633104680614, 0.9100410711717126, 0.5031201019247622, 0.9968162958639303, 0.29374145701342114, 0.31354341923389173, 0.5861849453956558, 0.9201844172862765, 0.33106690724707066, 0.9049191595874134, 0.2077793564148407, 0.5071617555151601]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "33115b67-c003-4fdb-8f3e-63242bb71457", "fitness": 0.6027772325184049, "name": "AdaptiveGaussianCauchySearchLSHADE", "description": "An adaptive population-based algorithm that combines Gaussian and Cauchy mutations, adjusts mutation probabilities and step sizes based on success and population diversity, and incorporates a simplified L-SHADE-like historical memory for mutation.", "code": "import numpy as np\n\nclass AdaptiveGaussianCauchySearchLSHADE:\n    def __init__(self, budget=10000, dim=10, population_size=20, initial_gaussian_std=0.5, initial_cauchy_std=1.0, success_threshold=0.25, initial_cauchy_prob=0.1, memory_size=5):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = population_size\n        self.initial_gaussian_std = initial_gaussian_std\n        self.initial_cauchy_std = initial_cauchy_std\n        self.success_threshold = success_threshold\n        self.cauchy_prob = initial_cauchy_prob\n        self.gaussian_std = initial_gaussian_std\n        self.cauchy_std = initial_cauchy_std\n        self.memory_size = memory_size\n        self.memory_gaussian_std = np.full(memory_size, initial_gaussian_std)\n        self.memory_cauchy_std = np.full(memory_size, initial_cauchy_std)\n        self.memory_cauchy_prob = np.full(memory_size, initial_cauchy_prob)\n        self.memory_idx = 0\n        self.archive = []\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        eval_count = 0\n\n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.population_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        eval_count += self.population_size\n\n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index]\n\n        while eval_count < self.budget:\n            new_samples = np.zeros((self.population_size, self.dim))\n            new_fitness = np.zeros(self.population_size)\n\n            for i in range(self.population_size):\n                # Select mutation strategy based on probability\n                if np.random.rand() < self.cauchy_prob:\n                    # Cauchy mutation for exploration\n                    mutation = np.random.standard_cauchy(size=self.dim) * self.cauchy_std\n                    new_sample = population[i] + mutation # Mutate around current individual\n                else:\n                    # Gaussian mutation around the best\n                    new_sample = np.random.normal(loc=self.x_opt, scale=self.gaussian_std, size=self.dim)\n\n                new_sample = np.clip(new_sample, func.bounds.lb, func.bounds.ub)\n                new_samples[i] = new_sample\n                new_fitness[i] = func(new_sample)\n                eval_count += 1\n                \n                if new_fitness[i] < self.f_opt:\n                    self.f_opt = new_fitness[i]\n                    self.x_opt = new_samples[i]\n\n            improved = new_fitness < fitness\n            num_improved = np.sum(improved)\n\n            # Update population and fitness\n            for i in range(self.population_size):\n                 if new_fitness[i] < fitness[i]:\n                    self.archive.append(population[i].copy())  # Add replaced solutions to the archive (L-SHADE style)\n                    population[i] = new_samples[i]\n                    fitness[i] = new_fitness[i]\n            \n            best_index = np.argmin(fitness)\n            self.x_opt = population[best_index]\n            self.f_opt = fitness[best_index]\n\n\n            # Adapt standard deviations and cauchy probability\n            success_rate = num_improved / self.population_size\n            \n            # L-SHADE-like memory update (simplified)\n            self.memory_gaussian_std[self.memory_idx] = self.gaussian_std\n            self.memory_cauchy_std[self.memory_idx] = self.cauchy_std\n            self.memory_cauchy_prob[self.memory_idx] = self.cauchy_prob\n\n            self.memory_idx = (self.memory_idx + 1) % self.memory_size\n            \n            # Use memory values with some probability\n            if np.random.rand() < 0.2:\n                self.gaussian_std = np.random.choice(self.memory_gaussian_std)\n                self.cauchy_std = np.random.choice(self.memory_cauchy_std)\n                self.cauchy_prob = np.random.choice(self.memory_cauchy_prob)\n            else:    \n                if success_rate > self.success_threshold:\n                    self.gaussian_std *= 1.1\n                    self.cauchy_std *= 1.05\n                    self.cauchy_prob = min(self.cauchy_prob * 1.05, 0.9) #Increase chance of Cauchy\n                else:\n                    self.gaussian_std *= 0.9\n                    self.cauchy_std *= 0.95\n                    self.cauchy_prob = max(self.cauchy_prob * 0.95, 0.1) #Decrease chance of Cauchy\n\n            if eval_count >= self.budget:\n                break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "The algorithm AdaptiveGaussianCauchySearchLSHADE scored 0.603 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["dd524f86-fb83-498a-89b8-95b03d054dd3"], "operator": null, "metadata": {"aucs": [0.13204627897928856, 0.178559726699404, 0.7960534900139986, 0.9075963037483757, 0.2647596072152961, 0.8358628581756309, 0.26451766411099864, 0.5657626351934035, 0.8159203081388768, 0.7893802255235541, 0.9472786562645837, 0.9991279380309194, 0.2723745523691642, 0.4615512488191098, 0.9383388059531867, 0.6138440537709325, 0.6832182379430546, 0.9002741152510397, 0.18426504995047943, 0.5048128942168022]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "c34405e8-ce2c-432f-b429-1b294b98a738", "fitness": "-inf", "name": "AdaptiveDifferentialEvolution", "description": "Adaptive Differential Evolution with a dynamic population size reduction and orthogonal learning for enhanced exploration and exploitation.", "code": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget=10000, dim=10, pop_size=40, F_base=0.5, CR_base=0.7, learning_rate=0.1, success_history_size=10, reduction_factor=0.95):\n        \"\"\"\n        Args:\n            budget (int): The maximum number of function evaluations.\n            dim (int): The dimensionality of the search space.\n            pop_size (int): The initial population size.\n            F_base (float): Base mutation factor.\n            CR_base (float): Base crossover rate.\n            learning_rate (float): The learning rate for adaptive CR/F.\n            success_history_size (int): Size of the success history for adapting CR/F.\n            reduction_factor (float): Factor by which population size is reduced after stagnation.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.initial_pop_size = pop_size\n        self.F_base = F_base\n        self.CR_base = CR_base\n        self.learning_rate = learning_rate\n        self.success_history_size = success_history_size\n        self.success_F = np.ones(self.success_history_size) * self.F_base\n        self.success_CR = np.ones(self.success_history_size) * self.CR_base\n        self.success_idx = 0\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.reduction_factor = reduction_factor\n        self.stagnation_counter = 0\n        self.max_stagnation = 50 # Number of iterations before population reduction\n\n    def orthogonal_learning(self, population, fitness, num_samples=5):\n        \"\"\"Performs orthogonal learning to generate new candidate solutions.\"\"\"\n        best_idx = np.argmin(fitness)\n        best_solution = population[best_idx]\n        \n        new_solutions = []\n        for _ in range(num_samples):\n            # Generate a random orthogonal matrix\n            Q = np.linalg.qr(np.random.randn(self.dim, self.dim))[0]\n            \n            # Generate a random vector in the range [-1, 1]\n            random_vector = np.random.uniform(-1, 1, self.dim)\n            \n            # Apply the orthogonal transformation\n            new_solution = best_solution + np.dot(random_vector, Q)\n            new_solutions.append(new_solution)\n        \n        return np.array(new_solutions)\n\n    def __call__(self, func):\n        # Initialize population within bounds\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size # Update remaining budget\n\n        # Find the best individual in the initial population\n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index].copy()\n        \n        last_f_opt = self.f_opt\n\n        while self.budget > 0:\n            # Rank population\n            ranked_indices = np.argsort(fitness)\n            \n            for i in range(self.pop_size):\n                # Mutation: Rank-based selection of parents\n                pbest_idx = ranked_indices[np.random.randint(0, max(1, int(0.2 * self.pop_size)))] # Select best 20%\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b = np.random.choice(idxs, 2, replace=False)\n                \n                # Adaptive F and CR\n                F = np.random.normal(self.F_base, 0.1)\n                F = np.clip(F, 0.1, 0.9)\n                CR = np.random.normal(self.CR_base, 0.1)\n                CR = np.clip(CR, 0.1, 0.9)\n                \n                mutant = population[i] + F * (population[pbest_idx] - population[a]) + F * (population[b] - population[i])\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Boundary handling (clip to bounds)\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Evaluation\n                f = func(trial)\n                self.budget -= 1\n                \n                if f < fitness[i]:\n                    delta = fitness[i] - f\n                    fitness[i] = f\n                    population[i] = trial\n                    \n                    # Update global best\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = trial.copy()\n\n                    # Update success history for F and CR\n                    self.success_F[self.success_idx] = F\n                    self.success_CR[self.success_idx] = CR\n                    self.success_idx = (self.success_idx + 1) % self.success_history_size\n\n                    # Adjust base values based on recent success\n                    self.F_base = np.mean(self.success_F)\n                    self.CR_base = np.mean(self.success_CR)\n                    self.F_base = np.clip(self.F_base, 0.1, 0.9)\n                    self.CR_base = np.clip(self.CR_base, 0.1, 0.9)\n\n                if self.budget <= 0:\n                    break\n            \n            #Stagnation check and population reduction\n            if abs(self.f_opt - last_f_opt) < 1e-6:\n                self.stagnation_counter +=1\n            else:\n                self.stagnation_counter = 0\n            \n            if self.stagnation_counter >= self.max_stagnation and self.pop_size > 10:  #Reduce population size\n                self.pop_size = int(self.pop_size * self.reduction_factor)\n                self.pop_size = max(10, self.pop_size)\n                \n                #Keep the best individuals and re-initialize the rest\n                best_indices = np.argsort(fitness)[:self.pop_size]\n                population = population[best_indices]\n                fitness = fitness[best_indices]\n\n                remaining_pop = self.initial_pop_size - self.pop_size\n                new_population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(remaining_pop, self.dim))\n                new_fitness = np.array([func(x) for x in new_population])\n                self.budget -= remaining_pop\n\n                population = np.concatenate((population, new_population), axis=0)\n                fitness = np.concatenate((fitness, new_fitness))\n                self.pop_size = self.initial_pop_size\n                \n                #Orthogonal learning after reduction\n                orthogonal_solutions = self.orthogonal_learning(population, fitness, num_samples=5)\n\n                #Evaluate orthogonal solutions\n                orthogonal_fitness = np.array([func(x) for x in orthogonal_solutions])\n                self.budget -= orthogonal_solutions.shape[0]\n\n                #Replace worst individuals with orthogonal solutions if better\n                worst_indices = np.argsort(fitness)[-orthogonal_solutions.shape[0]:]\n\n                for idx, orth_idx in enumerate(range(orthogonal_solutions.shape[0])):\n                    if orthogonal_fitness[orth_idx] < fitness[worst_indices[idx]]:\n                        population[worst_indices[idx]] = orthogonal_solutions[orth_idx]\n                        fitness[worst_indices[idx]] = orthogonal_fitness[orth_idx]\n                \n                best_index = np.argmin(fitness)\n                self.f_opt = fitness[best_index]\n                self.x_opt = population[best_index].copy()\n                self.stagnation_counter = 0\n            \n            last_f_opt = self.f_opt #update\n\n            if self.budget <= 0:\n                break\n            \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "Evaluation timed out after 300 seconds.", "error": "Evaluation timed out after 300 seconds.", "parent_ids": ["ca08a41a-418b-4ee4-ba18-aaa3d01359f5"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "b555e937-83c1-4706-a913-bd5f0460d0dd", "fitness": 0.4542430158546781, "name": "SelfOrganizingSearch", "description": "A self-organizing algorithm that adapts its search behavior based on local landscape features and distance to the global best, employing a dynamic balance between exploration and exploitation.", "code": "import numpy as np\n\nclass SelfOrganizingSearch:\n    def __init__(self, budget=10000, dim=10, pop_size=40, initial_step_size=0.5, exploration_rate=0.1, contraction_rate=0.5, expansion_rate=2.0, min_step_size=0.01):\n        \"\"\"\n        Args:\n            budget (int): The maximum number of function evaluations.\n            dim (int): The dimensionality of the search space.\n            pop_size (int): The population size.\n            initial_step_size (float): Initial step size for each individual.\n            exploration_rate (float): Probability of exploration move.\n            contraction_rate (float): Rate at which step size is contracted after failure.\n            expansion_rate (float): Rate at which step size is expanded after success.\n            min_step_size (float): Minimum step size allowed.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.initial_step_size = initial_step_size\n        self.exploration_rate = exploration_rate\n        self.contraction_rate = contraction_rate\n        self.expansion_rate = expansion_rate\n        self.min_step_size = min_step_size\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        # Initialize population and step sizes\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n        step_sizes = np.full(self.pop_size, self.initial_step_size)  # Individual step sizes\n\n        # Find the best individual in the initial population\n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index].copy()\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Exploration vs. Exploitation\n                if np.random.rand() < self.exploration_rate:\n                    # Exploration: Random move\n                    new_solution = population[i] + np.random.uniform(-step_sizes[i], step_sizes[i], size=self.dim)\n                else:\n                    # Exploitation: Move towards best solution with some noise\n                    direction = self.x_opt - population[i]\n                    new_solution = population[i] + direction + np.random.normal(0, step_sizes[i], size=self.dim)\n\n                # Boundary handling\n                new_solution = np.clip(new_solution, func.bounds.lb, func.bounds.ub)\n\n                # Evaluate new solution\n                f = func(new_solution)\n                self.budget -= 1\n\n                # Update if better\n                if f < fitness[i]:\n                    fitness[i] = f\n                    population[i] = new_solution\n                    step_sizes[i] *= self.expansion_rate  # Increase step size\n\n                    # Update global best\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = new_solution.copy()\n                else:\n                    step_sizes[i] *= self.contraction_rate  # Decrease step size\n\n                # Ensure minimum step size\n                step_sizes[i] = max(step_sizes[i], self.min_step_size)\n\n                if self.budget <= 0:\n                    break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "The algorithm SelfOrganizingSearch scored 0.454 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["ca08a41a-418b-4ee4-ba18-aaa3d01359f5"], "operator": null, "metadata": {"aucs": [0.16074035030258094, 0.19440654373661792, 0.5628989945617846, 0.9521742586720803, 0.36325030506974054, 0.6285743886366356, 0.28089030312080254, 0.4878824113974273, 0.549959576537536, 0.18754945049270277, 0.9595828221374638, 0.9942833042901398, 0.20850897183560113, 0.3375577634367083, 0.5884462450317364, 0.3855986355761747, 0.4070213175989046, 0.19798341155425614, 0.16296797120333228, 0.4745832919013354]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "4b32bff8-cd88-433a-808a-6ae723864a6a", "fitness": 0.5460616397445377, "name": "AdaptiveGaussianCauchySearch", "description": "An adaptive population-based algorithm that blends Gaussian and Cauchy mutations with a dynamic probability based on recent success and restarts the population if it stagnates.", "code": "import numpy as np\n\nclass AdaptiveGaussianCauchySearch:\n    def __init__(self, budget=10000, dim=10, population_size=20, initial_gaussian_std=0.5, initial_cauchy_std=1.0, success_threshold=0.25, stagnation_threshold=500):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = population_size\n        self.initial_gaussian_std = initial_gaussian_std\n        self.initial_cauchy_std = initial_cauchy_std\n        self.success_threshold = success_threshold\n        self.gaussian_std = initial_gaussian_std\n        self.cauchy_std = initial_cauchy_std\n        self.stagnation_threshold = stagnation_threshold\n        self.stagnation_counter = 0\n        self.best_fitness_history = []\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        eval_count = 0\n\n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.population_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        eval_count += self.population_size\n\n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index]\n        self.best_fitness_history.append(self.f_opt)\n\n        while eval_count < self.budget:\n            new_samples = np.zeros((self.population_size, self.dim))\n            new_fitness = np.zeros(self.population_size)\n\n            for i in range(self.population_size):\n                # Adaptive Cauchy Probability\n                cauchy_prob = 0.1 + 0.4 * np.exp(-abs(self.gaussian_std - self.cauchy_std)) #dynamically adjust\n                if np.random.rand() < cauchy_prob:\n                    # Cauchy mutation for exploration\n                    mutation = np.random.standard_cauchy(size=self.dim) * self.cauchy_std\n                    new_sample = self.x_opt + mutation\n                else:\n                    # Gaussian mutation around the best\n                    new_sample = np.random.normal(loc=self.x_opt, scale=self.gaussian_std, size=self.dim)\n\n                new_sample = np.clip(new_sample, func.bounds.lb, func.bounds.ub)\n                new_samples[i] = new_sample\n                new_fitness[i] = func(new_sample)\n                eval_count += 1\n                \n                if new_fitness[i] < self.f_opt:\n                    self.f_opt = new_fitness[i]\n                    self.x_opt = new_sample\n\n            # Update best solution\n            improved = new_fitness < fitness\n            num_improved = np.sum(improved)\n            \n            # Adapt standard deviations\n            success_rate = num_improved / self.population_size\n            if success_rate > self.success_threshold:\n                self.gaussian_std *= 1.05\n                self.cauchy_std *= 1.02\n            else:\n                self.gaussian_std *= 0.95\n                self.cauchy_std *= 0.98\n\n            fitness = np.minimum(fitness, new_fitness)\n            population = np.where(improved[:, None], new_samples, population)\n\n            best_index = np.argmin(fitness)\n            self.best_fitness_history.append(self.f_opt)\n\n\n            # Stagnation check and restart\n            if len(self.best_fitness_history) > self.stagnation_threshold:\n                if np.std(self.best_fitness_history[-self.stagnation_threshold:]) < 1e-6:  #Stagnation detected\n                    # Restart the population\n                    population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.population_size, self.dim))\n                    fitness = np.array([func(x) for x in population])\n                    eval_count += self.population_size - self.population_size #Correct count\n                    best_index = np.argmin(fitness)\n                    if fitness[best_index] < self.f_opt:\n                        self.f_opt = fitness[best_index]\n                        self.x_opt = population[best_index]\n                    self.gaussian_std = self.initial_gaussian_std\n                    self.cauchy_std = self.initial_cauchy_std\n                    self.best_fitness_history = [self.f_opt]\n\n            if eval_count >= self.budget:\n                break\n            \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "The algorithm AdaptiveGaussianCauchySearch scored 0.546 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["dd524f86-fb83-498a-89b8-95b03d054dd3"], "operator": null, "metadata": {"aucs": [0.2356033490567616, 0.2010294026542453, 0.7852193518523158, 0.16941411661368233, 0.7878017956189478, 0.8212916474650022, 0.30069641703305705, 0.5692147831235372, 0.7941165500739439, 0.2630845954676698, 0.920852042955613, 0.9969540607048843, 0.2682011240017472, 0.26063470753486306, 0.8994973443185744, 0.7576740493075128, 0.31271307576690244, 0.8752286221874367, 0.2178265846267382, 0.48417917452731696]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "cff6eec8-5ee8-442c-bade-46bf95517405", "fitness": 0.0, "name": "AdaptivePSO", "description": "Adaptive PSO with self-adaptive learning rates and a stagnation detection mechanism to restart the swarm.", "code": "import numpy as np\n\nclass AdaptivePSO:\n    def __init__(self, budget=10000, dim=10, pop_size=None, w=0.7, c1_init=2.5, c1_final=1.5, c2_init=0.5, c2_final=1.5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size if pop_size is not None else 20\n        self.w = w  # Inertia weight\n        self.c1_init = c1_init  # Initial cognitive coefficient\n        self.c1_final = c1_final  # Final cognitive coefficient\n        self.c2_init = c2_init  # Initial social coefficient\n        self.c2_final = c2_final  # Final social coefficient\n        self.swarm = None\n        self.velocities = None\n        self.personal_best_positions = None\n        self.personal_best_fitness = None\n        self.global_best_position = None\n        self.global_best_fitness = np.inf\n        self.evaluations = 0\n        self.iteration = 0\n        self.stagnation_counter = 0\n        self.stagnation_threshold = 50 # Number of iterations without improvement before restart\n\n    def initialize_swarm(self, func):\n        self.swarm = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.velocities = np.random.uniform(-0.1 * (func.bounds.ub - func.bounds.lb), 0.1 * (func.bounds.ub - func.bounds.lb), size=(self.pop_size, self.dim))\n        self.personal_best_positions = self.swarm.copy()\n        self.personal_best_fitness = np.array([func(x) for x in self.swarm])\n        self.evaluations += self.pop_size\n\n        self.global_best_position = self.personal_best_positions[np.argmin(self.personal_best_fitness)].copy()\n        self.global_best_fitness = np.min(self.personal_best_fitness)\n\n    def update_velocities(self):\n        # Adaptive learning rates\n        c1 = self.c1_init - (self.c1_init - self.c1_final) * (self.iteration / self.budget)\n        c2 = self.c2_init + (self.c2_final - self.c2_init) * (self.iteration / self.budget)\n        \n        for i in range(self.pop_size):\n            r1 = np.random.rand(self.dim)\n            r2 = np.random.rand(self.dim)\n\n            self.velocities[i] = self.w * self.velocities[i] + \\\n                                 c1 * r1 * (self.personal_best_positions[i] - self.swarm[i]) + \\\n                                 c2 * r2 * (self.global_best_position - self.swarm[i])\n\n    def update_positions(self, func):\n        self.swarm += self.velocities\n        self.swarm = np.clip(self.swarm, func.bounds.lb, func.bounds.ub)\n\n        fitness = np.array([func(x) for x in self.swarm])\n        self.evaluations += self.pop_size\n\n        for i in range(self.pop_size):\n            if fitness[i] < self.personal_best_fitness[i]:\n                self.personal_best_fitness[i] = fitness[i]\n                self.personal_best_positions[i] = self.swarm[i].copy()\n\n                if fitness[i] < self.global_best_fitness:\n                    self.global_best_fitness = fitness[i]\n                    self.global_best_position = self.swarm[i].copy()\n                    self.stagnation_counter = 0  # Reset stagnation counter\n            else:\n                self.stagnation_counter += 1\n\n    def restart_swarm(self, func):\n        # Re-initialize swarm if stagnation is detected\n        self.swarm = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.velocities = np.random.uniform(-0.1 * (func.bounds.ub - func.bounds.lb), 0.1 * (func.bounds.ub - func.bounds.lb), size=(self.pop_size, self.dim))\n        self.personal_best_positions = self.swarm.copy()\n        self.personal_best_fitness = np.array([func(x) for x in self.swarm])\n        self.evaluations += self.pop_size\n\n        self.global_best_position = self.personal_best_positions[np.argmin(self.personal_best_fitness)].copy()\n        self.global_best_fitness = np.min(self.personal_best_fitness)\n        self.stagnation_counter = 0\n    \n    def __call__(self, func):\n        self.initialize_swarm(func)\n\n        while self.evaluations < self.budget:\n            self.update_velocities()\n            self.update_positions(func)\n\n            if self.stagnation_counter > self.stagnation_threshold:\n                self.restart_swarm(func)\n\n            self.iteration += self.pop_size\n            if self.evaluations >= self.budget:\n                break\n        return self.global_best_fitness, self.global_best_position", "configspace": "", "generation": 7, "feedback": "The algorithm AdaptivePSO scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["520eb34a-3e7b-4531-bdec-f93eae7ec3c7"], "operator": null, "metadata": {"aucs": [0]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "026d6d18-c26f-450b-bf84-2034005cac9f", "fitness": 0.0, "name": "AdaptiveHybridGaussianCauchySearchOLSHADE", "description": "An adaptive hybrid algorithm using Gaussian and Cauchy mutations with dynamically adjusted parameters and orthogonal learning to enhance exploration and exploitation, incorporating L-SHADE's historical memory for adaptation.", "code": "import numpy as np\n\nclass AdaptiveHybridGaussianCauchySearchOLSHADE:\n    def __init__(self, budget=10000, dim=10, population_size=20, initial_gaussian_std=0.5, initial_cauchy_std=1.0, success_threshold=0.25, initial_cauchy_prob=0.1, memory_size=5, ol_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = population_size\n        self.initial_gaussian_std = initial_gaussian_std\n        self.initial_cauchy_std = initial_cauchy_std\n        self.success_threshold = success_threshold\n        self.cauchy_prob = initial_cauchy_prob\n        self.gaussian_std = initial_gaussian_std\n        self.cauchy_std = initial_cauchy_std\n        self.memory_size = memory_size\n        self.memory_gaussian_std = np.full(memory_size, initial_gaussian_std)\n        self.memory_cauchy_std = np.full(memory_size, initial_cauchy_std)\n        self.memory_cauchy_prob = np.full(memory_size, initial_cauchy_prob)\n        self.memory_idx = 0\n        self.archive = []\n        self.ol_rate = ol_rate  # Orthogonal learning rate\n\n    def orthogonal_learning(self, x, func):\n        \"\"\"Performs orthogonal learning to generate a potentially better solution.\"\"\"\n        basis = np.random.randn(self.dim, self.dim)\n        basis, _ = np.linalg.qr(basis)  # Orthogonalize the basis\n        \n        delta = np.linspace(-1, 1, self.dim)  # Sample points along each basis vector\n        candidates = x + delta[:, None] * basis\n        candidates = np.clip(candidates, func.bounds.lb, func.bounds.ub)\n\n        fitness_values = np.array([func(c) for c in candidates])\n        best_index = np.argmin(fitness_values)\n        return candidates[best_index], fitness_values[best_index]\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        eval_count = 0\n\n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.population_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        eval_count += self.population_size\n\n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index]\n\n        while eval_count < self.budget:\n            new_samples = np.zeros((self.population_size, self.dim))\n            new_fitness = np.zeros(self.population_size)\n\n            for i in range(self.population_size):\n                # Select mutation strategy based on probability\n                if np.random.rand() < self.cauchy_prob:\n                    # Cauchy mutation for exploration\n                    mutation = np.random.standard_cauchy(size=self.dim) * self.cauchy_std\n                    new_sample = population[i] + mutation # Mutate around current individual\n                else:\n                    # Gaussian mutation around the best\n                    new_sample = np.random.normal(loc=self.x_opt, scale=self.gaussian_std, size=self.dim)\n\n                new_sample = np.clip(new_sample, func.bounds.lb, func.bounds.ub)\n\n                # Apply orthogonal learning with a certain probability\n                if np.random.rand() < self.ol_rate:\n                    new_sample, new_fitness_ol = self.orthogonal_learning(new_sample, func)\n                    eval_count += self.dim # Account for function evaluations in OL\n                else:\n                    new_fitness_ol = func(new_sample)\n                    eval_count += 1\n\n                new_samples[i] = new_sample\n                new_fitness[i] = new_fitness_ol\n                \n                if new_fitness[i] < self.f_opt:\n                    self.f_opt = new_fitness[i]\n                    self.x_opt = new_samples[i]\n\n            improved = new_fitness < fitness\n            num_improved = np.sum(improved)\n\n            # Update population and fitness\n            for i in range(self.population_size):\n                 if new_fitness[i] < fitness[i]:\n                    self.archive.append(population[i].copy())  # Add replaced solutions to the archive (L-SHADE style)\n                    population[i] = new_samples[i]\n                    fitness[i] = new_fitness[i]\n            \n            best_index = np.argmin(fitness)\n            self.x_opt = population[best_index]\n            self.f_opt = fitness[best_index]\n\n\n            # Adapt standard deviations and cauchy probability\n            success_rate = num_improved / self.population_size\n            \n            # L-SHADE-like memory update (simplified)\n            self.memory_gaussian_std[self.memory_idx] = self.gaussian_std\n            self.memory_cauchy_std[self.memory_idx] = self.cauchy_std\n            self.memory_cauchy_prob[self.memory_idx] = self.cauchy_prob\n\n            self.memory_idx = (self.memory_idx + 1) % self.memory_size\n            \n            # Use memory values with some probability\n            if np.random.rand() < 0.2:\n                self.gaussian_std = np.random.choice(self.memory_gaussian_std)\n                self.cauchy_std = np.random.choice(self.memory_cauchy_std)\n                self.cauchy_prob = np.random.choice(self.memory_cauchy_prob)\n            else:    \n                if success_rate > self.success_threshold:\n                    self.gaussian_std *= 1.1\n                    self.cauchy_std *= 1.05\n                    self.cauchy_prob = min(self.cauchy_prob * 1.05, 0.9) #Increase chance of Cauchy\n                else:\n                    self.gaussian_std *= 0.9\n                    self.cauchy_std *= 0.95\n                    self.cauchy_prob = max(self.cauchy_prob * 0.95, 0.1) #Decrease chance of Cauchy\n\n            if eval_count >= self.budget:\n                break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "The algorithm AdaptiveHybridGaussianCauchySearchOLSHADE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["33115b67-c003-4fdb-8f3e-63242bb71457"], "operator": null, "metadata": {"aucs": [0]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "c2509f9e-35a1-4f33-9f73-8fe53d9be9cf", "fitness": "-inf", "name": "AdaptiveHybridES", "description": "An adaptive hybrid algorithm combining Gaussian and Cauchy mutations with L-SHADE inspired adaptation, incorporating a restart mechanism based on stagnation and a dynamic population size adjustment.", "code": "import numpy as np\n\nclass AdaptiveHybridES:\n    def __init__(self, budget=10000, dim=10, population_size=20, initial_gaussian_std=0.5, initial_cauchy_std=1.0, success_threshold=0.25, initial_cauchy_prob=0.1, memory_size=5, stagnation_threshold=500, restart_prob=0.05, pop_size_adapt_freq=200):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = population_size\n        self.initial_gaussian_std = initial_gaussian_std\n        self.initial_cauchy_std = initial_cauchy_std\n        self.success_threshold = success_threshold\n        self.cauchy_prob = initial_cauchy_prob\n        self.gaussian_std = initial_gaussian_std\n        self.cauchy_std = initial_cauchy_std\n        self.memory_size = memory_size\n        self.memory_gaussian_std = np.full(memory_size, initial_gaussian_std)\n        self.memory_cauchy_std = np.full(memory_size, initial_cauchy_std)\n        self.memory_cauchy_prob = np.full(memory_size, initial_cauchy_prob)\n        self.memory_idx = 0\n        self.archive = []\n        self.stagnation_threshold = stagnation_threshold\n        self.restart_prob = restart_prob\n        self.best_fitness_history = []\n        self.pop_size_adapt_freq = pop_size_adapt_freq\n\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        eval_count = 0\n\n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.population_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        eval_count += self.population_size\n\n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index]\n        self.best_fitness_history.append(self.f_opt)\n\n        iteration = 0\n\n        while eval_count < self.budget:\n            new_samples = np.zeros((self.population_size, self.dim))\n            new_fitness = np.zeros(self.population_size)\n\n            for i in range(self.population_size):\n                # Select mutation strategy based on probability\n                if np.random.rand() < self.cauchy_prob:\n                    # Cauchy mutation for exploration\n                    mutation = np.random.standard_cauchy(size=self.dim) * self.cauchy_std\n                    new_sample = population[i] + mutation # Mutate around current individual\n                else:\n                    # Gaussian mutation around the best\n                    new_sample = np.random.normal(loc=self.x_opt, scale=self.gaussian_std, size=self.dim)\n\n                new_sample = np.clip(new_sample, func.bounds.lb, func.bounds.ub)\n                new_samples[i] = new_sample\n                new_fitness[i] = func(new_sample)\n                eval_count += 1\n                \n                if new_fitness[i] < self.f_opt:\n                    self.f_opt = new_fitness[i]\n                    self.x_opt = new_samples[i]\n\n            improved = new_fitness < fitness\n            num_improved = np.sum(improved)\n\n            # Update population and fitness\n            for i in range(self.population_size):\n                 if new_fitness[i] < fitness[i]:\n                    self.archive.append(population[i].copy())  # Add replaced solutions to the archive (L-SHADE style)\n                    population[i] = new_samples[i]\n                    fitness[i] = new_fitness[i]\n            \n            best_index = np.argmin(fitness)\n            self.x_opt = population[best_index]\n            self.f_opt = fitness[best_index]\n            self.best_fitness_history.append(self.f_opt)\n\n\n            # Adapt standard deviations and cauchy probability\n            success_rate = num_improved / self.population_size\n            \n            # L-SHADE-like memory update (simplified)\n            self.memory_gaussian_std[self.memory_idx] = self.gaussian_std\n            self.memory_cauchy_std[self.memory_idx] = self.cauchy_std\n            self.memory_cauchy_prob[self.memory_idx] = self.cauchy_prob\n\n            self.memory_idx = (self.memory_idx + 1) % self.memory_size\n            \n            # Use memory values with some probability\n            if np.random.rand() < 0.2:\n                self.gaussian_std = np.random.choice(self.memory_gaussian_std)\n                self.cauchy_std = np.random.choice(self.memory_cauchy_std)\n                self.cauchy_prob = np.random.choice(self.memory_cauchy_prob)\n            else:    \n                if success_rate > self.success_threshold:\n                    self.gaussian_std *= 1.1\n                    self.cauchy_std *= 1.05\n                    self.cauchy_prob = min(self.cauchy_prob * 1.05, 0.9) #Increase chance of Cauchy\n                else:\n                    self.gaussian_std *= 0.9\n                    self.cauchy_std *= 0.95\n                    self.cauchy_prob = max(self.cauchy_prob * 0.95, 0.1) #Decrease chance of Cauchy\n            \n            # Stagnation check and restart\n            if iteration > self.stagnation_threshold and np.random.rand() < self.restart_prob:\n                population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.population_size, self.dim))\n                fitness = np.array([func(x) for x in population])\n                eval_count += self.population_size\n                best_index = np.argmin(fitness)\n                self.f_opt = fitness[best_index]\n                self.x_opt = population[best_index]\n                self.gaussian_std = self.initial_gaussian_std\n                self.cauchy_std = self.initial_cauchy_std\n                self.cauchy_prob = self.initial_cauchy_prob\n                self.best_fitness_history.append(self.f_opt)\n\n            # Population size adjustment\n            if iteration % self.pop_size_adapt_freq == 0:\n                 if len(self.best_fitness_history) > self.stagnation_threshold:\n                    if np.std(self.best_fitness_history[-self.stagnation_threshold:]) < 1e-6:  #Stagnation detected\n                        self.population_size = min(self.population_size * 2, 100) #Increase population\n                        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.population_size, self.dim))\n                        fitness = np.array([func(x) for x in population])\n                        eval_count += self.population_size\n                        best_index = np.argmin(fitness)\n                        self.f_opt = fitness[best_index]\n                        self.x_opt = population[best_index]\n                    else:\n                        self.population_size = max(int(self.population_size * 0.9), 10)  # Decrease population\n                        population = population[:self.population_size]\n                        fitness = fitness[:self.population_size]\n\n\n            iteration += 1\n            if eval_count >= self.budget:\n                break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "Evaluation timed out after 300 seconds.", "error": "Evaluation timed out after 300 seconds.", "parent_ids": ["33115b67-c003-4fdb-8f3e-63242bb71457"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "b605355b-ca9e-4bc9-98b5-0ff0918d63c6", "fitness": 0.0, "name": "AdaptiveDifferentialEvolution", "description": "Adaptive Differential Evolution with improved population diversity maintenance via orthogonal learning and a restart mechanism.", "code": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget=10000, dim=10, pop_size=40, F_base=0.5, CR_base=0.7, learning_rate=0.1, success_history_size=10, orthogonal_learning_rate=0.1):\n        \"\"\"\n        Args:\n            budget (int): The maximum number of function evaluations.\n            dim (int): The dimensionality of the search space.\n            pop_size (int): The population size.\n            F_base (float): Base mutation factor.\n            CR_base (float): Base crossover rate.\n            learning_rate (float): The learning rate for adaptive CR/F.\n            success_history_size (int): Size of the success history for adapting CR/F.\n            orthogonal_learning_rate (float): Learning rate for orthogonal learning.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F_base = F_base\n        self.CR_base = CR_base\n        self.learning_rate = learning_rate\n        self.success_history_size = success_history_size\n        self.success_F = np.ones(self.success_history_size) * self.F_base\n        self.success_CR = np.ones(self.success_history_size) * self.CR_base\n        self.success_idx = 0\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.orthogonal_learning_rate = orthogonal_learning_rate\n        self.restart_trigger = 0.05 * budget # Restart after no improvement for 5% of budget\n        self.last_improvement = 0\n\n    def orthogonal_learning(self, population, fitness, func):\n        \"\"\"Performs orthogonal learning to enhance population diversity.\"\"\"\n        best_index = np.argmin(fitness)\n        best_individual = population[best_index]\n        \n        for i in range(self.pop_size):\n            if np.random.rand() < self.orthogonal_learning_rate:\n                orthogonal_vector = np.random.normal(0, 1, self.dim)\n                orthogonal_vector /= np.linalg.norm(orthogonal_vector)\n                step_size = np.random.uniform(0, 0.1)  # Adjust the step size\n\n                new_individual = best_individual + step_size * orthogonal_vector\n                new_individual = np.clip(new_individual, func.bounds.lb, func.bounds.ub)\n                \n                new_fitness = func(new_individual)\n                self.budget -= 1\n\n                if new_fitness < fitness[i]:\n                    population[i] = new_individual\n                    fitness[i] = new_fitness\n\n    def restart_population(self, func):\n        \"\"\"Restarts the population with new random individuals.\"\"\"\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n        return population, fitness\n\n    def __call__(self, func):\n        # Initialize population within bounds\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size # Update remaining budget\n\n        # Find the best individual in the initial population\n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index].copy()\n        self.last_improvement = 0 #reset last improvement\n\n        generation = 0\n        while self.budget > 0:\n            generation +=1\n            # Rank population\n            ranked_indices = np.argsort(fitness)\n            \n            for i in range(self.pop_size):\n                # Mutation: Rank-based selection of parents\n                pbest_idx = ranked_indices[np.random.randint(0, max(1, int(0.2 * self.pop_size)))] # Select best 20%\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b = np.random.choice(idxs, 2, replace=False)\n                \n                # Adaptive F and CR\n                F = np.random.normal(self.F_base, 0.1)\n                F = np.clip(F, 0.1, 0.9)\n                CR = np.random.normal(self.CR_base, 0.1)\n                CR = np.clip(CR, 0.1, 0.9)\n                \n                mutant = population[i] + F * (population[pbest_idx] - population[a]) + F * (population[b] - population[i])\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Boundary handling (clip to bounds)\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Evaluation\n                f = func(trial)\n                self.budget -= 1\n                \n                if f < fitness[i]:\n                    delta = fitness[i] - f\n                    fitness[i] = f\n                    population[i] = trial\n                    \n                    # Update global best\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = trial.copy()\n                        self.last_improvement = generation #update last improvement\n\n                    # Update success history for F and CR\n                    self.success_F[self.success_idx] = F\n                    self.success_CR[self.success_idx] = CR\n                    self.success_idx = (self.success_idx + 1) % self.success_history_size\n\n                    # Adjust base values based on recent success\n                    self.F_base = np.mean(self.success_F)\n                    self.CR_base = np.mean(self.success_CR)\n                    self.F_base = np.clip(self.F_base, 0.1, 0.9)\n                    self.CR_base = np.clip(self.CR_base, 0.1, 0.9)\n\n                if self.budget <= 0:\n                    break\n            \n            # Orthogonal learning step\n            self.orthogonal_learning(population, fitness, func)\n            \n            # Restart mechanism if no improvement for a while\n            if generation - self.last_improvement > self.restart_trigger and self.budget > self.pop_size:\n                population, fitness = self.restart_population(func)\n                best_index = np.argmin(fitness)\n                f = fitness[best_index]\n                if f < self.f_opt:\n                    self.f_opt = f\n                    self.x_opt = population[best_index].copy()\n                self.last_improvement = generation\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "The algorithm AdaptiveDifferentialEvolution scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["ca08a41a-418b-4ee4-ba18-aaa3d01359f5"], "operator": null, "metadata": {"aucs": [0]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "12a3c812-87be-4ccf-8881-a78c6e0e4c98", "fitness": "-inf", "name": "AdaptiveConstrictionPSO", "description": "Adaptive PSO with dynamic population size adjustment, constriction factor adaptation based on success, and velocity mutation for exploration.", "code": "import numpy as np\n\nclass AdaptiveConstrictionPSO:\n    def __init__(self, budget=10000, dim=10, pop_size_init=20, w_init=0.9, w_final=0.4, c1=2.0, c2=2.0, constriction_factor=0.72984, adapt_pop_interval=500):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size_init = pop_size_init\n        self.pop_size = pop_size_init\n        self.w_init = w_init  # Initial inertia weight\n        self.w_final = w_final # Final inertia weight\n        self.c1 = c1  # Cognitive coefficient\n        self.c2 = c2  # Social coefficient\n        self.constriction_factor = constriction_factor # Default constriction factor\n        self.swarm = None\n        self.velocities = None\n        self.personal_best_positions = None\n        self.personal_best_fitness = None\n        self.global_best_position = None\n        self.global_best_fitness = np.inf\n        self.evaluations = 0\n        self.iteration = 0\n        self.success_history = []\n        self.adapt_pop_interval = adapt_pop_interval\n\n    def initialize_swarm(self, func):\n        self.swarm = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.velocities = np.random.uniform(-0.1 * (func.bounds.ub - func.bounds.lb), 0.1 * (func.bounds.ub - func.bounds.lb), size=(self.pop_size, self.dim))\n        self.personal_best_positions = self.swarm.copy()\n        self.personal_best_fitness = np.array([func(x) for x in self.swarm])\n        self.evaluations += self.pop_size\n\n        self.global_best_position = self.personal_best_positions[np.argmin(self.personal_best_fitness)].copy()\n        self.global_best_fitness = np.min(self.personal_best_fitness)\n\n    def update_velocities(self, func):\n        # Linearly decreasing inertia weight\n        w = self.w_init - (self.w_init - self.w_final) * (self.iteration / self.budget)\n\n        for i in range(self.pop_size):\n            r1 = np.random.rand(self.dim)\n            r2 = np.random.rand(self.dim)\n\n            cognitive_component = self.c1 * r1 * (self.personal_best_positions[i] - self.swarm[i])\n            social_component = self.c2 * r2 * (self.global_best_position - self.swarm[i])\n\n            self.velocities[i] = self.constriction_factor * (w * self.velocities[i] + cognitive_component + social_component)\n\n            # Velocity Mutation: Add small random velocity component for exploration\n            if np.random.rand() < 0.05:  # 5% probability\n                 self.velocities[i] += np.random.uniform(-0.01, 0.01, self.dim)\n                 self.velocities[i] = np.clip(self.velocities[i], -0.2 * (func.bounds.ub - func.bounds.lb), 0.2 * (func.bounds.ub - func.bounds.lb))\n\n\n    def update_positions(self, func):\n        self.swarm += self.velocities\n        self.swarm = np.clip(self.swarm, func.bounds.lb, func.bounds.ub)\n\n        fitness = np.array([func(x) for x in self.swarm])\n        self.evaluations += self.pop_size\n\n        for i in range(self.pop_size):\n            if fitness[i] < self.personal_best_fitness[i]:\n                self.personal_best_fitness[i] = fitness[i]\n                self.personal_best_positions[i] = self.swarm[i].copy()\n\n                if fitness[i] < self.global_best_fitness:\n                    self.global_best_fitness = fitness[i]\n                    self.global_best_position = self.swarm[i].copy()\n                    self.success_history.append(self.iteration)\n\n    def adapt_constriction(self):\n         #Adapt constriction based on success history\n        if len(self.success_history) > 1:\n            recent_success = [x for x in self.success_history if x > self.iteration - 500] #Check for success in last 500 iterations\n\n            if len(recent_success) > 0:\n                #Increase constriction if recent successes\n                self.constriction_factor = min(0.75, self.constriction_factor + 0.01)\n\n            else:\n                #Decrease constriction to increase exploration\n                self.constriction_factor = max(0.6, self.constriction_factor - 0.01)\n\n\n    def adapt_population(self):\n        if self.iteration > 0 and self.iteration % self.adapt_pop_interval == 0:\n            if len(self.success_history) > 1:\n                recent_success = [x for x in self.success_history if x > self.iteration - self.adapt_pop_interval]\n\n                if len(recent_success) > 0:\n                    #reduce pop size if finding good solutions frequently\n                    self.pop_size = max(10, int(self.pop_size * 0.9))\n\n                else:\n                    #increase pop size to try new areas\n                    self.pop_size = min(self.pop_size_init, int(self.pop_size * 1.1)) #limit the max pop size.\n\n                #Re-initialize swarm if pop_size changes\n                if self.pop_size != len(self.swarm):\n                    self.initialize_swarm(func) #Reinitialize using the function and new pop size.\n            else:\n                #if no successes increase pop size\n                self.pop_size = min(self.pop_size_init, int(self.pop_size * 1.1))\n\n    def __call__(self, func):\n        self.initialize_swarm(func)\n\n        while self.evaluations < self.budget:\n            self.update_velocities(func)\n            self.update_positions(func)\n            self.adapt_constriction() # Adapt the constriction factor\n            self.adapt_population() #Adapt the population size\n            self.iteration += self.pop_size\n            if self.evaluations >= self.budget:\n                break\n        return self.global_best_fitness, self.global_best_position", "configspace": "", "generation": 7, "feedback": "In the code, line 100, in adapt_population, the following error occurred:\nNameError: name 'func' is not defined\nOn line: self.initialize_swarm(func) #Reinitialize using the function and new pop size.", "error": "In the code, line 100, in adapt_population, the following error occurred:\nNameError: name 'func' is not defined\nOn line: self.initialize_swarm(func) #Reinitialize using the function and new pop size.", "parent_ids": ["520eb34a-3e7b-4531-bdec-f93eae7ec3c7"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "97bf552c-c025-465c-a995-a828dc20beea", "fitness": 0.5584425783233581, "name": "AdaptiveGaussianCauchySearchLSHADE", "description": "Population-based optimization with adaptive Gaussian and Cauchy mutations, L-SHADE inspired step size adaptation, and simplified archive-based exploration.", "code": "import numpy as np\n\nclass AdaptiveGaussianCauchySearchLSHADE:\n    def __init__(self, budget=10000, dim=10, population_size=20, initial_gaussian_std=0.5, initial_cauchy_std=1.0, success_threshold=0.25, initial_cauchy_prob=0.1, memory_size=5):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = population_size\n        self.initial_gaussian_std = initial_gaussian_std\n        self.initial_cauchy_std = initial_cauchy_std\n        self.success_threshold = success_threshold\n        self.cauchy_prob = initial_cauchy_prob\n        self.gaussian_std = initial_gaussian_std\n        self.cauchy_std = initial_cauchy_std\n        self.memory_size = memory_size\n        self.memory_gaussian_std = np.full(memory_size, initial_gaussian_std)\n        self.memory_cauchy_std = np.full(memory_size, initial_cauchy_std)\n        self.memory_idx = 0\n        self.archive = []\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        eval_count = 0\n\n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.population_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        eval_count += self.population_size\n\n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index]\n\n        while eval_count < self.budget:\n            new_samples = np.zeros((self.population_size, self.dim))\n            new_fitness = np.zeros(self.population_size)\n\n            for i in range(self.population_size):\n                # Select mutation strategy based on probability\n                if np.random.rand() < self.cauchy_prob:\n                    # Cauchy mutation for exploration\n                    mutation = np.random.standard_cauchy(size=self.dim) * self.cauchy_std\n                    new_sample = population[i] + mutation\n                else:\n                    # Gaussian mutation around the best\n                    new_sample = np.random.normal(loc=self.x_opt, scale=self.gaussian_std, size=self.dim)\n\n                new_sample = np.clip(new_sample, func.bounds.lb, func.bounds.ub)\n                new_samples[i] = new_sample\n                new_fitness[i] = func(new_sample)\n                eval_count += 1\n                \n                if new_fitness[i] < self.f_opt:\n                    self.f_opt = new_fitness[i]\n                    self.x_opt = new_samples[i]\n\n            # Update population and fitness\n            improved = new_fitness < fitness\n            for i in range(self.population_size):\n                 if improved[i]:\n                    if len(self.archive) < self.population_size:\n                        self.archive.append(population[i].copy())  # Simplified archive handling\n                    else:\n                        self.archive[np.random.randint(0, len(self.archive))] = population[i].copy()\n\n                    population[i] = new_samples[i]\n                    fitness[i] = new_fitness[i]\n\n            best_index = np.argmin(fitness)\n            self.x_opt = population[best_index]\n            self.f_opt = fitness[best_index]\n            \n            success_rate = np.mean(improved)  # More stable success rate\n\n            # L-SHADE-like memory update (simplified)\n            self.memory_gaussian_std[self.memory_idx] = self.gaussian_std\n            self.memory_cauchy_std[self.memory_idx] = self.cauchy_std\n            self.memory_idx = (self.memory_idx + 1) % self.memory_size\n\n            #Adapt Cauchy Probability and Step Sizes based on Success\n            if success_rate > self.success_threshold:\n                self.gaussian_std *= 1.05\n                self.cauchy_std *= 1.02\n                self.cauchy_prob = min(self.cauchy_prob * 1.02, 0.4)\n            else:\n                self.gaussian_std *= 0.95\n                self.cauchy_std *= 0.98\n                self.cauchy_prob = max(self.cauchy_prob * 0.98, 0.01)\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "The algorithm AdaptiveGaussianCauchySearchLSHADE scored 0.558 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["33115b67-c003-4fdb-8f3e-63242bb71457"], "operator": null, "metadata": {"aucs": [0.14068629359454132, 0.1837649241648831, 0.7693165423670006, 0.19995152526551552, 0.8110342528132182, 0.8227116979276307, 0.32164323656608285, 0.6734027749609466, 0.7730901038379518, 0.1701378485235897, 0.9279649698053068, 0.9971519623564171, 0.2258178459647665, 0.6669391230044104, 0.9426639967144732, 0.38400437013532585, 0.4063909875961731, 0.8825695125524851, 0.35454033412386854, 0.5150692641925732]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "0b20a372-d5e6-4da8-ba2e-89b36eb59095", "fitness": 0.7680248364110716, "name": "AdaptiveDifferentialEvolution", "description": "Simplified Adaptive Differential Evolution with self-adaptive parameters and a focus on exploitation around the current best solution.", "code": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget=10000, dim=10, pop_size=30, F=0.5, CR=0.7):\n        \"\"\"\n        Args:\n            budget (int): The maximum number of function evaluations.\n            dim (int): The dimensionality of the search space.\n            pop_size (int): The population size.\n            F (float): Mutation factor.\n            CR (float): Crossover rate.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        # Initialize population within bounds\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        # Find the best individual in the initial population\n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index].copy()\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation: Exploit the best solution\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b = np.random.choice(idxs, 2, replace=False)\n\n                mutant = population[i] + self.F * (self.x_opt - population[a]) + self.F * (population[b] - population[i])\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Boundary handling (clip to bounds)\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Evaluation\n                f = func(trial)\n                self.budget -= 1\n\n                if f < fitness[i]:\n                    fitness[i] = f\n                    population[i] = trial\n\n                    # Update global best\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = trial.copy()\n\n                if self.budget <= 0:\n                    break\n\n            # Self-adaptation of F and CR (simplified)\n            self.F = np.clip(np.random.normal(0.5, 0.1), 0.1, 0.9)\n            self.CR = np.clip(np.random.normal(0.7, 0.1), 0.1, 0.9)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "The algorithm AdaptiveDifferentialEvolution scored 0.768 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["ca08a41a-418b-4ee4-ba18-aaa3d01359f5"], "operator": null, "metadata": {"aucs": [0.34625060343136693, 0.7999224575758979, 0.8635171244978219, 0.9452961305002022, 0.8765408093994155, 0.9124049134064466, 0.33896218724164384, 0.8542118292006768, 0.8900648787089324, 0.8496888275454967, 0.9416018768671305, 0.9908206522947915, 0.3828348118667779, 0.8779459999006002, 0.9471674383792202, 0.9093120825676401, 0.8519996827528526, 0.9303423795280442, 0.3453946044148427, 0.5062174381416298]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "ecd6318b-e8bd-45e3-bc6b-f06076a6ff74", "fitness": 0.5903729862006267, "name": "AdaptiveGaussianCauchySearch", "description": "An adaptive algorithm that blends Gaussian and Cauchy mutations with a decreasing exploration range, focusing on exploitation as the budget depletes.", "code": "import numpy as np\n\nclass AdaptiveGaussianCauchySearch:\n    def __init__(self, budget=10000, dim=10, population_size=20, initial_std=0.5, cauchy_prob=0.1, exploration_decay=0.999):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = population_size\n        self.initial_std = initial_std\n        self.cauchy_prob = cauchy_prob\n        self.exploration_decay = exploration_decay\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.std = self.initial_std\n        self.exploration_range = 5.0  # Start with the full range\n        eval_count = 0\n\n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.population_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        eval_count += self.population_size\n        \n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index]\n\n        while eval_count < self.budget:\n            new_samples = np.zeros((self.population_size, self.dim))\n            for i in range(self.population_size):\n                if np.random.rand() < self.cauchy_prob:\n                    # Cauchy mutation\n                    mutation = self.x_opt + self.std * np.random.standard_cauchy(size=self.dim) * self.exploration_range\n                else:\n                    # Gaussian mutation\n                    mutation = self.x_opt + self.std * np.random.randn(self.dim) * self.exploration_range\n                new_samples[i] = np.clip(mutation, func.bounds.lb, func.bounds.ub)\n            \n            new_fitness = np.array([func(x) for x in new_samples])\n            eval_count += self.population_size\n\n            # Update best solution\n            for i in range(self.population_size):\n                if new_fitness[i] < self.f_opt:\n                    self.f_opt = new_fitness[i]\n                    self.x_opt = new_samples[i]\n\n            # Adapt standard deviation and exploration range\n            if np.mean(new_fitness < fitness) > 0.2:\n                self.std *= 1.05\n            else:\n                self.std *= 0.95\n                \n            self.exploration_range *= self.exploration_decay # Reduce exploration range over time\n\n\n            fitness = np.minimum(fitness, new_fitness) #replace only if better\n            best_index = np.argmin(fitness)\n\n            if eval_count >= self.budget:\n                break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "The algorithm AdaptiveGaussianCauchySearch scored 0.590 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["65ca3433-814b-4d91-9fcf-c34d334aadd4"], "operator": null, "metadata": {"aucs": [0.2383105722743255, 0.18524444939208073, 0.6841765865039948, 0.8599082953183383, 0.7088282808966233, 0.7451253031070204, 0.3291234815349995, 0.6462356147897639, 0.7173051876504467, 0.2048831444539887, 0.8519866725791758, 0.9971338526503344, 0.37605334794509615, 0.29554087419094344, 0.8615738129429985, 0.750276094540729, 0.2964200243009264, 0.8290205270460838, 0.6939311825891632, 0.5363824193055029]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "598a1c65-07e0-4931-8d19-01ceafbf9712", "fitness": 0.0, "name": "AdaptiveGaussianCauchySearchOrthogonal", "description": "Adaptive Gaussian Cauchy Search with Orthogonal Learning, dynamically adjusting mutation parameters and integrating orthogonal learning for enhanced exploration and exploitation.", "code": "import numpy as np\n\nclass AdaptiveGaussianCauchySearchOrthogonal:\n    def __init__(self, budget=10000, dim=10, population_size=20, initial_gaussian_std=0.5, initial_cauchy_std=1.0, success_threshold=0.25, initial_cauchy_prob=0.1, orthogonal_learning_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = population_size\n        self.initial_gaussian_std = initial_gaussian_std\n        self.initial_cauchy_std = initial_cauchy_std\n        self.success_threshold = success_threshold\n        self.cauchy_prob = initial_cauchy_prob\n        self.gaussian_std = initial_gaussian_std\n        self.cauchy_std = initial_cauchy_std\n        self.orthogonal_learning_rate = orthogonal_learning_rate\n\n\n    def orthogonal_design(self, n):\n        \"\"\"\n        Generates an orthogonal design of size n x n using the Hadamard matrix approach.\n        Only works for n that are powers of 2.\n        \"\"\"\n        if not (n > 0 and (n & (n - 1) == 0)):\n            raise ValueError(\"n must be a power of 2\")\n\n        if n == 1:\n            return np.array([[1]])\n\n        H = np.array([[1, 1], [1, -1]])\n        while H.shape[0] < n:\n            H = np.kron(H, np.array([[1, 1], [1, -1]]))\n        \n        return H\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        eval_count = 0\n\n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.population_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        eval_count += self.population_size\n\n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index]\n\n        while eval_count < self.budget:\n            new_samples = np.zeros((self.population_size, self.dim))\n            new_fitness = np.zeros(self.population_size)\n\n            for i in range(self.population_size):\n                # Select mutation strategy based on probability\n                if np.random.rand() < self.cauchy_prob:\n                    # Cauchy mutation for exploration\n                    mutation = np.random.standard_cauchy(size=self.dim) * self.cauchy_std\n                    new_sample = population[i] + mutation # Mutate around current individual\n                else:\n                    # Gaussian mutation around the best\n                    new_sample = np.random.normal(loc=self.x_opt, scale=self.gaussian_std, size=self.dim)\n\n                new_sample = np.clip(new_sample, func.bounds.lb, func.bounds.ub)\n                new_samples[i] = new_sample\n                new_fitness[i] = func(new_sample)\n                eval_count += 1\n                \n                if new_fitness[i] < self.f_opt:\n                    self.f_opt = new_fitness[i]\n                    self.x_opt = new_samples[i]\n\n            improved = new_fitness < fitness\n            num_improved = np.sum(improved)\n\n            # Update population and fitness\n            for i in range(self.population_size):\n                 if new_fitness[i] < fitness[i]:\n                    population[i] = new_samples[i]\n                    fitness[i] = new_fitness[i]\n            \n            best_index = np.argmin(fitness)\n            self.x_opt = population[best_index]\n            self.f_opt = fitness[best_index]\n\n\n            # Adapt standard deviations and cauchy probability\n            success_rate = num_improved / self.population_size\n            \n            if success_rate > self.success_threshold:\n                self.gaussian_std *= 1.1\n                self.cauchy_std *= 1.05\n                self.cauchy_prob = min(self.cauchy_prob * 1.05, 0.9) #Increase chance of Cauchy\n            else:\n                self.gaussian_std *= 0.9\n                self.cauchy_std *= 0.95\n                self.cauchy_prob = max(self.cauchy_prob * 0.95, 0.1) #Decrease chance of Cauchy\n            \n            # Orthogonal Learning\n            if self.dim > 1:\n                if eval_count + self.dim <= self.budget:\n                    orthogonal_matrix = self.orthogonal_design(2**int(np.ceil(np.log2(self.dim))))\n                    orthogonal_matrix = orthogonal_matrix[:self.dim, :self.dim]\n                    \n                    # Perturb the current best solution along orthogonal directions\n                    orthogonal_samples = np.zeros((self.dim, self.dim))\n                    orthogonal_fitness = np.zeros(self.dim)\n                    \n                    for j in range(self.dim):\n                        perturbation = orthogonal_matrix[j] * self.orthogonal_learning_rate * self.gaussian_std\n                        new_sample = self.x_opt + perturbation\n                        new_sample = np.clip(new_sample, func.bounds.lb, func.bounds.ub)\n                        orthogonal_samples[j] = new_sample\n                        orthogonal_fitness[j] = func(new_sample)\n                        eval_count += 1\n                    \n                    best_orthogonal_index = np.argmin(orthogonal_fitness)\n                    if orthogonal_fitness[best_orthogonal_index] < self.f_opt:\n                        self.f_opt = orthogonal_fitness[best_orthogonal_index]\n                        self.x_opt = orthogonal_samples[best_orthogonal_index]\n\n\n            if eval_count >= self.budget:\n                break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "The algorithm AdaptiveGaussianCauchySearchOrthogonal scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["33115b67-c003-4fdb-8f3e-63242bb71457"], "operator": null, "metadata": {"aucs": [0]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "a42186a0-8c0b-43fa-93b2-ee93787a11b3", "fitness": 0.6042148498984233, "name": "AdaptiveGaussianCauchySearch", "description": "An adaptive algorithm using Gaussian and Cauchy mutations, dynamically adjusting mutation rates based on population diversity and success, while focusing exploration near the best solution and reducing exploration range over time.", "code": "import numpy as np\n\nclass AdaptiveGaussianCauchySearch:\n    def __init__(self, budget=10000, dim=10, population_size=20, initial_std=0.5, cauchy_prob=0.1, exploration_decay=0.999):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = population_size\n        self.initial_std = initial_std\n        self.cauchy_prob = cauchy_prob\n        self.exploration_decay = exploration_decay\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.std = self.initial_std\n        self.exploration_range = 5.0\n        eval_count = 0\n\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.population_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        eval_count += self.population_size\n        \n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index]\n\n        while eval_count < self.budget:\n            new_samples = np.zeros((self.population_size, self.dim))\n            new_fitness = np.zeros(self.population_size)\n\n            for i in range(self.population_size):\n                # Blend Gaussian and Cauchy mutations, favoring the best solution\n                if np.random.rand() < self.cauchy_prob:\n                    mutation = self.x_opt + self.std * np.random.standard_cauchy(size=self.dim) * self.exploration_range\n                else:\n                    mutation = self.x_opt + self.std * np.random.randn(self.dim) * self.exploration_range\n                \n                new_sample = np.clip(mutation, func.bounds.lb, func.bounds.ub)\n                new_samples[i] = new_sample\n                new_fitness[i] = func(new_sample)\n                eval_count += 1\n\n                # Update best solution immediately if improvement found\n                if new_fitness[i] < self.f_opt:\n                    self.f_opt = new_fitness[i]\n                    self.x_opt = new_sample\n\n            # Adapt standard deviation based on success rate\n            success_rate = np.mean(new_fitness < fitness)\n            if success_rate > 0.2:\n                self.std *= 1.05\n            else:\n                self.std *= 0.95\n            \n            self.exploration_range *= self.exploration_decay # Reduce exploration range over time\n            fitness = np.minimum(fitness, new_fitness)  # replace only if better\n            best_index = np.argmin(fitness)\n            \n            if eval_count >= self.budget:\n                break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "The algorithm AdaptiveGaussianCauchySearch scored 0.604 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["ecd6318b-e8bd-45e3-bc6b-f06076a6ff74"], "operator": null, "metadata": {"aucs": [0.25157265368460713, 0.2098701619419644, 0.718444362593418, 0.8644288115708785, 0.7146998110261408, 0.7489562180908506, 0.3072577289858054, 0.633033032659493, 0.6991144576093196, 0.69681857765048, 0.8691004403248027, 0.9945674571016868, 0.21789707592764196, 0.31984407873138676, 0.9100212577615495, 0.7114555037669085, 0.5760648456962358, 0.8187752330938802, 0.31733728458187205, 0.505038005169544]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "6696a900-5d12-4e06-bc95-d36db268a812", "fitness": 0.0, "name": "AdaptiveGaussianCauchySearchEnhanced", "description": "Adaptively combines Gaussian and Cauchy mutations with a decaying exploration range and a self-adjusting standard deviation, plus a success-history based mutation strategy and a population update mechanism that encourages diversity.", "code": "import numpy as np\n\nclass AdaptiveGaussianCauchySearchEnhanced:\n    def __init__(self, budget=10000, dim=10, population_size=20, initial_std=0.5, cauchy_prob=0.1, exploration_decay=0.999, success_history_size=10):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = population_size\n        self.initial_std = initial_std\n        self.cauchy_prob = cauchy_prob\n        self.exploration_decay = exploration_decay\n        self.success_history_size = success_history_size\n        self.success_history = []\n        self.cr_history = [] #History for CR\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.std = self.initial_std\n        self.exploration_range = 5.0  # Start with the full range\n\n\n    def __call__(self, func):\n        eval_count = 0\n\n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.population_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        eval_count += self.population_size\n        \n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index]\n\n        while eval_count < self.budget:\n            new_samples = np.zeros((self.population_size, self.dim))\n            new_fitness = np.zeros(self.population_size)\n            \n            for i in range(self.population_size):\n                # Success-history based adaptation for CR\n                if self.cr_history:\n                    cr_mean = np.mean(self.cr_history)\n                    cr = np.clip(np.random.normal(cr_mean, 0.1), 0, 1)\n                else:\n                    cr = 0.5  # Initial CR value\n\n                # Mutation strategy\n                if np.random.rand() < self.cauchy_prob:\n                    # Cauchy mutation\n                    mutation = self.x_opt + self.std * np.random.standard_cauchy(size=self.dim) * self.exploration_range\n                else:\n                    # Gaussian mutation\n                    mutation = self.x_opt + self.std * np.random.randn(self.dim) * self.exploration_range\n\n                # Crossover with current individual (differential evolution style)\n                j_rand = np.random.randint(self.dim)\n                for j in range(self.dim):\n                    if np.random.rand() > cr and j != j_rand:\n                        mutation[j] = population[i, j]\n\n                new_samples[i] = np.clip(mutation, func.bounds.lb, func.bounds.ub)\n                new_fitness[i] = func(new_samples[i])\n                eval_count += 1\n\n            # Update best solution\n            for i in range(self.population_size):\n                if new_fitness[i] < self.f_opt:\n                    self.f_opt = new_fitness[i]\n                    self.x_opt = new_samples[i]\n                \n\n            # Population update with diversity preservation\n            for i in range(self.population_size):\n                if new_fitness[i] < fitness[i]:\n                    # Update individual if better\n                    population[i] = new_samples[i]\n                    fitness[i] = new_fitness[i]\n\n                    # Update success history for CR\n                    delta = np.abs(fitness[i] - new_fitness[i])\n                    self.success_history.append(delta)\n                    self.cr_history.append(cr) #Store CR value\n\n                    if len(self.success_history) > self.success_history_size:\n                        self.success_history.pop(0)\n                        self.cr_history.pop(0)\n\n                else:\n                    # Encourage diversity: replace with random individual with small probability\n                    if np.random.rand() < 0.05:\n                        population[i] = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                        fitness[i] = func(population[i])\n                        eval_count += 1\n\n            # Adapt standard deviation and exploration range\n            if np.mean(new_fitness < fitness) > 0.2:\n                self.std *= 1.05\n            else:\n                self.std *= 0.95\n                \n            self.exploration_range *= self.exploration_decay # Reduce exploration range over time\n\n            best_index = np.argmin(fitness)\n\n            if eval_count >= self.budget:\n                break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "The algorithm AdaptiveGaussianCauchySearchEnhanced scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["ecd6318b-e8bd-45e3-bc6b-f06076a6ff74"], "operator": null, "metadata": {"aucs": [0]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "c3e0a5d6-9fa6-4549-80c1-fd082675b144", "fitness": 0.0, "name": "AdaptiveHybridSearchOrthogonal", "description": "An adaptive hybrid algorithm employing Gaussian and Cauchy mutations with dynamic step size control and a simple archive mechanism for enhanced exploration and exploitation, coupled with orthogonal learning for improved search efficiency.", "code": "import numpy as np\n\nclass AdaptiveHybridSearchOrthogonal:\n    def __init__(self, budget=10000, dim=10, population_size=20, initial_gaussian_std=0.5, initial_cauchy_std=1.0, success_threshold=0.25, initial_cauchy_prob=0.1, archive_size=10, orthogonal_sample_size=5):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = population_size\n        self.initial_gaussian_std = initial_gaussian_std\n        self.initial_cauchy_std = initial_cauchy_std\n        self.success_threshold = success_threshold\n        self.cauchy_prob = initial_cauchy_prob\n        self.gaussian_std = initial_gaussian_std\n        self.cauchy_std = initial_cauchy_std\n        self.archive_size = archive_size\n        self.archive = []\n        self.orthogonal_sample_size = orthogonal_sample_size\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        eval_count = 0\n\n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.population_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        eval_count += self.population_size\n\n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index]\n\n        while eval_count < self.budget:\n            new_samples = np.zeros((self.population_size, self.dim))\n            new_fitness = np.zeros(self.population_size)\n\n            for i in range(self.population_size):\n                # Select mutation strategy based on probability\n                if np.random.rand() < self.cauchy_prob:\n                    # Cauchy mutation for exploration\n                    mutation = np.random.standard_cauchy(size=self.dim) * self.cauchy_std\n                    new_sample = population[i] + mutation  # Mutate around current individual\n                else:\n                    # Gaussian mutation around the best\n                    new_sample = np.random.normal(loc=self.x_opt, scale=self.gaussian_std, size=self.dim)\n\n                new_sample = np.clip(new_sample, func.bounds.lb, func.bounds.ub)\n\n                # Orthogonal Learning: Sample around the new sample\n                orthogonal_samples = np.random.normal(loc=new_sample, scale=0.1 * self.gaussian_std, size=(self.orthogonal_sample_size, self.dim))\n                orthogonal_samples = np.clip(orthogonal_samples, func.bounds.lb, func.bounds.ub)\n                orthogonal_fitness = np.array([func(x) for x in orthogonal_samples])\n                eval_count += self.orthogonal_sample_size\n\n                best_orthogonal_index = np.argmin(orthogonal_fitness)\n                if orthogonal_fitness[best_orthogonal_index] < func(new_sample):  # Evaluate only once!\n                    new_sample = orthogonal_samples[best_orthogonal_index]\n                    \n                new_samples[i] = new_sample\n                new_fitness[i] = func(new_sample) # Evaluate only once!\n                eval_count += 1\n                \n                if new_fitness[i] < self.f_opt:\n                    self.f_opt = new_fitness[i]\n                    self.x_opt = new_samples[i]\n\n            improved = new_fitness < fitness\n            num_improved = np.sum(improved)\n\n            # Update population and fitness\n            for i in range(self.population_size):\n                if new_fitness[i] < fitness[i]:\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(population[i].copy())  # Add replaced solutions to the archive\n                    else:\n                         self.archive[np.random.randint(0,self.archive_size)] = population[i].copy()\n                    population[i] = new_samples[i]\n                    fitness[i] = new_fitness[i]\n            \n            best_index = np.argmin(fitness)\n            self.x_opt = population[best_index]\n            self.f_opt = fitness[best_index]\n\n\n            # Adapt standard deviations and cauchy probability\n            success_rate = num_improved / self.population_size\n\n            if success_rate > self.success_threshold:\n                self.gaussian_std *= 1.1\n                self.cauchy_std *= 1.05\n                self.cauchy_prob = min(self.cauchy_prob * 1.05, 0.9)  # Increase chance of Cauchy\n            else:\n                self.gaussian_std *= 0.9\n                self.cauchy_std *= 0.95\n                self.cauchy_prob = max(self.cauchy_prob * 0.95, 0.1)  # Decrease chance of Cauchy\n\n            # Archive-based learning: perturb based on a random archive member\n            if self.archive and np.random.rand() < 0.1:\n                archive_member = self.archive[np.random.randint(0, len(self.archive))]\n                population[np.random.randint(0, self.population_size)] = archive_member + np.random.normal(0, 0.01 * self.gaussian_std, self.dim) #perturbation based on archive\n\n            if eval_count >= self.budget:\n                break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "The algorithm AdaptiveHybridSearchOrthogonal scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["33115b67-c003-4fdb-8f3e-63242bb71457"], "operator": null, "metadata": {"aucs": [0]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "1d24ca94-f803-4845-a12a-19e3a4f7bc44", "fitness": 0.0, "name": "AdaptiveGaussianCauchySearchOL", "description": "An adaptive population-based algorithm that blends Gaussian and Cauchy mutations with a self-adaptive exploration range and step size, using a success rate based adaptation and orthogonal learning to improve exploration and exploitation.", "code": "import numpy as np\n\nclass AdaptiveGaussianCauchySearchOL:\n    def __init__(self, budget=10000, dim=10, population_size=20, initial_std=0.5, cauchy_prob=0.1, exploration_decay=0.999, success_rate_threshold=0.2):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = population_size\n        self.initial_std = initial_std\n        self.cauchy_prob = cauchy_prob\n        self.exploration_decay = exploration_decay\n        self.success_rate_threshold = success_rate_threshold\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.std = self.initial_std\n        self.exploration_range = 5.0  # Start with the full range\n        eval_count = 0\n\n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.population_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        eval_count += self.population_size\n        \n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index]\n\n        archive = [] # store solutions which will be used for orthogonal learning\n\n        while eval_count < self.budget:\n            new_samples = np.zeros((self.population_size, self.dim))\n            new_fitness = np.zeros(self.population_size)\n            improvements = 0\n            \n            for i in range(self.population_size):\n                # Mutation\n                if np.random.rand() < self.cauchy_prob:\n                    # Cauchy mutation\n                    mutation = self.x_opt + self.std * np.random.standard_cauchy(size=self.dim) * self.exploration_range\n                else:\n                    # Gaussian mutation\n                    mutation = self.x_opt + self.std * np.random.randn(self.dim) * self.exploration_range\n                \n                mutation = np.clip(mutation, func.bounds.lb, func.bounds.ub)\n                new_samples[i] = mutation\n                new_fitness[i] = func(mutation)\n                eval_count += 1\n            \n            # Update best solution\n            for i in range(self.population_size):\n                if new_fitness[i] < self.f_opt:\n                    self.f_opt = new_fitness[i]\n                    self.x_opt = new_samples[i]\n                    improvements += 1\n\n            # Adapt standard deviation and exploration range based on success rate\n            success_rate = improvements / self.population_size\n            if success_rate > self.success_rate_threshold:\n                self.std *= 1.05  # Increase exploration\n            else:\n                self.std *= 0.95  # Decrease exploration\n            \n            self.exploration_range *= self.exploration_decay  # Reduce exploration range over time\n            \n            # Orthogonal Learning\n            if len(archive) > 5 and np.random.rand() < 0.1:  # Use archive for orthogonal learning with a probability\n                # Select two parents from archive and current population\n                parent1 = archive[np.random.randint(0, len(archive))]\n                parent2 = population[np.random.randint(0, self.population_size)]\n\n                # Generate offspring using orthogonal crossover (simple example)\n                offspring = 0.5 * (parent1 + parent2) + 0.1 * np.random.randn(self.dim) # adding random noise\n\n                offspring = np.clip(offspring, func.bounds.lb, func.bounds.ub)\n                offspring_fitness = func(offspring)\n                eval_count += 1\n\n                if offspring_fitness < self.f_opt:\n                    self.f_opt = offspring_fitness\n                    self.x_opt = offspring\n            \n            # Update archive with diverse and promising solutions\n            for i in range(self.population_size):\n                if new_fitness[i] < np.mean(fitness):\n                    archive.append(new_samples[i])\n            \n            if len(archive) > 20:\n                archive = archive[-20:]  # Limit the size of the archive\n\n            # Update the population\n            for i in range(self.population_size):\n                if new_fitness[i] < fitness[i]:\n                    population[i] = new_samples[i]\n                    fitness[i] = new_fitness[i]\n\n            best_index = np.argmin(fitness)\n\n\n            if eval_count >= self.budget:\n                break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "The algorithm AdaptiveGaussianCauchySearchOL scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["ecd6318b-e8bd-45e3-bc6b-f06076a6ff74"], "operator": null, "metadata": {"aucs": [0]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "4f882f64-ca6f-4d33-b1a5-08b945ed72d5", "fitness": 0.6597887237157376, "name": "AdaptiveDifferentialEvolution", "description": "Simplified Adaptive Differential Evolution with a focus on population diversity maintenance through dynamic parameter adaptation and a restart mechanism.", "code": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget=10000, dim=10, pop_size=40, F=0.5, CR=0.7, restart_prob=0.05):\n        \"\"\"\n        Args:\n            budget (int): The maximum number of function evaluations.\n            dim (int): The dimensionality of the search space.\n            pop_size (int): The population size.\n            F (float): Mutation factor.\n            CR (float): Crossover rate.\n            restart_prob (float): Probability of restarting a solution.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.restart_prob = restart_prob\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        # Initialize population within bounds\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size # Update remaining budget\n\n        # Find the best individual in the initial population\n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index].copy()\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = np.random.choice(idxs, 3, replace=False)\n                mutant = population[a] + self.F * (population[b] - population[c])\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Boundary handling (clip to bounds)\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Restart mechanism (inject diversity)\n                if np.random.rand() < self.restart_prob:\n                    trial = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n\n                # Evaluation\n                f = func(trial)\n                self.budget -= 1\n                \n                if f < fitness[i]:\n                    fitness[i] = f\n                    population[i] = trial\n                    \n                    # Update global best\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = trial.copy()\n\n                if self.budget <= 0:\n                    break\n            \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "The algorithm AdaptiveDifferentialEvolution scored 0.660 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["ca08a41a-418b-4ee4-ba18-aaa3d01359f5"], "operator": null, "metadata": {"aucs": [0.23004602018009102, 0.48897253042035393, 0.6621168807251933, 0.8428900913324171, 0.7377572412214055, 0.7889919595137265, 0.6187646762482246, 0.6692844302339489, 0.7800521791853573, 0.6340847445328388, 0.8140820568930501, 0.9974471989413154, 0.37960893168169885, 0.6707855961019082, 0.8808657262767041, 0.7859145633767941, 0.5797325639705087, 0.8603044136809979, 0.2535159763429209, 0.5205566934552945]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "f8a20225-ad13-4f67-915d-b4160e0f83ce", "fitness": 0.5677707249014452, "name": "MultiStrategyAdaptiveSearch", "description": "An adaptive algorithm that combines multiple mutation strategies (Gaussian, Cauchy, and DE), adjusts mutation probabilities based on success rates, and incorporates a local search step around the best solution.", "code": "import numpy as np\n\nclass MultiStrategyAdaptiveSearch:\n    def __init__(self, budget=10000, dim=10, population_size=20, initial_std=0.5,\n                 cauchy_prob=0.1, de_prob=0.1, exploration_decay=0.999, local_search_prob=0.05):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = population_size\n        self.initial_std = initial_std\n        self.cauchy_prob = cauchy_prob\n        self.de_prob = de_prob\n        self.exploration_decay = exploration_decay\n        self.local_search_prob = local_search_prob\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.std = self.initial_std\n        self.exploration_range = 5.0\n        eval_count = 0\n\n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.population_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        eval_count += self.population_size\n\n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index]\n\n        success_cauchy = 0\n        success_de = 0\n        total_cauchy = 0\n        total_de = 0\n\n        while eval_count < self.budget:\n            new_samples = np.zeros((self.population_size, self.dim))\n            for i in range(self.population_size):\n                rand = np.random.rand()\n\n                if rand < self.cauchy_prob:\n                    # Cauchy mutation\n                    mutation = self.x_opt + self.std * np.random.standard_cauchy(size=self.dim) * self.exploration_range\n                    total_cauchy += 1\n                elif rand < self.cauchy_prob + self.de_prob:\n                    # Differential Evolution mutation\n                    idxs = np.random.choice(self.population_size, 3, replace=False)\n                    x_r1, x_r2, x_r3 = population[idxs]\n                    mutation = self.x_opt + 0.8 * (x_r1 - x_r2)\n                    total_de += 1\n                else:\n                    # Gaussian mutation\n                    mutation = self.x_opt + self.std * np.random.randn(self.dim) * self.exploration_range\n\n                mutation = np.clip(mutation, func.bounds.lb, func.bounds.ub)\n\n                # Local search around the best\n                if np.random.rand() < self.local_search_prob:\n                    mutation = self.x_opt + 0.01 * np.random.randn(self.dim)\n                    mutation = np.clip(mutation, func.bounds.lb, func.bounds.ub)\n                \n                new_samples[i] = mutation\n\n            new_fitness = np.array([func(x) for x in new_samples])\n            eval_count += self.population_size\n\n            # Update best solution and strategy success\n            for i in range(self.population_size):\n                if new_fitness[i] < self.f_opt:\n                    if total_cauchy > 0 and rand < self.cauchy_prob:\n                        success_cauchy += 1\n                    elif total_de > 0 and rand >= self.cauchy_prob and rand < self.cauchy_prob + self.de_prob:\n                        success_de += 1\n\n                    self.f_opt = new_fitness[i]\n                    self.x_opt = new_samples[i]\n\n            # Adapt probabilities\n            if total_cauchy > 0:\n                self.cauchy_prob = 0.1 * (1 + success_cauchy / total_cauchy)\n            if total_de > 0:\n                self.de_prob = 0.1 * (1 + success_de / total_de)\n            \n            self.cauchy_prob = np.clip(self.cauchy_prob, 0.01, 0.99)\n            self.de_prob = np.clip(self.de_prob, 0.01, 0.99)\n\n\n            # Adapt standard deviation and exploration range\n            if np.mean(new_fitness < fitness) > 0.2:\n                self.std *= 1.05\n            else:\n                self.std *= 0.95\n\n            self.exploration_range *= self.exploration_decay\n\n            fitness = np.minimum(fitness, new_fitness)\n            best_index = np.argmin(fitness)\n\n            if eval_count >= self.budget:\n                break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "The algorithm MultiStrategyAdaptiveSearch scored 0.568 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["ecd6318b-e8bd-45e3-bc6b-f06076a6ff74"], "operator": null, "metadata": {"aucs": [0.21746001279731986, 0.18619747427107802, 0.7281908010899008, 0.8635903721270916, 0.7124696703379665, 0.7876344760938604, 0.32756392696579095, 0.6560414761358306, 0.7266470909484417, 0.1786576760386832, 0.8900532074507531, 0.9997809659703408, 0.22096026241539968, 0.3149038767477741, 0.8926564131325444, 0.6913422381047329, 0.4281737406937064, 0.8224321156097587, 0.21191094615394246, 0.4987477549439885]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "9bc2e347-536c-45a6-9f38-6aa02a0cd3ba", "fitness": 0.4578904742534792, "name": "EnhancedAdaptiveGaussianCauchySearch", "description": "Enhanced Adaptive Gaussian Cauchy Search with Archive-based mutation and dynamic parameter control using a sigmoid function for smoother adaptation.", "code": "import numpy as np\n\nclass EnhancedAdaptiveGaussianCauchySearch:\n    def __init__(self, budget=10000, dim=10, population_size=20, initial_gaussian_std=0.5, initial_cauchy_std=1.0, archive_size=10, adaptation_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = population_size\n        self.initial_gaussian_std = initial_gaussian_std\n        self.initial_cauchy_std = initial_cauchy_std\n        self.archive_size = archive_size\n        self.archive = []\n        self.gaussian_std = initial_gaussian_std\n        self.cauchy_std = initial_cauchy_std\n        self.adaptation_rate = adaptation_rate\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        eval_count = 0\n\n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.population_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        eval_count += self.population_size\n\n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index]\n\n        while eval_count < self.budget:\n            new_samples = np.zeros((self.population_size, self.dim))\n            new_fitness = np.zeros(self.population_size)\n\n            for i in range(self.population_size):\n                # Select mutation source: best, current, or archive\n                rand = np.random.rand()\n                if rand < 0.33:\n                    mutation_source = self.x_opt\n                elif rand < 0.66:\n                    mutation_source = population[i]\n                elif self.archive:\n                    mutation_source = self.archive[np.random.randint(len(self.archive))]\n                else:\n                    mutation_source = population[i] # Default to current if archive is empty\n\n                # Gaussian or Cauchy mutation\n                if np.random.rand() < 0.5:\n                    mutation = np.random.normal(loc=0, scale=self.gaussian_std, size=self.dim)\n                else:\n                    mutation = np.random.standard_cauchy(size=self.dim) * self.cauchy_std\n\n                new_sample = mutation_source + mutation\n                new_sample = np.clip(new_sample, func.bounds.lb, func.bounds.ub)\n\n                new_samples[i] = new_sample\n                new_fitness[i] = func(new_sample)\n                eval_count += 1\n\n                if new_fitness[i] < self.f_opt:\n                    self.f_opt = new_fitness[i]\n                    self.x_opt = new_samples[i]\n\n            # Update population and archive\n            for i in range(self.population_size):\n                if new_fitness[i] < fitness[i]:\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(population[i].copy())\n                    else:\n                         # Replace a random element in the archive\n                         replace_index = np.random.randint(self.archive_size)\n                         self.archive[replace_index] = population[i].copy()\n                    population[i] = new_samples[i]\n                    fitness[i] = new_fitness[i]\n\n            best_index = np.argmin(fitness)\n            self.x_opt = population[best_index]\n            self.f_opt = fitness[best_index]\n            \n            # Dynamic adaptation of stds using sigmoid\n            improvement_ratio = np.mean(new_fitness < fitness)\n            adaptation_factor = 1 / (1 + np.exp(-10 * (improvement_ratio - 0.5))) # Sigmoid for smooth adaptation\n            \n            self.gaussian_std *= (1 - self.adaptation_rate) + self.adaptation_rate * adaptation_factor\n            self.cauchy_std *= (1 - self.adaptation_rate) + self.adaptation_rate * adaptation_factor\n            \n            self.gaussian_std = max(self.gaussian_std, 0.01)\n            self.cauchy_std = max(self.cauchy_std, 0.01)\n\n\n            if eval_count >= self.budget:\n                break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "The algorithm EnhancedAdaptiveGaussianCauchySearch scored 0.458 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["33115b67-c003-4fdb-8f3e-63242bb71457"], "operator": null, "metadata": {"aucs": [0.14972853593397795, 0.18095969171604065, 0.5496524281212298, 0.25818300647071546, 0.5533598012998089, 0.6161640773511377, 0.36970062696073624, 0.473607025954776, 0.28391075285390166, 0.18377510853773094, 0.8104154477023636, 0.995708605409912, 0.2686581062480792, 0.3928718204716066, 0.7772809614960742, 0.5907988637655925, 0.2644754790115784, 0.7377784059202375, 0.24864780931565578, 0.45213293052842685]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "7e8c3a9a-f4fd-485f-9898-a7435c859f8d", "fitness": 0.7096372551613517, "name": "AdaptiveDifferentialEvolution", "description": "A simplified Adaptive Differential Evolution with self-adaptive parameters and a combined mutation strategy using both current-to-best and random differential vectors, along with a reduced restart probability and population size.", "code": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget=10000, dim=10, pop_size=20, F=0.5, CR=0.7, restart_prob=0.01):\n        \"\"\"\n        Args:\n            budget (int): The maximum number of function evaluations.\n            dim (int): The dimensionality of the search space.\n            pop_size (int): The population size.\n            F (float): Initial mutation factor.\n            CR (float): Initial crossover rate.\n            restart_prob (float): Probability of restarting a solution.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.restart_prob = restart_prob\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        # Initialize population within bounds\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size # Update remaining budget\n\n        # Find the best individual in the initial population\n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index].copy()\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation: Combine current-to-best and random differential\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b = np.random.choice(idxs, 2, replace=False)\n                mutant = population[i] + self.F * (self.x_opt - population[i]) + self.F * (population[a] - population[b])\n\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Boundary handling (clip to bounds)\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Restart mechanism (inject diversity) - reduced probability\n                if np.random.rand() < self.restart_prob:\n                    trial = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n\n                # Evaluation\n                f = func(trial)\n                self.budget -= 1\n                \n                if f < fitness[i]:\n                    fitness[i] = f\n                    population[i] = trial\n                    \n                    # Update global best\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = trial.copy()\n                        self.F = np.clip(np.random.normal(0.5, 0.1), 0.1, 0.9) # Adapt F\n                        self.CR = np.clip(np.random.normal(0.7, 0.1), 0.1, 0.9) # Adapt CR\n\n                if self.budget <= 0:\n                    break\n            \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 9, "feedback": "The algorithm AdaptiveDifferentialEvolution scored 0.710 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["4f882f64-ca6f-4d33-b1a5-08b945ed72d5"], "operator": null, "metadata": {"aucs": [0.42205776771625836, 0.8003502313024453, 0.6417410151239897, 0.963431615965492, 0.9141136132064783, 0.9425246511058669, 0.3442711535714629, 0.8743063775117639, 0.9184847139401203, 0.2210792933048168, 0.9511623334826982, 0.994292396750761, 0.3523148620513503, 0.8836562306810619, 0.9732297108127838, 0.40664315285645647, 0.8429416965482612, 0.9542111145432118, 0.29057986867477126, 0.5013533040769862]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "f0587149-cbe3-4456-8707-59385ed16fb0", "fitness": "-inf", "name": "AdaptiveDifferentialEvolution", "description": "An enhanced Adaptive Differential Evolution with orthogonal learning, adaptive parameter control using success history, and a restart mechanism to escape local optima.", "code": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget=10000, dim=10, pop_size=30, F=0.5, CR=0.7, ortho_group_size=5):\n        \"\"\"\n        Args:\n            budget (int): The maximum number of function evaluations.\n            dim (int): The dimensionality of the search space.\n            pop_size (int): The population size.\n            F (float): Initial mutation factor.\n            CR (float): Initial Crossover rate.\n            ortho_group_size (int): Number of individuals for orthogonal learning.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.ortho_group_size = ortho_group_size\n        self.success_F = []\n        self.success_CR = []\n        self.memory_size = 10\n        self.restart_trigger = 100  # trigger restart every 100 iterations\n\n    def orthogonal_learning(self, population, fitness, func):\n        \"\"\"\n        Orthogonal learning strategy to generate new candidate solutions.\n        \"\"\"\n        best_index = np.argmin(fitness)\n        best_individual = population[best_index]\n        indices = np.argsort(fitness)[:self.ortho_group_size]\n        selected_individuals = population[indices]\n        \n        # Create orthogonal array (simplified: random combinations for now)\n        num_levels = 3  # Example: perturb each dimension by -sigma, 0, +sigma\n        sigma = 0.1 * (func.bounds.ub - func.bounds.lb)  # Example perturbation\n\n        trial_solutions = []\n        for _ in range(self.ortho_group_size):\n            trial = best_individual.copy()\n            for d in range(self.dim):\n                level = np.random.randint(num_levels)\n                if level == 0:\n                    trial[d] += np.random.uniform(-sigma, 0)\n                elif level == 1:\n                    pass  # No change\n                else:\n                    trial[d] += np.random.uniform(0, sigma)\n\n            trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n            trial_solutions.append(trial)\n\n        return np.array(trial_solutions)\n\n\n    def __call__(self, func):\n        # Initialize population within bounds\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        # Find the best individual in the initial population\n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index].copy()\n\n        generation = 0\n        no_improvement_count = 0\n\n        while self.budget > 0:\n            generation += 1\n\n            for i in range(self.pop_size):\n                # Mutation: Exploit the best solution\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b = np.random.choice(idxs, 2, replace=False)\n\n                mutant = population[i] + self.F * (self.x_opt - population[a]) + self.F * (population[b] - population[i])\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Boundary handling (clip to bounds)\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Evaluation\n                f = func(trial)\n                self.budget -= 1\n\n                if f < fitness[i]:\n                    self.success_F.append(self.F)\n                    self.success_CR.append(self.CR)\n                    fitness[i] = f\n                    population[i] = trial\n\n                    # Update global best\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = trial.copy()\n                        no_improvement_count = 0  # Reset counter\n                    else:\n                        no_improvement_count += 1\n                else:\n                    no_improvement_count += 1\n\n                if self.budget <= 0:\n                    break\n\n            # Orthogonal Learning\n            if self.budget > 0:\n                trial_solutions = self.orthogonal_learning(population, fitness, func)\n                for trial in trial_solutions:\n                    f = func(trial)\n                    self.budget -= 1\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = trial.copy()\n                        \n                    if self.budget <= 0:\n                        break\n                if self.budget <= 0:\n                    break\n                    \n            # Adaptive F and CR (using success history)\n            if self.success_F:\n                self.F = np.mean(self.success_F[-self.memory_size:])\n            else:\n                self.F = np.clip(np.random.normal(0.5, 0.1), 0.1, 0.9)  # Fallback\n\n            if self.success_CR:\n                self.CR = np.mean(self.success_CR[-self.memory_size:])\n            else:\n                self.CR = np.clip(np.random.normal(0.7, 0.1), 0.1, 0.9) # Fallback\n            \n            self.F = np.clip(self.F, 0.1, 0.9)\n            self.CR = np.clip(self.CR, 0.1, 0.9)\n\n            # Restart Mechanism\n            if no_improvement_count > self.restart_trigger and self.budget > self.pop_size:\n                # Re-initialize a portion of the population\n                num_reinitialized = self.pop_size // 2\n                indices_to_reinitialize = np.random.choice(self.pop_size, num_reinitialized, replace=False)\n                population[indices_to_reinitialize] = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(num_reinitialized, self.dim))\n                fitness[indices_to_reinitialize] = [func(x) for x in population[indices_to_reinitialize]]\n                self.budget -= num_reinitialized\n                \n                # Update best solution if needed\n                best_index = np.argmin(fitness)\n                if fitness[best_index] < self.f_opt:\n                    self.f_opt = fitness[best_index]\n                    self.x_opt = population[best_index].copy()\n\n                no_improvement_count = 0  # Reset counter after restart\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 9, "feedback": "In the code, line 46, in orthogonal_learning, the following error occurred:\nValueError: setting an array element with a sequence.\nOn line: trial[d] += np.random.uniform(-sigma, 0)", "error": "In the code, line 46, in orthogonal_learning, the following error occurred:\nValueError: setting an array element with a sequence.\nOn line: trial[d] += np.random.uniform(-sigma, 0)", "parent_ids": ["0b20a372-d5e6-4da8-ba2e-89b36eb59095"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "102f3719-ec1b-4850-adb7-92a6ce2df0bf", "fitness": "-inf", "name": "EnhancedAdaptiveDifferentialEvolution", "description": "Enhanced Adaptive Differential Evolution with orthogonal learning and covariance matrix adaptation for improved exploration and exploitation.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDifferentialEvolution:\n    def __init__(self, budget=10000, dim=10, pop_size=40, F=0.5, CR=0.7, restart_prob=0.05, orthogonal_learning_rate=0.1):\n        \"\"\"\n        Args:\n            budget (int): The maximum number of function evaluations.\n            dim (int): The dimensionality of the search space.\n            pop_size (int): The population size.\n            F (float): Mutation factor.\n            CR (float): Crossover rate.\n            restart_prob (float): Probability of restarting a solution.\n            orthogonal_learning_rate (float): Probability of applying orthogonal learning.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.restart_prob = restart_prob\n        self.orthogonal_learning_rate = orthogonal_learning_rate\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.covariance_matrix = np.eye(dim)  # Initialize covariance matrix\n        self.learning_rate_covariance = 0.1  # Learning rate for covariance matrix adaptation\n\n    def __call__(self, func):\n        # Initialize population within bounds\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size # Update remaining budget\n\n        # Find the best individual in the initial population\n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index].copy()\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = np.random.choice(idxs, 3, replace=False)\n                mutant = population[a] + self.F * (population[b] - population[c])\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Boundary handling (clip to bounds)\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Restart mechanism (inject diversity)\n                if np.random.rand() < self.restart_prob:\n                    trial = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n\n                # Orthogonal Learning\n                if np.random.rand() < self.orthogonal_learning_rate:\n                    trial = self.orthogonal_learning(trial, func.bounds.lb, func.bounds.ub)\n\n                # Evaluation\n                f = func(trial)\n                self.budget -= 1\n                \n                if f < fitness[i]:\n                    fitness[i] = f\n                    population[i] = trial\n                    \n                    # Update global best\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = trial.copy()\n\n                if self.budget <= 0:\n                    break\n\n            # Covariance Matrix Adaptation\n            self.update_covariance_matrix(population)\n            \n        return self.f_opt, self.x_opt\n\n    def orthogonal_learning(self, x, lb, ub):\n        \"\"\"\n        Performs orthogonal learning to generate a new candidate solution.\n        \"\"\"\n        basis = np.random.randn(self.dim, self.dim)  # Generate a random orthogonal basis\n        Q, _ = np.linalg.qr(basis)\n\n        # Sample a new solution along each orthogonal direction\n        candidates = []\n        for i in range(self.dim):\n            direction = Q[:, i]\n            step_size = np.random.uniform(-1.0, 1.0)\n            candidate = x + step_size * direction\n            candidate = np.clip(candidate, lb, ub)\n            candidates.append(candidate)\n        \n        # Select the best candidate\n        best_candidate = min(candidates, key=lambda c: self.evaluate_candidate(c))\n        return best_candidate\n\n    def evaluate_candidate(self, candidate):\n        \"\"\"\n        Evaluates a candidate solution and decrements the budget.\n        \"\"\"\n        if self.budget > 0:\n            f = func(candidate) # Accessing func within the class instance\n            self.budget -= 1\n            return f\n        else:\n            return np.inf # Return a high value if budget is exhausted\n\n    def update_covariance_matrix(self, population):\n        \"\"\"\n        Updates the covariance matrix based on the current population.\n        \"\"\"\n        mean = np.mean(population, axis=0)\n        diffs = population - mean\n        self.covariance_matrix = (1 - self.learning_rate_covariance) * self.covariance_matrix + \\\n                                 self.learning_rate_covariance * (diffs.T @ diffs) / self.pop_size\n", "configspace": "", "generation": 9, "feedback": "In the code, line 108, in evaluate_candidate, the following error occurred:\nNameError: name 'func' is not defined\nOn line: f = func(candidate) # Accessing func within the class instance", "error": "In the code, line 108, in evaluate_candidate, the following error occurred:\nNameError: name 'func' is not defined\nOn line: f = func(candidate) # Accessing func within the class instance", "parent_ids": ["4f882f64-ca6f-4d33-b1a5-08b945ed72d5"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "b780ad63-3adc-4473-9f08-ef08fa38c964", "fitness": 0.4577830954876406, "name": "AdaptiveDifferentialEvolution", "description": "Simplified Adaptive Differential Evolution with self-adaptive mutation factor and a focus on global search by incorporating information from the population mean.", "code": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget=10000, dim=10, pop_size=40, CR=0.7):\n        \"\"\"\n        Args:\n            budget (int): The maximum number of function evaluations.\n            dim (int): The dimensionality of the search space.\n            pop_size (int): The population size.\n            CR (float): Crossover rate.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.CR = CR\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        # Initialize population within bounds\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size # Update remaining budget\n\n        # Find the best individual in the initial population\n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index].copy()\n\n        while self.budget > 0:\n            # Calculate population mean\n            mean_position = np.mean(population, axis=0)\n\n            for i in range(self.pop_size):\n                # Adaptive Mutation Factor\n                F = np.random.normal(0.5, 0.3)  # Self-adaptive F\n                F = np.clip(F, 0.1, 1.0)  # Ensure F is within reasonable bounds\n\n                # Mutation incorporating population mean\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b = np.random.choice(idxs, 2, replace=False)\n                mutant = population[i] + F * (population[a] - population[b]) + 0.1 * (mean_position - population[i])\n\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Boundary handling (clip to bounds)\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n\n                # Evaluation\n                f = func(trial)\n                self.budget -= 1\n                \n                if f < fitness[i]:\n                    fitness[i] = f\n                    population[i] = trial\n                    \n                    # Update global best\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = trial.copy()\n\n                if self.budget <= 0:\n                    break\n            \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 9, "feedback": "The algorithm AdaptiveDifferentialEvolution scored 0.458 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["4f882f64-ca6f-4d33-b1a5-08b945ed72d5"], "operator": null, "metadata": {"aucs": [0.17034666450905478, 0.3130045200668874, 0.4307709477227786, 0.7076116563854036, 0.3567754310503386, 0.535685694159135, 0.30223861011039543, 0.3886178585112301, 0.4099784330946017, 0.22004141915620534, 0.6995756973586469, 0.9974143549011456, 0.322923482789257, 0.34227485377513256, 0.7725431301254907, 0.5200744256696344, 0.3644660523169809, 0.576913391213345, 0.23354457145230378, 0.4908607153848442]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "abb84877-6156-4ced-a4b2-d85773c1a2f0", "fitness": 0.0, "name": "AdaptiveDifferentialEvolution", "description": "An enhanced Adaptive Differential Evolution with dynamic F and CR adaptation based on population diversity and individual success, coupled with orthogonal learning to enhance exploration and exploitation.", "code": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget=10000, dim=10, pop_size=40, F=0.5, CR=0.7, restart_prob=0.05, F_adapt=True, CR_adapt=True, orthogonal_learning=True):\n        \"\"\"\n        Args:\n            budget (int): The maximum number of function evaluations.\n            dim (int): The dimensionality of the search space.\n            pop_size (int): The population size.\n            F (float): Initial mutation factor.\n            CR (float): Initial Crossover rate.\n            restart_prob (float): Probability of restarting a solution.\n            F_adapt (bool): Enable adaptive F.\n            CR_adapt (bool): Enable adaptive CR.\n            orthogonal_learning (bool): Enable orthogonal learning.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.restart_prob = restart_prob\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.F_adapt = F_adapt\n        self.CR_adapt = CR_adapt\n        self.orthogonal_learning = orthogonal_learning\n\n    def __call__(self, func):\n        # Initialize population within bounds\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size # Update remaining budget\n\n        # Find the best individual in the initial population\n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index].copy()\n\n        archive = []  # Archive for storing discarded solutions for orthogonal learning\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Parameter Adaptation\n                F = self.F\n                CR = self.CR\n\n                if self.F_adapt:\n                    F = np.random.normal(self.F, 0.1)\n                    F = np.clip(F, 0.1, 1.0)\n\n                if self.CR_adapt:\n                    CR = np.random.normal(self.CR, 0.1)\n                    CR = np.clip(CR, 0.1, 1.0)\n\n                # Mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = np.random.choice(idxs, 3, replace=False)\n                mutant = population[a] + F * (population[b] - population[c])\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Boundary handling (clip to bounds)\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Restart mechanism (inject diversity)\n                if np.random.rand() < self.restart_prob:\n                    trial = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n\n                # Evaluation\n                f = func(trial)\n                self.budget -= 1\n                \n                if f < fitness[i]:\n                    archive.append(population[i].copy())  # Store the replaced solution in the archive\n                    fitness[i] = f\n                    population[i] = trial\n                    \n                    # Update global best\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = trial.copy()\n                else:\n                    archive.append(trial.copy()) # Store the trial solution in the archive\n\n                # Orthogonal Learning\n                if self.orthogonal_learning and len(archive) > 5:  # Ensure enough solutions in the archive\n                    # Select a subset of archive solutions for orthogonal design\n                    num_samples = min(5, len(archive)) # Use a maximum of 5 archive solutions\n                    samples = np.random.choice(len(archive), num_samples, replace=False)\n                    orthogonal_matrix = np.array([archive[s] for s in samples])\n\n                    # Perform orthogonal design (simplified version)\n                    mean_vector = np.mean(orthogonal_matrix, axis=0)\n                    \n                    # Create a new trial point based on orthogonal learning\n                    orthogonal_trial = mean_vector + np.random.normal(0, 0.01, self.dim)  # Add some noise\n                    orthogonal_trial = np.clip(orthogonal_trial, func.bounds.lb, func.bounds.ub)\n                    \n                    f_orthogonal = func(orthogonal_trial)\n                    self.budget -= 1\n\n                    if f_orthogonal < self.f_opt:\n                        self.f_opt = f_orthogonal\n                        self.x_opt = orthogonal_trial.copy()\n                \n                if self.budget <= 0:\n                    break\n            \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 9, "feedback": "The algorithm AdaptiveDifferentialEvolution scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["4f882f64-ca6f-4d33-b1a5-08b945ed72d5"], "operator": null, "metadata": {"aucs": [0]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "7940680e-c87b-41b5-8dd2-212bc63797ba", "fitness": 0.6407195282217265, "name": "AdaptiveGaussianCauchySearch", "description": "An adaptive algorithm using Gaussian and Cauchy mutations, dynamically adjusting mutation rates based on a success-based learning rate, while focusing exploration near the best solution and reducing exploration range over time.", "code": "import numpy as np\n\nclass AdaptiveGaussianCauchySearch:\n    def __init__(self, budget=10000, dim=10, population_size=10, initial_std=0.5, cauchy_prob=0.1, exploration_decay=0.999, learning_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = population_size\n        self.initial_std = initial_std\n        self.cauchy_prob = cauchy_prob\n        self.exploration_decay = exploration_decay\n        self.learning_rate = learning_rate\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.std = self.initial_std\n        self.exploration_range = 5.0\n        eval_count = 0\n\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.population_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        eval_count += self.population_size\n        \n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index]\n\n        while eval_count < self.budget:\n            new_samples = np.zeros((self.population_size, self.dim))\n            new_fitness = np.zeros(self.population_size)\n\n            for i in range(self.population_size):\n                # Blend Gaussian and Cauchy mutations, favoring the best solution\n                if np.random.rand() < self.cauchy_prob:\n                    mutation = self.x_opt + self.std * np.random.standard_cauchy(size=self.dim) * self.exploration_range\n                else:\n                    mutation = self.x_opt + self.std * np.random.randn(self.dim) * self.exploration_range\n                \n                new_sample = np.clip(mutation, func.bounds.lb, func.bounds.ub)\n                new_samples[i] = new_sample\n                new_fitness[i] = func(new_sample)\n                eval_count += 1\n\n                # Update best solution immediately if improvement found\n                if new_fitness[i] < self.f_opt:\n                    self.f_opt = new_fitness[i]\n                    self.x_opt = new_sample\n\n            # Adapt standard deviation based on success rate using a learning rate\n            success_rate = np.mean(new_fitness < fitness)\n            self.std *= np.exp(self.learning_rate * (success_rate - 0.5))  # Learning rate based adaptation\n\n            self.exploration_range *= self.exploration_decay # Reduce exploration range over time\n            fitness = np.minimum(fitness, new_fitness)  # replace only if better\n            best_index = np.argmin(fitness)\n            \n            if eval_count >= self.budget:\n                break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 9, "feedback": "The algorithm AdaptiveGaussianCauchySearch scored 0.641 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["a42186a0-8c0b-43fa-93b2-ee93787a11b3"], "operator": null, "metadata": {"aucs": [0.2132237749808632, 0.2038182791121782, 0.8461681215213794, 0.9296301980285016, 0.8510756018589141, 0.8736248663010385, 0.28377887894043463, 0.7207787300726023, 0.8519652626487575, 0.18837851135679196, 0.8776679820380219, 0.9987083231649926, 0.20549504890390058, 0.7975095465133927, 0.901129352710669, 0.726103448052396, 0.6784142926790295, 0.902134198433961, 0.22456212500753892, 0.5402240221091671]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "56bda660-8c68-4bc0-955b-3b7c91f42eb6", "fitness": "-inf", "name": "EnhancedAdaptiveDifferentialEvolution", "description": "Enhanced Adaptive Differential Evolution with orthogonal learning and adaptive exploration-exploitation balance.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDifferentialEvolution:\n    def __init__(self, budget=10000, dim=10, pop_size=30, F=0.5, CR=0.7, ortho_group_size=5):\n        \"\"\"\n        Args:\n            budget (int): The maximum number of function evaluations.\n            dim (int): The dimensionality of the search space.\n            pop_size (int): The population size.\n            F (float): Mutation factor.\n            CR (float): Crossover rate.\n            ortho_group_size (int): Size of the group for orthogonal learning.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.ortho_group_size = min(ortho_group_size, dim) # Limit group size to dimension\n        self.adaptive_exploration_rate = 0.1 # initial rate\n\n    def orthogonal_design(self, group_size):\n        \"\"\"Generates an orthogonal design matrix for a given group size.\"\"\"\n        if group_size == 2:\n            return np.array([[1, 1], [1, -1]])\n        elif group_size == 3:\n             return np.array([[1, 1, 1], [1, -1, -1], [-1, 1, -1], [-1, -1, 1]]) # Hadammard matrix\n        elif group_size == 4:\n            return np.array([[1, 1, 1, 1], [1, -1, 1, -1], [1, 1, -1, -1], [1, -1, -1, 1]])\n        elif group_size == 5:\n            # A simple design.  Can be improved with more advanced orthogonal arrays\n            return np.array([[1, 1, 1, 1, 1], [1, -1, 1, -1, 1], [1, 1, -1, -1, 1], [1, -1, -1, 1, 1], [-1, -1, -1, -1, -1]])\n        else: # group_size is 1 or invalid\n            return np.array([[1]])\n\n\n    def __call__(self, func):\n        # Initialize population within bounds\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        # Find the best individual in the initial population\n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index].copy()\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Adaptive Exploration-Exploitation Balance\n                if np.random.rand() < self.adaptive_exploration_rate:\n                    # Exploration: Random mutation\n                    mutant = func.bounds.lb + np.random.rand(self.dim) * (func.bounds.ub - func.bounds.lb)\n                else:\n                    # Exploitation: Exploit the best solution\n                    idxs = [idx for idx in range(self.pop_size) if idx != i]\n                    a, b = np.random.choice(idxs, 2, replace=False)\n                    mutant = population[i] + self.F * (self.x_opt - population[a]) + self.F * (population[b] - population[i])\n\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Boundary handling (clip to bounds)\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Orthogonal Learning (applied with a probability)\n                if np.random.rand() < 0.2:\n                    # Select a random subset of dimensions for orthogonal learning\n                    dims_to_optimize = np.random.choice(self.dim, self.ortho_group_size, replace=False)\n                    design = self.orthogonal_design(self.ortho_group_size)\n                    num_samples = design.shape[0]\n\n                    best_trial = trial.copy()\n                    best_f = np.inf\n\n                    for j in range(num_samples):\n                        temp_trial = trial.copy()\n                        for k, dim_index in enumerate(dims_to_optimize):\n                            # Scale and shift design values to the variable range\n                            center = (func.bounds.ub + func.bounds.lb) / 2\n                            scale = (func.bounds.ub - func.bounds.lb) / 2\n                            temp_trial[dim_index] = center + scale * design[j, k] # [-1, 1] to [lb, ub]\n\n                        temp_trial = np.clip(temp_trial, func.bounds.lb, func.bounds.ub)  # boundary handling again\n\n                        f_temp = func(temp_trial)\n                        self.budget -= 1\n\n                        if f_temp < best_f:\n                            best_f = f_temp\n                            best_trial = temp_trial.copy()\n\n                        if self.budget <= 0:\n                            break\n\n                    trial = best_trial #Use the best from orthogonal design\n                    f = best_f\n\n                else:\n                    # Evaluation (if orthogonal learning is not applied)\n                    f = func(trial)\n                    self.budget -= 1\n                \n\n                if f < fitness[i]:\n                    fitness[i] = f\n                    population[i] = trial\n\n                    # Update global best\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = trial.copy()\n\n                if self.budget <= 0:\n                    break\n\n            # Self-adaptation of F and CR\n            self.F = np.clip(np.random.normal(0.5, 0.1), 0.1, 0.9)\n            self.CR = np.clip(np.random.normal(0.7, 0.1), 0.1, 0.9)\n\n            # Adapt exploration rate based on the improvement of the best solution\n            if np.random.rand() < 0.1:\n               self.adaptive_exploration_rate = np.clip(self.adaptive_exploration_rate + np.random.normal(0, 0.02), 0.05, 0.5)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 9, "feedback": "In the code, line 88, in __call__, the following error occurred:\nValueError: setting an array element with a sequence.\nOn line: temp_trial[dim_index] = center + scale * design[j, k] # [-1, 1] to [lb, ub]", "error": "In the code, line 88, in __call__, the following error occurred:\nValueError: setting an array element with a sequence.\nOn line: temp_trial[dim_index] = center + scale * design[j, k] # [-1, 1] to [lb, ub]", "parent_ids": ["0b20a372-d5e6-4da8-ba2e-89b36eb59095"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "88e80d96-1cf7-4044-babf-17c9b10b33b1", "fitness": 0.6757139723003416, "name": "AdaptiveDifferentialEvolution", "description": "Adaptive Differential Evolution with aging population, orthogonal crossover, and self-adaptive parameters, balancing exploration and exploitation.", "code": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget=10000, dim=10, pop_size=30, F=0.5, CR=0.7, age_limit=50):\n        \"\"\"\n        Args:\n            budget (int): The maximum number of function evaluations.\n            dim (int): The dimensionality of the search space.\n            pop_size (int): The population size.\n            F (float): Mutation factor.\n            CR (float): Crossover rate.\n            age_limit (int): Maximum age of an individual before replacement.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.age_limit = age_limit\n        self.ages = np.zeros(pop_size, dtype=int)\n\n    def __call__(self, func):\n        # Initialize population within bounds\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        # Find the best individual in the initial population\n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index].copy()\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation: Exploit the best solution\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b = np.random.choice(idxs, 2, replace=False)\n\n                mutant = population[i] + self.F * (self.x_opt - population[a]) + self.F * (population[b] - population[i])\n\n                # Orthogonal Crossover\n                trial = self.orthogonal_crossover(population[i], mutant)\n\n                # Boundary handling (clip to bounds)\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Evaluation\n                f = func(trial)\n                self.budget -= 1\n\n                if f < fitness[i]:\n                    fitness[i] = f\n                    population[i] = trial\n                    self.ages[i] = 0  # Reset age\n                    # Update global best\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = trial.copy()\n                else:\n                    self.ages[i] += 1  # Increment age\n\n                # Aging mechanism: replace old individuals\n                if self.ages[i] > self.age_limit:\n                    population[i] = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                    fitness[i] = func(population[i])\n                    self.budget -= 1\n                    self.ages[i] = 0\n\n                if self.budget <= 0:\n                    break\n\n            # Self-adaptation of F and CR\n            self.F = np.clip(np.random.normal(0.5, 0.1), 0.1, 0.9)\n            self.CR = np.clip(np.random.normal(0.7, 0.1), 0.1, 0.9)\n\n        return self.f_opt, self.x_opt\n\n    def orthogonal_crossover(self, x, mutant):\n        \"\"\"Performs orthogonal crossover between two individuals.\"\"\"\n        trial = x.copy()\n        num_vectors = 5\n        num_samples = self.dim // num_vectors\n\n        for k in range(num_vectors):\n            # Select indices for this vector\n            indices = np.random.choice(self.dim, num_samples, replace=False)\n            for index in indices:\n                if np.random.rand() < self.CR:\n                    trial[index] = mutant[index]\n        return trial", "configspace": "", "generation": 9, "feedback": "The algorithm AdaptiveDifferentialEvolution scored 0.676 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["0b20a372-d5e6-4da8-ba2e-89b36eb59095"], "operator": null, "metadata": {"aucs": [0.14287727632242753, 0.5262709531340739, 0.7992174364977566, 0.9078862606617496, 0.8448423018207769, 0.8950251567733299, 0.5732948480458862, 0.7515077385773878, 0.8485280673344466, 0.6492636474154986, 0.8996567160927187, 0.9927515614058453, 0.2652268094344743, 0.8097451306023972, 0.9397972514295261, 0.8857098445813055, 0.5999417679505578, 0.90459653119353, 0.27814014673314336, 0]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "50d926fc-0d15-4ecd-8395-f5022a63ca67", "fitness": "-inf", "name": "AdaptiveDifferentialEvolution", "description": "Adaptive Differential Evolution with a modified mutation strategy favoring exploration in early stages and exploitation in later stages, along with dynamic parameter adaptation based on population diversity.", "code": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget=10000, dim=10, pop_size=40, F_base=0.5, CR_base=0.7, restart_prob=0.05, diversity_threshold=0.1):\n        \"\"\"\n        Args:\n            budget (int): The maximum number of function evaluations.\n            dim (int): The dimensionality of the search space.\n            pop_size (int): The population size.\n            F_base (float): Base mutation factor.\n            CR_base (float): Base crossover rate.\n            restart_prob (float): Probability of restarting a solution.\n            diversity_threshold (float): Threshold for population diversity.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F_base = F_base\n        self.CR_base = CR_base\n        self.restart_prob = restart_prob\n        self.diversity_threshold = diversity_threshold\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.generation = 0\n\n    def calculate_diversity(self, population):\n        \"\"\"Calculates the population diversity.\"\"\"\n        centroid = np.mean(population, axis=0)\n        distances = np.linalg.norm(population - centroid, axis=1)\n        diversity = np.mean(distances) / (func.bounds.ub - func.bounds.lb)  # Normalized diversity\n        return diversity\n    \n    def __call__(self, func):\n        # Initialize population within bounds\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size # Update remaining budget\n\n        # Find the best individual in the initial population\n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index].copy()\n\n        while self.budget > 0:\n            self.generation += 1\n            diversity = self.calculate_diversity(population)\n\n            for i in range(self.pop_size):\n                # Dynamic parameter adaptation based on diversity\n                F = self.F_base * (1 + diversity)  # Increase F when diversity is high\n                CR = self.CR_base * (1 - diversity / 2) # Decrease CR when diversity is low\n                F = np.clip(F, 0.1, 0.9) # Clip F to reasonable bounds\n                CR = np.clip(CR, 0.1, 0.9) # Clip CR to reasonable bounds\n\n                # Mutation Strategy\n                if diversity > self.diversity_threshold:\n                    # High diversity: Explore more (DE/rand/1)\n                    idxs = [idx for idx in range(self.pop_size) if idx != i]\n                    a, b, c = np.random.choice(idxs, 3, replace=False)\n                    mutant = population[a] + F * (population[b] - population[c])\n                else:\n                    # Low diversity: Exploit more (DE/current-to-best/1)\n                    idxs = [idx for idx in range(self.pop_size) if idx != i]\n                    a, b = np.random.choice(idxs, 2, replace=False)\n                    mutant = population[i] + F * (self.x_opt - population[i]) + F * (population[a] - population[b])\n\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Boundary handling (clip to bounds)\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Restart mechanism (inject diversity)\n                if np.random.rand() < self.restart_prob:\n                    trial = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n\n                # Evaluation\n                f = func(trial)\n                self.budget -= 1\n                \n                if f < fitness[i]:\n                    fitness[i] = f\n                    population[i] = trial\n                    \n                    # Update global best\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = trial.copy()\n\n                if self.budget <= 0:\n                    break\n            \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 10, "feedback": "In the code, line 30, in calculate_diversity, the following error occurred:\nNameError: name 'func' is not defined\nOn line: diversity = np.mean(distances) / (func.bounds.ub - func.bounds.lb)  # Normalized diversity", "error": "In the code, line 30, in calculate_diversity, the following error occurred:\nNameError: name 'func' is not defined\nOn line: diversity = np.mean(distances) / (func.bounds.ub - func.bounds.lb)  # Normalized diversity", "parent_ids": ["4f882f64-ca6f-4d33-b1a5-08b945ed72d5"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "bde29bf1-7aec-4a25-858c-18335f5b223f", "fitness": "-inf", "name": "AdaptiveDifferentialEvolution", "description": "Simplified Adaptive Differential Evolution with improved parameter adaptation based on population diversity and a modified mutation strategy.", "code": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget=10000, dim=10, pop_size=25):\n        \"\"\"\n        Args:\n            budget (int): The maximum number of function evaluations.\n            dim (int): The dimensionality of the search space.\n            pop_size (int): The population size. Reduced for faster iterations.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.F = 0.5\n        self.CR = 0.7\n\n    def __call__(self, func):\n        # Initialize population within bounds\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        # Find the best individual in the initial population\n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index].copy()\n\n        while self.budget > 0:\n            # Adaptive parameter updates based on population diversity\n            diversity = self.calculate_diversity(population)\n            self.F = np.clip(self.F + 0.1 * (diversity - 0.5), 0.1, 0.9)  # Adjust F based on diversity\n            self.CR = np.clip(self.CR + 0.1 * (0.5 - diversity), 0.1, 0.9) # Adjust CR based on diversity\n\n\n            for i in range(self.pop_size):\n                # Mutation: Improved strategy\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = np.random.choice(idxs, 3, replace=False) # Pick 3 random indices\n\n                mutant = population[i] + self.F * (population[a] - population[b]) + self.F * (self.x_opt - population[c]) # Mutate with current-to-best\n\n                # Crossover\n                trial = population[i].copy()\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR:\n                        trial[j] = mutant[j]\n\n                # Boundary handling (clip to bounds)\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Evaluation\n                f = func(trial)\n                self.budget -= 1\n\n                if f < fitness[i]:\n                    fitness[i] = f\n                    population[i] = trial\n                    # Update global best\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = trial.copy()\n\n\n                if self.budget <= 0:\n                    break\n\n        return self.f_opt, self.x_opt\n\n    def calculate_diversity(self, population):\n        \"\"\"Calculates the diversity of the population (normalized standard deviation).\"\"\"\n        mean_position = np.mean(population, axis=0)\n        distances = np.linalg.norm(population - mean_position, axis=1)\n        diversity = np.std(distances) / (np.linalg.norm(func.bounds.ub - func.bounds.lb))\n        return diversity", "configspace": "", "generation": 10, "feedback": "In the code, line 75, in calculate_diversity, the following error occurred:\nNameError: name 'func' is not defined\nOn line: diversity = np.std(distances) / (np.linalg.norm(func.bounds.ub - func.bounds.lb))", "error": "In the code, line 75, in calculate_diversity, the following error occurred:\nNameError: name 'func' is not defined\nOn line: diversity = np.std(distances) / (np.linalg.norm(func.bounds.ub - func.bounds.lb))", "parent_ids": ["88e80d96-1cf7-4044-babf-17c9b10b33b1"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "71db7b01-9586-425b-ac3b-57051a828533", "fitness": 0.7145818883022891, "name": "AdaptiveDifferentialEvolution", "description": "Simplified Adaptive Differential Evolution with orthogonal crossover and a dynamic population size, focusing on enhanced exploration and exploitation balance.", "code": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget=10000, dim=10, pop_size_init=30, F=0.5, CR=0.7):\n        \"\"\"\n        Args:\n            budget (int): The maximum number of function evaluations.\n            dim (int): The dimensionality of the search space.\n            pop_size_init (int): Initial population size.\n            F (float): Mutation factor.\n            CR (float): Crossover rate.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.pop_size_init = pop_size_init\n        self.F = F\n        self.CR = CR\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        # Initialize population within bounds\n        pop_size = self.pop_size_init\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= pop_size\n\n        # Find the best individual in the initial population\n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index].copy()\n\n        generation = 0\n        while self.budget > 0:\n            generation += 1\n\n            # Dynamic population size adjustment\n            pop_size = max(10, int(self.pop_size_init * (1 - generation / (self.budget / self.pop_size_init))))\n\n\n            for i in range(population.shape[0]): #Iterate over current population\n\n                # Mutation: Current-to-best with differential\n                idxs = [idx for idx in range(population.shape[0]) if idx != i]\n                a, b = np.random.choice(idxs, 2, replace=False)\n                mutant = population[i] + self.F * (self.x_opt - population[i]) + self.F * (population[a] - population[b])\n\n                # Orthogonal Crossover\n                trial = self.orthogonal_crossover(population[i], mutant)\n\n                # Boundary handling (clip to bounds)\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Evaluation\n                f = func(trial)\n                self.budget -= 1\n\n                if f < fitness[i]:\n                    fitness[i] = f\n                    population[i] = trial\n\n                    # Update global best\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = trial.copy()\n\n                if self.budget <= 0:\n                    break\n\n            # Self-adaptation of F and CR (simplified)\n            self.F = np.clip(np.random.normal(0.5, 0.1), 0.1, 0.9)\n            self.CR = np.clip(np.random.normal(0.7, 0.1), 0.1, 0.9)\n            \n            #Elitism - Keep the best individual in the population\n            best_index = np.argmin(fitness)\n            if not np.array_equal(population[best_index], self.x_opt):\n                population[best_index] = self.x_opt.copy()\n                fitness[best_index] = self.f_opt\n            \n\n        return self.f_opt, self.x_opt\n\n    def orthogonal_crossover(self, x, mutant):\n        \"\"\"Performs orthogonal crossover between two individuals.\"\"\"\n        trial = x.copy()\n        num_vectors = 5\n        num_samples = self.dim // num_vectors\n\n        for k in range(num_vectors):\n            # Select indices for this vector\n            indices = np.random.choice(self.dim, num_samples, replace=False, ) if self.dim >= num_samples else np.arange(self.dim)\n            for index in indices:\n                if np.random.rand() < self.CR:\n                    trial[index] = mutant[index]\n        return trial", "configspace": "", "generation": 10, "feedback": "The algorithm AdaptiveDifferentialEvolution scored 0.715 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["88e80d96-1cf7-4044-babf-17c9b10b33b1"], "operator": null, "metadata": {"aucs": [0.20972179893585297, 0.40528429718768944, 0.8242374461360341, 0.9028006588052372, 0.8521206608917253, 0.8833580255226544, 0.7578799756699721, 0.7890214591209006, 0.8486503916615006, 0.7335012308929449, 0.9206814074058765, 0.9970591408077046, 0.29907087829188217, 0.6888786392486712, 0.9320370376388802, 0.8771759086130821, 0.6925914281435168, 0.9074566709218698, 0.25727384746469073, 0.5128368626850959]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "602c5118-8314-4146-8fe3-d00626a613bd", "fitness": 0.735044876248992, "name": "AdaptiveDifferentialEvolution", "description": "Enhanced Adaptive Differential Evolution with a combined mutation strategy incorporating rank-based selection and an improved orthogonal crossover.", "code": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget=10000, dim=10, pop_size=30, F=0.5, CR=0.7, age_limit=50, rank_prob=0.1):\n        \"\"\"\n        Args:\n            budget (int): The maximum number of function evaluations.\n            dim (int): The dimensionality of the search space.\n            pop_size (int): The population size.\n            F (float): Mutation factor.\n            CR (float): Crossover rate.\n            age_limit (int): Maximum age of an individual before replacement.\n            rank_prob (float): Probability of using rank-based selection in mutation.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.age_limit = age_limit\n        self.ages = np.zeros(pop_size, dtype=int)\n        self.rank_prob = rank_prob\n\n    def __call__(self, func):\n        # Initialize population within bounds\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        # Find the best individual in the initial population\n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index].copy()\n\n        while self.budget > 0:\n            # Sort population based on fitness\n            ranked_indices = np.argsort(fitness)\n\n            for i in range(self.pop_size):\n                # Mutation: Combined strategy\n                if np.random.rand() < self.rank_prob:\n                    # Rank-based selection: choose better individuals with higher probability\n                    rank_index = np.random.choice(ranked_indices[:self.pop_size // 2])  # Select from top half\n                    idxs = [idx for idx in range(self.pop_size) if idx != i and idx != rank_index]\n                    a, b = np.random.choice(idxs, 2, replace=False)\n                    mutant = population[i] + self.F * (population[rank_index] - population[a]) + self.F * (population[b] - population[i])\n                else:\n                    # Exploit the best solution\n                    idxs = [idx for idx in range(self.pop_size) if idx != i]\n                    a, b = np.random.choice(idxs, 2, replace=False)\n                    mutant = population[i] + self.F * (self.x_opt - population[a]) + self.F * (population[b] - population[i])\n\n                # Improved Orthogonal Crossover\n                trial = self.orthogonal_crossover(population[i], mutant, fitness)\n\n                # Boundary handling (clip to bounds)\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Evaluation\n                f = func(trial)\n                self.budget -= 1\n\n                if f < fitness[i]:\n                    fitness[i] = f\n                    population[i] = trial\n                    self.ages[i] = 0  # Reset age\n                    # Update global best\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = trial.copy()\n                else:\n                    self.ages[i] += 1  # Increment age\n\n                # Aging mechanism: replace old individuals\n                if self.ages[i] > self.age_limit:\n                    population[i] = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                    fitness[i] = func(population[i])\n                    self.budget -= 1\n                    self.ages[i] = 0\n\n                if self.budget <= 0:\n                    break\n\n            # Self-adaptation of F and CR\n            self.F = np.clip(np.random.normal(0.5, 0.1), 0.1, 0.9)\n            self.CR = np.clip(np.random.normal(0.7, 0.1), 0.1, 0.9)\n\n        return self.f_opt, self.x_opt\n\n    def orthogonal_crossover(self, x, mutant, fitness):\n        \"\"\"Performs orthogonal crossover between two individuals.\"\"\"\n        trial = x.copy()\n        num_vectors = 5\n        num_samples = self.dim // num_vectors\n\n        # Sort dimensions based on contribution to fitness (optional, but can improve performance)\n        dim_importance = np.abs(mutant - x)  # Estimate importance based on difference\n        sorted_indices = np.argsort(dim_importance)[::-1]  # Sort in descending order\n\n        for k in range(num_vectors):\n            # Select indices for this vector based on sorted indices\n            indices = sorted_indices[k*num_samples : (k+1)*num_samples]\n            for index in indices:\n                if np.random.rand() < self.CR:\n                    trial[index] = mutant[index]\n        return trial", "configspace": "", "generation": 10, "feedback": "The algorithm AdaptiveDifferentialEvolution scored 0.735 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["88e80d96-1cf7-4044-babf-17c9b10b33b1"], "operator": null, "metadata": {"aucs": [0.27649561825121016, 0.8266732869267157, 0.8455259439485988, 0.9378178219594652, 0.8786329225406446, 0.9059628979120907, 0.36703507118682543, 0.8520922736308432, 0.8999297675470939, 0.31723031973300375, 0.9282597527441395, 0.9969739511895377, 0.47613608703096844, 0.8578199684035446, 0.9360067760448413, 0.8980717846322948, 0.8036943787946579, 0.914398249790285, 0.27633753440528175, 0.5058031183077961]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "a4389ebd-b63b-4bec-8bff-0b6705613689", "fitness": "-inf", "name": "AdaptiveDifferentialEvolution", "description": "Self-Adaptive Differential Evolution with a combined mutation strategy, dynamic parameter adaptation based on success rate, and a diversity-enhancing orthogonal crossover.", "code": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget=10000, dim=10, pop_size=40, F=0.5, CR=0.7, restart_prob=0.05, ortho_groups=5):\n        \"\"\"\n        Args:\n            budget (int): The maximum number of function evaluations.\n            dim (int): The dimensionality of the search space.\n            pop_size (int): The population size.\n            F (float): Initial mutation factor.\n            CR (float): Initial crossover rate.\n            restart_prob (float): Probability of restarting a solution.\n            ortho_groups (int): Number of orthogonal groups for crossover.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.restart_prob = restart_prob\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.ortho_groups = ortho_groups\n        self.success_F = np.ones(self.pop_size) * self.F\n        self.success_CR = np.ones(self.pop_size) * self.CR\n        self.success_history_F = []\n        self.success_history_CR = []\n        self.archive_F = []\n        self.archive_CR = []\n\n    def __call__(self, func):\n        # Initialize population within bounds\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size # Update remaining budget\n\n        # Find the best individual in the initial population\n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index].copy()\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Parameter Adaptation\n                F = self.success_F[i]\n                CR = self.success_CR[i]\n                \n                # Mutation: Using both current-to-best and random\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = np.random.choice(idxs, 3, replace=False)\n                mutant = population[i] + self.F * (self.x_opt - population[i]) + self.F * (population[a] - population[b])\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Orthogonal Crossover (enhance diversity)\n                if self.dim > 1 and self.ortho_groups > 1:\n                    group_size = self.dim // self.ortho_groups\n                    for g in range(self.ortho_groups):\n                        start = g * group_size\n                        end = (g + 1) * group_size if g < self.ortho_groups - 1 else self.dim\n                        if np.random.rand() < 0.2:  # Apply with a probability\n                            idx1, idx2 = np.random.choice(range(start, end), 2, replace=False)\n                            trial[idx1], trial[idx2] = population[i][idx2], population[i][idx1]\n\n                # Boundary handling (clip to bounds)\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Restart mechanism (inject diversity) - reduced probability\n                if np.random.rand() < self.restart_prob * 0.5:\n                    trial = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n\n                # Evaluation\n                f = func(trial)\n                self.budget -= 1\n                \n                if f < fitness[i]:\n                    delta = abs(fitness[i] - f)\n                    self.success_history_F.append(F)\n                    self.success_history_CR.append(CR)\n                    fitness[i] = f\n                    population[i] = trial\n                    \n                    # Update success rate for F and CR\n                    self.success_F[i] = 0.9 * self.success_F[i] + 0.1 * F # Exponential smoothing\n                    self.success_CR[i] = 0.9 * self.success_CR[i] + 0.1 * CR\n\n                    # Update global best\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = trial.copy()\n                else:\n                    # Decrease the F and CR if the solution is not improving\n                    self.success_F[i] = max(0.1, self.success_F[i] * 0.95)\n                    self.success_CR[i] = min(0.9, self.success_CR[i] * 1.05)\n\n\n                if self.budget <= 0:\n                    break\n                    \n            # Update F and CR based on successful values (after each generation)\n            if self.success_history_F:\n                self.F = np.median(self.success_history_F)\n                self.CR = np.median(self.success_history_CR)\n                self.success_history_F = []\n                self.success_history_CR = []\n\n            \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 10, "feedback": "In the code, line 975, in numpy.random.mtrand.RandomState.choice, the following error occurred:\nValueError: 'a' cannot be empty unless no samples are taken", "error": "In the code, line 975, in numpy.random.mtrand.RandomState.choice, the following error occurred:\nValueError: 'a' cannot be empty unless no samples are taken", "parent_ids": ["4f882f64-ca6f-4d33-b1a5-08b945ed72d5"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "5f732b59-29ac-4965-9914-68c40121c957", "fitness": 0.5959203437225005, "name": "AdaptiveDifferentialEvolution", "description": "Adaptive Differential Evolution with a modified mutation strategy, enhanced self-adaptation of parameters, and a more aggressive aging mechanism promoting population diversity.", "code": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget=10000, dim=10, pop_size=30, F=0.5, CR=0.7, age_limit=50, p_best=0.1):\n        \"\"\"\n        Args:\n            budget (int): The maximum number of function evaluations.\n            dim (int): The dimensionality of the search space.\n            pop_size (int): The population size.\n            F (float): Mutation factor.\n            CR (float): Crossover rate.\n            age_limit (int): Maximum age of an individual before replacement.\n            p_best (float): Probability of selecting the best individual for mutation.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.age_limit = age_limit\n        self.ages = np.zeros(pop_size, dtype=int)\n        self.p_best = p_best\n\n    def __call__(self, func):\n        # Initialize population within bounds\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        # Find the best individual in the initial population\n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index].copy()\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation: Enhanced strategy incorporating p_best\n                if np.random.rand() < self.p_best:\n                    # Use best individual\n                    idxs = [idx for idx in range(self.pop_size) if idx != i]\n                    a, b = np.random.choice(idxs, 2, replace=False)\n                    mutant = population[i] + self.F * (self.x_opt - population[a]) + self.F * (population[b] - population[i])\n                else:\n                    # Use random individuals\n                    idxs = [idx for idx in range(self.pop_size) if idx != i]\n                    a, b, c = np.random.choice(idxs, 3, replace=False)\n                    mutant = population[a] + self.F * (population[b] - population[c])\n\n                # Orthogonal Crossover\n                trial = self.orthogonal_crossover(population[i], mutant)\n\n                # Boundary handling (clip to bounds)\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Evaluation\n                f = func(trial)\n                self.budget -= 1\n\n                if f < fitness[i]:\n                    fitness[i] = f\n                    population[i] = trial\n                    self.ages[i] = 0  # Reset age\n                    # Update global best\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = trial.copy()\n                else:\n                    self.ages[i] += 1  # Increment age\n\n                # Aging mechanism: replace old individuals more aggressively\n                if self.ages[i] > self.age_limit:\n                    population[i] = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                    fitness[i] = func(population[i])\n                    self.budget -= 1\n                    self.ages[i] = 0\n                    # Small perturbation to F after replacement\n                    self.F = np.clip(self.F + np.random.normal(0, 0.05), 0.1, 0.9)\n\n\n                if self.budget <= 0:\n                    break\n\n            # Self-adaptation of F and CR (enhanced)\n            self.F = np.clip(np.random.normal(self.F, 0.1), 0.1, 0.9)\n            self.CR = np.clip(np.random.normal(self.CR, 0.1), 0.1, 0.9)\n\n\n        return self.f_opt, self.x_opt\n\n    def orthogonal_crossover(self, x, mutant):\n        \"\"\"Performs orthogonal crossover between two individuals.\"\"\"\n        trial = x.copy()\n        num_vectors = 5\n        num_samples = self.dim // num_vectors\n\n        for k in range(num_vectors):\n            # Select indices for this vector\n            indices = np.random.choice(self.dim, num_samples, replace=False)\n            for index in indices:\n                if np.random.rand() < self.CR:\n                    trial[index] = mutant[index]\n        return trial", "configspace": "", "generation": 10, "feedback": "The algorithm AdaptiveDifferentialEvolution scored 0.596 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["88e80d96-1cf7-4044-babf-17c9b10b33b1"], "operator": null, "metadata": {"aucs": [0.2018514617903655, 0.2162842901350145, 0.49024271218218163, 0.8654203118002305, 0.6303806323980518, 0.7235578736574211, 0.6795737966166363, 0.5619807800458053, 0.7815659415072169, 0.5384547752610336, 0.671893765516157, 0.9994810628848546, 0.29137190673831515, 0.7089791855106999, 0.7493108118564069, 0.8545013306763498, 0.4408827675617679, 0.7688723937072404, 0.2103015838746919, 0.5334994907295703]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "a31510ac-7b43-439a-9689-dbaeb474490f", "fitness": 0.515857961711297, "name": "AdaptiveDifferentialEvolution", "description": "Adaptive Differential Evolution with enhanced exploration using a Cauchy mutation operator and a tournament selection-based population update, promoting diversity and convergence.", "code": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget=10000, dim=10, pop_size=30, F=0.5, CR=0.7, age_limit=50, cauchy_scale=0.1):\n        \"\"\"\n        Args:\n            budget (int): The maximum number of function evaluations.\n            dim (int): The dimensionality of the search space.\n            pop_size (int): The population size.\n            F (float): Mutation factor.\n            CR (float): Crossover rate.\n            age_limit (int): Maximum age of an individual before replacement.\n            cauchy_scale (float): Scale parameter for the Cauchy distribution.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.age_limit = age_limit\n        self.ages = np.zeros(pop_size, dtype=int)\n        self.cauchy_scale = cauchy_scale\n\n    def __call__(self, func):\n        # Initialize population within bounds\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        # Find the best individual in the initial population\n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index].copy()\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation: Use Cauchy mutation for enhanced exploration\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b = np.random.choice(idxs, 2, replace=False)\n\n                # Cauchy mutation\n                cauchy_noise = np.random.standard_cauchy(size=self.dim) * self.cauchy_scale\n                mutant = population[i] + self.F * (self.x_opt - population[a]) + self.F * (population[b] - population[i]) + cauchy_noise\n\n                # Orthogonal Crossover\n                trial = self.orthogonal_crossover(population[i], mutant)\n\n                # Boundary handling (clip to bounds)\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Evaluation\n                f = func(trial)\n                self.budget -= 1\n\n                # Tournament selection for population update\n                if f < fitness[i]:\n                    fitness[i] = f\n                    population[i] = trial\n                    self.ages[i] = 0  # Reset age\n                    # Update global best\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = trial.copy()\n                else:\n                    # Tournament selection: compete with a random individual\n                    j = np.random.choice(idxs, 1)[0]\n                    if f < fitness[j]:\n                        fitness[j] = f\n                        population[j] = trial\n                        self.ages[j] = 0\n                    else:\n                        self.ages[i] += 1  # Increment age\n\n                # Aging mechanism: replace old individuals\n                if self.ages[i] > self.age_limit:\n                    population[i] = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                    fitness[i] = func(population[i])\n                    self.budget -= 1\n                    self.ages[i] = 0\n\n                if self.budget <= 0:\n                    break\n\n            # Self-adaptation of F and CR\n            self.F = np.clip(np.random.normal(0.5, 0.1), 0.1, 0.9)\n            self.CR = np.clip(np.random.normal(0.7, 0.1), 0.1, 0.9)\n\n        return self.f_opt, self.x_opt\n\n    def orthogonal_crossover(self, x, mutant):\n        \"\"\"Performs orthogonal crossover between two individuals.\"\"\"\n        trial = x.copy()\n        num_vectors = 5\n        num_samples = self.dim // num_vectors\n\n        for k in range(num_vectors):\n            # Select indices for this vector\n            indices = np.random.choice(self.dim, num_samples, replace=False)\n            for index in indices:\n                if np.random.rand() < self.CR:\n                    trial[index] = mutant[index]\n        return trial", "configspace": "", "generation": 10, "feedback": "The algorithm AdaptiveDifferentialEvolution scored 0.516 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["88e80d96-1cf7-4044-babf-17c9b10b33b1"], "operator": null, "metadata": {"aucs": [0.2239299392551849, 0.47018707286877837, 0.4433517391719298, 0.8764813782924715, 0.4069582809925736, 0.4929145214328704, 0.37820132006288765, 0.39420142553410753, 0.4092425414565247, 0.36135208977297717, 0.7791181036952899, 0.9980523872030389, 0.2784890989796839, 0.4250684318905116, 0.8925501470977757, 0.5649758470946142, 0.4084232755577667, 0.5949163463119979, 0.36582069846173115, 0.5529245890932288]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "d699ff96-480e-469f-b937-a839b7f33e2c", "fitness": "-inf", "name": "HybridAdaptiveDifferentialEvolution", "description": "Hybrid Adaptive Differential Evolution with Lévy flight mutation and covariance matrix adaptation for enhanced exploration and exploitation.", "code": "import numpy as np\n\nclass HybridAdaptiveDifferentialEvolution:\n    def __init__(self, budget=10000, dim=10, pop_size=40, F=0.5, CR=0.7, restart_prob=0.05, CMA_learning_rate=0.1):\n        \"\"\"\n        Args:\n            budget (int): The maximum number of function evaluations.\n            dim (int): The dimensionality of the search space.\n            pop_size (int): The population size.\n            F (float): Mutation factor.\n            CR (float): Crossover rate.\n            restart_prob (float): Probability of restarting a solution.\n            CMA_learning_rate (float): Learning rate for covariance matrix adaptation.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.restart_prob = restart_prob\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.CMA_learning_rate = CMA_learning_rate\n\n        # CMA-ES related parameters initialization\n        self.mean = np.random.uniform(low=-5.0, high=5.0, size=self.dim)  # Initialize mean\n        self.sigma = 0.5  # Initialize step size\n        self.C = np.eye(self.dim)  # Initialize covariance matrix\n        self.pc = np.zeros(self.dim)  # Evolution path for C\n        self.ps = np.zeros(self.dim)  # Evolution path for sigma\n        self.chiN = np.sqrt(self.dim) * (1 - (1 / (4 * self.dim)) + 1 / (16 * self.dim**2))  # Expectation of ||N(0,I)||\n        self.c_sigma = (self.pop_size + 2) / (self.dim + 5) # time constant for cumulation for sigma control\n        self.d_sigma = 1 + 2 * max(0, np.sqrt((self.c_sigma * (2 - self.c_sigma)) * self.dim * self.dim)/(self.pop_size + 2) -1)\n        self.mu = self.pop_size // 2\n        self.weights = np.log(self.pop_size + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        self.c_c = (4 + self.mueff / self.dim) / (self.dim + 4 + 2 * self.mueff / self.dim)\n        self.c_1 = 2 / ((self.dim + 1.3)**2 + self.mueff)\n        self.c_mu = min(1 - self.c_1, 2 * (self.mueff - 2 + 1/self.mueff) / ((self.dim + 2)**2 + self.mueff))\n\n    def levy_flight(self, beta=1.5):\n        \"\"\"\n        Generate a Lévy flight step.\n        \"\"\"\n        sigma = (np.math.gamma(1 + beta) * np.sin(np.pi * beta / 2) / (np.math.gamma((1 + beta) / 2) * beta * (2 ** ((beta - 1) / 2)))) ** (1 / beta)\n        u = np.random.normal(0, sigma, size=self.dim)\n        v = np.random.normal(0, 1, size=self.dim)\n        step = u / (abs(v) ** (1 / beta))\n        return step\n    \n\n    def __call__(self, func):\n        # Initialize population within bounds\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size # Update remaining budget\n\n        # Find the best individual in the initial population\n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index].copy()\n\n        while self.budget > 0:\n            \n            offsprings = np.zeros((self.pop_size, self.dim))\n            for i in range(self.pop_size):\n                # Mutation using CMA-ES sampling and Lévy flight\n                z = np.random.normal(0, 1, size=self.dim) # Sample from normal distribution\n                y = np.dot(np.linalg.cholesky(self.C), z) # Apply covariance\n                mutant = self.mean + self.sigma * y # Add to mean\n                \n                # Incorporate Lévy flight\n                levy_step = self.levy_flight()\n                mutant = mutant + 0.01 * levy_step  # Scale the levy step\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Boundary handling (clip to bounds)\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Restart mechanism (inject diversity)\n                if np.random.rand() < self.restart_prob:\n                    trial = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n\n                # Evaluation\n                f = func(trial)\n                self.budget -= 1\n                \n                offsprings[i] = trial\n                fitness[i] = f\n                \n                # Update global best\n                if f < self.f_opt:\n                    self.f_opt = f\n                    self.x_opt = trial.copy()\n                    \n                if self.budget <= 0:\n                    break\n            \n            # CMA-ES Update\n            idx_sorted = np.argsort(fitness)\n            xmean = np.sum(self.weights[:, None] * offsprings[idx_sorted[:self.mu]], axis=0)\n            \n            y_k = (offsprings[idx_sorted[:self.mu]] - self.mean) / self.sigma\n            \n            self.ps = (1 - self.c_sigma) * self.ps + np.sqrt(self.c_sigma * (2 - self.c_sigma) * self.mueff) * np.dot(np.linalg.inv(np.linalg.cholesky(self.C)), (xmean - self.mean) / self.sigma)\n            \n            if np.linalg.norm(self.ps) / np.sqrt(1 - (1 - self.c_sigma)**(2 * self.budget / self.pop_size)) < self.chiN * (self.dim + 5) / self.dim:\n                hsig = 1\n            else:\n                hsig = 0\n            \n            self.pc = (1 - self.c_c) * self.pc + hsig * np.sqrt(self.c_c * (2 - self.c_c) * self.mueff) * (xmean - self.mean) / self.sigma\n            \n            self.C = (1 - self.c_1 - self.c_mu) * self.C + self.c_1 * (self.pc[:, None] @ self.pc[None, :])\n            \n            for k in range(self.mu):\n                self.C = self.C + self.c_mu * self.weights[k] * (y_k[k, :, None] @ y_k[k, None, :])\n            \n            self.sigma = self.sigma * np.exp((self.c_sigma / self.d_sigma) * (np.linalg.norm(self.ps) / self.chiN - 1))\n            \n            self.mean = xmean\n            population = offsprings.copy() # Simple update\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 10, "feedback": "In the code, line 805, in __getattr__, the following error occurred:\nAttributeError: module 'numpy' has no attribute 'math'. Did you mean: 'emath'?", "error": "In the code, line 805, in __getattr__, the following error occurred:\nAttributeError: module 'numpy' has no attribute 'math'. Did you mean: 'emath'?", "parent_ids": ["4f882f64-ca6f-4d33-b1a5-08b945ed72d5"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "9347515f-5e70-4208-93b3-5a7644922084", "fitness": 0.518868632863738, "name": "AdaptiveDifferentialEvolution", "description": "A simplified Adaptive Differential Evolution with self-adjusting parameters and a combined mutation strategy, featuring a reduced population size and periodic rejuvenation for enhanced exploration and exploitation.", "code": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget=10000, dim=10, pop_size=15, F=0.5, CR=0.7, restart_prob=0.005, rejuvenate_interval=500):\n        \"\"\"\n        Args:\n            budget (int): The maximum number of function evaluations.\n            dim (int): The dimensionality of the search space.\n            pop_size (int): The population size. Reduced for efficiency.\n            F (float): Initial mutation factor.\n            CR (float): Initial crossover rate.\n            restart_prob (float): Probability of restarting a solution. Reduced for efficiency.\n            rejuvenate_interval (int): Number of evaluations between population rejuvenations.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.restart_prob = restart_prob\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.rejuvenate_interval = rejuvenate_interval\n        self.eval_count = 0\n\n\n    def __call__(self, func):\n        # Initialize population within bounds\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size # Update remaining budget\n        self.eval_count += self.pop_size\n\n        # Find the best individual in the initial population\n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index].copy()\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation: Combine current-to-best and random differential\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b = np.random.choice(idxs, 2, replace=False)\n                mutant = population[i] + self.F * (self.x_opt - population[i]) + self.F * (population[a] - population[b])\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Boundary handling (clip to bounds)\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Restart mechanism (inject diversity) - reduced probability\n                if np.random.rand() < self.restart_prob:\n                    trial = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n\n                # Evaluation\n                f = func(trial)\n                self.budget -= 1\n                self.eval_count += 1\n                \n                if f < fitness[i]:\n                    fitness[i] = f\n                    population[i] = trial\n                    \n                    # Update global best\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = trial.copy()\n                        self.F = np.clip(np.random.normal(0.5, 0.1), 0.1, 0.9) # Adapt F\n                        self.CR = np.clip(np.random.normal(0.7, 0.1), 0.1, 0.9) # Adapt CR\n\n                if self.budget <= 0:\n                    break\n\n            # Population Rejuvenation\n            if self.eval_count % self.rejuvenate_interval == 0 and self.budget > 0:\n                worst_index = np.argmax(fitness)\n                population[worst_index] = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                fitness[worst_index] = func(population[worst_index])\n                self.budget -= 1\n                self.eval_count += 1\n                if fitness[worst_index] < self.f_opt:\n                    self.f_opt = fitness[worst_index]\n                    self.x_opt = population[worst_index].copy()\n            \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 11, "feedback": "The algorithm AdaptiveDifferentialEvolution scored 0.519 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["7e8c3a9a-f4fd-485f-9898-a7435c859f8d"], "operator": null, "metadata": {"aucs": [0.2568136485679263, 0.5050863735845375, 0.404661536202951, 0.9627203681392484, 0.7826768852479885, 0.29445840460483674, 0.3049358780226844, 0.36866701725808515, 0.8651236829636624, 0.23930549714785898, 0.6178283242619362, 0.9967296183316633, 0.386886820764484, 0.30109731674208295, 0.5873915626786288, 0.9475219306070573, 0.33202794002550073, 0.4916135354099017, 0.2659530888205962, 0.46587322789312846]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "5b52fa90-07a0-476d-bcdb-3aa7761040bb", "fitness": 0.3407158884675596, "name": "AdaptiveDifferentialEvolution", "description": "Adaptive Differential Evolution with a simplified mutation strategy and self-adaptive parameters for enhanced exploration around the best solution.", "code": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget=10000, dim=10, pop_size=30, F=0.5, CR=0.7):\n        \"\"\"\n        Args:\n            budget (int): The maximum number of function evaluations.\n            dim (int): The dimensionality of the search space.\n            pop_size (int): The population size.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        # Initialize population within bounds\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        # Find the best individual in the initial population\n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index].copy()\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Simplified Mutation: Focus on current-to-best\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a = np.random.choice(idxs)\n\n                mutant = population[i] + self.F * (self.x_opt - population[i]) + self.F * (population[a] - population[i])\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Boundary handling (clip to bounds)\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Evaluation\n                f = func(trial)\n                self.budget -= 1\n\n                if f < fitness[i]:\n                    fitness[i] = f\n                    population[i] = trial\n\n                    # Update global best\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = trial.copy()\n\n                if self.budget <= 0:\n                    break\n\n            # Self-adaptation of F and CR (simplified)\n            self.F = np.clip(np.random.normal(0.5, 0.1), 0.1, 0.9)\n            self.CR = np.clip(np.random.normal(0.7, 0.1), 0.1, 0.9)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 11, "feedback": "The algorithm AdaptiveDifferentialEvolution scored 0.341 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["0b20a372-d5e6-4da8-ba2e-89b36eb59095"], "operator": null, "metadata": {"aucs": [0.14658533199119717, 0.2961907264612197, 0.3001903608271005, 0.2836259189439483, 0.4666026254351301, 0.4035248364540702, 0.3126312026469511, 0.28782447570976233, 0.2044852377467008, 0.19676085160974177, 0.36893808362407743, 0.9978986830288435, 0.33181184658968776, 0.3087542803505098, 0.18859027582448384, 0.3151722049024386, 0.2640696181486121, 0.5097240315972908, 0.18307571406979062, 0.4478614633896364]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "6023ec08-f928-48d9-aa87-348dfb87f7e5", "fitness": 0.7129115230674435, "name": "AdaptiveDifferentialEvolution", "description": "Adaptive Differential Evolution with orthogonal crossover, dynamic population sizing, archive for diversity, and self-adaptive control parameters.", "code": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget=10000, dim=10, pop_size_init=30, archive_size=10, F=0.5, CR=0.7):\n        \"\"\"\n        Args:\n            budget (int): The maximum number of function evaluations.\n            dim (int): The dimensionality of the search space.\n            pop_size_init (int): Initial population size.\n            archive_size (int): Size of the archive for storing diverse solutions.\n            F (float): Mutation factor.\n            CR (float): Crossover rate.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.pop_size_init = pop_size_init\n        self.archive_size = archive_size\n        self.archive = []\n        self.F = F\n        self.CR = CR\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        # Initialize population within bounds\n        pop_size = self.pop_size_init\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= pop_size\n\n        # Find the best individual in the initial population\n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index].copy()\n\n        generation = 0\n        while self.budget > 0:\n            generation += 1\n\n            # Dynamic population size adjustment\n            pop_size = max(10, int(self.pop_size_init * (1 - generation / (self.budget / self.pop_size_init))))\n            if population.shape[0] != pop_size:\n                population = population[:pop_size]\n                fitness = fitness[:pop_size]\n\n\n            for i in range(population.shape[0]): #Iterate over current population\n\n                # Mutation: Current-to-best with differential\n                idxs = [idx for idx in range(population.shape[0]) if idx != i]\n                if len(idxs) < 2:\n                    continue\n\n                a, b = np.random.choice(idxs, 2, replace=False)\n\n                #Choose between population or archive\n                if np.random.rand() < 0.2 and len(self.archive) > 0:\n                    arc_idx = np.random.randint(len(self.archive))\n                    mutant = population[i] + self.F * (self.x_opt - population[i]) + self.F * (population[a] - self.archive[arc_idx])\n                else:\n                    mutant = population[i] + self.F * (self.x_opt - population[i]) + self.F * (population[a] - population[b])\n\n\n                # Orthogonal Crossover\n                trial = self.orthogonal_crossover(population[i], mutant)\n\n                # Boundary handling (clip to bounds)\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Evaluation\n                f = func(trial)\n                self.budget -= 1\n\n                if f < fitness[i]:\n                    # Update archive if the solution is diverse enough\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(population[i].copy())\n                    else:\n                        # Replace the oldest member\n                        self.archive[generation % self.archive_size] = population[i].copy()\n                            \n                    fitness[i] = f\n                    population[i] = trial\n\n                    # Update global best\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = trial.copy()\n\n                if self.budget <= 0:\n                    break\n\n            # Self-adaptation of F and CR (simplified)\n            self.F = np.clip(np.random.normal(0.5, 0.1), 0.1, 0.9)\n            self.CR = np.clip(np.random.normal(0.7, 0.1), 0.1, 0.9)\n            \n            #Elitism - Keep the best individual in the population\n            best_index = np.argmin(fitness)\n            if not np.array_equal(population[best_index], self.x_opt):\n                population[best_index] = self.x_opt.copy()\n                fitness[best_index] = self.f_opt\n            \n\n        return self.f_opt, self.x_opt\n\n    def orthogonal_crossover(self, x, mutant):\n        \"\"\"Performs orthogonal crossover between two individuals.\"\"\"\n        trial = x.copy()\n        num_vectors = 5\n        num_samples = self.dim // num_vectors\n\n        for k in range(num_vectors):\n            # Select indices for this vector\n            indices = np.random.choice(self.dim, num_samples, replace=False, ) if self.dim >= num_samples else np.arange(self.dim)\n            for index in indices:\n                if np.random.rand() < self.CR:\n                    trial[index] = mutant[index]\n        return trial", "configspace": "", "generation": 11, "feedback": "The algorithm AdaptiveDifferentialEvolution scored 0.713 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["71db7b01-9586-425b-ac3b-57051a828533"], "operator": null, "metadata": {"aucs": [0.24083248116562805, 0.5434571971900481, 0.8473719061860132, 0.9182983037176694, 0.8762683290923954, 0.8918539502281502, 0.7397724170770164, 0.830700101957513, 0.8576888636691, 0.20206835651075394, 0.9165337470405123, 0.9954134662501462, 0.32712893350306704, 0.834862252682889, 0.9264719915461203, 0.882946568205698, 0.7411214211321239, 0.9175502224739859, 0.24606959952700325, 0.5218203521930377]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "8c93eae2-6a73-4bbb-aaaf-de370acdc9bf", "fitness": 0.37612503515917767, "name": "AdaptiveDifferentialEvolution", "description": "Enhanced Adaptive Differential Evolution with a dual population structure, adaptive exploration/exploitation balance using a success-history based adaptation of parameters, and a more robust orthogonal crossover.", "code": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget=10000, dim=10, pop_size_init=30, F=0.5, CR=0.7, archive_size=5):\n        \"\"\"\n        Args:\n            budget (int): The maximum number of function evaluations.\n            dim (int): The dimensionality of the search space.\n            pop_size_init (int): Initial population size.\n            F (float): Initial mutation factor.\n            CR (float): Initial crossover rate.\n            archive_size (int): Size of the archive for storing successful F/CR values.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.pop_size_init = pop_size_init\n        self.F = F\n        self.CR = CR\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.archive_F = []\n        self.archive_CR = []\n        self.archive_size = archive_size\n\n    def __call__(self, func):\n        # Initialize population within bounds\n        pop_size = self.pop_size_init\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= pop_size\n\n        # Initialize archive for successful F/CR values\n        self.archive_F = np.full(self.archive_size, self.F)\n        self.archive_CR = np.full(self.archive_size, self.CR)\n\n        # Find the best individual in the initial population\n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index].copy()\n\n        generation = 0\n        while self.budget > 0:\n            generation += 1\n\n            # Dynamic population size adjustment (ensure minimum pop_size)\n            pop_size = max(10, int(self.pop_size_init * (1 - generation / (self.budget / self.pop_size_init))))\n\n            # Second population for exploration\n            pop_size2 = max(5, pop_size // 3)  # Smaller secondary population\n            population2 = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(pop_size2, self.dim))\n            fitness2 = np.array([func(x) for x in population2])\n            self.budget -= pop_size2\n            population = np.vstack((population, population2)) # Combine Populations\n            fitness = np.concatenate((fitness, fitness2))\n\n\n            for i in range(population.shape[0]): #Iterate over current population\n\n                # Adaptation of F and CR from the archive\n                F = np.random.choice(self.archive_F)\n                CR = np.random.choice(self.archive_CR)\n\n                # Mutation: Current-to-best with differential, incorporating F and CR\n                idxs = [idx for idx in range(population.shape[0]) if idx != i]\n                a, b = np.random.choice(idxs, 2, replace=False)\n                mutant = population[i] + F * (self.x_opt - population[i]) + F * (population[a] - population[b])\n\n                # Orthogonal Crossover\n                trial = self.orthogonal_crossover(population[i], mutant, CR)\n\n                # Boundary handling (clip to bounds)\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Evaluation\n                f = func(trial)\n                self.budget -= 1\n\n                if f < fitness[i]:\n                    # Success! Update the archive\n                    self.archive_F = np.concatenate((self.archive_F[1:], [F]))\n                    self.archive_CR = np.concatenate((self.archive_CR[1:], [CR]))\n\n                    fitness[i] = f\n                    population[i] = trial\n\n                    # Update global best\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = trial.copy()\n\n                if self.budget <= 0:\n                    break\n            \n\n            #Elitism - Keep the best individual in the population\n            best_index = np.argmin(fitness)\n            if not np.array_equal(population[best_index], self.x_opt):\n                population[best_index] = self.x_opt.copy()\n                fitness[best_index] = self.f_opt\n\n\n        return self.f_opt, self.x_opt\n\n    def orthogonal_crossover(self, x, mutant, CR):\n        \"\"\"Performs orthogonal crossover between two individuals, parameterized by CR.\"\"\"\n        trial = x.copy()\n        num_vectors = 5  # Can also be made adaptive based on dim\n        num_samples = self.dim // num_vectors\n\n        for k in range(num_vectors):\n            # Select indices for this vector\n            indices = np.random.choice(self.dim, num_samples, replace=False, ) if self.dim >= num_samples else np.arange(self.dim)\n            for index in indices:\n                if np.random.rand() < CR:\n                    trial[index] = mutant[index]\n        return trial", "configspace": "", "generation": 11, "feedback": "The algorithm AdaptiveDifferentialEvolution scored 0.376 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["71db7b01-9586-425b-ac3b-57051a828533"], "operator": null, "metadata": {"aucs": [0.16055553910035503, 0.24792602973121725, 0.36321982025229615, 0.39895267178298566, 0.2923675067188265, 0.3723053084481315, 0.2868661529319261, 0.31443664571942787, 0.3028897192331642, 0.21969618282300096, 0.38156515539750346, 0.9952802939812974, 0.3193305246440059, 0.3016615310231704, 0.7329466552240248, 0.3815638974446651, 0.3088399645872605, 0.4400457833411694, 0.20335671478014772, 0.49869460601897786]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "807670f1-b490-4b9e-86cd-684e61b3b0ea", "fitness": 0.6689224505677025, "name": "EnhancedAdaptiveDifferentialEvolution", "description": "Enhanced Adaptive Differential Evolution with a dynamically adjusted F parameter based on the success rate of the mutation and a simplified covariance matrix adaptation (CMA) inspired learning rate.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDifferentialEvolution:\n    def __init__(self, budget=10000, dim=10, pop_size=20, F=0.5, CR=0.7, restart_prob=0.01, learning_rate=0.1):\n        \"\"\"\n        Args:\n            budget (int): The maximum number of function evaluations.\n            dim (int): The dimensionality of the search space.\n            pop_size (int): The population size.\n            F (float): Initial mutation factor.\n            CR (float): Initial crossover rate.\n            restart_prob (float): Probability of restarting a solution.\n            learning_rate (float): Learning rate for F adaptation.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.restart_prob = restart_prob\n        self.learning_rate = learning_rate\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.success_history = []\n\n    def __call__(self, func):\n        # Initialize population within bounds\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size  # Update remaining budget\n\n        # Find the best individual in the initial population\n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index].copy()\n\n        while self.budget > 0:\n            successful_mutations = 0  # Track successful mutations for F adaptation\n\n            for i in range(self.pop_size):\n                # Mutation: Combine current-to-best and random differential\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b = np.random.choice(idxs, 2, replace=False)\n                mutant = population[i] + self.F * (self.x_opt - population[i]) + self.F * (population[a] - population[b])\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Boundary handling (clip to bounds)\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Restart mechanism (inject diversity) - reduced probability\n                if np.random.rand() < self.restart_prob:\n                    trial = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n\n                # Evaluation\n                f = func(trial)\n                self.budget -= 1\n\n                if f < fitness[i]:\n                    successful_mutations += 1  # Increment success counter\n                    fitness[i] = f\n                    population[i] = trial\n\n                    # Update global best\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = trial.copy()\n                        # Adapt CR using a simplified CMA-inspired approach\n                        self.CR = np.clip(np.random.normal(0.7, 0.1), 0.1, 0.9)  # Adapt CR\n\n                if self.budget <= 0:\n                    break\n\n            # Adapt F based on success rate\n            success_rate = successful_mutations / self.pop_size if self.pop_size > 0 else 0.0\n            self.success_history.append(success_rate)\n            if len(self.success_history) > 10:\n                self.success_history.pop(0)\n\n            avg_success_rate = np.mean(self.success_history)\n            self.F = np.clip(self.F + self.learning_rate * (avg_success_rate - 0.5), 0.1, 0.9)  # Adjust F based on success\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 11, "feedback": "The algorithm EnhancedAdaptiveDifferentialEvolution scored 0.669 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["7e8c3a9a-f4fd-485f-9898-a7435c859f8d"], "operator": null, "metadata": {"aucs": [0.20708135034595554, 0.6541730152814071, 0.8972340771590566, 0.9506329620304949, 0.917762903567468, 0.938493380358076, 0.36600421185723886, 0.7884709826536768, 0.9250321419005075, 0.48193219301265744, 0.3545785307637994, 0.9987717177353397, 0.32881803320743297, 0.5976647634792593, 0.916922876000266, 0.9314887655013564, 0.45029014519456145, 0.9524326231599688, 0.2043646155819514, 0.5162997225635773]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "af2fb312-10c3-49e1-8c61-f0b23c2e748d", "fitness": "-inf", "name": "AdaptiveDifferentialEvolution", "description": "An Adaptive Differential Evolution strategy employing a simplified rank-based mutation coupled with a parameter-free crossover and dynamic population adjustment based on success rate.", "code": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget=10000, dim=10, pop_size=30, F=0.5, CR=0.7, rank_prob=0.1, success_rate_threshold=0.2):\n        \"\"\"\n        Args:\n            budget (int): The maximum number of function evaluations.\n            dim (int): The dimensionality of the search space.\n            pop_size (int): Initial population size.\n            F (float): Mutation factor.\n            CR (float): Crossover rate.\n            rank_prob (float): Probability of using rank-based selection in mutation.\n            success_rate_threshold (float): Threshold for population size adjustment.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.rank_prob = rank_prob\n        self.success_rate_threshold = success_rate_threshold\n        self.success_history = []\n\n    def __call__(self, func):\n        # Initialize population within bounds\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        # Find the best individual in the initial population\n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index].copy()\n\n        while self.budget > 0:\n            # Sort population based on fitness\n            ranked_indices = np.argsort(fitness)\n            best_index = ranked_indices[0]\n\n            # Track success rate for population size adjustment\n            successful_individuals = 0\n\n            for i in range(self.pop_size):\n                # Mutation: Simplified Rank-Based\n                if np.random.rand() < self.rank_prob:\n                    # Select a random individual from the top 50%\n                    rank_index = np.random.choice(ranked_indices[:self.pop_size // 2])\n                    idxs = [idx for idx in range(self.pop_size) if idx != i and idx != rank_index]\n                    a = np.random.choice(idxs)\n                    mutant = population[i] + self.F * (population[rank_index] - population[a])\n                else:\n                     # Mutation based on current best solution\n                    idxs = [idx for idx in range(self.pop_size) if idx != i]\n                    a = np.random.choice(idxs)\n                    mutant = population[i] + self.F * (self.x_opt - population[a])\n\n                # Parameter-Free Crossover (Binomial)\n                trial = np.copy(population[i])\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR:\n                        trial[j] = mutant[j]\n\n                # Boundary handling (clip to bounds)\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Evaluation\n                f = func(trial)\n                self.budget -= 1\n\n                if f < fitness[i]:\n                    fitness[i] = f\n                    population[i] = trial\n                    successful_individuals += 1  # Increment success count\n\n                    # Update global best\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = trial.copy()\n\n                if self.budget <= 0:\n                    break\n\n            # Dynamic Population Size Adjustment (simplified)\n            success_rate = successful_individuals / self.pop_size\n            self.success_history.append(success_rate)\n\n            if len(self.success_history) > 5:\n                recent_success_rate = np.mean(self.success_history[-5:])\n                if recent_success_rate < self.success_rate_threshold and self.pop_size > 10:\n                    # Reduce population size if success rate is low\n                    self.pop_size = max(10, int(self.pop_size * 0.9))\n                    population = population[ranked_indices[:self.pop_size]]  # Truncate population\n                    fitness = fitness[ranked_indices[:self.pop_size]]\n                    print(f\"Reducing pop size to {self.pop_size}\")\n                elif recent_success_rate > self.success_rate_threshold + 0.1 and self.pop_size < 50:\n                    # Increase population size if success rate is high\n                    self.pop_size = min(50, int(self.pop_size * 1.1))\n                    new_individuals = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size - len(population), self.dim))\n                    new_fitness = np.array([func(x) for x in new_individuals])\n                    self.budget -= len(new_individuals)\n                    population = np.vstack((population, new_individuals))\n                    fitness = np.concatenate((fitness, new_fitness))\n                    print(f\"Increasing pop size to {self.pop_size}\")\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 11, "feedback": "Evaluation timed out after 300 seconds.", "error": "Evaluation timed out after 300 seconds.", "parent_ids": ["602c5118-8314-4146-8fe3-d00626a613bd"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "c75e7253-73ae-4b62-9867-2428a3b7b4ce", "fitness": 0.40744333154635815, "name": "CauchyAdaptiveDE", "description": "Population-based algorithm with a Cauchy mutation operator, a novel self-adaptive mechanism for mutation strength, and a selection method favoring diversity.", "code": "import numpy as np\n\nclass CauchyAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size_init=30, F_init=0.5, CR=0.7, cauchy_scale=0.1):\n        \"\"\"\n        Args:\n            budget (int): The maximum number of function evaluations.\n            dim (int): The dimensionality of the search space.\n            pop_size_init (int): Initial population size.\n            F_init (float): Initial mutation factor.\n            CR (float): Crossover rate.\n            cauchy_scale (float): Scale parameter for the Cauchy distribution.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.pop_size_init = pop_size_init\n        self.F_init = F_init\n        self.CR = CR\n        self.cauchy_scale = cauchy_scale\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        # Initialize population within bounds\n        pop_size = self.pop_size_init\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= pop_size\n\n        # Find the best individual in the initial population\n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index].copy()\n\n        generation = 0\n        F = self.F_init * np.ones(pop_size)  # Initialize F for each individual\n\n        while self.budget > 0:\n            generation += 1\n\n            for i in range(population.shape[0]):\n                # Mutation: Cauchy mutation\n                idxs = [idx for idx in range(population.shape[0]) if idx != i]\n                a, b, c = np.random.choice(idxs, 3, replace=False)\n                \n                #Cauchy distributed random variable for mutation strength\n                cauchy_rand = np.random.standard_cauchy() * self.cauchy_scale \n                F[i] = np.abs(cauchy_rand) #Cauchy distribution gives heavy tails and allows for larger jumps. Abs ensures positivity.\n                F[i] = np.clip(F[i], 0.1, 2.0) #clip F to a reasonable range.\n                \n                mutant = population[i] + F[i] * (population[a] - population[b] + population[c] - population[i])  # Modified DE mutation\n\n\n                # Crossover\n                trial = population[i].copy()\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR:\n                        trial[j] = mutant[j]\n\n                # Boundary handling (clip to bounds)\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Evaluation\n                f = func(trial)\n                self.budget -= 1\n\n                # Selection: Diversity-aware selection\n                if f < fitness[i]: # original condition: f < fitness[i]\n                    fitness[i] = f\n                    population[i] = trial\n\n                    # Update global best\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = trial.copy()\n                else:\n                    #If the trial vector is not better, replace a random member with the trial vector\n                    if np.random.rand() < 0.1: #With low probability\n                        rand_index = np.random.randint(0, population.shape[0])\n                        population[rand_index] = trial\n                        fitness[rand_index] = f\n\n\n                if self.budget <= 0:\n                    break\n            \n            # Elitism - Keep the best individual in the population\n            best_index = np.argmin(fitness)\n            if not np.array_equal(population[best_index], self.x_opt):\n                population[best_index] = self.x_opt.copy()\n                fitness[best_index] = self.f_opt\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 11, "feedback": "The algorithm CauchyAdaptiveDE scored 0.407 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["71db7b01-9586-425b-ac3b-57051a828533"], "operator": null, "metadata": {"aucs": [0.13424254634339505, 0.22464688570007108, 0.36088374531349265, 0.19727258824508864, 0.43629296132871853, 0.6073606244881387, 0.321937097373295, 0.36864166883815686, 0.34046693755982316, 0.17769465191184552, 0.4054594089076593, 0.999059964310602, 0.24898670200384354, 0.235963546924698, 0.6992478670373353, 0.5542768147000517, 0.35589953192453727, 0.7237054510969883, 0.26010384677044585, 0.49672379014897783]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "da6a1ec7-f407-4c59-9317-9c06ea0a740a", "fitness": 0.3350204232228676, "name": "AdaptiveDifferentialEvolution", "description": "Adaptive Differential Evolution with dynamic parameter adaptation, simplified mutation, and enhanced local search around the best solution.", "code": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget=10000, dim=10, pop_size=15, F=0.5, CR=0.7, local_search_prob=0.1):\n        \"\"\"\n        Args:\n            budget (int): The maximum number of function evaluations.\n            dim (int): The dimensionality of the search space.\n            pop_size (int): The population size.\n            F (float): Initial mutation factor.\n            CR (float): Initial crossover rate.\n            local_search_prob (float): Probability of performing local search.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.local_search_prob = local_search_prob\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        # Initialize population within bounds\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size # Update remaining budget\n\n        # Find the best individual in the initial population\n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index].copy()\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Simplified Mutation: current-to-best\n                mutant = population[i] + self.F * (self.x_opt - population[i])\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Boundary handling (clip to bounds)\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Local Search around best solution\n                if np.random.rand() < self.local_search_prob:\n                    trial = self.x_opt + np.random.normal(0, 0.05 * (func.bounds.ub - func.bounds.lb), size=self.dim)\n                    trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n\n                # Evaluation\n                f = func(trial)\n                self.budget -= 1\n                \n                if f < fitness[i]:\n                    fitness[i] = f\n                    population[i] = trial\n                    \n                    # Update global best and adapt parameters\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = trial.copy()\n                        self.F = np.clip(np.random.normal(0.5, 0.05), 0.1, 0.9) # Adapt F, reduced std\n                        self.CR = np.clip(np.random.normal(0.7, 0.05), 0.1, 0.9) # Adapt CR, reduced std\n\n                if self.budget <= 0:\n                    break\n            \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 11, "feedback": "The algorithm AdaptiveDifferentialEvolution scored 0.335 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["7e8c3a9a-f4fd-485f-9898-a7435c859f8d"], "operator": null, "metadata": {"aucs": [0.1208812799983836, 0.24040537896805003, 0.34849651456353536, 0.1806073890538522, 0.24254680401516282, 0.36518024688216966, 0.28026139589680243, 0.3381900593440206, 0.28242493054624496, 0.16936922623555406, 0.3585890375501607, 0.99682672541009, 0.2450402141539787, 0.23701923595037155, 0.6626768564567307, 0.3280304229476835, 0.2811365615875565, 0.3660233680373449, 0.17200639735458045, 0.48469641950508013]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "19d6f4a1-b994-469c-90c3-947d55b57ba6", "fitness": 0.0, "name": "HybridDifferentialEvolution", "description": "Hybrid Differential Evolution with dynamic parameter adaptation, stochastic ranking for constraint handling, and a novel local search based on Gaussian perturbation.", "code": "import numpy as np\n\nclass HybridDifferentialEvolution:\n    def __init__(self, budget=10000, dim=10, pop_size=30, F_init=0.5, CR_init=0.7, ls_prob=0.1, rank_prob=0.1, gaussian_scale=0.1):\n        \"\"\"\n        Args:\n            budget (int): The maximum number of function evaluations.\n            dim (int): The dimensionality of the search space.\n            pop_size (int): The population size.\n            F_init (float): Initial mutation factor.\n            CR_init (float): Initial crossover rate.\n            ls_prob (float): Probability of applying local search.\n            rank_prob (float): Probability of using rank-based selection in mutation.\n            gaussian_scale (float): Standard deviation for Gaussian perturbation in local search.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F_init\n        self.CR = CR_init\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.ls_prob = ls_prob\n        self.rank_prob = rank_prob\n        self.gaussian_scale = gaussian_scale\n        self.archive = []\n        self.archive_size = pop_size\n\n    def __call__(self, func):\n        # Initialize population within bounds\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        # Find the best individual in the initial population\n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index].copy()\n\n        while self.budget > 0:\n            # Sort population based on fitness\n            ranked_indices = np.argsort(fitness)\n\n            for i in range(self.pop_size):\n                # Mutation: Rank-based or Best-guided\n                if np.random.rand() < self.rank_prob:\n                    rank_index = np.random.choice(ranked_indices[:self.pop_size // 2])\n                    idxs = [idx for idx in range(self.pop_size) if idx != i and idx != rank_index]\n                    a, b = np.random.choice(idxs, 2, replace=False)\n                    mutant = population[i] + self.F * (population[rank_index] - population[a]) + self.F * (population[b] - population[i])\n                else:\n                    idxs = [idx for idx in range(self.pop_size) if idx != i]\n                    a, b = np.random.choice(idxs, 2, replace=False)\n                    mutant = population[i] + self.F * (self.x_opt - population[a]) + self.F * (population[b] - population[i])\n\n                # Crossover\n                trial = self.crossover(population[i], mutant)\n\n                # Boundary handling\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Local Search\n                if np.random.rand() < self.ls_prob:\n                    trial = self.local_search(trial, func)\n\n                # Evaluation\n                f = func(trial)\n                self.budget -= 1\n\n                # Stochastic Ranking Selection\n                if f < fitness[i] or np.random.rand() < 0.1:  # 10% chance to accept worse solution\n                    fitness[i] = f\n                    population[i] = trial\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = trial.copy()\n\n                if self.budget <= 0:\n                    break\n\n            # Dynamic Parameter Adaptation\n            self.F = np.clip(np.random.normal(0.5, 0.2), 0.1, 0.9)  # Broader range\n            self.CR = np.clip(np.random.normal(0.7, 0.2), 0.1, 0.9)\n\n            # Archive update\n            for i in range(self.pop_size):\n                if len(self.archive) < self.archive_size:\n                    self.archive.append(population[i].copy())\n                else:\n                    # Replace a random element in the archive\n                    replace_index = np.random.randint(0, self.archive_size)\n                    self.archive[replace_index] = population[i].copy()\n\n        return self.f_opt, self.x_opt\n\n    def crossover(self, x, mutant):\n        \"\"\"Performs binomial crossover.\"\"\"\n        trial = x.copy()\n        for j in range(self.dim):\n            if np.random.rand() < self.CR or j == np.random.randint(self.dim):\n                trial[j] = mutant[j]\n        return trial\n\n    def local_search(self, x, func):\n        \"\"\"Performs local search using Gaussian perturbation.\"\"\"\n        x_new = x + np.random.normal(0, self.gaussian_scale, self.dim)\n        x_new = np.clip(x_new, func.bounds.lb, func.bounds.ub)\n        f_new = func(x_new)\n        self.budget -= 1\n\n        if f_new < func(x):\n            return x_new\n        else:\n            return x", "configspace": "", "generation": 12, "feedback": "The algorithm HybridDifferentialEvolution scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["602c5118-8314-4146-8fe3-d00626a613bd"], "operator": null, "metadata": {"aucs": [0]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "5062ece2-7a44-4435-9416-b30499e1e14c", "fitness": "-inf", "name": "HybridDE_NM", "description": "A hybrid algorithm combining Differential Evolution with a Nelder-Mead local search, adaptively selecting search strategies based on population diversity.", "code": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridDE_NM:\n    def __init__(self, budget=10000, dim=10, pop_size_init=30, F=0.5, CR=0.7, nm_iterations=5):\n        \"\"\"\n        Args:\n            budget (int): The maximum number of function evaluations.\n            dim (int): The dimensionality of the search space.\n            pop_size_init (int): Initial population size.\n            F (float): Mutation factor.\n            CR (float): Crossover rate.\n            nm_iterations (int): Number of Nelder-Mead iterations.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.pop_size_init = pop_size_init\n        self.F = F\n        self.CR = CR\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.nm_iterations = nm_iterations\n\n\n    def __call__(self, func):\n        # Initialize population within bounds\n        pop_size = self.pop_size_init\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= pop_size\n\n        # Find the best individual in the initial population\n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index].copy()\n\n        generation = 0\n        while self.budget > 0:\n            generation += 1\n\n            # Dynamic population size adjustment\n            pop_size = max(10, int(self.pop_size_init * (1 - generation / (self.budget / self.pop_size_init))))\n            if population.shape[0] != pop_size:\n                 population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(pop_size, self.dim))\n\n            # Calculate population diversity (standard deviation of fitness values)\n            diversity = np.std(fitness)\n\n            for i in range(population.shape[0]): #Iterate over current population\n\n                # Adaptive Strategy Selection based on diversity\n                if diversity > 0.1:  # High diversity: Exploration\n                    # Mutation: Current-to-rand\n                    idxs = [idx for idx in range(population.shape[0]) if idx != i]\n                    a, b, c = np.random.choice(idxs, 3, replace=False)\n                    mutant = population[i] + self.F * (population[a] - population[b]) + self.F * (population[c] - population[i]) #Added current individual to exploration\n                else:  # Low diversity: Exploitation with Nelder-Mead\n                    # Mutation: Current-to-best\n                    idxs = [idx for idx in range(population.shape[0]) if idx != i]\n                    a, b = np.random.choice(idxs, 2, replace=False)\n                    mutant = population[i] + self.F * (self.x_opt - population[i]) + self.F * (population[a] - population[b])\n                    #Local search with Nelder-Mead\n                    res = minimize(func, population[i], method='Nelder-Mead',\n                                   bounds=np.array(list(zip(func.bounds.lb * np.ones(self.dim), func.bounds.ub * np.ones(self.dim)))),\n                                   options={'maxiter': self.nm_iterations})\n\n                    if self.budget > 0:\n                         f_nm = res.fun\n                         x_nm = res.x\n                         self.budget -= res.nit # Approximation of function calls by Nelder-Mead\n\n                         if f_nm < fitness[i]:\n                              fitness[i] = f_nm\n                              population[i] = x_nm\n                              if f_nm < self.f_opt:\n                                   self.f_opt = f_nm\n                                   self.x_opt = x_nm.copy()\n\n                # Crossover\n                trial = self.binomial_crossover(population[i], mutant)\n\n                # Boundary handling (clip to bounds)\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Evaluation\n                f = func(trial)\n                self.budget -= 1\n\n                if f < fitness[i]:\n                    fitness[i] = f\n                    population[i] = trial\n\n                    # Update global best\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = trial.copy()\n\n                if self.budget <= 0:\n                    break\n\n            # Self-adaptation of F and CR (simplified)\n            self.F = np.clip(np.random.normal(0.5, 0.1), 0.1, 0.9)\n            self.CR = np.clip(np.random.normal(0.7, 0.1), 0.1, 0.9)\n\n            #Elitism - Keep the best individual in the population\n            best_index = np.argmin(fitness)\n            if not np.array_equal(population[best_index], self.x_opt):\n                population[best_index] = self.x_opt.copy()\n                fitness[best_index] = self.f_opt\n\n\n        return self.f_opt, self.x_opt\n\n    def binomial_crossover(self, x, mutant):\n        \"\"\"Performs binomial crossover between two individuals.\"\"\"\n        trial = x.copy()\n        for j in range(self.dim):\n            if np.random.rand() < self.CR or j == np.random.randint(self.dim):\n                trial[j] = mutant[j]\n        return trial", "configspace": "", "generation": 12, "feedback": "In the code, line 107, in __call__, the following error occurred:\nIndexError: index 25 is out of bounds for axis 0 with size 17\nOn line: if not np.array_equal(population[best_index], self.x_opt):", "error": "In the code, line 107, in __call__, the following error occurred:\nIndexError: index 25 is out of bounds for axis 0 with size 17\nOn line: if not np.array_equal(population[best_index], self.x_opt):", "parent_ids": ["71db7b01-9586-425b-ac3b-57051a828533"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "548a8b2f-7834-4674-aab6-729cea86de98", "fitness": 0.4878822553606744, "name": "AdaptiveDifferentialEvolution", "description": "Simplified Adaptive Differential Evolution with current-to-pbest mutation and orthogonal crossover, dynamically adjusting population size and incorporating a diversity archive with adaptive F and CR.", "code": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget=10000, dim=10, pop_size_init=30, archive_size=10, pbest_factor=0.1):\n        \"\"\"\n        Args:\n            budget (int): The maximum number of function evaluations.\n            dim (int): The dimensionality of the search space.\n            pop_size_init (int): Initial population size.\n            archive_size (int): Size of the archive for storing diverse solutions.\n            pbest_factor (float): Percentage of top solutions to consider for pbest selection.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.pop_size_init = pop_size_init\n        self.archive_size = archive_size\n        self.archive = []\n        self.pbest_factor = pbest_factor\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.F = 0.5  # Initialize F\n        self.CR = 0.7 # Initialize CR\n\n    def __call__(self, func):\n        # Initialize population within bounds\n        pop_size = self.pop_size_init\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= pop_size\n\n        # Find the best individual in the initial population\n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index].copy()\n\n        generation = 0\n        while self.budget > 0:\n            generation += 1\n\n            # Dynamic population size adjustment\n            pop_size = max(10, int(self.pop_size_init * (1 - generation / (self.budget / self.pop_size_init))))\n            if population.shape[0] != pop_size:\n                population = population[:pop_size]\n                fitness = fitness[:pop_size]\n\n            # Determine pbest threshold index\n            pbest_threshold_index = max(1, int(self.pbest_factor * pop_size))\n            \n            #Get pbest individuals\n            pbest_indices = np.argsort(fitness)[:pbest_threshold_index]\n\n            for i in range(population.shape[0]): #Iterate over current population\n                # Mutation: Current-to-pbest\n                idxs = [idx for idx in range(population.shape[0]) if idx != i]\n                if len(idxs) < 1:\n                    continue\n\n                a = np.random.choice(idxs, 1, replace=False)[0]\n                pbest_index = np.random.choice(pbest_indices, 1, replace=False)[0]\n\n                #Choose between population or archive\n                if np.random.rand() < 0.2 and len(self.archive) > 0:\n                    arc_idx = np.random.randint(len(self.archive))\n                    mutant = population[i] + self.F * (population[pbest_index] - population[i]) + self.F * (population[a] - self.archive[arc_idx])\n                else:\n                    mutant = population[i] + self.F * (population[pbest_index] - population[i]) + self.F * (population[a] - population[i])\n\n\n                # Orthogonal Crossover\n                trial = self.orthogonal_crossover(population[i], mutant)\n\n                # Boundary handling (clip to bounds)\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Evaluation\n                f = func(trial)\n                self.budget -= 1\n\n                if f < fitness[i]:\n                    # Update archive if the solution is diverse enough\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(population[i].copy())\n                    else:\n                        # Replace the oldest member\n                        self.archive[generation % self.archive_size] = population[i].copy()\n                            \n                    fitness[i] = f\n                    population[i] = trial\n\n                    # Update global best\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = trial.copy()\n\n                if self.budget <= 0:\n                    break\n\n            # Self-adaptation of F and CR (simplified)\n            self.F = np.clip(np.random.normal(0.5, 0.1), 0.1, 0.9)\n            self.CR = np.clip(np.random.normal(0.7, 0.1), 0.1, 0.9)\n\n            #Elitism - Keep the best individual in the population\n            best_index = np.argmin(fitness)\n            if not np.array_equal(population[best_index], self.x_opt):\n                population[best_index] = self.x_opt.copy()\n                fitness[best_index] = self.f_opt\n\n        return self.f_opt, self.x_opt\n\n    def orthogonal_crossover(self, x, mutant):\n        \"\"\"Performs orthogonal crossover between two individuals.\"\"\"\n        trial = x.copy()\n        num_vectors = 5\n        num_samples = self.dim // num_vectors\n\n        for k in range(num_vectors):\n            # Select indices for this vector\n            indices = np.random.choice(self.dim, num_samples, replace=False) if self.dim >= num_samples else np.arange(self.dim)\n            for index in indices:\n                if np.random.rand() < self.CR:\n                    trial[index] = mutant[index]\n        return trial", "configspace": "", "generation": 12, "feedback": "The algorithm AdaptiveDifferentialEvolution scored 0.488 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["6023ec08-f928-48d9-aa87-348dfb87f7e5"], "operator": null, "metadata": {"aucs": [0.24149495138097765, 0.22875562445495345, 0.40701467570586114, 0.9493140246723017, 0.3516128593668594, 0.7669486246555696, 0.3375660359318977, 0.3284808851516693, 0.5234860491495521, 0.2019932173264377, 0.42351059070570507, 0.993279648044687, 0.2797486400649063, 0.5789239112413429, 0.7999290096772439, 0.3626820637567598, 0.36735045796304744, 0.918542742006595, 0.1971176558475085, 0.4998934401096108]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "12e1af1c-6890-4467-a849-401905e99810", "fitness": 0.4682725879773078, "name": "HybridDEBBM", "description": "Hybrid Differential Evolution with Biogeography-Based Migration and stochastic ranking for handling constraints.", "code": "import numpy as np\n\nclass HybridDEBBM:\n    def __init__(self, budget=10000, dim=10, pop_size=30, F=0.5, CR=0.7, migration_rate=0.1, constraint_handling=\"stochastic_ranking\"):\n        \"\"\"\n        Args:\n            budget (int): The maximum number of function evaluations.\n            dim (int): The dimensionality of the search space.\n            pop_size (int): The population size.\n            F (float): Mutation factor.\n            CR (float): Crossover rate.\n            migration_rate (float): Rate of biogeography-based migration.\n            constraint_handling (str): Method for handling constraints (\"stochastic_ranking\").\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.migration_rate = migration_rate\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.constraint_handling = constraint_handling\n\n    def __call__(self, func):\n        # Initialize population within bounds\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        # Find the best individual in the initial population\n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index].copy()\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Differential Evolution Mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = np.random.choice(idxs, 3, replace=False)\n                mutant = population[a] + self.F * (population[b] - population[c])\n\n                # Crossover\n                trial = population[i].copy()\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR:\n                        trial[j] = mutant[j]\n\n                # Boundary handling (clip to bounds)\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Evaluation\n                f = func(trial)\n                self.budget -= 1\n\n                # Selection: Stochastic Ranking\n                if self.constraint_handling == \"stochastic_ranking\":\n                    p_f = 0.45  # Probability of comparing based on objective value\n                    if (np.random.rand() < p_f):\n                        if f < fitness[i]:\n                            fitness[i] = f\n                            population[i] = trial\n                            # Update global best\n                            if f < self.f_opt:\n                                self.f_opt = f\n                                self.x_opt = trial.copy()\n                    else:\n                        #Always accept new solution\n                        fitness[i] = f\n                        population[i] = trial\n                        # Update global best\n                        if f < self.f_opt:\n                            self.f_opt = f\n                            self.x_opt = trial.copy()\n\n                # Biogeography-Based Migration\n                for j in range(self.dim):\n                    if np.random.rand() < self.migration_rate:\n                        # Immigration: Replace part of the individual with information from a better individual\n                        donor_index = np.argmin(fitness)  # Immigrate from the best\n                        population[i][j] = population[donor_index][j]\n\n\n                if self.budget <= 0:\n                    break\n\n            # Adaptive F and CR (optional)\n            self.F = np.clip(np.random.normal(0.5, 0.1), 0.1, 0.9)\n            self.CR = np.clip(np.random.normal(0.7, 0.1), 0.1, 0.9)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 12, "feedback": "The algorithm HybridDEBBM scored 0.468 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["602c5118-8314-4146-8fe3-d00626a613bd"], "operator": null, "metadata": {"aucs": [0.17461699480974602, 0.34567665098013345, 0.37108606680827205, 0.6877533981125779, 0.44129220680922976, 0.614362140789026, 0.33073079127795546, 0.48448034016228236, 0.4486389610221263, 0.3032383971010222, 0.4519920830324078, 0.9930145372143347, 0.26353937701833074, 0.38290307194277806, 0.726904932056072, 0.5222731830049069, 0.3345991922482251, 0.6924301723152393, 0.2922727752637232, 0.5036464875777658]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "ef9c89fc-da13-4ac0-af2d-0feb8d032379", "fitness": "-inf", "name": "AdaptiveDifferentialEvolution", "description": "An enhanced Adaptive Differential Evolution with a novel orthogonal crossover, dynamic diversity control via clustering, and adaptive parameter tuning.", "code": "import numpy as np\nfrom sklearn.cluster import KMeans\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget=10000, dim=10, pop_size_init=30, archive_size=10, F=0.5, CR=0.7, cluster_threshold=0.1):\n        \"\"\"\n        Args:\n            budget (int): The maximum number of function evaluations.\n            dim (int): The dimensionality of the search space.\n            pop_size_init (int): Initial population size.\n            archive_size (int): Size of the archive for storing diverse solutions.\n            F (float): Mutation factor.\n            CR (float): Crossover rate.\n            cluster_threshold(float): Threshold for diversity measure using clustering\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.pop_size_init = pop_size_init\n        self.archive_size = archive_size\n        self.archive = []\n        self.F = F\n        self.CR = CR\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.cluster_threshold = cluster_threshold\n\n    def __call__(self, func):\n        # Initialize population within bounds\n        pop_size = self.pop_size_init\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= pop_size\n\n        # Find the best individual in the initial population\n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index].copy()\n\n        generation = 0\n        while self.budget > 0:\n            generation += 1\n\n            # Dynamic population size adjustment\n            pop_size = max(10, int(self.pop_size_init * (1 - generation / (self.budget / self.pop_size_init))))\n            if population.shape[0] != pop_size:\n                population = population[:pop_size]\n                fitness = fitness[:pop_size]\n\n\n            for i in range(population.shape[0]): #Iterate over current population\n\n                # Mutation: Current-to-best with differential\n                idxs = [idx for idx in range(population.shape[0]) if idx != i]\n                if len(idxs) < 2:\n                    continue\n\n                a, b = np.random.choice(idxs, 2, replace=False)\n\n                #Choose between population or archive\n                if np.random.rand() < 0.2 and len(self.archive) > 0:\n                    arc_idx = np.random.randint(len(self.archive))\n                    mutant = population[i] + self.F * (self.x_opt - population[i]) + self.F * (population[a] - self.archive[arc_idx])\n                else:\n                    mutant = population[i] + self.F * (self.x_opt - population[i]) + self.F * (population[a] - population[b])\n\n\n                # Orthogonal Crossover\n                trial = self.orthogonal_crossover(population[i], mutant, func.bounds.lb, func.bounds.ub)\n\n                # Boundary handling (clip to bounds)\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Evaluation\n                f = func(trial)\n                self.budget -= 1\n\n                if f < fitness[i]:\n                    # Update archive if the solution is diverse enough\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(population[i].copy())\n                    else:\n                        # Replace the oldest member\n                        self.archive[generation % self.archive_size] = population[i].copy()\n                            \n                    fitness[i] = f\n                    population[i] = trial\n\n                    # Update global best\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = trial.copy()\n\n                if self.budget <= 0:\n                    break\n\n            # Self-adaptation of F and CR\n            self.F = np.clip(np.random.normal(0.5, 0.1), 0.1, 0.9)\n            self.CR = np.clip(np.random.normal(0.7, 0.1), 0.1, 0.9)\n            \n            #Elitism - Keep the best individual in the population\n            best_index = np.argmin(fitness)\n            if not np.array_equal(population[best_index], self.x_opt):\n                population[best_index] = self.x_opt.copy()\n                fitness[best_index] = self.f_opt\n            \n            # Diversity Control using Clustering\n            self.diversity_control(population)\n\n        return self.f_opt, self.x_opt\n\n    def orthogonal_crossover(self, x, mutant, lb, ub):\n        \"\"\"Performs orthogonal crossover between two individuals.\"\"\"\n        trial = x.copy()\n        num_vectors = 5\n        num_samples = self.dim // num_vectors\n\n        for k in range(num_vectors):\n            # Select indices for this vector\n            indices = np.random.choice(self.dim, num_samples, replace=False) if self.dim >= num_samples else np.arange(self.dim)\n            for index in indices:\n                if np.random.rand() < self.CR:\n                    trial[index] = mutant[index]\n                # Boundary handling within the crossover itself\n                trial[index] = np.clip(trial[index], lb, ub)\n        return trial\n    \n    def diversity_control(self, population):\n        \"\"\"Maintains diversity by detecting and re-initializing clustered individuals.\"\"\"\n        kmeans = KMeans(n_clusters=min(10, population.shape[0]), random_state=0, n_init = 'auto').fit(population)\n        cluster_labels = kmeans.labels_\n        cluster_centers = kmeans.cluster_centers_\n\n        # Calculate distances of each point to its cluster center\n        distances = np.array([np.linalg.norm(population[i] - cluster_centers[cluster_labels[i]]) for i in range(population.shape[0])])\n\n        # Identify individuals close to their cluster centers\n        threshold = self.cluster_threshold * np.mean(distances)\n        clustered_indices = np.where(distances < threshold)[0]\n\n        # Re-initialize clustered individuals to enhance diversity\n        for i in clustered_indices:\n            population[i] = np.random.uniform(np.min(population), np.max(population), size=self.dim)  # Re-initialize within population bounds\n", "configspace": "", "generation": 12, "feedback": "In the code, line 129, in diversity_control, the following error occurred:\nNameError: name 'KMeans' is not defined. Did you mean: 'kmeans'?\nOn line: kmeans = KMeans(n_clusters=min(10, population.shape[0]), random_state=0, n_init = 'auto').fit(population)", "error": "In the code, line 129, in diversity_control, the following error occurred:\nNameError: name 'KMeans' is not defined. Did you mean: 'kmeans'?\nOn line: kmeans = KMeans(n_clusters=min(10, population.shape[0]), random_state=0, n_init = 'auto').fit(population)", "parent_ids": ["6023ec08-f928-48d9-aa87-348dfb87f7e5"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "afd3285b-72eb-41bf-9859-db129d1f34b5", "fitness": 0.11289885056803543, "name": "AdaptiveDifferentialEvolution", "description": "Adaptive Differential Evolution with dynamic parameter adaptation, a modified mutation strategy utilizing the mean of the population, and a restart mechanism for improved exploration.", "code": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget=10000, dim=10, pop_size=30, F=0.5, CR=0.7, restart_prob=0.05):\n        \"\"\"\n        Args:\n            budget (int): The maximum number of function evaluations.\n            dim (int): The dimensionality of the search space.\n            pop_size (int): The population size.\n            F (float): Initial mutation factor.\n            CR (float): Initial crossover rate.\n            restart_prob (float): Probability of restarting the population.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.restart_prob = restart_prob\n\n    def __call__(self, func):\n        # Initialize population within bounds\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        # Find the best individual in the initial population\n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index].copy()\n\n        while self.budget > 0:\n            # Calculate the mean of the population\n            mean_population = np.mean(population, axis=0)\n\n            for i in range(self.pop_size):\n                # Mutation: Exploit the best solution and population mean\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b = np.random.choice(idxs, 2, replace=False)\n\n                # Modified mutation strategy using population mean\n                mutant = population[i] + self.F * (self.x_opt - population[a]) + self.F * (population[b] - mean_population)\n\n                # Crossover\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial = np.where(cross_points, mutant, population[i])\n\n                # Boundary handling (clip to bounds)\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Evaluation\n                f = func(trial)\n                self.budget -= 1\n\n                if f < fitness[i]:\n                    fitness[i] = f\n                    population[i] = trial\n\n                    # Update global best\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = trial.copy()\n\n                if self.budget <= 0:\n                    break\n\n            # Self-adaptation of F and CR (simplified)\n            self.F = np.clip(np.random.normal(0.5, 0.1), 0.1, 0.9)\n            self.CR = np.clip(np.random.normal(0.7, 0.1), 0.1, 0.9)\n\n            # Restart mechanism: Periodically re-initialize the population\n            if np.random.rand() < self.restart_prob:\n                population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                fitness = np.array([func(x) for x in population])\n                self.budget -= self.pop_size\n\n                best_index = np.argmin(fitness)\n                if fitness[best_index] < self.f_opt:\n                    self.f_opt = fitness[best_index]\n                    self.x_opt = population[best_index].copy()\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 12, "feedback": "The algorithm AdaptiveDifferentialEvolution scored 0.113 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["0b20a372-d5e6-4da8-ba2e-89b36eb59095"], "operator": null, "metadata": {"aucs": [0.14821973762439866, 0.19047681407970762, 0]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "a5ff0308-34a2-4f27-bab1-37f3d6ed7d82", "fitness": "-inf", "name": "AdaptiveDifferentialEvolution", "description": "Adaptive Differential Evolution with a more robust orthogonal crossover based on design of experiments (DOE) and improved parameter adaptation using success history.", "code": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget=10000, dim=10, pop_size=30, F=0.5, CR=0.7, age_limit=50, rank_prob=0.1, archive_size=10):\n        \"\"\"\n        Args:\n            budget (int): The maximum number of function evaluations.\n            dim (int): The dimensionality of the search space.\n            pop_size (int): The population size.\n            F (float): Mutation factor.\n            CR (float): Crossover rate.\n            age_limit (int): Maximum age of an individual before replacement.\n            rank_prob (float): Probability of using rank-based selection in mutation.\n            archive_size (int): Size of the archive for storing successful solutions.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.age_limit = age_limit\n        self.ages = np.zeros(pop_size, dtype=int)\n        self.rank_prob = rank_prob\n        self.archive_size = archive_size\n        self.archive = []\n        self.archive_fitness = []\n        self.success_F = []\n        self.success_CR = []\n        self.memory_size = 10  # Size of success history memory\n        self.memory_F = np.ones(self.memory_size) * 0.5\n        self.memory_CR = np.ones(self.memory_size) * 0.7\n        self.memory_idx = 0\n\n    def __call__(self, func):\n        # Initialize population within bounds\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        # Find the best individual in the initial population\n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index].copy()\n\n        while self.budget > 0:\n            # Sort population based on fitness\n            ranked_indices = np.argsort(fitness)\n\n            for i in range(self.pop_size):\n                # Mutation: Combined strategy\n                if np.random.rand() < self.rank_prob:\n                    # Rank-based selection: choose better individuals with higher probability\n                    rank_index = np.random.choice(ranked_indices[:self.pop_size // 2])  # Select from top half\n                    idxs = [idx for idx in range(self.pop_size) if idx != i and idx != rank_index]\n                    a, b = np.random.choice(idxs, 2, replace=False)\n                    mutant = population[i] + self.F * (population[rank_index] - population[a]) + self.F * (population[b] - population[i])\n                else:\n                    # Exploit the best solution, including archive\n                    if len(self.archive) > 0 and np.random.rand() < 0.1:  # Use archive occasionally\n                        arc_idx = np.random.randint(len(self.archive))\n                        idxs = [idx for idx in range(self.pop_size) if idx != i]\n                        a = np.random.choice(idxs)\n                        mutant = population[i] + self.F * (self.archive[arc_idx] - population[a]) + self.F * (population[ranked_indices[0]] - population[i]) #using archive and current best\n                    else:\n                        idxs = [idx for idx in range(self.pop_size) if idx != i]\n                        a, b = np.random.choice(idxs, 2, replace=False)\n                        mutant = population[i] + self.F * (self.x_opt - population[a]) + self.F * (population[b] - population[i])\n\n                # Improved Orthogonal Crossover using DOE\n                trial = self.orthogonal_crossover(population[i], mutant, fitness)\n\n                # Boundary handling (clip to bounds)\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Evaluation\n                f = func(trial)\n                self.budget -= 1\n\n                if f < fitness[i]:\n                    # Success!\n                    delta_fitness = fitness[i] - f\n                    self.success_F.append(self.F)\n                    self.success_CR.append(self.CR)\n\n                    fitness[i] = f\n                    population[i] = trial\n                    self.ages[i] = 0  # Reset age\n\n                    # Update archive\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(trial.copy())\n                        self.archive_fitness.append(f)\n                    else:\n                        worst_arc_idx = np.argmax(self.archive_fitness)\n                        if f < self.archive_fitness[worst_arc_idx]:\n                            self.archive[worst_arc_idx] = trial.copy()\n                            self.archive_fitness[worst_arc_idx] = f\n\n                    # Update global best\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = trial.copy()\n                else:\n                    self.ages[i] += 1  # Increment age\n\n                # Aging mechanism: replace old individuals\n                if self.ages[i] > self.age_limit:\n                    population[i] = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                    fitness[i] = func(population[i])\n                    self.budget -= 1\n                    self.ages[i] = 0\n\n                if self.budget <= 0:\n                    break\n\n            # Self-adaptation of F and CR using success history\n            self.adapt_parameters()\n\n        return self.f_opt, self.x_opt\n\n    def orthogonal_crossover(self, x, mutant, fitness):\n        \"\"\"Performs orthogonal crossover between two individuals based on DOE.\"\"\"\n        trial = x.copy()\n        doe_level = 2  # Number of levels for DOE (2-level factorial design)\n        num_factors = min(10, self.dim)  # Number of factors (dimensions) to consider\n\n        # Select factors randomly\n        factors = np.random.choice(self.dim, num_factors, replace=False)\n\n        # Generate a 2-level factorial design\n        doe_matrix = np.array([[int(d) for d in bin(i)[2:].zfill(num_factors)] for i in range(doe_level**num_factors)])\n        \n        # Evaluate each design point\n        fitness_values = []\n        for design in doe_matrix:\n            temp_trial = x.copy()\n            for j, factor_index in enumerate(factors):\n                if design[j] == 1:\n                    temp_trial[factor_index] = mutant[factor_index]\n            \n            # No need to evaluate fitness here, use a heuristic to decide the best combination\n            # Simply count how many times the mutant value leads to better fitness\n            num_mutant_values = np.sum(design)\n            fitness_values.append(num_mutant_values)\n\n        # Select the best design point (the combination that uses the mutant values the most)\n        best_design_index = np.argmax(fitness_values)\n        best_design = doe_matrix[best_design_index]\n        \n        # Apply the best design to create the trial vector\n        for j, factor_index in enumerate(factors):\n            if best_design[j] == 1:\n                trial[factor_index] = mutant[factor_index]\n\n        return trial\n    \n    def adapt_parameters(self):\n        \"\"\"Adapt parameters F and CR based on success history.\"\"\"\n        if self.success_F and self.success_CR:\n            # Update F\n            self.memory_F[self.memory_idx] = np.mean(self.success_F)\n            self.F = np.clip(np.random.normal(self.memory_F[self.memory_idx], 0.1), 0.1, 0.9)\n\n            # Update CR\n            self.memory_CR[self.memory_idx] = np.mean(self.success_CR)\n            self.CR = np.clip(np.random.normal(self.memory_CR[self.memory_idx], 0.1), 0.1, 0.9)\n\n            self.memory_idx = (self.memory_idx + 1) % self.memory_size\n\n            # Clear success history\n            self.success_F = []\n            self.success_CR = []\n        else:\n            # If no success, revert to previous values with some noise\n            self.F = np.clip(np.random.normal(self.memory_F[self.memory_idx], 0.1), 0.1, 0.9)\n            self.CR = np.clip(np.random.normal(self.memory_CR[self.memory_idx], 0.1), 0.1, 0.9)", "configspace": "", "generation": 12, "feedback": "Evaluation timed out after 300 seconds.", "error": "Evaluation timed out after 300 seconds.", "parent_ids": ["602c5118-8314-4146-8fe3-d00626a613bd"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "3fd12c15-67e6-4395-ace5-ef0d590e4dfd", "fitness": 0.4370413290374776, "name": "AdaptiveDifferentialEvolution", "description": "Simplified Adaptive Differential Evolution with toroidal mutation and self-adjusting parameters, enhancing exploration while maintaining a balance with exploitation.", "code": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget=10000, dim=10, pop_size=30, F=0.5, CR=0.7):\n        \"\"\"\n        Args:\n            budget (int): The maximum number of function evaluations.\n            dim (int): The dimensionality of the search space.\n            pop_size (int): Population size.\n            F (float): Mutation factor.\n            CR (float): Crossover rate.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        # Initialize population within bounds\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        # Find the best individual in the initial population\n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index].copy()\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation: Toroidal mutation to handle boundary constraints effectively.\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b = np.random.choice(idxs, 2, replace=False)\n\n                mutant = population[i] + self.F * (population[a] - population[b])\n                \n                # Toroidal boundary handling\n                mutant = (mutant - func.bounds.lb) % (func.bounds.ub - func.bounds.lb) + func.bounds.lb\n\n\n                # Crossover\n                trial = population[i].copy()\n                for d in range(self.dim):\n                    if np.random.rand() < self.CR:\n                        trial[d] = mutant[d]\n\n                # Evaluation\n                f = func(trial)\n                self.budget -= 1\n\n                if f < fitness[i]:\n                    fitness[i] = f\n                    population[i] = trial\n\n                    # Update global best\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = trial.copy()\n\n                if self.budget <= 0:\n                    break\n\n            # Self-adaptation of F and CR (simplified)\n            self.F = np.clip(np.random.normal(0.5, 0.1), 0.1, 0.9)\n            self.CR = np.clip(np.random.normal(0.7, 0.1), 0.1, 0.9)\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 12, "feedback": "The algorithm AdaptiveDifferentialEvolution scored 0.437 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["71db7b01-9586-425b-ac3b-57051a828533"], "operator": null, "metadata": {"aucs": [0.17354105558798782, 0.29583672239891257, 0.4003879697184485, 0.567836214364857, 0.327599688276067, 0.5033453154081351, 0.3008887031777887, 0.36554810066625976, 0.34403760444462095, 0.19208408156632673, 0.675904905594156, 0.9962949882919029, 0.3505095794632048, 0.3086048479842487, 0.7746862270859358, 0.4909424942048948, 0.35446217116180645, 0.5949913569362562, 0.21686573635009954, 0.506458818067643]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
