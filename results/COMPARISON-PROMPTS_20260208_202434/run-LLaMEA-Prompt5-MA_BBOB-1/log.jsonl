{"id": "b0bd1928-6045-4e3b-b8bd-1b9183ba6d99", "fitness": 0.41554810590582186, "name": "AdaptiveDE", "description": "Adaptive Differential Evolution with stochastic ranking and archive for constraint handling, combined with a restart mechanism based on stagnation detection.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, CR=0.9, archive_size=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F  # Mutation factor\n        self.CR = CR # Crossover rate\n        self.archive_size = archive_size\n        self.archive = []\n        self.stagnation_threshold = 0.001\n        self.stagnation_count = 0\n        self.best_fitness_history = []\n\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.n_evals = self.pop_size\n        \n        self.best_fitness_history.append(np.min(self.fitness))\n\n\n        while self.n_evals < self.budget:\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = self.population[idxs]\n                v = self.population[i] + self.F * (x_r1 - x_r2)\n\n                # Ensure boundaries\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                u = np.random.rand(self.dim) < self.CR\n                u = v * u + self.population[i] * ~u\n\n                # Evaluation\n                f = func(u)\n                self.n_evals += 1\n\n                # Selection\n                if f < self.fitness[i]:\n                    self.fitness[i] = f\n                    self.population[i] = u\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = u\n\n\n            #Stagnation detection and restart\n            current_best_fitness = np.min(self.fitness)\n            self.best_fitness_history.append(current_best_fitness)\n\n            if len(self.best_fitness_history) > 50: # Check for stagnation after a reasonable number of iterations.\n                if np.abs(self.best_fitness_history[-1] - self.best_fitness_history[-50]) < self.stagnation_threshold:\n                    self.stagnation_count +=1\n                else:\n                    self.stagnation_count = 0\n\n                if self.stagnation_count > 10:\n                    # Restart: Re-initialize the population\n                    self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                    self.fitness = np.array([func(x) for x in self.population])\n                    self.n_evals += self.pop_size\n                    self.stagnation_count = 0 # reset stagnation counter\n                    self.best_fitness_history = []\n                    self.best_fitness_history.append(np.min(self.fitness)) #Record best fitness after restart\n\n\n\n\n            if self.n_evals >= self.budget:\n                break\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveDE scored 0.416 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.15237623854608828, 0.3008128046967117, 0.39015343215147147, 0.5492757565976238, 0.3058578118597368, 0.44193221814416783, 0.30949058686039266, 0.36223439779311317, 0.34099666315477584, 0.19719137582982427, 0.6220886999198544, 0.9877960876952938, 0.37326556351064244, 0.318848179437794, 0.6862723498477645, 0.4493134824928844, 0.33438160962145747, 0.5202970805122117, 0.18483639416696018, 0.4835413852776683]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "8d71c3b9-e3f8-4387-9e77-bd714d2be454", "fitness": "-inf", "name": "CMAES", "description": "Covariance Matrix Adaptation Evolution Strategy with population size adaptation and selective pressure based on function values.", "code": "import numpy as np\n\nclass CMAES:\n    def __init__(self, budget=10000, dim=10, popsize = None, cs = 0.3, damps = None, ccov1 = None, ccovmu = None, mu = None):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = popsize if popsize is not None else 4 + int(3 * np.log(self.dim))\n        self.mu = mu if mu is not None else self.popsize // 2  # Number of parents/selected individuals\n        self.weights = np.log(self.mu + 1) - np.log(np.arange(1, self.mu + 1))\n        self.weights /= np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n\n        self.cs = cs  # Step-size control parameter\n        self.damps = damps if damps is not None else 1 + 2 * np.max([0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1]) + self.cs # Damping for step-size\n        self.ccov1 = ccov1 if ccov1 is not None else 2 / ((self.dim + 1.3)**2 + self.mueff) # Learning rate for rank-one update\n        self.ccovmu = ccovmu if ccovmu is not None else min(1 - self.ccov1, 2 * (self.mueff - 2 + 1/self.mueff) / ((self.dim + 2)**2 + self.mueff)) # Learning rate for rank-mu update\n\n    def __call__(self, func):\n        # Initialize mean, step size, and covariance matrix\n        m = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim) # Mean vector\n        sigma = 0.3 * (func.bounds.ub - func.bounds.lb)  # Global step size\n        C = np.eye(self.dim) # Covariance matrix\n        pc = np.zeros(self.dim)  # Evolution path for C\n        ps = np.zeros(self.dim)  # Evolution path for sigma\n        \n        f_opt = np.inf\n        x_opt = None\n        eval_count = 0\n\n        while eval_count < self.budget:\n            # Sample population\n            z = np.random.randn(self.dim, self.popsize)\n            x = m[:, np.newaxis] + sigma * np.dot(np.linalg.cholesky(C), z)\n            x = np.clip(x, func.bounds.lb, func.bounds.ub) # Clip to bounds\n\n            # Evaluate population\n            f = np.array([func(x[:, i]) for i in range(self.popsize)])\n            eval_count += self.popsize\n\n            # Sort by fitness\n            idx = np.argsort(f)\n            x = x[:, idx]\n            f = f[idx]\n\n            # Update optimal solution\n            if f[0] < f_opt:\n                f_opt = f[0]\n                x_opt = x[:, 0]\n            \n            # Update mean\n            m_old = m.copy()\n            m = np.dot(x[:, :self.mu], self.weights)\n\n            # Update evolution paths\n            ps = (1 - self.cs) * ps + np.sqrt(self.cs * (2 - self.cs) * self.mueff) * np.dot(np.linalg.inv(np.linalg.cholesky(C)), (m - m_old)) / sigma\n            hsig = np.linalg.norm(ps) / np.sqrt(1 - (1 - self.cs)**(2 * eval_count / self.popsize)) < (1.4 + 2 / (self.dim + 1))\n            pc = (1 - self.ccov1) * pc + hsig * np.sqrt(self.ccov1 * (2 - self.ccov1) * self.mueff) * (m - m_old) / sigma\n\n            # Update covariance matrix\n            C = (1 - self.ccov1 - self.ccovmu) * C + self.ccov1 * np.outer(pc, pc) + self.ccovmu * np.dot((x[:, :self.mu] - m_old) / sigma, np.dot(np.diag(self.weights), (x[:, :self.mu] - m_old).T) / sigma)\n            \n            # Enforce symmetry\n            C = np.triu(C) + np.triu(C, 1).T\n            \n            # Handle potential numerical instability\n            try:\n                np.linalg.cholesky(C)\n            except np.linalg.LinAlgError:\n                C = C + 1e-6 * np.eye(self.dim)\n            \n            # Update step size\n            sigma *= np.exp((self.cs / self.damps) * (np.linalg.norm(ps) / np.sqrt(self.dim) - 1))\n\n            # Adaptive Population Size Adjustment based on stagnation\n            if eval_count > self.budget // 2 and np.std(f[:self.mu]) < 1e-8:  #Stagnation detected\n                 self.popsize = min(2 * self.popsize, self.budget // 2) # Increase popsize to explore\n                 self.mu = self.popsize // 2 #Adjust mu\n                 self.weights = np.log(self.mu + 1) - np.log(np.arange(1, self.mu + 1))\n                 self.weights /= np.sum(self.weights)\n                 self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n\n\n            if eval_count >= self.budget:\n                eval_count = self.budget #Correct number of function evaluations\n\n\n        return f_opt, x_opt", "configspace": "", "generation": 0, "feedback": "In the code, line 33, in __call__, the following error occurred:\nValueError: operands could not be broadcast together with shapes (2,) (2,6) \nOn line: x = m[:, np.newaxis] + sigma * np.dot(np.linalg.cholesky(C), z)", "error": "In the code, line 33, in __call__, the following error occurred:\nValueError: operands could not be broadcast together with shapes (2,) (2,6) \nOn line: x = m[:, np.newaxis] + sigma * np.dot(np.linalg.cholesky(C), z)", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "b563063f-14ec-4e97-b0b7-dcc664c07c73", "fitness": "-inf", "name": "OrthogonalCMAES", "description": "Population-based algorithm with covariance matrix adaptation and a novel mutation operator based on orthogonal learning.", "code": "import numpy as np\n\nclass OrthogonalCMAES:\n    def __init__(self, budget=10000, dim=10, popsize=None, sigma0=0.5):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = popsize if popsize is not None else 4 + int(3 * np.log(dim))\n        self.sigma = sigma0\n        self.mean = None  # Initialize in __call__\n        self.C = None     # Covariance matrix, initialized in __call__\n        self.pc = None    # Evolution path for C, initialized in __call__\n        self.ps = None    # Evolution path for sigma, initialized in __call__\n        self.eigenspace_initialized = False\n        self.B = None\n        self.D = None\n        self.c_sigma = None\n        self.c_c = None\n        self.c_1 = None\n        self.c_mu = None\n        self.damps = None\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n\n    def initialize_eigenspace(self):\n        self.B = np.eye(self.dim)\n        self.D = np.ones(self.dim)\n        self.eigenspace_initialized = True\n\n\n    def __call__(self, func):\n        self.mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.c_sigma = (self.mueff + 2) / (self.dim + self.mueff + 5)\n        self.c_c = (4 + self.mueff / self.dim) / (self.dim + 4 + 2 * self.mueff / self.dim)\n        self.c_1 = 2 / ((self.dim + 1.3)**2 + self.mueff)\n        self.c_mu = min(1 - self.c_1, 2 * (self.mueff - 2 + 1/self.mueff) / ((self.dim + 2)**2 + self.mueff))\n        self.damps = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1) + self.c_sigma\n        self.damps = 1/self.damps\n        \n        self.f_opt = np.inf\n        self.x_opt = None\n        evals = 0\n\n        while evals < self.budget:\n            # Generate population\n            z = np.random.randn(self.dim, self.popsize)\n\n            if not self.eigenspace_initialized:\n                self.initialize_eigenspace()\n\n            y = self.B @ (self.D * z)\n            x = self.mean.reshape(-1, 1) + self.sigma * y\n            x = np.clip(x, func.bounds.lb, func.bounds.ub)  # Clip to bounds\n            \n            fitness = np.array([func(x[:, i]) for i in range(self.popsize)])\n            evals += self.popsize\n\n            if np.any(fitness < self.f_opt):\n                best_idx = np.argmin(fitness)\n                if fitness[best_idx] < self.f_opt:\n                  self.f_opt = fitness[best_idx]\n                  self.x_opt = x[:, best_idx].copy()\n                \n\n            # Selection and recombination\n            idx = np.argsort(fitness)\n            x_mu = x[:, idx[:self.mu]]\n            z_mu = z[:, idx[:self.mu]]\n            y_mu = y[:, idx[:self.mu]]\n\n            mean_old = self.mean.copy()\n            self.mean = x_mu @ self.weights\n\n            # Update evolution paths\n            y_w = y_mu @ self.weights\n            self.ps = (1 - self.c_sigma) * self.ps + np.sqrt(self.c_sigma * (2 - self.c_sigma) * self.mueff) * (self.B @ y_w)\n            psnorm = np.sum(self.ps**2)\n            self.sigma = self.sigma * np.exp((self.c_sigma / self.damps) * (psnorm / self.dim - 1) / 2)\n            \n            self.pc = (1 - self.c_c) * self.pc + np.sqrt(self.c_c * (2 - self.c_c) * self.mueff) * (self.mean - mean_old) / self.sigma\n\n            # Orthogonal learning-based mutation\n            if np.random.rand() < 0.2:  # Apply orthogonal learning with probability 0.2\n                num_vectors = min(self.dim, self.mu)  # Number of orthogonal vectors to create\n                V = np.random.randn(self.dim, num_vectors)\n                Q, _ = np.linalg.qr(V)  # Orthonormalize the vectors\n                \n                # Create new candidate solutions using orthogonal directions\n                for i in range(num_vectors):\n                    x_new = self.mean + self.sigma * Q[:, i] * np.random.normal() # Add orthogonal direction to mean\n                    x_new = np.clip(x_new, func.bounds.lb, func.bounds.ub)  # Clip to bounds\n                    f_new = func(x_new)\n                    evals += 1\n                    if f_new < self.f_opt:\n                        self.f_opt = f_new\n                        self.x_opt = x_new.copy()\n\n\n            # Update covariance matrix\n            artmp = (1/self.sigma) * ((x_mu - mean_old) @ self.weights)\n            self.C = (1 - self.c_1 - self.c_mu) * self.C \\\n                     + self.c_1 * (self.pc[:,None] @ self.pc[:,None].T \\\n                     + (1 - self.c_c) * self.C) \\\n                     + self.c_mu * (artmp @ np.diag(self.weights) @ artmp.T)\n            \n            self.C = np.triu(self.C) + np.triu(self.C, 1).T\n\n            try:\n                self.D, self.B = np.linalg.eigh(self.C)  # Eigen decomposition\n                self.D = np.sqrt(np.maximum(self.D, 0))\n                self.eigenspace_initialized = True\n            except np.linalg.LinAlgError:\n                self.C = self.C + 1e-6 * np.eye(self.dim)\n                try:\n                    self.D, self.B = np.linalg.eigh(self.C)  # Eigen decomposition\n                    self.D = np.sqrt(np.maximum(self.D, 0))\n                    self.eigenspace_initialized = True\n                except:\n                    self.eigenspace_initialized = False\n                    self.C = np.eye(self.dim)\n                    self.B = np.eye(self.dim)\n                    self.D = np.ones(self.dim)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "In the code, line 55, in __call__, the following error occurred:\nValueError: operands could not be broadcast together with shapes (2,) (2,6) \nOn line: y = self.B @ (self.D * z)", "error": "In the code, line 55, in __call__, the following error occurred:\nValueError: operands could not be broadcast together with shapes (2,) (2,6) \nOn line: y = self.B @ (self.D * z)", "parent_ids": [], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "d0fddf51-e6ed-4acb-82bd-f12a4f7ace8d", "fitness": 0.5567881975565352, "name": "HybridOptimizer", "description": "A population-based algorithm that combines aspects of differential evolution, particle swarm optimization, and a local search, with adaptive parameter control and a restart mechanism to avoid stagnation.", "code": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget=10000, dim=10, pop_size=20, de_rate=0.5, pso_rate=0.3, local_rate=0.2, restart_trigger=0.05):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.de_rate = de_rate\n        self.pso_rate = pso_rate\n        self.local_rate = local_rate\n        self.restart_trigger = restart_trigger # Percentage of budget to trigger a restart if no improvement\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget_used = self.pop_size\n        \n        self.pbest_population = self.population.copy()\n        self.pbest_fitness = self.fitness.copy()\n\n        self.gbest_index = np.argmin(self.fitness)\n        self.gbest_x = self.population[self.gbest_index].copy()\n        self.gbest_fitness = self.fitness[self.gbest_index].copy()\n        \n        self.f_opt = self.gbest_fitness\n        self.x_opt = self.gbest_x\n\n        self.no_improvement_count = 0\n        self.max_no_improvement = int(self.budget * self.restart_trigger)\n        \n        while self.budget_used < self.budget:\n            new_population = np.zeros_like(self.population)\n\n            # Differential Evolution\n            for i in range(self.pop_size):\n                if np.random.rand() < self.de_rate:\n                    indices = np.random.choice(self.pop_size, 3, replace=False)\n                    x_r1, x_r2, x_r3 = self.population[indices]\n                    mutation = x_r1 + 0.5 * (x_r2 - x_r3)\n                    crossover = np.random.rand(self.dim) < 0.5  # binomial crossover\n                    new_population[i] = np.where(crossover, mutation, self.population[i])\n                \n                # Particle Swarm Optimization\n                elif np.random.rand() < self.pso_rate:\n                    inertia = 0.7\n                    cognitive_coeff = 1.5\n                    social_coeff = 1.5\n                    velocity = inertia * (self.population[i] - self.pbest_population[i]) + cognitive_coeff * np.random.rand(self.dim) * (self.pbest_population[i] - self.population[i]) + social_coeff * np.random.rand(self.dim) * (self.gbest_x - self.population[i])\n                    new_population[i] = self.population[i] + velocity\n                    \n                # Local Search\n                elif np.random.rand() < self.local_rate:\n                    new_population[i] = self.population[i] + np.random.normal(0, 0.1, self.dim)\n\n                # Random exploration if not DE/PSO/Local\n                else:\n                    new_population[i] = np.random.uniform(func.bounds.lb, func.bounds.ub, self.dim)\n                    \n                # Clip the solution\n                new_population[i] = np.clip(new_population[i], func.bounds.lb, func.bounds.ub)\n\n\n            new_fitness = np.array([func(x) for x in new_population])\n            self.budget_used += self.pop_size\n\n            for i in range(self.pop_size):\n                if new_fitness[i] < self.fitness[i]:\n                    self.population[i] = new_population[i]\n                    self.fitness[i] = new_fitness[i]\n\n                    if new_fitness[i] < self.pbest_fitness[i]:\n                        self.pbest_population[i] = new_population[i]\n                        self.pbest_fitness[i] = new_fitness[i]\n\n\n            self.gbest_index = np.argmin(self.fitness)\n            if self.fitness[self.gbest_index] < self.gbest_fitness:\n                self.gbest_x = self.population[self.gbest_index].copy()\n                self.gbest_fitness = self.fitness[self.gbest_index].copy()\n                \n                if self.gbest_fitness < self.f_opt:\n                    self.f_opt = self.gbest_fitness\n                    self.x_opt = self.gbest_x\n                    self.no_improvement_count = 0\n                else:\n                    self.no_improvement_count += 1\n            else:\n                self.no_improvement_count += 1\n\n\n            # Restart mechanism\n            if self.no_improvement_count > self.max_no_improvement:\n                self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                self.fitness = np.array([func(x) for x in self.population])\n\n                self.pbest_population = self.population.copy()\n                self.pbest_fitness = self.fitness.copy()\n\n                self.gbest_index = np.argmin(self.fitness)\n                self.gbest_x = self.population[self.gbest_index].copy()\n                self.gbest_fitness = self.fitness[self.gbest_index].copy()\n                \n                self.no_improvement_count = 0\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm HybridOptimizer scored 0.557 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.23328277322775215, 0.40627393962029956, 0.42395838676535524, 0.24049046545423292, 0.8974247651590233, 0.7857667927299818, 0.3639753622537677, 0.45593734293361554, 0.6799976748934284, 0.27821430008577974, 0.8167921808030385, 1.0, 0.3059672292562148, 0.5162662947914538, 0.8747317051781972, 0.87785760889061, 0.5376498976298036, 0.72685605992583, 0.2402535464995723, 0.47406762503274746]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "cc763cdf-0d5f-4cb2-a838-80b296693f1e", "fitness": "-inf", "name": "HybridCMAES_NM", "description": "A self-adaptive hybrid algorithm combining aspects of CMA-ES with a Nelder-Mead simplex search, dynamically allocating budget based on performance.", "code": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridCMAES_NM:\n    def __init__(self, budget=10000, dim=10, cmaes_popsize=None, nm_trials=5, nm_frac=0.2):\n        self.budget = budget\n        self.dim = dim\n        self.cmaes_popsize = cmaes_popsize if cmaes_popsize is not None else 4 + int(3 * np.log(self.dim))\n        self.nm_trials = nm_trials  # Number of Nelder-Mead trials per CMA-ES iteration\n        self.nm_frac = nm_frac  # Fraction of remaining budget for Nelder-Mead\n\n        self.cmaes = CMAES_Component(budget=self.budget, dim=self.dim, popsize=self.cmaes_popsize)\n\n\n    def __call__(self, func):\n        f_opt, x_opt = self.cmaes(func)\n        remaining_budget = self.budget - self.cmaes.eval_count\n        \n        nm_budget = int(remaining_budget * self.nm_frac)\n\n        for _ in range(self.nm_trials):\n            if remaining_budget <= 0:\n                break\n\n            # Initial guess for Nelder-Mead (perturbed best CMA-ES solution)\n            x0 = x_opt + 0.1 * np.random.randn(self.dim)\n            x0 = np.clip(x0, func.bounds.lb, func.bounds.ub)\n\n            # Perform Nelder-Mead optimization\n            nm_result = minimize(func, x0, method='Nelder-Mead', options={'maxfev': nm_budget // self.nm_trials})\n\n            #Update evaluation count\n            remaining_budget -= nm_result.nfev\n\n            # Update optimal solution if Nelder-Mead finds a better one\n            if nm_result.fun < f_opt:\n                f_opt = nm_result.fun\n                x_opt = nm_result.x\n\n        return f_opt, x_opt\n\nclass CMAES_Component:\n    def __init__(self, budget=10000, dim=10, popsize = None, cs = 0.3, damps = None, ccov1 = None, ccovmu = None, mu = None):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = popsize if popsize is not None else 4 + int(3 * np.log(self.dim))\n        self.mu = mu if mu is not None else self.popsize // 2  # Number of parents/selected individuals\n        self.weights = np.log(self.mu + 1) - np.log(np.arange(1, self.mu + 1))\n        self.weights /= np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n\n        self.cs = cs  # Step-size control parameter\n        self.damps = damps if damps is not None else 1 + 2 * np.max([0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1]) + self.cs # Damping for step-size\n        self.ccov1 = ccov1 if ccov1 is not None else 2 / ((self.dim + 1.3)**2 + self.mueff) # Learning rate for rank-one update\n        self.ccovmu = ccovmu if ccovmu is not None else min(1 - self.ccov1, 2 * (self.mueff - 2 + 1/self.mueff) / ((self.dim + 2)**2 + self.mueff)) # Learning rate for rank-mu update\n        self.eval_count = 0\n\n    def __call__(self, func):\n        # Initialize mean, step size, and covariance matrix\n        m = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim) # Mean vector\n        sigma = 0.3 * (func.bounds.ub - func.bounds.lb)  # Global step size\n        C = np.eye(self.dim) # Covariance matrix\n        pc = np.zeros(self.dim)  # Evolution path for C\n        ps = np.zeros(self.dim)  # Evolution path for sigma\n        \n        f_opt = np.inf\n        x_opt = None\n        self.eval_count = 0\n\n        while self.eval_count < self.budget:\n            # Sample population\n            z = np.random.randn(self.dim, self.popsize)\n            x = m[:, np.newaxis] + sigma * np.dot(np.linalg.cholesky(C), z)\n            x = np.clip(x, func.bounds.lb, func.bounds.ub) # Clip to bounds\n\n            # Evaluate population\n            f = np.array([func(x[:, i]) for i in range(self.popsize)])\n            self.eval_count += self.popsize\n\n            # Sort by fitness\n            idx = np.argsort(f)\n            x = x[:, idx]\n            f = f[idx]\n\n            # Update optimal solution\n            if f[0] < f_opt:\n                f_opt = f[0]\n                x_opt = x[:, 0]\n            \n            # Update mean\n            m_old = m.copy()\n            m = np.dot(x[:, :self.mu], self.weights)\n\n            # Update evolution paths\n            ps = (1 - self.cs) * ps + np.sqrt(self.cs * (2 - self.cs) * self.mueff) * np.dot(np.linalg.inv(np.linalg.cholesky(C)), (m - m_old)) / sigma\n            hsig = np.linalg.norm(ps) / np.sqrt(1 - (1 - self.cs)**(2 * self.eval_count / self.popsize)) < (1.4 + 2 / (self.dim + 1))\n            pc = (1 - self.ccov1) * pc + hsig * np.sqrt(self.ccov1 * (2 - self.ccov1) * self.mueff) * (m - m_old) / sigma\n\n            # Update covariance matrix\n            C = (1 - self.ccov1 - self.ccovmu) * C + self.ccov1 * np.outer(pc, pc) + self.ccovmu * np.dot((x[:, :self.mu] - m_old) / sigma, np.dot(np.diag(self.weights), (x[:, :self.mu] - m_old).T) / sigma)\n            \n            # Enforce symmetry\n            C = np.triu(C) + np.triu(C, 1).T\n            \n            # Handle potential numerical instability\n            try:\n                np.linalg.cholesky(C)\n            except np.linalg.LinAlgError:\n                C = C + 1e-6 * np.eye(self.dim)\n            \n            # Update step size\n            sigma *= np.exp((self.cs / self.damps) * (np.linalg.norm(ps) / np.sqrt(self.dim) - 1))\n\n            # Adaptive Population Size Adjustment based on stagnation\n            if self.eval_count > self.budget // 2 and np.std(f[:self.mu]) < 1e-8:  #Stagnation detected\n                 self.popsize = min(2 * self.popsize, self.budget // 2) # Increase popsize to explore\n                 self.mu = self.popsize // 2 #Adjust mu\n                 self.weights = np.log(self.mu + 1) - np.log(np.arange(1, self.mu + 1))\n                 self.weights /= np.sum(self.weights)\n                 self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n\n\n            if self.eval_count >= self.budget:\n                self.eval_count = self.budget #Correct number of function evaluations\n\n\n        return f_opt, x_opt", "configspace": "", "generation": 1, "feedback": "In the code, line 12, in __init__, the following error occurred:\nNameError: name 'CMAES_Component' is not defined\nOn line: self.cmaes = CMAES_Component(budget=self.budget, dim=self.dim, popsize=self.cmaes_popsize)", "error": "In the code, line 12, in __init__, the following error occurred:\nNameError: name 'CMAES_Component' is not defined\nOn line: self.cmaes = CMAES_Component(budget=self.budget, dim=self.dim, popsize=self.cmaes_popsize)", "parent_ids": ["8d71c3b9-e3f8-4387-9e77-bd714d2be454"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "e16e90d9-ba5f-43ec-b171-d0521e5848f9", "fitness": 0.5632537478110045, "name": "HybridOptimizer", "description": "A simplified hybrid optimizer that dynamically adjusts its exploration-exploitation balance based on the optimization progress using a combination of differential evolution, particle swarm optimization and random search.", "code": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget=10000, dim=10, pop_size=20):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lb = -5.0\n        self.ub = 5.0\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget_used = self.pop_size\n\n        self.best_index = np.argmin(self.fitness)\n        self.best_x = self.population[self.best_index].copy()\n        self.best_fitness = self.fitness[self.best_index].copy()\n\n        self.f_opt = self.best_fitness\n        self.x_opt = self.best_x\n        \n        exploration_rate = 0.7 \n        exploitation_rate = 0.2\n        de_rate = 0.5 #differential evolution rate\n        pso_rate = 0.5 #particle swarm optimisation rate\n\n\n        while self.budget_used < self.budget:\n            new_population = np.zeros_like(self.population)\n            \n            # Dynamic adjustment of exploration/exploitation balance\n            remaining_budget_ratio = (self.budget - self.budget_used) / self.budget\n            current_exploration_rate = exploration_rate * remaining_budget_ratio\n            current_exploitation_rate = exploitation_rate * (1 - remaining_budget_ratio)\n            \n            for i in range(self.pop_size):\n                if np.random.rand() < current_exploration_rate:\n                    #Differential evolution strategy\n                    if np.random.rand() < de_rate:\n                        indices = np.random.choice(self.pop_size, 3, replace=False)\n                        x_r1, x_r2, x_r3 = self.population[indices]\n                        mutation = x_r1 + 0.5 * (x_r2 - x_r3)\n                        crossover = np.random.rand(self.dim) < 0.5  # binomial crossover\n                        new_population[i] = np.where(crossover, mutation, self.population[i])\n                    #Particle swarm optimisation\n                    elif np.random.rand() < pso_rate:\n                        inertia = 0.7\n                        cognitive_coeff = 1.5\n                        social_coeff = 1.5\n                        velocity = inertia * (self.population[i] - self.best_x) + cognitive_coeff * np.random.rand(self.dim) * (self.best_x - self.population[i]) + social_coeff * np.random.rand(self.dim) * (self.best_x - self.population[i])\n                        new_population[i] = self.population[i] + velocity\n                        \n                    # Random exploration\n                    else:\n                        new_population[i] = np.random.uniform(self.lb, self.ub, self.dim)\n\n                elif np.random.rand() < current_exploitation_rate:\n                    # Local search around the best solution\n                    new_population[i] = self.best_x + np.random.normal(0, 0.1, self.dim)\n\n                else:\n                    #Default random exploration\n                     new_population[i] = np.random.uniform(self.lb, self.ub, self.dim)\n\n                new_population[i] = np.clip(new_population[i], self.lb, self.ub)\n\n            new_fitness = np.array([func(x) for x in new_population])\n            self.budget_used += self.pop_size\n\n            for i in range(self.pop_size):\n                if new_fitness[i] < self.fitness[i]:\n                    self.population[i] = new_population[i]\n                    self.fitness[i] = new_fitness[i]\n\n                    if new_fitness[i] < self.best_fitness:\n                        self.best_x = new_population[i].copy()\n                        self.best_fitness = new_fitness[i].copy()\n                        \n                        if self.best_fitness < self.f_opt:\n                            self.f_opt = self.best_fitness\n                            self.x_opt = self.best_x\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "The algorithm HybridOptimizer scored 0.563 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["d0fddf51-e6ed-4acb-82bd-f12a4f7ace8d"], "operator": null, "metadata": {"aucs": [0.26936272547603246, 0.26688796908649914, 0.596866289778965, 0.8482641554708834, 0.44900802438410614, 0.836622191016615, 0.3767136001647434, 0.7886730037956344, 0.7936825080711332, 0.2156703638969697, 0.5495533013815169, 0.9971788404327572, 0.27148050410071933, 0.3368110709755934, 0.7381964994423058, 0.8867063540071922, 0.39877501274970806, 0.8820514014009132, 0.25450076570261737, 0.5080703748851828]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "949c3a0b-d6c7-4c6c-bdb3-e14128bfb420", "fitness": 0.0, "name": "AdaptiveDE", "description": "Simplified Adaptive Differential Evolution with dynamic F/CR adaptation based on population diversity, archive and stagnation-based restarts.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=10, stagnation_threshold=1e-4, stagnation_window=20):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.archive = []\n        self.stagnation_threshold = stagnation_threshold\n        self.stagnation_window = stagnation_window\n        self.best_fitness_history = []\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.n_evals = self.pop_size\n        self.best_fitness_history.append(np.min(self.fitness))\n\n        while self.n_evals < self.budget:\n            # Dynamic F/CR adaptation\n            diversity = np.std(self.population)\n            F = 0.5 + 0.3 * diversity  # Example: F increases with diversity\n            CR = 0.7 + 0.2 * diversity  # Example: CR increases with diversity\n            F = np.clip(F, 0.1, 0.9)\n            CR = np.clip(CR, 0.1, 0.9)\n\n            for i in range(self.pop_size):\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = self.population[idxs]\n                v = self.population[i] + F * (x_r1 - x_r2)\n\n                # Ensure boundaries\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                u = np.random.rand(self.dim) < CR\n                u = v * u + self.population[i] * ~u\n\n                # Evaluation\n                f = func(u)\n                self.n_evals += 1\n\n                # Selection\n                if f < self.fitness[i]:\n                    # Archive best solutions\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(self.population[i].copy())\n                    else:\n                        # Replace a random element in the archive\n                        idx_to_replace = np.random.randint(0, self.archive_size)\n                        self.archive[idx_to_replace] = self.population[i].copy()\n\n                    self.fitness[i] = f\n                    self.population[i] = u\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = u\n\n            # Stagnation detection and restart\n            current_best_fitness = np.min(self.fitness)\n            self.best_fitness_history.append(current_best_fitness)\n\n            if len(self.best_fitness_history) > self.stagnation_window:\n                if np.abs(self.best_fitness_history[-1] - np.mean(self.best_fitness_history[-self.stagnation_window:])) < self.stagnation_threshold:\n                    # Restart: Re-initialize the population, including solutions from the archive\n                    archive_size = min(len(self.archive), self.pop_size // 2) #Take half of the pop size from archive\n                    \n                    if len(self.archive) > 0:\n                        archive_indices = np.random.choice(len(self.archive), archive_size, replace=False)\n                        self.population[:archive_size] = [self.archive[i] for i in archive_indices]\n\n                    #Initialize remaining individuals\n                    self.population[archive_size:] = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size - archive_size, self.dim))\n\n                    self.fitness = np.array([func(x) for x in self.population])\n                    self.n_evals += (self.pop_size - archive_size)\n                    self.best_fitness_history = []\n                    self.best_fitness_history.append(np.min(self.fitness))\n                \n            if self.n_evals >= self.budget:\n                break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "The algorithm AdaptiveDE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["b0bd1928-6045-4e3b-b8bd-1b9183ba6d99"], "operator": null, "metadata": {"aucs": [0]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "16fed2e1-cb8b-4329-b3e9-936fee6629e4", "fitness": 0.07183482567091942, "name": "AdaptiveDE", "description": "Adaptive Differential Evolution with self-adaptive mutation factor, crossover rate, and population size, combined with a neighborhood-based mutation and a restart mechanism based on stagnation detection.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.archive = []\n        self.stagnation_threshold = 0.001\n        self.stagnation_count = 0\n        self.best_fitness_history = []\n        self.F = 0.5 * np.ones(pop_size) # Initialize mutation factor for each individual\n        self.CR = 0.9 * np.ones(pop_size) # Initialize crossover rate for each individual\n        self.min_pop_size = 10\n        self.max_pop_size = 100\n\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.n_evals = self.pop_size\n        \n        self.best_fitness_history.append(np.min(self.fitness))\n\n        while self.n_evals < self.budget:\n            for i in range(self.pop_size):\n                # Adaptive F and CR\n                self.F[i] = np.clip(np.random.normal(0.5, 0.3), 0.1, 1.0)  # Adapt F\n                self.CR[i] = np.clip(np.random.normal(0.9, 0.1), 0.0, 1.0)  # Adapt CR\n\n\n                # Mutation using neighborhood information\n                neighbors = np.random.choice(self.pop_size, 4, replace=False)\n                x_r1, x_r2, x_r3, x_r4 = self.population[neighbors]\n                v = self.population[i] + self.F[i] * (x_r1 - x_r2) + self.F[i] * (x_r3 - x_r4)\n\n\n                # Ensure boundaries\n                v = np.clip(v, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                u = np.random.rand(self.dim) < self.CR[i]\n                u = v * u + self.population[i] * ~u\n\n                # Evaluation\n                f = func(u)\n                self.n_evals += 1\n\n                # Selection\n                if f < self.fitness[i]:\n                    self.fitness[i] = f\n                    self.population[i] = u\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = u\n\n\n            # Stagnation detection and restart\n            current_best_fitness = np.min(self.fitness)\n            self.best_fitness_history.append(current_best_fitness)\n\n            if len(self.best_fitness_history) > 50:  # Check for stagnation after a reasonable number of iterations.\n                if np.abs(self.best_fitness_history[-1] - self.best_fitness_history[-50]) < self.stagnation_threshold:\n                    self.stagnation_count += 1\n                else:\n                    self.stagnation_count = 0\n\n                if self.stagnation_count > 10:\n                    # Adaptive Population Size\n                    if np.random.rand() < 0.5: #Probabilistically change population size\n                       self.pop_size = int(np.clip(self.pop_size * (1 + np.random.normal(0, 0.1)), self.min_pop_size, self.max_pop_size))\n                    \n                    # Restart: Re-initialize the population with updated size\n                    self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                    self.fitness = np.array([func(x) for x in self.population])\n                    self.F = 0.5 * np.ones(self.pop_size) #Reinitialize F and CR\n                    self.CR = 0.9 * np.ones(self.pop_size)\n                    self.n_evals += self.pop_size\n                    self.stagnation_count = 0  # reset stagnation counter\n                    self.best_fitness_history = []\n                    self.best_fitness_history.append(np.min(self.fitness))  # Record best fitness after restart\n\n\n\n\n            if self.n_evals >= self.budget:\n                break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "The algorithm AdaptiveDE scored 0.072 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["b0bd1928-6045-4e3b-b8bd-1b9183ba6d99"], "operator": null, "metadata": {"aucs": [0.14366965134183884, 0]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "db45a104-2d51-4949-814d-1e4817ef7fbd", "fitness": 0.0, "name": "HybridOptimizer", "description": "A hybrid optimization algorithm combining Differential Evolution, Particle Swarm Optimization, and a gradient-based local search, adaptively adjusting exploration/exploitation balance based on performance feedback and employing a restart strategy with memory.", "code": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget=10000, dim=10, pop_size=20, de_rate=0.4, pso_rate=0.3, local_rate=0.3, restart_trigger=0.05, learning_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.de_rate = de_rate\n        self.pso_rate = pso_rate\n        self.local_rate = local_rate\n        self.restart_trigger = restart_trigger\n        self.learning_rate = learning_rate\n        self.memory_size = 10\n        self.f_history = np.inf * np.ones(self.memory_size) # Store the last self.memory_size best function values\n        self.de_success = 0\n        self.pso_success = 0\n        self.local_success = 0\n\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget_used = self.pop_size\n        \n        self.pbest_population = self.population.copy()\n        self.pbest_fitness = self.fitness.copy()\n\n        self.gbest_index = np.argmin(self.fitness)\n        self.gbest_x = self.population[self.gbest_index].copy()\n        self.gbest_fitness = self.fitness[self.gbest_index].copy()\n        \n        self.f_opt = self.gbest_fitness\n        self.x_opt = self.gbest_x\n        self.f_history[0] = self.f_opt\n\n        self.no_improvement_count = 0\n        self.max_no_improvement = int(self.budget * self.restart_trigger)\n        \n        while self.budget_used < self.budget:\n            new_population = np.zeros_like(self.population)\n            de_count = 0\n            pso_count = 0\n            local_count = 0\n\n            # Hybrid Strategy Selection\n            for i in range(self.pop_size):\n                rand_val = np.random.rand()\n                if rand_val < self.de_rate:\n                    # Differential Evolution\n                    indices = np.random.choice(self.pop_size, 3, replace=False)\n                    x_r1, x_r2, x_r3 = self.population[indices]\n                    mutation = x_r1 + 0.5 * (x_r2 - x_r3)\n                    crossover = np.random.rand(self.dim) < 0.5\n                    new_population[i] = np.where(crossover, mutation, self.population[i])\n                    de_count += 1\n                    \n                elif rand_val < self.de_rate + self.pso_rate:\n                    # Particle Swarm Optimization\n                    inertia = 0.7\n                    cognitive_coeff = 1.5\n                    social_coeff = 1.5\n                    velocity = inertia * (new_population[i] - self.pbest_population[i]) + cognitive_coeff * np.random.rand(self.dim) * (self.pbest_population[i] - new_population[i]) + social_coeff * np.random.rand(self.dim) * (self.gbest_x - new_population[i])\n                    new_population[i] = new_population[i] + velocity\n                    pso_count += 1\n                else:\n                    # Local Search (Gradient-based)\n                    gradient = self.approximate_gradient(func, self.population[i])\n                    new_population[i] = self.population[i] - self.learning_rate * gradient\n                    local_count += 1\n\n                new_population[i] = np.clip(new_population[i], func.bounds.lb, func.bounds.ub)\n\n            new_fitness = np.array([func(x) for x in new_population])\n            self.budget_used += self.pop_size\n\n            # Update population\n            for i in range(self.pop_size):\n                if new_fitness[i] < self.fitness[i]:\n                    self.population[i] = new_population[i]\n                    self.fitness[i] = new_fitness[i]\n\n                    if new_fitness[i] < self.pbest_fitness[i]:\n                        self.pbest_population[i] = new_population[i]\n                        self.pbest_fitness[i] = new_fitness[i]\n\n            # Update global best\n            self.gbest_index = np.argmin(self.fitness)\n            if self.fitness[self.gbest_index] < self.gbest_fitness:\n                self.gbest_x = self.population[self.gbest_index].copy()\n                self.gbest_fitness = self.fitness[self.gbest_index].copy()\n                \n                if self.gbest_fitness < self.f_opt:\n                    self.f_opt = self.gbest_fitness\n                    self.x_opt = self.gbest_x\n                    self.no_improvement_count = 0\n                    \n                    # Update success counters for adaptive adjustment\n                    if de_count > 0: self.de_success += 1\n                    if pso_count > 0: self.pso_success += 1\n                    if local_count > 0: self.local_success += 1\n\n                    self.f_history = np.concatenate(([self.f_opt], self.f_history[:-1])) # Add the value to the history\n\n                else:\n                    self.no_improvement_count += 1\n            else:\n                self.no_improvement_count += 1\n\n            # Adaptive adjustment of rates\n            total_success = self.de_success + self.pso_success + self.local_success\n            if total_success > 0:\n                self.de_rate = self.de_success / total_success\n                self.pso_rate = self.pso_success / total_success\n                self.local_rate = self.local_success / total_success\n\n            # Restart mechanism\n            if self.no_improvement_count > self.max_no_improvement:\n                self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                self.fitness = np.array([func(x) for x in self.population])\n\n                self.pbest_population = self.population.copy()\n                self.pbest_fitness = self.fitness.copy()\n\n                self.gbest_index = np.argmin(self.fitness)\n                self.gbest_x = self.population[self.gbest_index].copy()\n                self.gbest_fitness = self.fitness[self.gbest_index].copy()\n                \n                self.no_improvement_count = 0\n                self.de_success = 0\n                self.pso_success = 0\n                self.local_success = 0\n\n        return self.f_opt, self.x_opt\n\n    def approximate_gradient(self, func, x, h=1e-5):\n        gradient = np.zeros_like(x)\n        for i in range(self.dim):\n            x_plus_h = x.copy()\n            x_minus_h = x.copy()\n            x_plus_h[i] += h\n            x_minus_h[i] -= h\n            gradient[i] = (func(x_plus_h) - func(x_minus_h)) / (2 * h)\n        return gradient", "configspace": "", "generation": 1, "feedback": "The algorithm HybridOptimizer scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["d0fddf51-e6ed-4acb-82bd-f12a4f7ace8d"], "operator": null, "metadata": {"aucs": [0]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "7d3ddd42-b55a-4103-8d59-3d365a87eb06", "fitness": "-inf", "name": "CMAES", "description": "CMA-ES with adaptive step size control, covariance matrix adaptation, orthogonal sampling for diversity, and a restart mechanism based on stagnation detection.", "code": "import numpy as np\n\nclass CMAES:\n    def __init__(self, budget=10000, dim=10, popsize = None, cs = 0.3, damps = None, ccov1 = None, ccovmu = None, mu = None, restarts=3):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = popsize if popsize is not None else 4 + int(3 * np.log(self.dim))\n        self.mu = mu if mu is not None else self.popsize // 2  # Number of parents/selected individuals\n        self.weights = np.log(self.mu + 1) - np.log(np.arange(1, self.mu + 1))\n        self.weights /= np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n\n        self.cs = cs  # Step-size control parameter\n        self.damps = damps if damps is not None else 1 + 2 * np.max([0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1]) + self.cs # Damping for step-size\n        self.ccov1 = ccov1 if ccov1 is not None else 2 / ((self.dim + 1.3)**2 + self.mueff) # Learning rate for rank-one update\n        self.ccovmu = ccovmu if ccovmu is not None else min(1 - self.ccov1, 2 * (self.mueff - 2 + 1/self.mueff) / ((self.dim + 2)**2 + self.mueff)) # Learning rate for rank-mu update\n        self.restarts = restarts\n\n    def __call__(self, func):\n        f_opt = np.inf\n        x_opt = None\n        eval_count = 0\n\n        for restart in range(self.restarts):\n            # Initialize mean, step size, and covariance matrix\n            m = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim) # Mean vector\n            sigma = 0.3 * (func.bounds.ub - func.bounds.lb)  # Global step size\n            C = np.eye(self.dim) # Covariance matrix\n            pc = np.zeros(self.dim)  # Evolution path for C\n            ps = np.zeros(self.dim)  # Evolution path for sigma\n            \n            restart_eval_count = 0\n\n            while eval_count < self.budget and restart_eval_count < self.budget // self.restarts:\n                # Sample population\n                z = np.random.randn(self.dim, self.popsize)\n\n                #Orthogonal Sampling (improve diversity)\n                Q, _ = np.linalg.qr(z)\n                z = Q # orthogonal matrix\n\n                x = m[:, np.newaxis] + sigma * np.dot(np.linalg.cholesky(C), z)\n                x = np.clip(x, func.bounds.lb, func.bounds.ub) # Clip to bounds\n\n                # Evaluate population\n                f = np.array([func(x[:, i]) for i in range(self.popsize)])\n                eval_count += self.popsize\n                restart_eval_count += self.popsize\n\n                # Sort by fitness\n                idx = np.argsort(f)\n                x = x[:, idx]\n                f = f[idx]\n\n                # Update optimal solution\n                if f[0] < f_opt:\n                    f_opt = f[0]\n                    x_opt = x[:, 0]\n                \n                # Update mean\n                m_old = m.copy()\n                m = np.dot(x[:, :self.mu], self.weights)\n\n                # Update evolution paths\n                ps = (1 - self.cs) * ps + np.sqrt(self.cs * (2 - self.cs) * self.mueff) * np.dot(np.linalg.inv(np.linalg.cholesky(C)), (m - m_old)) / sigma\n                hsig = np.linalg.norm(ps) / np.sqrt(1 - (1 - self.cs)**(2 * eval_count / self.popsize)) < (1.4 + 2 / (self.dim + 1))\n                pc = (1 - self.ccov1) * pc + hsig * np.sqrt(self.ccov1 * (2 - self.ccov1) * self.mueff) * (m - m_old) / sigma\n\n                # Update covariance matrix\n                C = (1 - self.ccov1 - self.ccovmu) * C + self.ccov1 * np.outer(pc, pc) + self.ccovmu * np.dot((x[:, :self.mu] - m_old) / sigma, np.dot(np.diag(self.weights), (x[:, :self.mu] - m_old).T) / sigma)\n                \n                # Enforce symmetry\n                C = np.triu(C) + np.triu(C, 1).T\n                \n                # Handle potential numerical instability\n                try:\n                    np.linalg.cholesky(C)\n                except np.linalg.LinAlgError:\n                    C = C + 1e-6 * np.eye(self.dim)\n                \n                # Update step size\n                sigma *= np.exp((self.cs / self.damps) * (np.linalg.norm(ps) / np.sqrt(self.dim) - 1))\n\n                # Adaptive Population Size Adjustment based on stagnation\n                if restart_eval_count > (self.budget // self.restarts) // 2 and np.std(f[:self.mu]) < 1e-8:  #Stagnation detected\n                     self.popsize = min(2 * self.popsize, (self.budget // self.restarts) // 2) # Increase popsize to explore\n                     self.mu = self.popsize // 2 #Adjust mu\n                     self.weights = np.log(self.mu + 1) - np.log(np.arange(1, self.mu + 1))\n                     self.weights /= np.sum(self.weights)\n                     self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n\n\n                if eval_count >= self.budget:\n                    eval_count = self.budget #Correct number of function evaluations\n                    restart_eval_count = self.budget\n                    break\n            \n            # End of restart loop, re-initialize only if budget allows\n            if eval_count < self.budget and restart < self.restarts -1 :\n                m = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                sigma = 0.3 * (func.bounds.ub - func.bounds.lb)\n                C = np.eye(self.dim)\n                pc = np.zeros(self.dim)\n                ps = np.zeros(self.dim)\n\n        return f_opt, x_opt", "configspace": "", "generation": 1, "feedback": "In the code, line 46, in __call__, the following error occurred:\nIndexError: index 2 is out of bounds for axis 1 with size 2\nOn line: f = np.array([func(x[:, i]) for i in range(self.popsize)])", "error": "In the code, line 46, in __call__, the following error occurred:\nIndexError: index 2 is out of bounds for axis 1 with size 2\nOn line: f = np.array([func(x[:, i]) for i in range(self.popsize)])", "parent_ids": ["8d71c3b9-e3f8-4387-9e77-bd714d2be454"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "964c36f1-9b00-45c8-bf8f-6ed5db482397", "fitness": 0.40803841961403886, "name": "AdaptiveHybridOptimizer", "description": "Adaptive hybrid algorithm that combines differential evolution with a self-adjusting mutation strategy based on the success rate of past mutations, coupled with a local search refinement.", "code": "import numpy as np\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget=10000, dim=10, pop_size=20, de_rate=0.7, local_rate=0.3, learning_rate=0.1, restart_trigger=0.05):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.de_rate = de_rate\n        self.local_rate = local_rate\n        self.learning_rate = learning_rate\n        self.restart_trigger = restart_trigger\n\n        self.mutation_success_rates = np.ones(self.pop_size) * 0.5  # Initialize mutation success rates\n        self.mutation_scales = np.ones(self.pop_size) * 0.1 # Adaptive mutation scales\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget_used = self.pop_size\n        \n        self.best_index = np.argmin(self.fitness)\n        self.best_x = self.population[self.best_index].copy()\n        self.best_fitness = self.fitness[self.best_index].copy()\n        \n        self.f_opt = self.best_fitness\n        self.x_opt = self.best_x\n\n        self.no_improvement_count = 0\n        self.max_no_improvement = int(self.budget * self.restart_trigger)\n        \n        while self.budget_used < self.budget:\n            new_population = np.zeros_like(self.population)\n            new_fitness = np.zeros(self.pop_size)\n            mutation_success = np.zeros(self.pop_size, dtype=bool)\n\n            for i in range(self.pop_size):\n                # Differential Evolution with adaptive mutation scale\n                if np.random.rand() < self.de_rate:\n                    indices = np.random.choice(self.pop_size, 3, replace=False)\n                    x_r1, x_r2, x_r3 = self.population[indices]\n                    \n                    # Adaptive mutation scale\n                    mutation = x_r1 + self.mutation_scales[i] * (x_r2 - x_r3)\n                    \n                    crossover = np.random.rand(self.dim) < 0.7  # binomial crossover\n                    new_solution = np.where(crossover, mutation, self.population[i])\n                \n                # Local Search\n                elif np.random.rand() < self.local_rate:\n                    new_solution = self.population[i] + np.random.normal(0, 0.05, self.dim) # Reduced standard deviation\n                else:\n                    new_solution = np.random.uniform(func.bounds.lb, func.bounds.ub, self.dim)\n\n                # Clip and evaluate\n                new_solution = np.clip(new_solution, func.bounds.lb, func.bounds.ub)\n                new_fitness_val = func(new_solution)\n                self.budget_used += 1\n                \n                if new_fitness_val < self.fitness[i]:\n                    new_population[i] = new_solution\n                    new_fitness[i] = new_fitness_val\n                    mutation_success[i] = True\n                else:\n                    new_population[i] = self.population[i]\n                    new_fitness[i] = self.fitness[i]\n                    mutation_success[i] = False\n\n            # Update population\n            self.population = new_population\n            self.fitness = new_fitness\n            \n            # Update best solution\n            self.best_index = np.argmin(self.fitness)\n            if self.fitness[self.best_index] < self.best_fitness:\n                self.best_x = self.population[self.best_index].copy()\n                self.best_fitness = self.fitness[self.best_index].copy()\n                \n                if self.best_fitness < self.f_opt:\n                    self.f_opt = self.best_fitness\n                    self.x_opt = self.best_x\n                    self.no_improvement_count = 0\n                else:\n                    self.no_improvement_count += 1\n            else:\n                self.no_improvement_count += 1\n            \n            # Update mutation success rates\n            for i in range(self.pop_size):\n                self.mutation_success_rates[i] = (1 - self.learning_rate) * self.mutation_success_rates[i] + self.learning_rate * mutation_success[i]\n                \n                # Adjust mutation scales based on success rates\n                if self.mutation_success_rates[i] > 0.6:\n                    self.mutation_scales[i] *= 1.1  # Increase scale if successful\n                elif self.mutation_success_rates[i] < 0.4:\n                    self.mutation_scales[i] *= 0.9  # Decrease scale if unsuccessful\n                \n                self.mutation_scales[i] = np.clip(self.mutation_scales[i], 0.01, 1.0)\n\n\n            # Restart mechanism\n            if self.no_improvement_count > self.max_no_improvement:\n                self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                self.fitness = np.array([func(x) for x in self.population])\n\n                self.best_index = np.argmin(self.fitness)\n                self.best_x = self.population[self.best_index].copy()\n                self.best_fitness = self.fitness[self.best_index].copy()\n                \n                self.no_improvement_count = 0\n                self.mutation_success_rates = np.ones(self.pop_size) * 0.5\n                self.mutation_scales = np.ones(self.pop_size) * 0.1\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "The algorithm AdaptiveHybridOptimizer scored 0.408 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["d0fddf51-e6ed-4acb-82bd-f12a4f7ace8d"], "operator": null, "metadata": {"aucs": [0.17766623494746914, 0.23532697869534835, 0.4070174081350141, 0.8856219505529133, 0.28053939196573896, 0.5201682186141108, 0.3258637323902227, 0.41667020477239547, 0.2786502290973887, 0.17763485201354756, 0.2610340545285762, 0.9932050600453611, 0.2755357386339651, 0.2855232730479138, 0.7298314073905067, 0.3338157552032651, 0.30070330097129905, 0.5370046896102035, 0.22314806138208376, 0.5158078502834547]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "e93a4552-04bf-4c47-ac8f-461ac56143a5", "fitness": "-inf", "name": "CMAES", "description": "Covariance Matrix Adaptation Evolution Strategy with adaptive step size, population size adjustment based on stagnation, and a simplified rank-one covariance update to enhance exploration.", "code": "import numpy as np\n\nclass CMAES:\n    def __init__(self, budget=10000, dim=10, popsize=None, cs=0.3, damps=None, ccov1=None, mu=None):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = popsize if popsize is not None else 4 + int(3 * np.log(self.dim))\n        self.mu = mu if mu is not None else self.popsize // 2\n        self.weights = np.log(self.mu + 1) - np.log(np.arange(1, self.mu + 1))\n        self.weights /= np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n\n        self.cs = cs\n        self.damps = damps if damps is not None else 1 + 2 * np.max([0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1]) + self.cs\n        self.ccov1 = ccov1 if ccov1 is not None else 2 / ((self.dim + 1.3)**2 + self.mueff)\n\n    def __call__(self, func):\n        m = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        sigma = 0.3 * (func.bounds.ub - func.bounds.lb)\n        C = np.eye(self.dim)\n        pc = np.zeros(self.dim)\n\n        f_opt = np.inf\n        x_opt = None\n        eval_count = 0\n\n        while eval_count < self.budget:\n            z = np.random.randn(self.dim, self.popsize)\n            x = m[:, np.newaxis] + sigma * np.dot(np.linalg.cholesky(C), z)\n            x = np.clip(x, func.bounds.lb, func.bounds.ub)\n\n            f = np.array([func(x[:, i]) for i in range(self.popsize)])\n            eval_count += self.popsize\n\n            idx = np.argsort(f)\n            x = x[:, idx]\n            f = f[idx]\n\n            if f[0] < f_opt:\n                f_opt = f[0]\n                x_opt = x[:, 0]\n\n            m_old = m.copy()\n            m = np.dot(x[:, :self.mu], self.weights)\n\n            ps = (1 - self.cs) * pc + np.sqrt(self.cs * (2 - self.cs) * self.mueff) * (m - m_old) / sigma\n            pc = ps  #Simplified evolution path update\n\n            C = (1 - self.ccov1) * C + self.ccov1 * np.outer(pc, pc)  # Simplified rank-one update\n\n            # Enforce symmetry and positive definiteness\n            C = np.triu(C) + np.triu(C, 1).T\n            try:\n                np.linalg.cholesky(C)\n            except np.linalg.LinAlgError:\n                C += 1e-6 * np.eye(self.dim)\n\n            sigma *= np.exp((self.cs / self.damps) * (np.linalg.norm(ps) / np.sqrt(self.dim) - 1))\n\n            if eval_count > self.budget // 2 and np.std(f[:self.mu]) < 1e-8:\n                self.popsize = min(2 * self.popsize, self.budget // 2)\n                self.mu = self.popsize // 2\n                self.weights = np.log(self.mu + 1) - np.log(np.arange(1, self.mu + 1))\n                self.weights /= np.sum(self.weights)\n                self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n\n            if eval_count >= self.budget:\n                eval_count = self.budget\n\n        return f_opt, x_opt", "configspace": "", "generation": 1, "feedback": "In the code, line 29, in __call__, the following error occurred:\nValueError: operands could not be broadcast together with shapes (2,) (2,6) \nOn line: x = m[:, np.newaxis] + sigma * np.dot(np.linalg.cholesky(C), z)", "error": "In the code, line 29, in __call__, the following error occurred:\nValueError: operands could not be broadcast together with shapes (2,) (2,6) \nOn line: x = m[:, np.newaxis] + sigma * np.dot(np.linalg.cholesky(C), z)", "parent_ids": ["8d71c3b9-e3f8-4387-9e77-bd714d2be454"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "5e38f191-5068-42cd-8ef2-cf730df2fcac", "fitness": 0.0, "name": "AdaptiveJayaNeighborhood", "description": "A hybrid algorithm that combines a modified version of the Jaya algorithm with a neighborhood search strategy, employing a self-adaptive mechanism to balance exploration and exploitation, coupled with a restart strategy.", "code": "import numpy as np\n\nclass AdaptiveJayaNeighborhood:\n    def __init__(self, budget=10000, dim=10, pop_size=20, neighborhood_rate=0.3, learning_rate=0.1, restart_trigger=0.05):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.neighborhood_rate = neighborhood_rate\n        self.learning_rate = learning_rate\n        self.restart_trigger = restart_trigger\n\n        self.population = None\n        self.fitness = None\n        self.best_x = None\n        self.best_fitness = np.inf\n        self.worst_x = None\n        self.worst_fitness = -np.inf\n\n        self.neighborhood_size = np.ones(self.pop_size) * 0.1  # Adaptive neighborhood sizes\n        self.no_improvement_count = 0\n        self.max_no_improvement = int(self.budget * self.restart_trigger)\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n\n        best_index = np.argmin(self.fitness)\n        self.best_x = self.population[best_index].copy()\n        self.best_fitness = self.fitness[best_index].copy()\n\n        worst_index = np.argmax(self.fitness)\n        self.worst_x = self.population[worst_index].copy()\n        self.worst_fitness = self.fitness[worst_index].copy()\n        \n        self.f_opt = self.best_fitness\n        self.x_opt = self.best_x\n\n    def __call__(self, func):\n        self.budget_used = 0\n        self.initialize_population(func)\n        self.budget_used += self.pop_size\n        \n        while self.budget_used < self.budget:\n            new_population = np.zeros_like(self.population)\n            new_fitness = np.zeros(self.pop_size)\n\n            for i in range(self.pop_size):\n                # Jaya update\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                new_solution_jaya = self.population[i] + r1 * (self.best_x - np.abs(self.population[i])) - r2 * (self.worst_x - np.abs(self.population[i]))\n\n                # Neighborhood search\n                if np.random.rand() < self.neighborhood_rate:\n                    new_solution_neighborhood = self.population[i] + np.random.normal(0, self.neighborhood_size[i], self.dim)\n                else:\n                    new_solution_neighborhood = self.population[i].copy() # No neighborhood search\n\n                # Evaluate both and choose the better one\n                new_solution_jaya = np.clip(new_solution_jaya, func.bounds.lb, func.bounds.ub)\n                new_fitness_jaya = func(new_solution_jaya)\n                self.budget_used += 1\n\n                new_solution_neighborhood = np.clip(new_solution_neighborhood, func.bounds.lb, func.bounds.ub)\n                new_fitness_neighborhood = func(new_solution_neighborhood)\n                self.budget_used += 1\n                \n                if new_fitness_jaya < new_fitness_neighborhood and new_fitness_jaya < self.fitness[i]:\n                    new_population[i] = new_solution_jaya\n                    new_fitness[i] = new_fitness_jaya\n                elif new_fitness_neighborhood < new_fitness_jaya and new_fitness_neighborhood < self.fitness[i]:\n                    new_population[i] = new_solution_neighborhood\n                    new_fitness[i] = new_fitness_neighborhood\n                else:\n                    new_population[i] = self.population[i]\n                    new_fitness[i] = self.fitness[i]\n\n            # Update population\n            self.population = new_population\n            self.fitness = new_fitness\n\n            # Update best and worst\n            best_index = np.argmin(self.fitness)\n            worst_index = np.argmax(self.fitness)\n            \n            if self.fitness[best_index] < self.best_fitness:\n                self.best_x = self.population[best_index].copy()\n                self.best_fitness = self.fitness[best_index].copy()\n                \n                if self.best_fitness < self.f_opt:\n                    self.f_opt = self.best_fitness\n                    self.x_opt = self.best_x\n                    self.no_improvement_count = 0\n                else:\n                    self.no_improvement_count += 1\n            else:\n                self.no_improvement_count += 1\n                \n            self.worst_x = self.population[worst_index].copy()\n            self.worst_fitness = self.fitness[worst_index].copy()\n\n            # Adjust neighborhood size\n            for i in range(self.pop_size):\n                if self.fitness[i] < self.best_fitness:\n                    self.neighborhood_size[i] *= (1 + self.learning_rate) # Increase if better than current best\n                else:\n                    self.neighborhood_size[i] *= (1 - self.learning_rate) # Decrease if worse\n\n                self.neighborhood_size[i] = np.clip(self.neighborhood_size[i], 0.01, 0.5) #Limit the size\n                \n            if self.no_improvement_count > self.max_no_improvement:\n                self.initialize_population(func) # Restart\n                self.budget_used += self.pop_size\n                self.no_improvement_count = 0\n                self.neighborhood_size = np.ones(self.pop_size) * 0.1 # Reset the size\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "The algorithm AdaptiveJayaNeighborhood scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["964c36f1-9b00-45c8-bf8f-6ed5db482397"], "operator": null, "metadata": {"aucs": [0]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "53c1e70f-2503-4e9f-b848-9b8834f4a89a", "fitness": 0.4622352921921024, "name": "HybridOptimizer", "description": "Adaptive hybrid optimizer that adjusts exploration-exploitation balance using a success-history based adaptation and a simplified local search.", "code": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget=10000, dim=10, pop_size=20):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lb = -5.0\n        self.ub = 5.0\n        self.archive_size = int(self.budget/100)\n\n        self.archive = []\n        self.archive_fitness = []\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget_used = self.pop_size\n\n        self.best_index = np.argmin(self.fitness)\n        self.best_x = self.population[self.best_index].copy()\n        self.best_fitness = self.fitness[self.best_index].copy()\n\n        self.f_opt = self.best_fitness\n        self.x_opt = self.best_x\n        \n        exploration_rate = 0.7\n        learning_rate = 0.1\n\n        while self.budget_used < self.budget:\n            new_population = np.zeros_like(self.population)\n            new_fitness = np.zeros(self.pop_size)\n\n            for i in range(self.pop_size):\n                if np.random.rand() < exploration_rate:\n                    # Differential Evolution with Archive\n                    if len(self.archive) > 0:\n                        idx = np.random.randint(len(self.archive))\n                        x_archive = self.archive[idx]\n                        indices = np.random.choice(self.pop_size, 2, replace=False)\n                        x_r1, x_r2 = self.population[indices]\n                        mutation = x_r1 + 0.5 * (x_r2 - self.population[i]) + 0.5 * (x_archive - self.population[i])  \n                    else:\n                        indices = np.random.choice(self.pop_size, 3, replace=False)\n                        x_r1, x_r2, x_r3 = self.population[indices]\n                        mutation = x_r1 + 0.5 * (x_r2 - x_r3)\n\n                    crossover = np.random.rand(self.dim) < 0.5  # binomial crossover\n                    new_population[i] = np.where(crossover, mutation, self.population[i])\n                else:\n                    # Simplified Local Search around the best\n                    new_population[i] = self.best_x + np.random.normal(0, 0.05, self.dim)\n                \n                new_population[i] = np.clip(new_population[i], self.lb, self.ub)\n                new_fitness[i] = func(new_population[i])\n                self.budget_used += 1\n                if self.budget_used >= self.budget:\n                    break\n\n            for i in range(self.pop_size):\n                if new_fitness[i] < self.fitness[i]:\n                    #Update population\n                    self.population[i] = new_population[i]\n                    self.fitness[i] = new_fitness[i]\n                    \n                    # Update Archive\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(new_population[i].copy())\n                        self.archive_fitness.append(new_fitness[i])\n                    else:\n                        # Replace the worst element in archive with a small probability\n                        if np.random.rand() < learning_rate:\n                            worst_index = np.argmax(self.archive_fitness)\n                            self.archive[worst_index] = new_population[i].copy()\n                            self.archive_fitness[worst_index] = new_fitness[i]\n                            \n\n                    if new_fitness[i] < self.best_fitness:\n                        self.best_x = new_population[i].copy()\n                        self.best_fitness = new_fitness[i].copy()\n                        \n                        if self.best_fitness < self.f_opt:\n                            self.f_opt = self.best_fitness\n                            self.x_opt = self.best_x\n            if self.budget_used >= self.budget:\n                break\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "The algorithm HybridOptimizer scored 0.462 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["e16e90d9-ba5f-43ec-b171-d0521e5848f9"], "operator": null, "metadata": {"aucs": [0.20732063229244158, 0.16443350731023676, 0.47144632927676855, 0.9470981178182539, 0.3609621966966816, 0.5619636973555411, 0.34904077757026364, 0.4315822742855884, 0.4123781133398565, 0.1952256734063076, 0.6685881363577854, 0.995138908468721, 0.24979348660078926, 0.32176730412833654, 0.8102463532733456, 0.3202695619405981, 0.40824777887337227, 0.6278860623550192, 0.28282752969770386, 0.4584894027944387]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "0cf7e850-76ce-4666-a89a-619569e94679", "fitness": 0.3377365253242297, "name": "HybridOptimizer", "description": "Dynamically adjusts exploration-exploitation by shifting focus from global search to refinement around the best solution, utilizing differential evolution with a focused perturbation and a constrained random walk for fine-tuning.", "code": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget=10000, dim=10, pop_size=20):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lb = -5.0\n        self.ub = 5.0\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget_used = self.pop_size\n\n        self.best_index = np.argmin(self.fitness)\n        self.best_x = self.population[self.best_index].copy()\n        self.best_fitness = self.fitness[self.best_index].copy()\n\n        self.f_opt = self.best_fitness\n        self.x_opt = self.best_x\n        \n        while self.budget_used < self.budget:\n            new_population = np.zeros_like(self.population)\n            new_fitness = np.zeros(self.pop_size)\n\n            # Dynamic adjustment of exploration/exploitation balance\n            remaining_budget_ratio = (self.budget - self.budget_used) / self.budget\n            exploration_prob = 0.3 * remaining_budget_ratio\n            \n            for i in range(self.pop_size):\n                if np.random.rand() < exploration_prob:\n                    # Differential Evolution with focused perturbation\n                    indices = np.random.choice(self.pop_size, 3, replace=False)\n                    x_r1, x_r2, x_r3 = self.population[indices]\n                    mutation = self.best_x + 0.1 * (x_r1 - x_r2) # Perturbation around best\n                    new_population[i] = np.clip(mutation, self.lb, self.ub)\n                else:\n                    # Constrained Random Walk around the best\n                    step_size = 0.05 * (self.ub - self.lb) * (1 - remaining_budget_ratio) # Reduced step size\n                    new_population[i] = self.best_x + np.random.uniform(-step_size, step_size, self.dim)\n                    new_population[i] = np.clip(new_population[i], self.lb, self.ub)\n                \n                new_fitness[i] = func(new_population[i])\n                self.budget_used += 1\n\n                if new_fitness[i] < self.fitness[i]:\n                    self.population[i] = new_population[i]\n                    self.fitness[i] = new_fitness[i]\n                    if new_fitness[i] < self.best_fitness:\n                        self.best_fitness = new_fitness[i]\n                        self.best_x = new_population[i].copy()\n                        \n                        if self.best_fitness < self.f_opt:\n                            self.f_opt = self.best_fitness\n                            self.x_opt = self.best_x\n                            \n                if self.budget_used >= self.budget:\n                    break\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "The algorithm HybridOptimizer scored 0.338 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["e16e90d9-ba5f-43ec-b171-d0521e5848f9"], "operator": null, "metadata": {"aucs": [0.0676479593233742, 0.1751687697925709, 0.4749154009015051, 0.16608843948928043, 0.1492034966819865, 0.3227800565920774, 0.20081394881354153, 0.2796878545387075, 0.23585978511178396, 0.1617991683997494, 0.31754608885046187, 0.9988297263414053, 0.261340211534713, 0.18416363504853817, 0.7961613062118735, 0.38597690543561025, 0.30370307700312826, 0.7047081061842417, 0.15465499648163328, 0.41368157374841275]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "2f563a1f-fa0f-49f8-8ae2-6988639215ce", "fitness": 0.5011136162925203, "name": "HybridOptimizer", "description": "Adaptive hybrid optimizer with simplified parameter control, focusing on population diversity and local search intensification around promising solutions.", "code": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget=10000, dim=10, pop_size=20, local_rate=0.1, restart_trigger=0.05):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.local_rate = local_rate\n        self.restart_trigger = restart_trigger\n        self.exploration_rate = 0.5  # Probability of exploring a completely new solution\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget_used = self.pop_size\n\n        self.best_index = np.argmin(self.fitness)\n        self.best_x = self.population[self.best_index].copy()\n        self.best_fitness = self.fitness[self.best_index].copy()\n        self.f_opt = self.best_fitness\n        self.x_opt = self.best_x\n\n        self.no_improvement_count = 0\n        self.max_no_improvement = int(self.budget * self.restart_trigger)\n\n        while self.budget_used < self.budget:\n            new_population = np.zeros_like(self.population)\n            new_fitness = np.zeros(self.pop_size)\n\n            for i in range(self.pop_size):\n                # Exploit: Local search around the best solution\n                if np.random.rand() < self.local_rate:\n                    new_solution = self.best_x + np.random.normal(0, 0.1, self.dim)\n                    new_solution = np.clip(new_solution, func.bounds.lb, func.bounds.ub) # Clipping\n\n                # Explore: Create a new solution based on current population\n                else:\n                    if np.random.rand() < self.exploration_rate:\n                        new_solution = np.random.uniform(func.bounds.lb, func.bounds.ub, self.dim) # pure exploration\n                    else:\n                        # Differential Evolution inspired exploration\n                        indices = np.random.choice(self.pop_size, 3, replace=False)\n                        x_r1, x_r2, x_r3 = self.population[indices]\n                        new_solution = x_r1 + 0.5 * (x_r2 - x_r3)\n                        new_solution = np.clip(new_solution, func.bounds.lb, func.bounds.ub) # Clipping\n\n\n                new_fitness_i = func(new_solution)\n                self.budget_used += 1\n                new_population[i] = new_solution\n                new_fitness[i] = new_fitness_i\n\n\n            for i in range(self.pop_size):\n                if new_fitness[i] < self.fitness[i]:\n                    self.population[i] = new_population[i]\n                    self.fitness[i] = new_fitness[i]\n\n            self.best_index = np.argmin(self.fitness)\n            if self.fitness[self.best_index] < self.best_fitness:\n                self.best_x = self.population[self.best_index].copy()\n                self.best_fitness = self.fitness[self.best_index].copy()\n                self.no_improvement_count = 0\n                if self.best_fitness < self.f_opt:\n                    self.f_opt = self.best_fitness\n                    self.x_opt = self.best_x\n            else:\n                self.no_improvement_count += 1\n\n            if self.no_improvement_count > self.max_no_improvement:\n                self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                self.fitness = np.array([func(x) for x in self.population])\n                self.best_index = np.argmin(self.fitness)\n                self.best_x = self.population[self.best_index].copy()\n                self.best_fitness = self.fitness[self.best_index].copy()\n                self.no_improvement_count = 0\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "The algorithm HybridOptimizer scored 0.501 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["d0fddf51-e6ed-4acb-82bd-f12a4f7ace8d"], "operator": null, "metadata": {"aucs": [0.2647741612254252, 0.2088272522214326, 0.4511289561261267, 0.8969686691564427, 0.626233039310975, 0.5805446051357581, 0.3150678375920061, 0.6913585062046685, 0.41036235335045, 0.17460007627969376, 0.3753607551849142, 1.0, 0.25350068910869106, 0.5036363294959003, 0.8347576679089684, 0.5356549435874765, 0.3647637930565918, 0.8858561119997435, 0.1910501661163404, 0.45782641278880043]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "fd8a08e6-5531-4063-b767-af00fd27d370", "fitness": "-inf", "name": "AdaptiveHybridOptimizer", "description": "An adaptive hybrid algorithm that uses a combination of Differential Evolution, Gaussian Local Search, and Nelder-Mead Simplex, adaptively adjusting their probabilities based on success rates.", "code": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget=10000, dim=10, pop_size=20, de_rate=0.4, ls_rate=0.3, nm_rate=0.3):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lb = -5.0\n        self.ub = 5.0\n        self.de_rate = de_rate\n        self.ls_rate = ls_rate\n        self.nm_rate = nm_rate\n        self.success_history = {'de': 0, 'ls': 0, 'nm': 0}\n        self.attempt_history = {'de': 0, 'ls': 0, 'nm': 0}\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget_used = self.pop_size\n\n        self.best_index = np.argmin(self.fitness)\n        self.best_x = self.population[self.best_index].copy()\n        self.best_fitness = self.fitness[self.best_index].copy()\n\n        self.f_opt = self.best_fitness\n        self.x_opt = self.best_x\n\n        while self.budget_used < self.budget:\n            new_population = np.zeros_like(self.population)\n            new_fitness = np.zeros_like(self.fitness)\n\n            # Adaptive adjustment of probabilities\n            total_attempts = sum(self.attempt_history.values())\n            de_prob = (self.attempt_history['de'] > 0) * (self.success_history['de'] / self.attempt_history['de']) if self.attempt_history['de'] > 0 else self.de_rate\n            ls_prob = (self.attempt_history['ls'] > 0) * (self.success_history['ls'] / self.attempt_history['ls']) if self.attempt_history['ls'] > 0 else self.ls_rate\n            nm_prob = (self.attempt_history['nm'] > 0) * (self.success_history['nm'] / self.attempt_history['nm']) if self.attempt_history['nm'] > 0 else self.nm_rate\n\n            probs = np.array([de_prob, ls_prob, nm_prob])\n            probs = probs / np.sum(probs) # Normalize probabilities\n            \n            for i in range(self.pop_size):\n                rand = np.random.rand()\n\n                if rand < probs[0]: # Differential Evolution\n                    self.attempt_history['de'] += 1\n                    indices = np.random.choice(self.pop_size, 3, replace=False)\n                    x_r1, x_r2, x_r3 = self.population[indices]\n                    mutation = x_r1 + 0.5 * (x_r2 - x_r3)\n                    crossover = np.random.rand(self.dim) < 0.5\n                    new_x = np.where(crossover, mutation, self.population[i])\n                    new_x = np.clip(new_x, self.lb, self.ub)\n                    new_f = func(new_x)\n                    self.budget_used += 1\n                    if new_f < self.fitness[i]:\n                        self.success_history['de'] += 1\n                        new_population[i] = new_x\n                        new_fitness[i] = new_f\n                    else:\n                        new_population[i] = self.population[i]\n                        new_fitness[i] = self.fitness[i]\n\n\n                elif rand < probs[0] + probs[1]:  # Gaussian Local Search\n                    self.attempt_history['ls'] += 1\n                    new_x = self.population[i] + np.random.normal(0, 0.1, self.dim)\n                    new_x = np.clip(new_x, self.lb, self.ub)\n                    new_f = func(new_x)\n                    self.budget_used += 1\n                    if new_f < self.fitness[i]:\n                        self.success_history['ls'] += 1\n                        new_population[i] = new_x\n                        new_fitness[i] = new_f\n                    else:\n                        new_population[i] = self.population[i]\n                        new_fitness[i] = self.fitness[i]\n\n                else:  # Nelder-Mead Simplex\n                    self.attempt_history['nm'] += 1\n                    initial_simplex = self.population[np.random.choice(self.pop_size, self.dim + 1, replace=False)]\n                    \n                    def simplex_func(x):\n                        if self.budget_used >= self.budget:\n                            return np.inf\n                        fx = func(x)\n                        self.budget_used += 1\n                        return fx\n\n                    res = minimize(simplex_func, self.population[i], method='Nelder-Mead', bounds=[(self.lb, self.ub)] * self.dim, options={'maxfev':min(50, self.budget - self.budget_used)})\n                    \n                    if res.fun < self.fitness[i]:\n                        self.success_history['nm'] += 1\n                        new_population[i] = res.x\n                        new_fitness[i] = res.fun\n                    else:\n                        new_population[i] = self.population[i]\n                        new_fitness[i] = self.fitness[i]\n\n                if self.budget_used >= self.budget:\n                    break\n\n\n            for i in range(self.pop_size):\n                if new_fitness[i] < self.fitness[i]:\n                    self.population[i] = new_population[i]\n                    self.fitness[i] = new_fitness[i]\n\n                    if new_fitness[i] < self.best_fitness:\n                        self.best_x = new_population[i].copy()\n                        self.best_fitness = new_fitness[i].copy()\n\n                        if self.best_fitness < self.f_opt:\n                            self.f_opt = self.best_fitness\n                            self.x_opt = self.best_x\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "In the code, line 91, in __call__, the following error occurred:\nNameError: name 'minimize' is not defined\nOn line: res = minimize(simplex_func, self.population[i], method='Nelder-Mead', bounds=[(self.lb, self.ub)] * self.dim, options={'maxfev':min(50, self.budget - self.budget_used)})", "error": "In the code, line 91, in __call__, the following error occurred:\nNameError: name 'minimize' is not defined\nOn line: res = minimize(simplex_func, self.population[i], method='Nelder-Mead', bounds=[(self.lb, self.ub)] * self.dim, options={'maxfev':min(50, self.budget - self.budget_used)})", "parent_ids": ["e16e90d9-ba5f-43ec-b171-d0521e5848f9"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "d6932345-dba3-4f5d-b26b-d91108efa09c", "fitness": 0.2768657289629061, "name": "HybridDE_NM", "description": "A hybrid algorithm that combines a modified version of Differential Evolution with a Nelder-Mead simplex-based local search, dynamically allocating budget between exploration and exploitation based on performance feedback.", "code": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridDE_NM:\n    def __init__(self, budget=10000, dim=10, pop_size=20, de_rate=0.7, nm_rate=0.2, learning_rate=0.1, restart_trigger=0.05, nm_interval=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.de_rate = de_rate\n        self.nm_rate = nm_rate  # Probability of applying Nelder-Mead\n        self.learning_rate = learning_rate\n        self.restart_trigger = restart_trigger\n        self.nm_interval = nm_interval # Number of iterations between Nelder-Mead applications\n        self.nm_eval_budget = 10 # Number of evaluations Nelder-Mead can use each time\n        self.exploration_exploitation_balance = 0.5  # Initial balance between DE and NM (0 to 1)\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.eval_count = 0\n\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.eval_count = self.pop_size\n\n        self.best_index = np.argmin(self.fitness)\n        self.best_x = self.population[self.best_index].copy()\n        self.best_fitness = self.fitness[self.best_index].copy()\n        \n        self.f_opt = self.best_fitness\n        self.x_opt = self.best_x\n\n        self.no_improvement_count = 0\n        self.max_no_improvement = int(self.budget * self.restart_trigger)\n        self.iteration = 0\n        \n        while self.eval_count < self.budget:\n            new_population = np.zeros_like(self.population)\n            new_fitness = np.zeros(self.pop_size)\n\n            for i in range(self.pop_size):\n                if np.random.rand() < self.exploration_exploitation_balance:  # Differential Evolution\n                    indices = np.random.choice(self.pop_size, 3, replace=False)\n                    x_r1, x_r2, x_r3 = self.population[indices]\n                    mutation = x_r1 + 0.5 * (x_r2 - x_r3)\n                    crossover = np.random.rand(self.dim) < 0.7\n                    new_solution = np.where(crossover, mutation, self.population[i])\n\n                else:  # Local Search using Nelder-Mead\n                    if self.iteration % self.nm_interval == 0 and (self.eval_count + self.nm_eval_budget <= self.budget):\n                        try:\n                            nm_result = minimize(func, self.population[i], method='Nelder-Mead', bounds=func.bounds, options={'maxfev': self.nm_eval_budget})\n                            new_solution = nm_result.x\n                            new_fitness_val = nm_result.fun\n                            self.eval_count += nm_result.nfev\n\n                            new_population[i] = new_solution\n                            new_fitness[i] = new_fitness_val\n                            continue #Skip evaluation by DE below\n                        except Exception as e:\n                            # Nelder-Mead failed for some reason (e.g., invalid starting point)\n                            new_solution = self.population[i] + np.random.normal(0, 0.05, self.dim) #Small perturbation instead\n                    else:\n                        new_solution = self.population[i]\n\n                new_solution = np.clip(new_solution, func.bounds.lb, func.bounds.ub)\n                new_fitness_val = func(new_solution)\n                self.eval_count += 1\n                new_population[i] = new_solution\n                new_fitness[i] = new_fitness_val\n\n            self.population = new_population\n            self.fitness = new_fitness\n\n            # Update best solution\n            self.best_index = np.argmin(self.fitness)\n            if self.fitness[self.best_index] < self.best_fitness:\n                self.best_x = self.population[self.best_index].copy()\n                self.best_fitness = self.fitness[self.best_index].copy()\n\n                if self.best_fitness < self.f_opt:\n                    self.f_opt = self.best_fitness\n                    self.x_opt = self.best_x\n                    self.no_improvement_count = 0\n                else:\n                    self.no_improvement_count += 1\n\n                # Adjust exploration-exploitation balance towards exploitation\n                self.exploration_exploitation_balance -= self.learning_rate * (1 - self.exploration_exploitation_balance)\n            else:\n                self.no_improvement_count += 1\n                # Adjust exploration-exploitation balance towards exploration\n                self.exploration_exploitation_balance += self.learning_rate * self.exploration_exploitation_balance\n\n            self.exploration_exploitation_balance = np.clip(self.exploration_exploitation_balance, 0.1, 0.9)\n\n            # Restart mechanism\n            if self.no_improvement_count > self.max_no_improvement:\n                self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                self.fitness = np.array([func(x) for x in self.population])\n                self.eval_count += self.pop_size\n\n                self.best_index = np.argmin(self.fitness)\n                self.best_x = self.population[self.best_index].copy()\n                self.best_fitness = self.fitness[self.best_index].copy()\n\n                self.no_improvement_count = 0\n                self.exploration_exploitation_balance = 0.5\n            self.iteration += 1\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "The algorithm HybridDE_NM scored 0.277 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["964c36f1-9b00-45c8-bf8f-6ed5db482397"], "operator": null, "metadata": {"aucs": [0.12332362060658164, 0.17183155836344755, 0.2668480272774001, 0.19812347347368164, 0.1747888322277149, 0.23624571906928715, 0.21516416172794894, 0.19172084976526316, 0.21186198070993933, 0.1552942696330445, 0.1943053123819124, 0.9729498342380477, 0.23616532106584864, 0.22542576018487404, 0.603246859377501, 0.27089528893720727, 0.21131747170609194, 0.26733191181647287, 0.15781767312467654, 0.4526566535711817]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "b25fdd0b-dc27-4ff1-a51a-94d212f58e7e", "fitness": 0.39514109780440865, "name": "HybridOptimizer", "description": "A simplified hybrid optimizer using differential evolution, a simplified PSO, and random search with dynamic probability adjustments based on success.", "code": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget=10000, dim=10, pop_size=20, initial_de_rate=0.5, initial_pso_rate=0.3, restart_trigger=0.05):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.de_rate = initial_de_rate\n        self.pso_rate = initial_pso_rate\n        self.random_rate = 1.0 - initial_de_rate - initial_pso_rate\n        self.restart_trigger = restart_trigger\n        self.success_de = 0\n        self.success_pso = 0\n        self.total_de = 0\n        self.total_pso = 0\n\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget_used = self.pop_size\n\n        self.pbest_population = self.population.copy()\n        self.pbest_fitness = self.fitness.copy()\n\n        self.gbest_index = np.argmin(self.fitness)\n        self.gbest_x = self.population[self.gbest_index].copy()\n        self.gbest_fitness = self.fitness[self.gbest_index].copy()\n        \n        self.f_opt = self.gbest_fitness\n        self.x_opt = self.gbest_x\n\n        self.no_improvement_count = 0\n        self.max_no_improvement = int(self.budget * self.restart_trigger)\n        \n        while self.budget_used < self.budget:\n            new_population = np.zeros_like(self.population)\n            new_fitness = np.zeros(self.pop_size)\n\n            for i in range(self.pop_size):\n                rand = np.random.rand()\n                \n                # Differential Evolution\n                if rand < self.de_rate:\n                    self.total_de += 1\n                    indices = np.random.choice(self.pop_size, 3, replace=False)\n                    x_r1, x_r2, x_r3 = self.population[indices]\n                    mutation = x_r1 + 0.5 * (x_r2 - x_r3)\n                    crossover = np.random.rand(self.dim) < 0.5  # binomial crossover\n                    new_population[i] = np.where(crossover, mutation, self.population[i])\n                    new_population[i] = np.clip(new_population[i], func.bounds.lb, func.bounds.ub)\n                    new_fitness[i] = func(new_population[i])\n                    self.budget_used += 1\n                    if new_fitness[i] < self.fitness[i]:\n                        self.success_de += 1\n\n                # Particle Swarm Optimization\n                elif rand < self.de_rate + self.pso_rate:\n                    self.total_pso += 1\n                    inertia = 0.7\n                    cognitive_coeff = 1.5\n                    velocity = inertia * (self.population[i] - self.pbest_population[i]) + cognitive_coeff * np.random.rand(self.dim) * (self.gbest_x - self.population[i])\n                    new_population[i] = self.population[i] + velocity\n                    new_population[i] = np.clip(new_population[i], func.bounds.lb, func.bounds.ub)\n                    new_fitness[i] = func(new_population[i])\n                    self.budget_used += 1\n                    if new_fitness[i] < self.fitness[i]:\n                        self.success_pso += 1\n\n                # Random exploration\n                else:\n                    new_population[i] = np.random.uniform(func.bounds.lb, func.bounds.ub, self.dim)\n                    new_fitness[i] = func(new_population[i])\n                    self.budget_used += 1\n\n                if new_fitness[i] < self.fitness[i]:\n                    self.population[i] = new_population[i]\n                    self.fitness[i] = new_fitness[i]\n\n                    if new_fitness[i] < self.pbest_fitness[i]:\n                        self.pbest_population[i] = new_population[i]\n                        self.pbest_fitness[i] = new_fitness[i]\n\n            self.gbest_index = np.argmin(self.fitness)\n            if self.fitness[self.gbest_index] < self.gbest_fitness:\n                self.gbest_x = self.population[self.gbest_index].copy()\n                self.gbest_fitness = self.fitness[self.gbest_index].copy()\n                \n                if self.gbest_fitness < self.f_opt:\n                    self.f_opt = self.gbest_fitness\n                    self.x_opt = self.gbest_x\n                    self.no_improvement_count = 0\n                else:\n                    self.no_improvement_count += 1\n            else:\n                self.no_improvement_count += 1\n            \n            # Adjust probabilities based on success\n            if self.total_de > 5 and self.total_pso > 5:\n                success_rate_de = self.success_de / self.total_de\n                success_rate_pso = self.success_pso / self.total_pso\n                \n                self.de_rate = 0.2 + 0.8 * success_rate_de / (success_rate_de + success_rate_pso + 1e-6)\n                self.pso_rate = 0.2 + 0.8 * success_rate_pso / (success_rate_de + success_rate_pso + 1e-6)\n                self.random_rate = 1.0 - self.de_rate - self.pso_rate\n                \n                self.success_de = 0\n                self.success_pso = 0\n                self.total_de = 0\n                self.total_pso = 0\n\n            # Restart mechanism\n            if self.no_improvement_count > self.max_no_improvement:\n                self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                self.fitness = np.array([func(x) for x in self.population])\n\n                self.pbest_population = self.population.copy()\n                self.pbest_fitness = self.fitness.copy()\n\n                self.gbest_index = np.argmin(self.fitness)\n                self.gbest_x = self.population[self.gbest_index].copy()\n                self.gbest_fitness = self.fitness[self.gbest_index].copy()\n                \n                self.no_improvement_count = 0\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "The algorithm HybridOptimizer scored 0.395 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["d0fddf51-e6ed-4acb-82bd-f12a4f7ace8d"], "operator": null, "metadata": {"aucs": [0.2245102832468654, 0.6043327435206324, 0.383718911483532, 0.21425704679435853, 0.34321339110841154, 0.49992024677835056, 0.3274123366193721, 0.6699938466481868, 0.22591858009444865, 0.2008936686616828, 0.31569999711988517, 0.9989715410810734, 0.2326501218535656, 0.31103355888436957, 0.6378175853128577, 0.30889437322300517, 0.3154180709317256, 0.39522466496528896, 0.2251971814305206, 0.46774380633003965]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "0eceaf09-69d5-4b47-93e6-80e157e7b18f", "fitness": 0.5303470033509374, "name": "AdaptiveHybridOptimizer", "description": "A hybrid optimizer that uses a combination of differential evolution, covariance matrix adaptation evolution strategy (CMA-ES), and a local search with dynamic probability adjustments based on their individual success rates and a global archive of best solutions.", "code": "import numpy as np\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget=10000, dim=10, pop_size=20, archive_size=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lb = -5.0\n        self.ub = 5.0\n        self.archive_size = archive_size\n        self.archive = []\n        self.archive_fitness = []\n\n        self.de_success = 0\n        self.cmaes_success = 0\n        self.local_search_success = 0\n        self.de_count = 0\n        self.cmaes_count = 0\n        self.local_search_count = 0\n        self.epsilon = 1e-6\n\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget_used = self.pop_size\n\n        self.best_index = np.argmin(self.fitness)\n        self.best_x = self.population[self.best_index].copy()\n        self.best_fitness = self.fitness[self.best_index].copy()\n\n        self.f_opt = self.best_fitness\n        self.x_opt = self.best_x\n\n        self.add_to_archive(self.best_x, self.best_fitness)\n        \n        while self.budget_used < self.budget:\n            new_population = np.zeros_like(self.population)\n            new_fitness = np.zeros_like(self.fitness)\n            \n            de_prob = (self.de_success / (self.de_count + self.epsilon)) if self.de_count > 0 else 0.33\n            cmaes_prob = (self.cmaes_success / (self.cmaes_count + self.epsilon)) if self.cmaes_count > 0 else 0.33\n            local_search_prob = (self.local_search_success / (self.local_search_count + self.epsilon)) if self.local_search_count > 0 else 0.34\n            \n            # Normalize probabilities\n            total_prob = de_prob + cmaes_prob + local_search_prob\n            de_prob /= total_prob\n            cmaes_prob /= total_prob\n            local_search_prob /= total_prob\n            \n            for i in range(self.pop_size):\n                rand = np.random.rand()\n                \n                if rand < de_prob:\n                    # Differential Evolution\n                    self.de_count += 1\n                    indices = np.random.choice(self.pop_size, 3, replace=False)\n                    x_r1, x_r2, x_r3 = self.population[indices]\n                    mutation = x_r1 + 0.5 * (x_r2 - x_r3)\n                    crossover = np.random.rand(self.dim) < 0.5\n                    new_x = np.where(crossover, mutation, self.population[i])\n                    new_x = np.clip(new_x, self.lb, self.ub)\n                    new_fitness[i] = func(new_x)\n                    self.budget_used += 1\n\n                    if new_fitness[i] < self.fitness[i]:\n                        self.de_success += 1\n                        self.population[i] = new_x\n                        self.fitness[i] = new_fitness[i]\n                        self.add_to_archive(new_x, new_fitness[i])\n\n                elif rand < de_prob + cmaes_prob:\n                    # CMA-ES (Simplified)\n                    self.cmaes_count += 1\n                    step_size = 0.1\n                    new_x = self.population[i] + step_size * np.random.randn(self.dim)\n                    new_x = np.clip(new_x, self.lb, self.ub)\n                    new_fitness[i] = func(new_x)\n                    self.budget_used += 1\n\n                    if new_fitness[i] < self.fitness[i]:\n                        self.cmaes_success += 1\n                        self.population[i] = new_x\n                        self.fitness[i] = new_fitness[i]\n                        self.add_to_archive(new_x, new_fitness[i])\n\n                else:\n                    # Local Search\n                    self.local_search_count += 1\n                    step_size = 0.05\n                    new_x = self.population[i] + step_size * np.random.randn(self.dim)\n                    new_x = np.clip(new_x, self.lb, self.ub)\n                    new_fitness[i] = func(new_x)\n                    self.budget_used += 1\n\n                    if new_fitness[i] < self.fitness[i]:\n                        self.local_search_success += 1\n                        self.population[i] = new_x\n                        self.fitness[i] = new_fitness[i]\n                        self.add_to_archive(new_x, new_fitness[i])\n                \n                if self.fitness[i] < self.best_fitness:\n                    self.best_x = self.population[i].copy()\n                    self.best_fitness = self.fitness[i].copy()\n                    if self.best_fitness < self.f_opt:\n                        self.f_opt = self.best_fitness\n                        self.x_opt = self.best_x\n\n        return self.f_opt, self.x_opt\n\n    def add_to_archive(self, x, fitness):\n        if len(self.archive) < self.archive_size:\n            self.archive.append(x)\n            self.archive_fitness.append(fitness)\n        else:\n            max_fitness_index = np.argmax(self.archive_fitness)\n            if fitness < self.archive_fitness[max_fitness_index]:\n                self.archive[max_fitness_index] = x\n                self.archive_fitness[max_fitness_index] = fitness", "configspace": "", "generation": 2, "feedback": "The algorithm AdaptiveHybridOptimizer scored 0.530 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["e16e90d9-ba5f-43ec-b171-d0521e5848f9"], "operator": null, "metadata": {"aucs": [0.16701176810239704, 0.2469430080382108, 0.4919180580900323, 0.7248333938056091, 0.5437524562429572, 0.7377595615869732, 0.4736232677118335, 0.521160335849107, 0.5381717305297034, 0.26190002744718144, 0.7809512546154657, 0.9880352805233815, 0.28240169860006836, 0.45597934171959154, 0.8473704055291816, 0.7198585164588043, 0.3349302559093281, 0.7792534105023784, 0.17582750252988555, 0.5352587932266575]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "a3733dca-bf94-4f29-bf48-015662ebaaef", "fitness": "-inf", "name": "GP_CMAES_Optimizer", "description": "A self-adjusting hybrid optimizer that dynamically allocates function evaluations to promising regions based on a Gaussian process model and integrates a simplified CMA-ES for exploitation.", "code": "import numpy as np\nfrom sklearn.gaussian_process import GaussianProcessRegressor\nfrom sklearn.gaussian_process.kernels import RBF, ConstantKernel as C\n\nclass GP_CMAES_Optimizer:\n    def __init__(self, budget=10000, dim=10, pop_size=10, n_estimators=5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lb = -5.0\n        self.ub = 5.0\n        self.n_estimators = n_estimators\n        self.X = None\n        self.y = None\n        self.gpr = None\n        self.best_x = None\n        self.best_fitness = np.inf\n        self.budget_used = 0\n        self.f_opt = np.inf\n        self.x_opt = None\n\n    def __call__(self, func):\n        # Initial sampling\n        initial_samples = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        initial_fitness = np.array([func(x) for x in initial_samples])\n        self.budget_used += self.pop_size\n\n        self.X = initial_samples\n        self.y = initial_fitness\n\n        best_index = np.argmin(initial_fitness)\n        self.best_x = initial_samples[best_index].copy()\n        self.best_fitness = initial_fitness[best_index].copy()\n        self.f_opt = self.best_fitness\n        self.x_opt = self.best_x\n\n\n        while self.budget_used < self.budget:\n            # Gaussian Process Regression\n            kernel = C(1.0, (1e-3, 1e3)) * RBF(1.0, (1e-3, 1e3))\n            self.gpr = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=self.n_estimators, random_state=0)\n            self.gpr.fit(self.X, self.y)\n\n            # Acquisition function (Upper Confidence Bound)\n            def acquisition(x, kappa=2.0):\n                mu, sigma = self.gpr.predict(x.reshape(1, -1), return_std=True)\n                return -mu[0] + kappa * sigma[0]  # Maximize UCB\n\n            # Find the next point to evaluate using CMA-ES-like sampling around the best point\n            step_size = 0.5 # Adjust the step size\n            new_samples = np.zeros((self.pop_size, self.dim))\n            new_fitness = np.zeros(self.pop_size)\n\n            for i in range(self.pop_size):\n                new_x = self.best_x + step_size * np.random.randn(self.dim)\n                new_x = np.clip(new_x, self.lb, self.ub)\n                new_samples[i] = new_x\n                new_fitness[i] = func(new_x)\n                self.budget_used += 1\n\n            # Update the data with the new samples\n            self.X = np.vstack((self.X, new_samples))\n            self.y = np.concatenate((self.y, new_fitness))\n\n\n            # Update the best solution found so far\n            best_index = np.argmin(self.y)\n            if self.y[best_index] < self.best_fitness:\n                self.best_fitness = self.y[best_index].copy()\n                self.best_x = self.X[best_index].copy()\n                if self.best_fitness < self.f_opt:\n                    self.f_opt = self.best_fitness\n                    self.x_opt = self.best_x\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "In the code, line 40, in __call__, the following error occurred:\nNameError: name 'C' is not defined\nOn line: kernel = C(1.0, (1e-3, 1e3)) * RBF(1.0, (1e-3, 1e3))", "error": "In the code, line 40, in __call__, the following error occurred:\nNameError: name 'C' is not defined\nOn line: kernel = C(1.0, (1e-3, 1e3)) * RBF(1.0, (1e-3, 1e3))", "parent_ids": ["0eceaf09-69d5-4b47-93e6-80e157e7b18f"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "3731c9b4-315c-41e9-945f-5d14b4894f21", "fitness": 0.3340443233384899, "name": "HybridOptimizer", "description": "A simplified hybrid optimizer with adaptive exploration-exploitation balance using a self-adaptive learning rate and a simplified restart strategy.", "code": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget=10000, dim=10, pop_size=20, initial_lr=0.1, restart_trigger=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lr = initial_lr  # Learning rate for adaptation\n        self.restart_trigger = restart_trigger\n        self.lb = -5.0\n        self.ub = 5.0\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget_used = self.pop_size\n\n        self.best_index = np.argmin(self.fitness)\n        self.best_x = self.population[self.best_index].copy()\n        self.best_fitness = self.fitness[self.best_index].copy()\n        \n        self.f_opt = self.best_fitness\n        self.x_opt = self.best_x\n\n        self.no_improvement_count = 0\n        self.max_no_improvement = int(self.budget * self.restart_trigger)\n\n        while self.budget_used < self.budget:\n            # Create offspring using a blend of current population and best solution\n            offspring = np.zeros_like(self.population)\n            for i in range(self.pop_size):\n                # Blend with best solution\n                offspring[i] = self.population[i] + self.lr * (self.best_x - self.population[i]) + np.random.normal(0, 0.01, self.dim)\n                \n                # Add some random exploration\n                if np.random.rand() < 0.1: # 10% chance of random exploration\n                    offspring[i] = np.random.uniform(self.lb, self.ub, self.dim)\n                \n                offspring[i] = np.clip(offspring[i], self.lb, self.ub)\n\n            new_fitness = np.array([func(x) for x in offspring])\n            self.budget_used += self.pop_size\n\n            # Update population\n            for i in range(self.pop_size):\n                if new_fitness[i] < self.fitness[i]:\n                    self.population[i] = offspring[i]\n                    self.fitness[i] = new_fitness[i]\n\n            # Update best solution\n            best_index = np.argmin(self.fitness)\n            if self.fitness[best_index] < self.best_fitness:\n                self.best_x = self.population[best_index].copy()\n                self.best_fitness = self.fitness[best_index].copy()\n                \n                if self.best_fitness < self.f_opt:\n                    self.f_opt = self.best_fitness\n                    self.x_opt = self.best_x\n                    self.no_improvement_count = 0\n\n                    # Increase learning rate if improvement\n                    self.lr = min(self.lr * 1.1, 0.5) # Cap the learning rate\n                else:\n                    self.no_improvement_count += 1\n                    # Decrease learning rate if no improvement\n                    self.lr = max(self.lr * 0.9, 0.001)\n            else:\n                self.no_improvement_count += 1\n                # Decrease learning rate if no improvement\n                self.lr = max(self.lr * 0.9, 0.001)\n\n            # Restart if no improvement for too long\n            if self.no_improvement_count > self.max_no_improvement:\n                self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n                self.fitness = np.array([func(x) for x in self.population])\n                self.best_index = np.argmin(self.fitness)\n                self.best_x = self.population[self.best_index].copy()\n                self.best_fitness = self.fitness[self.best_index].copy()\n                self.no_improvement_count = 0\n                self.lr = 0.1 # Reset learning rate\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "The algorithm HybridOptimizer scored 0.334 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["d0fddf51-e6ed-4acb-82bd-f12a4f7ace8d"], "operator": null, "metadata": {"aucs": [0.14689395874003952, 0.19316638353410942, 0.5485476330315254, 0.25188951322600484, 0.1954946572890044, 0.25894645854989007, 0.29030751216466855, 0.215272759141092, 0.23166455415953058, 0.16469937018906255, 0.1950340589344387, 0.9938551622209648, 0.29497376553215393, 0.20380527213439814, 0.7023765799916561, 0.3076866242397118, 0.25128316204939083, 0.6061510195062865, 0.16641089659781505, 0.46242712553805254]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "a05fbc6a-1df1-4937-8386-f8703f815a0b", "fitness": "-inf", "name": "AdaptiveHybridOptimizer", "description": "A self-adjusting hybrid optimizer that dynamically allocates budget between global exploration (DE, PSO), local exploitation (gradient-free search), and random restarts based on their observed performance.", "code": "import numpy as np\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget=10000, dim=10, pop_size=20, archive_size=10, lr=0.05):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lb = -5.0\n        self.ub = 5.0\n        self.archive_size = archive_size\n        self.archive = []\n        self.archive_fitness = []\n        self.lr = lr  # Learning rate for probability adjustments\n\n        # Initialize algorithm probabilities\n        self.de_prob = 0.33\n        self.pso_prob = 0.33\n        self.local_search_prob = 0.34\n\n        self.de_success = 0\n        self.pso_success = 0\n        self.local_search_success = 0\n\n        self.de_count = 0\n        self.pso_count = 0\n        self.local_search_count = 0\n        self.epsilon = 1e-8\n\n        self.inertia = 0.7\n        self.cognitive_coeff = 1.5\n        self.social_coeff = 1.5\n\n        self.velocity = np.zeros((pop_size, dim))\n\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget_used = self.pop_size\n\n        self.best_index = np.argmin(self.fitness)\n        self.best_x = self.population[self.best_index].copy()\n        self.best_fitness = self.fitness[self.best_index].copy()\n\n        self.f_opt = self.best_fitness\n        self.x_opt = self.best_x\n\n        self.add_to_archive(self.best_x, self.best_fitness)\n\n        restart_counter = 0 # Track consecutive restarts\n        restart_threshold = 5 # Number of consecutive restarts before increasing exploration\n\n        while self.budget_used < self.budget:\n            \n            # Adaptive Probability Adjustment\n            total_success = self.de_success + self.pso_success + self.local_search_success\n            if total_success > 0:\n                de_reward = self.de_success / total_success\n                pso_reward = self.pso_success / total_success\n                local_search_reward = self.local_search_success / total_success\n\n                self.de_prob = (1 - self.lr) * self.de_prob + self.lr * de_reward\n                self.pso_prob = (1 - self.lr) * self.pso_prob + self.lr * pso_reward\n                self.local_search_prob = (1 - self.lr) * self.local_search_prob + self.lr * local_search_reward\n\n                # Normalize probabilities\n                total_prob = self.de_prob + self.pso_prob + self.local_search_prob\n                self.de_prob /= total_prob\n                self.pso_prob /= total_prob\n                self.local_search_prob /= total_prob\n\n\n            for i in range(self.pop_size):\n                rand = np.random.rand()\n\n                if rand < self.de_prob:\n                    # Differential Evolution\n                    self.de_count += 1\n                    indices = np.random.choice(self.pop_size, 3, replace=False)\n                    x_r1, x_r2, x_r3 = self.population[indices]\n                    mutation = x_r1 + 0.5 * (x_r2 - x_r3)\n                    crossover = np.random.rand(self.dim) < 0.5\n                    new_x = np.where(crossover, mutation, self.population[i])\n                    new_x = np.clip(new_x, self.lb, self.ub)\n                    new_fitness = func(new_x)\n                    self.budget_used += 1\n\n                    if new_fitness < self.fitness[i]:\n                        self.de_success += 1\n                        self.population[i] = new_x\n                        self.fitness[i] = new_fitness\n                        if new_fitness < self.best_fitness:\n                            self.best_x = new_x.copy()\n                            self.best_fitness = new_fitness.copy()\n                            self.add_to_archive(new_x, new_fitness)\n\n                elif rand < self.de_prob + self.pso_prob:\n                    # Particle Swarm Optimization\n                    self.pso_count += 1\n                    r1 = np.random.rand(self.dim)\n                    r2 = np.random.rand(self.dim)\n                    self.velocity[i] = (self.inertia * self.velocity[i] +\n                                        self.cognitive_coeff * r1 * (self.best_x - self.population[i]) +\n                                        self.social_coeff * r2 * (self.x_opt - self.population[i])) # Global best x_opt\n\n                    new_x = self.population[i] + self.velocity[i]\n                    new_x = np.clip(new_x, self.lb, self.ub)\n\n                    new_fitness = func(new_x)\n                    self.budget_used += 1\n\n                    if new_fitness < self.fitness[i]:\n                        self.pso_success += 1\n                        self.population[i] = new_x\n                        self.fitness[i] = new_fitness\n                        if new_fitness < self.best_fitness:\n                            self.best_x = new_x.copy()\n                            self.best_fitness = new_fitness.copy()\n                            self.add_to_archive(new_x, new_fitness)\n\n\n                else:\n                    # Local Search (Random Walk with decreasing step size)\n                    self.local_search_count += 1\n                    step_size = 0.05 * (1 - (self.budget_used / self.budget)) # Decaying step size\n                    new_x = self.population[i] + step_size * np.random.randn(self.dim)\n                    new_x = np.clip(new_x, self.lb, self.ub)\n                    new_fitness = func(new_x)\n                    self.budget_used += 1\n\n                    if new_fitness < self.fitness[i]:\n                        self.local_search_success += 1\n                        self.population[i] = new_x\n                        self.fitness[i] = new_fitness\n                        if new_fitness < self.best_fitness:\n                            self.best_x = new_x.copy()\n                            self.best_fitness = new_fitness.copy()\n                            self.add_to_archive(new_x, new_fitness)\n                \n                if self.fitness[i] < self.best_fitness:\n                    self.best_x = self.population[i].copy()\n                    self.best_fitness = self.fitness[i].copy()\n\n                    if self.best_fitness < self.f_opt:\n                        self.f_opt = self.best_fitness\n                        self.x_opt = self.best_x.copy()\n\n            # Stagnation Check and Restart\n            if np.abs(self.f_opt - self.best_fitness) < 1e-6 and self.local_search_prob > 0.6:\n                restart_counter += 1\n                if restart_counter >= restart_threshold:\n                    # Increase Exploration (Reset probabilities)\n                    self.de_prob = 0.5\n                    self.pso_prob = 0.5\n                    self.local_search_prob = 0.0\n\n                    # Reset success counters\n                    self.de_success = 0\n                    self.pso_success = 0\n                    self.local_search_success = 0\n\n                    # Restart Population\n                    self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n                    self.fitness = np.array([func(x) for x in self.population])\n                    self.budget_used += self.pop_size\n                    self.best_index = np.argmin(self.fitness)\n                    self.best_x = self.population[self.best_index].copy()\n                    self.best_fitness = self.fitness[self.best_index].copy()\n                    self.add_to_archive(self.best_x, self.best_fitness)\n                    restart_counter = 0 # Reset counter\n\n            else:\n                 restart_counter = 0 # Reset counter if no stagnation\n\n        return self.f_opt, self.x_opt\n\n    def add_to_archive(self, x, fitness):\n        if len(self.archive) < self.archive_size:\n            self.archive.append(x)\n            self.archive_fitness.append(fitness)\n        else:\n            max_fitness_index = np.argmax(self.archive_fitness)\n            if fitness < self.archive_fitness[max_fitness_index]:\n                self.archive[max_fitness_index] = x\n                self.archive_fitness[max_fitness_index] = fitness", "configspace": "", "generation": 3, "feedback": "In the code, line 120, in __call__, the following error occurred:\nAttributeError: 'float' object has no attribute 'copy'\nOn line: self.best_fitness = new_fitness.copy()", "error": "In the code, line 120, in __call__, the following error occurred:\nAttributeError: 'float' object has no attribute 'copy'\nOn line: self.best_fitness = new_fitness.copy()", "parent_ids": ["0eceaf09-69d5-4b47-93e6-80e157e7b18f"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "726ea57b-a88e-4514-988f-af2dd2860c01", "fitness": "-inf", "name": "AdaptiveHybridOptimizer", "description": "An adaptive hybrid optimizer that blends Differential Evolution, Particle Swarm Optimization, and a Nelder-Mead simplex local search, with self-adjusting probabilities based on performance and population diversity.", "code": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget=10000, dim=10, pop_size=20, de_rate=0.4, pso_rate=0.3, nm_rate=0.3):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lb = -5.0\n        self.ub = 5.0\n        self.de_rate = de_rate\n        self.pso_rate = pso_rate\n        self.nm_rate = nm_rate\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.history_length = 10  # Length of history for success rate calculation\n        self.de_success_history = []\n        self.pso_success_history = []\n        self.nm_success_history = []\n        self.exploration_rate = 0.7\n\n\n    def __call__(self, func):\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget_used = self.pop_size\n\n        self.best_index = np.argmin(self.fitness)\n        self.best_x = self.population[self.best_index].copy()\n        self.best_fitness = self.fitness[self.best_index].copy()\n        \n        self.f_opt = self.best_fitness\n        self.x_opt = self.best_x\n\n        while self.budget_used < self.budget:\n            new_population = np.zeros_like(self.population)\n            new_fitness = np.zeros_like(self.fitness)\n\n            # Adaptive probability adjustment\n            total_successes = sum(self.de_success_history[-self.history_length:] if len(self.de_success_history) > 0 else [0]) + \\\n                              sum(self.pso_success_history[-self.history_length:] if len(self.pso_success_history) > 0 else [0]) + \\\n                              sum(self.nm_success_history[-self.history_length:] if len(self.nm_success_history) > 0 else [0])\n\n            de_prob = (sum(self.de_success_history[-self.history_length:] if len(self.de_success_history) > 0 else [0]) / total_successes) if total_successes > 0 else self.de_rate\n            pso_prob = (sum(self.pso_success_history[-self.history_length:] if len(self.pso_success_history) > 0 else [0]) / total_successes) if total_successes > 0 else self.pso_rate\n            nm_prob = (sum(self.nm_success_history[-self.history_length:] if len(self.nm_success_history) > 0 else [0]) / total_successes) if total_successes > 0 else self.nm_rate\n            \n            #Ensure probabilities sum to one\n            total_prob = de_prob + pso_prob + nm_prob\n            de_prob /= total_prob\n            pso_prob /= total_prob\n            nm_prob /= total_prob\n\n            for i in range(self.pop_size):\n                rand = np.random.rand()\n                original_fitness = self.fitness[i]\n\n                if rand < de_prob:\n                    # Differential Evolution\n                    indices = np.random.choice(self.pop_size, 3, replace=False)\n                    x_r1, x_r2, x_r3 = self.population[indices]\n                    mutation = x_r1 + 0.5 * (x_r2 - x_r3)\n                    crossover = np.random.rand(self.dim) < 0.5\n                    new_x = np.where(crossover, mutation, self.population[i])\n                    new_x = np.clip(new_x, self.lb, self.ub)\n                    new_fitness_val = func(new_x)\n                    self.budget_used += 1\n                    new_population[i] = new_x\n                    new_fitness[i] = new_fitness_val\n\n\n                    if new_fitness_val < original_fitness:\n                        self.de_success_history.append(1)\n                    else:\n                        self.de_success_history.append(0)\n\n                elif rand < de_prob + pso_prob:\n                    # Particle Swarm Optimization\n                    inertia = 0.7\n                    cognitive_coeff = 1.5\n                    social_coeff = 1.5\n                    velocity = inertia * (self.population[i] - self.best_x) + cognitive_coeff * np.random.rand(self.dim) * (self.best_x - self.population[i]) + social_coeff * np.random.rand(self.dim) * (self.best_x - self.population[i])\n                    new_x = self.population[i] + velocity\n                    new_x = np.clip(new_x, self.lb, self.ub)\n                    new_fitness_val = func(new_x)\n                    self.budget_used += 1\n                    new_population[i] = new_x\n                    new_fitness[i] = new_fitness_val\n                    \n                    if new_fitness_val < original_fitness:\n                        self.pso_success_history.append(1)\n                    else:\n                        self.pso_success_history.append(0)\n\n                else:\n                    # Nelder-Mead Simplex\n                    initial_simplex = self.population[i] + 0.01 * np.random.randn(self.dim)\n                    \n                    def objective_function(x):\n                        return func(x)\n                    \n                    res = minimize(objective_function, self.population[i], method='Nelder-Mead', bounds=[(self.lb, self.ub)] * self.dim, options={'maxfev': self.budget - self.budget_used if self.budget > self.budget_used else 1})\n                    \n                    if res.success:\n                        new_x = res.x\n                        new_fitness_val = res.fun\n                        fevals = res.nfev\n                        self.budget_used += fevals\n\n                        if fevals > 0:\n                            new_population[i] = new_x\n                            new_fitness[i] = new_fitness_val\n                            if new_fitness_val < original_fitness:\n                                self.nm_success_history.append(1)\n                            else:\n                                self.nm_success_history.append(0)\n                        else:\n                            new_population[i] = self.population[i]\n                            new_fitness[i] = self.fitness[i]\n                            self.nm_success_history.append(0)\n\n                    else:\n                         new_population[i] = self.population[i]\n                         new_fitness[i] = self.fitness[i]\n                         self.nm_success_history.append(0)\n\n\n                # Ensure success history doesn't grow indefinitely\n                if len(self.de_success_history) > 2 * self.history_length:\n                    self.de_success_history.pop(0)\n                if len(self.pso_success_history) > 2 * self.history_length:\n                    self.pso_success_history.pop(0)\n                if len(self.nm_success_history) > 2 * self.history_length:\n                    self.nm_success_history.pop(0)\n\n\n\n            for i in range(self.pop_size):\n                if new_fitness[i] < self.fitness[i]:\n                    self.population[i] = new_population[i]\n                    self.fitness[i] = new_fitness[i]\n\n                    if new_fitness[i] < self.best_fitness:\n                        self.best_x = new_population[i].copy()\n                        self.best_fitness = new_fitness[i].copy()\n                        \n                        if self.best_fitness < self.f_opt:\n                            self.f_opt = self.best_fitness\n                            self.x_opt = self.best_x\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "In the code, line 102, in __call__, the following error occurred:\nNameError: name 'minimize' is not defined\nOn line: res = minimize(objective_function, self.population[i], method='Nelder-Mead', bounds=[(self.lb, self.ub)] * self.dim, options={'maxfev': self.budget - self.budget_used if self.budget > self.budget_used else 1})", "error": "In the code, line 102, in __call__, the following error occurred:\nNameError: name 'minimize' is not defined\nOn line: res = minimize(objective_function, self.population[i], method='Nelder-Mead', bounds=[(self.lb, self.ub)] * self.dim, options={'maxfev': self.budget - self.budget_used if self.budget > self.budget_used else 1})", "parent_ids": ["e16e90d9-ba5f-43ec-b171-d0521e5848f9"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "ad8b636b-0389-440c-90b8-05167b71eb29", "fitness": "-inf", "name": "AdaptiveHybridOptimizer", "description": "An adaptive hybrid optimizer that balances exploration and exploitation using a combination of differential evolution, CMA-ES, and a simplified local search, dynamically adjusting probabilities based on recent success and leveraging a fitness-weighted archive for enhanced exploration.", "code": "import numpy as np\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget=10000, dim=10, pop_size=20, archive_size=10, local_search_intensity=0.05):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lb = -5.0\n        self.ub = 5.0\n        self.archive_size = archive_size\n        self.archive = []\n        self.archive_fitness = []\n        self.local_search_intensity = local_search_intensity # Local search step size\n\n        self.de_success = 0\n        self.cmaes_success = 0\n        self.local_search_success = 0\n        self.de_count = 0\n        self.cmaes_count = 0\n        self.local_search_count = 0\n        self.epsilon = 1e-6\n\n        self.success_memory = 10  # Number of past iterations to consider for success rate calculation\n        self.de_success_history = np.zeros(self.success_memory)\n        self.cmaes_success_history = np.zeros(self.success_memory)\n        self.local_search_success_history = np.zeros(self.success_memory)\n        self.current_iter = 0\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget_used = self.pop_size\n\n        self.best_index = np.argmin(self.fitness)\n        self.best_x = self.population[self.best_index].copy()\n        self.best_fitness = self.fitness[self.best_index].copy()\n\n        self.f_opt = self.best_fitness\n        self.x_opt = self.best_x\n\n        self.add_to_archive(self.best_x, self.best_fitness)\n        \n        while self.budget_used < self.budget:\n            new_population = np.zeros_like(self.population)\n            new_fitness = np.zeros_like(self.fitness)\n            \n            # Calculate success probabilities using history\n            de_prob = np.mean(self.de_success_history) if self.current_iter > 0 else 0.33\n            cmaes_prob = np.mean(self.cmaes_success_history) if self.current_iter > 0 else 0.33\n            local_search_prob = np.mean(self.local_search_success_history) if self.current_iter > 0 else 0.34\n            \n            # Normalize probabilities\n            total_prob = de_prob + cmaes_prob + local_search_prob\n            de_prob /= total_prob\n            cmaes_prob /= total_prob\n            local_search_prob /= total_prob\n            \n            for i in range(self.pop_size):\n                rand = np.random.rand()\n                \n                if rand < de_prob:\n                    # Differential Evolution\n                    self.de_count += 1\n                    indices = np.random.choice(self.pop_size, 3, replace=False)\n                    x_r1, x_r2, x_r3 = self.population[indices]\n                    mutation = x_r1 + 0.5 * (x_r2 - x_r3)\n                    crossover = np.random.rand(self.dim) < 0.5\n                    new_x = np.where(crossover, mutation, self.population[i])\n                    new_x = np.clip(new_x, self.lb, self.ub)\n                    new_fitness[i] = func(new_x)\n                    self.budget_used += 1\n\n                    if new_fitness[i] < self.fitness[i]:\n                        self.de_success += 1\n                        self.population[i] = new_x\n                        self.fitness[i] = new_fitness[i]\n                        self.add_to_archive(new_x, new_fitness[i])\n\n                elif rand < de_prob + cmaes_prob:\n                    # CMA-ES (Simplified)\n                    self.cmaes_count += 1\n                    step_size = 0.1\n                    new_x = self.population[i] + step_size * np.random.randn(self.dim)\n                    new_x = np.clip(new_x, self.lb, self.ub)\n                    new_fitness[i] = func(new_x)\n                    self.budget_used += 1\n\n                    if new_fitness[i] < self.fitness[i]:\n                        self.cmaes_success += 1\n                        self.population[i] = new_x\n                        self.fitness[i] = new_fitness[i]\n                        self.add_to_archive(new_x, new_fitness[i])\n\n                else:\n                    # Local Search\n                    self.local_search_count += 1\n                    \n                    # Use best archive member as base if possible\n                    if len(self.archive) > 0:\n                         best_archive_idx = np.argmin(self.archive_fitness)\n                         base_x = self.archive[best_archive_idx]\n                    else:\n                         base_x = self.population[i]\n                    \n                    new_x = base_x + self.local_search_intensity * np.random.randn(self.dim)\n                    new_x = np.clip(new_x, self.lb, self.ub)\n                    new_fitness[i] = func(new_x)\n                    self.budget_used += 1\n\n                    if new_fitness[i] < self.fitness[i]:\n                        self.local_search_success += 1\n                        self.population[i] = new_x\n                        self.fitness[i] = new_fitness[i]\n                        self.add_to_archive(new_x, new_fitness[i])\n                \n                if self.fitness[i] < self.best_fitness:\n                    self.best_x = self.population[i].copy()\n                    self.best_fitness = self.fitness[i].copy()\n                    if self.best_fitness < self.f_opt:\n                        self.f_opt = self.best_fitness\n                        self.x_opt = self.best_x\n\n            # Update success rate history\n            self.de_success_history[self.current_iter % self.success_memory] = self.de_success / (self.de_count + self.epsilon) if self.de_count > 0 else 0\n            self.cmaes_success_history[self.current_iter % self.success_memory] = self.cmaes_success / (self.cmaes_count + self.epsilon) if self.cmaes_count > 0 else 0\n            self.local_search_success_history[self.current_iter % self.success_memory] = self.local_search_success / (self.local_search_count + self.epsilon) if self.local_search_count > 0 else 0\n\n            self.de_success = 0\n            self.cmaes_success = 0\n            self.local_search_success = 0\n            self.de_count = 0\n            self.cmaes_count = 0\n            self.local_search_count = 0\n\n            self.current_iter += 1\n\n        return self.f_opt, self.x_opt\n\n    def add_to_archive(self, x, fitness):\n         # Archive maintains diversity by fitness-weighted replacement\n        if len(self.archive) < self.archive_size:\n            self.archive.append(x)\n            self.archive_fitness.append(fitness)\n        else:\n            # Replace worst with probability proportional to fitness improvement\n            fitness_diff = np.maximum(0, np.array(self.archive_fitness) - fitness)  # Avoid negative differences\n            probabilities = fitness_diff / np.sum(fitness_diff) if np.sum(fitness_diff) > 0 else np.ones(self.archive_size) / self.archive_size # Uniform if no improvement\n            index_to_replace = np.random.choice(self.archive_size, p=probabilities)\n            self.archive[index_to_replace] = x\n            self.archive_fitness[index_to_replace] = fitness", "configspace": "", "generation": 3, "feedback": "Evaluation timed out after 300 seconds.", "error": "Evaluation timed out after 300 seconds.", "parent_ids": ["0eceaf09-69d5-4b47-93e6-80e157e7b18f"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "323a8737-455f-4ba6-b517-39826877d68e", "fitness": 0.0, "name": "HybridOptimizer", "description": "A simplified hybrid optimizer using only differential evolution and particle swarm optimization with self-adaptive probabilities, and a simplified restart mechanism based on budget consumption.", "code": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget=10000, dim=10, pop_size=20, de_rate=0.5, pso_rate=0.3):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.de_rate = de_rate\n        self.pso_rate = pso_rate\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget_used = self.pop_size\n        \n        self.pbest_population = self.population.copy()\n        self.pbest_fitness = self.fitness.copy()\n\n        self.gbest_index = np.argmin(self.fitness)\n        self.gbest_x = self.population[self.gbest_index].copy()\n        self.gbest_fitness = self.fitness[self.gbest_index].copy()\n        \n        self.f_opt = self.gbest_fitness\n        self.x_opt = self.gbest_x\n\n        while self.budget_used < self.budget:\n            new_population = np.zeros_like(self.population)\n            new_fitness = np.zeros_like(self.fitness)\n\n            # Adaptive probability adjustment\n            if np.random.rand() < 0.5:  # Dynamically switch between DE and PSO dominance\n                de_prob = min(1.0, self.de_rate + 0.01)\n                pso_prob = max(0.0, self.pso_rate - 0.01)\n            else:\n                de_prob = max(0.0, self.de_rate - 0.01)\n                pso_prob = min(1.0, self.pso_rate + 0.01)\n                \n            self.de_rate = de_prob\n            self.pso_rate = pso_prob\n\n\n            for i in range(self.pop_size):\n                if np.random.rand() < self.de_rate:\n                    # Differential Evolution\n                    indices = np.random.choice(self.pop_size, 3, replace=False)\n                    x_r1, x_r2, x_r3 = self.population[indices]\n                    mutation = x_r1 + 0.5 * (x_r2 - x_r3)\n                    crossover = np.random.rand(self.dim) < 0.5  # binomial crossover\n                    new_population[i] = np.where(crossover, mutation, self.population[i])\n                \n                elif np.random.rand() < self.pso_rate:\n                    # Particle Swarm Optimization\n                    inertia = 0.7\n                    cognitive_coeff = 1.5\n                    social_coeff = 1.5\n                    velocity = inertia * (new_population[i-1] - self.population[i]) + cognitive_coeff * np.random.rand(self.dim) * (self.pbest_population[i] - self.population[i]) + social_coeff * np.random.rand(self.dim) * (self.gbest_x - self.population[i])\n                    new_population[i] = self.population[i] + velocity\n\n                else:\n                    new_population[i] = np.random.uniform(func.bounds.lb, func.bounds.ub, self.dim)\n                \n                new_population[i] = np.clip(new_population[i], func.bounds.lb, func.bounds.ub)\n                new_fitness[i] = func(new_population[i])\n                self.budget_used += 1\n                \n                if new_fitness[i] < self.fitness[i]:\n                    self.population[i] = new_population[i]\n                    self.fitness[i] = new_fitness[i]\n\n                    if new_fitness[i] < self.pbest_fitness[i]:\n                        self.pbest_population[i] = new_population[i]\n                        self.pbest_fitness[i] = new_fitness[i]\n\n            self.gbest_index = np.argmin(self.fitness)\n            if self.fitness[self.gbest_index] < self.gbest_fitness:\n                self.gbest_x = self.population[self.gbest_index].copy()\n                self.gbest_fitness = self.fitness[self.gbest_index].copy()\n                \n                self.f_opt = self.gbest_fitness\n                self.x_opt = self.gbest_x\n\n\n            # Restart mechanism\n            if self.budget_used > 0.75 * self.budget:  # Restart in the last quarter of budget\n                self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                self.fitness = np.array([func(x) for x in self.population])\n                self.budget_used += self.pop_size\n                self.pbest_population = self.population.copy()\n                self.pbest_fitness = self.fitness.copy()\n                self.gbest_index = np.argmin(self.fitness)\n                self.gbest_x = self.population[self.gbest_index].copy()\n                self.gbest_fitness = self.fitness[self.gbest_index].copy()\n                self.f_opt = self.gbest_fitness\n                self.x_opt = self.gbest_x\n                \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "The algorithm HybridOptimizer scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["d0fddf51-e6ed-4acb-82bd-f12a4f7ace8d"], "operator": null, "metadata": {"aucs": [0]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "415411bb-9745-4b77-b8d6-e1033bde5393", "fitness": 0.5137939721632828, "name": "SelfAdjustingOptimizer", "description": "A self-adjusting population-based algorithm that utilizes a combination of differential evolution, Gaussian local search, and a Cauchy mutation operator with dynamically adjusted probabilities and step sizes based on success rates and population diversity.", "code": "import numpy as np\n\nclass SelfAdjustingOptimizer:\n    def __init__(self, budget=10000, dim=10, pop_size=20):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lb = -5.0\n        self.ub = 5.0\n        self.population = None\n        self.fitness = None\n        self.best_x = None\n        self.best_fitness = np.inf\n        self.budget_used = 0\n\n        self.de_success = 0\n        self.gauss_success = 0\n        self.cauchy_success = 0\n        self.de_count = 0\n        self.gauss_count = 0\n        self.cauchy_count = 0\n        self.epsilon = 1e-6\n        self.diversity_threshold = 0.01\n\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        \n        while self.budget_used < self.budget:\n            self.adjust_probabilities()\n\n            for i in range(self.pop_size):\n                rand = np.random.rand()\n\n                if rand < self.de_prob:\n                    self.differential_evolution(func, i)\n                elif rand < self.de_prob + self.gauss_prob:\n                    self.gaussian_local_search(func, i)\n                else:\n                    self.cauchy_mutation(func, i)\n\n                if self.fitness[i] < self.best_fitness:\n                    self.best_fitness = self.fitness[i]\n                    self.best_x = self.population[i].copy()\n\n            if self.population_diversity() < self.diversity_threshold:\n                self.reinitialize_population(func)\n        return self.best_fitness, self.best_x\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget_used += self.pop_size\n        self.best_index = np.argmin(self.fitness)\n        self.best_x = self.population[self.best_index].copy()\n        self.best_fitness = self.fitness[self.best_index].copy()\n        self.de_prob = 0.33\n        self.gauss_prob = 0.33\n        self.cauchy_prob = 0.34\n        \n    def reinitialize_population(self, func):\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget_used += self.pop_size\n        self.best_index = np.argmin(self.fitness)\n        self.best_x = self.population[self.best_index].copy()\n        self.best_fitness = self.fitness[self.best_index].copy()\n\n    def adjust_probabilities(self):\n        self.de_prob = (self.de_success / (self.de_count + self.epsilon)) if self.de_count > 0 else 0.33\n        self.gauss_prob = (self.gauss_success / (self.gauss_count + self.epsilon)) if self.gauss_count > 0 else 0.33\n        self.cauchy_prob = (self.cauchy_success / (self.cauchy_count + self.epsilon)) if self.cauchy_count > 0 else 0.34\n\n        total_prob = self.de_prob + self.gauss_prob + self.cauchy_prob\n        self.de_prob /= total_prob\n        self.gauss_prob /= total_prob\n        self.cauchy_prob /= total_prob\n\n    def differential_evolution(self, func, i):\n        self.de_count += 1\n        indices = np.random.choice(self.pop_size, 3, replace=False)\n        x_r1, x_r2, x_r3 = self.population[indices]\n        mutation = x_r1 + 0.5 * (x_r2 - x_r3)\n        crossover = np.random.rand(self.dim) < 0.5\n        new_x = np.where(crossover, mutation, self.population[i])\n        new_x = np.clip(new_x, self.lb, self.ub)\n        new_fitness = func(new_x)\n        self.budget_used += 1\n\n        if new_fitness < self.fitness[i]:\n            self.de_success += 1\n            self.population[i] = new_x\n            self.fitness[i] = new_fitness\n\n    def gaussian_local_search(self, func, i):\n        self.gauss_count += 1\n        step_size = 0.1 * (self.ub - self.lb) # Step size relative to search space\n        new_x = self.population[i] + step_size * np.random.randn(self.dim)\n        new_x = np.clip(new_x, self.lb, self.ub)\n        new_fitness = func(new_x)\n        self.budget_used += 1\n\n        if new_fitness < self.fitness[i]:\n            self.gauss_success += 1\n            self.population[i] = new_x\n            self.fitness[i] = new_fitness\n\n    def cauchy_mutation(self, func, i):\n        self.cauchy_count += 1\n        scale = 0.05 * (self.ub - self.lb)  # Scale parameter for Cauchy distribution\n        new_x = self.population[i] + scale * np.random.standard_cauchy(size=self.dim)\n        new_x = np.clip(new_x, self.lb, self.ub)\n        new_fitness = func(new_x)\n        self.budget_used += 1\n\n        if new_fitness < self.fitness[i]:\n            self.cauchy_success += 1\n            self.population[i] = new_x\n            self.fitness[i] = new_fitness\n            \n    def population_diversity(self):\n        centroid = np.mean(self.population, axis=0)\n        distances = np.linalg.norm(self.population - centroid, axis=1)\n        mean_distance = np.mean(distances)\n        return mean_distance", "configspace": "", "generation": 3, "feedback": "The algorithm SelfAdjustingOptimizer scored 0.514 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["0eceaf09-69d5-4b47-93e6-80e157e7b18f"], "operator": null, "metadata": {"aucs": [0.18216552120820328, 0.23871610530348342, 0.4803692906397755, 0.8315831035912692, 0.5002867438031006, 0.5888372436395788, 0.4371426039895596, 0.4413113414953326, 0.4893357423059711, 0.3423922266510706, 0.6675338287394734, 0.9988950804323441, 0.28265751770712044, 0.4725487353374602, 0.8365935937366948, 0.6130391265312389, 0.43976046503704136, 0.7454736721436515, 0.18631228270045563, 0.5009252182728321]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "d96ee706-6ab0-43f1-ae4b-7f05ca2e9fbb", "fitness": "-inf", "name": "VoronoiAdaptiveOptimizer", "description": "An adaptive population-based optimizer with a self-organizing search strategy utilizing Voronoi tessellations to dynamically adjust search regions and exploitation levels based on population density and fitness improvements.", "code": "import numpy as np\nfrom scipy.spatial import Voronoi, voronoi_plot_2d\n\nclass VoronoiAdaptiveOptimizer:\n    def __init__(self, budget=10000, dim=10, pop_size=20, exploration_rate=0.7, exploitation_rate=0.2, voronoi_neighbors=5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lb = -5.0\n        self.ub = 5.0\n        self.exploration_rate = exploration_rate\n        self.exploitation_rate = exploitation_rate\n        self.voronoi_neighbors = voronoi_neighbors\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget_used = self.pop_size\n\n        self.best_index = np.argmin(self.fitness)\n        self.best_x = self.population[self.best_index].copy()\n        self.best_fitness = self.fitness[self.best_index].copy()\n\n        self.f_opt = self.best_fitness\n        self.x_opt = self.best_x\n        \n        while self.budget_used < self.budget:\n            new_population = np.zeros_like(self.population)\n\n            # Dynamic adjustment of exploration/exploitation balance\n            remaining_budget_ratio = (self.budget - self.budget_used) / self.budget\n            current_exploration_rate = self.exploration_rate * remaining_budget_ratio\n            current_exploitation_rate = self.exploitation_rate * (1 - remaining_budget_ratio)\n\n            # Voronoi tessellation for adaptive search\n            try:\n                vor = Voronoi(self.population)\n            except:\n                # Handle potential errors when Voronoi computation fails (e.g., duplicate points)\n                new_population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim)) # Fallback to random sampling\n                new_fitness = np.array([func(x) for x in new_population])\n                self.budget_used += self.pop_size\n\n                for i in range(self.pop_size):\n                    if new_fitness[i] < self.fitness[i]:\n                        self.population[i] = new_population[i]\n                        self.fitness[i] = new_fitness[i]\n\n                        if new_fitness[i] < self.best_fitness:\n                            self.best_x = new_population[i].copy()\n                            self.best_fitness = new_fitness[i].copy()\n\n                            if self.best_fitness < self.f_opt:\n                                self.f_opt = self.best_fitness\n                                self.x_opt = self.best_x\n\n                continue\n            \n            #Adaptive scaling based on Voronoi cell size\n            region_volumes = []\n            for i, region_index in enumerate(vor.point_region):\n                if region_index != -1 and all(v in vor.vertices for v in vor.regions[region_index]):\n                    region_vertices = vor.vertices[vor.regions[region_index]]\n                    try:\n                        volume = self.calculate_volume(region_vertices)\n                        region_volumes.append(volume)\n                    except:\n                        region_volumes.append(0) # if the calculation is not possible, add 0\n                else:\n                    region_volumes.append(0) #Assign zero volume if the region doesn't exist\n                    \n            region_volumes = np.array(region_volumes)\n            if np.sum(region_volumes) == 0:\n                region_volumes = np.ones(len(region_volumes)) #Handle the division by zero in the next line\n            region_volumes = region_volumes/np.sum(region_volumes) #Normalise region volumes\n\n            for i in range(self.pop_size):\n                if np.random.rand() < current_exploration_rate:\n                    # Exploration: Sample from a neighbor's Voronoi region\n                    neighbors_indices = np.argsort(np.linalg.norm(self.population - self.population[i], axis=1))[:self.voronoi_neighbors]\n                    neighbor_index = np.random.choice(neighbors_indices)\n                    \n                    # Scale the exploration range by Voronoi cell volume\n                    exploration_scale = region_volumes[i]\n                    new_population[i] = self.population[neighbor_index] + np.random.uniform(-exploration_scale, exploration_scale, self.dim)\n\n                elif np.random.rand() < current_exploitation_rate:\n                    # Exploitation: Local search around the best solution scaled by cell size\n                    exploitation_scale = region_volumes[i]\n                    new_population[i] = self.best_x + np.random.normal(0, 0.1*exploitation_scale, self.dim)\n\n                else:\n                    # Random exploration, also scale the random exploration\n                    random_scale = region_volumes[i]\n                    new_population[i] = np.random.uniform(self.lb*random_scale, self.ub*random_scale, self.dim)\n\n                new_population[i] = np.clip(new_population[i], self.lb, self.ub)\n\n            new_fitness = np.array([func(x) for x in new_population])\n            self.budget_used += self.pop_size\n\n            for i in range(self.pop_size):\n                if new_fitness[i] < self.fitness[i]:\n                    self.population[i] = new_population[i]\n                    self.fitness[i] = new_fitness[i]\n\n                    if new_fitness[i] < self.best_fitness:\n                        self.best_x = new_population[i].copy()\n                        self.best_fitness = new_fitness[i].copy()\n                        \n                        if self.best_fitness < self.f_opt:\n                            self.f_opt = self.best_fitness\n                            self.x_opt = self.best_x\n        return self.f_opt, self.x_opt\n\n    def calculate_volume(self, vertices):\n        # Calculate the volume of a simplex (generalization of a triangle or tetrahedron)\n        # using the Cayley-Menger determinant\n        n = len(vertices)\n        if n <= 1:\n            return 0\n        \n        distances = np.zeros((n, n))\n        for i in range(n):\n            for j in range(i + 1, n):\n                distances[i, j] = np.linalg.norm(vertices[i] - vertices[j])**2\n                distances[j, i] = distances[i, j]\n\n        M = np.zeros((n + 1, n + 1))\n        M[1:, 0] = 1\n        M[0, 1:] = 1\n        M[1:, 1:] = distances\n\n        volume_squared = ((-1)**(n+1) / (2**(n) * np.math.factorial(n-1)**2)) * np.linalg.det(M) # Modified factorial for integer input\n        if volume_squared < 0:\n            return 0 #Volume is zero, or negative due to numerical errors\n        return np.sqrt(volume_squared)", "configspace": "", "generation": 3, "feedback": "Evaluation timed out after 300 seconds.", "error": "Evaluation timed out after 300 seconds.", "parent_ids": ["e16e90d9-ba5f-43ec-b171-d0521e5848f9"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "64262cfc-dcb7-40b2-87b5-c25943b36beb", "fitness": 0.5158281100558626, "name": "SelfAdjustingOptimizer", "description": "A self-adjusting population-based algorithm that combines differential evolution, Gaussian local search, and a Cauchy mutation operator with dynamically adjusted probabilities and step sizes, incorporating a restart mechanism triggered by stagnation detection based on fitness improvement.", "code": "import numpy as np\n\nclass SelfAdjustingOptimizer:\n    def __init__(self, budget=10000, dim=10, pop_size=20):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lb = -5.0\n        self.ub = 5.0\n        self.population = None\n        self.fitness = None\n        self.best_x = None\n        self.best_fitness = np.inf\n        self.budget_used = 0\n\n        self.de_success = 0\n        self.gauss_success = 0\n        self.cauchy_success = 0\n        self.de_count = 0\n        self.gauss_count = 0\n        self.cauchy_count = 0\n        self.epsilon = 1e-6\n        self.diversity_threshold = 0.01\n        self.stagnation_counter = 0\n        self.stagnation_threshold = 50 # Increased stagnation threshold\n\n\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        \n        while self.budget_used < self.budget:\n            old_best_fitness = self.best_fitness\n            self.adjust_probabilities()\n\n            for i in range(self.pop_size):\n                rand = np.random.rand()\n\n                if rand < self.de_prob:\n                    self.differential_evolution(func, i)\n                elif rand < self.de_prob + self.gauss_prob:\n                    self.gaussian_local_search(func, i)\n                else:\n                    self.cauchy_mutation(func, i)\n\n                if self.fitness[i] < self.best_fitness:\n                    self.best_fitness = self.fitness[i]\n                    self.best_x = self.population[i].copy()\n\n            if self.population_diversity() < self.diversity_threshold:\n                self.reinitialize_population(func)\n                self.stagnation_counter = 0  # Reset stagnation counter\n\n            # Stagnation detection based on fitness improvement\n            if self.best_fitness >= old_best_fitness:\n                self.stagnation_counter += 1\n            else:\n                self.stagnation_counter = 0  # Reset if improvement\n\n            if self.stagnation_counter > self.stagnation_threshold:\n                self.reinitialize_population(func)\n                self.stagnation_counter = 0\n                # Reduce probabilities upon restart for increased exploration\n                self.de_prob *= 0.9\n                self.gauss_prob *= 0.9\n                self.cauchy_prob *= 0.9\n                total_prob = self.de_prob + self.gauss_prob + self.cauchy_prob\n                self.de_prob /= total_prob\n                self.gauss_prob /= total_prob\n                self.cauchy_prob /= total_prob\n        return self.best_fitness, self.best_x\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget_used += self.pop_size\n        self.best_index = np.argmin(self.fitness)\n        self.best_x = self.population[self.best_index].copy()\n        self.best_fitness = self.fitness[self.best_index].copy()\n        self.de_prob = 0.33\n        self.gauss_prob = 0.33\n        self.cauchy_prob = 0.34\n        \n    def reinitialize_population(self, func):\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget_used += self.pop_size\n        self.best_index = np.argmin(self.fitness)\n        self.best_x = self.population[self.best_index].copy()\n        self.best_fitness = self.fitness[self.best_index].copy()\n\n    def adjust_probabilities(self):\n        self.de_prob = (self.de_success / (self.de_count + self.epsilon)) if self.de_count > 0 else 0.33\n        self.gauss_prob = (self.gauss_success / (self.gauss_count + self.epsilon)) if self.gauss_count > 0 else 0.33\n        self.cauchy_prob = (self.cauchy_success / (self.cauchy_count + self.epsilon)) if self.cauchy_count > 0 else 0.34\n\n        total_prob = self.de_prob + self.gauss_prob + self.cauchy_prob\n        self.de_prob /= total_prob\n        self.gauss_prob /= total_prob\n        self.cauchy_prob /= total_prob\n\n    def differential_evolution(self, func, i):\n        self.de_count += 1\n        indices = np.random.choice(self.pop_size, 3, replace=False)\n        x_r1, x_r2, x_r3 = self.population[indices]\n        mutation = x_r1 + 0.5 * (x_r2 - x_r3)\n        crossover = np.random.rand(self.dim) < 0.5\n        new_x = np.where(crossover, mutation, self.population[i])\n        new_x = np.clip(new_x, self.lb, self.ub)\n        new_fitness = func(new_x)\n        self.budget_used += 1\n\n        if new_fitness < self.fitness[i]:\n            self.de_success += 1\n            self.population[i] = new_x\n            self.fitness[i] = new_fitness\n\n    def gaussian_local_search(self, func, i):\n        self.gauss_count += 1\n        step_size = 0.1 * (self.ub - self.lb) # Step size relative to search space\n        new_x = self.population[i] + step_size * np.random.randn(self.dim)\n        new_x = np.clip(new_x, self.lb, self.ub)\n        new_fitness = func(new_x)\n        self.budget_used += 1\n\n        if new_fitness < self.fitness[i]:\n            self.gauss_success += 1\n            self.population[i] = new_x\n            self.fitness[i] = new_fitness\n\n    def cauchy_mutation(self, func, i):\n        self.cauchy_count += 1\n        scale = 0.05 * (self.ub - self.lb)  # Scale parameter for Cauchy distribution\n        new_x = self.population[i] + scale * np.random.standard_cauchy(size=self.dim)\n        new_x = np.clip(new_x, self.lb, self.ub)\n        new_fitness = func(new_x)\n        self.budget_used += 1\n\n        if new_fitness < self.fitness[i]:\n            self.cauchy_success += 1\n            self.population[i] = new_x\n            self.fitness[i] = new_fitness\n            \n    def population_diversity(self):\n        centroid = np.mean(self.population, axis=0)\n        distances = np.linalg.norm(self.population - centroid, axis=1)\n        mean_distance = np.mean(distances)\n        return mean_distance", "configspace": "", "generation": 4, "feedback": "The algorithm SelfAdjustingOptimizer scored 0.516 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["415411bb-9745-4b77-b8d6-e1033bde5393"], "operator": null, "metadata": {"aucs": [0.15830087343353283, 0.22382770576228195, 0.4860385447293726, 0.8254063052352663, 0.5312738282954399, 0.5833308560671423, 0.4039932506149385, 0.4561238741556929, 0.5102507899514284, 0.3692882938031917, 0.7030066451938912, 0.9971981755004838, 0.2749011751302829, 0.5243276607612919, 0.7895377383934977, 0.6301054208822701, 0.4649738563275746, 0.6855576321512807, 0.1921799574750216, 0.5069396172533693]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "db99e080-e1b6-486a-ba3e-e2c41e10b2f7", "fitness": "-inf", "name": "AdaptiveSimplexHybridOptimizer", "description": "A hybrid metaheuristic combining differential evolution, particle swarm optimization, and a simplex-based local search, with adaptive strategy selection based on recent performance and a mirrored sampling technique to enhance exploration.", "code": "import numpy as np\n\nclass AdaptiveSimplexHybridOptimizer:\n    def __init__(self, budget=10000, dim=10, pop_size=20, simplex_size=5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lb = -5.0\n        self.ub = 5.0\n        self.simplex_size = simplex_size  # Number of vertices in the simplex\n\n        self.de_weight = 0.33\n        self.pso_weight = 0.33\n        self.simplex_weight = 0.34\n        self.weights_history = []\n\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget_used = self.pop_size\n\n        self.best_index = np.argmin(self.fitness)\n        self.best_x = self.population[self.best_index].copy()\n        self.best_fitness = self.fitness[self.best_index].copy()\n\n        self.f_opt = self.best_fitness\n        self.x_opt = self.best_x\n        \n        def mirrored_sampling(x):\n            x_mirrored = x.copy()\n            for i in range(self.dim):\n                if np.random.rand() < 0.5:\n                    if x[i] > 0:\n                        x_mirrored[i] = -x[i]\n                    else:\n                         x_mirrored[i] = -x[i]\n            return x_mirrored\n\n\n        while self.budget_used < self.budget:\n            new_population = np.zeros_like(self.population)\n            new_fitness = np.zeros(self.pop_size)\n\n\n            # Adaptive strategy selection based on weights\n            cumulative_weights = np.cumsum([self.de_weight, self.pso_weight, self.simplex_weight])\n\n            for i in range(self.pop_size):\n                rand = np.random.rand()\n\n                if rand < cumulative_weights[0]: # Differential Evolution\n                    indices = np.random.choice(self.pop_size, 3, replace=False)\n                    x_r1, x_r2, x_r3 = self.population[indices]\n                    mutation = x_r1 + 0.5 * (x_r2 - x_r3)\n                    crossover = np.random.rand(self.dim) < 0.5  # binomial crossover\n                    new_x = np.where(crossover, mutation, self.population[i])\n                    new_x = np.clip(new_x, self.lb, self.ub)\n                    new_population[i] = new_x\n\n                elif rand < cumulative_weights[1]: # Particle Swarm Optimization\n                    inertia = 0.7\n                    cognitive_coeff = 1.5\n                    social_coeff = 1.5\n                    velocity = inertia * (self.population[i] - self.best_x) + cognitive_coeff * np.random.rand(self.dim) * (self.best_x - self.population[i]) + social_coeff * np.random.rand(self.dim) * (self.best_x - self.population[i])\n                    new_x = self.population[i] + velocity\n                    new_x = np.clip(new_x, self.lb, self.ub)\n                    new_population[i] = new_x\n\n                else:  # Simplex search\n                    # Select simplex vertices (including the current point)\n                    simplex_indices = np.random.choice(self.pop_size, self.simplex_size -1 , replace=False)\n                    simplex_points = self.population[simplex_indices]\n                    simplex_points = np.vstack((simplex_points, self.population[i]))\n                    \n                    # Calculate centroid (excluding the worst point)\n                    simplex_fitness = np.array([func(x) for x in simplex_points])\n                    worst_index = np.argmax(simplex_fitness)\n                    centroid = np.mean(np.delete(simplex_points, worst_index, axis=0), axis=0)\n                    \n                    # Reflection\n                    reflection_point = centroid + 1.3 * (centroid - simplex_points[worst_index])\n                    reflection_point = np.clip(reflection_point, self.lb, self.ub)\n                    new_population[i] = reflection_point\n                \n                new_population[i] = np.clip(new_population[i], self.lb, self.ub)\n\n                # Mirrored sampling\n                if np.random.rand() < 0.1:\n                    mirrored_x = mirrored_sampling(new_population[i])\n                    mirrored_x = np.clip(mirrored_x, self.lb, self.ub)\n                    mirrored_fitness = func(mirrored_x)\n                    self.budget_used += 1\n                    if mirrored_fitness < func(new_population[i]):\n                        new_population[i] = mirrored_x\n                        new_fitness[i] = mirrored_fitness\n                    else:\n                        new_fitness[i] = func(new_population[i])\n                        self.budget_used +=1 \n                else:\n                    new_fitness[i] = func(new_population[i])\n                    self.budget_used += 1\n\n            # Update population and best solution\n            for i in range(self.pop_size):\n                if new_fitness[i] < self.fitness[i]:\n                    self.population[i] = new_population[i]\n                    self.fitness[i] = new_fitness[i]\n\n                    if new_fitness[i] < self.best_fitness:\n                        self.best_x = new_population[i].copy()\n                        self.best_fitness = new_fitness[i].copy()\n\n                        if self.best_fitness < self.f_opt:\n                            self.f_opt = self.best_fitness\n                            self.x_opt = self.best_x\n\n            # Adjust strategy weights based on recent performance (simplified)\n            success_de = np.sum(new_fitness[0:int(self.pop_size*self.de_weight)] < self.fitness[0:int(self.pop_size*self.de_weight)])\n            success_pso = np.sum(new_fitness[int(self.pop_size*self.de_weight):int(self.pop_size*(self.de_weight+self.pso_weight))] < self.fitness[int(self.pop_size*self.de_weight):int(self.pop_size*(self.de_weight+self.pso_weight))])\n            success_simplex = np.sum(new_fitness[int(self.pop_size*(self.de_weight+self.pso_weight)):] < self.fitness[int(self.pop_size*(self.de_weight+self.pso_weight)):])\n\n            total_success = success_de + success_pso + success_simplex\n\n            if total_success > 0:\n                self.de_weight = success_de / total_success\n                self.pso_weight = success_pso / total_success\n                self.simplex_weight = success_simplex / total_success\n            else:\n                # If no improvement, revert to default weights\n                self.de_weight = 0.33\n                self.pso_weight = 0.33\n                self.simplex_weight = 0.34\n        \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "Evaluation timed out after 300 seconds.", "error": "Evaluation timed out after 300 seconds.", "parent_ids": ["e16e90d9-ba5f-43ec-b171-d0521e5848f9"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "cad5116a-e655-41e7-8b78-e6ae0f373ca3", "fitness": "-inf", "name": "AdaptiveHybridOptimizer", "description": "A simplified hybrid optimizer that adaptively adjusts probabilities of DE, PSO, and a simplified CMA-ES based on their recent success, employing a smaller population and archive.", "code": "import numpy as np\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget=10000, dim=10, pop_size=10, archive_size=5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lb = -5.0\n        self.ub = 5.0\n        self.archive_size = archive_size\n        self.archive = []\n        self.archive_fitness = []\n\n        self.de_success = 0\n        self.pso_success = 0\n        self.cmaes_success = 0\n        self.de_count = 0\n        self.pso_count = 0\n        self.cmaes_count = 0\n        self.epsilon = 1e-6\n        self.inertia = 0.7\n        self.cognitive_coeff = 1.5\n        self.social_coeff = 1.5\n        self.personal_best_positions = None\n        self.personal_best_fitness = None\n\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x)] for x in self.population)\n        self.budget_used = self.pop_size\n\n        self.best_index = np.argmin(self.fitness)\n        self.best_x = self.population[self.best_index].copy()\n        self.best_fitness = self.fitness[self.best_index].copy()\n\n        self.f_opt = self.best_fitness\n        self.x_opt = self.best_x\n\n        self.add_to_archive(self.best_x, self.best_fitness)\n        \n        self.velocities = np.zeros_like(self.population)\n        self.personal_best_positions = self.population.copy()\n        self.personal_best_fitness = self.fitness.copy()\n        global_best_index = np.argmin(self.personal_best_fitness)\n        self.global_best_position = self.personal_best_positions[global_best_index].copy()\n        \n        while self.budget_used < self.budget:\n            new_population = np.zeros_like(self.population)\n            new_fitness = np.zeros_like(self.fitness)\n            \n            de_prob = (self.de_success / (self.de_count + self.epsilon)) if self.de_count > 0 else 0.33\n            pso_prob = (self.pso_success / (self.pso_count + self.epsilon)) if self.pso_count > 0 else 0.33\n            cmaes_prob = (self.cmaes_success / (self.cmaes_count + self.epsilon)) if self.cmaes_count > 0 else 0.34\n            \n            # Normalize probabilities\n            total_prob = de_prob + pso_prob + cmaes_prob\n            de_prob /= total_prob\n            pso_prob /= total_prob\n            cmaes_prob /= total_prob\n            \n            for i in range(self.pop_size):\n                rand = np.random.rand()\n                \n                if rand < de_prob:\n                    # Differential Evolution\n                    self.de_count += 1\n                    indices = np.random.choice(self.pop_size, 3, replace=False)\n                    x_r1, x_r2, x_r3 = self.population[indices]\n                    mutation = x_r1 + 0.5 * (x_r2 - x_r3)\n                    crossover = np.random.rand(self.dim) < 0.5\n                    new_x = np.where(crossover, mutation, self.population[i])\n                    new_x = np.clip(new_x, self.lb, self.ub)\n                    new_fitness[i] = func(new_x)\n                    self.budget_used += 1\n\n                    if new_fitness[i] < self.fitness[i]:\n                        self.de_success += 1\n                        self.population[i] = new_x\n                        self.fitness[i] = new_fitness[i]\n                        self.add_to_archive(new_x, new_fitness[i])\n\n                elif rand < de_prob + pso_prob:\n                    # Particle Swarm Optimization\n                    self.pso_count += 1\n                    r1 = np.random.rand(self.dim)\n                    r2 = np.random.rand(self.dim)\n                    self.velocities[i] = (self.inertia * self.velocities[i] +\n                                        self.cognitive_coeff * r1 * (self.personal_best_positions[i] - self.population[i]) +\n                                        self.social_coeff * r2 * (self.global_best_position - self.population[i]))\n                    new_x = self.population[i] + self.velocities[i]\n                    new_x = np.clip(new_x, self.lb, self.ub)\n                    new_fitness[i] = func(new_x)\n                    self.budget_used += 1\n\n                    if new_fitness[i] < self.fitness[i]:\n                        self.pso_success += 1\n                        self.population[i] = new_x\n                        self.fitness[i] = new_fitness[i]\n                        self.add_to_archive(new_x, new_fitness[i])\n\n                        if new_fitness[i] < self.personal_best_fitness[i]:\n                            self.personal_best_fitness[i] = new_fitness[i]\n                            self.personal_best_positions[i] = new_x.copy()\n\n                            if new_fitness[i] < self.f_opt:\n                                self.f_opt = new_fitness[i]\n                                self.x_opt = new_x\n                                self.global_best_position = new_x.copy()\n                else:\n                    # CMA-ES (Simplified)\n                    self.cmaes_count += 1\n                    step_size = 0.1\n                    new_x = self.population[i] + step_size * np.random.randn(self.dim)\n                    new_x = np.clip(new_x, self.lb, self.ub)\n                    new_fitness[i] = func(new_x)\n                    self.budget_used += 1\n\n                    if new_fitness[i] < self.fitness[i]:\n                        self.cmaes_success += 1\n                        self.population[i] = new_x\n                        self.fitness[i] = new_fitness[i]\n                        self.add_to_archive(new_x, new_fitness[i])\n                \n                if self.fitness[i] < self.best_fitness:\n                    self.best_x = self.population[i].copy()\n                    self.best_fitness = self.fitness[i].copy()\n                    if self.best_fitness < self.f_opt:\n                        self.f_opt = self.best_fitness\n                        self.x_opt = self.best_x\n\n        return self.f_opt, self.x_opt\n\n    def add_to_archive(self, x, fitness):\n        if len(self.archive) < self.archive_size:\n            self.archive.append(x)\n            self.archive_fitness.append(fitness)\n        else:\n            max_fitness_index = np.argmax(self.archive_fitness)\n            if fitness < self.archive_fitness[max_fitness_index]:\n                self.archive[max_fitness_index] = x\n                self.archive_fitness[max_fitness_index] = fitness", "configspace": "", "generation": 4, "feedback": "In the code, line 37, in __call__, the following error occurred:\nIndexError: too many indices for array: array is 0-dimensional, but 1 were indexed\nOn line: self.best_fitness = self.fitness[self.best_index].copy()", "error": "In the code, line 37, in __call__, the following error occurred:\nIndexError: too many indices for array: array is 0-dimensional, but 1 were indexed\nOn line: self.best_fitness = self.fitness[self.best_index].copy()", "parent_ids": ["0eceaf09-69d5-4b47-93e6-80e157e7b18f"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "c94fa84c-8039-4ae1-b408-0aa0eea0594d", "fitness": 0.0, "name": "AdaptiveHybridOptimizer", "description": "A hybrid metaheuristic combining a modified Differential Evolution with a Neighborhood Search, adaptively adjusting search behavior based on stagnation detection and fitness landscape characteristics.", "code": "import numpy as np\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget=10000, dim=10, pop_size=20, neighborhood_size=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lb = -5.0\n        self.ub = 5.0\n        self.neighborhood_size = neighborhood_size #Radius for neighborhood search\n        self.stagnation_counter = 0\n        self.max_stagnation = 50 #Maximum number of iterations to allow without improvement\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget_used = self.pop_size\n\n        self.best_index = np.argmin(self.fitness)\n        self.best_x = self.population[self.best_index].copy()\n        self.best_fitness = self.fitness[self.best_index].copy()\n\n        self.f_opt = self.best_fitness\n        self.x_opt = self.best_x\n\n        #Adaptive parameter control\n        de_mutation_factor = 0.5\n        de_crossover_rate = 0.7\n        neighborhood_search_prob = 0.1 #Probability of performing a neighborhood search.\n\n        while self.budget_used < self.budget:\n            new_population = np.zeros_like(self.population)\n\n            for i in range(self.pop_size):\n                #Adaptive DE\n                indices = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = self.population[indices]\n                mutation = x_r1 + de_mutation_factor * (x_r2 - x_r3)\n                crossover = np.random.rand(self.dim) < de_crossover_rate\n                trial_vector = np.where(crossover, mutation, self.population[i])\n                trial_vector = np.clip(trial_vector, self.lb, self.ub)\n                new_population[i] = trial_vector\n\n                #Neighborhood search around best solution with probability\n                if np.random.rand() < neighborhood_search_prob:\n                    neighbor = self.best_x + np.random.normal(0, self.neighborhood_size, self.dim)\n                    neighbor = np.clip(neighbor, self.lb, self.ub)\n                    new_population[i] = neighbor\n\n            new_fitness = np.array([func(x) for x in new_population])\n            self.budget_used += self.pop_size\n\n            #Greedy selection\n            for i in range(self.pop_size):\n                if new_fitness[i] < self.fitness[i]:\n                    self.population[i] = new_population[i]\n                    self.fitness[i] = new_fitness[i]\n\n                    if new_fitness[i] < self.best_fitness:\n                        self.best_x = new_population[i].copy()\n                        self.best_fitness = new_fitness[i].copy()\n                        self.stagnation_counter = 0 #Reset stagnation counter\n\n                        if self.best_fitness < self.f_opt:\n                            self.f_opt = self.best_fitness\n                            self.x_opt = self.best_x\n                else:\n                    self.stagnation_counter +=1 #Increase stagnation count if no improvement\n\n            #Adaptive adjustment of search behavior based on stagnation\n            if self.stagnation_counter > self.max_stagnation:\n                 #Increase exploration if stagnating: Increase mutation factor, neighborhood search probability, or restart population\n                de_mutation_factor = min(de_mutation_factor * 1.1, 1.0) #Increase mutation\n                neighborhood_search_prob = min(neighborhood_search_prob*1.2, 0.8) # Increase neighborhood search prob.\n\n                #Optionally, restart a portion of the population\n                num_to_restart = int(0.1 * self.pop_size)\n                indices_to_restart = np.random.choice(self.pop_size, num_to_restart, replace=False)\n                self.population[indices_to_restart] = np.random.uniform(self.lb, self.ub, size=(num_to_restart, self.dim))\n                self.fitness[indices_to_restart] = np.array([func(x) for x in self.population[indices_to_restart]])\n                self.budget_used += num_to_restart\n                self.stagnation_counter = 0 #reset counter\n\n\n            #Reduce Exploration Over time: Decrease mutation factor\n            de_mutation_factor = max(de_mutation_factor * 0.99, 0.1)\n            neighborhood_search_prob = max(neighborhood_search_prob * 0.99, 0.01)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "The algorithm AdaptiveHybridOptimizer scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["e16e90d9-ba5f-43ec-b171-d0521e5848f9"], "operator": null, "metadata": {"aucs": [0]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "812405bc-51f8-4f53-9b1d-586cc16d008e", "fitness": 0.4570282972758701, "name": "AdaptiveMirroredDE", "description": "A hybrid optimizer combining differential evolution, a mirrored sampling technique, and a Cauchy mutation operator, adaptively adjusting probabilities based on success and population diversity to enhance exploration and exploitation.", "code": "import numpy as np\n\nclass AdaptiveMirroredDE:\n    def __init__(self, budget=10000, dim=10, pop_size=20, de_rate=0.6, mirror_rate=0.2, cauchy_rate=0.2, diversity_threshold=0.1, success_history_size=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.de_rate = de_rate\n        self.mirror_rate = mirror_rate\n        self.cauchy_rate = cauchy_rate\n        self.diversity_threshold = diversity_threshold\n        self.success_history_size = success_history_size\n        self.success_history = {'de': [], 'mirror': [], 'cauchy': []}\n        self.op_counts = {'de': 0, 'mirror': 0, 'cauchy': 0}\n\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget_used = self.pop_size\n\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n\n        while self.budget_used < self.budget:\n            new_population = np.zeros_like(self.population)\n            new_fitness = np.zeros_like(self.fitness)\n            \n            # Calculate diversity\n            diversity = np.std(self.population)\n            \n            # Adaptive Probability Adjustment\n            de_prob = self.de_rate #if diversity > self.diversity_threshold else 0.2\n            mirror_prob = self.mirror_rate #if diversity > self.diversity_threshold else 0.4\n            cauchy_prob = self.cauchy_rate #if diversity > self.diversity_threshold else 0.4\n            \n            total_prob = de_prob + mirror_prob + cauchy_prob\n            de_prob /= total_prob\n            mirror_prob /= total_prob\n            cauchy_prob /= total_prob\n\n            for i in range(self.pop_size):\n                rand = np.random.rand()\n\n                if rand < de_prob:\n                    # Differential Evolution\n                    indices = np.random.choice(self.pop_size, 3, replace=False)\n                    x_r1, x_r2, x_r3 = self.population[indices]\n                    mutation = x_r1 + 0.5 * (x_r2 - x_r3)\n                    crossover = np.random.rand(self.dim) < 0.5  # binomial crossover\n                    new_solution = np.where(crossover, mutation, self.population[i])\n                    op = 'de'\n\n                elif rand < de_prob + mirror_prob:\n                    # Mirrored Sampling\n                    reflection = 2 * np.mean(self.population, axis=0) - self.population[i]\n                    new_solution = np.random.uniform(np.minimum(self.population[i], reflection), np.maximum(self.population[i], reflection))\n                    op = 'mirror'\n\n                else:\n                    # Cauchy Mutation\n                    scale = 0.1 * (func.bounds.ub - func.bounds.lb)  # Scale based on bounds\n                    new_solution = self.population[i] + scale * np.random.standard_cauchy(size=self.dim)\n                    op = 'cauchy'\n\n                # Clip the solution\n                new_solution = np.clip(new_solution, func.bounds.lb, func.bounds.ub)\n\n                new_fitness_val = func(new_solution)\n                self.budget_used += 1\n                \n                if new_fitness_val < self.fitness[i]:\n                    new_population[i] = new_solution\n                    new_fitness[i] = new_fitness_val\n                    self.success_history[op].append(1)\n                    if len(self.success_history[op]) > self.success_history_size:\n                        self.success_history[op].pop(0)\n\n                else:\n                    new_population[i] = self.population[i]\n                    new_fitness[i] = self.fitness[i]\n                    self.success_history[op].append(0)\n                    if len(self.success_history[op]) > self.success_history_size:\n                        self.success_history[op].pop(0)\n                \n                self.op_counts[op] += 1\n                if new_fitness_val < self.f_opt:\n                    self.f_opt = new_fitness_val\n                    self.x_opt = new_solution\n                \n                if self.budget_used >= self.budget:\n                    break\n\n            self.population = new_population\n            self.fitness = new_fitness\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "The algorithm AdaptiveMirroredDE scored 0.457 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["d0fddf51-e6ed-4acb-82bd-f12a4f7ace8d"], "operator": null, "metadata": {"aucs": [0.14291990265847976, 0.33204571274057904, 0.3769870007783308, 0.6197933160754368, 0.5701979808170592, 0.4036046707405987, 0.3692776491187939, 0.3499631434904448, 0.5816621500189518, 0.40676901680613564, 0.4838689097413288, 0.9948355278346902, 0.28585612455100795, 0.40758493730487666, 0.7359398112449533, 0.3258524547960373, 0.4650004093829232, 0.5493728458468616, 0.2296323326411912, 0.5094020489287195]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "c2f99787-9262-4af7-b312-b4b3b7414e05", "fitness": 0.08147453709693508, "name": "AdaptiveHybridOptimizer", "description": "A self-adjusting hybrid optimizer that adaptively combines differential evolution, a simplified CMA-ES, and a gradient-based local search, dynamically adjusting probabilities and parameters based on success rates and population fitness.", "code": "import numpy as np\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget=10000, dim=10, pop_size=20):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lb = -5.0\n        self.ub = 5.0\n\n        self.de_success = 0\n        self.cmaes_success = 0\n        self.local_search_success = 0\n        self.de_count = 0\n        self.cmaes_count = 0\n        self.local_search_count = 0\n        self.epsilon = 1e-6\n        self.learning_rate = 0.1\n\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget_used = self.pop_size\n\n        self.best_index = np.argmin(self.fitness)\n        self.best_x = self.population[self.best_index].copy()\n        self.best_fitness = self.fitness[self.best_index].copy()\n\n        self.f_opt = self.best_fitness\n        self.x_opt = self.best_x\n\n        while self.budget_used < self.budget:\n            de_prob = (self.de_success / (self.de_count + self.epsilon)) if self.de_count > 0 else 0.33\n            cmaes_prob = (self.cmaes_success / (self.cmaes_count + self.epsilon)) if self.cmaes_count > 0 else 0.33\n            local_search_prob = (self.local_search_success / (self.local_search_count + self.epsilon)) if self.local_search_count > 0 else 0.34\n\n            total_prob = de_prob + cmaes_prob + local_search_prob\n            de_prob /= total_prob\n            cmaes_prob /= total_prob\n            local_search_prob /= total_prob\n\n            for i in range(self.pop_size):\n                rand = np.random.rand()\n\n                if rand < de_prob:\n                    # Differential Evolution\n                    self.de_count += 1\n                    indices = np.random.choice(self.pop_size, 3, replace=False)\n                    x_r1, x_r2, x_r3 = self.population[indices]\n                    mutation = x_r1 + 0.5 * (x_r2 - x_r3)\n                    crossover = np.random.rand(self.dim) < 0.5\n                    new_x = np.where(crossover, mutation, self.population[i])\n                    new_x = np.clip(new_x, self.lb, self.ub)\n                    new_fitness = func(new_x)\n                    self.budget_used += 1\n\n                    if new_fitness < self.fitness[i]:\n                        self.de_success += 1\n                        self.population[i] = new_x\n                        self.fitness[i] = new_fitness\n                    \n                elif rand < de_prob + cmaes_prob:\n                    # Simplified CMA-ES\n                    self.cmaes_count += 1\n                    step_size = 0.1\n                    new_x = self.population[i] + step_size * np.random.randn(self.dim)\n                    new_x = np.clip(new_x, self.lb, self.ub)\n                    new_fitness = func(new_x)\n                    self.budget_used += 1\n\n                    if new_fitness < self.fitness[i]:\n                        self.cmaes_success += 1\n                        self.population[i] = new_x\n                        self.fitness[i] = new_fitness\n\n                else:\n                    # Gradient-based Local Search\n                    self.local_search_count += 1\n                    step_size = 0.01  # Smaller step for gradient descent\n                    gradient = self.approximate_gradient(func, self.population[i])\n                    new_x = self.population[i] - step_size * gradient\n                    new_x = np.clip(new_x, self.lb, self.ub)\n                    new_fitness = func(new_x)\n                    self.budget_used += 1\n\n                    if new_fitness < self.fitness[i]:\n                        self.local_search_success += 1\n                        self.population[i] = new_x\n                        self.fitness[i] = new_fitness\n                        \n                if self.fitness[i] < self.best_fitness:\n                    self.best_x = self.population[i].copy()\n                    self.best_fitness = self.fitness[i].copy()\n                    if self.best_fitness < self.f_opt:\n                        self.f_opt = self.best_fitness\n                        self.x_opt = self.best_x\n\n        return self.f_opt, self.x_opt\n    \n    def approximate_gradient(self, func, x, h=1e-5):\n        gradient = np.zeros_like(x)\n        for i in range(self.dim):\n            x_plus_h = x.copy()\n            x_minus_h = x.copy()\n            x_plus_h[i] += h\n            x_minus_h[i] -= h\n            gradient[i] = (func(x_plus_h) - func(x_minus_h)) / (2 * h)\n            self.budget_used += 2 #gradient requires 2 evaluations per dimension.\n            if self.budget_used >= self.budget: #early stopping\n              break \n        return gradient", "configspace": "", "generation": 4, "feedback": "The algorithm AdaptiveHybridOptimizer scored 0.081 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["0eceaf09-69d5-4b47-93e6-80e157e7b18f"], "operator": null, "metadata": {"aucs": [0.16294907419387017, 0]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "a34b5679-a3a4-4e03-9413-7587a7aa4eb4", "fitness": 0.48860416667516315, "name": "AdaptiveHybridOptimizer", "description": "A simplified adaptive hybrid optimizer that balances differential evolution and a modified CMA-ES based on success rates, coupled with a simple archive.", "code": "import numpy as np\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget=10000, dim=10, pop_size=20, archive_size=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lb = -5.0\n        self.ub = 5.0\n        self.archive_size = archive_size\n        self.archive = []\n        self.archive_fitness = []\n\n        self.de_success = 0\n        self.cmaes_success = 0\n        self.de_count = 0\n        self.cmaes_count = 0\n        self.epsilon = 1e-6\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget_used = self.pop_size\n\n        self.best_index = np.argmin(self.fitness)\n        self.best_x = self.population[self.best_index].copy()\n        self.best_fitness = self.fitness[self.best_index].copy()\n\n        self.f_opt = self.best_fitness\n        self.x_opt = self.best_x\n\n        self.add_to_archive(self.best_x, self.best_fitness)\n\n        while self.budget_used < self.budget:\n            de_prob = (self.de_success / (self.de_count + self.epsilon)) if self.de_count > 0 else 0.5\n            cmaes_prob = 1 - de_prob\n\n            for i in range(self.pop_size):\n                rand = np.random.rand()\n\n                if rand < de_prob:\n                    # Differential Evolution\n                    self.de_count += 1\n                    indices = np.random.choice(self.pop_size, 3, replace=False)\n                    x_r1, x_r2, x_r3 = self.population[indices]\n                    mutation = x_r1 + 0.5 * (x_r2 - x_r3)\n                    crossover = np.random.rand(self.dim) < 0.5\n                    new_x = np.where(crossover, mutation, self.population[i])\n                    new_x = np.clip(new_x, self.lb, self.ub)\n                    new_fitness = func(new_x)\n                    self.budget_used += 1\n\n                    if new_fitness < self.fitness[i]:\n                        self.de_success += 1\n                        self.population[i] = new_x\n                        self.fitness[i] = new_fitness\n                        self.add_to_archive(new_x, new_fitness)\n                        if new_fitness < self.f_opt:\n                            self.f_opt = new_fitness\n                            self.x_opt = new_x\n\n                else:\n                    # Simplified CMA-ES with adaptive step size\n                    self.cmaes_count += 1\n                    step_size = 0.1 * np.exp(np.random.normal(0, 0.1)) # Adaptive step size\n                    new_x = self.population[i] + step_size * np.random.randn(self.dim)\n                    new_x = np.clip(new_x, self.lb, self.ub)\n                    new_fitness = func(new_x)\n                    self.budget_used += 1\n\n                    if new_fitness < self.fitness[i]:\n                        self.cmaes_success += 1 # This variable isn't currently used, but it's kept for future extension.\n                        self.population[i] = new_x\n                        self.fitness[i] = new_fitness\n                        self.add_to_archive(new_x, new_fitness)\n                        if new_fitness < self.f_opt:\n                            self.f_opt = new_fitness\n                            self.x_opt = new_x\n\n        return self.f_opt, self.x_opt\n\n    def add_to_archive(self, x, fitness):\n        if len(self.archive) < self.archive_size:\n            self.archive.append(x)\n            self.archive_fitness.append(fitness)\n        else:\n            max_fitness_index = np.argmax(self.archive_fitness)\n            if fitness < self.archive_fitness[max_fitness_index]:\n                self.archive[max_fitness_index] = x\n                self.archive_fitness[max_fitness_index] = fitness", "configspace": "", "generation": 4, "feedback": "The algorithm AdaptiveHybridOptimizer scored 0.489 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["0eceaf09-69d5-4b47-93e6-80e157e7b18f"], "operator": null, "metadata": {"aucs": [0.13957621306260792, 0.24095869197645292, 0.5052242634468509, 0.6954878897799611, 0.4515270461355694, 0.7319444726155084, 0.3310687955678989, 0.4663867069565859, 0.42960620535648864, 0.2813460362792326, 0.6023633794551229, 0.9799765316914533, 0.3143484129290005, 0.3464636946650328, 0.7791510737275471, 0.654258184441746, 0.3362076485306337, 0.7884450877360109, 0.20718945195957295, 0.490553547189984]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "c731ccf5-e1be-4e18-bde6-7c0bfeac22ee", "fitness": 0.46267247838887304, "name": "NeighborhoodAwareDE", "description": "A hybrid optimizer that combines a modified differential evolution with a neighborhood-based mutation and dynamically adjusts the mutation strength based on population diversity.", "code": "import numpy as np\n\nclass NeighborhoodAwareDE:\n    def __init__(self, budget=10000, dim=10, pop_size=20, neighborhood_size=5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lb = -5.0\n        self.ub = 5.0\n        self.neighborhood_size = neighborhood_size\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget_used = self.pop_size\n\n        self.best_index = np.argmin(self.fitness)\n        self.best_x = self.population[self.best_index].copy()\n        self.best_fitness = self.fitness[self.best_index].copy()\n\n        self.f_opt = self.best_fitness\n        self.x_opt = self.best_x\n        \n        mutation_factor = 0.5\n        crossover_rate = 0.7\n        \n        while self.budget_used < self.budget:\n            new_population = np.zeros_like(self.population)\n            \n            # Calculate population diversity\n            diversity = np.std(self.population)\n\n            # Dynamically adjust mutation strength based on diversity\n            current_mutation_factor = mutation_factor * (1 + diversity)  # Increase mutation when diversity is high\n\n            for i in range(self.pop_size):\n                # Neighborhood-based mutation\n                neighbors_indices = np.random.choice(self.pop_size, self.neighborhood_size, replace=False)\n                x_neighbors = self.population[neighbors_indices]\n                \n                #Modified DE\n                \n                indices = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = self.population[indices]\n\n\n                # Base vector selection (current individual or best individual)\n                if np.random.rand() < 0.5:\n                    base_vector = self.population[i]\n                else:\n                    base_vector = self.best_x\n\n\n                # Create a difference vector using the neighborhood\n                diff_vector = np.mean(x_neighbors, axis=0) - base_vector\n                mutant_vector = base_vector + current_mutation_factor * diff_vector\n\n                #Crossover\n                crossover = np.random.rand(self.dim) < crossover_rate  # binomial crossover\n                new_population[i] = np.where(crossover, mutant_vector, self.population[i])\n                \n                new_population[i] = np.clip(new_population[i], self.lb, self.ub)\n\n            new_fitness = np.array([func(x) for x in new_population])\n            self.budget_used += self.pop_size\n\n            for i in range(self.pop_size):\n                if new_fitness[i] < self.fitness[i]:\n                    self.population[i] = new_population[i]\n                    self.fitness[i] = new_fitness[i]\n\n                    if new_fitness[i] < self.best_fitness:\n                        self.best_x = new_population[i].copy()\n                        self.best_fitness = new_fitness[i].copy()\n                        \n                        if self.best_fitness < self.f_opt:\n                            self.f_opt = self.best_fitness\n                            self.x_opt = self.best_x\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "The algorithm NeighborhoodAwareDE scored 0.463 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["e16e90d9-ba5f-43ec-b171-d0521e5848f9"], "operator": null, "metadata": {"aucs": [0.13958622561677914, 0.2193699588938458, 0.2611714088718723, 0.1806903432428264, 0.8202283956445051, 0.2718843152103968, 0.7547230031959107, 0.2840669872122127, 0.22243914606905335, 0.6834537829201832, 0.9166815756872019, 0.9913628083862577, 0.24490675919396454, 0.22940981937828941, 0.7961013667197661, 0.3174168756452165, 0.3149485141986652, 0.9183885412339386, 0.1861471364873305, 0.5004726039692459]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "48cb07d6-ebee-453b-9cfa-4ad6da9f66f7", "fitness": "-inf", "name": "SelfAdjustingOptimizer", "description": "A self-adjusting population-based algorithm that combines differential evolution, Gaussian local search, and adaptive Cauchy mutation with dynamically adjusted probabilities and step sizes, incorporating a restart mechanism and momentum to enhance exploration and exploitation.", "code": "import numpy as np\n\nclass SelfAdjustingOptimizer:\n    def __init__(self, budget=10000, dim=10, pop_size=20):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lb = -5.0\n        self.ub = 5.0\n        self.population = None\n        self.fitness = None\n        self.best_x = None\n        self.best_fitness = np.inf\n        self.budget_used = 0\n\n        self.de_success = 0\n        self.gauss_success = 0\n        self.cauchy_success = 0\n        self.de_count = 0\n        self.gauss_count = 0\n        self.cauchy_count = 0\n        self.epsilon = 1e-6\n        self.diversity_threshold = 0.01\n        self.stagnation_counter = 0\n        self.stagnation_threshold = 50\n        self.mutation_scale = 0.1\n        self.momentum = 0.1 #Introduced momentum\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        self.velocities = np.zeros_like(self.population) # Initialize velocities\n\n        while self.budget_used < self.budget:\n            old_best_fitness = self.best_fitness\n            self.adjust_probabilities()\n\n            for i in range(self.pop_size):\n                rand = np.random.rand()\n\n                if rand < self.de_prob:\n                    self.differential_evolution(func, i)\n                elif rand < self.de_prob + self.gauss_prob:\n                    self.gaussian_local_search(func, i)\n                else:\n                    self.cauchy_mutation(func, i)\n\n                if self.fitness[i] < self.best_fitness:\n                    self.best_fitness = self.fitness[i]\n                    self.best_x = self.population[i].copy()\n\n            if self.population_diversity() < self.diversity_threshold:\n                self.reinitialize_population(func)\n                self.stagnation_counter = 0\n\n            if self.best_fitness >= old_best_fitness:\n                self.stagnation_counter += 1\n            else:\n                self.stagnation_counter = 0\n\n            if self.stagnation_counter > self.stagnation_threshold:\n                self.reinitialize_population(func)\n                self.stagnation_counter = 0\n                self.de_prob = 0.33\n                self.gauss_prob = 0.33\n                self.cauchy_prob = 0.34\n        return self.best_fitness, self.best_x\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget_used += self.pop_size\n        self.best_index = np.argmin(self.fitness)\n        self.best_x = self.population[self.best_index].copy()\n        self.best_fitness = self.fitness[self.best_index].copy()\n        self.de_prob = 0.33\n        self.gauss_prob = 0.33\n        self.cauchy_prob = 0.34\n        \n    def reinitialize_population(self, func):\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget_used += self.pop_size\n        self.best_index = np.argmin(self.fitness)\n        self.best_x = self.population[self.best_index].copy()\n        self.best_fitness = self.fitness[self.best_index].copy()\n        self.velocities = np.zeros_like(self.population) # Reinitialize velocities\n\n\n    def adjust_probabilities(self):\n        self.de_prob = (self.de_success / (self.de_count + self.epsilon)) if self.de_count > 0 else 0.33\n        self.gauss_prob = (self.gauss_success / (self.gauss_count + self.epsilon)) if self.gauss_count > 0 else 0.33\n        self.cauchy_prob = (self.cauchy_success / (self.cauchy_count + self.epsilon)) if self.cauchy_count > 0 else 0.34\n\n        total_prob = self.de_prob + self.gauss_prob + self.cauchy_prob\n        self.de_prob /= total_prob\n        self.gauss_prob /= total_prob\n        self.cauchy_prob /= total_prob\n\n    def differential_evolution(self, func, i):\n        self.de_count += 1\n        indices = np.random.choice(self.pop_size, 3, replace=False)\n        x_r1, x_r2, x_r3 = self.population[indices]\n        mutation = x_r1 + 0.5 * (x_r2 - x_r3)\n        crossover = np.random.rand(self.dim) < 0.5\n        new_x = np.where(crossover, mutation, self.population[i])\n        new_x = np.clip(new_x, self.lb, self.ub)\n        \n        #Apply momentum\n        self.velocities[i] = self.momentum * self.velocities[i] + (1-self.momentum) * (new_x - self.population[i])\n        new_x = self.population[i] + self.velocities[i]\n        new_x = np.clip(new_x, self.lb, self.ub)\n        \n        new_fitness = func(new_x)\n        self.budget_used += 1\n\n        if new_fitness < self.fitness[i]:\n            self.de_success += 1\n            self.population[i] = new_x\n            self.fitness[i] = new_fitness\n\n    def gaussian_local_search(self, func, i):\n        self.gauss_count += 1\n        step_size = self.mutation_scale * (self.ub - self.lb)\n        new_x = self.population[i] + step_size * np.random.randn(self.dim)\n        new_x = np.clip(new_x, self.lb, self.ub)\n        \n        #Apply momentum\n        self.velocities[i] = self.momentum * self.velocities[i] + (1-self.momentum) * (new_x - self.population[i])\n        new_x = self.population[i] + self.velocities[i]\n        new_x = np.clip(new_x, self.lb, self.ub)\n\n        new_fitness = func(new_x)\n        self.budget_used += 1\n\n        if new_fitness < self.fitness[i]:\n            self.gauss_success += 1\n            self.population[i] = new_x\n            self.fitness[i] = new_fitness\n\n    def cauchy_mutation(self, func, i):\n        self.cauchy_count += 1\n        scale = self.mutation_scale * (self.ub - self.lb) * np.random.rand()  # Adaptive scale\n        new_x = self.population[i] + scale * np.random.standard_cauchy(size=self.dim)\n        new_x = np.clip(new_x, self.lb, self.ub)\n\n        #Apply momentum\n        self.velocities[i] = self.momentum * self.velocities[i] + (1-self.momentum) * (new_x - self.population[i])\n        new_x = self.population[i] + self.velocities[i]\n        new_x = np.clip(new_x, self.lb, self.ub)\n        \n        new_fitness = func(new_x)\n        self.budget_used += 1\n\n        if new_fitness < self.fitness[i]:\n            self.cauchy_success += 1\n            self.population[i] = new_x\n            self.fitness[i] = new_fitness\n            \n    def population_diversity(self):\n        centroid = np.mean(self.population, axis=0)\n        distances = np.linalg.norm(self.population - centroid, axis=1)\n        mean_distance = np.mean(distances)\n        return mean_distance", "configspace": "", "generation": 5, "feedback": "Evaluation timed out after 300 seconds.", "error": "Evaluation timed out after 300 seconds.", "parent_ids": ["64262cfc-dcb7-40b2-87b5-c25943b36beb"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "16b77497-ae98-4de9-b3d5-16d7f41272fa", "fitness": "-inf", "name": "AdaptiveSAMetaheuristic", "description": "A self-adjusting optimizer that uses a combination of Differential Evolution, Gaussian Local Search, and a Simulated Annealing-inspired acceptance criterion, dynamically adjusting probabilities and temperatures based on success rates and fitness landscape characteristics.", "code": "import numpy as np\n\nclass AdaptiveSAMetaheuristic:\n    def __init__(self, budget=10000, dim=10, pop_size=20, initial_temp=1.0, cooling_rate=0.95):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lb = -5.0\n        self.ub = 5.0\n        self.population = None\n        self.fitness = None\n        self.best_x = None\n        self.best_fitness = np.inf\n        self.budget_used = 0\n\n        self.de_success = 0\n        self.gauss_success = 0\n        self.sa_success = 0  # Track SA success\n        self.de_count = 0\n        self.gauss_count = 0\n        self.sa_count = 0\n        self.epsilon = 1e-6\n        self.diversity_threshold = 0.01\n        self.stagnation_counter = 0\n        self.stagnation_threshold = 50\n        self.initial_temp = initial_temp\n        self.current_temp = initial_temp\n        self.cooling_rate = cooling_rate\n\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        \n        while self.budget_used < self.budget:\n            old_best_fitness = self.best_fitness\n            self.adjust_probabilities()\n\n            for i in range(self.pop_size):\n                rand = np.random.rand()\n\n                if rand < self.de_prob:\n                    self.differential_evolution(func, i)\n                elif rand < self.de_prob + self.gauss_prob:\n                    self.gaussian_local_search(func, i)\n                else:\n                    self.simulated_annealing_move(func, i) # Replaced Cauchy with SA\n\n                if self.fitness[i] < self.best_fitness:\n                    self.best_fitness = self.fitness[i]\n                    self.best_x = self.population[i].copy()\n\n            if self.population_diversity() < self.diversity_threshold:\n                self.reinitialize_population(func)\n                self.stagnation_counter = 0\n\n            if self.best_fitness >= old_best_fitness:\n                self.stagnation_counter += 1\n            else:\n                self.stagnation_counter = 0\n\n            if self.stagnation_counter > self.stagnation_threshold:\n                self.reinitialize_population(func)\n                self.stagnation_counter = 0\n                self.de_prob *= 0.9\n                self.gauss_prob *= 0.9\n                self.sa_prob *= 0.9 # Update SA probability\n                total_prob = self.de_prob + self.gauss_prob + self.sa_prob\n                self.de_prob /= total_prob\n                self.gauss_prob /= total_prob\n                self.sa_prob /= total_prob\n            \n            self.current_temp *= self.cooling_rate # Cool down the temperature\n\n        return self.best_fitness, self.best_x\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget_used += self.pop_size\n        self.best_index = np.argmin(self.fitness)\n        self.best_x = self.population[self.best_index].copy()\n        self.best_fitness = self.fitness[self.best_index].copy()\n        self.de_prob = 0.33\n        self.gauss_prob = 0.33\n        self.sa_prob = 0.34 # SA Probability\n        \n    def reinitialize_population(self, func):\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget_used += self.pop_size\n        self.best_index = np.argmin(self.fitness)\n        self.best_x = self.population[self.best_index].copy()\n        self.best_fitness = self.fitness[self.best_index].copy()\n        self.current_temp = self.initial_temp # Reset temp after restart\n\n    def adjust_probabilities(self):\n        self.de_prob = (self.de_success / (self.de_count + self.epsilon)) if self.de_count > 0 else 0.33\n        self.gauss_prob = (self.gauss_success / (self.gauss_count + self.epsilon)) if self.gauss_count > 0 else 0.33\n        self.sa_prob = (self.sa_success / (self.sa_count + self.epsilon)) if self.sa_count > 0 else 0.34 # SA Probability\n\n        total_prob = self.de_prob + self.gauss_prob + self.sa_prob\n        self.de_prob /= total_prob\n        self.gauss_prob /= total_prob\n        self.sa_prob /= total_prob\n\n    def differential_evolution(self, func, i):\n        self.de_count += 1\n        indices = np.random.choice(self.pop_size, 3, replace=False)\n        x_r1, x_r2, x_r3 = self.population[indices]\n        mutation = x_r1 + 0.5 * (x_r2 - x_r3)\n        crossover = np.random.rand(self.dim) < 0.5\n        new_x = np.where(crossover, mutation, self.population[i])\n        new_x = np.clip(new_x, self.lb, self.ub)\n        new_fitness = func(new_x)\n        self.budget_used += 1\n\n        if new_fitness < self.fitness[i]:\n            self.de_success += 1\n            self.population[i] = new_x\n            self.fitness[i] = new_fitness\n\n    def gaussian_local_search(self, func, i):\n        self.gauss_count += 1\n        step_size = 0.1 * (self.ub - self.lb)\n        new_x = self.population[i] + step_size * np.random.randn(self.dim)\n        new_x = np.clip(new_x, self.lb, self.ub)\n        new_fitness = func(new_x)\n        self.budget_used += 1\n\n        if new_fitness < self.fitness[i]:\n            self.gauss_success += 1\n            self.population[i] = new_x\n            self.fitness[i] = new_fitness\n\n    def simulated_annealing_move(self, func, i):\n        self.sa_count += 1\n        step_size = 0.1 * (self.ub - self.lb)\n        new_x = self.population[i] + step_size * np.random.randn(self.dim)\n        new_x = np.clip(new_x, self.lb, self.ub)\n        new_fitness = func(new_x)\n        self.budget_used += 1\n\n        delta_e = new_fitness - self.fitness[i]\n\n        if delta_e < 0:\n            self.sa_success += 1\n            self.population[i] = new_x\n            self.fitness[i] = new_fitness\n        else:\n            # Acceptance probability based on Metropolis criterion\n            acceptance_probability = np.exp(-delta_e / self.current_temp)\n            if np.random.rand() < acceptance_probability:\n                self.population[i] = new_x\n                self.fitness[i] = new_fitness\n            # No success update if rejected\n            \n    def population_diversity(self):\n        centroid = np.mean(self.population, axis=0)\n        distances = np.linalg.norm(self.population - centroid, axis=1)\n        mean_distance = np.mean(distances)\n        return mean_distance", "configspace": "", "generation": 5, "feedback": "Evaluation timed out after 300 seconds.", "error": "Evaluation timed out after 300 seconds.", "parent_ids": ["64262cfc-dcb7-40b2-87b5-c25943b36beb"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "5f359b1c-9188-4736-bf69-3975597a430a", "fitness": "-inf", "name": "AdaptiveHybridOptimizer", "description": "An adaptive hybrid optimizer that dynamically adjusts algorithm probabilities based on success, incorporates a velocity clamping mechanism for PSO, and employs a CMA-ES based mutation for enhanced exploration.", "code": "import numpy as np\nfrom scipy.linalg import sqrtm\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget=10000, dim=10, pop_size=20, \n                 de_rate=0.4, pso_rate=0.3, cma_rate=0.2, local_rate=0.1,\n                 restart_trigger=0.05, pso_clamp=2.0, cma_sigma=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.de_rate = de_rate\n        self.pso_rate = pso_rate\n        self.cma_rate = cma_rate\n        self.local_rate = local_rate\n        self.restart_trigger = restart_trigger\n        self.pso_clamp = pso_clamp  # Clamp velocity in PSO\n        self.cma_sigma = cma_sigma  # Initial CMA-ES step size\n\n        # Adaptive probabilities initialization\n        self.algo_probs = np.array([de_rate, pso_rate, cma_rate, local_rate])\n        self.algo_probs /= np.sum(self.algo_probs) # Normalize\n        self.success_history = np.zeros(4) # DE, PSO, CMA, Local\n        self.learning_rate = 0.1  # How quickly probabilities adapt\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget_used = self.pop_size\n\n        self.pbest_population = self.population.copy()\n        self.pbest_fitness = self.fitness.copy()\n\n        self.gbest_index = np.argmin(self.fitness)\n        self.gbest_x = self.population[self.gbest_index].copy()\n        self.gbest_fitness = self.fitness[self.gbest_index].copy()\n        \n        self.f_opt = self.gbest_fitness\n        self.x_opt = self.gbest_x\n\n        self.no_improvement_count = 0\n        self.max_no_improvement = int(self.budget * self.restart_trigger)\n        \n        # CMA-ES parameters initialization\n        self.C = np.eye(self.dim)  # Covariance matrix\n        self.D = np.eye(self.dim)\n        self.B = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)  # Evolution path for covariance\n        self.ps = np.zeros(self.dim)  # Evolution path for step size\n        self.c_sigma = 1/np.sqrt(self.dim)\n        self.d_sigma = 1 + 2*max(0, np.sqrt((self.dim-1)/(self.dim+1)) - 1)\n        self.mu = self.pop_size // 2\n        self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n        self.weights /= np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n\n        while self.budget_used < self.budget:\n            new_population = np.zeros_like(self.population)\n            new_fitness = np.zeros(self.pop_size)\n            algo_success = np.zeros(4) # Success indicators for this generation\n\n            for i in range(self.pop_size):\n                # Select algorithm based on adaptive probabilities\n                algo_choice = np.random.choice(4, p=self.algo_probs)\n\n                if algo_choice == 0: # Differential Evolution\n                    indices = np.random.choice(self.pop_size, 3, replace=False)\n                    x_r1, x_r2, x_r3 = self.population[indices]\n                    mutation = x_r1 + 0.5 * (x_r2 - x_r3)\n                    crossover = np.random.rand(self.dim) < 0.5\n                    new_population[i] = np.where(crossover, mutation, self.population[i])\n                    \n                elif algo_choice == 1: # Particle Swarm Optimization\n                    inertia = 0.7\n                    cognitive_coeff = 1.5\n                    social_coeff = 1.5\n                    velocity = inertia * (self.population[i] - self.pbest_population[i]) + cognitive_coeff * np.random.rand(self.dim) * (self.pbest_population[i] - self.population[i]) + social_coeff * np.random.rand(self.dim) * (self.gbest_x - self.population[i])\n                    \n                    # Velocity clamping\n                    velocity = np.clip(velocity, -self.pso_clamp, self.pso_clamp)\n                    new_population[i] = self.population[i] + velocity\n\n                elif algo_choice == 2: # CMA-ES Mutation\n                    z = np.random.randn(self.dim)\n                    new_population[i] = self.gbest_x + self.cma_sigma * (self.B @ (self.D @ z))\n\n                else: # Local Search\n                    new_population[i] = self.population[i] + np.random.normal(0, 0.1, self.dim)\n\n                new_population[i] = np.clip(new_population[i], func.bounds.lb, func.bounds.ub)\n                new_fitness[i] = func(new_population[i])\n                self.budget_used += 1\n\n                if new_fitness[i] < self.fitness[i]:\n                    algo_success[algo_choice] = 1 # Mark algorithm as successful\n                    self.population[i] = new_population[i]\n                    self.fitness[i] = new_fitness[i]\n\n                    if new_fitness[i] < self.pbest_fitness[i]:\n                        self.pbest_population[i] = new_population[i]\n                        self.pbest_fitness[i] = new_fitness[i]\n\n            # Update algorithm probabilities based on success\n            self.success_history = 0.9 * self.success_history + 0.1 * algo_success\n            self.algo_probs = (1 - self.learning_rate) * self.algo_probs + self.learning_rate * self.success_history / np.sum(self.success_history)\n            self.algo_probs /= np.sum(self.algo_probs)\n\n            self.gbest_index = np.argmin(self.fitness)\n            if self.fitness[self.gbest_index] < self.gbest_fitness:\n                self.gbest_x = self.population[self.gbest_index].copy()\n                self.gbest_fitness = self.fitness[self.gbest_index].copy()\n                \n                if self.gbest_fitness < self.f_opt:\n                    self.f_opt = self.gbest_fitness\n                    self.x_opt = self.gbest_x\n                    self.no_improvement_count = 0\n                else:\n                    self.no_improvement_count += 1\n            else:\n                self.no_improvement_count += 1\n\n            # Restart mechanism\n            if self.no_improvement_count > self.max_no_improvement:\n                self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                self.fitness = np.array([func(x) for x in self.population])\n\n                self.pbest_population = self.population.copy()\n                self.pbest_fitness = self.fitness.copy()\n\n                self.gbest_index = np.argmin(self.fitness)\n                self.gbest_x = self.population[self.gbest_index].copy()\n                self.gbest_fitness = self.fitness[self.gbest_index].copy()\n                \n                self.no_improvement_count = 0\n                self.C = np.eye(self.dim)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "Evaluation timed out after 300 seconds.", "error": "Evaluation timed out after 300 seconds.", "parent_ids": ["d0fddf51-e6ed-4acb-82bd-f12a4f7ace8d"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "7e13b121-2ba7-4a0a-b954-c17b4e745575", "fitness": 0.4722221578780397, "name": "SimplifiedHybridOptimizer", "description": "A simplified hybrid optimizer using only differential evolution and random search with adaptive mutation scaling and a dynamically adjusted restart mechanism.", "code": "import numpy as np\n\nclass SimplifiedHybridOptimizer:\n    def __init__(self, budget=10000, dim=10, pop_size=20, de_rate=0.7, restart_trigger=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.de_rate = de_rate\n        self.restart_trigger = restart_trigger\n        self.mutation_scaling = 0.5 # Initial mutation scaling\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget_used = self.pop_size\n        \n        self.best_index = np.argmin(self.fitness)\n        self.best_x = self.population[self.best_index].copy()\n        self.best_fitness = self.fitness[self.best_index].copy()\n\n        self.f_opt = self.best_fitness\n        self.x_opt = self.best_x\n\n        self.no_improvement_count = 0\n        self.max_no_improvement = int(self.budget * self.restart_trigger)\n\n        while self.budget_used < self.budget:\n            new_population = np.zeros_like(self.population)\n            new_fitness = np.zeros(self.pop_size)\n            \n            for i in range(self.pop_size):\n                if np.random.rand() < self.de_rate:\n                    # Differential Evolution\n                    indices = np.random.choice(self.pop_size, 3, replace=False)\n                    x_r1, x_r2, x_r3 = self.population[indices]\n                    mutation = x_r1 + self.mutation_scaling * (x_r2 - x_r3) # Adaptive mutation\n                    crossover = np.random.rand(self.dim) < 0.5\n                    new_population[i] = np.where(crossover, mutation, self.population[i])\n                else:\n                    # Random Exploration\n                    new_population[i] = np.random.uniform(func.bounds.lb, func.bounds.ub, self.dim)\n                    \n                # Clip the solution\n                new_population[i] = np.clip(new_population[i], func.bounds.lb, func.bounds.ub)\n                new_fitness[i] = func(new_population[i])\n                self.budget_used += 1\n\n            for i in range(self.pop_size):\n                if new_fitness[i] < self.fitness[i]:\n                    self.population[i] = new_population[i]\n                    self.fitness[i] = new_fitness[i]\n\n            current_best_index = np.argmin(self.fitness)\n            if self.fitness[current_best_index] < self.best_fitness:\n                self.best_x = self.population[current_best_index].copy()\n                self.best_fitness = self.fitness[current_best_index].copy()\n                \n                if self.best_fitness < self.f_opt:\n                    self.f_opt = self.best_fitness\n                    self.x_opt = self.best_x\n                    self.no_improvement_count = 0\n                    self.mutation_scaling = min(self.mutation_scaling * 1.1, 1.0) # Increase exploration\n                else:\n                    self.no_improvement_count += 1\n                    self.mutation_scaling = max(self.mutation_scaling * 0.9, 0.1) # Decrease exploration\n            else:\n                self.no_improvement_count += 1\n                self.mutation_scaling = max(self.mutation_scaling * 0.9, 0.1)  # Decrease exploration\n\n            # Restart mechanism\n            if self.no_improvement_count > self.max_no_improvement:\n                self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                self.fitness = np.array([func(x) for x in self.population])\n                self.budget_used += self.pop_size # account for the fitness evaluation.\n                \n                self.best_index = np.argmin(self.fitness)\n                self.best_x = self.population[self.best_index].copy()\n                self.best_fitness = self.fitness[self.best_index].copy()\n                \n                self.no_improvement_count = 0\n                self.mutation_scaling = 0.5 # Reset mutation scaling\n\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "The algorithm SimplifiedHybridOptimizer scored 0.472 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["d0fddf51-e6ed-4acb-82bd-f12a4f7ace8d"], "operator": null, "metadata": {"aucs": [0.3092269184905033, 0.40437600083434255, 0.31497106852808954, 0.543900010160267, 0.48623945283445824, 0.5889018211511761, 0.49129552041500424, 0.35078748017602657, 0.34780006595146806, 0.4354625375151022, 0.35887940119808626, 0.9963322969464289, 0.31926305598727256, 0.611638534099923, 0.720335484545, 0.3600923091583045, 0.4043640985160153, 0.527949316475442, 0.33552794903032535, 0.537099835547558]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "f8439448-7e56-4d77-abf3-c4aac0965c83", "fitness": "-inf", "name": "SelfAdjustingOptimizer", "description": "A self-adjusting hybrid optimizer combining differential evolution, Gaussian local search, and Cauchy mutation with dynamic probability adjustments, adaptive step sizes, and a toroidal boundary handling mechanism to enhance exploration and exploitation, and a mirrored sampling strategy to improve diversity.", "code": "import numpy as np\n\nclass SelfAdjustingOptimizer:\n    def __init__(self, budget=10000, dim=10, pop_size=20):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lb = -5.0\n        self.ub = 5.0\n        self.population = None\n        self.fitness = None\n        self.best_x = None\n        self.best_fitness = np.inf\n        self.budget_used = 0\n\n        self.de_success = 0\n        self.gauss_success = 0\n        self.cauchy_success = 0\n        self.de_count = 0\n        self.gauss_count = 0\n        self.cauchy_count = 0\n        self.epsilon = 1e-6\n        self.diversity_threshold = 0.01\n        self.stagnation_counter = 0\n        self.stagnation_threshold = 50\n        self.step_size_de = 0.5\n        self.step_size_gauss = 0.1\n        self.step_size_cauchy = 0.05\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        \n        while self.budget_used < self.budget:\n            old_best_fitness = self.best_fitness\n            self.adjust_probabilities()\n\n            for i in range(self.pop_size):\n                rand = np.random.rand()\n\n                if rand < self.de_prob:\n                    self.differential_evolution(func, i)\n                elif rand < self.de_prob + self.gauss_prob:\n                    self.gaussian_local_search(func, i)\n                else:\n                    self.cauchy_mutation(func, i)\n\n                if self.fitness[i] < self.best_fitness:\n                    self.best_fitness = self.fitness[i]\n                    self.best_x = self.population[i].copy()\n\n            if self.population_diversity() < self.diversity_threshold:\n                self.reinitialize_population(func)\n                self.stagnation_counter = 0\n\n            if self.best_fitness >= old_best_fitness:\n                self.stagnation_counter += 1\n            else:\n                self.stagnation_counter = 0\n\n            if self.stagnation_counter > self.stagnation_threshold:\n                self.reinitialize_population(func)\n                self.stagnation_counter = 0\n                self.de_prob *= 0.9\n                self.gauss_prob *= 0.9\n                self.cauchy_prob *= 0.9\n                total_prob = self.de_prob + self.gauss_prob + self.cauchy_prob\n                self.de_prob /= total_prob\n                self.gauss_prob /= total_prob\n                self.cauchy_prob /= total_prob\n\n            # Adaptive step size adjustment based on success\n            if self.de_count > 10:\n                self.step_size_de *= (1 + 0.1 * (self.de_success / self.de_count - 0.5))\n                self.step_size_de = np.clip(self.step_size_de, 0.1, 1.0)\n                self.de_success = 0\n                self.de_count = 0\n\n            if self.gauss_count > 10:\n                self.step_size_gauss *= (1 + 0.1 * (self.gauss_success / self.gauss_count - 0.5))\n                self.step_size_gauss = np.clip(self.step_size_gauss, 0.01, 0.5)\n                self.gauss_success = 0\n                self.gauss_count = 0\n\n            if self.cauchy_count > 10:\n                self.step_size_cauchy *= (1 + 0.1 * (self.cauchy_success / self.cauchy_count - 0.5))\n                self.step_size_cauchy = np.clip(self.step_size_cauchy, 0.005, 0.2)\n                self.cauchy_success = 0\n                self.cauchy_count = 0\n\n        return self.best_fitness, self.best_x\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget_used += self.pop_size\n        self.best_index = np.argmin(self.fitness)\n        self.best_x = self.population[self.best_index].copy()\n        self.best_fitness = self.fitness[self.best_index].copy()\n        self.de_prob = 0.33\n        self.gauss_prob = 0.33\n        self.cauchy_prob = 0.34\n\n    def reinitialize_population(self, func):\n        # Mirrored Sampling\n        half_pop_size = self.pop_size // 2\n        self.population[:half_pop_size] = np.random.uniform(self.lb, self.ub, size=(half_pop_size, self.dim))\n        self.population[half_pop_size:] = self.lb + self.ub - self.population[:half_pop_size]\n\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget_used += self.pop_size\n        self.best_index = np.argmin(self.fitness)\n        self.best_x = self.population[self.best_index].copy()\n        self.best_fitness = self.fitness[self.best_index].copy()\n\n    def adjust_probabilities(self):\n        self.de_prob = (self.de_success / (self.de_count + self.epsilon)) if self.de_count > 0 else 0.33\n        self.gauss_prob = (self.gauss_success / (self.gauss_count + self.epsilon)) if self.gauss_count > 0 else 0.33\n        self.cauchy_prob = (self.cauchy_success / (self.cauchy_count + self.epsilon)) if self.cauchy_count > 0 else 0.34\n\n        total_prob = self.de_prob + self.gauss_prob + self.cauchy_prob\n        self.de_prob /= total_prob\n        self.gauss_prob /= total_prob\n        self.cauchy_prob /= total_prob\n\n    def differential_evolution(self, func, i):\n        self.de_count += 1\n        indices = np.random.choice(self.pop_size, 3, replace=False)\n        x_r1, x_r2, x_r3 = self.population[indices]\n        mutation = x_r1 + self.step_size_de * (x_r2 - x_r3)\n        crossover = np.random.rand(self.dim) < 0.5\n        new_x = np.where(crossover, mutation, self.population[i])\n        new_x = self.toroidal_boundary_handling(new_x)\n\n        new_fitness = func(new_x)\n        self.budget_used += 1\n\n        if new_fitness < self.fitness[i]:\n            self.de_success += 1\n            self.population[i] = new_x\n            self.fitness[i] = new_fitness\n\n    def gaussian_local_search(self, func, i):\n        self.gauss_count += 1\n        new_x = self.population[i] + self.step_size_gauss * (self.ub - self.lb) * np.random.randn(self.dim)\n        new_x = self.toroidal_boundary_handling(new_x)\n        new_fitness = func(new_x)\n        self.budget_used += 1\n\n        if new_fitness < self.fitness[i]:\n            self.gauss_success += 1\n            self.population[i] = new_x\n            self.fitness[i] = new_fitness\n\n    def cauchy_mutation(self, func, i):\n        self.cauchy_count += 1\n        new_x = self.population[i] + self.step_size_cauchy * (self.ub - self.lb) * np.random.standard_cauchy(size=self.dim)\n        new_x = self.toroidal_boundary_handling(new_x)\n        new_fitness = func(new_x)\n        self.budget_used += 1\n\n        if new_fitness < self.fitness[i]:\n            self.cauchy_success += 1\n            self.population[i] = new_x\n            self.fitness[i] = new_fitness\n\n    def toroidal_boundary_handling(self, x):\n        x = np.where(x < self.lb, self.ub - (self.lb - x) % (self.ub - self.lb), x)\n        x = np.where(x > self.ub, self.lb + (x - self.ub) % (self.ub - self.lb), x)\n        return x\n\n    def population_diversity(self):\n        centroid = np.mean(self.population, axis=0)\n        distances = np.linalg.norm(self.population - centroid, axis=1)\n        mean_distance = np.mean(distances)\n        return mean_distance", "configspace": "", "generation": 5, "feedback": "Evaluation timed out after 300 seconds.", "error": "Evaluation timed out after 300 seconds.", "parent_ids": ["64262cfc-dcb7-40b2-87b5-c25943b36beb"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "fd18fc76-5934-46b2-b568-c0ca82e79647", "fitness": "-inf", "name": "AdaptiveHybridOptimizer", "description": "A self-adjusting hybrid optimizer with simplified CMA-ES, DE and a restart mechanism based on stagnation detection.", "code": "import numpy as np\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget=10000, dim=10, pop_size=20, archive_size=10, stagnation_threshold=1000):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lb = -5.0\n        self.ub = 5.0\n        self.archive_size = archive_size\n        self.archive = []\n        self.archive_fitness = []\n        self.stagnation_threshold = stagnation_threshold\n\n        self.de_success = 0\n        self.cmaes_success = 0\n        self.local_search_success = 0\n        self.de_count = 0\n        self.cmaes_count = 0\n        self.local_search_count = 0\n        self.epsilon = 1e-6\n        self.last_improvement = 0\n        self.restart_count = 0\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget_used = self.pop_size\n\n        self.best_index = np.argmin(self.fitness)\n        self.best_x = self.population[self.best_index].copy()\n        self.best_fitness = self.fitness[self.best_index].copy()\n\n        self.f_opt = self.best_fitness\n        self.x_opt = self.best_x\n\n        self.add_to_archive(self.best_x, self.best_fitness)\n        self.last_improvement = 0\n        \n        while self.budget_used < self.budget:\n            \n            if self.budget_used - self.last_improvement > self.stagnation_threshold:\n                # Restart mechanism\n                self.restart()\n\n            de_prob = (self.de_success / (self.de_count + self.epsilon)) if self.de_count > 0 else 0.33\n            cmaes_prob = (self.cmaes_success / (self.cmaes_count + self.epsilon)) if self.cmaes_count > 0 else 0.33\n            local_search_prob = (self.local_search_success / (self.local_search_count + self.epsilon)) if self.local_search_count > 0 else 0.34\n            \n            # Normalize probabilities\n            total_prob = de_prob + cmaes_prob + local_search_prob\n            de_prob /= total_prob\n            cmaes_prob /= total_prob\n            local_search_prob /= total_prob\n            \n            i = self.budget_used % self.pop_size\n            rand = np.random.rand()\n                \n            if rand < de_prob:\n                # Differential Evolution\n                self.de_count += 1\n                indices = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = self.population[indices]\n                mutation = x_r1 + 0.5 * (x_r2 - x_r3)\n                crossover = np.random.rand(self.dim) < 0.5\n                new_x = np.where(crossover, mutation, self.population[i])\n                new_x = np.clip(new_x, self.lb, self.ub)\n                new_fitness = func(new_x)\n                self.budget_used += 1\n\n                if new_fitness < self.fitness[i]:\n                    self.de_success += 1\n                    self.population[i] = new_x\n                    self.fitness[i] = new_fitness\n                    self.add_to_archive(new_x, new_fitness)\n                    if new_fitness < self.best_fitness:\n                        self.best_fitness = new_fitness\n                        self.best_x = new_x\n                        self.last_improvement = self.budget_used\n                        if self.best_fitness < self.f_opt:\n                            self.f_opt = self.best_fitness\n                            self.x_opt = self.best_x\n\n            elif rand < de_prob + cmaes_prob:\n                # CMA-ES (Simplified)\n                self.cmaes_count += 1\n                step_size = 0.1\n                new_x = self.population[i] + step_size * np.random.randn(self.dim)\n                new_x = np.clip(new_x, self.lb, self.ub)\n                new_fitness = func(new_x)\n                self.budget_used += 1\n\n                if new_fitness < self.fitness[i]:\n                    self.cmaes_success += 1\n                    self.population[i] = new_x\n                    self.fitness[i] = new_fitness\n                    self.add_to_archive(new_x, new_fitness)\n\n                    if new_fitness < self.best_fitness:\n                        self.best_fitness = new_fitness\n                        self.best_x = new_x\n                        self.last_improvement = self.budget_used\n                        if self.best_fitness < self.f_opt:\n                            self.f_opt = self.best_fitness\n                            self.x_opt = self.best_x\n\n            else:\n                # Local Search\n                self.local_search_count += 1\n                step_size = 0.05\n                new_x = self.population[i] + step_size * np.random.randn(self.dim)\n                new_x = np.clip(new_x, self.lb, self.ub)\n                new_fitness = func(new_x)\n                self.budget_used += 1\n\n                if new_fitness < self.fitness[i]:\n                    self.local_search_success += 1\n                    self.population[i] = new_x\n                    self.fitness[i] = new_fitness\n                    self.add_to_archive(new_x, new_fitness)\n\n                    if new_fitness < self.best_fitness:\n                        self.best_fitness = new_fitness\n                        self.best_x = new_x\n                        self.last_improvement = self.budget_used\n                        if self.best_fitness < self.f_opt:\n                            self.f_opt = self.best_fitness\n                            self.x_opt = self.best_x\n                \n        return self.f_opt, self.x_opt\n\n    def add_to_archive(self, x, fitness):\n        if len(self.archive) < self.archive_size:\n            self.archive.append(x)\n            self.archive_fitness.append(fitness)\n        else:\n            max_fitness_index = np.argmax(self.archive_fitness)\n            if fitness < self.archive_fitness[max_fitness_index]:\n                self.archive[max_fitness_index] = x\n                self.archive_fitness[max_fitness_index] = fitness\n\n    def restart(self):\n        self.restart_count += 1\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget_used += self.pop_size\n\n        self.best_index = np.argmin(self.fitness)\n        self.best_x = self.population[self.best_index].copy()\n        self.best_fitness = self.fitness[self.best_index].copy()\n\n        if self.best_fitness < self.f_opt:\n             self.f_opt = self.best_fitness\n             self.x_opt = self.best_x\n\n        self.add_to_archive(self.best_x, self.best_fitness)\n        self.last_improvement = self.budget_used\n\n        self.de_success = 0\n        self.cmaes_success = 0\n        self.local_search_success = 0\n        self.de_count = 0\n        self.cmaes_count = 0\n        self.local_search_count = 0", "configspace": "", "generation": 5, "feedback": "In the code, line 147, in restart, the following error occurred:\nNameError: name 'func' is not defined\nOn line: self.fitness = np.array([func(x) for x in self.population])", "error": "In the code, line 147, in restart, the following error occurred:\nNameError: name 'func' is not defined\nOn line: self.fitness = np.array([func(x) for x in self.population])", "parent_ids": ["0eceaf09-69d5-4b47-93e6-80e157e7b18f"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "19d50bba-679d-4ab1-b298-70092f3d3e8b", "fitness": "-inf", "name": "AdaptiveNichingDE", "description": "An adaptive population-based algorithm that blends differential evolution with a niching strategy and a self-tuning mutation scale to maintain diversity and focus on promising regions.", "code": "import numpy as np\n\nclass AdaptiveNichingDE:\n    def __init__(self, budget=10000, dim=10, pop_size=20, num_niches=5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lb = -5.0\n        self.ub = 5.0\n        self.num_niches = num_niches\n        self.mutation_scale = 0.5\n        self.mutation_scale_decay = 0.995\n        self.crossover_rate = 0.7\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget_used = self.pop_size\n\n        self.best_index = np.argmin(self.fitness)\n        self.best_x = self.population[self.best_index].copy()\n        self.best_fitness = self.fitness[self.best_index].copy()\n\n        self.f_opt = self.best_fitness\n        self.x_opt = self.best_x\n\n        # Initialize niches\n        self.niches = np.random.uniform(self.lb, self.ub, size=(self.num_niches, self.dim))\n        self.niche_fitness = np.array([func(x) for x in self.niches])\n        self.budget_used += self.num_niches\n        \n\n        while self.budget_used < self.budget:\n            new_population = np.zeros_like(self.population)\n            new_fitness = np.zeros_like(self.fitness)\n\n            for i in range(self.pop_size):\n                # Niche selection: assign each individual to its closest niche\n                distances = np.linalg.norm(self.niches - self.population[i], axis=1)\n                niche_index = np.argmin(distances)\n\n                # Differential Evolution within the niche\n                indices = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = self.population[indices]\n                \n                # Mutation with self-tuning scale\n                mutation = x_r1 + self.mutation_scale * (x_r2 - x_r3)\n                mutation = np.clip(mutation, self.lb, self.ub)\n\n\n                # Crossover\n                crossover = np.random.rand(self.dim) < self.crossover_rate\n                new_individual = np.where(crossover, mutation, self.population[i])\n                new_individual = np.clip(new_individual, self.lb, self.ub)\n\n\n                new_fitness_val = func(new_individual)\n                self.budget_used += 1\n\n                if new_fitness_val < self.fitness[i]:\n                    new_population[i] = new_individual\n                    new_fitness[i] = new_fitness_val\n                else:\n                    new_population[i] = self.population[i]\n                    new_fitness[i] = self.fitness[i]\n\n            # Update population\n            self.population = new_population\n            self.fitness = new_fitness\n\n            # Update best solution\n            best_index = np.argmin(self.fitness)\n            if self.fitness[best_index] < self.best_fitness:\n                self.best_x = self.population[best_index].copy()\n                self.best_fitness = self.fitness[best_index].copy()\n\n                if self.best_fitness < self.f_opt:\n                    self.f_opt = self.best_fitness\n                    self.x_opt = self.best_x\n            \n            #Update niches\n            for j in range(self.num_niches):\n                niche_members = np.where(np.argmin(np.linalg.norm(self.niches - self.population, axis=2), axis=1) == j)[0]\n                if len(niche_members) > 0:\n                    best_member_index = np.argmin(self.fitness[niche_members])\n                    best_niche_member = self.population[niche_members[best_member_index]]\n                    self.niches[j] = best_niche_member\n                    self.niche_fitness[j] = self.fitness[niche_members[best_member_index]]\n\n            # Decay mutation scale\n            self.mutation_scale *= self.mutation_scale_decay\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "In the code, line 85, in __call__, the following error occurred:\nValueError: operands could not be broadcast together with shapes (5,2) (20,2) \nOn line: niche_members = np.where(np.argmin(np.linalg.norm(self.niches - self.population, axis=2), axis=1) == j)[0]", "error": "In the code, line 85, in __call__, the following error occurred:\nValueError: operands could not be broadcast together with shapes (5,2) (20,2) \nOn line: niche_members = np.where(np.argmin(np.linalg.norm(self.niches - self.population, axis=2), axis=1) == j)[0]", "parent_ids": ["e16e90d9-ba5f-43ec-b171-d0521e5848f9"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "693f8b75-0c04-4b8b-96e4-c1e581eae30d", "fitness": "-inf", "name": "AdaptiveHybridOptimizer", "description": "A simplified hybrid optimizer that dynamically adjusts its exploration-exploitation balance using differential evolution and a shrinking random search, focusing on intensifying the search around promising regions.", "code": "import numpy as np\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget=10000, dim=10, pop_size=20):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lb = -5.0\n        self.ub = 5.0\n        self.epsilon = 1e-6\n        self.exploration_prob = 0.5  # Initial exploration probability\n        self.exploration_decay = 0.995 # Decay factor for exploration probability\n\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget_used = self.pop_size\n\n        self.best_index = np.argmin(self.fitness)\n        self.best_x = self.population[self.best_index].copy()\n        self.best_fitness = self.fitness[self.best_index].copy()\n\n        self.f_opt = self.best_fitness\n        self.x_opt = self.best_x\n        \n        while self.budget_used < self.budget:\n            for i in range(self.pop_size):\n                if np.random.rand() < self.exploration_prob:\n                    # Shrinking Random Search\n                    radius = (self.ub - self.lb) * (1 - self.budget_used / self.budget)\n                    new_x = self.best_x + np.random.uniform(-radius, radius, size=self.dim)\n                    new_x = np.clip(new_x, self.lb, self.ub)\n\n                else:\n                    # Differential Evolution\n                    indices = np.random.choice(self.pop_size, 3, replace=False)\n                    x_r1, x_r2, x_r3 = self.population[indices]\n                    mutation = x_r1 + 0.5 * (x_r2 - x_r3)\n                    crossover = np.random.rand(self.dim) < 0.5\n                    new_x = np.where(crossover, mutation, self.population[i])\n                    new_x = np.clip(new_x, self.lb, self.ub)\n\n                new_fitness = func(new_x)\n                self.budget_used += 1\n\n                if new_fitness < self.fitness[i]:\n                    self.population[i] = new_x\n                    self.fitness[i] = new_fitness\n                    \n                    if new_fitness < self.best_fitness:\n                        self.best_x = new_x.copy()\n                        self.best_fitness = new_fitness.copy()\n                        if self.best_fitness < self.f_opt:\n                            self.f_opt = self.best_fitness\n                            self.x_opt = self.best_x\n\n            self.exploration_prob *= self.exploration_decay  # Reduce exploration over time\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "In the code, line 55, in __call__, the following error occurred:\nAttributeError: 'float' object has no attribute 'copy'\nOn line: self.best_fitness = new_fitness.copy()", "error": "In the code, line 55, in __call__, the following error occurred:\nAttributeError: 'float' object has no attribute 'copy'\nOn line: self.best_fitness = new_fitness.copy()", "parent_ids": ["0eceaf09-69d5-4b47-93e6-80e157e7b18f"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "64c062a7-7036-486f-9415-41fdca90f79d", "fitness": "-inf", "name": "AgingHybridOptimizer", "description": "A hybrid algorithm using differential evolution, PSO, and a Cauchy mutation based local search with an aging-based population diversity maintenance strategy.", "code": "import numpy as np\n\nclass AgingHybridOptimizer:\n    def __init__(self, budget=10000, dim=10, pop_size=20, de_rate=0.4, pso_rate=0.3, cauchy_rate=0.3, aging_threshold=0.1, restart_trigger=0.05):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.de_rate = de_rate\n        self.pso_rate = pso_rate\n        self.cauchy_rate = cauchy_rate\n        self.aging_threshold = aging_threshold # Percentage of budget after which to start aging\n        self.restart_trigger = restart_trigger\n\n        self.population = None\n        self.fitness = None\n        self.ages = None\n        self.pbest_population = None\n        self.pbest_fitness = None\n        self.gbest_x = None\n        self.gbest_fitness = np.inf\n        self.x_opt = None\n        self.f_opt = np.inf\n        self.budget_used = 0\n        self.no_improvement_count = 0\n        self.max_no_improvement = int(self.budget * self.restart_trigger)\n\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget_used += self.pop_size\n        self.ages = np.zeros(self.pop_size)\n        self.pbest_population = self.population.copy()\n        self.pbest_fitness = self.fitness.copy()\n\n        self.gbest_index = np.argmin(self.fitness)\n        self.gbest_x = self.population[self.gbest_index].copy()\n        self.gbest_fitness = self.fitness[self.gbest_index].copy()\n\n        self.f_opt = self.gbest_fitness\n        self.x_opt = self.gbest_x\n        \n    def apply_differential_evolution(self, func):\n        for i in range(self.pop_size):\n            if np.random.rand() < self.de_rate:\n                indices = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = self.population[indices]\n                mutation = x_r1 + 0.5 * (x_r2 - x_r3)\n                crossover = np.random.rand(self.dim) < 0.5  # binomial crossover\n                new_solution = np.where(crossover, mutation, self.population[i])\n                new_solution = np.clip(new_solution, func.bounds.lb, func.bounds.ub)\n                new_fitness = func(new_solution)\n                self.budget_used += 1\n                self.update_population(i, new_solution, new_fitness)\n\n    def apply_particle_swarm_optimization(self, func):\n        inertia = 0.7\n        cognitive_coeff = 1.5\n        social_coeff = 1.5\n        for i in range(self.pop_size):\n            if np.random.rand() < self.pso_rate:\n                velocity = inertia * (self.population[i] - self.pbest_population[i]) + cognitive_coeff * np.random.rand(self.dim) * (self.pbest_population[i] - self.population[i]) + social_coeff * np.random.rand(self.dim) * (self.gbest_x - self.population[i])\n                new_solution = self.population[i] + velocity\n                new_solution = np.clip(new_solution, func.bounds.lb, func.bounds.ub)\n                new_fitness = func(new_solution)\n                self.budget_used += 1\n                self.update_population(i, new_solution, new_fitness)\n                \n    def apply_cauchy_mutation(self, func):\n        for i in range(self.pop_size):\n            if np.random.rand() < self.cauchy_rate:\n                new_solution = self.population[i] + 0.1 * np.random.standard_cauchy(size=self.dim)\n                new_solution = np.clip(new_solution, func.bounds.lb, func.bounds.ub)\n                new_fitness = func(new_solution)\n                self.budget_used += 1\n                self.update_population(i, new_solution, new_fitness)\n\n    def update_population(self, i, new_solution, new_fitness):\n        if new_fitness < self.fitness[i]:\n            self.population[i] = new_solution\n            self.fitness[i] = new_fitness\n            self.ages[i] = 0  # Reset age\n\n            if new_fitness < self.pbest_fitness[i]:\n                self.pbest_population[i] = new_solution\n                self.pbest_fitness[i] = new_fitness\n\n            if new_fitness < self.gbest_fitness:\n                self.gbest_x = new_solution.copy()\n                self.gbest_fitness = new_fitness\n                if new_fitness < self.f_opt:\n                    self.f_opt = new_fitness\n                    self.x_opt = new_solution\n                    self.no_improvement_count = 0\n                else:\n                    self.no_improvement_count += 1\n        else:\n            self.ages[i] += 1 # Increment age\n\n    def restart_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget_used += self.pop_size\n        self.ages = np.zeros(self.pop_size)\n        self.pbest_population = self.population.copy()\n        self.pbest_fitness = self.fitness.copy()\n\n        self.gbest_index = np.argmin(self.fitness)\n        self.gbest_x = self.population[self.gbest_index].copy()\n        self.gbest_fitness = self.fitness[self.gbest_index].copy()\n        self.no_improvement_count = 0\n        \n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.budget_used < self.budget:\n            self.apply_differential_evolution(func)\n            self.apply_particle_swarm_optimization(func)\n            self.apply_cauchy_mutation(func)\n\n            # Aging mechanism: Replace old individuals with random ones\n            if self.budget_used > self.aging_threshold * self.budget:\n                for i in range(self.pop_size):\n                    if self.ages[i] > 5:  # Threshold for age\n                        new_solution = np.random.uniform(func.bounds.lb, func.bounds.ub, self.dim)\n                        new_fitness = func(new_solution)\n                        self.budget_used += 1\n                        self.population[i] = new_solution\n                        self.fitness[i] = new_fitness\n                        self.ages[i] = 0\n\n                        if new_fitness < self.pbest_fitness[i]:\n                            self.pbest_population[i] = new_solution\n                            self.pbest_fitness[i] = new_fitness\n                        \n                        if new_fitness < self.gbest_fitness:\n                            self.gbest_x = new_solution.copy()\n                            self.gbest_fitness = new_fitness\n                            if new_fitness < self.f_opt:\n                                self.f_opt = new_fitness\n                                self.x_opt = new_solution\n                                self.no_improvement_count = 0\n                            else:\n                                self.no_improvement_count += 1\n            \n            \n            if self.gbest_fitness > self.f_opt:\n                self.f_opt = self.gbest_fitness\n                self.x_opt = self.gbest_x\n\n            # Restart mechanism\n            if self.no_improvement_count > self.max_no_improvement:\n                self.restart_population(func)\n                \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "Evaluation timed out after 300 seconds.", "error": "Evaluation timed out after 300 seconds.", "parent_ids": ["d0fddf51-e6ed-4acb-82bd-f12a4f7ace8d"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "960a69c1-ae7c-40d6-a8a4-e37f14eeb6eb", "fitness": "-inf", "name": "AdaptiveHybridOptimizer", "description": "Combines differential evolution, particle swarm optimization, and a CMA-ES-inspired adaptation of covariance, with adaptive probabilities and a concentrated local search to refine promising solutions.", "code": "import numpy as np\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget=10000, dim=10, pop_size=20, de_rate=0.4, pso_rate=0.3, cma_rate=0.2, local_rate=0.1, restart_trigger=0.05, local_search_radius=0.05):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.de_rate = de_rate\n        self.pso_rate = pso_rate\n        self.cma_rate = cma_rate\n        self.local_rate = local_rate\n        self.restart_trigger = restart_trigger\n        self.local_search_radius = local_search_radius\n\n        self.success_de = 0.0\n        self.success_pso = 0.0\n        self.success_cma = 0.0\n        self.success_local = 0.0\n        self.total_trials = 0.0\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget_used = self.pop_size\n        \n        self.pbest_population = self.population.copy()\n        self.pbest_fitness = self.fitness.copy()\n\n        self.gbest_index = np.argmin(self.fitness)\n        self.gbest_x = self.population[self.gbest_index].copy()\n        self.gbest_fitness = self.fitness[self.gbest_index].copy()\n        \n        self.f_opt = self.gbest_fitness\n        self.x_opt = self.gbest_x\n\n        self.no_improvement_count = 0\n        self.max_no_improvement = int(self.budget * self.restart_trigger)\n\n        self.C = np.eye(self.dim)  # Covariance matrix for CMA-ES-like adaptation\n        self.learning_rate_cma = 0.1\n\n        while self.budget_used < self.budget:\n            new_population = np.zeros_like(self.population)\n            new_fitness = np.zeros_like(self.fitness)\n\n            for i in range(self.pop_size):\n                rand = np.random.rand()\n                de_prob = self.success_de / (self.success_de + self.success_pso + self.success_cma + self.success_local + 1e-6)\n                pso_prob = self.success_pso / (self.success_de + self.success_pso + self.success_cma + self.success_local + 1e-6)\n                cma_prob = self.success_cma / (self.success_de + self.success_pso + self.success_cma + self.success_local + 1e-6)\n                local_prob = self.success_local / (self.success_de + self.success_pso + self.success_cma + self.success_local + 1e-6)\n\n                if rand < de_prob: # Differential Evolution\n                    indices = np.random.choice(self.pop_size, 3, replace=False)\n                    x_r1, x_r2, x_r3 = self.population[indices]\n                    mutation = x_r1 + 0.5 * (x_r2 - x_r3)\n                    crossover = np.random.rand(self.dim) < 0.5  # binomial crossover\n                    new_population[i] = np.where(crossover, mutation, self.population[i])\n                    \n                    new_population[i] = np.clip(new_population[i], func.bounds.lb, func.bounds.ub)\n                    new_fitness[i] = func(new_population[i])\n                    self.budget_used += 1\n                    if new_fitness[i] < self.fitness[i]:\n                        self.success_de += 1\n                    self.total_trials += 1\n\n\n                elif rand < de_prob + pso_prob: # Particle Swarm Optimization\n                    inertia = 0.7\n                    cognitive_coeff = 1.5\n                    social_coeff = 1.5\n                    velocity = inertia * (self.population[i] - self.pbest_population[i]) + cognitive_coeff * np.random.rand(self.dim) * (self.pbest_population[i] - self.population[i]) + social_coeff * np.random.rand(self.dim) * (self.gbest_x - self.population[i])\n                    new_population[i] = self.population[i] + velocity\n                    \n                    new_population[i] = np.clip(new_population[i], func.bounds.lb, func.bounds.ub)\n                    new_fitness[i] = func(new_population[i])\n                    self.budget_used += 1\n\n                    if new_fitness[i] < self.fitness[i]:\n                        self.success_pso += 1\n                    self.total_trials += 1\n                    \n                elif rand < de_prob + pso_prob + cma_prob: # CMA-ES-inspired adaptation\n                    z = np.random.multivariate_normal(np.zeros(self.dim), self.C)\n                    new_population[i] = self.gbest_x + 0.1 * z  # Step size 0.1\n                    new_population[i] = np.clip(new_population[i], func.bounds.lb, func.bounds.ub)\n                    new_fitness[i] = func(new_population[i])\n                    self.budget_used += 1\n\n                    if new_fitness[i] < self.fitness[i]:\n                        self.success_cma += 1\n                        diff = new_population[i] - self.gbest_x\n                        self.C = (1 - self.learning_rate_cma) * self.C + self.learning_rate_cma * np.outer(diff, diff)\n\n                    self.total_trials += 1\n\n\n                else: # Local Search\n                    new_population[i] = self.population[i] + np.random.normal(0, self.local_search_radius, self.dim)\n                    new_population[i] = np.clip(new_population[i], func.bounds.lb, func.bounds.ub)\n                    new_fitness[i] = func(new_population[i])\n                    self.budget_used += 1\n\n                    if new_fitness[i] < self.fitness[i]:\n                        self.success_local += 1\n                    self.total_trials += 1\n\n\n            for i in range(self.pop_size):\n                if new_fitness[i] < self.fitness[i]:\n                    self.population[i] = new_population[i]\n                    self.fitness[i] = new_fitness[i]\n\n                    if new_fitness[i] < self.pbest_fitness[i]:\n                        self.pbest_population[i] = new_population[i]\n                        self.pbest_fitness[i] = new_fitness[i]\n\n\n            self.gbest_index = np.argmin(self.fitness)\n            if self.fitness[self.gbest_index] < self.gbest_fitness:\n                self.gbest_x = self.population[self.gbest_index].copy()\n                self.gbest_fitness = self.fitness[self.gbest_index].copy()\n                \n                if self.gbest_fitness < self.f_opt:\n                    self.f_opt = self.gbest_fitness\n                    self.x_opt = self.gbest_x\n                    self.no_improvement_count = 0\n                else:\n                    self.no_improvement_count += 1\n            else:\n                self.no_improvement_count += 1\n\n\n            # Restart mechanism\n            if self.no_improvement_count > self.max_no_improvement:\n                self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                self.fitness = np.array([func(x) for x in self.population])\n\n                self.pbest_population = self.population.copy()\n                self.pbest_fitness = self.fitness.copy()\n\n                self.gbest_index = np.argmin(self.fitness)\n                self.gbest_x = self.population[self.gbest_index].copy()\n                self.gbest_fitness = self.fitness[self.gbest_index].copy()\n                \n                self.no_improvement_count = 0\n                self.C = np.eye(self.dim)  # Reset Covariance Matrix\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "Evaluation timed out after 300 seconds.", "error": "Evaluation timed out after 300 seconds.", "parent_ids": ["d0fddf51-e6ed-4acb-82bd-f12a4f7ace8d"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "0b425558-6eeb-40ad-bcf3-f291fcad25eb", "fitness": "-inf", "name": "HybridOptimizer", "description": "A hybrid optimizer that blends differential evolution, particle swarm optimization, and a dynamically scaled local search using a success-based adaptation mechanism for component probabilities and a niching strategy to enhance exploration.", "code": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget=10000, dim=10, pop_size=20, de_rate=0.4, pso_rate=0.3, local_rate=0.3, restart_trigger=0.05, local_scale=0.1, niche_radius=0.5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.de_rate = de_rate\n        self.pso_rate = pso_rate\n        self.local_rate = local_rate\n        self.restart_trigger = restart_trigger\n        self.local_scale = local_scale # Initial scale for local search\n        self.niche_radius = niche_radius\n        self.component_success = np.array([de_rate, pso_rate, local_rate])\n        self.component_probs = self.component_success / np.sum(self.component_success)\n\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget_used = self.pop_size\n\n        self.pbest_population = self.population.copy()\n        self.pbest_fitness = self.fitness.copy()\n\n        self.gbest_index = np.argmin(self.fitness)\n        self.gbest_x = self.population[self.gbest_index].copy()\n        self.gbest_fitness = self.fitness[self.gbest_index].copy()\n\n        self.f_opt = self.gbest_fitness\n        self.x_opt = self.gbest_x\n\n        self.no_improvement_count = 0\n        self.max_no_improvement = int(self.budget * self.restart_trigger)\n        self.archive_x = []\n        self.archive_f = []\n\n        while self.budget_used < self.budget:\n            new_population = np.zeros_like(self.population)\n            new_fitness = np.zeros(self.pop_size)\n            component_counts = np.zeros(3) # DE, PSO, Local\n\n            for i in range(self.pop_size):\n                # Select component based on adaptive probabilities\n                component = np.random.choice(3, p=self.component_probs)\n                component_counts[component] += 1\n\n                if component == 0:  # Differential Evolution\n                    indices = np.random.choice(self.pop_size, 3, replace=False)\n                    x_r1, x_r2, x_r3 = self.population[indices]\n                    mutation = x_r1 + 0.5 * (x_r2 - x_r3)\n                    crossover = np.random.rand(self.dim) < 0.5\n                    new_population[i] = np.where(crossover, mutation, self.population[i])\n\n                elif component == 1:  # Particle Swarm Optimization\n                    inertia = 0.7\n                    cognitive_coeff = 1.5\n                    social_coeff = 1.5\n                    velocity = inertia * (self.population[i] - self.pbest_population[i]) + cognitive_coeff * np.random.rand(self.dim) * (self.pbest_population[i] - self.population[i]) + social_coeff * np.random.rand(self.dim) * (self.gbest_x - self.population[i])\n                    new_population[i] = self.population[i] + velocity\n\n                elif component == 2:  # Local Search with adaptive scaling\n                    new_population[i] = self.population[i] + np.random.normal(0, self.local_scale, self.dim)\n\n                # Random exploration - Niching strategy\n                else:\n                    # Explore different regions using niching\n                    valid_sample = False\n                    for _ in range(100):  # Try up to 100 times\n                        candidate = np.random.uniform(func.bounds.lb, func.bounds.ub, self.dim)\n                        too_close = False\n                        for archive_x_i in self.archive_x:\n                            if np.linalg.norm(candidate - archive_x_i) < self.niche_radius:\n                                too_close = True\n                                break\n                        if not too_close:\n                            new_population[i] = candidate\n                            valid_sample = True\n                            break\n                    if not valid_sample:\n                        new_population[i] = np.random.uniform(func.bounds.lb, func.bounds.ub, self.dim)\n\n\n\n                new_population[i] = np.clip(new_population[i], func.bounds.lb, func.bounds.ub)\n                new_fitness[i] = func(new_population[i])\n                self.budget_used += 1\n                if self.budget_used >= self.budget:\n                    break\n\n            # Success-based adaptation and population update\n            success = np.zeros(3)\n            for i in range(self.pop_size):\n                if new_fitness[i] < self.fitness[i]:\n                    self.population[i] = new_population[i]\n                    self.fitness[i] = new_fitness[i]\n\n                    if new_fitness[i] < self.pbest_fitness[i]:\n                        self.pbest_population[i] = new_population[i]\n                        self.pbest_fitness[i] = new_fitness[i]\n\n                    if component_counts[0] > 0 and i < component_counts[0]:\n                        success[0] += 1\n                    elif component_counts[0] > 0 and component_counts[1] > 0 and i >= component_counts[0] and i < (component_counts[0] + component_counts[1]):\n                        success[1] += 1\n                    elif component_counts[0] > 0 and component_counts[1] > 0 and component_counts[2] > 0 and i >= (component_counts[0] + component_counts[1]):\n                        success[2] += 1\n                    elif component_counts[0] == 0 and component_counts[1] > 0 and i < component_counts[1]:\n                        success[1] += 1\n                    elif component_counts[0] == 0 and component_counts[1] == 0 and component_counts[2] > 0:\n                        success[2] += 1\n                    elif component_counts[0] > 0 and component_counts[1] == 0 and i < component_counts[0]:\n                        success[0] += 1\n                    elif component_counts[0] > 0 and component_counts[1] == 0 and component_counts[2] > 0 and i >= component_counts[0]:\n                        success[2] += 1\n                    elif component_counts[0] == 0 and component_counts[1] > 0 and component_counts[2] > 0 and i >= component_counts[1]:\n                        success[2] += 1\n\n\n\n            #Update component probabilities\n            if np.sum(component_counts) > 0:\n\n                success_rates = success / (component_counts + 1e-6) # avoid division by zero\n\n                self.component_success = 0.9 * self.component_success + 0.1 * success_rates\n                self.component_probs = self.component_success / np.sum(self.component_success)\n\n\n            self.gbest_index = np.argmin(self.fitness)\n            if self.fitness[self.gbest_index] < self.gbest_fitness:\n                self.gbest_x = self.population[self.gbest_index].copy()\n                self.gbest_fitness = self.fitness[self.gbest_index].copy()\n\n                if self.gbest_fitness < self.f_opt:\n                    self.f_opt = self.gbest_fitness\n                    self.x_opt = self.gbest_x\n                    self.no_improvement_count = 0\n                    #Archive the best\n                    self.archive_x.append(self.gbest_x)\n                    self.archive_f.append(self.gbest_fitness)\n\n                    #Adaptive local search scale\n                    self.local_scale *= 0.9 #Reduce local search scale if improvement found.\n                else:\n                    self.no_improvement_count += 1\n            else:\n                self.no_improvement_count += 1\n                self.local_scale *= 1.1 #Increase local search scale if no improvement found.\n                self.local_scale = min(self.local_scale, 1.0)\n\n            # Restart mechanism\n            if self.no_improvement_count > self.max_no_improvement:\n                self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                self.fitness = np.array([func(x) for x in self.population])\n\n                self.pbest_population = self.population.copy()\n                self.pbest_fitness = self.fitness.copy()\n\n                self.gbest_index = np.argmin(self.fitness)\n                self.gbest_x = self.population[self.gbest_index].copy()\n                self.gbest_fitness = self.fitness[self.gbest_index].copy()\n\n                self.no_improvement_count = 0\n                self.local_scale = 0.1 # Reset local search scale\n\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "Evaluation timed out after 300 seconds.", "error": "Evaluation timed out after 300 seconds.", "parent_ids": ["d0fddf51-e6ed-4acb-82bd-f12a4f7ace8d"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "0be00307-2ba8-4835-8ba3-7eb6617ea150", "fitness": "-inf", "name": "NovelOptimizer", "description": "A self-adjusting algorithm combining differential evolution, Gaussian local search, and Levy flight mutation with dynamic probability adjustment and a niching mechanism to maintain population diversity.", "code": "import numpy as np\n\nclass NovelOptimizer:\n    def __init__(self, budget=10000, dim=10, pop_size=20, num_niches=5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lb = -5.0\n        self.ub = 5.0\n        self.population = None\n        self.fitness = None\n        self.best_x = None\n        self.best_fitness = np.inf\n        self.budget_used = 0\n\n        self.de_success = 0\n        self.gauss_success = 0\n        self.levy_success = 0\n        self.de_count = 0\n        self.gauss_count = 0\n        self.levy_count = 0\n        self.epsilon = 1e-6\n        self.diversity_threshold = 0.01\n        self.stagnation_counter = 0\n        self.stagnation_threshold = 50\n        self.num_niches = num_niches\n        self.niches = None\n        self.niche_fitness = None\n\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.budget_used < self.budget:\n            old_best_fitness = self.best_fitness\n            self.adjust_probabilities()\n\n            for i in range(self.pop_size):\n                rand = np.random.rand()\n\n                if rand < self.de_prob:\n                    self.differential_evolution(func, i)\n                elif rand < self.de_prob + self.gauss_prob:\n                    self.gaussian_local_search(func, i)\n                else:\n                    self.levy_flight_mutation(func, i)\n\n                if self.fitness[i] < self.best_fitness:\n                    self.best_fitness = self.fitness[i]\n                    self.best_x = self.population[i].copy()\n\n            self.maintain_niches()\n\n            if self.population_diversity() < self.diversity_threshold:\n                self.reinitialize_population(func)\n                self.stagnation_counter = 0\n\n            if self.best_fitness >= old_best_fitness:\n                self.stagnation_counter += 1\n            else:\n                self.stagnation_counter = 0\n\n            if self.stagnation_counter > self.stagnation_threshold:\n                self.reinitialize_population(func)\n                self.stagnation_counter = 0\n                self.de_prob *= 0.9\n                self.gauss_prob *= 0.9\n                self.levy_prob *= 0.9\n                total_prob = self.de_prob + self.gauss_prob + self.levy_prob\n                self.de_prob /= total_prob\n                self.gauss_prob /= total_prob\n                self.levy_prob /= total_prob\n        return self.best_fitness, self.best_x\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget_used += self.pop_size\n        self.best_index = np.argmin(self.fitness)\n        self.best_x = self.population[self.best_index].copy()\n        self.best_fitness = self.fitness[self.best_index].copy()\n        self.de_prob = 0.33\n        self.gauss_prob = 0.33\n        self.levy_prob = 0.34\n        \n        # Initialize niches\n        self.niches = np.random.uniform(self.lb, self.ub, size=(self.num_niches, self.dim))\n        self.niche_fitness = np.array([func(x) for x in self.niches])\n        self.budget_used += self.num_niches\n\n    def reinitialize_population(self, func):\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget_used += self.pop_size\n        self.best_index = np.argmin(self.fitness)\n        self.best_x = self.population[self.best_index].copy()\n        self.best_fitness = self.fitness[self.best_index].copy()\n        \n        #Reinitialize niches\n        self.niches = np.random.uniform(self.lb, self.ub, size=(self.num_niches, self.dim))\n        self.niche_fitness = np.array([func(x) for x in self.niches])\n        self.budget_used += self.num_niches\n\n    def adjust_probabilities(self):\n        self.de_prob = (self.de_success / (self.de_count + self.epsilon)) if self.de_count > 0 else 0.33\n        self.gauss_prob = (self.gauss_success / (self.gauss_count + self.epsilon)) if self.gauss_count > 0 else 0.33\n        self.levy_prob = (self.levy_success / (self.levy_count + self.epsilon)) if self.levy_count > 0 else 0.34\n\n        total_prob = self.de_prob + self.gauss_prob + self.levy_prob\n        self.de_prob /= total_prob\n        self.gauss_prob /= total_prob\n        self.levy_prob /= total_prob\n\n    def differential_evolution(self, func, i):\n        self.de_count += 1\n        indices = np.random.choice(self.pop_size, 3, replace=False)\n        x_r1, x_r2, x_r3 = self.population[indices]\n        mutation = x_r1 + 0.5 * (x_r2 - x_r3)\n        crossover = np.random.rand(self.dim) < 0.5\n        new_x = np.where(crossover, mutation, self.population[i])\n        new_x = np.clip(new_x, self.lb, self.ub)\n        new_fitness = func(new_x)\n        self.budget_used += 1\n\n        if new_fitness < self.fitness[i]:\n            self.de_success += 1\n            self.population[i] = new_x\n            self.fitness[i] = new_fitness\n\n    def gaussian_local_search(self, func, i):\n        self.gauss_count += 1\n        step_size = 0.1 * (self.ub - self.lb)\n        new_x = self.population[i] + step_size * np.random.randn(self.dim)\n        new_x = np.clip(new_x, self.lb, self.ub)\n        new_fitness = func(new_x)\n        self.budget_used += 1\n\n        if new_fitness < self.fitness[i]:\n            self.gauss_success += 1\n            self.population[i] = new_x\n            self.fitness[i] = new_fitness\n\n    def levy_flight_mutation(self, func, i):\n        self.levy_count += 1\n        beta = 1.5\n        sigma = (np.math.gamma(1 + beta) * np.sin(np.pi * beta / 2) / (np.math.gamma((1 + beta) / 2) * beta * (2 ** ((beta - 1) / 2)))) ** (1 / beta)\n        u = np.random.randn(self.dim) * sigma\n        v = np.random.randn(self.dim)\n        step = u / abs(v) ** (1 / beta)\n        step_size = 0.01 * (self.ub - self.lb)  # Adjust step size\n        new_x = self.population[i] + step_size * step\n        new_x = np.clip(new_x, self.lb, self.ub)\n        new_fitness = func(new_x)\n        self.budget_used += 1\n\n        if new_fitness < self.fitness[i]:\n            self.levy_success += 1\n            self.population[i] = new_x\n            self.fitness[i] = new_fitness\n            \n    def population_diversity(self):\n        centroid = np.mean(self.population, axis=0)\n        distances = np.linalg.norm(self.population - centroid, axis=1)\n        mean_distance = np.mean(distances)\n        return mean_distance\n    \n    def maintain_niches(self):\n        # Assign individuals to niches\n        for i in range(self.pop_size):\n            distances = np.linalg.norm(self.niches - self.population[i], axis=1)\n            closest_niche = np.argmin(distances)\n            \n            # Replace niche if individual is better\n            if self.fitness[i] < self.niche_fitness[closest_niche]:\n                self.niches[closest_niche] = self.population[i].copy()\n                self.niche_fitness[closest_niche] = self.fitness[i]", "configspace": "", "generation": 6, "feedback": "In the code, line 805, in __getattr__, the following error occurred:\nAttributeError: module 'numpy' has no attribute 'math'. Did you mean: 'emath'?", "error": "In the code, line 805, in __getattr__, the following error occurred:\nAttributeError: module 'numpy' has no attribute 'math'. Did you mean: 'emath'?", "parent_ids": ["64262cfc-dcb7-40b2-87b5-c25943b36beb"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "3b2853e0-07fa-434c-9e67-7cb0f856c073", "fitness": 0.5115018574384688, "name": "SelfAdjustingOptimizer", "description": "A self-adjusting optimizer combining differential evolution, Gaussian local search, and Cauchy mutation with adaptive probabilities, step sizes, and a diversity-guided restart mechanism.", "code": "import numpy as np\n\nclass SelfAdjustingOptimizer:\n    def __init__(self, budget=10000, dim=10, pop_size=20):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lb = -5.0\n        self.ub = 5.0\n        self.population = None\n        self.fitness = None\n        self.best_x = None\n        self.best_fitness = np.inf\n        self.budget_used = 0\n\n        self.de_success = 0\n        self.gauss_success = 0\n        self.cauchy_success = 0\n        self.de_count = 0\n        self.gauss_count = 0\n        self.cauchy_count = 0\n        self.epsilon = 1e-6\n        self.diversity_threshold = 0.01\n        self.stagnation_counter = 0\n        self.stagnation_threshold = 50\n\n        self.de_prob = 0.33\n        self.gauss_prob = 0.33\n        self.cauchy_prob = 0.34\n\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        \n        while self.budget_used < self.budget:\n            old_best_fitness = self.best_fitness\n            self.adjust_probabilities()\n\n            for i in range(self.pop_size):\n                rand = np.random.rand()\n\n                if rand < self.de_prob:\n                    self.differential_evolution(func, i)\n                elif rand < self.de_prob + self.gauss_prob:\n                    self.gaussian_local_search(func, i)\n                else:\n                    self.cauchy_mutation(func, i)\n\n                if self.fitness[i] < self.best_fitness:\n                    self.best_fitness = self.fitness[i]\n                    self.best_x = self.population[i].copy()\n\n            if self.population_diversity() < self.diversity_threshold:\n                self.reinitialize_population(func)\n                self.stagnation_counter = 0\n\n            if self.best_fitness >= old_best_fitness:\n                self.stagnation_counter += 1\n            else:\n                self.stagnation_counter = 0\n\n            if self.stagnation_counter > self.stagnation_threshold:\n                self.reinitialize_population(func)\n                self.stagnation_counter = 0\n        return self.best_fitness, self.best_x\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget_used += self.pop_size\n        self.best_index = np.argmin(self.fitness)\n        self.best_x = self.population[self.best_index].copy()\n        self.best_fitness = self.fitness[self.best_index].copy()\n        \n    def reinitialize_population(self, func):\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget_used += self.pop_size\n        self.best_index = np.argmin(self.fitness)\n        self.best_x = self.population[self.best_index].copy()\n        self.best_fitness = self.fitness[self.best_index].copy()\n\n    def adjust_probabilities(self):\n        self.de_prob = (self.de_success / (self.de_count + self.epsilon)) if self.de_count > 0 else 0.33\n        self.gauss_prob = (self.gauss_success / (self.gauss_count + self.epsilon)) if self.gauss_count > 0 else 0.33\n        self.cauchy_prob = (self.cauchy_success / (self.cauchy_count + self.epsilon)) if self.cauchy_count > 0 else 0.34\n\n        total_prob = self.de_prob + self.gauss_prob + self.cauchy_prob\n        self.de_prob /= total_prob\n        self.gauss_prob /= total_prob\n        self.cauchy_prob /= total_prob\n\n    def differential_evolution(self, func, i):\n        self.de_count += 1\n        indices = np.random.choice(self.pop_size, 3, replace=False)\n        x_r1, x_r2, x_r3 = self.population[indices]\n        mutation = x_r1 + 0.5 * (x_r2 - x_r3)\n        crossover = np.random.rand(self.dim) < 0.5\n        new_x = np.where(crossover, mutation, self.population[i])\n        new_x = np.clip(new_x, self.lb, self.ub)\n        new_fitness = func(new_x)\n        self.budget_used += 1\n\n        if new_fitness < self.fitness[i]:\n            self.de_success += 1\n            self.population[i] = new_x\n            self.fitness[i] = new_fitness\n\n    def gaussian_local_search(self, func, i):\n        self.gauss_count += 1\n        step_size = 0.1 * (self.ub - self.lb)\n        new_x = self.population[i] + step_size * np.random.randn(self.dim)\n        new_x = np.clip(new_x, self.lb, self.ub)\n        new_fitness = func(new_x)\n        self.budget_used += 1\n\n        if new_fitness < self.fitness[i]:\n            self.gauss_success += 1\n            self.population[i] = new_x\n            self.fitness[i] = new_fitness\n\n    def cauchy_mutation(self, func, i):\n        self.cauchy_count += 1\n        scale = 0.05 * (self.ub - self.lb)\n        new_x = self.population[i] + scale * np.random.standard_cauchy(size=self.dim)\n        new_x = np.clip(new_x, self.lb, self.ub)\n        new_fitness = func(new_x)\n        self.budget_used += 1\n\n        if new_fitness < self.fitness[i]:\n            self.cauchy_success += 1\n            self.population[i] = new_x\n            self.fitness[i] = new_fitness\n            \n    def population_diversity(self):\n        centroid = np.mean(self.population, axis=0)\n        distances = np.linalg.norm(self.population - centroid, axis=1)\n        mean_distance = np.mean(distances)\n        return mean_distance", "configspace": "", "generation": 6, "feedback": "The algorithm SelfAdjustingOptimizer scored 0.512 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["64262cfc-dcb7-40b2-87b5-c25943b36beb"], "operator": null, "metadata": {"aucs": [0.17717105478847572, 0.25098880023505654, 0.32200274119050787, 0.8129843981572781, 0.4873319304295971, 0.5877229132478339, 0.44565182682425986, 0.46572390695827515, 0.4798324238752095, 0.3905887245945482, 0.7576606097588389, 0.997897421112242, 0.2847311093772239, 0.4911980224345549, 0.8328622489641413, 0.6144105470375507, 0.4436687046837535, 0.7009163891311607, 0.19159629330989236, 0.4950970826589738]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "73c3751d-1f04-4007-9bfd-e836120be6e7", "fitness": "-inf", "name": "AdaptiveHybridOptimizer", "description": "A hybrid optimizer employing differential evolution, particle swarm optimization, and a CMA-ES-based local search, with self-adaptive probabilities and parameter control using a success-history based adaptation, and a restart mechanism.", "code": "import numpy as np\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget=10000, dim=10, pop_size=20, \n                 de_rate_initial=0.4, pso_rate_initial=0.3, cmaes_rate_initial=0.3,\n                 learning_rate=0.1, restart_trigger=0.05, cmaes_sigma_initial=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.de_rate = de_rate_initial\n        self.pso_rate = pso_rate_initial\n        self.cmaes_rate = cmaes_rate_initial\n        self.learning_rate = learning_rate\n        self.restart_trigger = restart_trigger\n        self.cmaes_sigma = cmaes_sigma_initial\n        self.archive = []\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget_used = self.pop_size\n\n        self.pbest_population = self.population.copy()\n        self.pbest_fitness = self.fitness.copy()\n\n        self.gbest_index = np.argmin(self.fitness)\n        self.gbest_x = self.population[self.gbest_index].copy()\n        self.gbest_fitness = self.fitness[self.gbest_index].copy()\n\n        self.f_opt = self.gbest_fitness\n        self.x_opt = self.gbest_x\n        self.archive.append((self.f_opt, self.x_opt.copy()))\n        \n        self.no_improvement_count = 0\n        self.max_no_improvement = int(self.budget * self.restart_trigger)\n\n        self.success_de = 0\n        self.success_pso = 0\n        self.success_cmaes = 0\n        self.total_de = 0\n        self.total_pso = 0\n        self.total_cmaes = 0\n        \n        while self.budget_used < self.budget:\n            new_population = np.zeros_like(self.population)\n            new_fitness = np.zeros(self.pop_size)\n            \n            rates = np.array([self.de_rate, self.pso_rate, self.cmaes_rate])\n            rates /= np.sum(rates)  # Normalize to ensure they sum to 1\n\n            for i in range(self.pop_size):\n                rand = np.random.rand()\n                \n                if rand < rates[0]: # Differential Evolution\n                    self.total_de += 1\n                    indices = np.random.choice(self.pop_size, 3, replace=False)\n                    x_r1, x_r2, x_r3 = self.population[indices]\n                    mutation = x_r1 + 0.5 * (x_r2 - x_r3)\n                    crossover = np.random.rand(self.dim) < 0.5  # binomial crossover\n                    new_population[i] = np.where(crossover, mutation, self.population[i])\n                    new_population[i] = np.clip(new_population[i], func.bounds.lb, func.bounds.ub)\n                    new_fitness[i] = func(new_population[i])\n                    self.budget_used += 1\n                    if new_fitness[i] < self.fitness[i]:\n                        self.success_de += 1\n\n                elif rand < rates[0] + rates[1]:  # Particle Swarm Optimization\n                    self.total_pso += 1\n                    inertia = 0.7\n                    cognitive_coeff = 1.5\n                    social_coeff = 1.5\n                    velocity = inertia * (self.population[i] - self.pbest_population[i]) + cognitive_coeff * np.random.rand(self.dim) * (self.pbest_population[i] - self.population[i]) + social_coeff * np.random.rand(self.dim) * (self.gbest_x - self.population[i])\n                    new_population[i] = self.population[i] + velocity\n                    new_population[i] = np.clip(new_population[i], func.bounds.lb, func.bounds.ub)\n                    new_fitness[i] = func(new_population[i])\n                    self.budget_used += 1\n                    if new_fitness[i] < self.fitness[i]:\n                        self.success_pso += 1\n\n                else: # CMA-ES-based local search\n                    self.total_cmaes += 1\n                    mean = self.population[i].copy()\n                    new_population[i] = np.random.normal(mean, self.cmaes_sigma, self.dim)\n                    new_population[i] = np.clip(new_population[i], func.bounds.lb, func.bounds.ub)\n                    new_fitness[i] = func(new_population[i])\n                    self.budget_used += 1\n                    if new_fitness[i] < self.fitness[i]:\n                        self.success_cmaes += 1\n\n                \n            # Adaptive Probability Adjustment\n            if self.total_de > 0 and self.total_pso > 0 and self.total_cmaes > 0:\n                success_rate_de = self.success_de / self.total_de\n                success_rate_pso = self.success_pso / self.total_pso\n                success_rate_cmaes = self.success_cmaes / self.total_cmaes\n\n                self.de_rate += self.learning_rate * (success_rate_de - self.de_rate)\n                self.pso_rate += self.learning_rate * (success_rate_pso - self.pso_rate)\n                self.cmaes_rate += self.learning_rate * (success_rate_cmaes - self.cmaes_rate)\n                \n                # Normalize rates\n                total_rate = self.de_rate + self.pso_rate + self.cmaes_rate\n                self.de_rate /= total_rate\n                self.pso_rate /= total_rate\n                self.cmaes_rate /= total_rate\n                \n                self.success_de, self.success_pso, self.success_cmaes = 0, 0, 0\n                self.total_de, self.total_pso, self.total_cmaes = 0, 0, 0\n\n            for i in range(self.pop_size):\n                if new_fitness[i] < self.fitness[i]:\n                    self.population[i] = new_population[i]\n                    self.fitness[i] = new_fitness[i]\n\n                    if new_fitness[i] < self.pbest_fitness[i]:\n                        self.pbest_population[i] = new_population[i]\n                        self.pbest_fitness[i] = new_fitness[i]\n\n            self.gbest_index = np.argmin(self.fitness)\n            if self.fitness[self.gbest_index] < self.gbest_fitness:\n                self.gbest_x = self.population[self.gbest_index].copy()\n                self.gbest_fitness = self.fitness[self.gbest_index].copy()\n                \n                if self.gbest_fitness < self.f_opt:\n                    self.f_opt = self.gbest_fitness\n                    self.x_opt = self.gbest_x\n                    self.archive.append((self.f_opt, self.x_opt.copy()))\n                    self.no_improvement_count = 0\n                else:\n                    self.no_improvement_count += 1\n            else:\n                self.no_improvement_count += 1\n\n            # Restart mechanism\n            if self.no_improvement_count > self.max_no_improvement:\n                self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                self.fitness = np.array([func(x) for x in self.population])\n\n                self.pbest_population = self.population.copy()\n                self.pbest_fitness = self.fitness.copy()\n\n                self.gbest_index = np.argmin(self.fitness)\n                self.gbest_x = self.population[self.gbest_index].copy()\n                self.gbest_fitness = self.fitness[self.gbest_index].copy()\n                self.archive.append((self.f_opt, self.x_opt.copy()))\n                self.no_improvement_count = 0\n                \n                # Adjust CMA-ES sigma after restart\n                self.cmaes_sigma *= 0.8 # Reduce sigma after restart\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "Evaluation timed out after 300 seconds.", "error": "Evaluation timed out after 300 seconds.", "parent_ids": ["d0fddf51-e6ed-4acb-82bd-f12a4f7ace8d"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "1d6be602-5ca6-4f5d-a0b0-2a4e92ca1a7f", "fitness": "-inf", "name": "SOMDifferentialEvolution", "description": "An adaptive population-based algorithm combining differential evolution with a self-organizing map (SOM) for neighborhood-based search and a local search, adjusting exploration based on fitness landscape characteristics.", "code": "import numpy as np\nfrom minisom import MiniSom  # Ensure minisom is installed: pip install MiniSom\n\nclass SOMDifferentialEvolution:\n    def __init__(self, budget=10000, dim=10, pop_size=20, som_grid_size=5, learning_rate=0.5, sigma=0.3):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lb = -5.0\n        self.ub = 5.0\n        self.som_grid_size = som_grid_size\n        self.learning_rate = learning_rate\n        self.sigma = sigma  # Initial SOM radius\n        self.som = None\n        self.population = None\n        self.fitness = None\n        self.best_x = None\n        self.best_fitness = np.inf\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.budget_used = 0\n\n    def initialize_som(self):\n        \"\"\"Initializes the Self-Organizing Map.\"\"\"\n        self.som = MiniSom(self.som_grid_size, self.som_grid_size, self.dim, sigma=self.sigma, learning_rate=self.learning_rate)\n        initial_weights = np.random.uniform(self.lb, self.ub, size=(self.som_grid_size * self.som_grid_size, self.dim))\n        self.som.train_random(initial_weights, 1000) # Train SOM with random data points\n\n    def differential_evolution(self, pop, fitness, func, F=0.5, Cr=0.7):\n        \"\"\"Performs differential evolution on the population.\"\"\"\n        new_pop = np.copy(pop)\n        new_fitness = np.copy(fitness)\n\n        for i in range(self.pop_size):\n            idxs = [idx for idx in range(self.pop_size) if idx != i]\n            a, b, c = np.random.choice(idxs, 3, replace=False)\n            mutant = pop[a] + F * (pop[b] - pop[c])\n            mutant = np.clip(mutant, self.lb, self.ub)\n\n            # Crossover\n            cross_mask = np.random.rand(self.dim) < Cr\n            trial = np.where(cross_mask, mutant, pop[i])\n            trial_fitness = func(trial)\n            self.budget_used += 1\n\n            if trial_fitness < fitness[i]:\n                new_pop[i] = trial\n                new_fitness[i] = trial_fitness\n\n        return new_pop, new_fitness\n\n    def som_guided_search(self, pop, fitness, func):\n        \"\"\"Guides the search based on the SOM.\"\"\"\n        new_pop = np.copy(pop)\n        new_fitness = np.copy(fitness)\n\n        for i in range(self.pop_size):\n            winner_node = self.som.winner(pop[i])\n            winner_coords = np.array(winner_node)\n            \n            # Explore neighborhood of the winning node\n            neighbor_nodes = []\n            for x in range(max(0, winner_coords[0]-1), min(self.som_grid_size, winner_coords[0]+2)):\n                for y in range(max(0, winner_coords[1]-1), min(self.som_grid_size, winner_coords[1]+2)):\n                    neighbor_nodes.append((x,y))\n\n            # Select a random neighbor node\n            neighbor_node = neighbor_nodes[np.random.choice(len(neighbor_nodes))]\n            neighbor_weight = self.som.get_weights()[neighbor_node[0]][neighbor_node[1]]\n\n            # Move individual towards the neighbor's weight (exploration)\n            exploration_factor = 0.1 # Adjust this\n            new_x = pop[i] + exploration_factor * (neighbor_weight - pop[i])\n            new_x = np.clip(new_x, self.lb, self.ub)\n            new_fitness[i] = func(new_x)\n            self.budget_used += 1\n            \n            if new_fitness[i] < fitness[i]:\n                new_pop[i] = new_x\n                new_fitness[i] = new_fitness[i]\n\n        return new_pop, new_fitness\n\n    def local_search(self, x, func, step_size=0.05):\n        \"\"\"Performs a local search around a given solution.\"\"\"\n        new_x = x + step_size * np.random.randn(self.dim)\n        new_x = np.clip(new_x, self.lb, self.ub)\n        new_fitness = func(new_x)\n        self.budget_used += 1\n        return new_x, new_fitness\n\n    def __call__(self, func):\n        # Initialization\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget_used += self.pop_size\n\n        self.best_x = self.population[np.argmin(self.fitness)].copy()\n        self.best_fitness = np.min(self.fitness).copy()\n        self.f_opt = self.best_fitness\n        self.x_opt = self.best_x\n\n        self.initialize_som()\n\n        while self.budget_used < self.budget:\n            # 1. Differential Evolution\n            self.population, self.fitness = self.differential_evolution(self.population, self.fitness, func)\n            \n            # 2. SOM Guided Search\n            self.population, self.fitness = self.som_guided_search(self.population, self.fitness, func)\n\n            # 3. Local Search (applied to the best individual)\n            best_index = np.argmin(self.fitness)\n            new_x, new_fitness = self.local_search(self.population[best_index].copy(), func)\n            if new_fitness < self.fitness[best_index]:\n                self.population[best_index] = new_x\n                self.fitness[best_index] = new_fitness\n                \n            #Update SOM\n            for x in self.population:\n                self.som.update(x, self.som.winner(x), self.learning_rate)\n\n            # Update best solution\n            best_index = np.argmin(self.fitness)\n            if self.fitness[best_index] < self.best_fitness:\n                self.best_x = self.population[best_index].copy()\n                self.best_fitness = self.fitness[best_index].copy()\n                if self.best_fitness < self.f_opt:\n                    self.f_opt = self.best_fitness\n                    self.x_opt = self.best_x\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "In the code, line 2, in <module>, the following error occurred:\nModuleNotFoundError: No module named 'minisom'\nOn line: from minisom import MiniSom  # Ensure minisom is installed: pip install MiniSom", "error": "In the code, line 2, in <module>, the following error occurred:\nModuleNotFoundError: No module named 'minisom'\nOn line: from minisom import MiniSom  # Ensure minisom is installed: pip install MiniSom", "parent_ids": ["0eceaf09-69d5-4b47-93e6-80e157e7b18f"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "1e74c5f0-f489-41e9-b124-27e514d9a233", "fitness": 0.5636287559512078, "name": "HybridOptimizer", "description": "A simplified hybrid algorithm with self-adaptive exploration rates and a Cauchy mutation-based local search to enhance exploration.", "code": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget=10000, dim=10, pop_size=20, initial_de_rate=0.5, initial_pso_rate=0.3, local_rate=0.2, restart_trigger=0.05):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.de_rate = initial_de_rate\n        self.pso_rate = initial_pso_rate\n        self.local_rate = local_rate\n        self.restart_trigger = restart_trigger\n        self.initial_de_rate = initial_de_rate\n        self.initial_pso_rate = initial_pso_rate\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget_used = self.pop_size\n        \n        self.pbest_population = self.population.copy()\n        self.pbest_fitness = self.fitness.copy()\n\n        self.gbest_index = np.argmin(self.fitness)\n        self.gbest_x = self.population[self.gbest_index].copy()\n        self.gbest_fitness = self.fitness[self.gbest_index].copy()\n        \n        self.f_opt = self.gbest_fitness\n        self.x_opt = self.gbest_x\n\n        self.no_improvement_count = 0\n        self.max_no_improvement = int(self.budget * self.restart_trigger)\n        \n        while self.budget_used < self.budget:\n            new_population = np.zeros_like(self.population)\n            new_fitness = np.zeros(self.pop_size)\n\n            for i in range(self.pop_size):\n                rand = np.random.rand()\n\n                # Differential Evolution\n                if rand < self.de_rate:\n                    indices = np.random.choice(self.pop_size, 3, replace=False)\n                    x_r1, x_r2, x_r3 = self.population[indices]\n                    mutation = x_r1 + 0.5 * (x_r2 - x_r3)\n                    crossover = np.random.rand(self.dim) < 0.5\n                    new_population[i] = np.where(crossover, mutation, self.population[i])\n                    new_population[i] = np.clip(new_population[i], func.bounds.lb, func.bounds.ub)\n                    new_fitness[i] = func(new_population[i])\n                    self.budget_used += 1\n\n                # Particle Swarm Optimization\n                elif rand < self.de_rate + self.pso_rate:\n                    inertia = 0.7\n                    cognitive_coeff = 1.5\n                    social_coeff = 1.5\n                    velocity = inertia * (self.population[i] - self.pbest_population[i]) + cognitive_coeff * np.random.rand(self.dim) * (self.pbest_population[i] - self.population[i]) + social_coeff * np.random.rand(self.dim) * (self.gbest_x - self.population[i])\n                    new_population[i] = self.population[i] + velocity\n                    new_population[i] = np.clip(new_population[i], func.bounds.lb, func.bounds.ub)\n                    new_fitness[i] = func(new_population[i])\n                    self.budget_used += 1\n                    \n                # Local Search with Cauchy Mutation\n                else:\n                    new_population[i] = self.population[i] + 0.01 * np.random.standard_cauchy(size=self.dim)\n                    new_population[i] = np.clip(new_population[i], func.bounds.lb, func.bounds.ub)\n                    new_fitness[i] = func(new_population[i])\n                    self.budget_used += 1\n\n            for i in range(self.pop_size):\n                if new_fitness[i] < self.fitness[i]:\n                    self.population[i] = new_population[i]\n                    self.fitness[i] = new_fitness[i]\n\n                    if new_fitness[i] < self.pbest_fitness[i]:\n                        self.pbest_population[i] = new_population[i]\n                        self.pbest_fitness[i] = new_fitness[i]\n\n            self.gbest_index = np.argmin(self.fitness)\n            if self.fitness[self.gbest_index] < self.gbest_fitness:\n                self.gbest_x = self.population[self.gbest_index].copy()\n                self.gbest_fitness = self.fitness[self.gbest_index].copy()\n                \n                if self.gbest_fitness < self.f_opt:\n                    self.f_opt = self.gbest_fitness\n                    self.x_opt = self.gbest_x\n                    self.no_improvement_count = 0\n\n                    # Adaptive rate adjustment (simplified)\n                    if self.de_rate > 0.1:\n                         self.de_rate *= 0.95\n                    if self.pso_rate > 0.1:\n                        self.pso_rate *= 0.95\n                    self.local_rate = 1 - self.de_rate - self.pso_rate\n                else:\n                    self.no_improvement_count += 1\n            else:\n                self.no_improvement_count += 1\n\n            # Restart mechanism\n            if self.no_improvement_count > self.max_no_improvement:\n                self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                self.fitness = np.array([func(x) for x in self.population])\n                self.budget_used += self.pop_size\n\n                self.pbest_population = self.population.copy()\n                self.pbest_fitness = self.fitness.copy()\n\n                self.gbest_index = np.argmin(self.fitness)\n                self.gbest_x = self.population[self.gbest_index].copy()\n                self.gbest_fitness = self.fitness[self.gbest_index].copy()\n                \n                self.no_improvement_count = 0\n                self.de_rate = self.initial_de_rate\n                self.pso_rate = self.initial_pso_rate\n                self.local_rate = 1 - self.de_rate - self.pso_rate\n                \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "The algorithm HybridOptimizer scored 0.564 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["d0fddf51-e6ed-4acb-82bd-f12a4f7ace8d"], "operator": null, "metadata": {"aucs": [0.1916384132301684, 0.18979246147882722, 0.6022732675768836, 0.956855018144015, 0.5202776235131598, 0.7797555295663907, 0.45952715083750295, 0.46836332509007583, 0.6895311644681698, 0.1871658210497329, 0.6554438333576935, 0.9906767288280464, 0.2869198662494248, 0.5383914338240681, 0.9399203683065838, 0.7831953594692566, 0.4517271883294004, 0.831169812638324, 0.24486837390753435, 0.5050823791588974]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "fe1e2cb6-c6f1-4bd5-84ee-fc6afb9e825d", "fitness": 0.3504341052614296, "name": "HybridOptimizer", "description": "A self-adaptive hybrid optimizer that balances exploration and exploitation using differential evolution, particle swarm optimization, and a novel selection mechanism biased towards promising solutions.", "code": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget=10000, dim=10, pop_size=20):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lb = -5.0\n        self.ub = 5.0\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget_used = self.pop_size\n\n        self.best_index = np.argmin(self.fitness)\n        self.best_x = self.population[self.best_index].copy()\n        self.best_fitness = self.fitness[self.best_index].copy()\n\n        self.f_opt = self.best_fitness\n        self.x_opt = self.best_x\n        \n        exploration_rate = 0.7 # Initial exploration rate\n\n        # PSO parameters\n        inertia = 0.7\n        cognitive_coeff = 1.5\n        social_coeff = 1.5\n\n\n        while self.budget_used < self.budget:\n            new_population = np.zeros_like(self.population)\n            new_fitness = np.zeros(self.pop_size)\n\n            # Adaptive exploration rate decay\n            exploration_rate = 0.7 * (1 - self.budget_used / self.budget)\n\n\n            for i in range(self.pop_size):\n                # Selection mechanism biased towards promising solutions\n                if np.random.rand() < 0.9: # Bias towards selecting better individuals\n                    selected_index = np.argmin(self.fitness)  # Select best individual\n                else:\n                    selected_index = np.random.randint(self.pop_size)\n\n                x_selected = self.population[selected_index]\n\n                if np.random.rand() < exploration_rate:\n                    # Differential Evolution\n                    indices = np.random.choice(self.pop_size, 3, replace=False)\n                    x_r1, x_r2, x_r3 = self.population[indices]\n                    mutation = x_r1 + 0.5 * (x_r2 - x_r3)\n                    crossover = np.random.rand(self.dim) < 0.5  # binomial crossover\n                    new_x = np.where(crossover, mutation, x_selected)\n                    \n                else:\n                    # Particle Swarm Optimization\n                    velocity = inertia * (self.population[i] - self.best_x) + \\\n                                cognitive_coeff * np.random.rand(self.dim) * (self.best_x - self.population[i]) + \\\n                                social_coeff * np.random.rand(self.dim) * (x_selected - self.population[i])\n                    new_x = self.population[i] + velocity\n\n                new_x = np.clip(new_x, self.lb, self.ub)\n                new_population[i] = new_x\n                new_fitness[i] = func(new_x)\n                self.budget_used += 1\n                \n                if new_fitness[i] < self.best_fitness:\n                    self.best_x = new_x.copy()\n                    self.best_fitness = new_fitness[i].copy()\n                    if self.best_fitness < self.f_opt:\n                        self.f_opt = self.best_fitness\n                        self.x_opt = self.best_x\n                \n\n                if self.budget_used >= self.budget:\n                    break\n\n            # Update population\n            for i in range(self.pop_size):\n                if new_fitness[i] < self.fitness[i]:\n                    self.population[i] = new_population[i]\n                    self.fitness[i] = new_fitness[i]\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "The algorithm HybridOptimizer scored 0.350 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["e16e90d9-ba5f-43ec-b171-d0521e5848f9"], "operator": null, "metadata": {"aucs": [0.13937909930610437, 0.18327718352203248, 0.39120734629856213, 0.17104104504655915, 0.3528037021787942, 0.3366839129715259, 0.2792218064380291, 0.2651486410290035, 0.1696933109641413, 0.17708204969697272, 0.36193000213629556, 0.9977419075183868, 0.21500719570696247, 0.26272162027419776, 0.6441306269110638, 0.3358232257613384, 0.3002006972987594, 0.7809670998207161, 0.17485797700883066, 0.4697636553403163]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "589d3b77-7658-4477-889e-bdbc65069ee1", "fitness": "-inf", "name": "HybridOptimizer", "description": "A self-adjusting hybrid algorithm blending Differential Evolution, Particle Swarm Optimization, and a Cauchy mutation-based local search, with adaptive parameter control based on success history and a simplified restart mechanism.", "code": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget=10000, dim=10, pop_size=20, initial_de_rate=0.5, initial_pso_rate=0.3, local_rate=0.2, restart_trigger=0.05):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.de_rate = initial_de_rate\n        self.pso_rate = initial_pso_rate\n        self.local_rate = local_rate\n        self.restart_trigger = restart_trigger\n        self.initial_de_rate = initial_de_rate\n        self.initial_pso_rate = initial_pso_rate\n        self.success_history_de = []\n        self.success_history_pso = []\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget_used = self.pop_size\n        \n        self.pbest_population = self.population.copy()\n        self.pbest_fitness = self.fitness.copy()\n\n        self.gbest_index = np.argmin(self.fitness)\n        self.gbest_x = self.population[self.gbest_index].copy()\n        self.gbest_fitness = self.fitness[self.gbest_index].copy()\n        \n        self.f_opt = self.gbest_fitness\n        self.x_opt = self.gbest_x\n\n        self.no_improvement_count = 0\n        self.max_no_improvement = int(self.budget * self.restart_trigger)\n        \n        while self.budget_used < self.budget:\n            new_population = np.zeros_like(self.population)\n            new_fitness = np.zeros(self.pop_size)\n\n            for i in range(self.pop_size):\n                rand = np.random.rand()\n                original_fitness = self.fitness[i]\n\n                # Differential Evolution\n                if rand < self.de_rate:\n                    indices = np.random.choice(self.pop_size, 3, replace=False)\n                    x_r1, x_r2, x_r3 = self.population[indices]\n                    mutation = x_r1 + 0.5 * (x_r2 - x_r3)\n                    crossover = np.random.rand(self.dim) < 0.5\n                    new_population[i] = np.where(crossover, mutation, self.population[i])\n                    new_population[i] = np.clip(new_population[i], func.bounds.lb, func.bounds.ub)\n                    new_fitness[i] = func(new_population[i])\n                    self.budget_used += 1\n                    if new_fitness[i] < original_fitness:\n                        self.success_history_de.append(1)\n                    else:\n                        self.success_history_de.append(0)\n\n                # Particle Swarm Optimization\n                elif rand < self.de_rate + self.pso_rate:\n                    inertia = 0.7\n                    cognitive_coeff = 1.5\n                    social_coeff = 1.5\n                    velocity = inertia * (self.population[i] - self.pbest_population[i]) + cognitive_coeff * np.random.rand(self.dim) * (self.pbest_population[i] - self.population[i]) + social_coeff * np.random.rand(self.dim) * (self.gbest_x - self.population[i])\n                    new_population[i] = self.population[i] + velocity\n                    new_population[i] = np.clip(new_population[i], func.bounds.lb, func.bounds.ub)\n                    new_fitness[i] = func(new_population[i])\n                    self.budget_used += 1\n                    if new_fitness[i] < original_fitness:\n                        self.success_history_pso.append(1)\n                    else:\n                        self.success_history_pso.append(0)\n                    \n                # Local Search with Cauchy Mutation\n                else:\n                    new_population[i] = self.population[i] + 0.01 * np.random.standard_cauchy(size=self.dim)\n                    new_population[i] = np.clip(new_population[i], func.bounds.lb, func.bounds.ub)\n                    new_fitness[i] = func(new_population[i])\n                    self.budget_used += 1\n\n            for i in range(self.pop_size):\n                if new_fitness[i] < self.fitness[i]:\n                    self.population[i] = new_population[i]\n                    self.fitness[i] = new_fitness[i]\n\n                    if new_fitness[i] < self.pbest_fitness[i]:\n                        self.pbest_population[i] = new_population[i]\n                        self.pbest_fitness[i] = new_fitness[i]\n\n            self.gbest_index = np.argmin(self.fitness)\n            if self.fitness[self.gbest_index] < self.gbest_fitness:\n                self.gbest_x = self.population[self.gbest_index].copy()\n                self.gbest_fitness = self.fitness[self.gbest_index].copy()\n                \n                if self.gbest_fitness < self.f_opt:\n                    self.f_opt = self.gbest_fitness\n                    self.x_opt = self.gbest_x\n                    self.no_improvement_count = 0\n\n                else:\n                    self.no_improvement_count += 1\n            else:\n                self.no_improvement_count += 1\n                \n            # Adaptive rate adjustment based on success history\n            window_size = min(100, len(self.success_history_de))\n            if window_size > 0:\n                success_rate_de = np.mean(self.success_history_de[-window_size:])\n                success_rate_pso = np.mean(self.success_history_pso[-window_size:])\n                \n                self.de_rate = max(0.1, min(0.9, self.de_rate + 0.1 * (success_rate_de - 0.5)))\n                self.pso_rate = max(0.1, min(0.9, self.pso_rate + 0.1 * (success_rate_pso - 0.5)))\n                \n                # Normalize the rates\n                total_rate = self.de_rate + self.pso_rate\n                self.de_rate /= total_rate\n                self.pso_rate /= total_rate\n                \n                self.local_rate = 1 - self.de_rate - self.pso_rate\n\n            # Restart mechanism\n            if self.no_improvement_count > self.max_no_improvement:\n                self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                self.fitness = np.array([func(x) for x in self.population])\n                self.budget_used += self.pop_size\n\n                self.pbest_population = self.population.copy()\n                self.pbest_fitness = self.fitness.copy()\n\n                self.gbest_index = np.argmin(self.fitness)\n                self.gbest_x = self.population[self.gbest_index].copy()\n                self.gbest_fitness = self.fitness[self.gbest_index].copy()\n                \n                self.no_improvement_count = 0\n                self.de_rate = self.initial_de_rate\n                self.pso_rate = self.initial_pso_rate\n                self.local_rate = 1 - self.de_rate - self.pso_rate\n                self.success_history_de = []\n                self.success_history_pso = []\n                \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "Evaluation timed out after 300 seconds.", "error": "Evaluation timed out after 300 seconds.", "parent_ids": ["1e74c5f0-f489-41e9-b124-27e514d9a233"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "c10c0f67-4a81-4a7d-9b5f-7583fae21403", "fitness": "-inf", "name": "HybridOptimizer", "description": "A self-adjusting hybrid algorithm with dynamic exploration rates and orthogonal learning to accelerate convergence and maintain diversity.", "code": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget=10000, dim=10, pop_size=20, initial_de_rate=0.5, initial_pso_rate=0.3, local_rate=0.2, restart_trigger=0.05, orthogonal_learning_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.de_rate = initial_de_rate\n        self.pso_rate = initial_pso_rate\n        self.local_rate = local_rate\n        self.restart_trigger = restart_trigger\n        self.initial_de_rate = initial_de_rate\n        self.initial_pso_rate = initial_pso_rate\n        self.orthogonal_learning_rate = orthogonal_learning_rate\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget_used = self.pop_size\n        \n        self.pbest_population = self.population.copy()\n        self.pbest_fitness = self.fitness.copy()\n\n        self.gbest_index = np.argmin(self.fitness)\n        self.gbest_x = self.population[self.gbest_index].copy()\n        self.gbest_fitness = self.fitness[self.gbest_index].copy()\n        \n        self.f_opt = self.gbest_fitness\n        self.x_opt = self.gbest_x\n\n        self.no_improvement_count = 0\n        self.max_no_improvement = int(self.budget * self.restart_trigger)\n        \n        while self.budget_used < self.budget:\n            new_population = np.zeros_like(self.population)\n            new_fitness = np.zeros(self.pop_size)\n\n            for i in range(self.pop_size):\n                rand = np.random.rand()\n\n                # Differential Evolution\n                if rand < self.de_rate:\n                    indices = np.random.choice(self.pop_size, 3, replace=False)\n                    x_r1, x_r2, x_r3 = self.population[indices]\n                    mutation = x_r1 + 0.5 * (x_r2 - x_r3)\n                    crossover = np.random.rand(self.dim) < 0.5\n                    new_population[i] = np.where(crossover, mutation, self.population[i])\n                    new_population[i] = np.clip(new_population[i], func.bounds.lb, func.bounds.ub)\n                    new_fitness[i] = func(new_population[i])\n                    self.budget_used += 1\n\n                # Particle Swarm Optimization\n                elif rand < self.de_rate + self.pso_rate:\n                    inertia = 0.7\n                    cognitive_coeff = 1.5\n                    social_coeff = 1.5\n                    velocity = inertia * (self.population[i] - self.pbest_population[i]) + cognitive_coeff * np.random.rand(self.dim) * (self.pbest_population[i] - self.population[i]) + social_coeff * np.random.rand(self.dim) * (self.gbest_x - self.population[i])\n                    new_population[i] = self.population[i] + velocity\n                    new_population[i] = np.clip(new_population[i], func.bounds.lb, func.bounds.ub)\n                    new_fitness[i] = func(new_population[i])\n                    self.budget_used += 1\n                    \n                # Local Search with Cauchy Mutation\n                else:\n                    new_population[i] = self.population[i] + 0.01 * np.random.standard_cauchy(size=self.dim)\n                    new_population[i] = np.clip(new_population[i], func.bounds.lb, func.bounds.ub)\n                    new_fitness[i] = func(new_population[i])\n                    self.budget_used += 1\n\n            for i in range(self.pop_size):\n                if new_fitness[i] < self.fitness[i]:\n                    self.population[i] = new_population[i]\n                    self.fitness[i] = new_fitness[i]\n\n                    if new_fitness[i] < self.pbest_fitness[i]:\n                        self.pbest_population[i] = new_population[i]\n                        self.pbest_fitness[i] = new_fitness[i]\n\n            self.gbest_index = np.argmin(self.fitness)\n            if self.fitness[self.gbest_index] < self.gbest_fitness:\n                self.gbest_x = self.population[self.gbest_index].copy()\n                self.gbest_fitness = self.fitness[self.gbest_index].copy()\n                \n                if self.gbest_fitness < self.f_opt:\n                    self.f_opt = self.gbest_fitness\n                    self.x_opt = self.gbest_x\n                    self.no_improvement_count = 0\n\n                    # Adaptive rate adjustment (simplified)\n                    if self.de_rate > 0.1:\n                         self.de_rate *= 0.95\n                    if self.pso_rate > 0.1:\n                        self.pso_rate *= 0.95\n                    self.local_rate = 1 - self.de_rate - self.pso_rate\n                else:\n                    self.no_improvement_count += 1\n            else:\n                self.no_improvement_count += 1\n            \n            # Orthogonal learning\n            if np.random.rand() < self.orthogonal_learning_rate:\n                self.orthogonal_learning(func)\n\n            # Restart mechanism\n            if self.no_improvement_count > self.max_no_improvement:\n                self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                self.fitness = np.array([func(x) for x in self.population])\n                self.budget_used += self.pop_size\n\n                self.pbest_population = self.population.copy()\n                self.pbest_fitness = self.fitness.copy()\n\n                self.gbest_index = np.argmin(self.fitness)\n                self.gbest_x = self.population[self.gbest_index].copy()\n                self.gbest_fitness = self.fitness[self.gbest_index].copy()\n                \n                self.no_improvement_count = 0\n                self.de_rate = self.initial_de_rate\n                self.pso_rate = self.initial_pso_rate\n                self.local_rate = 1 - self.de_rate - self.pso_rate\n                \n        return self.f_opt, self.x_opt\n        \n    def orthogonal_learning(self, func):\n        \"\"\"\n        Orthogonal learning to generate a new candidate solution.\n        \"\"\"\n        \n        # Select two random individuals\n        indices = np.random.choice(self.pop_size, 2, replace=False)\n        x1, x2 = self.population[indices]\n        \n        # Create an orthogonal array (OA). For simplicity, use a fixed OA.\n        # The levels are based on the selected individuals x1 and x2\n        oa = np.array([[1, 1], [1, 2], [2, 1], [2, 2]])  # Example OA(4, 2, 2)\n        \n        new_candidates = np.zeros((oa.shape[0], self.dim))\n        \n        for i in range(oa.shape[0]):\n            for j in range(self.dim):\n                if oa[i, 0] == 1:\n                    level1 = x1[j]\n                else:\n                    level1 = x2[j]\n                if oa[i, 1] == 1:\n                    level2 = x1[j]\n                else:\n                    level2 = x2[j]\n                \n                # Create a new candidate solution using the orthogonal array\n                new_candidates[i, j] = (level1 + level2) / 2  # Simplified combination\n                \n        # Evaluate the candidate solutions\n        new_fitness = np.array([func(x) for x in new_candidates])\n        self.budget_used += len(new_fitness)\n        \n        # Select the best candidate solution\n        best_index = np.argmin(new_fitness)\n        best_candidate = new_candidates[best_index]\n        best_fitness = new_fitness[best_index]\n        \n        # Replace the worst individual in the population if the new candidate is better\n        worst_index = np.argmax(self.fitness)\n        if best_fitness < self.fitness[worst_index]:\n            self.population[worst_index] = best_candidate\n            self.fitness[worst_index] = best_fitness\n            \n            if best_fitness < self.pbest_fitness[worst_index]:\n                self.pbest_population[worst_index] = best_candidate\n                self.pbest_fitness[worst_index] = best_fitness\n            \n            if best_fitness < self.gbest_fitness:\n                self.gbest_x = best_candidate.copy()\n                self.gbest_fitness = best_fitness.copy()\n                \n                if best_fitness < self.f_opt:\n                    self.f_opt = best_fitness\n                    self.x_opt = best_candidate\n", "configspace": "", "generation": 7, "feedback": "Evaluation timed out after 300 seconds.", "error": "Evaluation timed out after 300 seconds.", "parent_ids": ["1e74c5f0-f489-41e9-b124-27e514d9a233"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "05fe28ec-8b1f-4fe9-b7c3-0e64cc34860c", "fitness": 0.0, "name": "HybridOptimizer", "description": "A hybrid algorithm combining Differential Evolution, Particle Swarm Optimization, and Simulated Annealing with adaptive parameter control and a local search to balance exploration and exploitation.", "code": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget=10000, dim=10, pop_size=20):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lb = -5.0\n        self.ub = 5.0\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget_used = self.pop_size\n\n        self.best_index = np.argmin(self.fitness)\n        self.best_x = self.population[self.best_index].copy()\n        self.best_fitness = self.fitness[self.best_index].copy()\n\n        self.f_opt = self.best_fitness\n        self.x_opt = self.best_x\n        \n        exploration_rate = 0.7 \n        exploitation_rate = 0.2\n        de_rate = 0.4 #differential evolution rate\n        pso_rate = 0.4 #particle swarm optimisation rate\n        sa_rate = 0.2 # simulated annealing rate\n\n        T = 1.0 # Initial temperature for simulated annealing\n        T_min = 0.0001 # Minimum temperature\n        alpha = 0.95 # Cooling rate\n\n\n        while self.budget_used < self.budget:\n            new_population = np.zeros_like(self.population)\n            \n            # Dynamic adjustment of exploration/exploitation balance\n            remaining_budget_ratio = (self.budget - self.budget_used) / self.budget\n            current_exploration_rate = exploration_rate * remaining_budget_ratio\n            current_exploitation_rate = exploitation_rate * (1 - remaining_budget_ratio)\n            \n            for i in range(self.pop_size):\n                if np.random.rand() < current_exploration_rate:\n                    #Differential evolution strategy\n                    if np.random.rand() < de_rate:\n                        indices = np.random.choice(self.pop_size, 3, replace=False)\n                        x_r1, x_r2, x_r3 = self.population[indices]\n                        mutation = x_r1 + 0.5 * (x_r2 - x_r3)\n                        crossover = np.random.rand(self.dim) < 0.5  # binomial crossover\n                        new_population[i] = np.where(crossover, mutation, self.population[i])\n                    #Particle swarm optimisation\n                    elif np.random.rand() < pso_rate:\n                        inertia = 0.7\n                        cognitive_coeff = 1.5\n                        social_coeff = 1.5\n                        velocity = inertia * (self.population[i] - self.best_x) + cognitive_coeff * np.random.rand(self.dim) * (self.best_x - self.population[i]) + social_coeff * np.random.rand(self.dim) * (self.best_x - self.population[i])\n                        new_population[i] = self.population[i] + velocity\n                    \n                    elif np.random.rand() < sa_rate:\n                        # Simulated Annealing\n                        neighbor = self.population[i] + np.random.normal(0, 0.1, self.dim)\n                        neighbor = np.clip(neighbor, self.lb, self.ub)\n                        delta_e = func(neighbor) - self.fitness[i]\n\n                        if delta_e < 0:\n                            new_population[i] = neighbor\n                        else:\n                            acceptance_probability = np.exp(-delta_e / T)\n                            if np.random.rand() < acceptance_probability:\n                                new_population[i] = neighbor\n                            else:\n                                new_population[i] = self.population[i]\n\n                    # Random exploration\n                    else:\n                        new_population[i] = np.random.uniform(self.lb, self.ub, self.dim)\n\n                elif np.random.rand() < current_exploitation_rate:\n                    # Local search around the best solution\n                    new_population[i] = self.best_x + np.random.normal(0, 0.1, self.dim)\n\n                else:\n                    #Default random exploration\n                     new_population[i] = np.random.uniform(self.lb, self.ub, self.dim)\n\n                new_population[i] = np.clip(new_population[i], self.lb, self.ub)\n\n            new_fitness = np.array([func(x) for x in new_population])\n            self.budget_used += self.pop_size\n\n            for i in range(self.pop_size):\n                if new_fitness[i] < self.fitness[i]:\n                    self.population[i] = new_population[i]\n                    self.fitness[i] = new_fitness[i]\n\n                    if new_fitness[i] < self.best_fitness:\n                        self.best_x = new_population[i].copy()\n                        self.best_fitness = new_fitness[i].copy()\n                        \n                        if self.best_fitness < self.f_opt:\n                            self.f_opt = self.best_fitness\n                            self.x_opt = self.best_x\n            \n            T = max(T * alpha, T_min) # Cool down the temperature\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "The algorithm HybridOptimizer scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["e16e90d9-ba5f-43ec-b171-d0521e5848f9"], "operator": null, "metadata": {"aucs": [0]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "8c3d4b9c-ad39-4b01-b5a5-d364b8aec6bd", "fitness": "-inf", "name": "HybridOptimizer", "description": "A hybrid optimizer that combines differential evolution, particle swarm optimization, and a Nelder-Mead local search, with adaptive probability adjustments and a CMA-ES based mutation to enhance exploration and exploitation.", "code": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOptimizer:\n    def __init__(self, budget=10000, dim=10, pop_size=20, initial_de_rate=0.4, initial_pso_rate=0.3, initial_local_rate=0.2, initial_cmaes_rate=0.1, restart_trigger=0.05, local_search_iterations=5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.de_rate = initial_de_rate\n        self.pso_rate = initial_pso_rate\n        self.local_rate = initial_local_rate\n        self.cmaes_rate = initial_cmaes_rate\n        self.restart_trigger = restart_trigger\n        self.initial_de_rate = initial_de_rate\n        self.initial_pso_rate = initial_pso_rate\n        self.initial_local_rate = initial_local_rate\n        self.initial_cmaes_rate = initial_cmaes_rate\n        self.local_search_iterations = local_search_iterations\n\n    def nelder_mead(self, func, x0, bounds, budget_left):\n        if budget_left <= 0:\n            return func(x0), x0\n        \n        def obj_func(x):\n            return func(x)\n\n        res = minimize(obj_func, x0, method='Nelder-Mead', bounds=bounds, options={'maxfev': budget_left})\n        return res.fun, res.x\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        bounds = [(func.bounds.lb, func.bounds.ub)] * self.dim\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget_used = self.pop_size\n        \n        self.pbest_population = self.population.copy()\n        self.pbest_fitness = self.fitness.copy()\n\n        self.gbest_index = np.argmin(self.fitness)\n        self.gbest_x = self.population[self.gbest_index].copy()\n        self.gbest_fitness = self.fitness[self.gbest_index].copy()\n        \n        self.f_opt = self.gbest_fitness\n        self.x_opt = self.gbest_x\n\n        self.no_improvement_count = 0\n        self.max_no_improvement = int(self.budget * self.restart_trigger)\n        \n        while self.budget_used < self.budget:\n            new_population = np.zeros_like(self.population)\n            new_fitness = np.zeros(self.pop_size)\n\n            for i in range(self.pop_size):\n                rand = np.random.rand()\n\n                # Differential Evolution\n                if rand < self.de_rate:\n                    indices = np.random.choice(self.pop_size, 3, replace=False)\n                    x_r1, x_r2, x_r3 = self.population[indices]\n                    mutation = x_r1 + 0.5 * (x_r2 - x_r3)\n                    crossover = np.random.rand(self.dim) < 0.5\n                    new_population[i] = np.where(crossover, mutation, self.population[i])\n                    new_population[i] = np.clip(new_population[i], func.bounds.lb, func.bounds.ub)\n                    new_fitness[i] = func(new_population[i])\n                    self.budget_used += 1\n\n                # Particle Swarm Optimization\n                elif rand < self.de_rate + self.pso_rate:\n                    inertia = 0.7\n                    cognitive_coeff = 1.5\n                    social_coeff = 1.5\n                    velocity = inertia * (self.population[i] - self.pbest_population[i]) + cognitive_coeff * np.random.rand(self.dim) * (self.pbest_population[i] - self.population[i]) + social_coeff * np.random.rand(self.dim) * (self.gbest_x - self.population[i])\n                    new_population[i] = self.population[i] + velocity\n                    new_population[i] = np.clip(new_population[i], func.bounds.lb, func.bounds.ub)\n                    new_fitness[i] = func(new_population[i])\n                    self.budget_used += 1\n                    \n                # Local Search with Nelder-Mead\n                elif rand < self.de_rate + self.pso_rate + self.local_rate:\n                    budget_for_local_search = min(self.local_search_iterations, self.budget - self.budget_used)\n                    if budget_for_local_search > 0:\n                        new_fitness[i], new_population[i] = self.nelder_mead(func, self.population[i], bounds, budget_for_local_search)\n                        self.budget_used += budget_for_local_search\n                \n                # CMA-ES inspired mutation\n                else:\n                    mean = self.population[i]\n                    covariance = np.eye(self.dim) * 0.01  # Small initial covariance\n                    new_population[i] = np.random.multivariate_normal(mean, covariance)\n                    new_population[i] = np.clip(new_population[i], func.bounds.lb, func.bounds.ub)\n                    new_fitness[i] = func(new_population[i])\n                    self.budget_used += 1\n\n            for i in range(self.pop_size):\n                if new_fitness[i] < self.fitness[i]:\n                    self.population[i] = new_population[i]\n                    self.fitness[i] = new_fitness[i]\n\n                    if new_fitness[i] < self.pbest_fitness[i]:\n                        self.pbest_population[i] = new_population[i]\n                        self.pbest_fitness[i] = new_fitness[i]\n\n            self.gbest_index = np.argmin(self.fitness)\n            if self.fitness[self.gbest_index] < self.gbest_fitness:\n                self.gbest_x = self.population[self.gbest_index].copy()\n                self.gbest_fitness = self.fitness[self.gbest_index].copy()\n                \n                if self.gbest_fitness < self.f_opt:\n                    self.f_opt = self.gbest_fitness\n                    self.x_opt = self.gbest_x\n                    self.no_improvement_count = 0\n\n                    # Adaptive rate adjustment (simplified)\n                    if self.de_rate > 0.05:\n                         self.de_rate *= 0.95\n                    if self.pso_rate > 0.05:\n                        self.pso_rate *= 0.95\n                    if self.local_rate > 0.05:\n                        self.local_rate *= 0.95\n                    self.cmaes_rate = 1 - self.de_rate - self.pso_rate - self.local_rate\n                else:\n                    self.no_improvement_count += 1\n            else:\n                self.no_improvement_count += 1\n\n            # Restart mechanism\n            if self.no_improvement_count > self.max_no_improvement:\n                self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                self.fitness = np.array([func(x) for x in self.population])\n                self.budget_used += self.pop_size\n\n                self.pbest_population = self.population.copy()\n                self.pbest_fitness = self.fitness.copy()\n\n                self.gbest_index = np.argmin(self.fitness)\n                self.gbest_x = self.population[self.gbest_index].copy()\n                self.gbest_fitness = self.fitness[self.gbest_index].copy()\n                \n                self.no_improvement_count = 0\n                self.de_rate = self.initial_de_rate\n                self.pso_rate = self.initial_pso_rate\n                self.local_rate = self.initial_local_rate\n                self.cmaes_rate = self.initial_cmaes_rate\n                \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "In the code, line 27, in nelder_mead, the following error occurred:\nNameError: name 'minimize' is not defined\nOn line: res = minimize(obj_func, x0, method='Nelder-Mead', bounds=bounds, options={'maxfev': budget_left})", "error": "In the code, line 27, in nelder_mead, the following error occurred:\nNameError: name 'minimize' is not defined\nOn line: res = minimize(obj_func, x0, method='Nelder-Mead', bounds=bounds, options={'maxfev': budget_left})", "parent_ids": ["1e74c5f0-f489-41e9-b124-27e514d9a233"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "ebcee807-bee8-46e5-9401-fb529b0ae6c7", "fitness": "-inf", "name": "AdaptiveHybridOptimizer", "description": "A novel hybrid algorithm with dynamically adjusted exploration-exploitation balance, incorporating a success-history based parameter adaptation and a covariance matrix adaptation strategy for enhanced exploration.", "code": "import numpy as np\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget=10000, dim=10, pop_size=20, initial_de_rate=0.5, initial_cma_rate=0.3, local_rate=0.2, restart_trigger=0.05, archive_size=100):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.de_rate = initial_de_rate\n        self.cma_rate = initial_cma_rate\n        self.local_rate = local_rate\n        self.restart_trigger = restart_trigger\n        self.archive_size = archive_size\n        self.archive = []\n        self.archive_fitness = []\n        self.success_history_de = []\n        self.success_history_cma = []\n        self.initial_de_rate = initial_de_rate\n        self.initial_cma_rate = initial_cma_rate\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget_used = self.pop_size\n\n        self.best_index = np.argmin(self.fitness)\n        self.best_x = self.population[self.best_index].copy()\n        self.best_fitness = self.fitness[self.best_index].copy()\n        \n        self.f_opt = self.best_fitness\n        self.x_opt = self.best_x\n\n        self.no_improvement_count = 0\n        self.max_no_improvement = int(self.budget * self.restart_trigger)\n\n        self.C = np.eye(self.dim)  # Covariance matrix for CMA-ES\n        self.mean = np.mean(self.population, axis=0)\n        self.sigma = 0.1 # Overall standard deviation\n\n        while self.budget_used < self.budget:\n            new_population = np.zeros_like(self.population)\n            new_fitness = np.zeros(self.pop_size)\n\n            for i in range(self.pop_size):\n                rand = np.random.rand()\n\n                # Differential Evolution\n                if rand < self.de_rate:\n                    indices = np.random.choice(self.pop_size, 3, replace=False)\n                    x_r1, x_r2, x_r3 = self.population[indices]\n                    mutation = x_r1 + 0.5 * (x_r2 - x_r3)\n                    crossover = np.random.rand(self.dim) < 0.5\n                    new_population[i] = np.where(crossover, mutation, self.population[i])\n                    new_population[i] = np.clip(new_population[i], func.bounds.lb, func.bounds.ub)\n                    new_fitness[i] = func(new_population[i])\n                    self.budget_used += 1\n                    if new_fitness[i] < self.fitness[i]:\n                        self.success_history_de.append(1)\n                    else:\n                        self.success_history_de.append(0)\n\n\n                # CMA-ES\n                elif rand < self.de_rate + self.cma_rate:\n                    z = np.random.multivariate_normal(np.zeros(self.dim), self.C)\n                    new_population[i] = self.mean + self.sigma * z\n                    new_population[i] = np.clip(new_population[i], func.bounds.lb, func.bounds.ub)\n                    new_fitness[i] = func(new_population[i])\n                    self.budget_used += 1\n                    if new_fitness[i] < self.fitness[i]:\n                        self.success_history_cma.append(1)\n                    else:\n                        self.success_history_cma.append(0)\n\n                # Local Search with Cauchy Mutation\n                else:\n                    new_population[i] = self.population[i] + 0.01 * np.random.standard_cauchy(size=self.dim)\n                    new_population[i] = np.clip(new_population[i], func.bounds.lb, func.bounds.ub)\n                    new_fitness[i] = func(new_population[i])\n                    self.budget_used += 1\n\n            for i in range(self.pop_size):\n                if new_fitness[i] < self.fitness[i]:\n                    self.population[i] = new_population[i]\n                    self.fitness[i] = new_fitness[i]\n\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(new_population[i])\n                        self.archive_fitness.append(new_fitness[i])\n                    else:\n                        worst_index = np.argmax(self.archive_fitness)\n                        if new_fitness[i] < self.archive_fitness[worst_index]:\n                            self.archive[worst_index] = new_population[i]\n                            self.archive_fitness[worst_index] = new_fitness[i]\n\n\n            self.best_index = np.argmin(self.fitness)\n            if self.fitness[self.best_index] < self.best_fitness:\n                self.best_x = self.population[self.best_index].copy()\n                self.best_fitness = self.fitness[self.best_index].copy()\n                \n                if self.best_fitness < self.f_opt:\n                    self.f_opt = self.best_fitness\n                    self.x_opt = self.best_x\n                    self.no_improvement_count = 0\n\n                    # Adaptive rate adjustment (success-history based)\n                    success_rate_de = np.mean(self.success_history_de[-10:] if len(self.success_history_de) > 0 else 0)\n                    success_rate_cma = np.mean(self.success_history_cma[-10:] if len(self.success_history_cma) > 0 else 0)\n\n                    self.de_rate = min(1.0, self.de_rate * (1 + 0.1 * (success_rate_de - 0.5)))\n                    self.cma_rate = min(1.0, self.cma_rate * (1 + 0.1 * (success_rate_cma - 0.5)))\n                    remaining_rate = 1.0 - self.de_rate - self.cma_rate\n                    self.local_rate = max(0.0, remaining_rate) # Ensure non-negative\n                else:\n                    self.no_improvement_count += 1\n            else:\n                self.no_improvement_count += 1\n\n            # Restart mechanism\n            if self.no_improvement_count > self.max_no_improvement:\n                self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                self.fitness = np.array([func(x) for x in self.population])\n                self.budget_used += self.pop_size\n\n                self.best_index = np.argmin(self.fitness)\n                self.best_x = self.population[self.best_index].copy()\n                self.best_fitness = self.fitness[self.best_index].copy()\n\n                self.no_improvement_count = 0\n                self.de_rate = self.initial_de_rate\n                self.cma_rate = self.initial_cma_rate\n                self.local_rate = 1 - self.de_rate - self.cma_rate\n\n            # Update CMA-ES parameters\n            self.mean = np.mean(self.population, axis=0)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "Evaluation timed out after 300 seconds.", "error": "Evaluation timed out after 300 seconds.", "parent_ids": ["1e74c5f0-f489-41e9-b124-27e514d9a233"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "d6cf2f4b-280c-4652-a8a2-4082d3ee176f", "fitness": 0.0, "name": "HybridOptimizer", "description": "A simplified hybrid algorithm that dynamically adjusts exploration-exploitation using a single parameter and restarts when stagnating.", "code": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget=10000, dim=10, pop_size=20, stagnation_limit=500):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lb = -5.0\n        self.ub = 5.0\n        self.stagnation_limit = stagnation_limit\n        self.stagnation_counter = 0\n        self.best_fitness_history = []\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget_used = self.pop_size\n\n        self.best_index = np.argmin(self.fitness)\n        self.best_x = self.population[self.best_index].copy()\n        self.best_fitness = self.fitness[self.best_index].copy()\n\n        self.f_opt = self.best_fitness\n        self.x_opt = self.best_x\n\n        exploration_rate = 0.7  # Initial exploration rate\n\n        while self.budget_used < self.budget:\n            new_population = np.zeros_like(self.population)\n\n            # Dynamic adjustment of exploration/exploitation balance using a single parameter\n            exploration_prob = exploration_rate * (1 - (self.budget_used / self.budget))\n\n            for i in range(self.pop_size):\n                if np.random.rand() < exploration_prob:\n                    # Exploration: Differential Evolution\n                    indices = np.random.choice(self.pop_size, 3, replace=False)\n                    x_r1, x_r2, x_r3 = self.population[indices]\n                    mutation = x_r1 + 0.5 * (x_r2 - x_r3)\n                    crossover = np.random.rand(self.dim) < 0.5\n                    new_population[i] = np.where(crossover, mutation, self.population[i])\n\n                else:\n                    # Exploitation: Local search around the best\n                    new_population[i] = self.best_x + np.random.normal(0, 0.1, self.dim)\n                    new_population[i] = np.clip(new_population[i], self.lb, self.ub)\n\n            new_fitness = np.array([func(x) for x in new_population])\n            self.budget_used += self.pop_size\n\n            for i in range(self.pop_size):\n                if new_fitness[i] < self.fitness[i]:\n                    self.population[i] = new_population[i]\n                    self.fitness[i] = new_fitness[i]\n\n                    if new_fitness[i] < self.best_fitness:\n                        self.best_x = new_population[i].copy()\n                        self.best_fitness = new_fitness[i].copy()\n                        self.stagnation_counter = 0 # Reset stagnation counter\n\n                        if self.best_fitness < self.f_opt:\n                            self.f_opt = self.best_fitness\n                            self.x_opt = self.best_x\n                \n            #Stagnation Check\n            self.stagnation_counter += self.pop_size\n            if self.stagnation_counter > self.stagnation_limit:\n                #Restart the population\n                self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n                self.fitness = np.array([func(x) for x in self.population])\n                self.best_index = np.argmin(self.fitness)\n                self.best_x = self.population[self.best_index].copy()\n                self.best_fitness = self.fitness[self.best_index].copy()\n                self.stagnation_counter = 0\n                exploration_rate = 0.7\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "The algorithm HybridOptimizer scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["e16e90d9-ba5f-43ec-b171-d0521e5848f9"], "operator": null, "metadata": {"aucs": [0]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "59e08520-aba7-4f15-aa99-a6381700d26e", "fitness": 0.38942696877118044, "name": "AdaptiveHybridOptimizer", "description": "A self-adjusting hybrid algorithm that prioritizes exploration early on, gradually shifting towards exploitation using a decaying exploration rate and a simplified local search.", "code": "import numpy as np\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget=10000, dim=10, pop_size=20):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lb = -5.0\n        self.ub = 5.0\n        self.exploration_rate = 1.0  # Initial exploration rate\n        self.exploration_decay = 0.995  # Decay factor for exploration rate\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget_used = self.pop_size\n\n        self.best_index = np.argmin(self.fitness)\n        self.best_x = self.population[self.best_index].copy()\n        self.best_fitness = self.fitness[self.best_index].copy()\n\n        self.f_opt = self.best_fitness\n        self.x_opt = self.best_x\n        \n        while self.budget_used < self.budget:\n            for i in range(self.pop_size):\n                if np.random.rand() < self.exploration_rate:\n                    # Exploration: Random perturbation\n                    new_x = self.population[i] + np.random.normal(0, 0.5, self.dim)\n                else:\n                    # Exploitation: Simplified Local Search\n                    new_x = self.population[i] + np.random.normal(0, 0.1, self.dim)\n\n                new_x = np.clip(new_x, self.lb, self.ub)\n                new_fitness = func(new_x)\n                self.budget_used += 1\n\n                if new_fitness < self.fitness[i]:\n                    self.population[i] = new_x\n                    self.fitness[i] = new_fitness\n\n                    if new_fitness < self.best_fitness:\n                        self.best_x = new_x.copy()\n                        self.best_fitness = new_fitness\n                        if new_fitness < self.f_opt:\n                            self.f_opt = new_fitness\n                            self.x_opt = new_x\n            self.exploration_rate *= self.exploration_decay # Decay exploration rate\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "The algorithm AdaptiveHybridOptimizer scored 0.389 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["0eceaf09-69d5-4b47-93e6-80e157e7b18f"], "operator": null, "metadata": {"aucs": [0.12353409744144939, 0.2135125173547362, 0.4209310963845463, 0.4458867942822451, 0.3159144271425338, 0.415293165829022, 0.27613478484129206, 0.34612679391318724, 0.3178954638947883, 0.17502278810526894, 0.4611453399963563, 0.9911573533712679, 0.3381976462525318, 0.3327271698862915, 0.7468613032730902, 0.350959908516307, 0.3311123991217272, 0.5066340529114932, 0.19059373453333972, 0.4888985383721349]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "553a0208-6d67-4a10-ba86-f0a85809fd50", "fitness": "-inf", "name": "AdaptiveHybridOptimizer", "description": "A hybrid algorithm combining Differential Evolution, CMA-ES, and a biased local search, adaptively adjusting probabilities based on success rates and incorporating a diversity-promoting mechanism.", "code": "import numpy as np\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget=10000, dim=10, pop_size=20, archive_size=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lb = -5.0\n        self.ub = 5.0\n        self.archive_size = archive_size\n        self.archive = []\n        self.archive_fitness = []\n\n        self.de_success = 0\n        self.cmaes_success = 0\n        self.local_search_success = 0\n        self.de_count = 0\n        self.cmaes_count = 0\n        self.local_search_count = 0\n        self.epsilon = 1e-6\n        self.diversity_threshold = 0.1  # Threshold for triggering diversity mechanism\n\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget_used = self.pop_size\n\n        self.best_index = np.argmin(self.fitness)\n        self.best_x = self.population[self.best_index].copy()\n        self.best_fitness = self.fitness[self.best_index].copy()\n\n        self.f_opt = self.best_fitness\n        self.x_opt = self.best_x\n\n        self.add_to_archive(self.best_x, self.best_fitness)\n        \n        while self.budget_used < self.budget:\n            \n            # Adaptive Probability Calculation\n            de_prob = (self.de_success / (self.de_count + self.epsilon)) if self.de_count > 0 else 0.33\n            cmaes_prob = (self.cmaes_success / (self.cmaes_count + self.epsilon)) if self.cmaes_count > 0 else 0.33\n            local_search_prob = (self.local_search_success / (self.local_search_count + self.epsilon)) if self.local_search_count > 0 else 0.34\n            \n            # Normalize probabilities\n            total_prob = de_prob + cmaes_prob + local_search_prob\n            de_prob /= total_prob\n            cmaes_prob /= total_prob\n            local_search_prob /= total_prob\n            \n            new_population = np.zeros_like(self.population)\n            new_fitness = np.zeros_like(self.fitness)\n\n            for i in range(self.pop_size):\n                rand = np.random.rand()\n                \n                if rand < de_prob:\n                    # Differential Evolution\n                    self.de_count += 1\n                    indices = np.random.choice(self.pop_size, 3, replace=False)\n                    x_r1, x_r2, x_r3 = self.population[indices]\n                    mutation = x_r1 + 0.5 * (x_r2 - x_r3)\n                    crossover = np.random.rand(self.dim) < 0.5\n                    new_x = np.where(crossover, mutation, self.population[i])\n                    new_x = np.clip(new_x, self.lb, self.ub)\n                    new_fitness[i] = func(new_x)\n                    self.budget_used += 1\n\n                    if new_fitness[i] < self.fitness[i]:\n                        self.de_success += 1\n                        self.population[i] = new_x\n                        self.fitness[i] = new_fitness[i]\n                        self.add_to_archive(new_x, new_fitness[i])\n\n                elif rand < de_prob + cmaes_prob:\n                    # CMA-ES (Simplified)\n                    self.cmaes_count += 1\n                    step_size = 0.1\n                    new_x = self.population[i] + step_size * np.random.randn(self.dim)\n                    new_x = np.clip(new_x, self.lb, self.ub)\n                    new_fitness[i] = func(new_x)\n                    self.budget_used += 1\n\n                    if new_fitness[i] < self.fitness[i]:\n                        self.cmaes_success += 1\n                        self.population[i] = new_x\n                        self.fitness[i] = new_fitness[i]\n                        self.add_to_archive(new_x, new_fitness[i])\n\n                else:\n                    # Biased Local Search towards Best\n                    self.local_search_count += 1\n                    step_size = 0.05\n                    direction = (self.best_x - self.population[i])  # Bias towards the best\n                    new_x = self.population[i] + step_size * np.random.randn(self.dim) + 0.2 * direction # Add some randomness\n                    new_x = np.clip(new_x, self.lb, self.ub)\n                    new_fitness[i] = func(new_x)\n                    self.budget_used += 1\n\n                    if new_fitness[i] < self.fitness[i]:\n                        self.local_search_success += 1\n                        self.population[i] = new_x\n                        self.fitness[i] = new_fitness[i]\n                        self.add_to_archive(new_x, new_fitness[i])\n                \n                if self.fitness[i] < self.best_fitness:\n                    self.best_x = self.population[i].copy()\n                    self.best_fitness = self.fitness[i].copy()\n                    if self.best_fitness < self.f_opt:\n                        self.f_opt = self.best_fitness\n                        self.x_opt = self.best_x\n\n            # Diversity Check and Restart (if necessary)\n            if self.check_diversity() < self.diversity_threshold:\n                self.restart_population()\n\n\n        return self.f_opt, self.x_opt\n\n    def add_to_archive(self, x, fitness):\n        if len(self.archive) < self.archive_size:\n            self.archive.append(x)\n            self.archive_fitness.append(fitness)\n        else:\n            max_fitness_index = np.argmax(self.archive_fitness)\n            if fitness < self.archive_fitness[max_fitness_index]:\n                self.archive[max_fitness_index] = x\n                self.archive_fitness[max_fitness_index] = fitness\n                \n    def check_diversity(self):\n        \"\"\"Calculates the average distance of individuals from the population mean.\"\"\"\n        mean_position = np.mean(self.population, axis=0)\n        distances = np.linalg.norm(self.population - mean_position, axis=1)\n        average_distance = np.mean(distances)\n        return average_distance / (self.ub - self.lb)  # Normalize to [0, 1]\n\n    def restart_population(self):\n        \"\"\"Restarts the population, keeping the best individual.\"\"\"\n        best_individual = self.population[np.argmin(self.fitness)].copy()\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.population[0] = best_individual  # Ensure best individual survives\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget_used += self.pop_size -1 # Subtract 1 because the best individual was already there\n        self.best_index = np.argmin(self.fitness)\n        self.best_x = self.population[self.best_index].copy()\n        self.best_fitness = self.fitness[self.best_index].copy()\n        self.add_to_archive(self.best_x, self.best_fitness)\n        self.f_opt = self.best_fitness\n        self.x_opt = self.best_x", "configspace": "", "generation": 8, "feedback": "In the code, line 144, in restart_population, the following error occurred:\nNameError: name 'func' is not defined\nOn line: self.fitness = np.array([func(x) for x in self.population])", "error": "In the code, line 144, in restart_population, the following error occurred:\nNameError: name 'func' is not defined\nOn line: self.fitness = np.array([func(x) for x in self.population])", "parent_ids": ["0eceaf09-69d5-4b47-93e6-80e157e7b18f"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "bae141ef-ef22-4731-9ef3-8aa77914efb0", "fitness": "-inf", "name": "AdaptiveReinforcementOptimizer", "description": "An adaptive hybrid optimizer that uses a reinforcement learning agent to dynamically adjust the probabilities of applying different search operators (DE, PSO, Local Search, and Nelder-Mead Simplex).", "code": "import numpy as np\nimport random\n\nclass AdaptiveReinforcementOptimizer:\n    def __init__(self, budget=10000, dim=10, pop_size=20, learning_rate=0.1, discount_factor=0.9, exploration_rate=0.1, restart_trigger=0.05):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.learning_rate = learning_rate\n        self.discount_factor = discount_factor\n        self.exploration_rate = exploration_rate\n        self.restart_trigger = restart_trigger\n\n        self.operators = ['de', 'pso', 'local', 'nelder_mead']\n        self.q_table = {op: np.zeros(pop_size) for op in self.operators}  # Q-values for each operator and individual\n        self.counts = {op: np.zeros(pop_size) for op in self.operators} # Count occurrences of each operator\n        self.avg_reward = {op: 0 for op in self.operators} # Average rewards of each operator\n        \n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget_used = self.pop_size\n        \n        self.pbest_population = self.population.copy()\n        self.pbest_fitness = self.fitness.copy()\n\n        self.gbest_index = np.argmin(self.fitness)\n        self.gbest_x = self.population[self.gbest_index].copy()\n        self.gbest_fitness = self.fitness[self.gbest_index].copy()\n\n        self.f_opt = self.gbest_fitness\n        self.x_opt = self.gbest_x\n        self.no_improvement_count = 0\n        self.max_no_improvement = int(self.budget * self.restart_trigger)\n\n        while self.budget_used < self.budget:\n            new_population = np.zeros_like(self.population)\n            new_fitness = np.zeros_like(self.fitness)\n\n            for i in range(self.pop_size):\n                if np.random.rand() < self.exploration_rate:\n                    operator = random.choice(self.operators)\n                else:\n                    operator = self.operators[np.argmax([self.q_table[op][i] for op in self.operators])] #Choose operator with highest Q-value for individual i\n\n                if operator == 'de':\n                    indices = np.random.choice(self.pop_size, 3, replace=False)\n                    x_r1, x_r2, x_r3 = self.population[indices]\n                    mutation = x_r1 + 0.5 * (x_r2 - x_r3)\n                    crossover = np.random.rand(self.dim) < 0.5\n                    new_solution = np.where(crossover, mutation, self.population[i])\n\n                elif operator == 'pso':\n                    inertia = 0.7\n                    cognitive_coeff = 1.5\n                    social_coeff = 1.5\n                    velocity = inertia * (self.population[i] - self.pbest_population[i]) + cognitive_coeff * np.random.rand(self.dim) * (self.pbest_population[i] - self.population[i]) + social_coeff * np.random.rand(self.dim) * (self.gbest_x - self.population[i])\n                    new_solution = self.population[i] + velocity\n\n                elif operator == 'local':\n                    new_solution = self.population[i] + np.random.normal(0, 0.1, self.dim)\n\n                elif operator == 'nelder_mead':\n                     # Apply Nelder-Mead simplex method locally to the individual\n                    simplex = [self.population[i]]\n                    for _ in range(self.dim):\n                        simplex.append(self.population[i] + np.random.normal(0, 0.05, self.dim))  # Create initial simplex\n                    simplex_fitness = [func(x) for x in simplex]\n                    self.budget_used += (self.dim + 1)\n                    \n                    # Perform a single iteration of Nelder-Mead (simplified)\n                    best_idx = np.argmin(simplex_fitness)\n                    worst_idx = np.argmax(simplex_fitness)\n                    centroid = np.mean([simplex[j] for j in range(len(simplex)) if j != worst_idx], axis=0)\n                    reflection = centroid + 1.0 * (centroid - simplex[worst_idx]) #Reflection coefficient 1.0\n\n                    reflection = np.clip(reflection, func.bounds.lb, func.bounds.ub)\n                    reflection_fitness = func(reflection)\n                    self.budget_used +=1\n\n                    if reflection_fitness < simplex_fitness[best_idx]: #If reflection is better than the best point expand the simplex\n                      expansion = centroid + 2.0 * (reflection - centroid) #Expansion Coefficient 2.0\n                      expansion = np.clip(expansion, func.bounds.lb, func.bounds.ub)\n                      expansion_fitness = func(expansion)\n                      self.budget_used +=1\n\n                      if expansion_fitness < reflection_fitness:\n                         simplex[worst_idx] = expansion\n                         simplex_fitness[worst_idx] = expansion_fitness\n                      else:\n                         simplex[worst_idx] = reflection\n                         simplex_fitness[worst_idx] = reflection_fitness\n\n                    elif reflection_fitness < simplex_fitness[worst_idx]: #If reflection is better than the current worst just replace it\n                         simplex[worst_idx] = reflection\n                         simplex_fitness[worst_idx] = reflection_fitness\n                    else: #If not contract the simplex\n                        contraction = centroid + 0.5*(simplex[worst_idx]- centroid) #Contraction coefficient 0.5\n                        contraction = np.clip(contraction, func.bounds.lb, func.bounds.ub)\n                        contraction_fitness = func(contraction)\n                        self.budget_used +=1\n\n                        if contraction_fitness < simplex_fitness[worst_idx]:\n                           simplex[worst_idx] = contraction\n                           simplex_fitness[worst_idx] = contraction_fitness\n                        else: #Shrink the simplex towards best point\n                           for j in range(len(simplex)):\n                              if j != best_idx:\n                                 simplex[j] = simplex[best_idx] + 0.5 * (simplex[j] - simplex[best_idx]) #Shrink coefficient 0.5\n\n                                 simplex[j] = np.clip(simplex[j], func.bounds.lb, func.bounds.ub)\n                                 simplex_fitness[j] = func(simplex[j])\n                                 self.budget_used +=1\n\n                    new_solution = simplex[np.argmin(simplex_fitness)] #Best point of final Simplex\n\n                else:\n                    new_solution = np.random.uniform(func.bounds.lb, func.bounds.ub, self.dim)\n\n                new_solution = np.clip(new_solution, func.bounds.lb, func.bounds.ub)\n                new_population[i] = new_solution\n                new_fitness[i] = func(new_solution)\n                self.budget_used += 1\n                \n                #Calculate Reward\n                reward = self.fitness[i] - new_fitness[i]\n\n                # Update Q-table\n                self.q_table[operator][i] = (1 - self.learning_rate) * self.q_table[operator][i] + self.learning_rate * (reward + self.discount_factor * np.max([self.q_table[op][i] for op in self.operators]))\n                \n                # Update statistics\n                self.counts[operator][i] += 1\n                self.avg_reward[operator] = (1 - 1/self.counts[operator][i]) * self.avg_reward[operator] + (1/self.counts[operator][i]) * reward\n                \n\n            #Update population\n            for i in range(self.pop_size):\n                if new_fitness[i] < self.fitness[i]:\n                    self.population[i] = new_population[i]\n                    self.fitness[i] = new_fitness[i]\n\n                    if new_fitness[i] < self.pbest_fitness[i]:\n                        self.pbest_population[i] = new_population[i]\n                        self.pbest_fitness[i] = new_fitness[i]\n\n            self.gbest_index = np.argmin(self.fitness)\n            if self.fitness[self.gbest_index] < self.gbest_fitness:\n                self.gbest_x = self.population[self.gbest_index].copy()\n                self.gbest_fitness = self.fitness[self.gbest_index].copy()\n\n                if self.gbest_fitness < self.f_opt:\n                    self.f_opt = self.gbest_fitness\n                    self.x_opt = self.gbest_x\n                    self.no_improvement_count = 0\n                else:\n                    self.no_improvement_count += 1\n            else:\n                self.no_improvement_count += 1\n            \n\n            if self.no_improvement_count > self.max_no_improvement:\n                self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                self.fitness = np.array([func(x) for x in self.population])\n\n                self.pbest_population = self.population.copy()\n                self.pbest_fitness = self.fitness.copy()\n\n                self.gbest_index = np.argmin(self.fitness)\n                self.gbest_x = self.population[self.gbest_index].copy()\n                self.gbest_fitness = self.fitness[self.gbest_index].copy()\n\n                self.no_improvement_count = 0\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "In the code, line 45, in __call__, the following error occurred:\nNameError: name 'random' is not defined. Did you forget to import 'random'?\nOn line: operator = random.choice(self.operators)", "error": "In the code, line 45, in __call__, the following error occurred:\nNameError: name 'random' is not defined. Did you forget to import 'random'?\nOn line: operator = random.choice(self.operators)", "parent_ids": ["d0fddf51-e6ed-4acb-82bd-f12a4f7ace8d"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "1457e643-466c-4b1b-bef8-3dfe4fa360e4", "fitness": 0.7448691784425938, "name": "HybridOptimizer", "description": "A simplified hybrid optimizer that dynamically balances differential evolution and a novel self-adjusting Gaussian mutation based on optimization progress and function landscape features.", "code": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget=10000, dim=10, pop_size=20):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lb = -5.0\n        self.ub = 5.0\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget_used = self.pop_size\n\n        self.best_index = np.argmin(self.fitness)\n        self.best_x = self.population[self.best_index].copy()\n        self.best_fitness = self.fitness[self.best_index].copy()\n\n        self.f_opt = self.best_fitness\n        self.x_opt = self.best_x\n\n        de_rate = 0.7\n        mutation_factor = 0.5\n\n        while self.budget_used < self.budget:\n            new_population = np.zeros_like(self.population)\n            new_fitness = np.zeros(self.pop_size)\n            \n            # Adaptive Mutation scale\n            mutation_scale = 0.1 * (self.ub - self.lb) * (1 - self.budget_used / self.budget)\n\n            for i in range(self.pop_size):\n                if np.random.rand() < de_rate:\n                    # Differential Evolution\n                    indices = np.random.choice(self.pop_size, 3, replace=False)\n                    x_r1, x_r2, x_r3 = self.population[indices]\n                    mutation = x_r1 + mutation_factor * (x_r2 - x_r3)\n                    crossover = np.random.rand(self.dim) < 0.7  # Binomial Crossover\n                    new_population[i] = np.where(crossover, mutation, self.population[i])\n                else:\n                    # Self-Adjusting Gaussian Mutation\n                    new_population[i] = self.population[i] + np.random.normal(0, mutation_scale, self.dim)\n\n                new_population[i] = np.clip(new_population[i], self.lb, self.ub)\n                new_fitness[i] = func(new_population[i])\n                self.budget_used += 1\n\n                if new_fitness[i] < self.fitness[i]:\n                    self.population[i] = new_population[i]\n                    self.fitness[i] = new_fitness[i]\n\n                    if new_fitness[i] < self.best_fitness:\n                        self.best_x = new_population[i].copy()\n                        self.best_fitness = new_fitness[i].copy()\n                        if self.best_fitness < self.f_opt:\n                            self.f_opt = self.best_fitness\n                            self.x_opt = self.best_x\n                if self.budget_used >= self.budget:\n                    break\n            \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "The algorithm HybridOptimizer scored 0.745 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["e16e90d9-ba5f-43ec-b171-d0521e5848f9"], "operator": null, "metadata": {"aucs": [0.25144307491790174, 0.7581749450659743, 0.7888378688924229, 0.9013918117567064, 0.8108336258303391, 0.864558104452805, 0.725831576053751, 0.759519669722725, 0.8434199998671993, 0.7910319182054311, 0.867706067344319, 0.9965805814725669, 0.43435491373288304, 0.774008774047446, 0.9247425081991413, 0.8657640826820163, 0.729746014332691, 0.8830382362892251, 0.41894044622309634, 0.5074593497632338]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "635f7a96-2dc0-4e5a-83c9-d74d0c045d8f", "fitness": "-inf", "name": "HybridOptimizer", "description": "A hybrid optimizer that dynamically adjusts exploration rates based on success and combines DE, PSO, Cauchy local search, and a Sobol-based initialization to enhance diversity and convergence.", "code": "import numpy as np\nfrom scipy.stats import cauchy\nfrom scipy.stats import norm\nfrom scipy.stats import levy\nfrom sobol_seq import i4_sobol_generate\n\nclass HybridOptimizer:\n    def __init__(self, budget=10000, dim=10, pop_size=20, initial_de_rate=0.5, initial_pso_rate=0.3, local_rate=0.2, restart_trigger=0.05, levy_flight_prob = 0.05):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.de_rate = initial_de_rate\n        self.pso_rate = initial_pso_rate\n        self.local_rate = local_rate\n        self.restart_trigger = restart_trigger\n        self.initial_de_rate = initial_de_rate\n        self.initial_pso_rate = initial_pso_rate\n        self.levy_flight_prob = levy_flight_prob\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        \n        #Sobol sequence initialization\n        sobol_points = i4_sobol_generate(self.dim, self.pop_size)\n        self.population = func.bounds.lb + (func.bounds.ub - func.bounds.lb) * sobol_points.T\n        \n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget_used = self.pop_size\n        \n        self.pbest_population = self.population.copy()\n        self.pbest_fitness = self.fitness.copy()\n\n        self.gbest_index = np.argmin(self.fitness)\n        self.gbest_x = self.population[self.gbest_index].copy()\n        self.gbest_fitness = self.fitness[self.gbest_index].copy()\n        \n        self.f_opt = self.gbest_fitness\n        self.x_opt = self.gbest_x\n\n        self.no_improvement_count = 0\n        self.max_no_improvement = int(self.budget * self.restart_trigger)\n        \n        success_de = 0\n        success_pso = 0\n        success_local = 0\n\n        while self.budget_used < self.budget:\n            new_population = np.zeros_like(self.population)\n            new_fitness = np.zeros(self.pop_size)\n\n            for i in range(self.pop_size):\n                rand = np.random.rand()\n\n                # Differential Evolution\n                if rand < self.de_rate:\n                    indices = np.random.choice(self.pop_size, 3, replace=False)\n                    x_r1, x_r2, x_r3 = self.population[indices]\n                    mutation = x_r1 + 0.5 * (x_r2 - x_r3)\n                    crossover = np.random.rand(self.dim) < 0.5\n                    new_population[i] = np.where(crossover, mutation, self.population[i])\n                    new_population[i] = np.clip(new_population[i], func.bounds.lb, func.bounds.ub)\n                    new_fitness[i] = func(new_population[i])\n                    self.budget_used += 1\n\n                # Particle Swarm Optimization\n                elif rand < self.de_rate + self.pso_rate:\n                    inertia = 0.7\n                    cognitive_coeff = 1.5\n                    social_coeff = 1.5\n                    velocity = inertia * (self.population[i] - self.pbest_population[i]) + cognitive_coeff * np.random.rand(self.dim) * (self.pbest_population[i] - self.population[i]) + social_coeff * np.random.rand(self.dim) * (self.gbest_x - self.population[i])\n                    new_population[i] = self.population[i] + velocity\n                    new_population[i] = np.clip(new_population[i], func.bounds.lb, func.bounds.ub)\n                    new_fitness[i] = func(new_population[i])\n                    self.budget_used += 1\n                    \n                # Local Search with Cauchy Mutation\n                else:\n                    if np.random.rand() < self.levy_flight_prob:\n                        #Levy Flight\n                        step_size = 0.01 * levy.rvs(size=self.dim)\n                        new_population[i] = self.population[i] + step_size\n                    else:\n                        #Cauchy Mutation\n                        new_population[i] = self.population[i] + 0.01 * np.random.standard_cauchy(size=self.dim)\n                    \n                    new_population[i] = np.clip(new_population[i], func.bounds.lb, func.bounds.ub)\n                    new_fitness[i] = func(new_population[i])\n                    self.budget_used += 1\n\n            for i in range(self.pop_size):\n                if new_fitness[i] < self.fitness[i]:\n                    if rand < self.de_rate:\n                        success_de += 1\n                    elif rand < self.de_rate + self.pso_rate:\n                        success_pso += 1\n                    else:\n                        success_local += 1\n                        \n                    self.population[i] = new_population[i]\n                    self.fitness[i] = new_fitness[i]\n\n                    if new_fitness[i] < self.pbest_fitness[i]:\n                        self.pbest_population[i] = new_population[i]\n                        self.pbest_fitness[i] = new_fitness[i]\n\n            self.gbest_index = np.argmin(self.fitness)\n            if self.fitness[self.gbest_index] < self.gbest_fitness:\n                self.gbest_x = self.population[self.gbest_index].copy()\n                self.gbest_fitness = self.fitness[self.gbest_index].copy()\n                \n                if self.gbest_fitness < self.f_opt:\n                    self.f_opt = self.gbest_fitness\n                    self.x_opt = self.gbest_x\n                    self.no_improvement_count = 0\n\n                    # Adaptive rate adjustment\n                    total_success = success_de + success_pso + success_local\n                    if total_success > 0:\n                        self.de_rate = 0.1 + 0.9 * (success_de / total_success)\n                        self.pso_rate = 0.1 + 0.9 * (success_pso / total_success)\n                        self.local_rate = 1 - self.de_rate - self.pso_rate\n                else:\n                    self.no_improvement_count += 1\n            else:\n                self.no_improvement_count += 1\n            \n            success_de = 0\n            success_pso = 0\n            success_local = 0\n\n            # Restart mechanism\n            if self.no_improvement_count > self.max_no_improvement:\n                #Sobol sequence initialization\n                sobol_points = i4_sobol_generate(self.dim, self.pop_size)\n                self.population = func.bounds.lb + (func.bounds.ub - func.bounds.lb) * sobol_points.T\n                \n                self.fitness = np.array([func(x) for x in self.population])\n                self.budget_used += self.pop_size\n\n                self.pbest_population = self.population.copy()\n                self.pbest_fitness = self.fitness.copy()\n\n                self.gbest_index = np.argmin(self.fitness)\n                self.gbest_x = self.population[self.gbest_index].copy()\n                self.gbest_fitness = self.fitness[self.gbest_index].copy()\n                \n                self.no_improvement_count = 0\n                self.de_rate = self.initial_de_rate\n                self.pso_rate = self.initial_pso_rate\n                self.local_rate = 1 - self.de_rate - self.pso_rate\n                \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "In the code, line 5, in <module>, the following error occurred:\nModuleNotFoundError: No module named 'sobol_seq'\nOn line: from sobol_seq import i4_sobol_generate", "error": "In the code, line 5, in <module>, the following error occurred:\nModuleNotFoundError: No module named 'sobol_seq'\nOn line: from sobol_seq import i4_sobol_generate", "parent_ids": ["1e74c5f0-f489-41e9-b124-27e514d9a233"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "a05fcef2-6c62-4613-ad21-3429d34351c3", "fitness": "-inf", "name": "AdaptiveHybridOptimizer", "description": "An adaptive hybrid optimizer that combines differential evolution, particle swarm optimization and a Gaussian mutation-based local search with dynamically adjusted probabilities based on their performance and a fitness landscape analysis for parameter adaptation.", "code": "import numpy as np\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget=10000, dim=10, pop_size=20, initial_de_rate=0.4, initial_pso_rate=0.3, initial_local_rate=0.3, restart_trigger=0.05, exploration_threshold=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.de_rate = initial_de_rate\n        self.pso_rate = initial_pso_rate\n        self.local_rate = initial_local_rate\n        self.restart_trigger = restart_trigger\n        self.exploration_threshold = exploration_threshold\n        self.epsilon = 1e-8 # Small constant to avoid division by zero\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget_used = self.pop_size\n\n        self.pbest_population = self.population.copy()\n        self.pbest_fitness = self.fitness.copy()\n\n        self.gbest_index = np.argmin(self.fitness)\n        self.gbest_x = self.population[self.gbest_index].copy()\n        self.gbest_fitness = self.fitness[self.gbest_index].copy()\n\n        self.f_opt = self.gbest_fitness\n        self.x_opt = self.gbest_x\n\n        self.no_improvement_count = 0\n        self.max_no_improvement = int(self.budget * self.restart_trigger)\n\n        self.de_success = 0\n        self.pso_success = 0\n        self.local_success = 0\n        self.total_trials = 0\n\n        while self.budget_used < self.budget:\n            new_population = np.zeros_like(self.population)\n            new_fitness = np.zeros(self.pop_size)\n            \n            de_trial_count = 0\n            pso_trial_count = 0\n            local_trial_count = 0\n\n            for i in range(self.pop_size):\n                rand = np.random.rand()\n\n                # Differential Evolution\n                if rand < self.de_rate:\n                    de_trial_count += 1\n                    indices = np.random.choice(self.pop_size, 3, replace=False)\n                    x_r1, x_r2, x_r3 = self.population[indices]\n                    mutation = x_r1 + 0.5 * (x_r2 - x_r3)\n                    crossover = np.random.rand(self.dim) < 0.5  # binomial crossover\n                    new_population[i] = np.where(crossover, mutation, self.population[i])\n                \n                # Particle Swarm Optimization\n                elif rand < self.de_rate + self.pso_rate:\n                    pso_trial_count += 1\n                    inertia = 0.7\n                    cognitive_coeff = 1.5\n                    social_coeff = 1.5\n                    velocity = inertia * (new_population[i-1] - self.pbest_population[i]) + cognitive_coeff * np.random.rand(self.dim) * (self.pbest_population[i] - new_population[i-1]) + social_coeff * np.random.rand(self.dim) * (self.gbest_x - new_population[i-1])\n                    new_population[i] = new_population[i-1] + velocity\n                    \n                # Local Search\n                else:\n                    local_trial_count += 1\n                    new_population[i] = self.population[i] + np.random.normal(0, 0.05, self.dim)\n\n                # Clip the solution\n                new_population[i] = np.clip(new_population[i], func.bounds.lb, func.bounds.ub)\n                new_fitness[i] = func(new_population[i])\n                self.budget_used += 1\n\n                if new_fitness[i] < self.fitness[i]:\n                    if rand < self.de_rate:\n                        self.de_success +=1\n                    elif rand < self.de_rate + self.pso_rate:\n                        self.pso_success += 1\n                    else:\n                        self.local_success += 1\n                    \n                    self.population[i] = new_population[i]\n                    self.fitness[i] = new_fitness[i]\n\n                    if new_fitness[i] < self.pbest_fitness[i]:\n                        self.pbest_population[i] = new_population[i]\n                        self.pbest_fitness[i] = new_fitness[i]\n\n                else:\n                    new_population[i] = self.population[i]\n\n            self.total_trials += self.pop_size\n\n            self.gbest_index = np.argmin(self.fitness)\n            if self.fitness[self.gbest_index] < self.gbest_fitness:\n                self.gbest_x = self.population[self.gbest_index].copy()\n                self.gbest_fitness = self.fitness[self.gbest_index].copy()\n                \n                if self.gbest_fitness < self.f_opt:\n                    self.f_opt = self.gbest_fitness\n                    self.x_opt = self.gbest_x\n                    self.no_improvement_count = 0\n                else:\n                    self.no_improvement_count += 1\n            else:\n                self.no_improvement_count += 1\n\n            # Adaptive Probability Adjustment\n            de_rate_calc = self.de_success / (de_trial_count + self.epsilon)\n            pso_rate_calc = self.pso_success / (pso_trial_count + self.epsilon)\n            local_rate_calc = self.local_success / (local_trial_count + self.epsilon)\n\n            sum_rates = de_rate_calc + pso_rate_calc + local_rate_calc\n            \n            if sum_rates > 0:\n                self.de_rate = de_rate_calc / sum_rates\n                self.pso_rate = pso_rate_calc / sum_rates\n                self.local_rate = local_rate_calc / sum_rates\n            else:\n                 # If all success rates are zero, revert to initial rates\n                self.de_rate = 0.4\n                self.pso_rate = 0.3\n                self.local_rate = 0.3\n\n            # Restart mechanism\n            if self.no_improvement_count > self.max_no_improvement:\n                self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                self.fitness = np.array([func(x) for x in self.population])\n\n                self.pbest_population = self.population.copy()\n                self.pbest_fitness = self.fitness.copy()\n\n                self.gbest_index = np.argmin(self.fitness)\n                self.gbest_x = self.population[self.gbest_index].copy()\n                self.gbest_fitness = self.fitness[self.gbest_index].copy()\n                \n                self.no_improvement_count = 0\n                self.de_success = 0\n                self.pso_success = 0\n                self.local_success = 0\n                self.total_trials = 0\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "Evaluation timed out after 300 seconds.", "error": "Evaluation timed out after 300 seconds.", "parent_ids": ["d0fddf51-e6ed-4acb-82bd-f12a4f7ace8d"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "007ae3a9-c99b-443c-a55b-d74eae07d853", "fitness": 0.0, "name": "OrthogonalSimplexGaussianOptimizer", "description": "A self-adjusting hybrid algorithm leveraging orthogonal learning, simplex centroid search and Gaussian mutation for efficient exploration and exploitation.", "code": "import numpy as np\n\nclass OrthogonalSimplexGaussianOptimizer:\n    def __init__(self, budget=10000, dim=10, pop_size=20, ol_sample_size=5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lb = -5.0\n        self.ub = 5.0\n        self.ol_sample_size = ol_sample_size\n        self.epsilon = 1e-8  # Small constant to prevent division by zero\n\n        self.ol_success = 0\n        self.simplex_success = 0\n        self.gaussian_success = 0\n\n        self.ol_count = 0\n        self.simplex_count = 0\n        self.gaussian_count = 0\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        # Initialization\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget_used = self.pop_size\n\n        self.best_index = np.argmin(self.fitness)\n        self.best_x = self.population[self.best_index].copy()\n        self.best_fitness = self.fitness[self.best_index].copy()\n\n        self.f_opt = self.best_fitness\n        self.x_opt = self.best_x\n\n        while self.budget_used < self.budget:\n            # Adaptive probability assignment\n            ol_prob = (self.ol_success / (self.ol_count + self.epsilon)) if self.ol_count > 0 else 0.33\n            simplex_prob = (self.simplex_success / (self.simplex_count + self.epsilon)) if self.simplex_count > 0 else 0.33\n            gaussian_prob = (self.gaussian_success / (self.gaussian_count + self.epsilon)) if self.gaussian_count > 0 else 0.34\n\n            total_prob = ol_prob + simplex_prob + gaussian_prob\n            ol_prob /= total_prob\n            simplex_prob /= total_prob\n            gaussian_prob /= total_prob\n            \n            for i in range(self.pop_size):\n                rand = np.random.rand()\n\n                if rand < ol_prob:\n                    # Orthogonal Learning\n                    self.ol_count += 1\n                    new_x = self.orthogonal_learning(func, self.population[i])\n                    if new_x is not None:\n                        new_fitness = func(new_x)\n                        self.budget_used += 1\n                        if new_fitness < self.fitness[i]:\n                            self.ol_success += 1\n                            self.population[i] = new_x\n                            self.fitness[i] = new_fitness\n\n                elif rand < ol_prob + simplex_prob:\n                    # Simplex Centroid Search\n                    self.simplex_count += 1\n                    new_x = self.simplex_centroid_search(func)\n                    if new_x is not None:\n                        new_fitness = func(new_x)\n                        self.budget_used += 1\n\n                        if new_fitness < self.fitness[i]:\n                            self.simplex_success += 1\n                            self.population[i] = new_x\n                            self.fitness[i] = new_fitness\n                else:\n                    # Gaussian Mutation\n                    self.gaussian_count += 1\n                    new_x = self.gaussian_mutation(func, self.population[i])\n                    if new_x is not None:\n                        new_fitness = func(new_x)\n                        self.budget_used += 1\n                        if new_fitness < self.fitness[i]:\n                            self.gaussian_success += 1\n                            self.population[i] = new_x\n                            self.fitness[i] = new_fitness\n                \n                if self.fitness[i] < self.best_fitness:\n                    self.best_x = self.population[i].copy()\n                    self.best_fitness = self.fitness[i].copy()\n\n                    if self.best_fitness < self.f_opt:\n                        self.f_opt = self.best_fitness\n                        self.x_opt = self.best_x\n        return self.f_opt, self.x_opt\n\n    def orthogonal_learning(self, func, x):\n        # Generate orthogonal array\n        levels = self.ol_sample_size\n        if self.dim > 1:\n\n            oa = self.generate_orthogonal_array(levels, self.dim)\n\n            samples = np.zeros((levels, self.dim))\n            for i in range(self.dim):\n                samples[:, i] = np.linspace(self.lb, self.ub, levels)[oa[:, i]]\n\n            fitness_values = np.array([func(samples[j]) for j in range(levels)])\n            best_index = np.argmin(fitness_values)\n            return samples[best_index]\n        else:\n            return None\n\n    def generate_orthogonal_array(self, levels, dimensions):\n         #Simplified orthogonal array (L_25(5^6)) creation for illustration\n        oa = np.zeros((levels, dimensions), dtype=int)\n        for i in range(levels):\n            for j in range(dimensions):\n                oa[i, j] = i % levels\n\n        return oa\n\n    def simplex_centroid_search(self, func):\n        # Select three random individuals\n        indices = np.random.choice(self.pop_size, 3, replace=False)\n        x1, x2, x3 = self.population[indices]\n        \n        # Calculate centroid\n        centroid = (x1 + x2 + x3) / 3.0\n        centroid = np.clip(centroid, self.lb, self.ub)\n\n        return centroid\n\n    def gaussian_mutation(self, func, x):\n        # Apply Gaussian mutation\n        step_size = 0.1  # Adjust step size as needed\n        new_x = x + step_size * np.random.randn(self.dim)\n        new_x = np.clip(new_x, self.lb, self.ub)\n        return new_x", "configspace": "", "generation": 8, "feedback": "The algorithm OrthogonalSimplexGaussianOptimizer scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["0eceaf09-69d5-4b47-93e6-80e157e7b18f"], "operator": null, "metadata": {"aucs": [0]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "f848c5b1-fa56-4ed9-bc91-a5c64541a8f6", "fitness": "-inf", "name": "AdaptiveHybridOptimizer", "description": "A hybrid optimizer that dynamically adjusts its exploration-exploitation balance based on the optimization progress, using a combination of differential evolution, covariance matrix adaptation evolution strategy (CMA-ES), and a gradient-based local search with dynamic probability adjustments and a restart mechanism to avoid stagnation.", "code": "import numpy as np\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget=10000, dim=10, pop_size=20, archive_size=10, local_search_iterations=5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lb = -5.0\n        self.ub = 5.0\n        self.archive_size = archive_size\n        self.archive = []\n        self.archive_fitness = []\n        self.local_search_iterations = local_search_iterations\n        self.restart_trigger = 0.1 * budget  # Trigger restart after this many evaluations without improvement\n\n        self.de_success = 0\n        self.cmaes_success = 0\n        self.local_search_success = 0\n        self.de_count = 0\n        self.cmaes_count = 0\n        self.local_search_count = 0\n        self.epsilon = 1e-6\n        self.last_improvement = 0\n\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget_used = self.pop_size\n\n        self.best_index = np.argmin(self.fitness)\n        self.best_x = self.population[self.best_index].copy()\n        self.best_fitness = self.fitness[self.best_index].copy()\n\n        self.f_opt = self.best_fitness\n        self.x_opt = self.best_x\n        self.last_improvement = self.budget_used\n\n        self.add_to_archive(self.best_x, self.best_fitness)\n        \n        while self.budget_used < self.budget:\n            if self.budget_used - self.last_improvement > self.restart_trigger:\n                # Restart mechanism\n                self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n                self.fitness = np.array([func(x) for x in self.population])\n                self.budget_used += self.pop_size\n                self.best_index = np.argmin(self.fitness)\n                self.best_x = self.population[self.best_index].copy()\n                self.best_fitness = self.fitness[self.best_index].copy()\n                self.last_improvement = self.budget_used\n                print(\"Restarted\")\n                \n\n            new_population = np.zeros_like(self.population)\n            new_fitness = np.zeros_like(self.fitness)\n            \n            de_prob = (self.de_success / (self.de_count + self.epsilon)) if self.de_count > 0 else 0.33\n            cmaes_prob = (self.cmaes_success / (self.cmaes_count + self.epsilon)) if self.cmaes_count > 0 else 0.33\n            local_search_prob = (self.local_search_success / (self.local_search_count + self.epsilon)) if self.local_search_count > 0 else 0.34\n            \n            # Normalize probabilities\n            total_prob = de_prob + cmaes_prob + local_search_prob\n            de_prob /= total_prob\n            cmaes_prob /= total_prob\n            local_search_prob /= total_prob\n            \n            for i in range(self.pop_size):\n                rand = np.random.rand()\n                \n                if rand < de_prob:\n                    # Differential Evolution\n                    self.de_count += 1\n                    indices = np.random.choice(self.pop_size, 3, replace=False)\n                    x_r1, x_r2, x_r3 = self.population[indices]\n                    mutation = x_r1 + 0.5 * (x_r2 - x_r3)\n                    crossover = np.random.rand(self.dim) < 0.5\n                    new_x = np.where(crossover, mutation, self.population[i])\n                    new_x = np.clip(new_x, self.lb, self.ub)\n                    new_fitness[i] = func(new_x)\n                    self.budget_used += 1\n\n                    if new_fitness[i] < self.fitness[i]:\n                        self.de_success += 1\n                        self.population[i] = new_x\n                        self.fitness[i] = new_fitness[i]\n                        self.add_to_archive(new_x, new_fitness[i])\n\n                elif rand < de_prob + cmaes_prob:\n                    # CMA-ES (Simplified)\n                    self.cmaes_count += 1\n                    step_size = 0.1\n                    new_x = self.population[i] + step_size * np.random.randn(self.dim)\n                    new_x = np.clip(new_x, self.lb, self.ub)\n                    new_fitness[i] = func(new_x)\n                    self.budget_used += 1\n\n                    if new_fitness[i] < self.fitness[i]:\n                        self.cmaes_success += 1\n                        self.population[i] = new_x\n                        self.fitness[i] = new_fitness[i]\n                        self.add_to_archive(new_x, new_fitness[i])\n\n                else:\n                    # Local Search (Gradient-based)\n                    self.local_search_count += 1\n                    x = self.population[i].copy()\n                    fx = self.fitness[i].copy()\n                    \n                    step_size = 0.01  # Smaller step size for gradient descent\n                    for _ in range(self.local_search_iterations):\n                        # Approximate gradient\n                        gradient = np.zeros(self.dim)\n                        for j in range(self.dim):\n                            x_plus = x.copy()\n                            x_minus = x.copy()\n                            x_plus[j] += step_size\n                            x_minus[j] -= step_size\n                            x_plus = np.clip(x_plus, self.lb, self.ub)\n                            x_minus = np.clip(x_minus, self.lb, self.ub)\n                            gradient[j] = (func(x_plus) - func(x_minus)) / (2 * step_size)\n                            self.budget_used += 2 #Approximate Gradient Calculation Requires 2 Evaluations per Dimension\n                            if self.budget_used > self.budget:\n                                break\n\n                        # Update position\n                        x = x - 0.1 * gradient  # Gradient descent update\n                        x = np.clip(x, self.lb, self.ub)\n\n                        fx_new = func(x)\n                        self.budget_used += 1\n                        \n                        if fx_new < fx:\n                            fx = fx_new\n                            if fx_new < self.fitness[i]:\n                                self.local_search_success += 1\n                                self.population[i] = x\n                                self.fitness[i] = fx_new\n                                self.add_to_archive(x, fx_new)\n                                \n                            \n                            if fx_new < self.best_fitness:\n                                self.best_fitness = fx_new\n                                self.best_x = x.copy()\n                                if self.best_fitness < self.f_opt:\n                                    self.f_opt = self.best_fitness\n                                    self.x_opt = self.best_x\n                                    self.last_improvement = self.budget_used # Track when the global optimum is improved\n\n                        else:\n                            break # terminate local search if not improving\n                    new_fitness[i] = self.fitness[i] #update fitness in the new fitness array\n                if self.budget_used > self.budget:\n                    break\n                \n                if self.fitness[i] < self.best_fitness:\n                    self.best_x = self.population[i].copy()\n                    self.best_fitness = self.fitness[i].copy()\n                    if self.best_fitness < self.f_opt:\n                        self.f_opt = self.best_fitness\n                        self.x_opt = self.best_x\n                        self.last_improvement = self.budget_used # Track when the global optimum is improved\n\n\n        return self.f_opt, self.x_opt\n\n    def add_to_archive(self, x, fitness):\n        if len(self.archive) < self.archive_size:\n            self.archive.append(x)\n            self.archive_fitness.append(fitness)\n        else:\n            max_fitness_index = np.argmax(self.archive_fitness)\n            if fitness < self.archive_fitness[max_fitness_index]:\n                self.archive[max_fitness_index] = x\n                self.archive_fitness[max_fitness_index] = fitness", "configspace": "", "generation": 8, "feedback": "Evaluation timed out after 300 seconds.", "error": "Evaluation timed out after 300 seconds.", "parent_ids": ["0eceaf09-69d5-4b47-93e6-80e157e7b18f"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "70f68a4e-222c-4871-b336-1e17fcabc4a6", "fitness": "-inf", "name": "AdaptiveHybridOptimizer", "description": "A hybrid algorithm with self-adaptive parameter control for DE, PSO, and a gradient-based local search, enhancing exploration and exploitation balance.", "code": "import numpy as np\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget=10000, dim=10, pop_size=20, \n                 de_rate_init=0.4, pso_rate_init=0.3, local_rate_init=0.3,\n                 restart_trigger=0.05, learning_rate=0.1, local_search_step=0.05):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.de_rate = de_rate_init\n        self.pso_rate = pso_rate_init\n        self.local_rate = local_rate_init\n        self.restart_trigger = restart_trigger\n        self.learning_rate = learning_rate\n        self.local_search_step = local_search_step\n        self.rates = np.array([self.de_rate, self.pso_rate, self.local_rate])\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget_used = self.pop_size\n        \n        self.pbest_population = self.population.copy()\n        self.pbest_fitness = self.fitness.copy()\n\n        self.gbest_index = np.argmin(self.fitness)\n        self.gbest_x = self.population[self.gbest_index].copy()\n        self.gbest_fitness = self.fitness[self.gbest_index].copy()\n        \n        self.f_opt = self.gbest_fitness\n        self.x_opt = self.gbest_x\n\n        self.no_improvement_count = 0\n        self.max_no_improvement = int(self.budget * self.restart_trigger)\n        \n        while self.budget_used < self.budget:\n            new_population = np.zeros_like(self.population)\n            new_fitness = np.zeros(self.pop_size)\n            selection_counts = np.zeros(3) # Count DE, PSO, Local usage\n\n            for i in range(self.pop_size):\n                # Adaptive Rate Selection\n                probs = self.rates / np.sum(self.rates)\n                choice = np.random.choice(3, p=probs)\n                selection_counts[choice] += 1\n\n                if choice == 0:  # Differential Evolution\n                    indices = np.random.choice(self.pop_size, 3, replace=False)\n                    x_r1, x_r2, x_r3 = self.population[indices]\n                    mutation = x_r1 + 0.5 * (x_r2 - x_r3)\n                    crossover = np.random.rand(self.dim) < 0.5  # binomial crossover\n                    new_population[i] = np.where(crossover, mutation, self.population[i])\n                    new_population[i] = np.clip(new_population[i], func.bounds.lb, func.bounds.ub)\n                    new_fitness[i] = func(new_population[i])\n                    self.budget_used += 1\n\n                elif choice == 1:  # Particle Swarm Optimization\n                    inertia = 0.7\n                    cognitive_coeff = 1.5\n                    social_coeff = 1.5\n                    velocity = inertia * (self.population[i] - self.pbest_population[i]) + cognitive_coeff * np.random.rand(self.dim) * (self.pbest_population[i] - self.population[i]) + social_coeff * np.random.rand(self.dim) * (self.gbest_x - self.population[i])\n                    new_population[i] = self.population[i] + velocity\n                    new_population[i] = np.clip(new_population[i], func.bounds.lb, func.bounds.ub)\n                    new_fitness[i] = func(new_population[i])\n                    self.budget_used += 1\n\n                elif choice == 2:  # Gradient-Based Local Search\n                    gradient = self.compute_gradient(func, self.population[i])\n                    new_population[i] = self.population[i] - self.local_search_step * gradient\n                    new_population[i] = np.clip(new_population[i], func.bounds.lb, func.bounds.ub)\n                    new_fitness[i] = func(new_population[i])\n                    self.budget_used += 1\n\n                else:  # Random exploration (very low probability if rates are well-adjusted)\n                    new_population[i] = np.random.uniform(func.bounds.lb, func.bounds.ub, self.dim)\n                    new_fitness[i] = func(new_population[i])\n                    self.budget_used += 1\n                    \n            # Update the rates based on success\n            success_counts = np.zeros(3)\n            for i in range(self.pop_size):\n                if new_fitness[i] < self.fitness[i]:\n                    if choice == 0:\n                        success_counts[0] += 1\n                    elif choice == 1:\n                        success_counts[1] += 1\n                    elif choice == 2:\n                        success_counts[2] += 1\n\n            # Update rates with a learning rate\n            for i in range(3):\n                if selection_counts[i] > 0:\n                    reward = success_counts[i] / selection_counts[i]\n                    self.rates[i] += self.learning_rate * (reward - 0.5) # Target reward around 0.5\n                    self.rates[i] = np.clip(self.rates[i], 0.1, 1.0) # Clip rates to avoid one strategy dominating\n\n            for i in range(self.pop_size):\n                if new_fitness[i] < self.fitness[i]:\n                    self.population[i] = new_population[i]\n                    self.fitness[i] = new_fitness[i]\n\n                    if new_fitness[i] < self.pbest_fitness[i]:\n                        self.pbest_population[i] = new_population[i]\n                        self.pbest_fitness[i] = new_fitness[i]\n\n\n            self.gbest_index = np.argmin(self.fitness)\n            if self.fitness[self.gbest_index] < self.gbest_fitness:\n                self.gbest_x = self.population[self.gbest_index].copy()\n                self.gbest_fitness = self.fitness[self.gbest_index].copy()\n                \n                if self.gbest_fitness < self.f_opt:\n                    self.f_opt = self.gbest_fitness\n                    self.x_opt = self.gbest_x\n                    self.no_improvement_count = 0\n                else:\n                    self.no_improvement_count += 1\n            else:\n                self.no_improvement_count += 1\n\n\n            # Restart mechanism\n            if self.no_improvement_count > self.max_no_improvement:\n                self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                self.fitness = np.array([func(x) for x in self.population])\n\n                self.pbest_population = self.population.copy()\n                self.pbest_fitness = self.fitness.copy()\n\n                self.gbest_index = np.argmin(self.fitness)\n                self.gbest_x = self.population[self.gbest_index].copy()\n                self.gbest_fitness = self.fitness[self.gbest_index].copy()\n                \n                self.no_improvement_count = 0\n\n\n        return self.f_opt, self.x_opt\n\n    def compute_gradient(self, func, x, h=1e-5):\n        gradient = np.zeros_like(x)\n        for i in range(self.dim):\n            x_plus_h = x.copy()\n            x_minus_h = x.copy()\n            x_plus_h[i] += h\n            x_minus_h[i] -= h\n            gradient[i] = (func(x_plus_h) - func(x_minus_h)) / (2 * h)\n        return gradient", "configspace": "", "generation": 8, "feedback": "Evaluation timed out after 300 seconds.", "error": "Evaluation timed out after 300 seconds.", "parent_ids": ["d0fddf51-e6ed-4acb-82bd-f12a4f7ace8d"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "5a67ee9f-3881-497a-b2b4-cc0e0a323b91", "fitness": "-inf", "name": "HybridOptimizer", "description": "A self-adjusting hybrid algorithm that blends differential evolution, particle swarm optimization, and a Lvy flight-based local search, dynamically adapting exploration rates based on optimization progress and function landscape analysis.", "code": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget=10000, dim=10, pop_size=20, initial_de_rate=0.5, initial_pso_rate=0.3, initial_local_rate=0.2, restart_trigger=0.05, levy_exponent=1.5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.de_rate = initial_de_rate\n        self.pso_rate = initial_pso_rate\n        self.local_rate = initial_local_rate\n        self.restart_trigger = restart_trigger\n        self.levy_exponent = levy_exponent\n        self.initial_de_rate = initial_de_rate\n        self.initial_pso_rate = initial_pso_rate\n        self.initial_local_rate = initial_local_rate\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget_used = self.pop_size\n        \n        self.pbest_population = self.population.copy()\n        self.pbest_fitness = self.fitness.copy()\n\n        self.gbest_index = np.argmin(self.fitness)\n        self.gbest_x = self.population[self.gbest_index].copy()\n        self.gbest_fitness = self.fitness[self.gbest_index].copy()\n        \n        self.f_opt = self.gbest_fitness\n        self.x_opt = self.gbest_x\n\n        self.no_improvement_count = 0\n        self.max_no_improvement = int(self.budget * self.restart_trigger)\n\n        def levy_flight(beta, dimension):\n            num = np.gamma(1 + beta) * np.sin(np.pi * beta / 2)\n            den = np.gamma((1 + beta) / 2) * beta * (2 ** ((beta - 1) / 2))\n            sigma = (num / den) ** (1 / beta)\n            u = np.random.normal(0, sigma, size=dimension)\n            v = np.random.normal(0, 1, size=dimension)\n            step = u / (np.abs(v) ** (1 / beta))\n            return step\n\n        while self.budget_used < self.budget:\n            new_population = np.zeros_like(self.population)\n            new_fitness = np.zeros(self.pop_size)\n\n            for i in range(self.pop_size):\n                rand = np.random.rand()\n\n                # Differential Evolution\n                if rand < self.de_rate:\n                    indices = np.random.choice(self.pop_size, 3, replace=False)\n                    x_r1, x_r2, x_r3 = self.population[indices]\n                    mutation = x_r1 + 0.5 * (x_r2 - x_r3)\n                    crossover = np.random.rand(self.dim) < 0.5\n                    new_population[i] = np.where(crossover, mutation, self.population[i])\n                    new_population[i] = np.clip(new_population[i], func.bounds.lb, func.bounds.ub)\n                    new_fitness[i] = func(new_population[i])\n                    self.budget_used += 1\n\n                # Particle Swarm Optimization\n                elif rand < self.de_rate + self.pso_rate:\n                    inertia = 0.7\n                    cognitive_coeff = 1.5\n                    social_coeff = 1.5\n                    velocity = inertia * (self.population[i] - self.pbest_population[i]) + cognitive_coeff * np.random.rand(self.dim) * (self.pbest_population[i] - self.population[i]) + social_coeff * np.random.rand(self.dim) * (self.gbest_x - self.population[i])\n                    new_population[i] = self.population[i] + velocity\n                    new_population[i] = np.clip(new_population[i], func.bounds.lb, func.bounds.ub)\n                    new_fitness[i] = func(new_population[i])\n                    self.budget_used += 1\n                    \n                # Local Search with Levy Flight\n                else:\n                    step_size = 0.01 * levy_flight(self.levy_exponent, self.dim)\n                    new_population[i] = self.population[i] + step_size\n                    new_population[i] = np.clip(new_population[i], func.bounds.lb, func.bounds.ub)\n                    new_fitness[i] = func(new_population[i])\n                    self.budget_used += 1\n\n            for i in range(self.pop_size):\n                if new_fitness[i] < self.fitness[i]:\n                    self.population[i] = new_population[i]\n                    self.fitness[i] = new_fitness[i]\n\n                    if new_fitness[i] < self.pbest_fitness[i]:\n                        self.pbest_population[i] = new_population[i]\n                        self.pbest_fitness[i] = new_fitness[i]\n\n            self.gbest_index = np.argmin(self.fitness)\n            if self.fitness[self.gbest_index] < self.gbest_fitness:\n                self.gbest_x = self.population[self.gbest_index].copy()\n                self.gbest_fitness = self.fitness[self.gbest_index].copy()\n                \n                if self.gbest_fitness < self.f_opt:\n                    self.f_opt = self.gbest_fitness\n                    self.x_opt = self.gbest_x\n                    self.no_improvement_count = 0\n\n                    # Adaptive rate adjustment (simplified)\n                    if self.de_rate > 0.1:\n                         self.de_rate *= 0.95\n                    if self.pso_rate > 0.1:\n                        self.pso_rate *= 0.95\n                    self.local_rate = 1 - self.de_rate - self.pso_rate\n                else:\n                    self.no_improvement_count += 1\n            else:\n                self.no_improvement_count += 1\n\n            # Restart mechanism\n            if self.no_improvement_count > self.max_no_improvement:\n                self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                self.fitness = np.array([func(x) for x in self.population])\n                self.budget_used += self.pop_size\n\n                self.pbest_population = self.population.copy()\n                self.pbest_fitness = self.fitness.copy()\n\n                self.gbest_index = np.argmin(self.fitness)\n                self.gbest_x = self.population[self.gbest_index].copy()\n                self.gbest_fitness = self.fitness[self.gbest_index].copy()\n                \n                self.no_improvement_count = 0\n                self.de_rate = self.initial_de_rate\n                self.pso_rate = self.initial_pso_rate\n                self.local_rate = self.initial_local_rate\n                \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 9, "feedback": "In the code, line 805, in __getattr__, the following error occurred:\nAttributeError: module 'numpy' has no attribute 'gamma'", "error": "In the code, line 805, in __getattr__, the following error occurred:\nAttributeError: module 'numpy' has no attribute 'gamma'", "parent_ids": ["1e74c5f0-f489-41e9-b124-27e514d9a233"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "d3ff7365-e638-42b4-ad60-87074b8f90c6", "fitness": "-inf", "name": "HybridOptimizer", "description": "A hybrid algorithm with dynamic resource allocation between global (DE), local (Cauchy), and gradient-based (BFGS) search, guided by optimization progress and landscape features, coupled with population diversity maintenance.", "code": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridOptimizer:\n    def __init__(self, budget=10000, dim=10, pop_size=20, initial_de_rate=0.4, initial_bfgs_rate=0.1, local_rate=0.5, restart_trigger=0.05, diversity_threshold=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.de_rate = initial_de_rate\n        self.bfgs_rate = initial_bfgs_rate\n        self.local_rate = local_rate\n        self.restart_trigger = restart_trigger\n        self.diversity_threshold = diversity_threshold\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget_used = self.pop_size\n\n        self.best_index = np.argmin(self.fitness)\n        self.best_x = self.population[self.best_index].copy()\n        self.best_fitness = self.fitness[self.best_index].copy()\n\n        self.f_opt = self.best_fitness\n        self.x_opt = self.best_x\n\n        self.no_improvement_count = 0\n        self.max_no_improvement = int(self.budget * self.restart_trigger)\n\n        while self.budget_used < self.budget:\n            new_population = np.zeros_like(self.population)\n            new_fitness = np.zeros(self.pop_size)\n\n            for i in range(self.pop_size):\n                rand = np.random.rand()\n\n                # Differential Evolution\n                if rand < self.de_rate:\n                    indices = np.random.choice(self.pop_size, 3, replace=False)\n                    x_r1, x_r2, x_r3 = self.population[indices]\n                    mutation = x_r1 + 0.5 * (x_r2 - x_r3)\n                    crossover = np.random.rand(self.dim) < 0.5\n                    new_population[i] = np.where(crossover, mutation, self.population[i])\n                    new_population[i] = np.clip(new_population[i], func.bounds.lb, func.bounds.ub)\n                    new_fitness[i] = func(new_population[i])\n                    self.budget_used += 1\n\n                # BFGS Local Search\n                elif rand < self.de_rate + self.bfgs_rate:\n                    x0 = self.population[i].copy()\n                    bounds = [(func.bounds.lb, func.bounds.ub)] * self.dim\n                    result = minimize(func, x0, method='L-BFGS-B', bounds=bounds, options={'maxfun': max(1, int(0.05 * self.budget))})  # Limit BFGS evaluations\n                    if result.success:\n                        new_population[i] = result.x\n                        new_fitness[i] = result.fun\n                    else:\n                        new_population[i] = self.population[i]\n                        new_fitness[i] = self.fitness[i]\n                    self.budget_used = min(self.budget, self.budget_used + result.nfev) #Account for BFGS evaluations\n\n                # Local Search with Cauchy Mutation\n                else:\n                    new_population[i] = self.population[i] + 0.01 * np.random.standard_cauchy(size=self.dim)\n                    new_population[i] = np.clip(new_population[i], func.bounds.lb, func.bounds.ub)\n                    new_fitness[i] = func(new_population[i])\n                    self.budget_used += 1\n\n            # Update population\n            for i in range(self.pop_size):\n                if new_fitness[i] < self.fitness[i]:\n                    self.population[i] = new_population[i]\n                    self.fitness[i] = new_fitness[i]\n\n            # Update global best\n            best_index = np.argmin(self.fitness)\n            if self.fitness[best_index] < self.best_fitness:\n                self.best_x = self.population[best_index].copy()\n                self.best_fitness = self.fitness[best_index].copy()\n\n                if self.best_fitness < self.f_opt:\n                    self.f_opt = self.best_fitness\n                    self.x_opt = self.best_x\n                    self.no_improvement_count = 0\n\n                    # Adaptive rate adjustment\n                    if self.de_rate > 0.1:\n                        self.de_rate *= 0.95\n                    if self.bfgs_rate < 0.4:\n                        self.bfgs_rate *= 1.05\n                    self.local_rate = 1 - self.de_rate - self.bfgs_rate\n\n                else:\n                    self.no_improvement_count += 1\n            else:\n                self.no_improvement_count += 1\n\n            # Restart mechanism and diversity check\n            if self.no_improvement_count > self.max_no_improvement or self.population_diversity() < self.diversity_threshold:\n                self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                self.fitness = np.array([func(x) for x in self.population])\n                self.budget_used += self.pop_size\n\n                self.best_index = np.argmin(self.fitness)\n                self.best_x = self.population[self.best_index].copy()\n                self.best_fitness = self.fitness[self.best_index].copy()\n\n                self.no_improvement_count = 0\n                self.de_rate = 0.4 # was self.initial_de_rate\n                self.bfgs_rate = 0.1 #was self.initial_bfgs_rate\n                self.local_rate = 0.5 #was 1 - self.de_rate - self.bfgs_rate\n\n        return self.f_opt, self.x_opt\n\n    def population_diversity(self):\n        \"\"\"Calculates the average pairwise distance between individuals in the population.\"\"\"\n        distances = []\n        for i in range(self.pop_size):\n            for j in range(i + 1, self.pop_size):\n                distances.append(np.linalg.norm(self.population[i] - self.population[j]))\n        return np.mean(distances) if distances else 1.0", "configspace": "", "generation": 9, "feedback": "In the code, line 54, in __call__, the following error occurred:\nNameError: name 'minimize' is not defined\nOn line: result = minimize(func, x0, method='L-BFGS-B', bounds=bounds, options={'maxfun': max(1, int(0.05 * self.budget))})  # Limit BFGS evaluations", "error": "In the code, line 54, in __call__, the following error occurred:\nNameError: name 'minimize' is not defined\nOn line: result = minimize(func, x0, method='L-BFGS-B', bounds=bounds, options={'maxfun': max(1, int(0.05 * self.budget))})  # Limit BFGS evaluations", "parent_ids": ["1e74c5f0-f489-41e9-b124-27e514d9a233"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "5127f57f-fca9-43d7-af02-0834c80b9b28", "fitness": 0.5612629849741245, "name": "HybridOptimizer", "description": "A simplified hybrid optimizer with adaptive learning rates between differential evolution, particle swarm optimization, and a Cauchy mutation-based local search strategy.", "code": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget=10000, dim=10, pop_size=20, initial_de_rate=0.5, initial_pso_rate=0.3, restart_trigger=0.05):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.de_rate = initial_de_rate\n        self.pso_rate = initial_pso_rate\n        self.local_rate = 1 - initial_de_rate - initial_pso_rate\n        self.restart_trigger = restart_trigger\n        self.initial_de_rate = initial_de_rate\n        self.initial_pso_rate = initial_pso_rate\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget_used = self.pop_size\n        \n        self.pbest_population = self.population.copy()\n        self.pbest_fitness = self.fitness.copy()\n\n        self.gbest_index = np.argmin(self.fitness)\n        self.gbest_x = self.population[self.gbest_index].copy()\n        self.gbest_fitness = self.fitness[self.gbest_index].copy()\n        \n        self.f_opt = self.gbest_fitness\n        self.x_opt = self.gbest_x\n\n        self.no_improvement_count = 0\n        self.max_no_improvement = int(self.budget * self.restart_trigger)\n        \n        while self.budget_used < self.budget:\n            new_population = np.zeros_like(self.population)\n            new_fitness = np.zeros(self.pop_size)\n\n            for i in range(self.pop_size):\n                rand = np.random.rand()\n\n                # Differential Evolution\n                if rand < self.de_rate:\n                    indices = np.random.choice(self.pop_size, 3, replace=False)\n                    x_r1, x_r2, x_r3 = self.population[indices]\n                    mutation = x_r1 + 0.5 * (x_r2 - x_r3)\n                    crossover = np.random.rand(self.dim) < 0.5\n                    new_population[i] = np.where(crossover, mutation, self.population[i])\n                    new_population[i] = np.clip(new_population[i], func.bounds.lb, func.bounds.ub)\n                    new_fitness[i] = func(new_population[i])\n                    self.budget_used += 1\n\n                # Particle Swarm Optimization\n                elif rand < self.de_rate + self.pso_rate:\n                    inertia = 0.7\n                    cognitive_coeff = 1.5\n                    social_coeff = 1.5\n                    velocity = inertia * (self.population[i] - self.pbest_population[i]) + cognitive_coeff * np.random.rand(self.dim) * (self.pbest_population[i] - self.population[i]) + social_coeff * np.random.rand(self.dim) * (self.gbest_x - self.population[i])\n                    new_population[i] = self.population[i] + velocity\n                    new_population[i] = np.clip(new_population[i], func.bounds.lb, func.bounds.ub)\n                    new_fitness[i] = func(new_population[i])\n                    self.budget_used += 1\n                    \n                # Local Search with Cauchy Mutation\n                else:\n                    new_population[i] = self.population[i] + 0.01 * np.random.standard_cauchy(size=self.dim)\n                    new_population[i] = np.clip(new_population[i], func.bounds.lb, func.bounds.ub)\n                    new_fitness[i] = func(new_population[i])\n                    self.budget_used += 1\n\n            for i in range(self.pop_size):\n                if new_fitness[i] < self.fitness[i]:\n                    self.population[i] = new_population[i]\n                    self.fitness[i] = new_fitness[i]\n\n                    if new_fitness[i] < self.pbest_fitness[i]:\n                        self.pbest_population[i] = new_population[i]\n                        self.pbest_fitness[i] = new_fitness[i]\n\n            self.gbest_index = np.argmin(self.fitness)\n            if self.fitness[self.gbest_index] < self.gbest_fitness:\n                self.gbest_x = self.population[self.gbest_index].copy()\n                self.gbest_fitness = self.fitness[self.gbest_index].copy()\n                \n                if self.gbest_fitness < self.f_opt:\n                    self.f_opt = self.gbest_fitness\n                    self.x_opt = self.gbest_x\n                    self.no_improvement_count = 0\n\n                    # Adaptive rate adjustment (simplified)\n                    if self.de_rate > 0.1:\n                         self.de_rate *= 0.95\n                    if self.pso_rate > 0.1:\n                        self.pso_rate *= 0.95\n                    self.local_rate = 1 - self.de_rate - self.pso_rate\n                else:\n                    self.no_improvement_count += 1\n            else:\n                self.no_improvement_count += 1\n\n            # Restart mechanism\n            if self.no_improvement_count > self.max_no_improvement:\n                self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                self.fitness = np.array([func(x) for x in self.population])\n                self.budget_used += self.pop_size\n\n                self.pbest_population = self.population.copy()\n                self.pbest_fitness = self.fitness.copy()\n\n                self.gbest_index = np.argmin(self.fitness)\n                self.gbest_x = self.population[self.gbest_index].copy()\n                self.gbest_fitness = self.fitness[self.gbest_index].copy()\n                \n                self.no_improvement_count = 0\n                self.de_rate = self.initial_de_rate\n                self.pso_rate = self.initial_pso_rate\n                self.local_rate = 1 - self.de_rate - self.pso_rate\n                \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 9, "feedback": "The algorithm HybridOptimizer scored 0.561 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["1e74c5f0-f489-41e9-b124-27e514d9a233"], "operator": null, "metadata": {"aucs": [0.17677198836802166, 0.501727226992331, 0.5696796259566658, 0.866206434803194, 0.5964047439251157, 0.7584004356132935, 0.3363120173296993, 0.4579452829323821, 0.7038349003653261, 0.20194234794166366, 0.6499490204366762, 0.9965220510260042, 0.39713862562303004, 0.5062037643379351, 0.9026540803144623, 0.6511624557704369, 0.3122331404375067, 0.8964081923746205, 0.24597339641024685, 0.49778996852388135]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "62d97630-a837-49e8-992a-2dfcaa3daef9", "fitness": "-inf", "name": "AdaptiveHybridOptimizer", "description": "An adaptive hybrid algorithm using a decaying exploration rate and a combination of differential evolution, covariance matrix adaptation evolution strategy (CMA-ES) inspired sampling, and a gradient-based local search, with a restart mechanism.", "code": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget=10000, dim=10, pop_size=20, de_rate=0.5, cma_rate=0.3, local_rate=0.2, restart_trigger=0.05, initial_exploration_rate=0.5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.de_rate = de_rate\n        self.cma_rate = cma_rate\n        self.local_rate = local_rate\n        self.restart_trigger = restart_trigger\n        self.initial_exploration_rate = initial_exploration_rate # Initial exploration rate, decays over time\n\n        self.exploration_rate = initial_exploration_rate\n        self.exploration_decay = 0.995  # Decay factor for exploration rate\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget_used = self.pop_size\n\n        self.best_index = np.argmin(self.fitness)\n        self.best_x = self.population[self.best_index].copy()\n        self.best_fitness = self.fitness[self.best_index].copy()\n\n        self.f_opt = self.best_fitness\n        self.x_opt = self.best_x\n\n        self.no_improvement_count = 0\n        self.max_no_improvement = int(self.budget * self.restart_trigger)\n\n        self.mean = np.mean(self.population, axis=0)\n        self.covariance = np.cov(self.population, rowvar=False) + np.eye(self.dim) * 1e-6 # Add small value to avoid singular matrix\n        \n        while self.budget_used < self.budget:\n            new_population = np.zeros_like(self.population)\n\n            # Differential Evolution\n            for i in range(self.pop_size):\n                if np.random.rand() < self.de_rate * self.exploration_rate:\n                    indices = np.random.choice(self.pop_size, 3, replace=False)\n                    x_r1, x_r2, x_r3 = self.population[indices]\n                    mutation = x_r1 + 0.5 * (x_r2 - x_r3)\n                    crossover = np.random.rand(self.dim) < 0.5  # binomial crossover\n                    new_population[i] = np.where(crossover, mutation, self.population[i])\n\n                # CMA-ES inspired sampling\n                elif np.random.rand() < self.cma_rate * self.exploration_rate:\n                    try:\n                        new_population[i] = np.random.multivariate_normal(self.mean, self.covariance)\n                    except:\n                        new_population[i] = np.random.uniform(func.bounds.lb, func.bounds.ub, self.dim)\n                    \n                # Gradient-based Local Search\n                elif np.random.rand() < self.local_rate:\n                    def local_func(x):\n                        return func(x)\n                    \n                    bounds_list = [(func.bounds.lb, func.bounds.ub) for _ in range(self.dim)]\n                    \n                    res = minimize(local_func, self.population[i], method='L-BFGS-B', bounds=bounds_list, options={'maxiter': 5}) # Limited iterations\n                    if res.success:\n                        new_population[i] = res.x\n                    else:\n                        new_population[i] = self.population[i] # Keep the old one if optimization fails\n\n                # Random exploration\n                else:\n                    new_population[i] = np.random.uniform(func.bounds.lb, func.bounds.ub, self.dim)\n\n                new_population[i] = np.clip(new_population[i], func.bounds.lb, func.bounds.ub)\n\n            new_fitness = np.array([func(x) for x in new_population])\n            self.budget_used += self.pop_size\n\n            for i in range(self.pop_size):\n                if new_fitness[i] < self.fitness[i]:\n                    self.population[i] = new_population[i]\n                    self.fitness[i] = new_fitness[i]\n                    \n                    if self.fitness[i] < self.best_fitness:\n                        self.best_fitness = self.fitness[i]\n                        self.best_x = self.population[i].copy()\n                        \n                        if self.best_fitness < self.f_opt:\n                            self.f_opt = self.best_fitness\n                            self.x_opt = self.best_x\n                            self.no_improvement_count = 0\n                        else:\n                            self.no_improvement_count += 1\n                else:\n                    self.no_improvement_count += 1\n\n            # Update mean and covariance matrix\n            self.mean = np.mean(self.population, axis=0)\n            try:\n                self.covariance = np.cov(self.population, rowvar=False) + np.eye(self.dim) * 1e-6\n            except:\n                self.covariance = np.eye(self.dim)\n            \n\n            # Restart mechanism\n            if self.no_improvement_count > self.max_no_improvement:\n                self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                self.fitness = np.array([func(x) for x in self.population])\n                self.best_index = np.argmin(self.fitness)\n                self.best_x = self.population[self.best_index].copy()\n                self.best_fitness = self.fitness[self.best_index].copy()\n                self.no_improvement_count = 0\n                \n                self.mean = np.mean(self.population, axis=0)\n                self.covariance = np.cov(self.population, rowvar=False) + np.eye(self.dim) * 1e-6\n\n            # Decay exploration rate\n            self.exploration_rate *= self.exploration_decay\n            self.exploration_rate = max(self.exploration_rate, 0.05)  # Ensure minimal exploration\n            \n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 9, "feedback": "In the code, line 64, in __call__, the following error occurred:\nNameError: name 'minimize' is not defined\nOn line: res = minimize(local_func, self.population[i], method='L-BFGS-B', bounds=bounds_list, options={'maxiter': 5}) # Limited iterations", "error": "In the code, line 64, in __call__, the following error occurred:\nNameError: name 'minimize' is not defined\nOn line: res = minimize(local_func, self.population[i], method='L-BFGS-B', bounds=bounds_list, options={'maxiter': 5}) # Limited iterations", "parent_ids": ["d0fddf51-e6ed-4acb-82bd-f12a4f7ace8d"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "9c9f490e-38fd-42e3-94a7-83b4f9274c3c", "fitness": 0.4192444945452073, "name": "HybridOptimizer", "description": "A hybrid optimizer that combines Differential Evolution with a dynamically adjusted Gaussian mutation, and introduces a Simulated Annealing-based acceptance criterion to escape local optima.", "code": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget=10000, dim=10, pop_size=20):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lb = -5.0\n        self.ub = 5.0\n        self.T = 1.0  # Initial temperature for Simulated Annealing\n        self.T_min = 0.0001  # Minimum temperature\n        self.alpha = 0.99  # Cooling rate\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget_used = self.pop_size\n\n        self.best_index = np.argmin(self.fitness)\n        self.best_x = self.population[self.best_index].copy()\n        self.best_fitness = self.fitness[self.best_index].copy()\n\n        self.f_opt = self.best_fitness\n        self.x_opt = self.best_x\n\n        de_rate = 0.7\n        mutation_factor = 0.5\n\n        while self.budget_used < self.budget:\n            new_population = np.zeros_like(self.population)\n            new_fitness = np.zeros(self.pop_size)\n            \n            # Adaptive Mutation scale\n            mutation_scale = 0.1 * (self.ub - self.lb) * (1 - self.budget_used / self.budget)\n\n            for i in range(self.pop_size):\n                if np.random.rand() < de_rate:\n                    # Differential Evolution\n                    indices = np.random.choice(self.pop_size, 3, replace=False)\n                    x_r1, x_r2, x_r3 = self.population[indices]\n                    mutation = x_r1 + mutation_factor * (x_r2 - x_r3)\n                    crossover = np.random.rand(self.dim) < 0.7  # Binomial Crossover\n                    new_population[i] = np.where(crossover, mutation, self.population[i])\n                else:\n                    # Self-Adjusting Gaussian Mutation\n                    new_population[i] = self.population[i] + np.random.normal(0, mutation_scale, self.dim)\n\n                new_population[i] = np.clip(new_population[i], self.lb, self.ub)\n                new_fitness[i] = func(new_population[i])\n                self.budget_used += 1\n\n                delta = new_fitness[i] - self.fitness[i]\n                if delta < 0:\n                    self.population[i] = new_population[i]\n                    self.fitness[i] = new_fitness[i]\n\n                    if new_fitness[i] < self.best_fitness:\n                        self.best_x = new_population[i].copy()\n                        self.best_fitness = new_fitness[i].copy()\n                        if self.best_fitness < self.f_opt:\n                            self.f_opt = self.best_fitness\n                            self.x_opt = self.best_x\n                else:\n                    # Simulated Annealing acceptance criterion\n                    if self.T > 0 and np.random.rand() < np.exp(-delta / self.T):\n                        self.population[i] = new_population[i]\n                        self.fitness[i] = new_fitness[i]\n\n                if self.budget_used >= self.budget:\n                    break\n            \n            # Cooling\n            self.T = max(self.T * self.alpha, self.T_min)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 9, "feedback": "The algorithm HybridOptimizer scored 0.419 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["1457e643-466c-4b1b-bef8-3dfe4fa360e4"], "operator": null, "metadata": {"aucs": [0.28353343553508314, 0.4061889339268159, 0.34631790446958255, 0.47418288974675005, 0.3796037552335837, 0.4009128443369042, 0.3313662232141902, 0.35146883643679394, 0.36645610824714436, 0.3965545679332162, 0.5444444646512568, 0.9921831009527993, 0.2521285134708299, 0.399871032351083, 0.6302661544854853, 0.36104282929554565, 0.3346203855729498, 0.45331004168597355, 0.22216593983827537, 0.4582719295198845]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "017c7498-f7bf-4b11-860f-f5223bda8aa3", "fitness": "-inf", "name": "HybridOptimizer", "description": "A simplified adaptive hybrid optimizer that balances differential evolution and Gaussian mutation with dynamically adjusted parameters based on optimization progress, incorporating a focused local search.", "code": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget=10000, dim=10, pop_size=20):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lb = -5.0\n        self.ub = 5.0\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget_used = self.pop_size\n\n        self.best_index = np.argmin(self.fitness)\n        self.best_x = self.population[self.best_index].copy()\n        self.best_fitness = self.fitness[self.best_index].copy()\n\n        self.f_opt = self.best_fitness\n        self.x_opt = self.best_x\n\n        de_rate_initial = 0.7\n        mutation_factor_initial = 0.5\n        local_search_rate = 0.1\n\n        while self.budget_used < self.budget:\n            de_rate = de_rate_initial * (1 - self.budget_used / self.budget) # adaptive de_rate\n            mutation_factor = mutation_factor_initial * (1 - self.budget_used / self.budget) # adaptive mutation_factor\n\n            for i in range(self.pop_size):\n                # Differential Evolution\n                indices = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = self.population[indices]\n                mutation = x_r1 + mutation_factor * (x_r2 - x_r3)\n                crossover = np.random.rand(self.dim) < de_rate  # Binomial Crossover\n\n                new_x = np.where(crossover, mutation, self.population[i])\n                new_x = np.clip(new_x, self.lb, self.ub)\n\n                # Local Search around the best solution\n                if np.random.rand() < local_search_rate:\n                    new_x = self.best_x + np.random.normal(0, 0.01 * (self.ub - self.lb), self.dim)\n                    new_x = np.clip(new_x, self.lb, self.ub)\n                \n                new_fitness = func(new_x)\n                self.budget_used += 1\n                \n                if new_fitness < self.fitness[i]:\n                    self.population[i] = new_x\n                    self.fitness[i] = new_fitness\n\n                    if new_fitness < self.best_fitness:\n                        self.best_x = new_x.copy()\n                        self.best_fitness = new_fitness.copy()\n                        if self.best_fitness < self.f_opt:\n                            self.f_opt = self.best_fitness\n                            self.x_opt = self.best_x\n\n                if self.budget_used >= self.budget:\n                    break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 9, "feedback": "In the code, line 57, in __call__, the following error occurred:\nAttributeError: 'float' object has no attribute 'copy'\nOn line: self.best_fitness = new_fitness.copy()", "error": "In the code, line 57, in __call__, the following error occurred:\nAttributeError: 'float' object has no attribute 'copy'\nOn line: self.best_fitness = new_fitness.copy()", "parent_ids": ["1457e643-466c-4b1b-bef8-3dfe4fa360e4"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "3ff5d920-31f6-4d81-9448-edacb4e02112", "fitness": "-inf", "name": "HybridOptimizer", "description": "A self-adaptive hybrid algorithm with dynamic probability adjustment based on success rates of DE, PSO and Cauchy mutation, plus a simplified restart strategy.", "code": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget=10000, dim=10, pop_size=20, initial_de_rate=0.5, initial_pso_rate=0.3):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.de_rate = initial_de_rate\n        self.pso_rate = initial_pso_rate\n        self.local_rate = 1 - initial_de_rate - initial_pso_rate\n        self.lb = -5.0\n        self.ub = 5.0\n        self.success_de = 0\n        self.success_pso = 0\n        self.success_local = 0\n        self.count_de = 0\n        self.count_pso = 0\n        self.count_local = 0\n        self.restart_trigger = 0.05\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget_used = self.pop_size\n        self.best_index = np.argmin(self.fitness)\n        self.best_x = self.population[self.best_index].copy()\n        self.best_fitness = self.fitness[self.best_index].copy()\n        self.f_opt = self.best_fitness\n        self.x_opt = self.best_x\n        self.no_improvement_count = 0\n        self.max_no_improvement = int(self.budget * self.restart_trigger)\n\n        while self.budget_used < self.budget:\n            new_population = np.zeros_like(self.population)\n            new_fitness = np.zeros(self.pop_size)\n\n            for i in range(self.pop_size):\n                rand = np.random.rand()\n\n                # Differential Evolution\n                if rand < self.de_rate:\n                    self.count_de += 1\n                    indices = np.random.choice(self.pop_size, 3, replace=False)\n                    x_r1, x_r2, x_r3 = self.population[indices]\n                    mutation = x_r1 + 0.5 * (x_r2 - x_r3)\n                    crossover = np.random.rand(self.dim) < 0.5\n                    new_population[i] = np.where(crossover, mutation, self.population[i])\n                    new_population[i] = np.clip(new_population[i], self.lb, self.ub)\n                    new_fitness[i] = func(new_population[i])\n                    self.budget_used += 1\n                    if new_fitness[i] < self.fitness[i]:\n                        self.success_de += 1\n\n                # Particle Swarm Optimization\n                elif rand < self.de_rate + self.pso_rate:\n                    self.count_pso += 1\n                    inertia = 0.7\n                    cognitive_coeff = 1.5\n                    social_coeff = 1.5\n                    velocity = inertia * (self.population[i] - self.population[np.argmin(self.fitness)]) + cognitive_coeff * np.random.rand(self.dim) * (self.population[np.argmin(self.fitness)] - self.population[i]) + social_coeff * np.random.rand(self.dim) * (self.best_x - self.population[i])\n                    new_population[i] = self.population[i] + velocity\n                    new_population[i] = np.clip(new_population[i], self.lb, self.ub)\n                    new_fitness[i] = func(new_population[i])\n                    self.budget_used += 1\n                    if new_fitness[i] < self.fitness[i]:\n                        self.success_pso += 1\n                    \n                # Local Search with Cauchy Mutation\n                else:\n                    self.count_local += 1\n                    new_population[i] = self.population[i] + 0.01 * np.random.standard_cauchy(size=self.dim)\n                    new_population[i] = np.clip(new_population[i], self.lb, self.ub)\n                    new_fitness[i] = func(new_population[i])\n                    self.budget_used += 1\n                    if new_fitness[i] < self.fitness[i]:\n                        self.success_local += 1\n\n            for i in range(self.pop_size):\n                if new_fitness[i] < self.fitness[i]:\n                    self.population[i] = new_population[i]\n                    self.fitness[i] = new_fitness[i]\n                    if new_fitness[i] < self.best_fitness:\n                        self.best_x = new_population[i].copy()\n                        self.best_fitness = new_fitness[i].copy()\n                        if self.best_fitness < self.f_opt:\n                            self.f_opt = self.best_fitness\n                            self.x_opt = self.best_x\n                            self.no_improvement_count = 0\n                \n\n            self.best_index = np.argmin(self.fitness)\n            if self.fitness[self.best_index] < self.best_fitness:\n                self.best_x = self.population[self.best_index].copy()\n                self.best_fitness = self.fitness[self.best_index].copy()\n                if self.best_fitness < self.f_opt:\n                    self.f_opt = self.best_fitness\n                    self.x_opt = self.best_x\n                    self.no_improvement_count = 0\n            else:\n                self.no_improvement_count += 1\n\n            # Adaptive rate adjustment\n            total_count = self.count_de + self.count_pso + self.count_local\n            if total_count > 0:\n                self.de_rate = self.success_de / total_count if total_count > 0 else 0.33\n                self.pso_rate = self.success_pso / total_count if total_count > 0 else 0.33\n                self.local_rate = self.success_local / total_count if total_count > 0 else 0.34\n                \n                # Normalize rates\n                total_rate = self.de_rate + self.pso_rate + self.local_rate\n                self.de_rate /= total_rate\n                self.pso_rate /= total_rate\n                self.local_rate /= total_rate\n\n            # Restart mechanism\n            if self.no_improvement_count > self.max_no_improvement:\n                self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n                self.fitness = np.array([func(x) for x in self.population])\n                self.budget_used += self.pop_size\n                self.best_index = np.argmin(self.fitness)\n                self.best_x = self.population[self.best_index].copy()\n                self.best_fitness = self.fitness[self.best_index].copy()\n                self.no_improvement_count = 0\n\n                self.success_de = 0\n                self.success_pso = 0\n                self.success_local = 0\n                self.count_de = 0\n                self.count_pso = 0\n                self.count_local = 0\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 9, "feedback": "Evaluation timed out after 300 seconds.", "error": "Evaluation timed out after 300 seconds.", "parent_ids": ["1e74c5f0-f489-41e9-b124-27e514d9a233"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "752410bd-edeb-4c0a-b11e-27285dfeb9f2", "fitness": "-inf", "name": "AdaptiveHybridOptimizer", "description": "An adaptive hybrid algorithm that uses a combination of CMA-ES and a gradient-based method, with a mechanism to switch between exploration and exploitation phases based on the optimization progress.", "code": "import numpy as np\nimport cma\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget=10000, dim=10, pop_size=20, exploration_prob=0.7, exploitation_prob=0.3, learning_rate=0.01, restart_trigger=0.05):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.exploration_prob = exploration_prob\n        self.exploitation_prob = exploitation_prob\n        self.learning_rate = learning_rate\n        self.restart_trigger = restart_trigger\n        self.es = None\n        self.x_opt = None\n        self.f_opt = np.inf\n        self.no_improvement_count = 0\n        self.max_no_improvement = int(self.budget * self.restart_trigger)\n\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.budget_used = 0\n        \n        # Initialize CMA-ES\n        x0 = np.random.uniform(func.bounds.lb, func.bounds.ub, self.dim)\n        sigma0 = 0.5\n        self.es = cma.CMAEvolutionStrategy(x0, sigma0, {'popsize': self.pop_size, 'bounds': [func.bounds.lb, func.bounds.ub]})\n        \n        while self.budget_used < self.budget:\n            # Exploration phase (CMA-ES)\n            if np.random.rand() < self.exploration_prob:\n                solutions = self.es.ask()\n                fitness_values = np.array([func(x) for x in solutions])\n                self.budget_used += self.pop_size\n                self.es.tell(solutions, fitness_values)\n                \n                best_index = np.argmin(fitness_values)\n                if fitness_values[best_index] < self.f_opt:\n                    self.f_opt = fitness_values[best_index]\n                    self.x_opt = solutions[best_index]\n                    self.no_improvement_count = 0\n                else:\n                    self.no_improvement_count += 1\n\n            # Exploitation phase (Gradient Descent-like)\n            else:\n                # Take the current best solution from CMA-ES\n                if self.es.best.x is not None:\n                    x = self.es.best.x.copy()\n                elif self.x_opt is not None:\n                     x = self.x_opt.copy()\n                else:\n                    x = self.es.ask(1)[0]\n\n                # Estimate gradient (simple finite difference)\n                gradient = np.zeros(self.dim)\n                delta = 1e-5\n                for i in range(self.dim):\n                    x_plus = x.copy()\n                    x_minus = x.copy()\n                    x_plus[i] += delta\n                    x_minus[i] -= delta\n                    \n                    # Clip values to stay within bounds\n                    x_plus[i] = np.clip(x_plus[i], func.bounds.lb, func.bounds.ub)\n                    x_minus[i] = np.clip(x_minus[i], func.bounds.lb, func.bounds.ub)\n\n                    f_plus = func(x_plus)\n                    f_minus = func(x_minus)\n                    self.budget_used += 2 #account for two calls to function\n                    gradient[i] = (f_plus - f_minus) / (2 * delta)\n                    \n\n                # Update solution (gradient descent)\n                x_new = x - self.learning_rate * gradient\n                x_new = np.clip(x_new, func.bounds.lb, func.bounds.ub)  # Clip values\n\n                f_new = func(x_new)\n                self.budget_used += 1\n\n                if f_new < self.f_opt:\n                    self.f_opt = f_new\n                    self.x_opt = x_new\n                    self.no_improvement_count = 0\n                else:\n                    self.no_improvement_count += 1\n\n            # Restart mechanism\n            if self.no_improvement_count > self.max_no_improvement:\n                x0 = np.random.uniform(func.bounds.lb, func.bounds.ub, self.dim)\n                sigma0 = 0.5\n                self.es = cma.CMAEvolutionStrategy(x0, sigma0, {'popsize': self.pop_size, 'bounds': [func.bounds.lb, func.bounds.ub]})\n                self.no_improvement_count = 0\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 9, "feedback": "In the code, line 2, in <module>, the following error occurred:\nModuleNotFoundError: No module named 'cma'\nOn line: import cma", "error": "In the code, line 2, in <module>, the following error occurred:\nModuleNotFoundError: No module named 'cma'\nOn line: import cma", "parent_ids": ["d0fddf51-e6ed-4acb-82bd-f12a4f7ace8d"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "fc12533c-7d3f-48db-9dfd-a9a49decc3af", "fitness": 0.34738843002612413, "name": "HybridOptimizer", "description": "An adaptive hybrid optimizer that blends differential evolution with a Cauchy mutation-based local search, dynamically adjusting exploration based on the population diversity and function evaluations.", "code": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget=10000, dim=10, pop_size=20):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lb = -5.0\n        self.ub = 5.0\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget_used = self.pop_size\n\n        self.best_index = np.argmin(self.fitness)\n        self.best_x = self.population[self.best_index].copy()\n        self.best_fitness = self.fitness[self.best_index].copy()\n\n        self.f_opt = self.best_fitness\n        self.x_opt = self.best_x\n\n        de_rate_initial = 0.7\n        mutation_factor = 0.5\n\n        while self.budget_used < self.budget:\n            new_population = np.zeros_like(self.population)\n            new_fitness = np.zeros(self.pop_size)\n\n            # Population Diversity measure (std dev)\n            diversity = np.std(self.population)\n            \n            # Adaptive DE rate based on diversity and budget\n            de_rate = de_rate_initial * (0.5 + 0.5 * (1 - self.budget_used / self.budget)) * (1 - np.tanh(diversity))\n\n            for i in range(self.pop_size):\n                if self.budget_used >= self.budget:\n                    break\n\n                if np.random.rand() < de_rate:\n                    # Differential Evolution\n                    indices = np.random.choice(self.pop_size, 3, replace=False)\n                    x_r1, x_r2, x_r3 = self.population[indices]\n                    mutation = x_r1 + mutation_factor * (x_r2 - x_r3)\n                    crossover = np.random.rand(self.dim) < 0.7  # Binomial Crossover\n                    new_population[i] = np.where(crossover, mutation, self.population[i])\n                else:\n                    # Cauchy Mutation-based Local Search\n                    scale = 0.1 * (self.ub - self.lb) * (1 - self.budget_used / self.budget)\n                    new_population[i] = self.population[i] + scale * np.random.standard_cauchy(self.dim)\n\n                new_population[i] = np.clip(new_population[i], self.lb, self.ub)\n                new_fitness[i] = func(new_population[i])\n                self.budget_used += 1\n\n                if new_fitness[i] < self.fitness[i]:\n                    self.population[i] = new_population[i]\n                    self.fitness[i] = new_fitness[i]\n\n                    if new_fitness[i] < self.best_fitness:\n                        self.best_x = new_population[i].copy()\n                        self.best_fitness = new_fitness[i].copy()\n                        if self.best_fitness < self.f_opt:\n                            self.f_opt = self.best_fitness\n                            self.x_opt = self.best_x\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 10, "feedback": "The algorithm HybridOptimizer scored 0.347 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["1457e643-466c-4b1b-bef8-3dfe4fa360e4"], "operator": null, "metadata": {"aucs": [0.15362777366903468, 0.23359606166281055, 0.35543556895241946, 0.32713234412548264, 0.26262928609041636, 0.3292024476489869, 0.27703501005786746, 0.30429769848178134, 0.26243344031776017, 0.2037509425724987, 0.32136270028533453, 0.9883799292142988, 0.2648532536901623, 0.27281522203821573, 0.6938868889025374, 0.3405727653619247, 0.27886789311540694, 0.40540960485426836, 0.19189747863432327, 0.4805822908469518]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "e86847df-130d-4ef3-a8f5-391f0e65e2eb", "fitness": 0.5039554731601622, "name": "HybridOptimizer", "description": "A hybrid optimizer that dynamically adjusts exploration/exploitation via a combination of differential evolution with dynamic scaling, particle swarm optimization with velocity clamping and a mutation operator based on the current best solution.", "code": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget=10000, dim=10, pop_size=20):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lb = -5.0\n        self.ub = 5.0\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget_used = self.pop_size\n\n        self.best_index = np.argmin(self.fitness)\n        self.best_x = self.population[self.best_index].copy()\n        self.best_fitness = self.fitness[self.best_index].copy()\n\n        self.f_opt = self.best_fitness\n        self.x_opt = self.best_x\n        \n        exploration_rate = 0.7\n        exploitation_rate = 0.2\n        de_rate = 0.5 #differential evolution rate\n        pso_rate = 0.5 #particle swarm optimisation rate\n        mutation_rate = 0.1\n\n        # PSO Velocity Clamping\n        v_max = (self.ub - self.lb) * 0.2\n        v_min = -v_max\n\n\n        while self.budget_used < self.budget:\n            new_population = np.zeros_like(self.population)\n            \n            # Dynamic adjustment of exploration/exploitation balance\n            remaining_budget_ratio = (self.budget - self.budget_used) / self.budget\n            current_exploration_rate = exploration_rate * remaining_budget_ratio\n            current_exploitation_rate = exploitation_rate * (1 - remaining_budget_ratio)\n            \n            for i in range(self.pop_size):\n                if np.random.rand() < current_exploration_rate:\n                    #Differential evolution strategy\n                    if np.random.rand() < de_rate:\n                        indices = np.random.choice(self.pop_size, 3, replace=False)\n                        x_r1, x_r2, x_r3 = self.population[indices]\n                        # Dynamic scaling factor\n                        scaling_factor = 0.1 + 0.4 * np.random.rand()\n                        mutation = x_r1 + scaling_factor * (x_r2 - x_r3)\n                        crossover = np.random.rand(self.dim) < 0.5  # binomial crossover\n                        new_population[i] = np.where(crossover, mutation, self.population[i])\n                    #Particle swarm optimisation\n                    elif np.random.rand() < pso_rate:\n                        inertia = 0.7\n                        cognitive_coeff = 1.5\n                        social_coeff = 1.5\n                        velocity = inertia * (self.population[i] - self.best_x) + cognitive_coeff * np.random.rand(self.dim) * (self.best_x - self.population[i]) + social_coeff * np.random.rand(self.dim) * (self.best_x - self.population[i])\n                        \n                        # Velocity clamping\n                        velocity = np.clip(velocity, v_min, v_max)\n                        new_population[i] = self.population[i] + velocity\n                        \n                    # Random exploration\n                    else:\n                        new_population[i] = np.random.uniform(self.lb, self.ub, self.dim)\n\n                elif np.random.rand() < current_exploitation_rate:\n                    # Mutation around the best solution\n                    mutation_strength = 0.05 + 0.1 * np.random.rand()\n                    new_population[i] = self.best_x + mutation_strength * np.random.normal(0, 1, self.dim)\n\n\n                else:\n                    #Default random exploration\n                     new_population[i] = np.random.uniform(self.lb, self.ub, self.dim)\n\n                new_population[i] = np.clip(new_population[i], self.lb, self.ub)\n\n            new_fitness = np.array([func(x) for x in new_population])\n            self.budget_used += self.pop_size\n\n            for i in range(self.pop_size):\n                if new_fitness[i] < self.fitness[i]:\n                    self.population[i] = new_population[i]\n                    self.fitness[i] = new_fitness[i]\n\n                    if new_fitness[i] < self.best_fitness:\n                        self.best_x = new_population[i].copy()\n                        self.best_fitness = new_fitness[i].copy()\n                        \n                        if self.best_fitness < self.f_opt:\n                            self.f_opt = self.best_fitness\n                            self.x_opt = self.best_x\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 10, "feedback": "The algorithm HybridOptimizer scored 0.504 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["e16e90d9-ba5f-43ec-b171-d0521e5848f9"], "operator": null, "metadata": {"aucs": [0.1822099078405326, 0.28026630752108805, 0.5060109307107252, 0.8432184660067693, 0.35999317551580257, 0.6304826056635218, 0.2965540196338283, 0.3581586802071359, 0.7931712048602809, 0.21427386545345317, 0.4400411416727902, 0.9959572228418458, 0.2826389795266586, 0.4004540308836556, 0.8058118256236668, 0.8416572219951689, 0.35410624937275215, 0.7270116546939444, 0.2617721614465939, 0.5053198117330313]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "fc758bd4-69b6-45d8-b19f-911e70aeb629", "fitness": 0.0, "name": "HybridOptimizer", "description": "A hybrid optimizer that combines a modified Differential Evolution with a novel Cauchy-based mutation, dynamically adjusting mutation strength based on the function's gradient norm.", "code": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget=10000, dim=10, pop_size=20):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lb = -5.0\n        self.ub = 5.0\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget_used = self.pop_size\n\n        self.best_index = np.argmin(self.fitness)\n        self.best_x = self.population[self.best_index].copy()\n        self.best_fitness = self.fitness[self.best_index].copy()\n\n        self.f_opt = self.best_fitness\n        self.x_opt = self.best_x\n\n        de_rate = 0.7\n        mutation_factor = 0.5\n\n        while self.budget_used < self.budget:\n            new_population = np.zeros_like(self.population)\n            new_fitness = np.zeros(self.pop_size)\n\n            for i in range(self.pop_size):\n                if np.random.rand() < de_rate:\n                    # Differential Evolution\n                    indices = np.random.choice(self.pop_size, 3, replace=False)\n                    x_r1, x_r2, x_r3 = self.population[indices]\n                    mutation = x_r1 + mutation_factor * (x_r2 - x_r3)\n                    crossover = np.random.rand(self.dim) < 0.7  # Binomial Crossover\n                    new_population[i] = np.where(crossover, mutation, self.population[i])\n                else:\n                    # Cauchy Mutation with gradient-based adaptation\n                    # Approximate gradient using finite differences\n                    gradient = np.zeros(self.dim)\n                    delta = 1e-4  # Step size for finite differences\n                    for j in range(self.dim):\n                        x_plus_delta = self.population[i].copy()\n                        x_plus_delta[j] += delta\n                        x_plus_delta = np.clip(x_plus_delta, self.lb, self.ub)\n                        fitness_plus_delta = func(x_plus_delta)\n                        self.budget_used += 1\n\n                        x_minus_delta = self.population[i].copy()\n                        x_minus_delta[j] -= delta\n                        x_minus_delta = np.clip(x_minus_delta, self.lb, self.ub)\n                        fitness_minus_delta = func(x_minus_delta)\n                        self.budget_used += 1\n\n                        gradient[j] = (fitness_plus_delta - fitness_minus_delta) / (2 * delta)\n\n                    gradient_norm = np.linalg.norm(gradient)\n\n                    # Adjust mutation strength based on gradient norm\n                    if gradient_norm > 1e-3:\n                        mutation_scale = 0.1 * (self.ub - self.lb) / (1 + gradient_norm)  # Smaller steps if gradient is large\n                    else:\n                        mutation_scale = 0.1 * (self.ub - self.lb)  # Larger steps if gradient is small\n\n                    cauchy_mutation = mutation_scale * np.random.standard_cauchy(self.dim)\n                    new_population[i] = self.population[i] + cauchy_mutation\n                    self.budget_used -= 1 # compensate over-budgeting from gradient estimation\n\n                new_population[i] = np.clip(new_population[i], self.lb, self.ub)\n                if self.budget_used < self.budget:\n                    new_fitness[i] = func(new_population[i])\n                    self.budget_used += 1\n                else:\n                    new_fitness[i] = np.inf\n\n                if new_fitness[i] < self.fitness[i]:\n                    self.population[i] = new_population[i]\n                    self.fitness[i] = new_fitness[i]\n\n                    if new_fitness[i] < self.best_fitness:\n                        self.best_x = new_population[i].copy()\n                        self.best_fitness = new_fitness[i].copy()\n                        if self.best_fitness < self.f_opt:\n                            self.f_opt = self.best_fitness\n                            self.x_opt = self.best_x\n                if self.budget_used >= self.budget:\n                    break\n\n            if self.budget_used >= self.budget:\n                break\n            \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 10, "feedback": "The algorithm HybridOptimizer scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["1457e643-466c-4b1b-bef8-3dfe4fa360e4"], "operator": null, "metadata": {"aucs": [0]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "85378eab-540e-488e-9005-2e7b547753b4", "fitness": 0.7330623090278718, "name": "HybridOptimizer", "description": "A hybrid optimizer combining differential evolution with a self-adjusting Gaussian mutation guided by success rate and magnitude of previous mutations, adapting exploration based on optimization progress and landscape features.", "code": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget=10000, dim=10, pop_size=20, de_rate=0.7, mutation_factor=0.5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lb = -5.0\n        self.ub = 5.0\n        self.de_rate = de_rate\n        self.mutation_factor = mutation_factor\n        self.mutation_scale = 0.1 * (self.ub - self.lb) \n        self.success_rates = np.zeros(self.pop_size)\n        self.mutation_magnitudes = np.zeros(self.pop_size)\n        self.learning_rate = 0.1\n\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget_used = self.pop_size\n\n        self.best_index = np.argmin(self.fitness)\n        self.best_x = self.population[self.best_index].copy()\n        self.best_fitness = self.fitness[self.best_index].copy()\n\n        self.f_opt = self.best_fitness\n        self.x_opt = self.best_x\n\n        while self.budget_used < self.budget:\n            new_population = np.zeros_like(self.population)\n            new_fitness = np.zeros(self.pop_size)\n            \n            # Adaptive Mutation scale based on progress\n            adaptive_mutation_scale = self.mutation_scale * (1 - self.budget_used / self.budget)\n\n            for i in range(self.pop_size):\n                old_fitness = self.fitness[i]\n\n                if np.random.rand() < self.de_rate:\n                    # Differential Evolution\n                    indices = np.random.choice(self.pop_size, 3, replace=False)\n                    x_r1, x_r2, x_r3 = self.population[indices]\n                    mutation = x_r1 + self.mutation_factor * (x_r2 - x_r3)\n                    crossover = np.random.rand(self.dim) < 0.7  # Binomial Crossover\n                    new_population[i] = np.where(crossover, mutation, self.population[i])\n                else:\n                    # Self-Adjusting Gaussian Mutation\n                    mutation_vector = np.random.normal(0, adaptive_mutation_scale * (1 + self.mutation_magnitudes[i]), self.dim)\n                    new_population[i] = self.population[i] + mutation_vector\n\n                new_population[i] = np.clip(new_population[i], self.lb, self.ub)\n                new_fitness[i] = func(new_population[i])\n                self.budget_used += 1\n\n                if new_fitness[i] < old_fitness:\n                    # Success! Update individual\n                    self.population[i] = new_population[i]\n                    self.fitness[i] = new_fitness[i]\n                    self.success_rates[i] = (1 - self.learning_rate) * self.success_rates[i] + self.learning_rate\n                    self.mutation_magnitudes[i] = (1 - self.learning_rate) * self.mutation_magnitudes[i] + self.learning_rate * np.linalg.norm(new_population[i] - self.population[i])\n\n                    if new_fitness[i] < self.best_fitness:\n                        self.best_x = new_population[i].copy()\n                        self.best_fitness = new_fitness[i].copy()\n                        if self.best_fitness < self.f_opt:\n                            self.f_opt = self.best_fitness\n                            self.x_opt = self.best_x\n                else:\n                     # Failure, decrease mutation magnitude\n                    self.success_rates[i] = (1 - self.learning_rate) * self.success_rates[i]\n                    self.mutation_magnitudes[i] *= (1 - self.learning_rate)\n\n                if self.budget_used >= self.budget:\n                    break\n            \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 10, "feedback": "The algorithm HybridOptimizer scored 0.733 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["1457e643-466c-4b1b-bef8-3dfe4fa360e4"], "operator": null, "metadata": {"aucs": [0.26081429959547775, 0.6938323174421542, 0.6518291898022349, 0.8987197927269219, 0.8257952253440306, 0.8414789044070329, 0.6967938908051421, 0.7605670183565909, 0.8347204883000143, 0.7260971247616665, 0.8856871301907904, 0.9995671003439991, 0.31280991250210644, 0.7653547136052282, 0.9213997905158833, 0.8610800274139809, 0.7273571972918296, 0.8799253465277638, 0.5959322318150292, 0.5214844788095643]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "40aef3dd-7a78-4fd0-ba8c-0f8b46ddacca", "fitness": 0.5378295786235567, "name": "HybridOptimizer", "description": "Adaptive hybrid optimizer blending differential evolution, particle swarm optimization, and a Gaussian mutation-based local search with dynamic strategy allocation based on performance and a simplified restart mechanism.", "code": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget=10000, dim=10, pop_size=20, initial_de_rate=0.5, initial_pso_rate=0.3, restart_trigger=0.05):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.de_rate = initial_de_rate\n        self.pso_rate = initial_pso_rate\n        self.local_rate = 1 - initial_de_rate - initial_pso_rate\n        self.restart_trigger = restart_trigger\n        self.initial_de_rate = initial_de_rate\n        self.initial_pso_rate = initial_pso_rate\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget_used = self.pop_size\n        \n        self.pbest_population = self.population.copy()\n        self.pbest_fitness = self.fitness.copy()\n\n        self.gbest_index = np.argmin(self.fitness)\n        self.gbest_x = self.population[self.gbest_index].copy()\n        self.gbest_fitness = self.fitness[self.gbest_index].copy()\n        \n        self.f_opt = self.gbest_fitness\n        self.x_opt = self.gbest_x\n\n        self.no_improvement_count = 0\n        self.max_no_improvement = int(self.budget * self.restart_trigger)\n        \n        while self.budget_used < self.budget:\n            new_population = np.zeros_like(self.population)\n            new_fitness = np.zeros(self.pop_size)\n\n            for i in range(self.pop_size):\n                rand = np.random.rand()\n\n                # Differential Evolution\n                if rand < self.de_rate:\n                    indices = np.random.choice(self.pop_size, 3, replace=False)\n                    x_r1, x_r2, x_r3 = self.population[indices]\n                    mutation = x_r1 + 0.5 * (x_r2 - x_r3)\n                    crossover = np.random.rand(self.dim) < 0.5\n                    new_population[i] = np.where(crossover, mutation, self.population[i])\n                    new_population[i] = np.clip(new_population[i], func.bounds.lb, func.bounds.ub)\n                # Particle Swarm Optimization\n                elif rand < self.de_rate + self.pso_rate:\n                    inertia = 0.7\n                    cognitive_coeff = 1.5\n                    social_coeff = 1.5\n                    velocity = inertia * (self.population[i] - self.pbest_population[i]) + cognitive_coeff * np.random.rand(self.dim) * (self.pbest_population[i] - self.population[i]) + social_coeff * np.random.rand(self.dim) * (self.gbest_x - self.population[i])\n                    new_population[i] = self.population[i] + velocity\n                    new_population[i] = np.clip(new_population[i], func.bounds.lb, func.bounds.ub)\n                    \n                # Local Search with Gaussian Mutation\n                else:\n                    new_population[i] = self.population[i] + 0.01 * np.random.normal(size=self.dim)\n                    new_population[i] = np.clip(new_population[i], func.bounds.lb, func.bounds.ub)\n                \n                new_fitness[i] = func(new_population[i])\n                self.budget_used += 1\n\n            for i in range(self.pop_size):\n                if new_fitness[i] < self.fitness[i]:\n                    self.population[i] = new_population[i]\n                    self.fitness[i] = new_fitness[i]\n\n                    if new_fitness[i] < self.pbest_fitness[i]:\n                        self.pbest_population[i] = new_population[i]\n                        self.pbest_fitness[i] = new_fitness[i]\n\n            self.gbest_index = np.argmin(self.fitness)\n            if self.fitness[self.gbest_index] < self.gbest_fitness:\n                self.gbest_x = self.population[self.gbest_index].copy()\n                self.gbest_fitness = self.fitness[self.gbest_index].copy()\n                \n                if self.gbest_fitness < self.f_opt:\n                    self.f_opt = self.gbest_fitness\n                    self.x_opt = self.gbest_x\n                    self.no_improvement_count = 0\n\n                    # Adaptive rate adjustment (simplified)\n                    self.de_rate *= 0.99\n                    self.pso_rate *= 0.99\n                    self.local_rate = 1 - self.de_rate - self.pso_rate\n                    self.de_rate = max(0.1, self.de_rate)\n                    self.pso_rate = max(0.1, self.pso_rate)\n                    self.local_rate = max(0.1, self.local_rate)\n                    norm_factor = self.de_rate + self.pso_rate + self.local_rate\n                    self.de_rate /= norm_factor\n                    self.pso_rate /= norm_factor\n                    self.local_rate /= norm_factor\n\n\n                else:\n                    self.no_improvement_count += 1\n            else:\n                self.no_improvement_count += 1\n\n            # Restart mechanism\n            if self.no_improvement_count > self.max_no_improvement:\n                self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                self.fitness = np.array([func(x) for x in self.population])\n                self.budget_used += self.pop_size\n\n                self.pbest_population = self.population.copy()\n                self.pbest_fitness = self.fitness.copy()\n\n                self.gbest_index = np.argmin(self.fitness)\n                self.gbest_x = self.population[self.gbest_index].copy()\n                self.gbest_fitness = self.fitness[self.gbest_index].copy()\n                \n                self.no_improvement_count = 0\n                self.de_rate = self.initial_de_rate\n                self.pso_rate = self.initial_pso_rate\n                self.local_rate = 1 - self.de_rate - self.pso_rate\n                \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 10, "feedback": "The algorithm HybridOptimizer scored 0.538 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["1e74c5f0-f489-41e9-b124-27e514d9a233"], "operator": null, "metadata": {"aucs": [0.1648318903387137, 0.663395448728457, 0.5931158227496405, 0.9424403859474824, 0.29746890062789166, 0.7541306597935035, 0.3087703004380845, 0.5039143928517533, 0.7128877883004111, 0.19538539855073522, 0.8823622818219565, 0.9980084167379075, 0.3641666709955649, 0.25568689159556457, 0.7192411779131596, 0.5821008010885355, 0.2745349059151059, 0.8271350692506712, 0.21770923159086386, 0.49930513723513414]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "032f7ee3-c8b4-4be1-82a4-f3fca80f0066", "fitness": "-inf", "name": "HybridOptimizer", "description": "A hybrid optimizer that adaptively adjusts the balance between exploration and exploitation based on population diversity and function landscape curvature, incorporating a Lvy flight-based mutation strategy for enhanced global search.", "code": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget=10000, dim=10, pop_size=20, initial_de_rate=0.4, initial_pso_rate=0.3, initial_levy_rate=0.3, restart_trigger=0.05, diversity_threshold=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.de_rate = initial_de_rate\n        self.pso_rate = initial_pso_rate\n        self.levy_rate = initial_levy_rate\n        self.restart_trigger = restart_trigger\n        self.diversity_threshold = diversity_threshold\n        self.initial_de_rate = initial_de_rate\n        self.initial_pso_rate = initial_pso_rate\n        self.initial_levy_rate = initial_levy_rate\n\n    def levy_flight(self, beta=1.5):\n        sigma = (np.math.gamma(1 + beta) * np.sin(np.pi * beta / 2) / (np.math.gamma((1 + beta) / 2) * beta * (2 ** ((beta - 1) / 2)))) ** (1 / beta)\n        u = np.random.normal(0, sigma, size=self.dim)\n        v = np.random.normal(0, 1, size=self.dim)\n        step = u / (abs(v) ** (1 / beta))\n        return step\n\n    def calculate_diversity(self):\n        return np.std(self.population)\n\n    def calculate_curvature(self, func):\n        # Estimate curvature based on fitness differences (simplified)\n        fitness_range = np.max(self.fitness) - np.min(self.fitness)\n        return fitness_range\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget_used = self.pop_size\n\n        self.pbest_population = self.population.copy()\n        self.pbest_fitness = self.fitness.copy()\n\n        self.gbest_index = np.argmin(self.fitness)\n        self.gbest_x = self.population[self.gbest_index].copy()\n        self.gbest_fitness = self.fitness[self.gbest_index].copy()\n        \n        self.f_opt = self.gbest_fitness\n        self.x_opt = self.gbest_x\n\n        self.no_improvement_count = 0\n        self.max_no_improvement = int(self.budget * self.restart_trigger)\n\n        while self.budget_used < self.budget:\n            new_population = np.zeros_like(self.population)\n            new_fitness = np.zeros(self.pop_size)\n\n            diversity = self.calculate_diversity()\n            curvature = self.calculate_curvature(func)\n\n            for i in range(self.pop_size):\n                rand = np.random.rand()\n\n                # Differential Evolution\n                if rand < self.de_rate:\n                    indices = np.random.choice(self.pop_size, 3, replace=False)\n                    x_r1, x_r2, x_r3 = self.population[indices]\n                    mutation = x_r1 + 0.5 * (x_r2 - x_r3)\n                    crossover = np.random.rand(self.dim) < 0.5\n                    new_population[i] = np.where(crossover, mutation, self.population[i])\n                    new_population[i] = np.clip(new_population[i], func.bounds.lb, func.bounds.ub)\n                    new_fitness[i] = func(new_population[i])\n                    self.budget_used += 1\n\n                # Particle Swarm Optimization\n                elif rand < self.de_rate + self.pso_rate:\n                    inertia = 0.7\n                    cognitive_coeff = 1.5\n                    social_coeff = 1.5\n                    velocity = inertia * (self.population[i] - self.pbest_population[i]) + cognitive_coeff * np.random.rand(self.dim) * (self.pbest_population[i] - self.population[i]) + social_coeff * np.random.rand(self.dim) * (self.gbest_x - self.population[i])\n                    new_population[i] = self.population[i] + velocity\n                    new_population[i] = np.clip(new_population[i], func.bounds.lb, func.bounds.ub)\n                    new_fitness[i] = func(new_population[i])\n                    self.budget_used += 1\n                    \n                # Lvy Flight Mutation\n                else:\n                    step_size = 0.01 * self.levy_flight()  # Adjust step size as needed\n                    new_population[i] = self.population[i] + step_size\n                    new_population[i] = np.clip(new_population[i], func.bounds.lb, func.bounds.ub)\n                    new_fitness[i] = func(new_population[i])\n                    self.budget_used += 1\n\n            for i in range(self.pop_size):\n                if new_fitness[i] < self.fitness[i]:\n                    self.population[i] = new_population[i]\n                    self.fitness[i] = new_fitness[i]\n\n                    if new_fitness[i] < self.pbest_fitness[i]:\n                        self.pbest_population[i] = new_population[i]\n                        self.pbest_fitness[i] = new_fitness[i]\n\n            self.gbest_index = np.argmin(self.fitness)\n            if self.fitness[self.gbest_index] < self.gbest_fitness:\n                self.gbest_x = self.population[self.gbest_index].copy()\n                self.gbest_fitness = self.fitness[self.gbest_index].copy()\n                \n                if self.gbest_fitness < self.f_opt:\n                    self.f_opt = self.gbest_fitness\n                    self.x_opt = self.gbest_x\n                    self.no_improvement_count = 0\n\n                    # Adaptive rate adjustment based on diversity and curvature\n                    if diversity < self.diversity_threshold:  # Low diversity, emphasize exploration\n                        self.de_rate = min(self.de_rate + 0.01, 0.7)\n                        self.pso_rate = min(self.pso_rate + 0.01, 0.7)\n                        self.levy_rate = max(1 - self.de_rate - self.pso_rate, 0.0)\n\n                    if curvature < 0.1:  # Flat landscape, increase Lvy flights\n                        self.levy_rate = min(self.levy_rate + 0.01, 0.7)\n                        self.de_rate = max(self.de_rate - 0.005, 0.0)\n                        self.pso_rate = max(self.pso_rate - 0.005, 0.0)\n\n                    else: # high curvature function, increase PSO/DE\n                        self.de_rate = min(self.de_rate + 0.005, 0.7)\n                        self.pso_rate = min(self.pso_rate + 0.005, 0.7)\n                        self.levy_rate = max(1-self.de_rate - self.pso_rate,0.0)\n                else:\n                    self.no_improvement_count += 1\n            else:\n                self.no_improvement_count += 1\n\n            # Restart mechanism\n            if self.no_improvement_count > self.max_no_improvement:\n                self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                self.fitness = np.array([func(x) for x in self.population])\n                self.budget_used += self.pop_size\n\n                self.pbest_population = self.population.copy()\n                self.pbest_fitness = self.fitness.copy()\n\n                self.gbest_index = np.argmin(self.fitness)\n                self.gbest_x = self.population[self.gbest_index].copy()\n                self.gbest_fitness = self.fitness[self.gbest_index].copy()\n\n                self.no_improvement_count = 0\n                self.de_rate = self.initial_de_rate\n                self.pso_rate = self.initial_pso_rate\n                self.levy_rate = self.initial_levy_rate\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 10, "feedback": "In the code, line 805, in __getattr__, the following error occurred:\nAttributeError: module 'numpy' has no attribute 'math'. Did you mean: 'emath'?", "error": "In the code, line 805, in __getattr__, the following error occurred:\nAttributeError: module 'numpy' has no attribute 'math'. Did you mean: 'emath'?", "parent_ids": ["5127f57f-fca9-43d7-af02-0834c80b9b28"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "c0a45f9a-0889-456e-ad8c-5b6f8a635e80", "fitness": 0.49204083636355733, "name": "HybridOptimizer", "description": "Dynamically adjusts exploration rates based on optimization progress and uses a simplified Cauchy mutation for enhanced local search, combined with a restart strategy.", "code": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget=10000, dim=10, pop_size=20, initial_de_rate=0.6, initial_local_rate=0.3, restart_trigger=0.05):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.de_rate = initial_de_rate\n        self.local_rate = initial_local_rate\n        self.restart_trigger = restart_trigger\n        self.initial_de_rate = initial_de_rate\n        self.initial_local_rate = initial_local_rate\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget_used = self.pop_size\n\n        self.best_index = np.argmin(self.fitness)\n        self.best_x = self.population[self.best_index].copy()\n        self.best_fitness = self.fitness[self.best_index].copy()\n        \n        self.f_opt = self.best_fitness\n        self.x_opt = self.best_x\n\n        self.no_improvement_count = 0\n        self.max_no_improvement = int(self.budget * self.restart_trigger)\n        \n        while self.budget_used < self.budget:\n            new_population = np.zeros_like(self.population)\n            new_fitness = np.zeros(self.pop_size)\n\n            for i in range(self.pop_size):\n                rand = np.random.rand()\n\n                # Differential Evolution\n                if rand < self.de_rate:\n                    indices = np.random.choice(self.pop_size, 3, replace=False)\n                    x_r1, x_r2, x_r3 = self.population[indices]\n                    mutation = x_r1 + 0.5 * (x_r2 - x_r3)\n                    crossover = np.random.rand(self.dim) < 0.5\n                    new_population[i] = np.where(crossover, mutation, self.population[i])\n                    new_population[i] = np.clip(new_population[i], func.bounds.lb, func.bounds.ub)\n                    new_fitness[i] = func(new_population[i])\n                    self.budget_used += 1\n\n                # Local Search with Cauchy Mutation\n                else:\n                    new_population[i] = self.population[i] + 0.01 * np.random.standard_cauchy(size=self.dim)\n                    new_population[i] = np.clip(new_population[i], func.bounds.lb, func.bounds.ub)\n                    new_fitness[i] = func(new_population[i])\n                    self.budget_used += 1\n\n            for i in range(self.pop_size):\n                if new_fitness[i] < self.fitness[i]:\n                    self.population[i] = new_population[i]\n                    self.fitness[i] = new_fitness[i]\n\n            self.best_index = np.argmin(self.fitness)\n            if self.fitness[self.best_index] < self.best_fitness:\n                self.best_x = self.population[self.best_index].copy()\n                self.best_fitness = self.fitness[self.best_index].copy()\n                \n                if self.best_fitness < self.f_opt:\n                    self.f_opt = self.best_fitness\n                    self.x_opt = self.best_x\n                    self.no_improvement_count = 0\n\n                    # Adaptive rate adjustment (simplified)\n                    if self.de_rate > 0.2:\n                         self.de_rate *= 0.95\n                    self.local_rate = 1 - self.de_rate\n                else:\n                    self.no_improvement_count += 1\n            else:\n                self.no_improvement_count += 1\n\n            # Restart mechanism\n            if self.no_improvement_count > self.max_no_improvement:\n                self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                self.fitness = np.array([func(x) for x in self.population])\n                self.budget_used += self.pop_size\n\n                self.best_index = np.argmin(self.fitness)\n                self.best_x = self.population[self.best_index].copy()\n                self.best_fitness = self.fitness[self.best_index].copy()\n\n                self.no_improvement_count = 0\n                self.de_rate = self.initial_de_rate\n                self.local_rate = self.initial_local_rate\n                \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 10, "feedback": "The algorithm HybridOptimizer scored 0.492 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["1e74c5f0-f489-41e9-b124-27e514d9a233"], "operator": null, "metadata": {"aucs": [0.17896007543063874, 0.2506082511786589, 0.48202293993766154, 0.780546670253209, 0.40176060423595594, 0.6240658713657401, 0.3307954786968328, 0.4304786231821951, 0.460688265171186, 0.3013321681103175, 0.5953745387046288, 0.9964302417498676, 0.3088985329444217, 0.43904331915194095, 0.8029419959080462, 0.6179909661295997, 0.3613358039381286, 0.728402111611782, 0.21830718824691908, 0.5308330813234173]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "744ce376-4bef-40c5-85a8-2af3543b5ffd", "fitness": "-inf", "name": "HybridOptimizer", "description": "A hybrid optimizer with dynamic strategy allocation, adaptive parameter control based on success rate, and a simplified covariance matrix adaptation for improved exploration.", "code": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget=10000, dim=10, pop_size=20):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lb = -5.0\n        self.ub = 5.0\n\n        self.de_rate = 0.33\n        self.pso_rate = 0.33\n        self.random_rate = 0.34\n\n        self.de_success_rate = 0.0\n        self.pso_success_rate = 0.0\n        self.random_success_rate = 0.0\n\n        self.success_memory = 10 #Number of iterations to remember the success\n\n        self.de_successes = []\n        self.pso_successes = []\n        self.random_successes = []\n        \n        self.learning_rate = 0.1 #How quickly to adapt exploration rates\n        self.cma_mean = np.zeros(dim)  # Initialize CMA mean\n        self.cma_sigma = 1.0         # Initialize CMA step size\n        self.cma_C = np.eye(dim)      # Initialize CMA covariance matrix\n        self.cma_path_c = np.zeros(dim)\n        self.cma_path_sigma = np.zeros(dim)\n        self.cma_eigen_decomposition_needed = True\n        self.cma_B = None\n        self.cma_D = None\n        self.cma_mu = pop_size // 2 #Number of individuals to use for recombination\n\n    def update_cma_eigen_decomposition(self):\n        self.cma_eigen_decomposition_needed = False\n        self.cma_D, self.cma_B = np.linalg.eig(self.cma_C)\n        self.cma_D = np.sqrt(np.abs(self.cma_D))  # Ensure positive values\n\n    def sample_cma(self):\n        if self.cma_eigen_decomposition_needed:\n            self.update_cma_eigen_decomposition()\n        z = np.random.normal(0, 1, self.dim)\n        return self.cma_mean + self.cma_sigma * self.cma_B @ (self.cma_D * z)\n\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget_used = self.pop_size\n\n        self.best_index = np.argmin(self.fitness)\n        self.best_x = self.population[self.best_index].copy()\n        self.best_fitness = self.fitness[self.best_index].copy()\n\n        self.f_opt = self.best_fitness\n        self.x_opt = self.best_x\n        \n        while self.budget_used < self.budget:\n            new_population = np.zeros_like(self.population)\n            new_fitness = np.zeros(self.pop_size)\n            de_success_count = 0\n            pso_success_count = 0\n            random_success_count = 0\n\n            for i in range(self.pop_size):\n                rand = np.random.rand()\n                \n                if rand < self.de_rate:\n                    #Differential evolution strategy\n                    indices = np.random.choice(self.pop_size, 3, replace=False)\n                    x_r1, x_r2, x_r3 = self.population[indices]\n                    mutation = x_r1 + 0.5 * (x_r2 - x_r3)\n                    crossover = np.random.rand(self.dim) < 0.5  # binomial crossover\n                    new_population[i] = np.where(crossover, mutation, self.population[i])\n\n                    new_population[i] = np.clip(new_population[i], self.lb, self.ub)\n                    new_fitness[i] = func(new_population[i])\n                    self.budget_used += 1\n\n                    if new_fitness[i] < self.fitness[i]:\n                        de_success_count += 1\n                    \n\n                elif rand < self.de_rate + self.pso_rate:\n                    #Particle swarm optimisation\n                    inertia = 0.7\n                    cognitive_coeff = 1.5\n                    social_coeff = 1.5\n                    velocity = inertia * (self.population[i] - self.best_x) + cognitive_coeff * np.random.rand(self.dim) * (self.best_x - self.population[i]) + social_coeff * np.random.rand(self.dim) * (self.best_x - self.population[i])\n                    new_population[i] = self.population[i] + velocity\n\n                    new_population[i] = np.clip(new_population[i], self.lb, self.ub)\n                    new_fitness[i] = func(new_population[i])\n                    self.budget_used += 1\n\n                    if new_fitness[i] < self.fitness[i]:\n                        pso_success_count += 1\n\n                else:\n                    # Simplified CMA-ES-based exploration\n                    new_population[i] = self.sample_cma()\n                    new_population[i] = np.clip(new_population[i], self.lb, self.ub)\n                    new_fitness[i] = func(new_population[i])\n                    self.budget_used += 1\n\n                    if new_fitness[i] < self.fitness[i]:\n                        random_success_count += 1\n\n            # Update population\n            for i in range(self.pop_size):\n                if new_fitness[i] < self.fitness[i]:\n                    self.population[i] = new_population[i]\n                    self.fitness[i] = new_fitness[i]\n\n                    if new_fitness[i] < self.best_fitness:\n                        self.best_x = new_population[i].copy()\n                        self.best_fitness = new_fitness[i].copy()\n                        self.cma_mean = self.best_x #Move the mean towards better solutions\n                        \n                        if self.best_fitness < self.f_opt:\n                            self.f_opt = self.best_fitness\n                            self.x_opt = self.best_x\n            \n            self.de_successes.append(de_success_count / self.pop_size)\n            self.pso_successes.append(pso_success_count / self.pop_size)\n            self.random_successes.append(random_success_count / self.pop_size)\n\n            if len(self.de_successes) > self.success_memory:\n                self.de_successes.pop(0)\n                self.pso_successes.pop(0)\n                self.random_successes.pop(0)\n            \n            self.de_success_rate = np.mean(self.de_successes)\n            self.pso_success_rate = np.mean(self.pso_successes)\n            self.random_success_rate = np.mean(self.random_successes)\n\n\n            # Adjust exploration rates based on success\n            total_success = self.de_success_rate + self.pso_success_rate + self.random_success_rate\n            if total_success > 0:\n                self.de_rate = (1 - self.learning_rate) * self.de_rate + self.learning_rate * (self.de_success_rate / total_success)\n                self.pso_rate = (1 - self.learning_rate) * self.pso_rate + self.learning_rate * (self.pso_success_rate / total_success)\n                self.random_rate = (1 - self.learning_rate) * self.random_rate + self.learning_rate * (self.random_success_rate / total_success)\n\n            # Normalize rates to ensure they sum to 1\n            total_rate = self.de_rate + self.pso_rate + self.random_rate\n            self.de_rate /= total_rate\n            self.pso_rate /= total_rate\n            self.random_rate /= total_rate\n\n            # CMA-ES adaptation (simplified)\n            z = (self.best_x - self.cma_mean) / self.cma_sigma  # Assuming best_x is recent best\n            self.cma_path_c = (1 - 0.5) * self.cma_path_c + np.sqrt(0.5 * (2 - 0.5)) * z\n            self.cma_path_sigma = (1 - 0.3) * self.cma_path_sigma + np.sqrt(0.3 * (2 - 0.3)) * np.linalg.norm(z) / (self.dim**0.5)\n\n            self.cma_C = (1 - 0.1) * self.cma_C + 0.1 * (self.cma_path_c[:, None] @ self.cma_path_c[None, :])\n            self.cma_sigma *= np.exp(0.2 * (np.linalg.norm(self.cma_path_sigma) - 1))\n            self.cma_eigen_decomposition_needed = True\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 10, "feedback": "Evaluation timed out after 300 seconds.", "error": "Evaluation timed out after 300 seconds.", "parent_ids": ["e16e90d9-ba5f-43ec-b171-d0521e5848f9"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "f569b709-9980-4fcb-9384-98e1e6b07205", "fitness": 0.0, "name": "HybridOptimizer", "description": "A simplified hybrid optimizer with adaptive exploration and exploitation, incorporating a Cauchy mutation-based perturbation around the best solution and dynamic population size adjustment based on budget usage.", "code": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget=10000, dim=10, pop_size=20):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lb = -5.0\n        self.ub = 5.0\n        self.exploration_rate = 0.7\n        self.cauchy_scale = 0.1\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget_used = self.pop_size\n\n        self.best_index = np.argmin(self.fitness)\n        self.best_x = self.population[self.best_index].copy()\n        self.best_fitness = self.fitness[self.best_index].copy()\n\n        self.f_opt = self.best_fitness\n        self.x_opt = self.best_x\n        \n        while self.budget_used < self.budget:\n            new_population = np.zeros_like(self.population)\n            new_fitness = np.zeros_like(self.fitness)\n            \n            # Adaptive population size\n            remaining_budget_ratio = (self.budget - self.budget_used) / self.budget\n            current_pop_size = max(10, int(self.pop_size * (0.5 + 0.5 * remaining_budget_ratio))) # Reduce pop size later\n\n            for i in range(self.pop_size):\n                if i < current_pop_size:\n\n                    # Exploration with Cauchy mutation around best solution\n                    if np.random.rand() < self.exploration_rate:\n                        mutation = self.best_x + np.random.standard_cauchy(size=self.dim) * self.cauchy_scale\n                        mutation = np.clip(mutation, self.lb, self.ub)\n                        new_population[i] = mutation\n                        new_fitness[i] = func(new_population[i])\n                        self.budget_used += 1\n                    # Exploitation: Perturb current solution\n                    else:\n                         new_population[i] = self.population[i] + np.random.normal(0, 0.05, self.dim)\n                         new_population[i] = np.clip(new_population[i], self.lb, self.ub)\n                         new_fitness[i] = func(new_population[i])\n                         self.budget_used += 1\n                else:\n                    new_population[i] = self.population[i].copy()\n                    new_fitness[i] = self.fitness[i].copy()\n\n\n\n            for i in range(self.pop_size):\n                if i < current_pop_size and new_fitness[i] < self.fitness[i]:\n                    self.population[i] = new_population[i]\n                    self.fitness[i] = new_fitness[i]\n\n                    if new_fitness[i] < self.best_fitness:\n                        self.best_x = new_population[i].copy()\n                        self.best_fitness = new_fitness[i].copy()\n                        \n                        if self.best_fitness < self.f_opt:\n                            self.f_opt = self.best_fitness\n                            self.x_opt = self.best_x\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 11, "feedback": "The algorithm HybridOptimizer scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["e16e90d9-ba5f-43ec-b171-d0521e5848f9"], "operator": null, "metadata": {"aucs": [0]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "987ac598-4889-4d9e-b121-2512b12d2784", "fitness": 0.7127427158836264, "name": "AdaptiveHybridOptimizer", "description": "An adaptive hybrid optimizer that combines differential evolution with a covariance matrix adaptation evolution strategy (CMA-ES)-inspired Gaussian mutation, adjusting exploration based on a success history and the current search space.", "code": "import numpy as np\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget=10000, dim=10, pop_size=20, learning_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lb = -5.0\n        self.ub = 5.0\n        self.learning_rate = learning_rate\n        self.success_history = []  # Store success/failure of recent DE trials\n        self.mutation_scale = 0.1 * (self.ub - self.lb) # Initial mutation scale\n        self.de_rate = 0.7 #Probability of differential evolution\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget_used = self.pop_size\n\n        self.best_index = np.argmin(self.fitness)\n        self.best_x = self.population[self.best_index].copy()\n        self.best_fitness = self.fitness[self.best_index].copy()\n\n        self.f_opt = self.best_fitness\n        self.x_opt = self.best_x\n        \n        mutation_factor = 0.5  #DE mutation factor\n\n\n        while self.budget_used < self.budget:\n            new_population = np.zeros_like(self.population)\n            new_fitness = np.zeros(self.pop_size)\n\n            for i in range(self.pop_size):\n                old_fitness = self.fitness[i]\n                \n                if np.random.rand() < self.de_rate:\n                    # Differential Evolution\n                    indices = np.random.choice(self.pop_size, 3, replace=False)\n                    x_r1, x_r2, x_r3 = self.population[indices]\n                    mutation = x_r1 + mutation_factor * (x_r2 - x_r3)\n                    crossover = np.random.rand(self.dim) < 0.7  # Binomial Crossover\n                    new_population[i] = np.where(crossover, mutation, self.population[i])\n                    \n                else:\n                    # CMA-ES-inspired Gaussian Mutation with adaptive scale\n                    new_population[i] = self.population[i] + np.random.normal(0, self.mutation_scale, self.dim)\n\n                new_population[i] = np.clip(new_population[i], self.lb, self.ub)\n                new_fitness[i] = func(new_population[i])\n                self.budget_used += 1\n\n                if new_fitness[i] < self.fitness[i]:\n                    self.population[i] = new_population[i]\n                    self.fitness[i] = new_fitness[i]\n                    self.success_history.append(1)  # Mark as success\n                    \n                    if new_fitness[i] < self.best_fitness:\n                        self.best_x = new_population[i].copy()\n                        self.best_fitness = new_fitness[i].copy()\n                        if self.best_fitness < self.f_opt:\n                            self.f_opt = self.best_fitness\n                            self.x_opt = self.best_x\n                else:\n                    self.success_history.append(0)  # Mark as failure\n                \n                # Update mutation scale based on success history\n                if len(self.success_history) > 50:\n                    success_rate = np.mean(self.success_history[-50:])\n                    self.mutation_scale *= np.exp(self.learning_rate * (success_rate - 0.2)) # Adapt based on success rate\n\n                self.mutation_scale = np.clip(self.mutation_scale, 1e-6 * (self.ub - self.lb), 0.2 * (self.ub - self.lb)) #Keep mutation scale in reasonable bounds\n\n                if self.budget_used >= self.budget:\n                    break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 11, "feedback": "The algorithm AdaptiveHybridOptimizer scored 0.713 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["1457e643-466c-4b1b-bef8-3dfe4fa360e4"], "operator": null, "metadata": {"aucs": [0.268386221565279, 0.6061101832328362, 0.67129147977761, 0.8980378913129159, 0.8105652874663847, 0.8421682635675292, 0.7347430265659316, 0.7056991295247084, 0.7949440832095376, 0.7730145551245304, 0.8630077161999398, 0.9977168893131166, 0.3320215204545379, 0.8040461924873582, 0.9206552722729492, 0.8663854053113496, 0.7034801318887464, 0.8912272791888306, 0.26405067674487126, 0.5073031124635681]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "39ea7dab-669e-4397-90af-3640e802013e", "fitness": 0.6974411193651264, "name": "OrthogonalCauchyOptimizer", "description": "A hybrid optimizer employing orthogonal learning to estimate promising search directions, combined with a Cauchy mutation-based local search for refined exploration.", "code": "import numpy as np\n\nclass OrthogonalCauchyOptimizer:\n    def __init__(self, budget=10000, dim=10, pop_size=20, orthogonal_size=5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lb = -5.0\n        self.ub = 5.0\n        self.orthogonal_size = orthogonal_size\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget_used = self.pop_size\n\n        self.best_index = np.argmin(self.fitness)\n        self.best_x = self.population[self.best_index].copy()\n        self.best_fitness = self.fitness[self.best_index].copy()\n\n        self.f_opt = self.best_fitness\n        self.x_opt = self.best_x\n\n        de_rate = 0.7\n        mutation_factor = 0.5\n\n        while self.budget_used < self.budget:\n            new_population = np.zeros_like(self.population)\n            new_fitness = np.zeros(self.pop_size)\n\n            # Orthogonal Learning\n            orthogonal_matrix = self.generate_orthogonal_array(self.dim, self.orthogonal_size)\n            \n            for i in range(self.pop_size):\n                if np.random.rand() < de_rate:\n                    # Differential Evolution\n                    indices = np.random.choice(self.pop_size, 3, replace=False)\n                    x_r1, x_r2, x_r3 = self.population[indices]\n                    mutation = x_r1 + mutation_factor * (x_r2 - x_r3)\n                    crossover = np.random.rand(self.dim) < 0.7  # Binomial Crossover\n                    new_population[i] = np.where(crossover, mutation, self.population[i])\n                else:\n                    # Orthogonal-guided search\n                    index = np.random.randint(0, orthogonal_matrix.shape[0])\n                    direction = orthogonal_matrix[index]\n                    step_size = np.random.uniform(0, 0.1 * (self.ub - self.lb))  # Adjust step size\n\n                    new_population[i] = self.population[i] + step_size * direction\n                    \n\n                # Cauchy Mutation for Local Search\n                if np.random.rand() < 0.2:\n                    new_population[i] += 0.01 * (self.ub - self.lb) * np.random.standard_cauchy(size=self.dim)\n\n                new_population[i] = np.clip(new_population[i], self.lb, self.ub)\n                new_fitness[i] = func(new_population[i])\n                self.budget_used += 1\n\n                if new_fitness[i] < self.fitness[i]:\n                    self.population[i] = new_population[i]\n                    self.fitness[i] = new_fitness[i]\n\n                    if new_fitness[i] < self.best_fitness:\n                        self.best_x = new_population[i].copy()\n                        self.best_fitness = new_fitness[i].copy()\n                        if self.best_fitness < self.f_opt:\n                            self.f_opt = self.best_fitness\n                            self.x_opt = self.best_x\n                if self.budget_used >= self.budget:\n                    break\n\n        return self.f_opt, self.x_opt\n\n    def generate_orthogonal_array(self, n, strength):\n      # A simplified version for demonstration.  For real use, consider external libraries\n      # or precomputed arrays for robustness and better orthogonal properties.\n      # This example generates a random matrix, not a true orthogonal array.\n      orthogonal_array = np.random.rand(strength, n) - 0.5\n      return orthogonal_array", "configspace": "", "generation": 11, "feedback": "The algorithm OrthogonalCauchyOptimizer scored 0.697 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["1457e643-466c-4b1b-bef8-3dfe4fa360e4"], "operator": null, "metadata": {"aucs": [0.22931307938088197, 0.5465579069526121, 0.7128726543234274, 0.8991255242713552, 0.8230343650698887, 0.8266127935350289, 0.6705019082540695, 0.7357478415457284, 0.7909744786190678, 0.7646187265678794, 0.8933961326082999, 0.9886836748156144, 0.37076683421020173, 0.7278668299192526, 0.9116579535034287, 0.8213295562840541, 0.582337011166435, 0.8699547676060496, 0.2410337336460029, 0.5424366150232511]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "f6a755f7-7c04-48ce-910d-aa2fbc393c9a", "fitness": 0.7249520799567709, "name": "HybridOptimizer", "description": "A hybrid optimizer combining differential evolution with a covariance matrix adaptation-guided mutation, adapting exploration based on optimization progress and landscape features using success rates and magnitude of changes.", "code": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget=10000, dim=10, pop_size=20, de_rate=0.7, mutation_factor=0.5, initial_sigma=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lb = -5.0\n        self.ub = 5.0\n        self.de_rate = de_rate\n        self.mutation_factor = mutation_factor\n        self.initial_sigma = initial_sigma * (self.ub - self.lb)\n        self.sigma = self.initial_sigma\n        self.success_rates = np.zeros(self.pop_size)\n        self.mutation_magnitudes = np.zeros(self.pop_size)\n        self.learning_rate = 0.1\n        self.C = np.eye(self.dim)  # Covariance matrix\n\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget_used = self.pop_size\n\n        self.best_index = np.argmin(self.fitness)\n        self.best_x = self.population[self.best_index].copy()\n        self.best_fitness = self.fitness[self.best_index].copy()\n\n        self.f_opt = self.best_fitness\n        self.x_opt = self.best_x\n\n        while self.budget_used < self.budget:\n            new_population = np.zeros_like(self.population)\n            new_fitness = np.zeros(self.pop_size)\n            \n            # Adaptive sigma based on progress\n            adaptive_sigma = self.initial_sigma * (1 - self.budget_used / self.budget)\n\n            for i in range(self.pop_size):\n                old_fitness = self.fitness[i]\n\n                if np.random.rand() < self.de_rate:\n                    # Differential Evolution\n                    indices = np.random.choice(self.pop_size, 3, replace=False)\n                    x_r1, x_r2, x_r3 = self.population[indices]\n                    mutation = x_r1 + self.mutation_factor * (x_r2 - x_r3)\n                    crossover = np.random.rand(self.dim) < 0.7  # Binomial Crossover\n                    new_population[i] = np.where(crossover, mutation, self.population[i])\n                else:\n                    # CMA-Guided Mutation\n                    mutation_vector = np.random.multivariate_normal(np.zeros(self.dim), self.C)\n                    new_population[i] = self.population[i] + adaptive_sigma * mutation_vector\n\n                new_population[i] = np.clip(new_population[i], self.lb, self.ub)\n                new_fitness[i] = func(new_population[i])\n                self.budget_used += 1\n\n                if new_fitness[i] < old_fitness:\n                    # Success! Update individual\n                    self.population[i] = new_population[i]\n                    self.fitness[i] = new_fitness[i]\n                    self.success_rates[i] = (1 - self.learning_rate) * self.success_rates[i] + self.learning_rate\n                    self.mutation_magnitudes[i] = (1 - self.learning_rate) * self.mutation_magnitudes[i] + self.learning_rate * np.linalg.norm(new_population[i] - self.population[i])\n\n                    if new_fitness[i] < self.best_fitness:\n                        self.best_x = new_population[i].copy()\n                        self.best_fitness = new_fitness[i].copy()\n                        if self.best_fitness < self.f_opt:\n                            self.f_opt = self.best_fitness\n                            self.x_opt = self.best_x\n                else:\n                     # Failure, decrease mutation magnitude\n                    self.success_rates[i] = (1 - self.learning_rate) * self.success_rates[i]\n                    self.mutation_magnitudes[i] *= (1 - self.learning_rate)\n\n                if self.budget_used >= self.budget:\n                    break\n            \n            # Update Covariance Matrix (simplified)\n            if self.budget_used > self.budget/2:  # Update only after some exploration\n                diff = self.population - np.mean(self.population, axis=0)\n                self.C = (1 - self.learning_rate) * self.C + self.learning_rate * np.cov(diff.T)\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 11, "feedback": "The algorithm HybridOptimizer scored 0.725 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["85378eab-540e-488e-9005-2e7b547753b4"], "operator": null, "metadata": {"aucs": [0.21515017900151623, 0.6894345646393344, 0.7515518622527071, 0.8929015561170831, 0.8168080890459414, 0.8739723865533928, 0.7375234973481014, 0.7872843801376547, 0.8318101252205948, 0.7585433681602711, 0.8609167533014954, 0.9956565866826199, 0.3934309305681093, 0.7444498069801398, 0.9327655776600072, 0.8623124975624643, 0.7016118169949304, 0.8864163546070954, 0.2630073012972399, 0.5034939650047201]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "e0ac4224-d9fa-451f-b7d6-ed479cce2764", "fitness": "-inf", "name": "HybridOptimizer", "description": "A dynamically self-adjusting hybrid algorithm that balances exploration and exploitation by probabilistically switching between differential evolution, Gaussian mutation, and a population-based update, guided by the success rate of recent mutations.", "code": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget=10000, dim=10, pop_size=20):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lb = -5.0\n        self.ub = 5.0\n        self.mutation_success_rate = 0.5\n        self.learning_rate = 0.1\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget_used = self.pop_size\n\n        self.best_index = np.argmin(self.fitness)\n        self.best_x = self.population[self.best_index].copy()\n        self.best_fitness = self.fitness[self.best_index].copy()\n\n        self.f_opt = self.best_fitness\n        self.x_opt = self.best_x\n        \n        while self.budget_used < self.budget:\n            new_population = np.zeros_like(self.population)\n            new_fitness = np.zeros_like(self.fitness)\n\n            for i in range(self.pop_size):\n                # Adaptive strategy selection based on success rate\n                if np.random.rand() < self.mutation_success_rate:\n                    # Differential Evolution\n                    indices = np.random.choice(self.pop_size, 3, replace=False)\n                    x_r1, x_r2, x_r3 = self.population[indices]\n                    mutation = x_r1 + 0.5 * (x_r2 - x_r3)\n                    crossover = np.random.rand(self.dim) < 0.5\n                    new_x = np.where(crossover, mutation, self.population[i])\n                else:\n                    # Gaussian Mutation around the best\n                    new_x = self.best_x + np.random.normal(0, 0.1, self.dim)\n                \n                #Population Based Update\n                if np.random.rand() < 0.1: #Small chance to improve exploration\n                    new_x = self.population[i] + np.random.normal(0, 0.05, self.dim)\n\n                new_x = np.clip(new_x, self.lb, self.ub)\n                new_fitness_val = func(new_x)\n                self.budget_used += 1\n                \n                if new_fitness_val < self.fitness[i]:\n                    new_population[i] = new_x\n                    new_fitness[i] = new_fitness_val\n                    \n                    # Update success rate\n                    self.mutation_success_rate += self.learning_rate * (1 - self.mutation_success_rate)\n                else:\n                    new_population[i] = self.population[i]\n                    new_fitness[i] = self.fitness[i]\n                    # Update success rate\n                    self.mutation_success_rate -= self.learning_rate * self.mutation_success_rate\n                    \n                if new_fitness_val < self.best_fitness:\n                    self.best_x = new_x.copy()\n                    self.best_fitness = new_fitness_val.copy()\n                    if self.best_fitness < self.f_opt:\n                        self.f_opt = self.best_fitness\n                        self.x_opt = self.best_x\n\n                if self.budget_used >= self.budget:\n                    break\n\n            self.population = new_population\n            self.fitness = new_fitness\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 11, "feedback": "In the code, line 66, in __call__, the following error occurred:\nAttributeError: 'float' object has no attribute 'copy'\nOn line: self.best_fitness = new_fitness_val.copy()", "error": "In the code, line 66, in __call__, the following error occurred:\nAttributeError: 'float' object has no attribute 'copy'\nOn line: self.best_fitness = new_fitness_val.copy()", "parent_ids": ["e16e90d9-ba5f-43ec-b171-d0521e5848f9"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "671d80cd-ee9d-467e-977c-a3de4814f7ba", "fitness": "-inf", "name": "SOMHybridOptimizer", "description": "A hybrid optimizer employing a self-organizing map (SOM) to cluster solutions and adaptively allocate resources to different search strategies based on cluster performance, enhancing exploration and exploitation balance.", "code": "import numpy as np\nfrom minisom import MiniSom\n\nclass SOMHybridOptimizer:\n    def __init__(self, budget=10000, dim=10, pop_size=50, som_grid_size=5, initial_de_rate=0.4, initial_pso_rate=0.3, local_rate=0.3, restart_trigger=0.05):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.som_grid_size = som_grid_size\n        self.de_rate = initial_de_rate\n        self.pso_rate = initial_pso_rate\n        self.local_rate = local_rate\n        self.restart_trigger = restart_trigger\n        self.initial_de_rate = initial_de_rate\n        self.initial_pso_rate = initial_pso_rate\n        self.initial_local_rate = initial_local_rate\n        self.som = MiniSom(som_grid_size, som_grid_size, dim, sigma=0.3, learning_rate=0.5) # Initialize SOM\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget_used = self.pop_size\n        \n        self.pbest_population = self.population.copy()\n        self.pbest_fitness = self.fitness.copy()\n\n        self.gbest_index = np.argmin(self.fitness)\n        self.gbest_x = self.population[self.gbest_index].copy()\n        self.gbest_fitness = self.fitness[self.gbest_index].copy()\n        \n        self.f_opt = self.gbest_fitness\n        self.x_opt = self.gbest_x\n\n        self.no_improvement_count = 0\n        self.max_no_improvement = int(self.budget * self.restart_trigger)\n        \n        # Initial SOM training\n        self.som.train_random(self.population, 100)\n\n        cluster_performance = np.zeros((self.som_grid_size, self.som_grid_size))\n        cluster_counts = np.zeros((self.som_grid_size, self.som_grid_size))\n\n        while self.budget_used < self.budget:\n            new_population = np.zeros_like(self.population)\n            new_fitness = np.zeros(self.pop_size)\n\n            for i in range(self.pop_size):\n                # Find the winning SOM node for the current solution\n                winner = self.som.winner(self.population[i])\n                cluster_counts[winner[0], winner[1]] += 1\n                \n                # Adjust search strategy probabilities based on cluster performance\n                normalized_performance = cluster_performance / np.sum(cluster_performance) if np.sum(cluster_performance) > 0 else np.ones_like(cluster_performance) / cluster_performance.size\n                de_prob = self.de_rate # + normalized_performance[winner[0], winner[1]] * 0.2 # Bias towards better performing clusters\n                pso_prob = de_prob + self.pso_rate #+ normalized_performance[winner[0], winner[1]] * 0.2\n                rand = np.random.rand()\n\n                # Differential Evolution\n                if rand < de_prob:\n                    indices = np.random.choice(self.pop_size, 3, replace=False)\n                    x_r1, x_r2, x_r3 = self.population[indices]\n                    mutation = x_r1 + 0.5 * (x_r2 - x_r3)\n                    crossover = np.random.rand(self.dim) < 0.5\n                    new_population[i] = np.where(crossover, mutation, self.population[i])\n                    new_population[i] = np.clip(new_population[i], func.bounds.lb, func.bounds.ub)\n                    new_fitness[i] = func(new_population[i])\n                    self.budget_used += 1\n\n                # Particle Swarm Optimization\n                elif rand < pso_prob:\n                    inertia = 0.7\n                    cognitive_coeff = 1.5\n                    social_coeff = 1.5\n                    velocity = inertia * (self.population[i] - self.pbest_population[i]) + cognitive_coeff * np.random.rand(self.dim) * (self.pbest_population[i] - self.population[i]) + social_coeff * np.random.rand(self.dim) * (self.gbest_x - self.population[i])\n                    new_population[i] = self.population[i] + velocity\n                    new_population[i] = np.clip(new_population[i], func.bounds.lb, func.bounds.ub)\n                    new_fitness[i] = func(new_population[i])\n                    self.budget_used += 1\n                    \n                # Local Search with Cauchy Mutation\n                else:\n                    new_population[i] = self.population[i] + 0.01 * np.random.standard_cauchy(size=self.dim)\n                    new_population[i] = np.clip(new_population[i], func.bounds.lb, func.bounds.ub)\n                    new_fitness[i] = func(new_population[i])\n                    self.budget_used += 1\n\n            for i in range(self.pop_size):\n                if new_fitness[i] < self.fitness[i]:\n                    # Update cluster performance based on improvement\n                    winner = self.som.winner(self.population[i])\n                    cluster_performance[winner[0], winner[1]] += (self.fitness[i] - new_fitness[i])  # Reward improvements\n\n                    self.population[i] = new_population[i]\n                    self.fitness[i] = new_fitness[i]\n\n                    if new_fitness[i] < self.pbest_fitness[i]:\n                        self.pbest_population[i] = new_population[i]\n                        self.pbest_fitness[i] = new_fitness[i]\n\n            self.gbest_index = np.argmin(self.fitness)\n            if self.fitness[self.gbest_index] < self.gbest_fitness:\n                self.gbest_x = self.population[self.gbest_index].copy()\n                self.gbest_fitness = self.fitness[self.gbest_index].copy()\n                \n                if self.gbest_fitness < self.f_opt:\n                    self.f_opt = self.gbest_fitness\n                    self.x_opt = self.gbest_x\n                    self.no_improvement_count = 0\n                else:\n                    self.no_improvement_count += 1\n            else:\n                self.no_improvement_count += 1\n\n            # Restart mechanism\n            if self.no_improvement_count > self.max_no_improvement:\n                self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                self.fitness = np.array([func(x) for x in self.population])\n                self.budget_used += self.pop_size\n\n                self.pbest_population = self.population.copy()\n                self.pbest_fitness = self.fitness.copy()\n\n                self.gbest_index = np.argmin(self.fitness)\n                self.gbest_x = self.population[self.gbest_index].copy()\n                self.gbest_fitness = self.fitness[self.gbest_index].copy()\n                \n                self.no_improvement_count = 0\n                \n                # Retrain SOM after restart\n                self.som = MiniSom(self.som_grid_size, self.som_grid_size, self.dim, sigma=0.3, learning_rate=0.5)\n                self.som.train_random(self.population, 100)\n                cluster_performance[:] = 0\n                cluster_counts[:] = 0\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 11, "feedback": "In the code, line 2, in <module>, the following error occurred:\nModuleNotFoundError: No module named 'minisom'\nOn line: from minisom import MiniSom", "error": "In the code, line 2, in <module>, the following error occurred:\nModuleNotFoundError: No module named 'minisom'\nOn line: from minisom import MiniSom", "parent_ids": ["1e74c5f0-f489-41e9-b124-27e514d9a233"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "7ffdd91d-bf49-4e1b-8076-e8a525bcd2d8", "fitness": 0.59059059680466, "name": "HybridOptimizer", "description": "A novel hybrid optimizer combining a modified differential evolution with a self-adjusting mutation strategy based on covariance matrix adaptation, further enhanced with a Cauchy mutation-based local search.", "code": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget=10000, dim=10, pop_size=20, exploration_rate=0.7):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lb = -5.0\n        self.ub = 5.0\n        self.exploration_rate = exploration_rate\n        self.mutation_scaling = 0.1 #Initial mutation scaling\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget_used = self.pop_size\n\n        self.best_index = np.argmin(self.fitness)\n        self.best_x = self.population[self.best_index].copy()\n        self.best_fitness = self.fitness[self.best_index].copy()\n\n        self.f_opt = self.best_fitness\n        self.x_opt = self.best_x\n\n        # Initialize covariance matrix for CMA-ES inspired mutation\n        self.covariance = np.eye(self.dim)\n        self.success_rate = 0.5  #Initial success rate for CMA-ES adaptation\n\n\n        while self.budget_used < self.budget:\n            new_population = np.zeros_like(self.population)\n            new_fitness = np.zeros(self.pop_size)\n            \n            # Dynamic adjustment of exploration based on remaining budget and success rate\n            remaining_budget_ratio = (self.budget - self.budget_used) / self.budget\n            current_exploration_rate = self.exploration_rate * remaining_budget_ratio + (1 - self.success_rate) * 0.2 #Bias towards exploration when budget remains or success is low\n            \n            mutation_scaling = self.mutation_scaling * (1 + (self.success_rate - 0.5))  #Adapt the scale based on success\n\n\n            for i in range(self.pop_size):\n                if np.random.rand() < current_exploration_rate:\n                    # Modified Differential Evolution with CMA-ES inspired mutation\n                    indices = np.random.choice(self.pop_size, 3, replace=False)\n                    x_r1, x_r2, x_r3 = self.population[indices]\n                    \n                    #CMA-ES like mutation\n                    mutation = x_r1 + mutation_scaling * np.random.multivariate_normal(np.zeros(self.dim), self.covariance) + 0.5 * (x_r2 - x_r3)\n\n                    crossover = np.random.rand(self.dim) < 0.7  # Increased crossover probability\n                    new_population[i] = np.where(crossover, mutation, self.population[i])\n\n\n                else:\n                    # Cauchy mutation based local search around the best solution\n                    new_population[i] = self.best_x + 0.01 * np.random.standard_cauchy(size=self.dim) # Smaller cauchy step\n\n\n                new_population[i] = np.clip(new_population[i], self.lb, self.ub)\n                new_fitness[i] = func(new_population[i])\n                self.budget_used += 1\n                \n                if new_fitness[i] < self.best_fitness:\n                    self.best_x = new_population[i].copy()\n                    self.best_fitness = new_fitness[i].copy()\n                    if self.best_fitness < self.f_opt:\n                        self.f_opt = self.best_fitness\n                        self.x_opt = self.best_x\n            \n            # Update population based on fitness\n            num_successful = 0\n            for i in range(self.pop_size):\n                if new_fitness[i] < self.fitness[i]:\n                    self.population[i] = new_population[i]\n                    self.fitness[i] = new_fitness[i]\n                    num_successful += 1\n\n            # Update success rate for CMA-ES adaptation\n            self.success_rate = 0.8 * self.success_rate + 0.2 * (num_successful / self.pop_size)\n\n            #Adapt the covariance matrix slowly\n            if self.budget_used % self.pop_size == 0:\n                diff = self.population - np.mean(self.population, axis=0)\n                self.covariance = 0.9 * self.covariance + 0.1 * np.cov(diff.T)\n                self.covariance = (self.covariance + self.covariance.T) / 2 #Ensure symmetry\n                #Regularize the covariance matrix to avoid singularity\n                self.covariance += 1e-6 * np.eye(self.dim)\n\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 11, "feedback": "The algorithm HybridOptimizer scored 0.591 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["e16e90d9-ba5f-43ec-b171-d0521e5848f9"], "operator": null, "metadata": {"aucs": [0.24417796456972796, 0.187102672680876, 0.6999172824945659, 0.19908913519498417, 0.7068712205118702, 0.8115503084392436, 0.307397237856046, 0.6242286182329704, 0.7427793298533756, 0.7604637886427563, 0.870958130881383, 0.9961205255660259, 0.23991606015334777, 0.7553799463066899, 0.905160424095493, 0.7885406797680831, 0.2816231464115486, 0.9295637081304841, 0.28119556796053213, 0.4797761883431938]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "46cb0d2c-ca37-459e-8e0a-2d1a807e1576", "fitness": 0.6407354914530081, "name": "AdaptiveHybridOptimizer", "description": "An adaptive hybrid optimizer that combines a modified differential evolution with a covariance matrix adaptation evolution strategy (CMA-ES) for enhanced exploration and exploitation, dynamically adjusting search parameters based on optimization progress.", "code": "import numpy as np\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget=10000, dim=10, pop_size=20):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lb = -5.0\n        self.ub = 5.0\n        self.cma_es_mean = np.random.uniform(self.lb, self.ub, size=self.dim)\n        self.cma_es_sigma = 0.5\n        self.cma_es_C = np.eye(self.dim) #Covariance Matrix\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget_used = self.pop_size\n\n        self.best_index = np.argmin(self.fitness)\n        self.best_x = self.population[self.best_index].copy()\n        self.best_fitness = self.fitness[self.best_index].copy()\n\n        self.f_opt = self.best_fitness\n        self.x_opt = self.best_x\n        \n        de_rate = 0.7\n        cma_es_rate = 0.3\n\n\n        while self.budget_used < self.budget:\n            new_population = np.zeros_like(self.population)\n            new_fitness = np.zeros_like(self.fitness)\n            \n            # Dynamic adjustment of exploration rates based on budget remaining\n            remaining_budget_ratio = (self.budget - self.budget_used) / self.budget\n            current_de_rate = de_rate * remaining_budget_ratio + 0.1 # Ensure minimal DE\n            current_cma_es_rate = cma_es_rate * (1 - remaining_budget_ratio) + 0.1 # Ensure minimal CMA-ES\n\n            for i in range(self.pop_size):\n                if np.random.rand() < current_de_rate:\n                    # Modified Differential Evolution with adaptive parameters\n                    indices = np.random.choice(self.pop_size, 3, replace=False)\n                    x_r1, x_r2, x_r3 = self.population[indices]\n                    mutation_factor = 0.5 + 0.2 * np.random.rand() # adaptive mutation factor\n                    mutation = x_r1 + mutation_factor * (x_r2 - x_r3)\n                    crossover = np.random.rand(self.dim) < 0.7  # binomial crossover\n                    new_population[i] = np.where(crossover, mutation, self.population[i])\n                elif np.random.rand() < current_cma_es_rate:\n                    # CMA-ES component\n                    new_population[i] = np.random.multivariate_normal(self.cma_es_mean, self.cma_es_sigma**2 * self.cma_es_C)\n                else:\n                    # Random restart for diversity\n                    new_population[i] = np.random.uniform(self.lb, self.ub, self.dim)\n\n                new_population[i] = np.clip(new_population[i], self.lb, self.ub)\n                new_fitness[i] = func(new_population[i])\n                self.budget_used += 1\n\n                if new_fitness[i] < self.best_fitness:\n                    self.best_x = new_population[i].copy()\n                    self.best_fitness = new_fitness[i].copy()\n                    if self.best_fitness < self.f_opt:\n                        self.f_opt = self.best_fitness\n                        self.x_opt = self.best_x\n\n\n            # Update population and fitness (elitist replacement)\n            for i in range(self.pop_size):\n                if new_fitness[i] < self.fitness[i]:\n                    self.population[i] = new_population[i]\n                    self.fitness[i] = new_fitness[i]\n\n            # Update CMA-ES parameters based on best solution found in this generation\n            if self.best_fitness < np.min(self.fitness):\n              self.cma_es_mean = self.best_x.copy()\n              self.cma_es_sigma *= np.exp(0.1*(1-self.budget_used/self.budget))  #Adaptively reducing the step size, i.e. exploitation\n\n              #Simplified adaptation of Covariance Matrix\n              z = (self.best_x - self.cma_es_mean) / self.cma_es_sigma\n              self.cma_es_C = (1-0.1) * self.cma_es_C + 0.1 * np.outer(z,z)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 11, "feedback": "The algorithm AdaptiveHybridOptimizer scored 0.641 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["e16e90d9-ba5f-43ec-b171-d0521e5848f9"], "operator": null, "metadata": {"aucs": [0.20388948301258059, 0.38729929139777675, 0.6487917409722204, 0.8846647845853008, 0.7263549033620113, 0.8216810976356299, 0.6433805575579309, 0.6406629704664432, 0.7504315829485689, 0.4789785174304746, 0.8394646585823685, 0.996088086662614, 0.3155026122307085, 0.6713651388882008, 0.92258066391713, 0.8207529218556661, 0.5223576964770689, 0.8185070054311568, 0.20175574875532365, 0.5202003668909889]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "ace9c308-e21c-4b9c-9aea-3f5dadde5b8c", "fitness": "-inf", "name": "SOMHybridOptimizer", "description": "A hybrid optimizer that uses a self-organizing map (SOM) to adaptively adjust mutation parameters and dynamically selects between differential evolution and a SOM-guided mutation strategy based on local landscape characteristics.", "code": "import numpy as np\nfrom minisom import MiniSom\n\nclass SOMHybridOptimizer:\n    def __init__(self, budget=10000, dim=10, pop_size=20, de_rate=0.7, mutation_factor=0.5, som_grid_size=5, initial_sigma=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lb = -5.0\n        self.ub = 5.0\n        self.de_rate = de_rate\n        self.mutation_factor = mutation_factor\n        self.som_grid_size = som_grid_size\n        self.initial_sigma = initial_sigma * (self.ub - self.lb)\n        self.sigma = self.initial_sigma\n        self.learning_rate = 0.1\n        self.som = MiniSom(self.som_grid_size, self.som_grid_size, self.dim, sigma=0.3, learning_rate=0.1)\n        self.mutation_params = np.ones((self.som_grid_size, self.som_grid_size, 2))  # [DE rate, Mutation factor]\n        self.population = None\n        self.fitness = None\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.budget_used = 0\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget_used = self.pop_size\n\n        self.best_index = np.argmin(self.fitness)\n        self.best_x = self.population[self.best_index].copy()\n        self.best_fitness = self.fitness[self.best_index].copy()\n\n        self.f_opt = self.best_fitness\n        self.x_opt = self.best_x\n\n        # Initial training of SOM\n        self.som.train_random(self.population, 100)\n\n        while self.budget_used < self.budget:\n            new_population = np.zeros_like(self.population)\n            new_fitness = np.zeros(self.pop_size)\n\n            for i in range(self.pop_size):\n                old_fitness = self.fitness[i]\n                winner = self.som.winner(self.population[i])\n                de_rate = self.mutation_params[winner[0], winner[1], 0]\n                mutation_factor = self.mutation_params[winner[0], winner[1], 1]\n                adaptive_sigma = self.initial_sigma * (1 - self.budget_used / self.budget)\n\n                if np.random.rand() < de_rate:\n                    # Differential Evolution\n                    indices = np.random.choice(self.pop_size, 3, replace=False)\n                    x_r1, x_r2, x_r3 = self.population[indices]\n                    mutation = x_r1 + mutation_factor * (x_r2 - x_r3)\n                    crossover = np.random.rand(self.dim) < 0.7  # Binomial Crossover\n                    new_population[i] = np.where(crossover, mutation, self.population[i])\n                else:\n                    # SOM-Guided Mutation\n                    mutation_vector = np.random.normal(0, adaptive_sigma, self.dim)\n                    new_population[i] = self.population[i] + mutation_vector\n\n                new_population[i] = np.clip(new_population[i], self.lb, self.ub)\n                new_fitness[i] = func(new_population[i])\n                self.budget_used += 1\n\n                if new_fitness[i] < old_fitness:\n                    # Success! Update individual\n                    self.population[i] = new_population[i]\n                    self.fitness[i] = new_fitness[i]\n                    self.som.update(self.population[i], winner, self.learning_rate)  # Update SOM\n                    \n                    # Adaptive parameter update\n                    self.mutation_params[winner[0], winner[1], 0] = min(1.0, de_rate + self.learning_rate)\n                    self.mutation_params[winner[0], winner[1], 1] = min(1.0, mutation_factor + self.learning_rate)\n\n\n                    if new_fitness[i] < self.best_fitness:\n                        self.best_x = new_population[i].copy()\n                        self.best_fitness = new_fitness[i].copy()\n                        if self.best_fitness < self.f_opt:\n                            self.f_opt = self.best_fitness\n                            self.x_opt = self.best_x\n\n                else:\n                    # Failure, decrease mutation parameters\n                    self.mutation_params[winner[0], winner[1], 0] = max(0.0, de_rate - self.learning_rate)\n                    self.mutation_params[winner[0], winner[1], 1] = max(0.0, mutation_factor - self.learning_rate)\n\n                if self.budget_used >= self.budget:\n                    break\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 12, "feedback": "In the code, line 2, in <module>, the following error occurred:\nModuleNotFoundError: No module named 'minisom'\nOn line: from minisom import MiniSom", "error": "In the code, line 2, in <module>, the following error occurred:\nModuleNotFoundError: No module named 'minisom'\nOn line: from minisom import MiniSom", "parent_ids": ["f6a755f7-7c04-48ce-910d-aa2fbc393c9a"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "e12e226d-deab-40c9-ba08-e2d3d629b647", "fitness": 0.7205409692985281, "name": "HybridOptimizer", "description": "A simplified hybrid optimizer using differential evolution and self-adaptive Gaussian mutation with success-history adaptation for exploration control.", "code": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget=10000, dim=10, pop_size=20, de_rate=0.7, mutation_factor=0.5, learning_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lb = -5.0\n        self.ub = 5.0\n        self.de_rate = de_rate\n        self.mutation_factor = mutation_factor\n        self.learning_rate = learning_rate\n        self.success_rate = 0.5  # Global success rate\n        self.mutation_scale = 0.1 * (self.ub - self.lb) # Initial mutation scale\n\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget_used = self.pop_size\n\n        self.best_index = np.argmin(self.fitness)\n        self.best_x = self.population[self.best_index].copy()\n        self.best_fitness = self.fitness[self.best_index].copy()\n\n        self.f_opt = self.best_fitness\n        self.x_opt = self.best_x\n\n        while self.budget_used < self.budget:\n            new_population = np.zeros_like(self.population)\n            new_fitness = np.zeros(self.pop_size)\n\n            for i in range(self.pop_size):\n                old_fitness = self.fitness[i]\n\n                if np.random.rand() < self.de_rate:\n                    # Differential Evolution\n                    indices = np.random.choice(self.pop_size, 3, replace=False)\n                    x_r1, x_r2, x_r3 = self.population[indices]\n                    mutation = x_r1 + self.mutation_factor * (x_r2 - x_r3)\n                    crossover = np.random.rand(self.dim) < 0.7  # Binomial Crossover\n                    new_population[i] = np.where(crossover, mutation, self.population[i])\n                else:\n                    # Self-Adjusting Gaussian Mutation\n                    mutation_vector = np.random.normal(0, self.mutation_scale, self.dim)\n                    new_population[i] = self.population[i] + mutation_vector\n\n                new_population[i] = np.clip(new_population[i], self.lb, self.ub)\n                new_fitness[i] = func(new_population[i])\n                self.budget_used += 1\n\n                if new_fitness[i] < old_fitness:\n                    # Success! Update individual\n                    self.population[i] = new_population[i]\n                    self.fitness[i] = new_fitness[i]\n\n                    # Update global success rate and mutation scale\n                    self.success_rate = (1 - self.learning_rate) * self.success_rate + self.learning_rate\n                    self.mutation_scale *= (1 + self.learning_rate * (self.success_rate - 0.5) )\n\n\n                    if new_fitness[i] < self.best_fitness:\n                        self.best_x = new_population[i].copy()\n                        self.best_fitness = new_fitness[i].copy()\n                        if self.best_fitness < self.f_opt:\n                            self.f_opt = self.best_fitness\n                            self.x_opt = self.best_x\n                else:\n                    # Failure, decrease mutation scale\n                    self.success_rate = (1 - self.learning_rate) * self.success_rate\n                    self.mutation_scale *= (1 - self.learning_rate * (0.5 - self.success_rate))\n\n\n                if self.budget_used >= self.budget:\n                    break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 12, "feedback": "The algorithm HybridOptimizer scored 0.721 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["85378eab-540e-488e-9005-2e7b547753b4"], "operator": null, "metadata": {"aucs": [0.2885356122890579, 0.6777206205225417, 0.7306682669617531, 0.9162618988626738, 0.7411387599469585, 0.8696695347343304, 0.7533036939709542, 0.7431687650574965, 0.8236388076056982, 0.7554571860638012, 0.8534630395144909, 0.9912038837261258, 0.3103203344925316, 0.7794201936418758, 0.9231307328684593, 0.8597562613302216, 0.6229360228834497, 0.8848466646062478, 0.3578909111383387, 0.5282881957535545]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "f69cfeb9-9c5c-4084-8ad6-47c7d2586cc0", "fitness": 0.7395332331925355, "name": "HybridOptimizer", "description": "Dynamically adjusts DE/Gaussian mutation rates based on recent success, coupled with a CMA-ES inspired adaptation of the Gaussian mutation's covariance matrix.", "code": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget=10000, dim=10, pop_size=20, initial_de_rate=0.7):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lb = -5.0\n        self.ub = 5.0\n        self.initial_de_rate = initial_de_rate\n        self.de_rate = initial_de_rate  # Initialize de_rate\n        self.mutation_factor = 0.5\n        self.success_history = []\n        self.history_length = 10\n        self.mutation_scale = 0.1 * (self.ub - self.lb) # Initial scale\n\n        # CMA-ES like parameters\n        self.C = np.eye(dim)  # Covariance matrix\n        self.pc = np.zeros(dim) # Evolution path for C\n        self.ps = np.zeros(dim)  # Evolution path for sigma (mutation scale)\n        self.c_sigma = 1/np.sqrt(dim) # Learning rate for sigma\n        self.c_c = 1/np.sqrt(dim)\n        self.c_mu = 0.1 # Rank-one update learning rate\n        self.damps = 1 + 2*max(0, np.sqrt((self.pop_size-1)/(dim+1)) - 1) # Damping for sigma\n        self.mu = self.pop_size // 2 # Number of parents for recombination\n        self.weights = np.log(self.mu+1/2) - np.log(np.arange(1, self.mu+1))\n        self.weights = self.weights / np.sum(self.weights)\n\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget_used = self.pop_size\n\n        self.best_index = np.argmin(self.fitness)\n        self.best_x = self.population[self.best_index].copy()\n        self.best_fitness = self.fitness[self.best_index].copy()\n\n        self.f_opt = self.best_fitness\n        self.x_opt = self.best_x\n\n        while self.budget_used < self.budget:\n            new_population = np.zeros_like(self.population)\n            new_fitness = np.zeros(self.pop_size)\n            success_count = 0\n            \n            # Recombine best individuals for CMA-ES\n            sorted_indices = np.argsort(self.fitness)\n            parents = self.population[sorted_indices[:self.mu]]\n            offspring_mean = np.sum(self.weights.reshape(-1, 1) * parents, axis=0)\n\n            for i in range(self.pop_size):\n                if np.random.rand() < self.de_rate:\n                    # Differential Evolution\n                    indices = np.random.choice(self.pop_size, 3, replace=False)\n                    x_r1, x_r2, x_r3 = self.population[indices]\n                    mutation = x_r1 + self.mutation_factor * (x_r2 - x_r3)\n                    crossover = np.random.rand(self.dim) < 0.7  # Binomial Crossover\n                    new_population[i] = np.where(crossover, mutation, self.population[i])\n                else:\n                    # CMA-ES-inspired Gaussian Mutation\n                    z = np.random.multivariate_normal(np.zeros(self.dim), self.C)\n                    new_population[i] = offspring_mean + self.mutation_scale * z\n\n                new_population[i] = np.clip(new_population[i], self.lb, self.ub)\n                new_fitness[i] = func(new_population[i])\n                self.budget_used += 1\n\n                if new_fitness[i] < self.fitness[i]:\n                    self.success_history.append(1)\n                    success_count += 1\n                    self.population[i] = new_population[i]\n                    self.fitness[i] = new_fitness[i]\n\n                    if new_fitness[i] < self.best_fitness:\n                        self.best_x = new_population[i].copy()\n                        self.best_fitness = new_fitness[i].copy()\n                        if self.best_fitness < self.f_opt:\n                            self.f_opt = self.best_fitness\n                            self.x_opt = self.best_x\n                else:\n                    self.success_history.append(0)\n\n                if self.budget_used >= self.budget:\n                    break\n\n            # Adapt DE rate based on recent success\n            if len(self.success_history) > self.history_length:\n                self.success_history = self.success_history[-self.history_length:]\n                success_rate = np.mean(self.success_history)\n                self.de_rate = min(1.0, max(0.1, self.de_rate + 0.1 * (success_rate - 0.5)))\n\n            # CMA-ES Adaptation (simplified)\n            ps = (1 - self.c_sigma) * self.ps + np.sqrt(self.c_sigma * (2 - self.c_sigma)) * (self.best_x - offspring_mean) / self.mutation_scale\n            self.mutation_scale *= np.exp((self.c_sigma / self.damps) * (np.linalg.norm(ps) / np.sqrt(self.dim) - 1))\n\n            # Simplified rank-one update\n            self.pc = (1 - self.c_c) * self.pc + np.sqrt(self.c_c * (2 - self.c_c)) * (self.best_x - offspring_mean) / self.mutation_scale\n            self.C = (1 - self.c_mu) * self.C + self.c_mu * np.outer(self.pc, self.pc)\n\n            # Keep C positive definite\n            try:\n                np.linalg.cholesky(self.C)\n            except np.linalg.LinAlgError:\n                self.C = np.eye(self.dim) # Reset if not positive definite\n                self.pc = np.zeros(self.dim)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 12, "feedback": "The algorithm HybridOptimizer scored 0.740 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["1457e643-466c-4b1b-bef8-3dfe4fa360e4"], "operator": null, "metadata": {"aucs": [0.21921759225439252, 0.3938705246656383, 0.9139775235294041, 0.9306749213824084, 0.9295499920132859, 0.940124289898602, 0.7644732237945528, 0.9168257918270839, 0.9297061505118864, 0.23931782282941805, 0.958208834714219, 0.9954689797896649, 0.4612210190551794, 0.8491911230841616, 0.9645522365695561, 0.9280021434145962, 0.7397292266674432, 0.9507470896889294, 0.26004455153253103, 0.5057616266277549]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "3cca99ef-e2ef-4505-84ac-7cb24058f672", "fitness": 0.5806368084160397, "name": "HybridOptimizer", "description": "An enhanced hybrid optimizer combining differential evolution with CMA-ES-guided mutation, dynamically adjusting exploration and exploitation via success history, mutation magnitude, and landscape sensitivity estimation.", "code": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget=10000, dim=10, pop_size=20, de_rate=0.7, mutation_factor=0.5, initial_sigma=0.1, learning_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lb = -5.0\n        self.ub = 5.0\n        self.de_rate = de_rate\n        self.mutation_factor = mutation_factor\n        self.initial_sigma = initial_sigma * (self.ub - self.lb)\n        self.sigma = self.initial_sigma\n        self.success_rates = np.zeros(self.pop_size)\n        self.mutation_magnitudes = np.zeros(self.pop_size)\n        self.learning_rate = learning_rate\n        self.C = np.eye(self.dim)  # Covariance matrix\n        self.ps = np.zeros(self.dim) # Evolution path for rank-one update\n        self.cs = 0.3 # Learning rate for ps\n        self.damps = 1 + (2*max(0, np.log(self.dim/2))) / ((self.pop_size+1e-9)**0.5) # Dampening for sigma\n\n        self.mu = self.pop_size // 2  # Number of parents/selected individuals\n\n        self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n        self.weights /= np.sum(self.weights)\n\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n\n        self.cc = (4 + self.mueff/self.dim) / (self.dim + 4 + 2*self.mueff/self.dim)\n        self.c1 = 2 / ((self.dim + 1.3)**2 + self.mueff)\n        self.cmu = min(1 - self.c1, 2 * (self.mueff - 2 + 1/self.mueff) / ((self.dim + 2)**2 + self.mueff))\n        self.B = np.eye(self.dim)\n        self.D = np.ones(self.dim)\n        self.chiN = self.dim**0.5 * (1 - (1/(4*self.dim)) + 1/(21*self.dim**2))\n\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget_used = self.pop_size\n\n        self.best_index = np.argmin(self.fitness)\n        self.best_x = self.population[self.best_index].copy()\n        self.best_fitness = self.fitness[self.best_index].copy()\n\n        self.f_opt = self.best_fitness\n        self.x_opt = self.best_x\n\n        mean = np.mean(self.population, axis=0)\n        \n        while self.budget_used < self.budget:\n            new_population = np.zeros_like(self.population)\n            new_fitness = np.zeros(self.pop_size)\n\n            z = np.random.randn(self.pop_size, self.dim)\n            y = self.B @ (self.D[:, None] * z.T)\n            x = mean[:, None] + self.sigma * y\n            x = x.T\n            \n            new_population = np.clip(x, self.lb, self.ub)\n\n            new_fitness = np.array([func(xi) for xi in new_population])\n            self.budget_used += self.pop_size\n\n            if self.budget_used >= self.budget:\n                break\n\n            # Selection and Recombination\n            idx = np.argsort(new_fitness)\n            best_indices = idx[:self.mu]\n            \n            weights = self.weights\n\n            # Update Mean\n            old_mean = mean.copy()\n            mean = np.sum(weights[:, None] * new_population[best_indices], axis=0)\n            \n            # Update Evolution Path\n            y_mean = np.mean(y[:, best_indices], axis=1)\n\n            self.ps = (1 - self.cs) * self.ps + (self.cs * (2 - self.cs))**0.5 * (self.B @ y_mean)\n            \n            # Update Covariance Matrix\n            hsig = np.linalg.norm(self.ps) / ((1 - (1 - self.cs)**(self.budget_used / self.pop_size))**0.5) < (1.4 + 2/(self.dim+1)) * self.chiN\n            dhsig = (1-hsig) * self.cc * (2-self.cc)\n            \n            self.C = (1 - self.c1 - self.cmu) * self.C + self.c1 * (self.ps[:, None] @ self.ps[None, :])\n            self.C += self.cmu * sum(weights[i] * (y[:, best_indices[i], None] @ y[:, best_indices[i], None].T) for i in range(self.mu))\n\n            # Eigenvalue Decomposition Update\n            if self.budget_used % (self.pop_size * 5) == 0:\n                self.C = np.triu(self.C) + np.triu(self.C, 1).T\n                self.D, self.B = np.linalg.eigh(self.C)\n                self.D = np.sqrt(self.D)\n\n            # Adapt step size\n            self.sigma *= np.exp((self.cs/self.damps) * (np.linalg.norm(self.ps)/self.chiN - 1))\n\n            # Elite update and selection\n            combined_population = np.vstack((self.population, new_population))\n            combined_fitness = np.concatenate((self.fitness, new_fitness))\n            \n            elite_indices = np.argsort(combined_fitness)[:self.pop_size]\n            self.population = combined_population[elite_indices]\n            self.fitness = combined_fitness[elite_indices]\n\n            best_index = np.argmin(self.fitness)\n            if self.fitness[best_index] < self.f_opt:\n                self.f_opt = self.fitness[best_index]\n                self.x_opt = self.population[best_index]\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 12, "feedback": "The algorithm HybridOptimizer scored 0.581 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["f6a755f7-7c04-48ce-910d-aa2fbc393c9a"], "operator": null, "metadata": {"aucs": [0.1198711365899866, 0.22479403634785056, 0.9478553815491433, 0.19974370124719276, 0.9429482427135525, 0.9494214182670068, 0.2868010882279285, 0.9374127068027558, 0.9475191221271289, 0.2017896673532873, 0.9026398580444757, 0.9870664755972673, 0.2692697533823052, 0.5389829034866802, 0.9247490481643287, 0.33150188887582777, 0.24242383697948167, 0.9482413598864473, 0.22938119568021986, 0.48032334699792556]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "eaaf8b67-137d-438f-9c3b-652b03ffc80e", "fitness": 0.7665221442909763, "name": "HybridOptimizer", "description": "A hybrid optimizer blending differential evolution with a self-adjusting Gaussian mutation guided by individual success history and a population-wide covariance adaptation, dynamically adjusting exploration based on function landscape and optimization stage.", "code": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget=10000, dim=10, pop_size=20, de_rate=0.7, mutation_factor=0.5, learning_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lb = -5.0\n        self.ub = 5.0\n        self.de_rate = de_rate\n        self.mutation_factor = mutation_factor\n        self.learning_rate = learning_rate\n        self.success_rates = np.zeros(self.pop_size)\n        self.mutation_magnitudes = np.zeros(self.pop_size)\n        self.mutation_scale = 0.1 * (self.ub - self.lb)\n\n        # CMA-ES-like covariance matrix adaptation\n        self.C = np.eye(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.damps = 1 + self.learning_rate\n        self.cs = 0.3\n        self.cc = 0.3\n        self.mu_eff = self.pop_size / 4  #Heuristic value\n        self.c_mu = 0.1\n        self.c_1 = 0.1\n        \n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget_used = self.pop_size\n\n        self.best_index = np.argmin(self.fitness)\n        self.best_x = self.population[self.best_index].copy()\n        self.best_fitness = self.fitness[self.best_index].copy()\n\n        self.f_opt = self.best_fitness\n        self.x_opt = self.best_x\n\n        while self.budget_used < self.budget:\n            new_population = np.zeros_like(self.population)\n            new_fitness = np.zeros(self.pop_size)\n            \n            # Adaptive Mutation scale based on progress\n            adaptive_mutation_scale = self.mutation_scale * (1 - self.budget_used / self.budget)\n\n            delta_xs = [] #Collect steps for CMA\n\n            for i in range(self.pop_size):\n                old_fitness = self.fitness[i]\n\n                if np.random.rand() < self.de_rate:\n                    # Differential Evolution\n                    indices = np.random.choice(self.pop_size, 3, replace=False)\n                    x_r1, x_r2, x_r3 = self.population[indices]\n                    mutation = x_r1 + self.mutation_factor * (x_r2 - x_r3)\n                    crossover = np.random.rand(self.dim) < 0.7  # Binomial Crossover\n                    new_population[i] = np.where(crossover, mutation, self.population[i])\n                else:\n                    # Self-Adjusting Gaussian Mutation with CMA\n                    z = np.random.normal(0, 1, self.dim)\n                    mutation_vector = adaptive_mutation_scale * np.dot(self.C**0.5, z)\n                    new_population[i] = self.population[i] + mutation_vector\n                    delta_xs.append(mutation_vector)\n\n\n                new_population[i] = np.clip(new_population[i], self.lb, self.ub)\n                new_fitness[i] = func(new_population[i])\n                self.budget_used += 1\n\n                if new_fitness[i] < old_fitness:\n                    # Success! Update individual\n                    self.population[i] = new_population[i]\n                    self.fitness[i] = new_fitness[i]\n                    self.success_rates[i] = (1 - self.learning_rate) * self.success_rates[i] + self.learning_rate\n                    self.mutation_magnitudes[i] = (1 - self.learning_rate) * self.mutation_magnitudes[i] + self.learning_rate * np.linalg.norm(new_population[i] - self.population[i])\n\n                    if new_fitness[i] < self.best_fitness:\n                        self.best_x = new_population[i].copy()\n                        self.best_fitness = new_fitness[i].copy()\n                        if self.best_fitness < self.f_opt:\n                            self.f_opt = self.best_fitness\n                            self.x_opt = self.best_x\n                else:\n                     # Failure, decrease mutation magnitude\n                    self.success_rates[i] = (1 - self.learning_rate) * self.success_rates[i]\n                    self.mutation_magnitudes[i] *= (1 - self.learning_rate)\n\n                if self.budget_used >= self.budget:\n                    break\n\n            #CMA-ES like update\n            delta_xs = np.array(delta_xs)\n            if len(delta_xs) > 0:\n                delta_x_mean = np.mean(delta_xs, axis=0)\n                self.ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs)) * delta_x_mean / adaptive_mutation_scale\n                self.pc = (1 - self.cc) * self.pc + np.sqrt(self.cc * (2 - self.cc)) * delta_x_mean\n                \n                self.C = (1 - self.c_1 - self.c_mu) * self.C + self.c_1 * np.outer(self.pc, self.pc) + self.c_mu * np.mean([np.outer(delta_xs[j], delta_xs[j]) for j in range(len(delta_xs))], axis=0)\n                self.C = np.triu(self.C) + np.triu(self.C, 1).T  # enforce symmetry\n            \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 12, "feedback": "The algorithm HybridOptimizer scored 0.767 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["85378eab-540e-488e-9005-2e7b547753b4"], "operator": null, "metadata": {"aucs": [0.2434654563025913, 0.7709774328909881, 0.8321199557949259, 0.953569881752841, 0.8651918994978469, 0.8998646976218805, 0.7929257724749139, 0.8373759651066008, 0.8714963035500984, 0.8305539995220151, 0.8875801725420387, 0.9935232739234856, 0.3367605340631522, 0.83914767067776, 0.9427705700745924, 0.8872546843884186, 0.7757749734518422, 0.9219734317429621, 0.3056242547255078, 0.5424919557150636]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "c07baf71-d07d-44f8-a379-f0d76495a192", "fitness": 0.5192076762150138, "name": "AdaptiveHybridOptimizer", "description": "An adaptive hybrid optimizer combining differential evolution with a CMA-ES-inspired mutation, adjusting exploration based on success history, magnitude of changes, and dynamically adjusting DE probability.", "code": "import numpy as np\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget=10000, dim=10, pop_size=20, learning_rate=0.1, de_rate_init=0.7):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lb = -5.0\n        self.ub = 5.0\n        self.learning_rate = learning_rate\n        self.success_history = []  # Store success/failure of recent DE trials\n        self.mutation_scale = 0.1 * (self.ub - self.lb)  # Initial mutation scale\n        self.de_rate = de_rate_init  # Probability of differential evolution, dynamically adjusted\n        self.magnitude_history = [] #Store the magnitudes of changes in the population\n        self.min_mutation_scale = 1e-6 * (self.ub - self.lb)\n        self.max_mutation_scale = 0.2 * (self.ub - self.lb)\n        self.mutation_factor = 0.5  # DE mutation factor\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget_used = self.pop_size\n\n        self.best_index = np.argmin(self.fitness)\n        self.best_x = self.population[self.best_index].copy()\n        self.best_fitness = self.fitness[self.best_index].copy()\n\n        self.f_opt = self.best_fitness\n        self.x_opt = self.best_x\n\n\n        while self.budget_used < self.budget:\n            new_population = np.zeros_like(self.population)\n            new_fitness = np.zeros(self.pop_size)\n            magnitudes = np.zeros(self.pop_size)\n\n            for i in range(self.pop_size):\n                old_fitness = self.fitness[i]\n                old_x = self.population[i].copy()\n\n                if np.random.rand() < self.de_rate:\n                    # Differential Evolution\n                    indices = np.random.choice(self.pop_size, 3, replace=False)\n                    x_r1, x_r2, x_r3 = self.population[indices]\n                    mutation = x_r1 + self.mutation_factor * (x_r2 - x_r3)\n                    crossover = np.random.rand(self.dim) < 0.7  # Binomial Crossover\n                    new_population[i] = np.where(crossover, mutation, self.population[i])\n\n                else:\n                    # CMA-ES-inspired Gaussian Mutation with adaptive scale\n                    new_population[i] = self.population[i] + np.random.normal(0, self.mutation_scale, self.dim)\n\n                new_population[i] = np.clip(new_population[i], self.lb, self.ub)\n                new_fitness[i] = func(new_population[i])\n                self.budget_used += 1\n                magnitudes[i] = np.linalg.norm(new_population[i] - old_x) #Measure of change\n                if new_fitness[i] < self.fitness[i]:\n                    self.population[i] = new_population[i]\n                    self.fitness[i] = new_fitness[i]\n                    self.success_history.append(1)  # Mark as success\n\n                    if new_fitness[i] < self.best_fitness:\n                        self.best_x = new_population[i].copy()\n                        self.best_fitness = new_fitness[i].copy()\n                        if self.best_fitness < self.f_opt:\n                            self.f_opt = self.best_fitness\n                            self.x_opt = self.best_x\n                else:\n                    self.success_history.append(0)  # Mark as failure\n                    \n\n                if self.budget_used >= self.budget:\n                    break\n            self.magnitude_history.extend(magnitudes)\n            # Update mutation scale based on success history and magnitude of changes\n            if len(self.success_history) > 50:\n                success_rate = np.mean(self.success_history[-50:])\n                mean_magnitude = np.mean(self.magnitude_history[-50:]) if len(self.magnitude_history) > 50 else np.mean(self.magnitude_history)\n                self.mutation_scale *= np.exp(self.learning_rate * (success_rate - 0.2)) # Adapt based on success rate\n                #Adjust mutation scale also based on magnitude of changes. If changes are small but success rate is low, increase mutation.\n                if success_rate < 0.2 and mean_magnitude < 0.01:\n                      self.mutation_scale *= 1.1 # increase mutation scale\n\n            self.mutation_scale = np.clip(self.mutation_scale, self.min_mutation_scale, self.max_mutation_scale) #Keep mutation scale in reasonable bounds\n\n            # Dynamically adjust DE rate based on success\n            if len(self.success_history) > 100:\n                recent_success_rate = np.mean(self.success_history[-100:])\n                if recent_success_rate > 0.3:\n                    self.de_rate = min(1.0, self.de_rate + 0.01)  # Increase DE rate if successful\n                else:\n                    self.de_rate = max(0.0, self.de_rate - 0.01)  # Decrease DE rate if unsuccessful\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 12, "feedback": "The algorithm AdaptiveHybridOptimizer scored 0.519 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["987ac598-4889-4d9e-b121-2512b12d2784"], "operator": null, "metadata": {"aucs": [0.20766010856977946, 0.284760381986887, 0.44915380258570836, 0.7597697365546605, 0.46787360739441064, 0.8985329644233828, 0.33382815138010935, 0.39647693029466646, 0.47848543668780263, 0.3413916159209609, 0.7584817605793392, 0.9949413954024897, 0.33514304883927126, 0.4394958284526026, 0.7941815315817152, 0.5850243850214198, 0.40593829055796304, 0.6618299519430602, 0.23027938474182386, 0.5609052113822264]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "52a697c7-d5d4-4ebc-87f2-0cba1e317883", "fitness": "-inf", "name": "AdaptiveHybridOptimizer", "description": "A simplified hybrid optimizer combining differential evolution with adaptive Gaussian mutation, adjusting exploration based on recent success and dynamically adapting crossover probability.", "code": "import numpy as np\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget=10000, dim=10, pop_size=20, learning_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lb = -5.0\n        self.ub = 5.0\n        self.learning_rate = learning_rate\n        self.success_history = []\n        self.mutation_scale = 0.1 * (self.ub - self.lb)\n        self.de_rate = 0.7\n        self.crossover_rate = 0.7 #Adaptive crossover rate\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x)] for x in self.population)\n        self.budget_used = self.pop_size\n\n        self.best_index = np.argmin(self.fitness)\n        self.best_x = self.population[self.best_index].copy()\n        self.best_fitness = self.fitness[self.best_index].copy()\n\n        self.f_opt = self.best_fitness\n        self.x_opt = self.best_x\n        \n        mutation_factor = 0.5\n\n        while self.budget_used < self.budget:\n            new_population = np.zeros_like(self.population)\n            new_fitness = np.zeros(self.pop_size)\n\n            for i in range(self.pop_size):\n                if np.random.rand() < self.de_rate:\n                    # Differential Evolution\n                    indices = np.random.choice(self.pop_size, 3, replace=False)\n                    x_r1, x_r2, x_r3 = self.population[indices]\n                    mutation = x_r1 + mutation_factor * (x_r2 - x_r3)\n                    crossover = np.random.rand(self.dim) < self.crossover_rate  # Binomial Crossover\n                    new_population[i] = np.where(crossover, mutation, self.population[i])\n                    \n                else:\n                    # Gaussian Mutation with adaptive scale\n                    new_population[i] = self.population[i] + np.random.normal(0, self.mutation_scale, self.dim)\n\n                new_population[i] = np.clip(new_population[i], self.lb, self.ub)\n                new_fitness[i] = func(new_population[i])\n                self.budget_used += 1\n\n                if new_fitness[i] < self.fitness[i]:\n                    #Improvement\n                    self.population[i] = new_population[i]\n                    self.fitness[i] = new_fitness[i]\n                    self.success_history.append(1)\n                    \n                    if new_fitness[i] < self.best_fitness:\n                        self.best_x = new_population[i].copy()\n                        self.best_fitness = new_fitness[i].copy()\n                        if self.best_fitness < self.f_opt:\n                            self.f_opt = self.best_fitness\n                            self.x_opt = self.best_x\n                else:\n                    self.success_history.append(0)\n                \n            #Adapt crossover rate and mutation scale only after each generation\n            if len(self.success_history) > self.pop_size:\n                success_rate = np.mean(self.success_history[-self.pop_size:])\n                self.mutation_scale *= np.exp(self.learning_rate * (success_rate - 0.2))\n                self.crossover_rate = np.clip(self.crossover_rate + 0.1 * (success_rate - 0.5), 0.1, 0.9) #Adapt crossover\n\n            self.mutation_scale = np.clip(self.mutation_scale, 1e-6 * (self.ub - self.lb), 0.2 * (self.ub - self.lb))\n\n            if self.budget_used >= self.budget:\n                break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 12, "feedback": "In the code, line 25, in __call__, the following error occurred:\nIndexError: too many indices for array: array is 0-dimensional, but 1 were indexed\nOn line: self.best_fitness = self.fitness[self.best_index].copy()", "error": "In the code, line 25, in __call__, the following error occurred:\nIndexError: too many indices for array: array is 0-dimensional, but 1 were indexed\nOn line: self.best_fitness = self.fitness[self.best_index].copy()", "parent_ids": ["987ac598-4889-4d9e-b121-2512b12d2784"], "operator": null, "metadata": {}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
{"id": "0547ab24-6d26-4d04-b392-bbff4ac75aba", "fitness": 0.6041484620295557, "name": "HybridOptimizer", "description": "Dynamically adjusts DE mutation factor and CMA learning rate based on success history, while using a more robust covariance matrix update.", "code": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget=10000, dim=10, pop_size=20, de_rate=0.7, initial_mutation_factor=0.5, initial_sigma=0.1, learning_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lb = -5.0\n        self.ub = 5.0\n        self.de_rate = de_rate\n        self.initial_mutation_factor = initial_mutation_factor\n        self.mutation_factor = initial_mutation_factor\n        self.initial_sigma = initial_sigma * (self.ub - self.lb)\n        self.sigma = self.initial_sigma\n        self.success_rates = np.zeros(self.pop_size)\n        self.mutation_magnitudes = np.zeros(self.pop_size)\n        self.learning_rate = learning_rate\n        self.C = np.eye(self.dim)  # Covariance matrix\n        self.ps = np.zeros(self.dim)  # Evolution path for sigma adaptation\n        self.pc = np.zeros(self.dim)  # Evolution path for covariance matrix adaptation\n        self.cs = 0.3  # Learning rate for sigma\n        self.cc = 0.3  # Learning rate for covariance matrix\n        self.mu_eff = self.pop_size / 4 # Effective population size\n        self.c_mu = min(1, self.mu_eff / self.pop_size)\n        self.c_1 = min(1, 2 / (self.dim + 1)**2 + self.mu_eff / 3)\n        self.damps = 1 + 2 * max(0, np.sqrt((self.mu_eff - 1) / (self.dim + 1)) - 1) + self.cs  # Damping for sigma\n\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget_used = self.pop_size\n\n        self.best_index = np.argmin(self.fitness)\n        self.best_x = self.population[self.best_index].copy()\n        self.best_fitness = self.fitness[self.best_index].copy()\n\n        self.f_opt = self.best_fitness\n        self.x_opt = self.best_x\n\n        while self.budget_used < self.budget:\n            new_population = np.zeros_like(self.population)\n            new_fitness = np.zeros(self.pop_size)\n            \n            # Adaptive sigma based on progress\n            adaptive_sigma = self.initial_sigma * (1 - self.budget_used / self.budget)\n\n            sorted_indices = np.argsort(self.fitness)\n            x_mean = np.mean(self.population[sorted_indices[:self.pop_size // 2]], axis=0)  # Weighted recombination\n\n            for i in range(self.pop_size):\n                old_fitness = self.fitness[i]\n\n                if np.random.rand() < self.de_rate:\n                    # Differential Evolution with adaptive mutation factor\n                    indices = np.random.choice(self.pop_size, 3, replace=False)\n                    x_r1, x_r2, x_r3 = self.population[indices]\n                    mutation = x_r1 + self.mutation_factor * (x_r2 - x_r3)\n                    crossover = np.random.rand(self.dim) < 0.7  # Binomial Crossover\n                    new_population[i] = np.where(crossover, mutation, self.population[i])\n                else:\n                    # CMA-Guided Mutation\n                    mutation_vector = np.random.multivariate_normal(np.zeros(self.dim), self.C)\n                    new_population[i] = self.population[i] + adaptive_sigma * mutation_vector\n\n                new_population[i] = np.clip(new_population[i], self.lb, self.ub)\n                new_fitness[i] = func(new_population[i])\n                self.budget_used += 1\n\n                if new_fitness[i] < old_fitness:\n                    # Success! Update individual\n                    self.population[i] = new_population[i]\n                    self.fitness[i] = new_fitness[i]\n                    self.success_rates[i] = (1 - self.learning_rate) * self.success_rates[i] + self.learning_rate\n                    self.mutation_magnitudes[i] = (1 - self.learning_rate) * self.mutation_magnitudes[i] + self.learning_rate * np.linalg.norm(new_population[i] - self.population[i])\n\n                    if new_fitness[i] < self.best_fitness:\n                        self.best_x = new_population[i].copy()\n                        self.best_fitness = new_fitness[i].copy()\n                        if self.best_fitness < self.f_opt:\n                            self.f_opt = self.best_fitness\n                            self.x_opt = self.best_x\n                else:\n                     # Failure, decrease mutation magnitude\n                    self.success_rates[i] = (1 - self.learning_rate) * self.success_rates[i]\n                    self.mutation_magnitudes[i] *= (1 - self.learning_rate)\n\n                if self.budget_used >= self.budget:\n                    break\n\n            # Update mutation factor based on success\n            success_rate_mean = np.mean(self.success_rates)\n            if success_rate_mean > 0.2:\n                self.mutation_factor *= 1.05\n            else:\n                self.mutation_factor *= 0.95\n            self.mutation_factor = np.clip(self.mutation_factor, 0.1, 1.0)\n            \n            # Update Covariance Matrix (CMA-ES style)\n            z = (self.population[sorted_indices[0]] - x_mean) / adaptive_sigma\n            self.ps = (1 - self.cs) * self.ps + np.sqrt(self.cs * (2 - self.cs)) * z\n            self.pc = (1 - self.cc) * self.pc + np.sqrt(self.cc * (2 - self.cc)) * z\n\n            delta = (1 - self.cc) * np.sqrt(np.sum(self.pc**2)) / adaptive_sigma\n            self.C = (1 - self.c_1 - self.c_mu) * self.C + self.c_1 * np.outer(self.pc, self.pc)\n\n            for k in range(min(self.pop_size // 2, self.pop_size)):\n                 z = (self.population[sorted_indices[k]] - x_mean) / adaptive_sigma\n                 self.C += self.c_mu * np.outer(z, z)\n\n            # Keep C positive definite\n            try:\n              _ = np.linalg.cholesky(self.C)\n            except np.linalg.LinAlgError:\n              self.C = np.eye(self.dim) # Reset covariance matrix\n\n\n            # Adjust learning rate\n            if success_rate_mean > 0.3:\n                 self.learning_rate *= 1.05\n            else:\n                 self.learning_rate *= 0.95\n\n            self.learning_rate = np.clip(self.learning_rate, 0.01, 0.5)\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 12, "feedback": "The algorithm HybridOptimizer scored 0.604 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["f6a755f7-7c04-48ce-910d-aa2fbc393c9a"], "operator": null, "metadata": {"aucs": [0.24824907167064025, 0.44962140833778763, 0.7377666504879803, 0.901223267084004, 0.796791450700982, 0.791055939182104, 0.33120700020977967, 0.576118191810407, 0.7946115817218339, 0.200292267504928, 0.4914089363144032, 0.9969266903846496, 0.37052359159256865, 0.4741210856871486, 0.8487071437253296, 0.8402323944903863, 0.6349570134330302, 0.8709651018024308, 0.22510400652593854, 0.5030864479247816]}, "task_prompt": "\nYou are a Python developer working on a new optimization algorithm.\nYour task is to develop a novel heuristic optimization algorithm for continuous optimization problems.\nThe optimization algorithm should handle a wide range of tasks, which is evaluated on the Many Affine BBOB test suite of noiseless functions. Your task is to write the optimization algorithm in Python code. \nEach of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound). The dimensionality can be varied.\nThe code should contain an `__init__(self, budget, dim)` function with optional additional arguments and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.\nThe func() can only be called as many times as the budget allows, not more. \n"}
