{"id": "2130ef74-ea52-4eb7-a52f-b5c521fba5ba", "fitness": 0.0, "name": "HybridDECMACauchy", "description": "A population-based algorithm that combines elements of differential evolution, covariance matrix adaptation, and a Cauchy mutation operator for enhanced exploration and exploitation.", "code": "import numpy as np\n\nclass HybridDECMACauchy:\n    def __init__(self, budget=10000, dim=10, pop_size=None, de_mutation_factor=0.5, de_crossover_rate=0.7, cma_learning_rate=0.1, cauchy_scale=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size if pop_size is not None else 4 + int(3 * np.log(dim))\n        self.de_mutation_factor = de_mutation_factor\n        self.de_crossover_rate = de_crossover_rate\n        self.cma_learning_rate = cma_learning_rate\n        self.cauchy_scale = cauchy_scale\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        \n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        # CMA initialization\n        mean = np.mean(population, axis=0)\n        covariance = np.eye(self.dim)\n        \n        while self.budget > 0:\n            # Differential Evolution\n            for i in range(self.pop_size):\n                if self.budget <= 0:\n                    break\n                \n                # DE mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = population[idxs]\n                x_mutated = population[i] + self.de_mutation_factor * (x_r2 - x_r3)\n                x_mutated = np.clip(x_mutated, func.bounds.lb, func.bounds.ub)\n\n                # DE crossover\n                x_trial = population[i].copy()\n                for j in range(self.dim):\n                    if np.random.rand() < self.de_crossover_rate:\n                        x_trial[j] = x_mutated[j]\n\n                # Evaluate trial vector\n                f_trial = func(x_trial)\n                self.budget -= 1\n                \n                # Selection\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    population[i] = x_trial\n                    \n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = x_trial.copy()\n            \n            # CMA Update\n            mean_old = mean.copy()\n            mean = np.mean(population, axis=0)\n            \n            # Update covariance matrix\n            y = mean - mean_old\n            covariance = (1 - self.cma_learning_rate) * covariance + self.cma_learning_rate * np.outer(y, y)\n            \n            # Cauchy Mutation for exploration\n            for i in range(self.pop_size):\n                if self.budget <= 0:\n                    break\n                    \n                z = np.random.standard_cauchy(size=self.dim)\n                x_cauchy = population[i] + self.cauchy_scale * np.dot(np.linalg.cholesky(covariance), z) # Scale with covariance\n                x_cauchy = np.clip(x_cauchy, func.bounds.lb, func.bounds.ub)\n\n                f_cauchy = func(x_cauchy)\n                self.budget -= 1\n\n                if f_cauchy < fitness[i]:\n                    fitness[i] = f_cauchy\n                    population[i] = x_cauchy\n                    \n                    if f_cauchy < self.f_opt:\n                        self.f_opt = f_cauchy\n                        self.x_opt = x_cauchy.copy()\n                    \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "", "error": "Evaluation error: Matrix is not positive definite", "parent_ids": [], "operator": "init", "metadata": {"llm_response": "# Description: A population-based algorithm that combines elements of differential evolution, covariance matrix adaptation, and a Cauchy mutation operator for enhanced exploration and exploitation.\n# Code: \n```python\nimport numpy as np\n\nclass HybridDECMACauchy:\n    def __init__(self, budget=10000, dim=10, pop_size=None, de_mutation_factor=0.5, de_crossover_rate=0.7, cma_learning_rate=0.1, cauchy_scale=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size if pop_size is not None else 4 + int(3 * np.log(dim))\n        self.de_mutation_factor = de_mutation_factor\n        self.de_crossover_rate = de_crossover_rate\n        self.cma_learning_rate = cma_learning_rate\n        self.cauchy_scale = cauchy_scale\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        \n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        # CMA initialization\n        mean = np.mean(population, axis=0)\n        covariance = np.eye(self.dim)\n        \n        while self.budget > 0:\n            # Differential Evolution\n            for i in range(self.pop_size):\n                if self.budget <= 0:\n                    break\n                \n                # DE mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = population[idxs]\n                x_mutated = population[i] + self.de_mutation_factor * (x_r2 - x_r3)\n                x_mutated = np.clip(x_mutated, func.bounds.lb, func.bounds.ub)\n\n                # DE crossover\n                x_trial = population[i].copy()\n                for j in range(self.dim):\n                    if np.random.rand() < self.de_crossover_rate:\n                        x_trial[j] = x_mutated[j]\n\n                # Evaluate trial vector\n                f_trial = func(x_trial)\n                self.budget -= 1\n                \n                # Selection\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    population[i] = x_trial\n                    \n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = x_trial.copy()\n            \n            # CMA Update\n            mean_old = mean.copy()\n            mean = np.mean(population, axis=0)\n            \n            # Update covariance matrix\n            y = mean - mean_old\n            covariance = (1 - self.cma_learning_rate) * covariance + self.cma_learning_rate * np.outer(y, y)\n            \n            # Cauchy Mutation for exploration\n            for i in range(self.pop_size):\n                if self.budget <= 0:\n                    break\n                    \n                z = np.random.standard_cauchy(size=self.dim)\n                x_cauchy = population[i] + self.cauchy_scale * np.dot(np.linalg.cholesky(covariance), z) # Scale with covariance\n                x_cauchy = np.clip(x_cauchy, func.bounds.lb, func.bounds.ub)\n\n                f_cauchy = func(x_cauchy)\n                self.budget -= 1\n\n                if f_cauchy < fitness[i]:\n                    fitness[i] = f_cauchy\n                    population[i] = x_cauchy\n                    \n                    if f_cauchy < self.f_opt:\n                        self.f_opt = f_cauchy\n                        self.x_opt = x_cauchy.copy()\n                    \n        return self.f_opt, self.x_opt\n```"}, "task_prompt": ""}
{"id": "2d086322-f43f-4b37-bed4-2235e826723e", "fitness": 0.4232712488575672, "name": "SlimeMoldOptimizer", "description": "A population-based algorithm inspired by the behavior of slime mold, adapting the search space based on the best-found solutions and dynamically adjusting exploration and exploitation.", "code": "import numpy as np\n\nclass SlimeMoldOptimizer:\n    def __init__(self, budget=10000, dim=10, population_size=20, w_decay=0.9, p_selection=0.2):\n        \"\"\"\n        Initializes the Slime Mold Optimizer.\n\n        Args:\n            budget (int): The maximum number of function evaluations.\n            dim (int): The dimensionality of the search space.\n            population_size (int): The number of slime mold agents.\n            w_decay (float): Decay rate for the weight parameter.\n            p_selection (float): Percentage of top solutions to consider for weighted averaging.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.population_size = population_size\n        self.w_decay = w_decay\n        self.p_selection = p_selection  # Percentage for best selection\n        self.lb = -5.0\n        self.ub = 5.0\n\n\n    def __call__(self, func):\n        \"\"\"\n        Optimizes the given function using the Slime Mold Optimizer.\n\n        Args:\n            func (callable): The black-box function to optimize.\n\n        Returns:\n            tuple: The best function value found and the corresponding solution.\n        \"\"\"\n        population = np.random.uniform(self.lb, self.ub, size=(self.population_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.population_size\n        \n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index].copy()\n\n\n        weights = np.ones(self.population_size) # Initialize weights\n\n        while self.budget > 0:\n            # Sort population based on fitness\n            sorted_indices = np.argsort(fitness)\n            population = population[sorted_indices]\n            fitness = fitness[sorted_indices]\n            \n            # Select top solutions based on p_selection\n            num_selected = int(self.p_selection * self.population_size)\n            selected_population = population[:num_selected]\n            selected_fitness = fitness[:num_selected]\n                \n            # Calculate weights based on fitness rank. Give the better ones more weight\n            rank = np.arange(1, num_selected + 1)\n            weights_selected = np.exp(-rank / num_selected)\n            weights_selected /= np.sum(weights_selected)  # Normalize\n\n            # Update population using weighted average of selected solutions\n            for i in range(self.population_size):\n                if self.budget <= 0:\n                    break\n                \n                # Small probability for random exploration\n                if np.random.rand() < 0.1:\n                    x = np.random.uniform(self.lb, self.ub, size=self.dim)\n\n                else: # Weighted average of selected\n                    x = np.zeros(self.dim)\n                    for j in range(num_selected):\n                        x += weights_selected[j] * selected_population[j]\n\n                    # Add small random perturbation\n                    x += 0.1 * np.random.randn(self.dim)\n                    x = np.clip(x, self.lb, self.ub)\n\n                f = func(x)\n                self.budget -= 1\n\n                if f < fitness[i]:\n                    fitness[i] = f\n                    population[i] = x.copy()\n\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = x.copy()\n        \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm SlimeMoldOptimizer scored 0.423 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": "init", "metadata": {"llm_response": "# Description: A population-based algorithm inspired by the behavior of slime mold, adapting the search space based on the best-found solutions and dynamically adjusting exploration and exploitation.\n# Code: \n```python\nimport numpy as np\n\nclass SlimeMoldOptimizer:\n    def __init__(self, budget=10000, dim=10, population_size=20, w_decay=0.9, p_selection=0.2):\n        \"\"\"\n        Initializes the Slime Mold Optimizer.\n\n        Args:\n            budget (int): The maximum number of function evaluations.\n            dim (int): The dimensionality of the search space.\n            population_size (int): The number of slime mold agents.\n            w_decay (float): Decay rate for the weight parameter.\n            p_selection (float): Percentage of top solutions to consider for weighted averaging.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.population_size = population_size\n        self.w_decay = w_decay\n        self.p_selection = p_selection  # Percentage for best selection\n        self.lb = -5.0\n        self.ub = 5.0\n\n\n    def __call__(self, func):\n        \"\"\"\n        Optimizes the given function using the Slime Mold Optimizer.\n\n        Args:\n            func (callable): The black-box function to optimize.\n\n        Returns:\n            tuple: The best function value found and the corresponding solution.\n        \"\"\"\n        population = np.random.uniform(self.lb, self.ub, size=(self.population_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.population_size\n        \n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index].copy()\n\n\n        weights = np.ones(self.population_size) # Initialize weights\n\n        while self.budget > 0:\n            # Sort population based on fitness\n            sorted_indices = np.argsort(fitness)\n            population = population[sorted_indices]\n            fitness = fitness[sorted_indices]\n            \n            # Select top solutions based on p_selection\n            num_selected = int(self.p_selection * self.population_size)\n            selected_population = population[:num_selected]\n            selected_fitness = fitness[:num_selected]\n                \n            # Calculate weights based on fitness rank. Give the better ones more weight\n            rank = np.arange(1, num_selected + 1)\n            weights_selected = np.exp(-rank / num_selected)\n            weights_selected /= np.sum(weights_selected)  # Normalize\n\n            # Update population using weighted average of selected solutions\n            for i in range(self.population_size):\n                if self.budget <= 0:\n                    break\n                \n                # Small probability for random exploration\n                if np.random.rand() < 0.1:\n                    x = np.random.uniform(self.lb, self.ub, size=self.dim)\n\n                else: # Weighted average of selected\n                    x = np.zeros(self.dim)\n                    for j in range(num_selected):\n                        x += weights_selected[j] * selected_population[j]\n\n                    # Add small random perturbation\n                    x += 0.1 * np.random.randn(self.dim)\n                    x = np.clip(x, self.lb, self.ub)\n\n                f = func(x)\n                self.budget -= 1\n\n                if f < fitness[i]:\n                    fitness[i] = f\n                    population[i] = x.copy()\n\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = x.copy()\n        \n        return self.f_opt, self.x_opt\n```", "aucs": [0.12651251623987936, 0.22308948404331375, 0.4623532737993641, 0.7509968081875764, 0.2974521719566273, 0.438467090095763, 0.254470829286188, 0.3613320402210979, 0.3588216078017421, 0.192283561614919, 0.9341180356018396, 0.9886571064183369, 0.32610853221308866, 0.25300703699297733, 0.7987289182729347, 0.33279068445136295, 0.22974208114022965, 0.5319992554144397, 0.1533790459565123, 0.4511148974431515], "generation": 0, "operator": "init"}, "task_prompt": ""}
{"id": "49b01a83-152a-4b37-86ca-d659431fcf99", "fitness": 0.0, "name": "AdaptiveDE", "description": "An adaptive differential evolution strategy with self-adaptive parameter control and a restart mechanism.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=None, F=0.5, CR=0.7, restart_factor=0.1):\n        \"\"\"\n        Adaptive Differential Evolution with self-adaptive parameters and restart.\n\n        Args:\n            budget (int): Total function evaluations allowed.\n            dim (int): Dimensionality of the problem.\n            pop_size (int, optional): Population size. Defaults to 10*dim if None.\n            F (float, optional): Initial scaling factor. Defaults to 0.5.\n            CR (float, optional): Initial crossover rate. Defaults to 0.7.\n            restart_factor (float, optional): Fraction of budget to be consumed before a restart is triggered.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size if pop_size is not None else 10 * dim\n        self.F = F\n        self.CR = CR\n        self.restart_factor = restart_factor\n        self.restart_iterations = int(restart_factor * budget)\n        self.lb = -5.0\n        self.ub = 5.0\n        self.archive_factor = 2.0 # Size of archive relative to population\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.population = np.random.uniform(self.lb, self.ub, (self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.eval_count = self.pop_size\n\n        self.archive = []\n\n        while self.eval_count < self.budget:\n            for i in range(self.pop_size):\n                # Parameter adaptation\n                F_i = np.clip(np.random.normal(self.F, 0.1), 0.1, 1.0)\n                CR_i = np.clip(np.random.normal(self.CR, 0.1), 0.1, 1.0)\n\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_1, x_2, x_3 = self.population[idxs]\n                mutant = self.population[i] + F_i * (x_2 - x_3)\n                mutant = np.clip(mutant, self.lb, self.ub)\n                \n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR_i\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial_vector = np.where(cross_points, mutant, self.population[i])\n\n                # Selection\n                f_trial = func(trial_vector)\n                self.eval_count += 1\n\n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial_vector\n\n                    # Update archive\n                    self.archive.append(self.population[i].copy())\n                    if len(self.archive) > self.archive_factor * self.pop_size:\n                        self.archive.pop(0)\n\n\n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = trial_vector\n\n            # Restart Mechanism\n            if self.eval_count % self.restart_iterations == 0:\n                best_idx = np.argmin(self.fitness)\n                best_individual = self.population[best_idx].copy()\n\n                self.population = np.random.uniform(self.lb, self.ub, (self.pop_size, self.dim))\n                self.population[0] = best_individual # Keep the best\n                self.fitness = np.array([func(x) for x in self.population])\n                self.eval_count += self.pop_size -1\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveDE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": "init", "metadata": {"llm_response": "# Description: An adaptive differential evolution strategy with self-adaptive parameter control and a restart mechanism.\n# Code: \n```python\nimport numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=None, F=0.5, CR=0.7, restart_factor=0.1):\n        \"\"\"\n        Adaptive Differential Evolution with self-adaptive parameters and restart.\n\n        Args:\n            budget (int): Total function evaluations allowed.\n            dim (int): Dimensionality of the problem.\n            pop_size (int, optional): Population size. Defaults to 10*dim if None.\n            F (float, optional): Initial scaling factor. Defaults to 0.5.\n            CR (float, optional): Initial crossover rate. Defaults to 0.7.\n            restart_factor (float, optional): Fraction of budget to be consumed before a restart is triggered.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size if pop_size is not None else 10 * dim\n        self.F = F\n        self.CR = CR\n        self.restart_factor = restart_factor\n        self.restart_iterations = int(restart_factor * budget)\n        self.lb = -5.0\n        self.ub = 5.0\n        self.archive_factor = 2.0 # Size of archive relative to population\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.population = np.random.uniform(self.lb, self.ub, (self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.eval_count = self.pop_size\n\n        self.archive = []\n\n        while self.eval_count < self.budget:\n            for i in range(self.pop_size):\n                # Parameter adaptation\n                F_i = np.clip(np.random.normal(self.F, 0.1), 0.1, 1.0)\n                CR_i = np.clip(np.random.normal(self.CR, 0.1), 0.1, 1.0)\n\n                # Mutation\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                x_1, x_2, x_3 = self.population[idxs]\n                mutant = self.population[i] + F_i * (x_2 - x_3)\n                mutant = np.clip(mutant, self.lb, self.ub)\n                \n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR_i\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial_vector = np.where(cross_points, mutant, self.population[i])\n\n                # Selection\n                f_trial = func(trial_vector)\n                self.eval_count += 1\n\n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial_vector\n\n                    # Update archive\n                    self.archive.append(self.population[i].copy())\n                    if len(self.archive) > self.archive_factor * self.pop_size:\n                        self.archive.pop(0)\n\n\n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = trial_vector\n\n            # Restart Mechanism\n            if self.eval_count % self.restart_iterations == 0:\n                best_idx = np.argmin(self.fitness)\n                best_individual = self.population[best_idx].copy()\n\n                self.population = np.random.uniform(self.lb, self.ub, (self.pop_size, self.dim))\n                self.population[0] = best_individual # Keep the best\n                self.fitness = np.array([func(x) for x in self.population])\n                self.eval_count += self.pop_size -1\n\n        return self.f_opt, self.x_opt\n```", "aucs": [0], "generation": 0, "operator": "init"}, "task_prompt": ""}
{"id": "41a2ca3a-0c0c-42f6-a0d7-9cf7b5cd7bf5", "fitness": 0.0, "name": "AdaptivePopulationSearch", "description": "A population-based algorithm with adaptive mutation and selection based on fitness rank and crowding distance.", "code": "import numpy as np\n\nclass AdaptivePopulationSearch:\n    def __init__(self, budget=10000, dim=10, pop_size=50):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lb = -5.0\n        self.ub = 5.0\n        self.mutation_rate = 0.1\n        self.mutation_scale = 1.0\n        self.selection_pressure = 2.0 # Higher value emphasizes fitter individuals\n\n    def initialize_population(self):\n        return np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        fitness = np.array([func(x) for x in population])\n        return fitness\n\n    def crowding_distance(self, fitness, population):\n        \"\"\"Calculates the crowding distance for each individual in the population.\"\"\"\n        distances = np.zeros(self.pop_size)\n        for i in range(self.dim):\n            # Sort by each dimension\n            sorted_indices = np.argsort(population[:, i])\n            distances[sorted_indices[0]] = distances[sorted_indices[-1]] = np.inf  # Boundary individuals\n\n            for j in range(1, self.pop_size - 1):\n                distances[sorted_indices[j]] += (population[sorted_indices[j + 1], i] - population[sorted_indices[j - 1], i]) / (self.ub - self.lb) # Normalizing with bounds\n        return distances\n    \n    def selection(self, population, fitness):\n          \"\"\"Selects individuals for the next generation using rank-based selection combined with crowding distance.\"\"\"\n          # Rank individuals based on fitness (lower fitness is better)\n          ranked_indices = np.argsort(fitness)\n\n          # Calculate selection probabilities based on rank\n          rank = np.arange(1, self.pop_size + 1)\n          probabilities = (2 - self.selection_pressure + 2 * (self.selection_pressure - 1) * (self.pop_size - rank) / (self.pop_size * (self.pop_size - 1)))\n          probabilities /= np.sum(probabilities)\n\n          # Calculate crowding distances\n          distances = self.crowding_distance(fitness, population)\n\n          # Combine probabilities with crowding distance (favor less crowded regions)\n          combined_metric = probabilities + distances / np.max(distances)  # Normalize distances\n\n          # Normalize the combined metric to get probabilities\n          probabilities = combined_metric / np.sum(combined_metric)\n\n          # Select parents based on probabilities\n          selected_indices = np.random.choice(self.pop_size, size=self.pop_size, replace=True, p=probabilities)\n          return population[selected_indices]\n\n\n    def mutation(self, population):\n        \"\"\"Applies mutation to the population.\"\"\"\n        mutation_mask = np.random.rand(*population.shape) < self.mutation_rate\n        mutation_values = np.random.normal(0, self.mutation_scale, size=population.shape)\n        population += mutation_mask * mutation_values\n        population = np.clip(population, self.lb, self.ub)  # Clip to bounds\n        return population\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        eval_count = 0\n        \n        population = self.initialize_population()\n        fitness = self.evaluate_population(func, population)\n        eval_count += self.pop_size\n        \n        best_index = np.argmin(fitness)\n        if fitness[best_index] < self.f_opt:\n                self.f_opt = fitness[best_index]\n                self.x_opt = population[best_index]\n\n        while eval_count < self.budget:\n            population = self.selection(population, fitness)\n            population = self.mutation(population)\n\n            new_fitness = self.evaluate_population(func, population)\n            eval_count += self.pop_size\n            \n            \n            best_index = np.argmin(new_fitness)\n\n            if new_fitness[best_index] < self.f_opt:\n                self.f_opt = new_fitness[best_index]\n                self.x_opt = population[best_index]\n            \n            fitness = new_fitness\n            \n            if eval_count > self.budget:\n                eval_count -= self.pop_size\n                break\n                \n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "", "error": "Evaluation error: probabilities contain NaN", "parent_ids": [], "operator": "init", "metadata": {"llm_response": "# Description: A population-based algorithm with adaptive mutation and selection based on fitness rank and crowding distance.\n\n# Code:\n```python\nimport numpy as np\n\nclass AdaptivePopulationSearch:\n    def __init__(self, budget=10000, dim=10, pop_size=50):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lb = -5.0\n        self.ub = 5.0\n        self.mutation_rate = 0.1\n        self.mutation_scale = 1.0\n        self.selection_pressure = 2.0 # Higher value emphasizes fitter individuals\n\n    def initialize_population(self):\n        return np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n\n    def evaluate_population(self, func, population):\n        fitness = np.array([func(x) for x in population])\n        return fitness\n\n    def crowding_distance(self, fitness, population):\n        \"\"\"Calculates the crowding distance for each individual in the population.\"\"\"\n        distances = np.zeros(self.pop_size)\n        for i in range(self.dim):\n            # Sort by each dimension\n            sorted_indices = np.argsort(population[:, i])\n            distances[sorted_indices[0]] = distances[sorted_indices[-1]] = np.inf  # Boundary individuals\n\n            for j in range(1, self.pop_size - 1):\n                distances[sorted_indices[j]] += (population[sorted_indices[j + 1], i] - population[sorted_indices[j - 1], i]) / (self.ub - self.lb) # Normalizing with bounds\n        return distances\n    \n    def selection(self, population, fitness):\n          \"\"\"Selects individuals for the next generation using rank-based selection combined with crowding distance.\"\"\"\n          # Rank individuals based on fitness (lower fitness is better)\n          ranked_indices = np.argsort(fitness)\n\n          # Calculate selection probabilities based on rank\n          rank = np.arange(1, self.pop_size + 1)\n          probabilities = (2 - self.selection_pressure + 2 * (self.selection_pressure - 1) * (self.pop_size - rank) / (self.pop_size * (self.pop_size - 1)))\n          probabilities /= np.sum(probabilities)\n\n          # Calculate crowding distances\n          distances = self.crowding_distance(fitness, population)\n\n          # Combine probabilities with crowding distance (favor less crowded regions)\n          combined_metric = probabilities + distances / np.max(distances)  # Normalize distances\n\n          # Normalize the combined metric to get probabilities\n          probabilities = combined_metric / np.sum(combined_metric)\n\n          # Select parents based on probabilities\n          selected_indices = np.random.choice(self.pop_size, size=self.pop_size, replace=True, p=probabilities)\n          return population[selected_indices]\n\n\n    def mutation(self, population):\n        \"\"\"Applies mutation to the population.\"\"\"\n        mutation_mask = np.random.rand(*population.shape) < self.mutation_rate\n        mutation_values = np.random.normal(0, self.mutation_scale, size=population.shape)\n        population += mutation_mask * mutation_values\n        population = np.clip(population, self.lb, self.ub)  # Clip to bounds\n        return population\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        eval_count = 0\n        \n        population = self.initialize_population()\n        fitness = self.evaluate_population(func, population)\n        eval_count += self.pop_size\n        \n        best_index = np.argmin(fitness)\n        if fitness[best_index] < self.f_opt:\n                self.f_opt = fitness[best_index]\n                self.x_opt = population[best_index]\n\n        while eval_count < self.budget:\n            population = self.selection(population, fitness)\n            population = self.mutation(population)\n\n            new_fitness = self.evaluate_population(func, population)\n            eval_count += self.pop_size\n            \n            \n            best_index = np.argmin(new_fitness)\n\n            if new_fitness[best_index] < self.f_opt:\n                self.f_opt = new_fitness[best_index]\n                self.x_opt = population[best_index]\n            \n            fitness = new_fitness\n            \n            if eval_count > self.budget:\n                eval_count -= self.pop_size\n                break\n                \n\n        return self.f_opt, self.x_opt\n```"}, "task_prompt": ""}
{"id": "195dcdea-cb38-4606-91a5-f62377e19be2", "fitness": 0.0, "name": "AdaptiveSearch", "description": "An adaptive search algorithm that dynamically adjusts its exploration-exploitation balance based on the success rate of previous iterations.", "code": "import numpy as np\n\nclass AdaptiveSearch:\n    def __init__(self, budget=10000, dim=10, initial_sigma=0.1, adaptation_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.sigma = initial_sigma\n        self.adaptation_rate = adaptation_rate\n        self.lb = -5.0\n        self.ub = 5.0\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        x = np.random.uniform(self.lb, self.ub, size=self.dim)\n        f = func(x)\n        self.f_opt = f\n        self.x_opt = x\n        success_count = 0\n\n        for i in range(self.budget):\n            # Generate a candidate solution by adding Gaussian noise\n            x_candidate = x + np.random.normal(0, self.sigma, size=self.dim)\n\n            # Clip the candidate solution to stay within the bounds\n            x_candidate = np.clip(x_candidate, self.lb, self.ub)\n            \n            f_candidate = func(x_candidate)\n\n            if f_candidate < self.f_opt:\n                self.f_opt = f_candidate\n                self.x_opt = x_candidate\n                x = x_candidate\n                f = f_candidate\n                success_count += 1\n            else:\n                # Explore around the current best if we didn't find anything better\n                x_explore = np.random.uniform(self.lb, self.ub, size=self.dim)\n                f_explore = func(x_explore)\n                if f_explore < self.f_opt:\n                    self.f_opt = f_explore\n                    self.x_opt = x_explore\n                    x = x_explore\n                    f = f_explore\n\n            # Adapt the step size (sigma) based on the success rate\n            success_rate = success_count / (i + 1)\n            if success_rate > 0.2:\n                self.sigma *= (1 - self.adaptation_rate)  # Reduce sigma (exploitation)\n            elif success_rate < 0.1:\n                self.sigma *= (1 + self.adaptation_rate)  # Increase sigma (exploration)\n            self.sigma = np.clip(self.sigma, 0.0001, 1.0)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "The algorithm AdaptiveSearch scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": "random_new", "metadata": {"llm_response": "# Description: An adaptive search algorithm that dynamically adjusts its exploration-exploitation balance based on the success rate of previous iterations.\n# Code: \n```python\nimport numpy as np\n\nclass AdaptiveSearch:\n    def __init__(self, budget=10000, dim=10, initial_sigma=0.1, adaptation_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.sigma = initial_sigma\n        self.adaptation_rate = adaptation_rate\n        self.lb = -5.0\n        self.ub = 5.0\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        x = np.random.uniform(self.lb, self.ub, size=self.dim)\n        f = func(x)\n        self.f_opt = f\n        self.x_opt = x\n        success_count = 0\n\n        for i in range(self.budget):\n            # Generate a candidate solution by adding Gaussian noise\n            x_candidate = x + np.random.normal(0, self.sigma, size=self.dim)\n\n            # Clip the candidate solution to stay within the bounds\n            x_candidate = np.clip(x_candidate, self.lb, self.ub)\n            \n            f_candidate = func(x_candidate)\n\n            if f_candidate < self.f_opt:\n                self.f_opt = f_candidate\n                self.x_opt = x_candidate\n                x = x_candidate\n                f = f_candidate\n                success_count += 1\n            else:\n                # Explore around the current best if we didn't find anything better\n                x_explore = np.random.uniform(self.lb, self.ub, size=self.dim)\n                f_explore = func(x_explore)\n                if f_explore < self.f_opt:\n                    self.f_opt = f_explore\n                    self.x_opt = x_explore\n                    x = x_explore\n                    f = f_explore\n\n            # Adapt the step size (sigma) based on the success rate\n            success_rate = success_count / (i + 1)\n            if success_rate > 0.2:\n                self.sigma *= (1 - self.adaptation_rate)  # Reduce sigma (exploitation)\n            elif success_rate < 0.1:\n                self.sigma *= (1 + self.adaptation_rate)  # Increase sigma (exploration)\n            self.sigma = np.clip(self.sigma, 0.0001, 1.0)\n\n        return self.f_opt, self.x_opt\n```", "aucs": [0], "operator": "random_new", "theta_sampled": 0.11504529503462152, "reward": 0.0, "generation": 1}, "task_prompt": ""}
{"id": "3e48c9d9-7ef3-463a-a36a-ac07be47b6ad", "fitness": 0.0, "name": "HybridOptimizer", "description": "A hybrid algorithm that combines Slime Mold's weighted averaging with Adaptive DE's mutation and crossover, incorporating archive and restart mechanisms for exploration and exploitation.", "code": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget=10000, dim=10, population_size=20, w_decay=0.9, p_selection=0.2, F=0.5, CR=0.7, restart_factor=0.1, archive_factor=2.0):\n        \"\"\"\n        Hybrid Optimization algorithm combining Slime Mold and Adaptive Differential Evolution.\n\n        Args:\n            budget (int): The maximum number of function evaluations.\n            dim (int): The dimensionality of the search space.\n            population_size (int): The number of slime mold agents/DE population size.\n            w_decay (float): Decay rate for the weight parameter (Slime Mold).\n            p_selection (float): Percentage of top solutions to consider for weighted averaging (Slime Mold).\n            F (float): Initial scaling factor (DE).\n            CR (float): Initial crossover rate (DE).\n            restart_factor (float): Fraction of budget to be consumed before a restart is triggered.\n            archive_factor (float): Size of archive relative to population size.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.population_size = population_size\n        self.w_decay = w_decay\n        self.p_selection = p_selection\n        self.F = F\n        self.CR = CR\n        self.restart_factor = restart_factor\n        self.restart_iterations = int(restart_factor * budget)\n        self.lb = -5.0\n        self.ub = 5.0\n        self.archive_factor = archive_factor\n        self.archive = []\n        self.eval_count = 0\n\n\n    def __call__(self, func):\n        \"\"\"\n        Optimizes the given function using the Hybrid Optimizer.\n\n        Args:\n            func (callable): The black-box function to optimize.\n\n        Returns:\n            tuple: The best function value found and the corresponding solution.\n        \"\"\"\n        population = np.random.uniform(self.lb, self.ub, size=(self.population_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.eval_count += self.population_size\n        \n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index].copy()\n\n        while self.eval_count < self.budget:\n            for i in range(self.population_size):\n                if self.eval_count >= self.budget:\n                    break\n                # Adaptive DE components\n                F_i = np.clip(np.random.normal(self.F, 0.1), 0.1, 1.0)\n                CR_i = np.clip(np.random.normal(self.CR, 0.1), 0.1, 1.0)\n\n                # Mutation\n                idxs = np.random.choice(self.population_size, 3, replace=False)\n                x_1, x_2, x_3 = population[idxs]\n                mutant = population[i] + F_i * (x_2 - x_3)\n                mutant = np.clip(mutant, self.lb, self.ub)\n                \n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR_i\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial_vector = np.where(cross_points, mutant, population[i])\n\n                # Slime Mold inspired weighted averaging - applied to trial vector before evaluation\n                # Sort population based on fitness (using current fitness)\n                sorted_indices = np.argsort(fitness)\n                sorted_population = population[sorted_indices]\n                sorted_fitness = fitness[sorted_indices]\n            \n                # Select top solutions based on p_selection\n                num_selected = int(self.p_selection * self.population_size)\n                selected_population = sorted_population[:num_selected]\n                selected_fitness = sorted_fitness[:num_selected]\n                \n                # Calculate weights based on fitness rank. Give the better ones more weight\n                rank = np.arange(1, num_selected + 1)\n                weights_selected = np.exp(-rank / num_selected)\n                weights_selected /= np.sum(weights_selected)  # Normalize\n\n                # Weighted average of selected solutions\n                weighted_avg = np.zeros(self.dim)\n                for j in range(num_selected):\n                    weighted_avg += weights_selected[j] * selected_population[j]\n                    \n                trial_vector = 0.5 * trial_vector + 0.5 * weighted_avg # Combine DE result with weighted average\n                trial_vector = np.clip(trial_vector, self.lb, self.ub)\n\n\n                # Selection\n                f_trial = func(trial_vector)\n                self.eval_count += 1\n\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    population[i] = trial_vector.copy()\n\n                    # Update archive\n                    self.archive.append(population[i].copy())\n                    if len(self.archive) > self.archive_factor * self.population_size:\n                        self.archive.pop(0)\n\n\n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = trial_vector.copy()\n\n            # Restart Mechanism\n            if self.eval_count % self.restart_iterations == 0:\n                best_idx = np.argmin(fitness)\n                best_individual = population[best_idx].copy()\n\n                population = np.random.uniform(self.lb, self.ub, (self.population_size, self.dim))\n                population[0] = best_individual # Keep the best\n                new_fitness = np.array([func(x) for x in population])\n                self.eval_count += self.population_size -1\n                fitness = new_fitness\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "The algorithm HybridOptimizer scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["2d086322-f43f-4b37-bed4-2235e826723e", "49b01a83-152a-4b37-86ca-d659431fcf99"], "operator": "crossover", "metadata": {"llm_response": "# Description: A hybrid algorithm that combines Slime Mold's weighted averaging with Adaptive DE's mutation and crossover, incorporating archive and restart mechanisms for exploration and exploitation.\n# Code: \n```python\nimport numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget=10000, dim=10, population_size=20, w_decay=0.9, p_selection=0.2, F=0.5, CR=0.7, restart_factor=0.1, archive_factor=2.0):\n        \"\"\"\n        Hybrid Optimization algorithm combining Slime Mold and Adaptive Differential Evolution.\n\n        Args:\n            budget (int): The maximum number of function evaluations.\n            dim (int): The dimensionality of the search space.\n            population_size (int): The number of slime mold agents/DE population size.\n            w_decay (float): Decay rate for the weight parameter (Slime Mold).\n            p_selection (float): Percentage of top solutions to consider for weighted averaging (Slime Mold).\n            F (float): Initial scaling factor (DE).\n            CR (float): Initial crossover rate (DE).\n            restart_factor (float): Fraction of budget to be consumed before a restart is triggered.\n            archive_factor (float): Size of archive relative to population size.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.population_size = population_size\n        self.w_decay = w_decay\n        self.p_selection = p_selection\n        self.F = F\n        self.CR = CR\n        self.restart_factor = restart_factor\n        self.restart_iterations = int(restart_factor * budget)\n        self.lb = -5.0\n        self.ub = 5.0\n        self.archive_factor = archive_factor\n        self.archive = []\n        self.eval_count = 0\n\n\n    def __call__(self, func):\n        \"\"\"\n        Optimizes the given function using the Hybrid Optimizer.\n\n        Args:\n            func (callable): The black-box function to optimize.\n\n        Returns:\n            tuple: The best function value found and the corresponding solution.\n        \"\"\"\n        population = np.random.uniform(self.lb, self.ub, size=(self.population_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.eval_count += self.population_size\n        \n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index].copy()\n\n        while self.eval_count < self.budget:\n            for i in range(self.population_size):\n                if self.eval_count >= self.budget:\n                    break\n                # Adaptive DE components\n                F_i = np.clip(np.random.normal(self.F, 0.1), 0.1, 1.0)\n                CR_i = np.clip(np.random.normal(self.CR, 0.1), 0.1, 1.0)\n\n                # Mutation\n                idxs = np.random.choice(self.population_size, 3, replace=False)\n                x_1, x_2, x_3 = population[idxs]\n                mutant = population[i] + F_i * (x_2 - x_3)\n                mutant = np.clip(mutant, self.lb, self.ub)\n                \n                # Crossover\n                cross_points = np.random.rand(self.dim) < CR_i\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial_vector = np.where(cross_points, mutant, population[i])\n\n                # Slime Mold inspired weighted averaging - applied to trial vector before evaluation\n                # Sort population based on fitness (using current fitness)\n                sorted_indices = np.argsort(fitness)\n                sorted_population = population[sorted_indices]\n                sorted_fitness = fitness[sorted_indices]\n            \n                # Select top solutions based on p_selection\n                num_selected = int(self.p_selection * self.population_size)\n                selected_population = sorted_population[:num_selected]\n                selected_fitness = sorted_fitness[:num_selected]\n                \n                # Calculate weights based on fitness rank. Give the better ones more weight\n                rank = np.arange(1, num_selected + 1)\n                weights_selected = np.exp(-rank / num_selected)\n                weights_selected /= np.sum(weights_selected)  # Normalize\n\n                # Weighted average of selected solutions\n                weighted_avg = np.zeros(self.dim)\n                for j in range(num_selected):\n                    weighted_avg += weights_selected[j] * selected_population[j]\n                    \n                trial_vector = 0.5 * trial_vector + 0.5 * weighted_avg # Combine DE result with weighted average\n                trial_vector = np.clip(trial_vector, self.lb, self.ub)\n\n\n                # Selection\n                f_trial = func(trial_vector)\n                self.eval_count += 1\n\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    population[i] = trial_vector.copy()\n\n                    # Update archive\n                    self.archive.append(population[i].copy())\n                    if len(self.archive) > self.archive_factor * self.population_size:\n                        self.archive.pop(0)\n\n\n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = trial_vector.copy()\n\n            # Restart Mechanism\n            if self.eval_count % self.restart_iterations == 0:\n                best_idx = np.argmin(fitness)\n                best_individual = population[best_idx].copy()\n\n                population = np.random.uniform(self.lb, self.ub, (self.population_size, self.dim))\n                population[0] = best_individual # Keep the best\n                new_fitness = np.array([func(x) for x in population])\n                self.eval_count += self.population_size -1\n                fitness = new_fitness\n\n\n        return self.f_opt, self.x_opt\n```", "aucs": [0], "operator": "crossover", "theta_sampled": 0.21928028627360516, "reward": 0.0, "generation": 1}, "task_prompt": ""}
{"id": "f18d9e39-c11d-47da-9e63-830a8dfabf80", "fitness": 0.0, "name": "AdaptiveGradientSearch", "description": "An adaptive search strategy that balances exploration and exploitation by dynamically adjusting the search radius based on local gradient estimation and fitness landscape characteristics.", "code": "import numpy as np\n\nclass AdaptiveGradientSearch:\n    def __init__(self, budget=10000, dim=10, initial_radius=0.5, exploration_prob=0.2):\n        self.budget = budget\n        self.dim = dim\n        self.radius = initial_radius\n        self.exploration_prob = exploration_prob\n        self.learning_rate = 0.1\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = np.random.uniform(func.bounds.lb, func.bounds.ub)  # Initialize at a random point\n        self.f_opt = func(self.x_opt)\n        eval_count = 1\n\n        while eval_count < self.budget:\n            # Exploration or Exploitation\n            if np.random.rand() < self.exploration_prob:\n                # Exploration: Randomly sample around the current best\n                x_new = self.x_opt + np.random.uniform(-self.radius, self.radius, size=self.dim)\n                x_new = np.clip(x_new, func.bounds.lb, func.bounds.ub)  # Ensure bounds are respected\n            else:\n                # Exploitation: Estimate gradient and move accordingly\n\n                # Numerical gradient estimation using central differences\n                gradient = np.zeros(self.dim)\n                for i in range(self.dim):\n                    x_plus = self.x_opt.copy()\n                    x_minus = self.x_opt.copy()\n                    delta = 0.001  # Small perturbation\n                    x_plus[i] += delta\n                    x_minus[i] -= delta\n\n                    x_plus = np.clip(x_plus, func.bounds.lb, func.bounds.ub)\n                    x_minus = np.clip(x_minus, func.bounds.lb, func.bounds.ub)\n\n                    f_plus = func(x_plus)\n                    f_minus = func(x_minus)\n                    \n                    eval_count += 2\n                    if eval_count > self.budget:\n                        return self.f_opt, self.x_opt\n\n                    gradient[i] = (f_plus - f_minus) / (2 * delta)\n\n                # Normalize gradient\n                norm = np.linalg.norm(gradient)\n                if norm > 0:\n                    gradient = gradient / norm\n\n                # Move in the direction of the negative gradient\n                x_new = self.x_opt - self.radius * gradient\n                x_new = np.clip(x_new, func.bounds.lb, func.bounds.ub)\n                \n            f_new = func(x_new)\n            eval_count += 1\n\n            if f_new < self.f_opt:\n                # Improvement: Accept the new solution and potentially increase the search radius\n                self.f_opt = f_new\n                self.x_opt = x_new\n                self.radius *= (1 + self.learning_rate) # increase radius slightly for finer adjustments around optimum\n            else:\n                # No improvement: Reduce the search radius to refine the search\n                self.radius *= (1 - self.learning_rate) # reduce radius to focus\n\n            # Ensure radius remains within reasonable bounds\n            self.radius = np.clip(self.radius, 1e-6, 1.0)\n            \n            if eval_count > self.budget:\n                  return self.f_opt, self.x_opt\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "The algorithm AdaptiveGradientSearch scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": "random_new", "metadata": {"llm_response": "# Description: An adaptive search strategy that balances exploration and exploitation by dynamically adjusting the search radius based on local gradient estimation and fitness landscape characteristics.\n# Code: \n```python\nimport numpy as np\n\nclass AdaptiveGradientSearch:\n    def __init__(self, budget=10000, dim=10, initial_radius=0.5, exploration_prob=0.2):\n        self.budget = budget\n        self.dim = dim\n        self.radius = initial_radius\n        self.exploration_prob = exploration_prob\n        self.learning_rate = 0.1\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = np.random.uniform(func.bounds.lb, func.bounds.ub)  # Initialize at a random point\n        self.f_opt = func(self.x_opt)\n        eval_count = 1\n\n        while eval_count < self.budget:\n            # Exploration or Exploitation\n            if np.random.rand() < self.exploration_prob:\n                # Exploration: Randomly sample around the current best\n                x_new = self.x_opt + np.random.uniform(-self.radius, self.radius, size=self.dim)\n                x_new = np.clip(x_new, func.bounds.lb, func.bounds.ub)  # Ensure bounds are respected\n            else:\n                # Exploitation: Estimate gradient and move accordingly\n\n                # Numerical gradient estimation using central differences\n                gradient = np.zeros(self.dim)\n                for i in range(self.dim):\n                    x_plus = self.x_opt.copy()\n                    x_minus = self.x_opt.copy()\n                    delta = 0.001  # Small perturbation\n                    x_plus[i] += delta\n                    x_minus[i] -= delta\n\n                    x_plus = np.clip(x_plus, func.bounds.lb, func.bounds.ub)\n                    x_minus = np.clip(x_minus, func.bounds.lb, func.bounds.ub)\n\n                    f_plus = func(x_plus)\n                    f_minus = func(x_minus)\n                    \n                    eval_count += 2\n                    if eval_count > self.budget:\n                        return self.f_opt, self.x_opt\n\n                    gradient[i] = (f_plus - f_minus) / (2 * delta)\n\n                # Normalize gradient\n                norm = np.linalg.norm(gradient)\n                if norm > 0:\n                    gradient = gradient / norm\n\n                # Move in the direction of the negative gradient\n                x_new = self.x_opt - self.radius * gradient\n                x_new = np.clip(x_new, func.bounds.lb, func.bounds.ub)\n                \n            f_new = func(x_new)\n            eval_count += 1\n\n            if f_new < self.f_opt:\n                # Improvement: Accept the new solution and potentially increase the search radius\n                self.f_opt = f_new\n                self.x_opt = x_new\n                self.radius *= (1 + self.learning_rate) # increase radius slightly for finer adjustments around optimum\n            else:\n                # No improvement: Reduce the search radius to refine the search\n                self.radius *= (1 - self.learning_rate) # reduce radius to focus\n\n            # Ensure radius remains within reasonable bounds\n            self.radius = np.clip(self.radius, 1e-6, 1.0)\n            \n            if eval_count > self.budget:\n                  return self.f_opt, self.x_opt\n\n        return self.f_opt, self.x_opt\n```", "aucs": [0], "operator": "random_new", "theta_sampled": -0.05658296968091885, "reward": 0.0, "generation": 1}, "task_prompt": ""}
{"id": "e28dc9c3-3d4b-4868-a051-3c9dc94d420f", "fitness": 0.37087561117792156, "name": "AdaptiveMutationOptimizer", "description": "Population-based algorithm using a combination of Gaussian mutation and Cauchy mutation to promote exploration and exploitation. Uses a dynamic adaptation of mutation probabilities based on population diversity.", "code": "import numpy as np\n\nclass AdaptiveMutationOptimizer:\n    def __init__(self, budget=10000, dim=10, pop_size=50):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lb = -5.0\n        self.ub = 5.0\n        self.mutation_prob_gaussian = 0.5  # Initial probability of Gaussian mutation\n        self.mutation_prob_cauchy = 0.5  # Initial probability of Cauchy mutation\n        self.mutation_scale = 1.0 # Initial mutation scale\n\n    def initialize_population(self):\n        return np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n\n    def calculate_diversity(self, population):\n        \"\"\"Calculates diversity based on the average distance to the centroid.\"\"\"\n        centroid = np.mean(population, axis=0)\n        distances = np.linalg.norm(population - centroid, axis=1)\n        return np.mean(distances)\n\n    def mutate_gaussian(self, x):\n        return x + self.mutation_scale * np.random.normal(0, 1, size=self.dim)\n\n    def mutate_cauchy(self, x):\n        return x + self.mutation_scale * np.random.standard_cauchy(size=self.dim)\n\n    def correct_bounds(self, x):\n         return np.clip(x, self.lb, self.ub)\n\n\n    def __call__(self, func):\n        population = self.initialize_population()\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n        \n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index]\n\n        while self.budget > 0:\n            diversity = self.calculate_diversity(population)\n\n            # Adjust mutation probabilities based on diversity\n            if diversity > 1.0:  # High diversity, favor exploitation\n                self.mutation_prob_gaussian = 0.8\n                self.mutation_prob_cauchy = 0.2\n            else:  # Low diversity, favor exploration\n                self.mutation_prob_gaussian = 0.2\n                self.mutation_prob_cauchy = 0.8\n\n            new_population = np.copy(population)\n            for i in range(self.pop_size):\n                if np.random.rand() < self.mutation_prob_gaussian:\n                    new_population[i] = self.mutate_gaussian(population[i])\n                else:\n                    new_population[i] = self.mutate_cauchy(population[i])\n\n                new_population[i] = self.correct_bounds(new_population[i])\n                \n                new_fitness = func(new_population[i])\n                self.budget -= 1\n\n                if new_fitness < fitness[i]:\n                    fitness[i] = new_fitness\n                    population[i] = new_population[i]\n\n                    if new_fitness < self.f_opt:\n                        self.f_opt = new_fitness\n                        self.x_opt = new_population[i]\n            \n            # Adapt mutation scale\n            self.mutation_scale = 0.95 * self.mutation_scale # Decay mutation scale\n            \n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "The algorithm AdaptiveMutationOptimizer scored 0.371 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": "random_new", "metadata": {"llm_response": "# Description: Population-based algorithm using a combination of Gaussian mutation and Cauchy mutation to promote exploration and exploitation. Uses a dynamic adaptation of mutation probabilities based on population diversity.\n# Code: \n```python\nimport numpy as np\n\nclass AdaptiveMutationOptimizer:\n    def __init__(self, budget=10000, dim=10, pop_size=50):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lb = -5.0\n        self.ub = 5.0\n        self.mutation_prob_gaussian = 0.5  # Initial probability of Gaussian mutation\n        self.mutation_prob_cauchy = 0.5  # Initial probability of Cauchy mutation\n        self.mutation_scale = 1.0 # Initial mutation scale\n\n    def initialize_population(self):\n        return np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n\n    def calculate_diversity(self, population):\n        \"\"\"Calculates diversity based on the average distance to the centroid.\"\"\"\n        centroid = np.mean(population, axis=0)\n        distances = np.linalg.norm(population - centroid, axis=1)\n        return np.mean(distances)\n\n    def mutate_gaussian(self, x):\n        return x + self.mutation_scale * np.random.normal(0, 1, size=self.dim)\n\n    def mutate_cauchy(self, x):\n        return x + self.mutation_scale * np.random.standard_cauchy(size=self.dim)\n\n    def correct_bounds(self, x):\n         return np.clip(x, self.lb, self.ub)\n\n\n    def __call__(self, func):\n        population = self.initialize_population()\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n        \n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index]\n\n        while self.budget > 0:\n            diversity = self.calculate_diversity(population)\n\n            # Adjust mutation probabilities based on diversity\n            if diversity > 1.0:  # High diversity, favor exploitation\n                self.mutation_prob_gaussian = 0.8\n                self.mutation_prob_cauchy = 0.2\n            else:  # Low diversity, favor exploration\n                self.mutation_prob_gaussian = 0.2\n                self.mutation_prob_cauchy = 0.8\n\n            new_population = np.copy(population)\n            for i in range(self.pop_size):\n                if np.random.rand() < self.mutation_prob_gaussian:\n                    new_population[i] = self.mutate_gaussian(population[i])\n                else:\n                    new_population[i] = self.mutate_cauchy(population[i])\n\n                new_population[i] = self.correct_bounds(new_population[i])\n                \n                new_fitness = func(new_population[i])\n                self.budget -= 1\n\n                if new_fitness < fitness[i]:\n                    fitness[i] = new_fitness\n                    population[i] = new_population[i]\n\n                    if new_fitness < self.f_opt:\n                        self.f_opt = new_fitness\n                        self.x_opt = new_population[i]\n            \n            # Adapt mutation scale\n            self.mutation_scale = 0.95 * self.mutation_scale # Decay mutation scale\n            \n\n        return self.f_opt, self.x_opt\n```", "aucs": [0.1489720460845838, 0.20305748223211662, 0.3755872445205771, 0.3899644668958282, 0.30586953665844974, 0.4804493993708221, 0.2735211292122093, 0.27228041622449495, 0.34189172827480274, 0.18285279082265105, 0.2753260098859879, 0.9995010101545647, 0.3232184233654446, 0.25388173389787017, 0.6936761813439848, 0.3697839370434556, 0.2746044913971728, 0.48463532243480545, 0.2789908536993848, 0.48944802003922516], "operator": "random_new", "theta_sampled": -0.03129285944085092, "reward": 0.0, "generation": 1}, "task_prompt": ""}
{"id": "125c52bb-8777-4feb-b57a-1eb805ed2e25", "fitness": 0.23200194149233472, "name": "HybridSlimeDE", "description": "Hybrid algorithm combining Slime Mold's weighted averaging with Adaptive DE's mutation and crossover, using a self-adaptive selection pressure for exploitation/exploration balance.", "code": "import numpy as np\n\nclass HybridSlimeDE:\n    def __init__(self, budget=10000, dim=10, population_size=20, w_decay=0.9, p_selection_initial=0.2, F=0.5, CR=0.7):\n        \"\"\"\n        Hybrid algorithm combining Slime Mold's weighted averaging with Adaptive DE's mutation and crossover.\n\n        Args:\n            budget (int): The maximum number of function evaluations.\n            dim (int): The dimensionality of the search space.\n            population_size (int): The number of agents.\n            w_decay (float): Decay rate for a weight parameter (unused currently, but can be added for future improvement)\n            p_selection_initial (float): Initial percentage of top solutions to consider for weighted averaging. This will be adapted over time.\n            F (float): Initial scaling factor for DE mutation.\n            CR (float): Initial crossover rate for DE crossover.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.population_size = population_size\n        self.w_decay = w_decay\n        self.p_selection = p_selection_initial\n        self.F = F\n        self.CR = CR\n        self.lb = -5.0\n        self.ub = 5.0\n        self.archive_factor = 2.0\n        self.archive = []\n\n\n    def __call__(self, func):\n        \"\"\"\n        Optimizes the given function using the hybrid algorithm.\n\n        Args:\n            func (callable): The black-box function to optimize.\n\n        Returns:\n            tuple: The best function value found and the corresponding solution.\n        \"\"\"\n        population = np.random.uniform(self.lb, self.ub, size=(self.population_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.population_size\n        \n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index].copy()\n\n        while self.budget > 0:\n            # Sort population based on fitness\n            sorted_indices = np.argsort(fitness)\n            population = population[sorted_indices]\n            fitness = fitness[sorted_indices]\n            \n            # Adaptive p_selection: Gradually reduce to shift from exploration to exploitation\n            self.p_selection = self.p_selection * 0.999 # Exponential decay\n\n\n            num_selected = int(max(2, self.p_selection * self.population_size)) # Ensure at least 2\n\n            selected_population = population[:num_selected]\n            selected_fitness = fitness[:num_selected]\n\n            # Calculate weights based on fitness rank. Give the better ones more weight\n            rank = np.arange(1, num_selected + 1)\n            weights_selected = np.exp(-rank / num_selected)\n            weights_selected /= np.sum(weights_selected)  # Normalize\n\n            # Update population using DE and Slime Mold concepts\n            for i in range(self.population_size):\n                if self.budget <= 0:\n                    break\n\n                # Adaptive DE parameters\n                F_i = np.clip(np.random.normal(self.F, 0.1), 0.1, 1.0)\n                CR_i = np.clip(np.random.normal(self.CR, 0.1), 0.1, 1.0)\n\n\n                # Mutation (DE part)\n                idxs = np.random.choice(self.population_size, 3, replace=False)\n                x_1, x_2, x_3 = population[idxs]\n                mutant = population[i] + F_i * (x_2 - x_3)\n                mutant = np.clip(mutant, self.lb, self.ub)\n\n\n                # Crossover (DE part)\n                cross_points = np.random.rand(self.dim) < CR_i\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial_vector_DE = np.where(cross_points, mutant, population[i])\n\n\n                # Slime Mold inspired perturbation: Weighted average of selected solutions\n                x_sm = np.zeros(self.dim)\n                for j in range(num_selected):\n                    x_sm += weights_selected[j] * selected_population[j]\n\n                # Combine DE and Slime Mold: \n                # Option 1: Average the trial vector from DE with Slime Mold perturbation\n                trial_vector = 0.5 * trial_vector_DE + 0.5 * x_sm\n\n                # Option 2: Apply Slime Mold perturbation to DE's trial vector\n                # trial_vector = trial_vector_DE + 0.1 * (x_sm - trial_vector_DE)\n\n\n                trial_vector = np.clip(trial_vector, self.lb, self.ub)\n\n\n\n                f_trial = func(trial_vector)\n                self.budget -= 1\n\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    population[i] = trial_vector.copy()\n\n                    #Update Archive\n                    self.archive.append(population[i].copy())\n                    if len(self.archive) > self.archive_factor * self.population_size:\n                        self.archive.pop(0)\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial_vector.copy()\n                        \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "The algorithm HybridSlimeDE scored 0.232 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["2d086322-f43f-4b37-bed4-2235e826723e", "49b01a83-152a-4b37-86ca-d659431fcf99"], "operator": "crossover", "metadata": {"llm_response": "# Description: Hybrid algorithm combining Slime Mold's weighted averaging with Adaptive DE's mutation and crossover, using a self-adaptive selection pressure for exploitation/exploration balance.\n# Code: \n```python\nimport numpy as np\n\nclass HybridSlimeDE:\n    def __init__(self, budget=10000, dim=10, population_size=20, w_decay=0.9, p_selection_initial=0.2, F=0.5, CR=0.7):\n        \"\"\"\n        Hybrid algorithm combining Slime Mold's weighted averaging with Adaptive DE's mutation and crossover.\n\n        Args:\n            budget (int): The maximum number of function evaluations.\n            dim (int): The dimensionality of the search space.\n            population_size (int): The number of agents.\n            w_decay (float): Decay rate for a weight parameter (unused currently, but can be added for future improvement)\n            p_selection_initial (float): Initial percentage of top solutions to consider for weighted averaging. This will be adapted over time.\n            F (float): Initial scaling factor for DE mutation.\n            CR (float): Initial crossover rate for DE crossover.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.population_size = population_size\n        self.w_decay = w_decay\n        self.p_selection = p_selection_initial\n        self.F = F\n        self.CR = CR\n        self.lb = -5.0\n        self.ub = 5.0\n        self.archive_factor = 2.0\n        self.archive = []\n\n\n    def __call__(self, func):\n        \"\"\"\n        Optimizes the given function using the hybrid algorithm.\n\n        Args:\n            func (callable): The black-box function to optimize.\n\n        Returns:\n            tuple: The best function value found and the corresponding solution.\n        \"\"\"\n        population = np.random.uniform(self.lb, self.ub, size=(self.population_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.population_size\n        \n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index].copy()\n\n        while self.budget > 0:\n            # Sort population based on fitness\n            sorted_indices = np.argsort(fitness)\n            population = population[sorted_indices]\n            fitness = fitness[sorted_indices]\n            \n            # Adaptive p_selection: Gradually reduce to shift from exploration to exploitation\n            self.p_selection = self.p_selection * 0.999 # Exponential decay\n\n\n            num_selected = int(max(2, self.p_selection * self.population_size)) # Ensure at least 2\n\n            selected_population = population[:num_selected]\n            selected_fitness = fitness[:num_selected]\n\n            # Calculate weights based on fitness rank. Give the better ones more weight\n            rank = np.arange(1, num_selected + 1)\n            weights_selected = np.exp(-rank / num_selected)\n            weights_selected /= np.sum(weights_selected)  # Normalize\n\n            # Update population using DE and Slime Mold concepts\n            for i in range(self.population_size):\n                if self.budget <= 0:\n                    break\n\n                # Adaptive DE parameters\n                F_i = np.clip(np.random.normal(self.F, 0.1), 0.1, 1.0)\n                CR_i = np.clip(np.random.normal(self.CR, 0.1), 0.1, 1.0)\n\n\n                # Mutation (DE part)\n                idxs = np.random.choice(self.population_size, 3, replace=False)\n                x_1, x_2, x_3 = population[idxs]\n                mutant = population[i] + F_i * (x_2 - x_3)\n                mutant = np.clip(mutant, self.lb, self.ub)\n\n\n                # Crossover (DE part)\n                cross_points = np.random.rand(self.dim) < CR_i\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial_vector_DE = np.where(cross_points, mutant, population[i])\n\n\n                # Slime Mold inspired perturbation: Weighted average of selected solutions\n                x_sm = np.zeros(self.dim)\n                for j in range(num_selected):\n                    x_sm += weights_selected[j] * selected_population[j]\n\n                # Combine DE and Slime Mold: \n                # Option 1: Average the trial vector from DE with Slime Mold perturbation\n                trial_vector = 0.5 * trial_vector_DE + 0.5 * x_sm\n\n                # Option 2: Apply Slime Mold perturbation to DE's trial vector\n                # trial_vector = trial_vector_DE + 0.1 * (x_sm - trial_vector_DE)\n\n\n                trial_vector = np.clip(trial_vector, self.lb, self.ub)\n\n\n\n                f_trial = func(trial_vector)\n                self.budget -= 1\n\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    population[i] = trial_vector.copy()\n\n                    #Update Archive\n                    self.archive.append(population[i].copy())\n                    if len(self.archive) > self.archive_factor * self.population_size:\n                        self.archive.pop(0)\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial_vector.copy()\n                        \n        return self.f_opt, self.x_opt\n```", "aucs": [0.12640045917019438, 0.21790149408289894, 0.3190487090069891, 0.18908394994290445, 0.2192106215853199, 0.19597798231305774, 0.205074613578639, 0.305846543271145, 0.20380150451137236, 0.1844096009376699, 0.20583526777188021, 0.38217745764321187, 0.23515249009546768, 0.17501679102103007, 0.1349785676435693, 0.29702064241058956, 0.19710510827265915, 0.1870536756957457, 0.2067945928396392, 0.4521487580527104], "operator": "crossover", "theta_sampled": 0.06435171866475377, "reward": 0.0, "generation": 1}, "task_prompt": ""}
{"id": "f1ae6e3d-e6de-4c6a-b84f-146f0acf3ebc", "fitness": 0.0, "name": "HybridOptimizer", "description": "Hybrid optimization leveraging Slime Mold's weighted averaging with Adaptive DE's mutation and crossover, and periodic restarts.", "code": "import numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget=10000, dim=10, population_size=20, w_decay=0.9, p_selection=0.2, F=0.5, CR=0.7, restart_factor=0.1):\n        \"\"\"\n        Hybrid optimization algorithm combining Slime Mold and Adaptive Differential Evolution.\n\n        Args:\n            budget (int): The maximum number of function evaluations.\n            dim (int): The dimensionality of the search space.\n            population_size (int): The number of individuals in the population.\n            w_decay (float): Decay rate for the weight parameter in Slime Mold component.\n            p_selection (float): Percentage of top solutions to consider for weighted averaging (Slime Mold).\n            F (float): Initial scaling factor for DE mutation.\n            CR (float): Initial crossover rate for DE.\n            restart_factor (float): Fraction of budget to consume before restarting.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.population_size = population_size\n        self.w_decay = w_decay\n        self.p_selection = p_selection\n        self.F = F\n        self.CR = CR\n        self.restart_factor = restart_factor\n        self.restart_iterations = int(restart_factor * budget)\n        self.lb = -5.0\n        self.ub = 5.0\n        self.eval_count = 0\n        self.archive_factor = 2.0\n\n    def __call__(self, func):\n        \"\"\"\n        Optimizes the given function using the hybrid algorithm.\n\n        Args:\n            func (callable): The black-box function to optimize.\n\n        Returns:\n            tuple: The best function value found and the corresponding solution.\n        \"\"\"\n        population = np.random.uniform(self.lb, self.ub, size=(self.population_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.eval_count += self.population_size\n        \n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index].copy()\n\n        weights = np.ones(self.population_size) # Initialize weights\n\n        self.archive = []\n\n        while self.eval_count < self.budget:\n            # Sort population based on fitness\n            sorted_indices = np.argsort(fitness)\n            population = population[sorted_indices]\n            fitness = fitness[sorted_indices]\n            \n            # Slime Mold Component: Weighted Averaging\n            num_selected = int(self.p_selection * self.population_size)\n            selected_population = population[:num_selected]\n            selected_fitness = fitness[:num_selected]\n                \n            rank = np.arange(1, num_selected + 1)\n            weights_selected = np.exp(-rank / num_selected)\n            weights_selected /= np.sum(weights_selected)\n\n            new_population = population.copy()\n            new_fitness = fitness.copy()\n            for i in range(self.population_size):\n                if self.eval_count >= self.budget:\n                    break\n                \n                # Combination strategy:  DE mutation/crossover OR Slime Mold weighted average\n                if np.random.rand() < 0.5: # Adaptive probability?\n\n                    # Adaptive DE component\n                    F_i = np.clip(np.random.normal(self.F, 0.1), 0.1, 1.0)\n                    CR_i = np.clip(np.random.normal(self.CR, 0.1), 0.1, 1.0)\n\n                    idxs = np.random.choice(self.population_size, 3, replace=False)\n                    x_1, x_2, x_3 = population[idxs]\n                    mutant = population[i] + F_i * (x_2 - x_3)\n                    mutant = np.clip(mutant, self.lb, self.ub)\n                    \n                    cross_points = np.random.rand(self.dim) < CR_i\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n                    trial_vector = np.where(cross_points, mutant, population[i])\n\n                    f_trial = func(trial_vector)\n                    self.eval_count += 1\n\n                    if f_trial < fitness[i]:\n                        new_fitness[i] = f_trial\n                        new_population[i] = trial_vector.copy()\n\n                        self.archive.append(new_population[i].copy())\n                        if len(self.archive) > self.archive_factor * self.population_size:\n                            self.archive.pop(0)\n\n                        if f_trial < self.f_opt:\n                            self.f_opt = f_trial\n                            self.x_opt = trial_vector.copy()\n\n                else: # Slime Mold Part\n                    x = np.zeros(self.dim)\n                    for j in range(num_selected):\n                        x += weights_selected[j] * selected_population[j]\n\n                    x += 0.1 * np.random.randn(self.dim)\n                    x = np.clip(x, self.lb, self.ub)\n\n                    f = func(x)\n                    self.eval_count += 1\n                    if f < fitness[i]:\n                        new_fitness[i] = f\n                        new_population[i] = x.copy()\n\n                        if f < self.f_opt:\n                            self.f_opt = f\n                            self.x_opt = x.copy()\n            \n            population = new_population.copy()\n            fitness = new_fitness.copy()\n\n\n            # Restart Mechanism\n            if self.eval_count % self.restart_iterations == 0:\n                best_idx = np.argmin(fitness)\n                best_individual = population[best_idx].copy()\n\n                population = np.random.uniform(self.lb, self.ub, (self.population_size, self.dim))\n                population[0] = best_individual # Keep the best\n                fitness = np.array([func(x) for x in population])\n                self.eval_count += self.population_size -1\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "The algorithm HybridOptimizer scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["2d086322-f43f-4b37-bed4-2235e826723e", "49b01a83-152a-4b37-86ca-d659431fcf99"], "operator": "crossover", "metadata": {"llm_response": "# Description: Hybrid optimization leveraging Slime Mold's weighted averaging with Adaptive DE's mutation and crossover, and periodic restarts.\n# Code: \n```python\nimport numpy as np\n\nclass HybridOptimizer:\n    def __init__(self, budget=10000, dim=10, population_size=20, w_decay=0.9, p_selection=0.2, F=0.5, CR=0.7, restart_factor=0.1):\n        \"\"\"\n        Hybrid optimization algorithm combining Slime Mold and Adaptive Differential Evolution.\n\n        Args:\n            budget (int): The maximum number of function evaluations.\n            dim (int): The dimensionality of the search space.\n            population_size (int): The number of individuals in the population.\n            w_decay (float): Decay rate for the weight parameter in Slime Mold component.\n            p_selection (float): Percentage of top solutions to consider for weighted averaging (Slime Mold).\n            F (float): Initial scaling factor for DE mutation.\n            CR (float): Initial crossover rate for DE.\n            restart_factor (float): Fraction of budget to consume before restarting.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.population_size = population_size\n        self.w_decay = w_decay\n        self.p_selection = p_selection\n        self.F = F\n        self.CR = CR\n        self.restart_factor = restart_factor\n        self.restart_iterations = int(restart_factor * budget)\n        self.lb = -5.0\n        self.ub = 5.0\n        self.eval_count = 0\n        self.archive_factor = 2.0\n\n    def __call__(self, func):\n        \"\"\"\n        Optimizes the given function using the hybrid algorithm.\n\n        Args:\n            func (callable): The black-box function to optimize.\n\n        Returns:\n            tuple: The best function value found and the corresponding solution.\n        \"\"\"\n        population = np.random.uniform(self.lb, self.ub, size=(self.population_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.eval_count += self.population_size\n        \n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index].copy()\n\n        weights = np.ones(self.population_size) # Initialize weights\n\n        self.archive = []\n\n        while self.eval_count < self.budget:\n            # Sort population based on fitness\n            sorted_indices = np.argsort(fitness)\n            population = population[sorted_indices]\n            fitness = fitness[sorted_indices]\n            \n            # Slime Mold Component: Weighted Averaging\n            num_selected = int(self.p_selection * self.population_size)\n            selected_population = population[:num_selected]\n            selected_fitness = fitness[:num_selected]\n                \n            rank = np.arange(1, num_selected + 1)\n            weights_selected = np.exp(-rank / num_selected)\n            weights_selected /= np.sum(weights_selected)\n\n            new_population = population.copy()\n            new_fitness = fitness.copy()\n            for i in range(self.population_size):\n                if self.eval_count >= self.budget:\n                    break\n                \n                # Combination strategy:  DE mutation/crossover OR Slime Mold weighted average\n                if np.random.rand() < 0.5: # Adaptive probability?\n\n                    # Adaptive DE component\n                    F_i = np.clip(np.random.normal(self.F, 0.1), 0.1, 1.0)\n                    CR_i = np.clip(np.random.normal(self.CR, 0.1), 0.1, 1.0)\n\n                    idxs = np.random.choice(self.population_size, 3, replace=False)\n                    x_1, x_2, x_3 = population[idxs]\n                    mutant = population[i] + F_i * (x_2 - x_3)\n                    mutant = np.clip(mutant, self.lb, self.ub)\n                    \n                    cross_points = np.random.rand(self.dim) < CR_i\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n                    trial_vector = np.where(cross_points, mutant, population[i])\n\n                    f_trial = func(trial_vector)\n                    self.eval_count += 1\n\n                    if f_trial < fitness[i]:\n                        new_fitness[i] = f_trial\n                        new_population[i] = trial_vector.copy()\n\n                        self.archive.append(new_population[i].copy())\n                        if len(self.archive) > self.archive_factor * self.population_size:\n                            self.archive.pop(0)\n\n                        if f_trial < self.f_opt:\n                            self.f_opt = f_trial\n                            self.x_opt = trial_vector.copy()\n\n                else: # Slime Mold Part\n                    x = np.zeros(self.dim)\n                    for j in range(num_selected):\n                        x += weights_selected[j] * selected_population[j]\n\n                    x += 0.1 * np.random.randn(self.dim)\n                    x = np.clip(x, self.lb, self.ub)\n\n                    f = func(x)\n                    self.eval_count += 1\n                    if f < fitness[i]:\n                        new_fitness[i] = f\n                        new_population[i] = x.copy()\n\n                        if f < self.f_opt:\n                            self.f_opt = f\n                            self.x_opt = x.copy()\n            \n            population = new_population.copy()\n            fitness = new_fitness.copy()\n\n\n            # Restart Mechanism\n            if self.eval_count % self.restart_iterations == 0:\n                best_idx = np.argmin(fitness)\n                best_individual = population[best_idx].copy()\n\n                population = np.random.uniform(self.lb, self.ub, (self.population_size, self.dim))\n                population[0] = best_individual # Keep the best\n                fitness = np.array([func(x) for x in population])\n                self.eval_count += self.population_size -1\n\n        return self.f_opt, self.x_opt\n```", "aucs": [0], "operator": "crossover", "theta_sampled": 0.015999854319878812, "reward": 0.0, "generation": 1}, "task_prompt": ""}
{"id": "dfb1bdf8-a198-4300-9a6c-c733e3e3973c", "fitness": 0.0, "name": "AdaptiveCMA_DE", "description": "An algorithm that combines aspects of Covariance Matrix Adaptation Evolution Strategy (CMA-ES) and differential evolution (DE), updating population and covariance matrix adaptively.", "code": "import numpy as np\n\nclass AdaptiveCMA_DE:\n    def __init__(self, budget=10000, dim=10, pop_size=None):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size if pop_size is not None else 4 + int(3 * np.log(self.dim))\n        self.mu = self.pop_size // 2  # Number of parents\n        self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n        self.weights /= np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        \n        self.cs = (self.mueff + 2) / (self.dim + self.mueff + 5)\n        self.damps = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1) + self.cs\n        self.cc = (4 + self.mueff / self.dim) / (self.dim + 4 + 2 * self.mueff / self.dim)\n        self.c1 = 2 / ((self.dim + 1.3)**2 + self.mueff)\n        self.cmu = min(1 - self.c1, 2 * (self.mueff - 2 + 1 / self.mueff) / ((self.dim + 2)**2 + self.mueff))\n        self.cmu *= (1.5 + self.cmu) \n        self.chiN = self.dim**0.5 * (1 - 1/(4*self.dim) + 1/(21*self.dim**2))  # Expectation of ||N(0,I)||\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        \n        # Initialization\n        mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        sigma = 0.5 # Initial step size\n        C = np.eye(self.dim)  # Covariance matrix\n        pc = np.zeros(self.dim)  # Evolution path for covariance matrix\n        ps = np.zeros(self.dim)  # Evolution path for step size\n        B = np.eye(self.dim)  # Eigenvectors of C\n        D = np.ones(self.dim)  # Eigenvalues of C\n\n        eval_count = 0\n        \n        while eval_count < self.budget:\n            # Generate and evaluate offspring\n            z = np.random.normal(0, 1, size=(self.pop_size, self.dim))\n            x = mean + sigma * (B @ (D * z.T)).T\n            \n            # Clip to bounds\n            x = np.clip(x, func.bounds.lb, func.bounds.ub)\n            \n            f = np.array([func(xi) for xi in x])\n            eval_count += self.pop_size\n            \n            # Sort by fitness\n            idx = np.argsort(f)\n            x = x[idx]\n            f = f[idx]\n\n            # Update optimal solution\n            if f[0] < self.f_opt:\n                self.f_opt = f[0]\n                self.x_opt = x[0]\n            \n            # Recombination\n            xmean = np.sum(self.weights[:, None] * x[:self.mu], axis=0)\n            \n            # Cumulation\n            y = (xmean - mean) / sigma\n            ps = (1 - self.cs) * ps + np.sqrt(self.cs * (2 - self.cs) * self.mueff) * (B @ (D**-1 * (xmean - mean).T / sigma))\n            hsig = (np.linalg.norm(ps)**2 / (1 - (1 - self.cs)**(2 * eval_count / self.pop_size)) / self.dim) < (2 + 4/(self.dim+1))\n            pc = (1 - self.cc) * pc + hsig * np.sqrt(self.cc * (2 - self.cc) * self.mueff) * y\n            \n            # Update covariance matrix\n            C = (1 - self.c1 - self.cmu) * C + self.c1 * (pc[:, None] @ pc[None, :]) + self.cmu * np.sum(self.weights[:, None, None] * ( (x[:self.mu] - mean)[:, :, None] @ (x[:self.mu] - mean)[:, None, :] / sigma**2 ), axis=0)\n            \n            # Adapt step size\n            sigma *= np.exp((self.cs / self.damps) * (np.linalg.norm(ps) / self.chiN - 1))\n            \n            # Update mean\n            mean = xmean\n\n            #Eigensystem update (only if necessary)\n            if eval_count > self.budget or eval_count % (self.dim // 2 + 1) == 0:\n                C = np.triu(C) + np.triu(C, 1).T # enforce symmetry\n                D, B = np.linalg.eigh(C)\n                D = np.sqrt(D)\n                \n            # Differential Evolution component: Perturb top solutions\n            for i in range(min(self.pop_size, 5)):  # perturb the best 5\n                if eval_count + 1 > self.budget:\n                    break\n\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                diff_vector = x[idxs[1]] - x[idxs[2]]\n                mutated_x = x[i] + 0.5 * diff_vector  # Simple DE mutation\n\n                mutated_x = np.clip(mutated_x, func.bounds.lb, func.bounds.ub)\n                mutated_f = func(mutated_x)\n                eval_count +=1\n\n                if mutated_f < f[i]:\n                    x[i] = mutated_x\n                    f[i] = mutated_f\n\n                    if f[i] < self.f_opt:\n                        self.f_opt = f[i]\n                        self.x_opt = x[i]\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "", "error": "Evaluation error: operands could not be broadcast together with shapes (2,) (2,6) ", "parent_ids": [], "operator": "random_new", "metadata": {"llm_response": "# Description: An algorithm that combines aspects of Covariance Matrix Adaptation Evolution Strategy (CMA-ES) and differential evolution (DE), updating population and covariance matrix adaptively.\n# Code:\n```python\nimport numpy as np\n\nclass AdaptiveCMA_DE:\n    def __init__(self, budget=10000, dim=10, pop_size=None):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size if pop_size is not None else 4 + int(3 * np.log(self.dim))\n        self.mu = self.pop_size // 2  # Number of parents\n        self.weights = np.log(self.mu + 0.5) - np.log(np.arange(1, self.mu + 1))\n        self.weights /= np.sum(self.weights)\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        \n        self.cs = (self.mueff + 2) / (self.dim + self.mueff + 5)\n        self.damps = 1 + 2 * max(0, np.sqrt((self.mueff - 1) / (self.dim + 1)) - 1) + self.cs\n        self.cc = (4 + self.mueff / self.dim) / (self.dim + 4 + 2 * self.mueff / self.dim)\n        self.c1 = 2 / ((self.dim + 1.3)**2 + self.mueff)\n        self.cmu = min(1 - self.c1, 2 * (self.mueff - 2 + 1 / self.mueff) / ((self.dim + 2)**2 + self.mueff))\n        self.cmu *= (1.5 + self.cmu) \n        self.chiN = self.dim**0.5 * (1 - 1/(4*self.dim) + 1/(21*self.dim**2))  # Expectation of ||N(0,I)||\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        \n        # Initialization\n        mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        sigma = 0.5 # Initial step size\n        C = np.eye(self.dim)  # Covariance matrix\n        pc = np.zeros(self.dim)  # Evolution path for covariance matrix\n        ps = np.zeros(self.dim)  # Evolution path for step size\n        B = np.eye(self.dim)  # Eigenvectors of C\n        D = np.ones(self.dim)  # Eigenvalues of C\n\n        eval_count = 0\n        \n        while eval_count < self.budget:\n            # Generate and evaluate offspring\n            z = np.random.normal(0, 1, size=(self.pop_size, self.dim))\n            x = mean + sigma * (B @ (D * z.T)).T\n            \n            # Clip to bounds\n            x = np.clip(x, func.bounds.lb, func.bounds.ub)\n            \n            f = np.array([func(xi) for xi in x])\n            eval_count += self.pop_size\n            \n            # Sort by fitness\n            idx = np.argsort(f)\n            x = x[idx]\n            f = f[idx]\n\n            # Update optimal solution\n            if f[0] < self.f_opt:\n                self.f_opt = f[0]\n                self.x_opt = x[0]\n            \n            # Recombination\n            xmean = np.sum(self.weights[:, None] * x[:self.mu], axis=0)\n            \n            # Cumulation\n            y = (xmean - mean) / sigma\n            ps = (1 - self.cs) * ps + np.sqrt(self.cs * (2 - self.cs) * self.mueff) * (B @ (D**-1 * (xmean - mean).T / sigma))\n            hsig = (np.linalg.norm(ps)**2 / (1 - (1 - self.cs)**(2 * eval_count / self.pop_size)) / self.dim) < (2 + 4/(self.dim+1))\n            pc = (1 - self.cc) * pc + hsig * np.sqrt(self.cc * (2 - self.cc) * self.mueff) * y\n            \n            # Update covariance matrix\n            C = (1 - self.c1 - self.cmu) * C + self.c1 * (pc[:, None] @ pc[None, :]) + self.cmu * np.sum(self.weights[:, None, None] * ( (x[:self.mu] - mean)[:, :, None] @ (x[:self.mu] - mean)[:, None, :] / sigma**2 ), axis=0)\n            \n            # Adapt step size\n            sigma *= np.exp((self.cs / self.damps) * (np.linalg.norm(ps) / self.chiN - 1))\n            \n            # Update mean\n            mean = xmean\n\n            #Eigensystem update (only if necessary)\n            if eval_count > self.budget or eval_count % (self.dim // 2 + 1) == 0:\n                C = np.triu(C) + np.triu(C, 1).T # enforce symmetry\n                D, B = np.linalg.eigh(C)\n                D = np.sqrt(D)\n                \n            # Differential Evolution component: Perturb top solutions\n            for i in range(min(self.pop_size, 5)):  # perturb the best 5\n                if eval_count + 1 > self.budget:\n                    break\n\n                idxs = np.random.choice(self.pop_size, 3, replace=False)\n                diff_vector = x[idxs[1]] - x[idxs[2]]\n                mutated_x = x[i] + 0.5 * diff_vector  # Simple DE mutation\n\n                mutated_x = np.clip(mutated_x, func.bounds.lb, func.bounds.ub)\n                mutated_f = func(mutated_x)\n                eval_count +=1\n\n                if mutated_f < f[i]:\n                    x[i] = mutated_x\n                    f[i] = mutated_f\n\n                    if f[i] < self.f_opt:\n                        self.f_opt = f[i]\n                        self.x_opt = x[i]\n\n        return self.f_opt, self.x_opt\n```", "operator": "random_new", "theta_sampled": -0.058112339692656306, "reward": 0.0, "generation": 1}, "task_prompt": ""}
{"id": "f2c5ca87-d3a3-4aa8-ab6c-01b1c299941c", "fitness": 0.2826214925050911, "name": "HybridPSODE", "description": "Hybrid algorithm combining aspects of particle swarm optimization (PSO) with differential evolution (DE), adaptively switching between exploration and exploitation phases based on population diversity.", "code": "import numpy as np\n\nclass HybridPSODE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, w=0.7, c1=1.5, c2=1.5, cr=0.7, f=0.8):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.w = w  # Inertia weight for PSO\n        self.c1 = c1 # Cognitive coefficient for PSO\n        self.c2 = c2 # Social coefficient for PSO\n        self.cr = cr # Crossover rate for DE\n        self.f = f   # Mutation factor for DE\n        self.pop = None\n        self.fitness = None\n        self.pbest_pos = None\n        self.pbest_fitness = None\n        self.gbest_pos = None\n        self.gbest_fitness = np.inf\n        self.eval_count = 0\n\n\n    def initialize_population(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        self.pop = np.random.uniform(lb, ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.eval_count += self.pop_size\n        self.pbest_pos = self.pop.copy()\n        self.pbest_fitness = self.fitness.copy()\n        self.gbest_pos = self.pop[np.argmin(self.fitness)].copy()\n        self.gbest_fitness = np.min(self.fitness)\n\n\n    def pso_step(self, func):\n        velocity = np.zeros_like(self.pop)\n        for i in range(self.pop_size):\n            r1 = np.random.rand(self.dim)\n            r2 = np.random.rand(self.dim)\n            velocity[i] = (self.w * velocity[i]\n                           + self.c1 * r1 * (self.pbest_pos[i] - self.pop[i])\n                           + self.c2 * r2 * (self.gbest_pos - self.pop[i]))\n            self.pop[i] += velocity[i]\n            self.pop[i] = np.clip(self.pop[i], func.bounds.lb, func.bounds.ub)\n\n            f = func(self.pop[i])\n            self.eval_count += 1\n\n            if f < self.fitness[i]:\n                self.fitness[i] = f\n\n                if f < self.pbest_fitness[i]:\n                    self.pbest_fitness[i] = f\n                    self.pbest_pos[i] = self.pop[i].copy()\n\n                    if f < self.gbest_fitness:\n                        self.gbest_fitness = f\n                        self.gbest_pos = self.pop[i].copy()\n\n\n\n    def de_step(self, func):\n        for i in range(self.pop_size):\n            idxs = np.random.choice(self.pop_size, 3, replace=False)\n            x_1, x_2, x_3 = self.pop[idxs]\n            v = x_1 + self.f * (x_2 - x_3)\n            v = np.clip(v, func.bounds.lb, func.bounds.ub)\n            \n            j_rand = np.random.randint(self.dim)\n            u = self.pop[i].copy()\n            for j in range(self.dim):\n                if np.random.rand() < self.cr or j == j_rand:\n                    u[j] = v[j]\n                    \n            f = func(u)\n            self.eval_count += 1\n            \n            if f < self.fitness[i]:\n                self.fitness[i] = f\n\n                if f < self.pbest_fitness[i]:\n                    self.pbest_fitness[i] = f\n                    self.pbest_pos[i] = u.copy()\n\n                    if f < self.gbest_fitness:\n                        self.gbest_fitness = f\n                        self.gbest_pos = u.copy()\n\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        \n        while self.eval_count < self.budget:\n            # Adaptive strategy: Switch between PSO and DE based on population diversity\n            if np.std(self.fitness) > 0.1 * np.abs(self.gbest_fitness):  #High Diversity -> Exploration (DE)\n                self.de_step(func)\n            else:  #Low Diversity -> Exploitation (PSO)\n                self.pso_step(func)\n\n        return self.gbest_fitness, self.gbest_pos", "configspace": "", "generation": 1, "feedback": "The algorithm HybridPSODE scored 0.283 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": "random_new", "metadata": {"llm_response": "# Description: Hybrid algorithm combining aspects of particle swarm optimization (PSO) with differential evolution (DE), adaptively switching between exploration and exploitation phases based on population diversity.\n# Code: \n```python\nimport numpy as np\n\nclass HybridPSODE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, w=0.7, c1=1.5, c2=1.5, cr=0.7, f=0.8):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.w = w  # Inertia weight for PSO\n        self.c1 = c1 # Cognitive coefficient for PSO\n        self.c2 = c2 # Social coefficient for PSO\n        self.cr = cr # Crossover rate for DE\n        self.f = f   # Mutation factor for DE\n        self.pop = None\n        self.fitness = None\n        self.pbest_pos = None\n        self.pbest_fitness = None\n        self.gbest_pos = None\n        self.gbest_fitness = np.inf\n        self.eval_count = 0\n\n\n    def initialize_population(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        self.pop = np.random.uniform(lb, ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.eval_count += self.pop_size\n        self.pbest_pos = self.pop.copy()\n        self.pbest_fitness = self.fitness.copy()\n        self.gbest_pos = self.pop[np.argmin(self.fitness)].copy()\n        self.gbest_fitness = np.min(self.fitness)\n\n\n    def pso_step(self, func):\n        velocity = np.zeros_like(self.pop)\n        for i in range(self.pop_size):\n            r1 = np.random.rand(self.dim)\n            r2 = np.random.rand(self.dim)\n            velocity[i] = (self.w * velocity[i]\n                           + self.c1 * r1 * (self.pbest_pos[i] - self.pop[i])\n                           + self.c2 * r2 * (self.gbest_pos - self.pop[i]))\n            self.pop[i] += velocity[i]\n            self.pop[i] = np.clip(self.pop[i], func.bounds.lb, func.bounds.ub)\n\n            f = func(self.pop[i])\n            self.eval_count += 1\n\n            if f < self.fitness[i]:\n                self.fitness[i] = f\n\n                if f < self.pbest_fitness[i]:\n                    self.pbest_fitness[i] = f\n                    self.pbest_pos[i] = self.pop[i].copy()\n\n                    if f < self.gbest_fitness:\n                        self.gbest_fitness = f\n                        self.gbest_pos = self.pop[i].copy()\n\n\n\n    def de_step(self, func):\n        for i in range(self.pop_size):\n            idxs = np.random.choice(self.pop_size, 3, replace=False)\n            x_1, x_2, x_3 = self.pop[idxs]\n            v = x_1 + self.f * (x_2 - x_3)\n            v = np.clip(v, func.bounds.lb, func.bounds.ub)\n            \n            j_rand = np.random.randint(self.dim)\n            u = self.pop[i].copy()\n            for j in range(self.dim):\n                if np.random.rand() < self.cr or j == j_rand:\n                    u[j] = v[j]\n                    \n            f = func(u)\n            self.eval_count += 1\n            \n            if f < self.fitness[i]:\n                self.fitness[i] = f\n\n                if f < self.pbest_fitness[i]:\n                    self.pbest_fitness[i] = f\n                    self.pbest_pos[i] = u.copy()\n\n                    if f < self.gbest_fitness:\n                        self.gbest_fitness = f\n                        self.gbest_pos = u.copy()\n\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        \n        while self.eval_count < self.budget:\n            # Adaptive strategy: Switch between PSO and DE based on population diversity\n            if np.std(self.fitness) > 0.1 * np.abs(self.gbest_fitness):  #High Diversity -> Exploration (DE)\n                self.de_step(func)\n            else:  #Low Diversity -> Exploitation (PSO)\n                self.pso_step(func)\n\n        return self.gbest_fitness, self.gbest_pos\n```", "aucs": [0.09585273421864526, 0.17707798014221132, 0.2653648794305632, 0.20443900403705706, 0.22580381465755495, 0.2244741526079168, 0.24040697532501043, 0.20611552487327722, 0.18990482598635905, 0.15753991655275534, 0.21011191523711226, 0.9983548172003237, 0.2624485654497043, 0.21590209621705658, 0.5809199468649114, 0.28754673359841754, 0.2496244269927025, 0.24269852927562086, 0.1683956595831878, 0.449447351851436], "operator": "random_new", "theta_sampled": 0.3135404914220854, "reward": 0.0, "generation": 1}, "task_prompt": ""}
{"id": "6c537388-4ccd-49e7-95d1-ae15369d9854", "fitness": 0.5314237833843445, "name": "AdaptiveMutationOptimizer", "description": "Adaptive Mutation Optimizer with elitism, rank-based selection for mutation, and dynamic population size adjustment based on stagnation detection.", "code": "import numpy as np\n\nclass AdaptiveMutationOptimizer:\n    def __init__(self, budget=10000, dim=10, pop_size=50, stagnation_threshold=1000):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lb = -5.0\n        self.ub = 5.0\n        self.mutation_prob_gaussian = 0.5\n        self.mutation_prob_cauchy = 0.5\n        self.mutation_scale = 1.0\n        self.stagnation_threshold = stagnation_threshold\n        self.stagnation_counter = 0\n        self.best_fitness_history = []\n\n    def initialize_population(self):\n        return np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n\n    def calculate_diversity(self, population):\n        centroid = np.mean(population, axis=0)\n        distances = np.linalg.norm(population - centroid, axis=1)\n        return np.mean(distances)\n\n    def mutate_gaussian(self, x):\n        return x + self.mutation_scale * np.random.normal(0, 1, size=self.dim)\n\n    def mutate_cauchy(self, x):\n        return x + self.mutation_scale * np.random.standard_cauchy(size=self.dim)\n\n    def correct_bounds(self, x):\n        return np.clip(x, self.lb, self.ub)\n\n    def rank_based_selection(self, population, fitness, num_parents):\n        ranked_indices = np.argsort(fitness)\n        parent_indices = ranked_indices[:num_parents]\n        return population[parent_indices]\n    \n    def adjust_population_size(self):\n        if self.stagnation_counter > self.stagnation_threshold:\n            self.pop_size = int(self.pop_size * 0.9)  # Reduce population size if stagnating\n            if self.pop_size < 10:\n                self.pop_size = 10 # Minimum population size\n\n            self.mutation_scale = 1.0 # Reset mutation scale\n            self.stagnation_counter = 0 # Reset stagnation counter\n            print(f\"Stagnation detected, reducing pop size to {self.pop_size}\")\n\n\n    def __call__(self, func):\n        population = self.initialize_population()\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n        \n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index]\n        self.best_fitness_history.append(self.f_opt)\n\n        while self.budget > 0:\n            diversity = self.calculate_diversity(population)\n\n            if diversity > 1.0:\n                self.mutation_prob_gaussian = 0.8\n                self.mutation_prob_cauchy = 0.2\n            else:\n                self.mutation_prob_gaussian = 0.2\n                self.mutation_prob_cauchy = 0.8\n\n            # Elitism: Keep the best individual\n            best_individual = np.copy(self.x_opt)\n            best_fitness = self.f_opt\n\n            # Rank-based selection of parents for mutation\n            num_parents = int(self.pop_size / 2)  # Select top half as parents\n            parents = self.rank_based_selection(population, fitness, num_parents)\n            \n            new_population = np.copy(population)\n            for i in range(self.pop_size):\n                # Choose a parent randomly\n                parent = parents[np.random.randint(0, num_parents)]\n                \n                if np.random.rand() < self.mutation_prob_gaussian:\n                    new_population[i] = self.mutate_gaussian(parent)\n                else:\n                    new_population[i] = self.mutate_cauchy(parent)\n\n                new_population[i] = self.correct_bounds(new_population[i])\n                \n                new_fitness = func(new_population[i])\n                self.budget -= 1\n\n                if new_fitness < fitness[i]:\n                    fitness[i] = new_fitness\n                    population[i] = new_population[i]\n\n                    if new_fitness < self.f_opt:\n                        self.f_opt = new_fitness\n                        self.x_opt = new_population[i]\n            \n            # Replace worst individual with the best from previous generation (Elitism)\n            worst_index = np.argmax(fitness)\n            population[worst_index] = best_individual\n            fitness[worst_index] = best_fitness\n\n            # Adapt mutation scale\n            self.mutation_scale = 0.95 * self.mutation_scale\n\n            # Stagnation detection and population size adjustment\n            if len(self.best_fitness_history) > self.stagnation_threshold:\n                if abs(self.f_opt - self.best_fitness_history[-self.stagnation_threshold]) < 1e-5:\n                    self.stagnation_counter += self.pop_size\n                else:\n                    self.stagnation_counter = 0\n\n            self.best_fitness_history.append(self.f_opt)\n            self.adjust_population_size()\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "The algorithm AdaptiveMutationOptimizer scored 0.531 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["e28dc9c3-3d4b-4868-a051-3c9dc94d420f"], "operator": "mutation", "metadata": {"llm_response": "# Description: Adaptive Mutation Optimizer with elitism, rank-based selection for mutation, and dynamic population size adjustment based on stagnation detection.\n# Code: \n```python\nimport numpy as np\n\nclass AdaptiveMutationOptimizer:\n    def __init__(self, budget=10000, dim=10, pop_size=50, stagnation_threshold=1000):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lb = -5.0\n        self.ub = 5.0\n        self.mutation_prob_gaussian = 0.5\n        self.mutation_prob_cauchy = 0.5\n        self.mutation_scale = 1.0\n        self.stagnation_threshold = stagnation_threshold\n        self.stagnation_counter = 0\n        self.best_fitness_history = []\n\n    def initialize_population(self):\n        return np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n\n    def calculate_diversity(self, population):\n        centroid = np.mean(population, axis=0)\n        distances = np.linalg.norm(population - centroid, axis=1)\n        return np.mean(distances)\n\n    def mutate_gaussian(self, x):\n        return x + self.mutation_scale * np.random.normal(0, 1, size=self.dim)\n\n    def mutate_cauchy(self, x):\n        return x + self.mutation_scale * np.random.standard_cauchy(size=self.dim)\n\n    def correct_bounds(self, x):\n        return np.clip(x, self.lb, self.ub)\n\n    def rank_based_selection(self, population, fitness, num_parents):\n        ranked_indices = np.argsort(fitness)\n        parent_indices = ranked_indices[:num_parents]\n        return population[parent_indices]\n    \n    def adjust_population_size(self):\n        if self.stagnation_counter > self.stagnation_threshold:\n            self.pop_size = int(self.pop_size * 0.9)  # Reduce population size if stagnating\n            if self.pop_size < 10:\n                self.pop_size = 10 # Minimum population size\n\n            self.mutation_scale = 1.0 # Reset mutation scale\n            self.stagnation_counter = 0 # Reset stagnation counter\n            print(f\"Stagnation detected, reducing pop size to {self.pop_size}\")\n\n\n    def __call__(self, func):\n        population = self.initialize_population()\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n        \n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index]\n        self.best_fitness_history.append(self.f_opt)\n\n        while self.budget > 0:\n            diversity = self.calculate_diversity(population)\n\n            if diversity > 1.0:\n                self.mutation_prob_gaussian = 0.8\n                self.mutation_prob_cauchy = 0.2\n            else:\n                self.mutation_prob_gaussian = 0.2\n                self.mutation_prob_cauchy = 0.8\n\n            # Elitism: Keep the best individual\n            best_individual = np.copy(self.x_opt)\n            best_fitness = self.f_opt\n\n            # Rank-based selection of parents for mutation\n            num_parents = int(self.pop_size / 2)  # Select top half as parents\n            parents = self.rank_based_selection(population, fitness, num_parents)\n            \n            new_population = np.copy(population)\n            for i in range(self.pop_size):\n                # Choose a parent randomly\n                parent = parents[np.random.randint(0, num_parents)]\n                \n                if np.random.rand() < self.mutation_prob_gaussian:\n                    new_population[i] = self.mutate_gaussian(parent)\n                else:\n                    new_population[i] = self.mutate_cauchy(parent)\n\n                new_population[i] = self.correct_bounds(new_population[i])\n                \n                new_fitness = func(new_population[i])\n                self.budget -= 1\n\n                if new_fitness < fitness[i]:\n                    fitness[i] = new_fitness\n                    population[i] = new_population[i]\n\n                    if new_fitness < self.f_opt:\n                        self.f_opt = new_fitness\n                        self.x_opt = new_population[i]\n            \n            # Replace worst individual with the best from previous generation (Elitism)\n            worst_index = np.argmax(fitness)\n            population[worst_index] = best_individual\n            fitness[worst_index] = best_fitness\n\n            # Adapt mutation scale\n            self.mutation_scale = 0.95 * self.mutation_scale\n\n            # Stagnation detection and population size adjustment\n            if len(self.best_fitness_history) > self.stagnation_threshold:\n                if abs(self.f_opt - self.best_fitness_history[-self.stagnation_threshold]) < 1e-5:\n                    self.stagnation_counter += self.pop_size\n                else:\n                    self.stagnation_counter = 0\n\n            self.best_fitness_history.append(self.f_opt)\n            self.adjust_population_size()\n\n\n        return self.f_opt, self.x_opt\n```", "aucs": [0.23680784583434156, 0.22124658055974056, 0.536135819145592, 0.8034429945677245, 0.5525409328385751, 0.5957690259933681, 0.339138727097266, 0.46581372735748805, 0.5446445303446243, 0.4926106253907868, 0.7929366100295929, 0.996333139810889, 0.26172374414393085, 0.5462321935914728, 0.7487330481562425, 0.5917637702134191, 0.40717190053315366, 0.7166839020035152, 0.2747353693461244, 0.5040111807290398], "operator": "mutation", "theta_sampled": -0.07054863488239323, "reward": 0.10815253452677731, "generation": 2}, "task_prompt": ""}
{"id": "56ed699f-9a0a-4575-a57d-d7099e29a214", "fitness": 0.4428506802808075, "name": "HarmonySearch", "description": "The Harmony Search algorithm mimics the improvisation process of musicians, iteratively adjusting solution vectors (harmonies) based on memory consideration, pitch adjustment, and random exploration.", "code": "import numpy as np\n\nclass HarmonySearch:\n    def __init__(self, budget=10000, dim=10, HMCR=0.9, PAR=0.3, BW=0.01):\n        self.budget = budget\n        self.dim = dim\n        self.HMCR = HMCR  # Harmony Memory Consideration Rate\n        self.PAR = PAR  # Pitch Adjustment Rate\n        self.BW = BW  # Bandwidth\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        \n        # Initialize Harmony Memory (HM)\n        HM = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(10, self.dim))  # HM size: 10 harmonies\n        HM_fitness = np.array([func(HM[i]) for i in range(10)])\n        \n        eval_count = 10  # Initialized HM requires 10 evaluations\n\n        while eval_count < self.budget:\n            # Improvise a new harmony\n            new_harmony = np.zeros(self.dim)\n            for i in range(self.dim):\n                if np.random.rand() < self.HMCR:\n                    # Memory Consideration\n                    new_harmony[i] = HM[np.random.randint(0, 10), i]\n                    # Pitch Adjustment\n                    if np.random.rand() < self.PAR:\n                        new_harmony[i] += np.random.uniform(-self.BW, self.BW)\n                        new_harmony[i] = np.clip(new_harmony[i], func.bounds.lb[i], func.bounds.ub[i]) #clip each dimension\n                else:\n                    # Random Exploration\n                    new_harmony[i] = np.random.uniform(func.bounds.lb[i], func.bounds.ub[i])  # Access lb and ub by index\n                    \n            new_fitness = func(new_harmony)\n            eval_count += 1\n\n            # Update Harmony Memory\n            worst_index = np.argmax(HM_fitness)\n            if new_fitness < HM_fitness[worst_index]:\n                HM[worst_index] = new_harmony\n                HM_fitness[worst_index] = new_fitness\n\n            # Update best solution found so far\n            best_index = np.argmin(HM_fitness)\n            if HM_fitness[best_index] < self.f_opt:\n                self.f_opt = HM_fitness[best_index]\n                self.x_opt = HM[best_index]\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "The algorithm HarmonySearch scored 0.443 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": "random_new", "metadata": {"llm_response": "# Description: The Harmony Search algorithm mimics the improvisation process of musicians, iteratively adjusting solution vectors (harmonies) based on memory consideration, pitch adjustment, and random exploration.\n# Code: \n```python\nimport numpy as np\n\nclass HarmonySearch:\n    def __init__(self, budget=10000, dim=10, HMCR=0.9, PAR=0.3, BW=0.01):\n        self.budget = budget\n        self.dim = dim\n        self.HMCR = HMCR  # Harmony Memory Consideration Rate\n        self.PAR = PAR  # Pitch Adjustment Rate\n        self.BW = BW  # Bandwidth\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        \n        # Initialize Harmony Memory (HM)\n        HM = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(10, self.dim))  # HM size: 10 harmonies\n        HM_fitness = np.array([func(HM[i]) for i in range(10)])\n        \n        eval_count = 10  # Initialized HM requires 10 evaluations\n\n        while eval_count < self.budget:\n            # Improvise a new harmony\n            new_harmony = np.zeros(self.dim)\n            for i in range(self.dim):\n                if np.random.rand() < self.HMCR:\n                    # Memory Consideration\n                    new_harmony[i] = HM[np.random.randint(0, 10), i]\n                    # Pitch Adjustment\n                    if np.random.rand() < self.PAR:\n                        new_harmony[i] += np.random.uniform(-self.BW, self.BW)\n                        new_harmony[i] = np.clip(new_harmony[i], func.bounds.lb[i], func.bounds.ub[i]) #clip each dimension\n                else:\n                    # Random Exploration\n                    new_harmony[i] = np.random.uniform(func.bounds.lb[i], func.bounds.ub[i])  # Access lb and ub by index\n                    \n            new_fitness = func(new_harmony)\n            eval_count += 1\n\n            # Update Harmony Memory\n            worst_index = np.argmax(HM_fitness)\n            if new_fitness < HM_fitness[worst_index]:\n                HM[worst_index] = new_harmony\n                HM_fitness[worst_index] = new_fitness\n\n            # Update best solution found so far\n            best_index = np.argmin(HM_fitness)\n            if HM_fitness[best_index] < self.f_opt:\n                self.f_opt = HM_fitness[best_index]\n                self.x_opt = HM[best_index]\n\n        return self.f_opt, self.x_opt\n```", "aucs": [0.1377727965932718, 0.17817412583628933, 0.62894323000459, 0.8944964049774452, 0.28037133763808253, 0.690851860899487, 0.2952782881301721, 0.3087894165097247, 0.24163129058140598, 0.16561130294844606, 0.33186792004782406, 0.995758562034282, 0.22934108593498992, 0.28763095008968875, 0.7842233323826611, 0.7025094916221155, 0.3068790888724737, 0.7359379322292021, 0.16939066990287865, 0.49155451838112063], "operator": "random_new", "theta_sampled": 0.21920671742162595, "reward": 0.019579431423240312, "generation": 2}, "task_prompt": ""}
{"id": "31533d65-bc44-4109-a001-7a477a212d28", "fitness": 0.0, "name": "BioGeographyLevy", "description": "Biogeography-Based Optimization with Lvy flight dispersal and adaptive mutation rates.", "code": "import numpy as np\n\nclass BioGeographyLevy:\n    def __init__(self, budget=10000, dim=10, pop_size=50, mutation_rate=0.01):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.mutation_rate = mutation_rate\n        self.lb = -5.0\n        self.ub = 5.0\n\n    def levy_flight(self, beta=1.5):\n        sigma = (np.math.gamma(1 + beta) * np.sin(np.pi * beta / 2) / (np.math.gamma((1 + beta) / 2) * beta * (2 ** ((beta - 1) / 2)))) ** (1 / beta)\n        u = np.random.randn(self.dim) * sigma\n        v = np.random.randn(self.dim)\n        step = u / abs(v) ** (1 / beta)\n        return step\n\n    def __call__(self, func):\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n        eval_count = self.pop_size\n\n        while eval_count < self.budget:\n            # Migration\n            immigration_rates = (self.fitness - np.min(self.fitness)) / (np.max(self.fitness) - np.min(self.fitness) + 1e-8)  # Avoid division by zero\n            emigration_rates = 1 - immigration_rates\n            \n            new_population = self.population.copy()\n            for i in range(self.pop_size):\n                for j in range(self.dim):\n                    if np.random.rand() < emigration_rates[i]:\n                        immigrant_index = np.random.choice(self.pop_size, p=immigration_rates/np.sum(immigration_rates))\n                        new_population[i, j] = self.population[immigrant_index, j]\n            \n            # Lvy Flight Dispersal\n            for i in range(self.pop_size):\n                 if np.random.rand() < 0.1:  # Probability of dispersal, tuneable\n                    step = self.levy_flight()\n                    new_population[i] = np.clip(self.population[i] + step, self.lb, self.ub)\n\n            # Mutation\n            adaptive_mutation_rate = self.mutation_rate * (1 + np.abs(self.fitness - np.mean(self.fitness)) / (np.std(self.fitness) + 1e-8)) #adapt rate to fitness diversity\n            for i in range(self.pop_size):\n                if np.random.rand() < adaptive_mutation_rate:\n                    mutation = np.random.uniform(-0.1, 0.1, self.dim) #smaller mutation range\n                    new_population[i] = np.clip(new_population[i] + mutation, self.lb, self.ub)\n\n            # Evaluation\n            new_fitness = np.array([func(x) for x in new_population])\n            eval_count += self.pop_size\n            \n            # Selection\n            for i in range(self.pop_size):\n                if new_fitness[i] < self.fitness[i]:\n                    self.population[i] = new_population[i]\n                    self.fitness[i] = new_fitness[i]\n\n            # Update optimal solution\n            if np.min(self.fitness) < self.f_opt:\n                self.f_opt = np.min(self.fitness)\n                self.x_opt = self.population[np.argmin(self.fitness)]\n\n            if eval_count >= self.budget:\n                break\n            \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "", "error": "Evaluation error: module 'numpy' has no attribute 'math'", "parent_ids": [], "operator": "random_new", "metadata": {"llm_response": "# Description: Biogeography-Based Optimization with Lvy flight dispersal and adaptive mutation rates.\n# Code: \n```python\nimport numpy as np\n\nclass BioGeographyLevy:\n    def __init__(self, budget=10000, dim=10, pop_size=50, mutation_rate=0.01):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.mutation_rate = mutation_rate\n        self.lb = -5.0\n        self.ub = 5.0\n\n    def levy_flight(self, beta=1.5):\n        sigma = (np.math.gamma(1 + beta) * np.sin(np.pi * beta / 2) / (np.math.gamma((1 + beta) / 2) * beta * (2 ** ((beta - 1) / 2)))) ** (1 / beta)\n        u = np.random.randn(self.dim) * sigma\n        v = np.random.randn(self.dim)\n        step = u / abs(v) ** (1 / beta)\n        return step\n\n    def __call__(self, func):\n        self.population = np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n        eval_count = self.pop_size\n\n        while eval_count < self.budget:\n            # Migration\n            immigration_rates = (self.fitness - np.min(self.fitness)) / (np.max(self.fitness) - np.min(self.fitness) + 1e-8)  # Avoid division by zero\n            emigration_rates = 1 - immigration_rates\n            \n            new_population = self.population.copy()\n            for i in range(self.pop_size):\n                for j in range(self.dim):\n                    if np.random.rand() < emigration_rates[i]:\n                        immigrant_index = np.random.choice(self.pop_size, p=immigration_rates/np.sum(immigration_rates))\n                        new_population[i, j] = self.population[immigrant_index, j]\n            \n            # Lvy Flight Dispersal\n            for i in range(self.pop_size):\n                 if np.random.rand() < 0.1:  # Probability of dispersal, tuneable\n                    step = self.levy_flight()\n                    new_population[i] = np.clip(self.population[i] + step, self.lb, self.ub)\n\n            # Mutation\n            adaptive_mutation_rate = self.mutation_rate * (1 + np.abs(self.fitness - np.mean(self.fitness)) / (np.std(self.fitness) + 1e-8)) #adapt rate to fitness diversity\n            for i in range(self.pop_size):\n                if np.random.rand() < adaptive_mutation_rate:\n                    mutation = np.random.uniform(-0.1, 0.1, self.dim) #smaller mutation range\n                    new_population[i] = np.clip(new_population[i] + mutation, self.lb, self.ub)\n\n            # Evaluation\n            new_fitness = np.array([func(x) for x in new_population])\n            eval_count += self.pop_size\n            \n            # Selection\n            for i in range(self.pop_size):\n                if new_fitness[i] < self.fitness[i]:\n                    self.population[i] = new_population[i]\n                    self.fitness[i] = new_fitness[i]\n\n            # Update optimal solution\n            if np.min(self.fitness) < self.f_opt:\n                self.f_opt = np.min(self.fitness)\n                self.x_opt = self.population[np.argmin(self.fitness)]\n\n            if eval_count >= self.budget:\n                break\n            \n        return self.f_opt, self.x_opt\n```", "operator": "random_new", "theta_sampled": 0.011979681437685848, "reward": 0.0, "generation": 2}, "task_prompt": ""}
{"id": "7c288fe7-5bf6-4401-b362-114c0928e20b", "fitness": 0.38119142802934464, "name": "AdaptiveSlimeMoldOptimizer", "description": "An improved Slime Mold Optimizer with adaptive parameter control and dynamic population size adjustment for enhanced exploration and exploitation.", "code": "import numpy as np\n\nclass AdaptiveSlimeMoldOptimizer:\n    def __init__(self, budget=10000, dim=10, initial_population_size=20, w_decay=0.9, p_selection=0.2, exploration_rate=0.1):\n        \"\"\"\n        Initializes the Adaptive Slime Mold Optimizer.\n\n        Args:\n            budget (int): The maximum number of function evaluations.\n            dim (int): The dimensionality of the search space.\n            initial_population_size (int): The initial number of slime mold agents.\n            w_decay (float): Decay rate for the weight parameter.\n            p_selection (float): Percentage of top solutions to consider for weighted averaging.\n            exploration_rate (float): Probability of exploration.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.population_size = initial_population_size\n        self.initial_population_size = initial_population_size\n        self.w_decay = w_decay\n        self.p_selection = p_selection  # Percentage for best selection\n        self.lb = -5.0\n        self.ub = 5.0\n        self.exploration_rate = exploration_rate\n        self.adaptive_exploration_rate = exploration_rate\n\n\n    def __call__(self, func):\n        \"\"\"\n        Optimizes the given function using the Adaptive Slime Mold Optimizer.\n\n        Args:\n            func (callable): The black-box function to optimize.\n\n        Returns:\n            tuple: The best function value found and the corresponding solution.\n        \"\"\"\n        population = np.random.uniform(self.lb, self.ub, size=(self.population_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.population_size\n        \n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index].copy()\n\n\n        weights = np.ones(self.population_size) # Initialize weights\n\n        iteration = 0\n        while self.budget > 0:\n            iteration += 1\n            # Sort population based on fitness\n            sorted_indices = np.argsort(fitness)\n            population = population[sorted_indices]\n            fitness = fitness[sorted_indices]\n            \n            # Select top solutions based on p_selection\n            num_selected = int(self.p_selection * self.population_size)\n            selected_population = population[:num_selected]\n            selected_fitness = fitness[:num_selected]\n                \n            # Calculate weights based on fitness rank. Give the better ones more weight\n            rank = np.arange(1, num_selected + 1)\n            weights_selected = np.exp(-rank / num_selected)\n            weights_selected /= np.sum(weights_selected)  # Normalize\n\n            # Update population using weighted average of selected solutions\n            for i in range(self.population_size):\n                if self.budget <= 0:\n                    break\n                \n                # Adaptive Exploration Rate: Increase when stuck, decrease when improving\n                if np.random.rand() < self.adaptive_exploration_rate:\n                    x = np.random.uniform(self.lb, self.ub, size=self.dim)\n\n                else: # Weighted average of selected\n                    x = np.zeros(self.dim)\n                    for j in range(num_selected):\n                        x += weights_selected[j] * selected_population[j]\n\n                    # Add small random perturbation\n                    x += 0.1 * np.random.randn(self.dim)\n                    x = np.clip(x, self.lb, self.ub)\n\n                f = func(x)\n                self.budget -= 1\n\n                if f < fitness[i]:\n                    fitness[i] = f\n                    population[i] = x.copy()\n\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = x.copy()\n                        self.adaptive_exploration_rate = max(0.01, self.adaptive_exploration_rate * 0.95) # Reduce exploration\n\n                else:\n                    self.adaptive_exploration_rate = min(self.exploration_rate, self.adaptive_exploration_rate * 1.05) # increase exploration\n\n            # Dynamic Population Size Adjustment (Optional):\n            if iteration % 10 == 0:  # Adjust every 10 iterations\n                if np.std(fitness) < 1e-6:  # Stagnation detected\n                    if self.population_size < 2 * self.initial_population_size: # Limit max pop size\n                        self.population_size = min(2 * self.initial_population_size, self.population_size + 5)\n                        new_population = np.random.uniform(self.lb, self.ub, size=(5, self.dim))\n                        new_fitness = np.array([func(x) for x in new_population])\n                        self.budget -= 5\n                        population = np.concatenate((population, new_population))\n                        fitness = np.concatenate((fitness, new_fitness))\n\n                elif self.population_size > self.initial_population_size and iteration % 20 == 0: #reduce population\n                    self.population_size = self.initial_population_size\n                    sorted_indices = np.argsort(fitness)\n                    population = population[sorted_indices[:self.initial_population_size]]\n                    fitness = fitness[sorted_indices[:self.initial_population_size]]\n\n        \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "The algorithm AdaptiveSlimeMoldOptimizer scored 0.381 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["2d086322-f43f-4b37-bed4-2235e826723e"], "operator": "mutation", "metadata": {"llm_response": "# Description: An improved Slime Mold Optimizer with adaptive parameter control and dynamic population size adjustment for enhanced exploration and exploitation.\n# Code: \n```python\nimport numpy as np\n\nclass AdaptiveSlimeMoldOptimizer:\n    def __init__(self, budget=10000, dim=10, initial_population_size=20, w_decay=0.9, p_selection=0.2, exploration_rate=0.1):\n        \"\"\"\n        Initializes the Adaptive Slime Mold Optimizer.\n\n        Args:\n            budget (int): The maximum number of function evaluations.\n            dim (int): The dimensionality of the search space.\n            initial_population_size (int): The initial number of slime mold agents.\n            w_decay (float): Decay rate for the weight parameter.\n            p_selection (float): Percentage of top solutions to consider for weighted averaging.\n            exploration_rate (float): Probability of exploration.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.population_size = initial_population_size\n        self.initial_population_size = initial_population_size\n        self.w_decay = w_decay\n        self.p_selection = p_selection  # Percentage for best selection\n        self.lb = -5.0\n        self.ub = 5.0\n        self.exploration_rate = exploration_rate\n        self.adaptive_exploration_rate = exploration_rate\n\n\n    def __call__(self, func):\n        \"\"\"\n        Optimizes the given function using the Adaptive Slime Mold Optimizer.\n\n        Args:\n            func (callable): The black-box function to optimize.\n\n        Returns:\n            tuple: The best function value found and the corresponding solution.\n        \"\"\"\n        population = np.random.uniform(self.lb, self.ub, size=(self.population_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.population_size\n        \n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index].copy()\n\n\n        weights = np.ones(self.population_size) # Initialize weights\n\n        iteration = 0\n        while self.budget > 0:\n            iteration += 1\n            # Sort population based on fitness\n            sorted_indices = np.argsort(fitness)\n            population = population[sorted_indices]\n            fitness = fitness[sorted_indices]\n            \n            # Select top solutions based on p_selection\n            num_selected = int(self.p_selection * self.population_size)\n            selected_population = population[:num_selected]\n            selected_fitness = fitness[:num_selected]\n                \n            # Calculate weights based on fitness rank. Give the better ones more weight\n            rank = np.arange(1, num_selected + 1)\n            weights_selected = np.exp(-rank / num_selected)\n            weights_selected /= np.sum(weights_selected)  # Normalize\n\n            # Update population using weighted average of selected solutions\n            for i in range(self.population_size):\n                if self.budget <= 0:\n                    break\n                \n                # Adaptive Exploration Rate: Increase when stuck, decrease when improving\n                if np.random.rand() < self.adaptive_exploration_rate:\n                    x = np.random.uniform(self.lb, self.ub, size=self.dim)\n\n                else: # Weighted average of selected\n                    x = np.zeros(self.dim)\n                    for j in range(num_selected):\n                        x += weights_selected[j] * selected_population[j]\n\n                    # Add small random perturbation\n                    x += 0.1 * np.random.randn(self.dim)\n                    x = np.clip(x, self.lb, self.ub)\n\n                f = func(x)\n                self.budget -= 1\n\n                if f < fitness[i]:\n                    fitness[i] = f\n                    population[i] = x.copy()\n\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = x.copy()\n                        self.adaptive_exploration_rate = max(0.01, self.adaptive_exploration_rate * 0.95) # Reduce exploration\n\n                else:\n                    self.adaptive_exploration_rate = min(self.exploration_rate, self.adaptive_exploration_rate * 1.05) # increase exploration\n\n            # Dynamic Population Size Adjustment (Optional):\n            if iteration % 10 == 0:  # Adjust every 10 iterations\n                if np.std(fitness) < 1e-6:  # Stagnation detected\n                    if self.population_size < 2 * self.initial_population_size: # Limit max pop size\n                        self.population_size = min(2 * self.initial_population_size, self.population_size + 5)\n                        new_population = np.random.uniform(self.lb, self.ub, size=(5, self.dim))\n                        new_fitness = np.array([func(x) for x in new_population])\n                        self.budget -= 5\n                        population = np.concatenate((population, new_population))\n                        fitness = np.concatenate((fitness, new_fitness))\n\n                elif self.population_size > self.initial_population_size and iteration % 20 == 0: #reduce population\n                    self.population_size = self.initial_population_size\n                    sorted_indices = np.argsort(fitness)\n                    population = population[sorted_indices[:self.initial_population_size]]\n                    fitness = fitness[sorted_indices[:self.initial_population_size]]\n\n        \n        return self.f_opt, self.x_opt\n```", "aucs": [0.11764685406323605, 0.17531374721003667, 0.448391442930799, 0.18980068645455417, 0.18689682882041259, 0.48177939179774054, 0.25184185157579686, 0.3768573541424334, 0.35041670984304873, 0.20464780890492573, 0.7835109537019296, 0.9844172050362109, 0.249428058844667, 0.2105490430853646, 0.7798193236564899, 0.3350728697237446, 0.2844658980976812, 0.5902860779313586, 0.1581506224334871, 0.46453583233297646], "operator": "mutation", "theta_sampled": 0.053911925603846146, "reward": 0.0, "generation": 2}, "task_prompt": ""}
{"id": "c3a3fd66-0c9c-4519-adb0-4233910710a1", "fitness": 0.1867423090186094, "name": "EnhancedHybridPSODE", "description": "Hybrid PSODE with self-adaptive parameter control, neighborhood-based communication in PSO, and a restart mechanism to escape local optima.", "code": "import numpy as np\n\nclass EnhancedHybridPSODE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, w_start=0.9, w_end=0.4, c1=1.5, c2=1.5, cr=0.7, f=0.8, neighborhood_size=5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.w_start = w_start  # Initial inertia weight for PSO\n        self.w_end = w_end      # Final inertia weight for PSO\n        self.c1 = c1            # Cognitive coefficient for PSO\n        self.c2 = c2            # Social coefficient for PSO\n        self.cr = cr            # Crossover rate for DE\n        self.f = f              # Mutation factor for DE\n        self.neighborhood_size = neighborhood_size # Size of the neighborhood for PSO\n        self.pop = None\n        self.fitness = None\n        self.pbest_pos = None\n        self.pbest_fitness = None\n        self.gbest_pos = None\n        self.gbest_fitness = np.inf\n        self.eval_count = 0\n        self.restart_counter = 0\n        self.restart_threshold = 0.1  # Restart if no improvement for 10% of budget\n\n\n    def initialize_population(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        self.pop = np.random.uniform(lb, ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.eval_count += self.pop_size\n        self.pbest_pos = self.pop.copy()\n        self.pbest_fitness = self.fitness.copy()\n        self.gbest_pos = self.pop[np.argmin(self.fitness)].copy()\n        self.gbest_fitness = np.min(self.fitness)\n        self.last_improvement = self.eval_count\n\n\n    def pso_step(self, func):\n        # Adaptive inertia weight\n        w = self.w_start - (self.w_start - self.w_end) * (self.eval_count / self.budget)\n\n        velocity = np.zeros_like(self.pop)\n        for i in range(self.pop_size):\n            # Neighborhood best\n            neighbors = np.random.choice(self.pop_size, self.neighborhood_size, replace=False)\n            best_neighbor_index = neighbors[np.argmin(self.fitness[neighbors])]\n            best_neighbor_pos = self.pop[best_neighbor_index]\n\n            r1 = np.random.rand(self.dim)\n            r2 = np.random.rand(self.dim)\n            velocity[i] = (w * velocity[i]\n                           + self.c1 * r1 * (self.pbest_pos[i] - self.pop[i])\n                           + self.c2 * r2 * (best_neighbor_pos - self.pop[i]))  # Use neighbor best\n            self.pop[i] += velocity[i]\n            self.pop[i] = np.clip(self.pop[i], func.bounds.lb, func.bounds.ub)\n\n            f = func(self.pop[i])\n            self.eval_count += 1\n\n            if f < self.fitness[i]:\n                self.fitness[i] = f\n\n                if f < self.pbest_fitness[i]:\n                    self.pbest_fitness[i] = f\n                    self.pbest_pos[i] = self.pop[i].copy()\n\n                    if f < self.gbest_fitness:\n                        self.gbest_fitness = f\n                        self.gbest_pos = self.pop[i].copy()\n                        self.last_improvement = self.eval_count\n\n\n\n    def de_step(self, func):\n        for i in range(self.pop_size):\n            idxs = np.random.choice(self.pop_size, 3, replace=False)\n            x_1, x_2, x_3 = self.pop[idxs]\n            v = x_1 + self.f * (x_2 - x_3)\n            v = np.clip(v, func.bounds.lb, func.bounds.ub)\n            \n            j_rand = np.random.randint(self.dim)\n            u = self.pop[i].copy()\n            for j in range(self.dim):\n                if np.random.rand() < self.cr or j == j_rand:\n                    u[j] = v[j]\n                    \n            f = func(u)\n            self.eval_count += 1\n            \n            if f < self.fitness[i]:\n                self.fitness[i] = f\n\n                if f < self.pbest_fitness[i]:\n                    self.pbest_fitness[i] = f\n                    self.pbest_pos[i] = u.copy()\n\n                    if f < self.gbest_fitness:\n                        self.gbest_fitness = f\n                        self.gbest_pos = u.copy()\n                        self.last_improvement = self.eval_count\n\n    def restart_population(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        self.pop = np.random.uniform(lb, ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.eval_count += self.pop_size\n        self.pbest_pos = self.pop.copy()\n        self.pbest_fitness = self.fitness.copy()\n\n        # Keep the previous best\n        self.pop[0] = self.gbest_pos.copy()\n        self.fitness[0] = self.gbest_fitness\n        self.pbest_pos[0] = self.gbest_pos.copy()\n        self.pbest_fitness[0] = self.gbest_fitness\n        \n        self.gbest_pos = self.pop[np.argmin(self.fitness)].copy()\n        self.gbest_fitness = np.min(self.fitness)\n        self.last_improvement = self.eval_count\n        self.restart_counter += 1\n\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        \n        while self.eval_count < self.budget:\n            # Adaptive strategy: Switch between PSO and DE based on population diversity\n            if np.std(self.fitness) > 0.1 * np.abs(self.gbest_fitness):  #High Diversity -> Exploration (DE)\n                self.de_step(func)\n            else:  #Low Diversity -> Exploitation (PSO)\n                self.pso_step(func)\n\n            # Restart mechanism\n            if self.eval_count - self.last_improvement > self.restart_threshold * self.budget:\n                self.restart_population(func)\n\n        return self.gbest_fitness, self.gbest_pos", "configspace": "", "generation": 2, "feedback": "The algorithm EnhancedHybridPSODE scored 0.187 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["f2c5ca87-d3a3-4aa8-ab6c-01b1c299941c"], "operator": "mutation", "metadata": {"llm_response": "# Description: Hybrid PSODE with self-adaptive parameter control, neighborhood-based communication in PSO, and a restart mechanism to escape local optima.\n# Code: \n```python\nimport numpy as np\n\nclass EnhancedHybridPSODE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, w_start=0.9, w_end=0.4, c1=1.5, c2=1.5, cr=0.7, f=0.8, neighborhood_size=5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.w_start = w_start  # Initial inertia weight for PSO\n        self.w_end = w_end      # Final inertia weight for PSO\n        self.c1 = c1            # Cognitive coefficient for PSO\n        self.c2 = c2            # Social coefficient for PSO\n        self.cr = cr            # Crossover rate for DE\n        self.f = f              # Mutation factor for DE\n        self.neighborhood_size = neighborhood_size # Size of the neighborhood for PSO\n        self.pop = None\n        self.fitness = None\n        self.pbest_pos = None\n        self.pbest_fitness = None\n        self.gbest_pos = None\n        self.gbest_fitness = np.inf\n        self.eval_count = 0\n        self.restart_counter = 0\n        self.restart_threshold = 0.1  # Restart if no improvement for 10% of budget\n\n\n    def initialize_population(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        self.pop = np.random.uniform(lb, ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.eval_count += self.pop_size\n        self.pbest_pos = self.pop.copy()\n        self.pbest_fitness = self.fitness.copy()\n        self.gbest_pos = self.pop[np.argmin(self.fitness)].copy()\n        self.gbest_fitness = np.min(self.fitness)\n        self.last_improvement = self.eval_count\n\n\n    def pso_step(self, func):\n        # Adaptive inertia weight\n        w = self.w_start - (self.w_start - self.w_end) * (self.eval_count / self.budget)\n\n        velocity = np.zeros_like(self.pop)\n        for i in range(self.pop_size):\n            # Neighborhood best\n            neighbors = np.random.choice(self.pop_size, self.neighborhood_size, replace=False)\n            best_neighbor_index = neighbors[np.argmin(self.fitness[neighbors])]\n            best_neighbor_pos = self.pop[best_neighbor_index]\n\n            r1 = np.random.rand(self.dim)\n            r2 = np.random.rand(self.dim)\n            velocity[i] = (w * velocity[i]\n                           + self.c1 * r1 * (self.pbest_pos[i] - self.pop[i])\n                           + self.c2 * r2 * (best_neighbor_pos - self.pop[i]))  # Use neighbor best\n            self.pop[i] += velocity[i]\n            self.pop[i] = np.clip(self.pop[i], func.bounds.lb, func.bounds.ub)\n\n            f = func(self.pop[i])\n            self.eval_count += 1\n\n            if f < self.fitness[i]:\n                self.fitness[i] = f\n\n                if f < self.pbest_fitness[i]:\n                    self.pbest_fitness[i] = f\n                    self.pbest_pos[i] = self.pop[i].copy()\n\n                    if f < self.gbest_fitness:\n                        self.gbest_fitness = f\n                        self.gbest_pos = self.pop[i].copy()\n                        self.last_improvement = self.eval_count\n\n\n\n    def de_step(self, func):\n        for i in range(self.pop_size):\n            idxs = np.random.choice(self.pop_size, 3, replace=False)\n            x_1, x_2, x_3 = self.pop[idxs]\n            v = x_1 + self.f * (x_2 - x_3)\n            v = np.clip(v, func.bounds.lb, func.bounds.ub)\n            \n            j_rand = np.random.randint(self.dim)\n            u = self.pop[i].copy()\n            for j in range(self.dim):\n                if np.random.rand() < self.cr or j == j_rand:\n                    u[j] = v[j]\n                    \n            f = func(u)\n            self.eval_count += 1\n            \n            if f < self.fitness[i]:\n                self.fitness[i] = f\n\n                if f < self.pbest_fitness[i]:\n                    self.pbest_fitness[i] = f\n                    self.pbest_pos[i] = u.copy()\n\n                    if f < self.gbest_fitness:\n                        self.gbest_fitness = f\n                        self.gbest_pos = u.copy()\n                        self.last_improvement = self.eval_count\n\n    def restart_population(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        self.pop = np.random.uniform(lb, ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.pop])\n        self.eval_count += self.pop_size\n        self.pbest_pos = self.pop.copy()\n        self.pbest_fitness = self.fitness.copy()\n\n        # Keep the previous best\n        self.pop[0] = self.gbest_pos.copy()\n        self.fitness[0] = self.gbest_fitness\n        self.pbest_pos[0] = self.gbest_pos.copy()\n        self.pbest_fitness[0] = self.gbest_fitness\n        \n        self.gbest_pos = self.pop[np.argmin(self.fitness)].copy()\n        self.gbest_fitness = np.min(self.fitness)\n        self.last_improvement = self.eval_count\n        self.restart_counter += 1\n\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        \n        while self.eval_count < self.budget:\n            # Adaptive strategy: Switch between PSO and DE based on population diversity\n            if np.std(self.fitness) > 0.1 * np.abs(self.gbest_fitness):  #High Diversity -> Exploration (DE)\n                self.de_step(func)\n            else:  #Low Diversity -> Exploitation (PSO)\n                self.pso_step(func)\n\n            # Restart mechanism\n            if self.eval_count - self.last_improvement > self.restart_threshold * self.budget:\n                self.restart_population(func)\n\n        return self.gbest_fitness, self.gbest_pos\n```", "aucs": [0.11856640415918607, 0.20125710479984882, 0.26993981270535516, 0.22855104430653606, 0.20735031392909054, 0.21683799411832716, 0.23873033265596955, 0.20954196820860393, 0.19533472098270133, 0.16805570333908437, 0], "operator": "mutation", "theta_sampled": 0.04309915966821338, "reward": 0.0, "generation": 2}, "task_prompt": ""}
{"id": "67163f20-e606-4949-ae8f-2c2ce6e468e2", "fitness": 0.2720442033159991, "name": "HybridSlimeDE_Archive", "description": "A hybrid algorithm combining Slime Mold's weighted averaging with a modified Differential Evolution using a dynamically updated archive and self-adaptive parameters.", "code": "import numpy as np\n\nclass HybridSlimeDE_Archive:\n    def __init__(self, budget=10000, dim=10, population_size=20, archive_factor=2.0, initial_F=0.5, initial_CR=0.7):\n        \"\"\"\n        Hybrid algorithm combining Slime Mold's weighted averaging with a modified Differential Evolution using a dynamically updated archive and self-adaptive parameters.\n\n        Args:\n            budget (int): The maximum number of function evaluations.\n            dim (int): The dimensionality of the search space.\n            population_size (int): The number of agents.\n            archive_factor (float): Factor to determine archive size relative to population size.\n            initial_F (float): Initial scaling factor for DE mutation.\n            initial_CR (float): Initial crossover rate for DE crossover.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.population_size = population_size\n        self.archive_factor = archive_factor\n        self.lb = -5.0\n        self.ub = 5.0\n        self.archive = []\n        self.F = initial_F\n        self.CR = initial_CR\n        self.p_selection = 0.2  # Initial percentage for Slime Mold selection\n\n    def __call__(self, func):\n        \"\"\"\n        Optimizes the given function using the hybrid algorithm.\n\n        Args:\n            func (callable): The black-box function to optimize.\n\n        Returns:\n            tuple: The best function value found and the corresponding solution.\n        \"\"\"\n        population = np.random.uniform(self.lb, self.ub, size=(self.population_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.population_size\n\n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index].copy()\n\n        while self.budget > 0:\n            # Sort population based on fitness\n            sorted_indices = np.argsort(fitness)\n            population = population[sorted_indices]\n            fitness = fitness[sorted_indices]\n\n            # Adaptive p_selection\n            self.p_selection *= 0.998  # Decay the selection pressure over time\n            num_selected = int(max(2, self.p_selection * self.population_size))\n\n            selected_population = population[:num_selected]\n            selected_fitness = fitness[:num_selected]\n\n            rank = np.arange(1, num_selected + 1)\n            weights_selected = np.exp(-rank / num_selected)\n            weights_selected /= np.sum(weights_selected)\n\n            for i in range(self.population_size):\n                if self.budget <= 0:\n                    break\n                \n                # Adaptive F and CR (Self-adaptive DE)\n                if np.random.rand() < 0.1:  # Probabilistic update\n                    self.F = np.clip(np.random.normal(self.F, 0.1), 0.1, 1.0)\n                if np.random.rand() < 0.1:\n                    self.CR = np.clip(np.random.normal(self.CR, 0.1), 0.1, 1.0)\n                \n                # Mutation (DE part) with Archive\n                if len(self.archive) > 2:\n                    use_archive = np.random.rand() < 0.5  # Probability of using archive\n                else:\n                    use_archive = False\n\n                idxs = np.random.choice(self.population_size, 2, replace=False)\n                x_1, x_2 = population[idxs]\n\n                if use_archive:\n                    idx_archive = np.random.randint(0, len(self.archive))\n                    x_3 = self.archive[idx_archive]\n                else:\n                    idx3 = np.random.choice(self.population_size, 1, replace=False)[0]\n                    x_3 = population[idx3]\n\n                mutant = population[i] + self.F * (x_1 - x_2) #Simplified DE mutation\n\n                mutant = np.clip(mutant, self.lb, self.ub)\n\n                # Crossover (DE part)\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial_vector_DE = np.where(cross_points, mutant, population[i])\n\n                # Slime Mold inspired perturbation\n                x_sm = np.zeros(self.dim)\n                for j in range(num_selected):\n                    x_sm += weights_selected[j] * selected_population[j]\n\n                # Combine DE and Slime Mold\n                trial_vector = 0.6 * trial_vector_DE + 0.4 * x_sm  # Adjust weights for balance\n                trial_vector = np.clip(trial_vector, self.lb, self.ub)\n\n                f_trial = func(trial_vector)\n                self.budget -= 1\n\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    population[i] = trial_vector.copy()\n                    \n                    #Update Archive\n                    self.archive.append(population[i].copy())\n                    if len(self.archive) > self.archive_factor * self.population_size:\n                        self.archive.pop(0)\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial_vector.copy()\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "The algorithm HybridSlimeDE_Archive scored 0.272 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["125c52bb-8777-4feb-b57a-1eb805ed2e25"], "operator": "mutation", "metadata": {"llm_response": "# Description: A hybrid algorithm combining Slime Mold's weighted averaging with a modified Differential Evolution using a dynamically updated archive and self-adaptive parameters.\n# Code: \n```python\nimport numpy as np\n\nclass HybridSlimeDE_Archive:\n    def __init__(self, budget=10000, dim=10, population_size=20, archive_factor=2.0, initial_F=0.5, initial_CR=0.7):\n        \"\"\"\n        Hybrid algorithm combining Slime Mold's weighted averaging with a modified Differential Evolution using a dynamically updated archive and self-adaptive parameters.\n\n        Args:\n            budget (int): The maximum number of function evaluations.\n            dim (int): The dimensionality of the search space.\n            population_size (int): The number of agents.\n            archive_factor (float): Factor to determine archive size relative to population size.\n            initial_F (float): Initial scaling factor for DE mutation.\n            initial_CR (float): Initial crossover rate for DE crossover.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.population_size = population_size\n        self.archive_factor = archive_factor\n        self.lb = -5.0\n        self.ub = 5.0\n        self.archive = []\n        self.F = initial_F\n        self.CR = initial_CR\n        self.p_selection = 0.2  # Initial percentage for Slime Mold selection\n\n    def __call__(self, func):\n        \"\"\"\n        Optimizes the given function using the hybrid algorithm.\n\n        Args:\n            func (callable): The black-box function to optimize.\n\n        Returns:\n            tuple: The best function value found and the corresponding solution.\n        \"\"\"\n        population = np.random.uniform(self.lb, self.ub, size=(self.population_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.population_size\n\n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index].copy()\n\n        while self.budget > 0:\n            # Sort population based on fitness\n            sorted_indices = np.argsort(fitness)\n            population = population[sorted_indices]\n            fitness = fitness[sorted_indices]\n\n            # Adaptive p_selection\n            self.p_selection *= 0.998  # Decay the selection pressure over time\n            num_selected = int(max(2, self.p_selection * self.population_size))\n\n            selected_population = population[:num_selected]\n            selected_fitness = fitness[:num_selected]\n\n            rank = np.arange(1, num_selected + 1)\n            weights_selected = np.exp(-rank / num_selected)\n            weights_selected /= np.sum(weights_selected)\n\n            for i in range(self.population_size):\n                if self.budget <= 0:\n                    break\n                \n                # Adaptive F and CR (Self-adaptive DE)\n                if np.random.rand() < 0.1:  # Probabilistic update\n                    self.F = np.clip(np.random.normal(self.F, 0.1), 0.1, 1.0)\n                if np.random.rand() < 0.1:\n                    self.CR = np.clip(np.random.normal(self.CR, 0.1), 0.1, 1.0)\n                \n                # Mutation (DE part) with Archive\n                if len(self.archive) > 2:\n                    use_archive = np.random.rand() < 0.5  # Probability of using archive\n                else:\n                    use_archive = False\n\n                idxs = np.random.choice(self.population_size, 2, replace=False)\n                x_1, x_2 = population[idxs]\n\n                if use_archive:\n                    idx_archive = np.random.randint(0, len(self.archive))\n                    x_3 = self.archive[idx_archive]\n                else:\n                    idx3 = np.random.choice(self.population_size, 1, replace=False)[0]\n                    x_3 = population[idx3]\n\n                mutant = population[i] + self.F * (x_1 - x_2) #Simplified DE mutation\n\n                mutant = np.clip(mutant, self.lb, self.ub)\n\n                # Crossover (DE part)\n                cross_points = np.random.rand(self.dim) < self.CR\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial_vector_DE = np.where(cross_points, mutant, population[i])\n\n                # Slime Mold inspired perturbation\n                x_sm = np.zeros(self.dim)\n                for j in range(num_selected):\n                    x_sm += weights_selected[j] * selected_population[j]\n\n                # Combine DE and Slime Mold\n                trial_vector = 0.6 * trial_vector_DE + 0.4 * x_sm  # Adjust weights for balance\n                trial_vector = np.clip(trial_vector, self.lb, self.ub)\n\n                f_trial = func(trial_vector)\n                self.budget -= 1\n\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    population[i] = trial_vector.copy()\n                    \n                    #Update Archive\n                    self.archive.append(population[i].copy())\n                    if len(self.archive) > self.archive_factor * self.population_size:\n                        self.archive.pop(0)\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial_vector.copy()\n\n        return self.f_opt, self.x_opt\n```", "aucs": [0.15383881016403467, 0.20002116472026, 0.2699120842798165, 0.17892106626527815, 0.18243200986485986, 0.3428204794047014, 0.22219275975534192, 0.22082307472804186, 0.25655895697896824, 0.20278483135026903, 0.1930966619264679, 0.9946424714259493, 0.28368307071769616, 0.18952829678130012, 0.1594449671690179, 0.27071289489853023, 0.21849393097880188, 0.18356525898411313, 0.2262489517713947, 0.4911623241551386], "operator": "mutation", "theta_sampled": 0.06729778547996076, "reward": 0.0, "generation": 2}, "task_prompt": ""}
{"id": "19122a76-87b8-404d-86bd-2bda6f4e75cc", "fitness": 0.48034985128174357, "name": "HybridSlimeAdaptive", "description": "A hybrid algorithm combining Slime Mold's weighted averaging with Adaptive Mutation's diversity-based mutation strategy to balance exploration and exploitation.", "code": "import numpy as np\n\nclass HybridSlimeAdaptive:\n    def __init__(self, budget=10000, dim=10, population_size=30, w_decay=0.9, p_selection=0.2, initial_mutation_scale=1.0):\n        \"\"\"\n        Initializes the Hybrid Slime Mold Adaptive Mutation Optimizer.\n\n        Args:\n            budget (int): The maximum number of function evaluations.\n            dim (int): The dimensionality of the search space.\n            population_size (int): The number of slime mold agents / population size.\n            w_decay (float): Decay rate for the weight parameter in Slime Mold phase.\n            p_selection (float): Percentage of top solutions for weighted averaging in Slime Mold phase.\n            initial_mutation_scale (float): Initial mutation scale for Adaptive Mutation phase.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.population_size = population_size\n        self.w_decay = w_decay\n        self.p_selection = p_selection\n        self.lb = -5.0\n        self.ub = 5.0\n        self.mutation_prob_gaussian = 0.5\n        self.mutation_prob_cauchy = 0.5\n        self.mutation_scale = initial_mutation_scale\n        self.slime_mold_phase = True # Start with Slime Mold phase\n\n    def calculate_diversity(self, population):\n        \"\"\"Calculates diversity based on the average distance to the centroid.\"\"\"\n        centroid = np.mean(population, axis=0)\n        distances = np.linalg.norm(population - centroid, axis=1)\n        return np.mean(distances)\n\n    def mutate_gaussian(self, x):\n        return x + self.mutation_scale * np.random.normal(0, 1, size=self.dim)\n\n    def mutate_cauchy(self, x):\n        return x + self.mutation_scale * np.random.standard_cauchy(size=self.dim)\n\n    def correct_bounds(self, x):\n         return np.clip(x, self.lb, self.ub)\n    \n    def slime_mold_update(self, population, fitness, func):\n        \"\"\"Performs the Slime Mold update step.\"\"\"\n        # Sort population based on fitness\n        sorted_indices = np.argsort(fitness)\n        population = population[sorted_indices]\n        fitness = fitness[sorted_indices]\n        \n        # Select top solutions based on p_selection\n        num_selected = int(self.p_selection * self.population_size)\n        selected_population = population[:num_selected]\n        selected_fitness = fitness[:num_selected]\n            \n        # Calculate weights based on fitness rank. Give the better ones more weight\n        rank = np.arange(1, num_selected + 1)\n        weights_selected = np.exp(-rank / num_selected)\n        weights_selected /= np.sum(weights_selected)  # Normalize\n\n        # Update population using weighted average of selected solutions\n        for i in range(self.population_size):\n            if self.budget <= 0:\n                break\n            \n            # Small probability for random exploration\n            if np.random.rand() < 0.1:\n                x = np.random.uniform(self.lb, self.ub, size=self.dim)\n\n            else: # Weighted average of selected\n                x = np.zeros(self.dim)\n                for j in range(num_selected):\n                    x += weights_selected[j] * selected_population[j]\n\n                # Add small random perturbation\n                x += 0.1 * np.random.randn(self.dim)\n                x = np.clip(x, self.lb, self.ub)\n\n            f = func(x)\n            self.budget -= 1\n\n            if f < fitness[i]:\n                fitness[i] = f\n                population[i] = x.copy()\n\n                if f < self.f_opt:\n                    self.f_opt = f\n                    self.x_opt = x.copy()\n        \n        return population, fitness\n    \n    def adaptive_mutation_update(self, population, fitness, func):\n        \"\"\"Performs the Adaptive Mutation update step.\"\"\"\n        diversity = self.calculate_diversity(population)\n\n        # Adjust mutation probabilities based on diversity\n        if diversity > 1.0:  # High diversity, favor exploitation\n            self.mutation_prob_gaussian = 0.8\n            self.mutation_prob_cauchy = 0.2\n        else:  # Low diversity, favor exploration\n            self.mutation_prob_gaussian = 0.2\n            self.mutation_prob_cauchy = 0.8\n\n        new_population = np.copy(population)\n        for i in range(self.population_size):\n            if np.random.rand() < self.mutation_prob_gaussian:\n                new_population[i] = self.mutate_gaussian(population[i])\n            else:\n                new_population[i] = self.mutate_cauchy(population[i])\n\n            new_population[i] = self.correct_bounds(new_population[i])\n            \n            new_fitness = func(new_population[i])\n            self.budget -= 1\n\n            if new_fitness < fitness[i]:\n                fitness[i] = new_fitness\n                population[i] = new_population[i]\n\n                if new_fitness < self.f_opt:\n                    self.f_opt = new_fitness\n                    self.x_opt = new_population[i]\n        \n        # Adapt mutation scale\n        self.mutation_scale = 0.95 * self.mutation_scale # Decay mutation scale\n        \n        return population, fitness\n\n    def __call__(self, func):\n        \"\"\"\n        Optimizes the given function using the Hybrid Slime Mold Adaptive Mutation Optimizer.\n\n        Args:\n            func (callable): The black-box function to optimize.\n\n        Returns:\n            tuple: The best function value found and the corresponding solution.\n        \"\"\"\n        population = np.random.uniform(self.lb, self.ub, size=(self.population_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.population_size\n        \n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index].copy()\n\n        while self.budget > 0:\n            if self.slime_mold_phase:\n                population, fitness = self.slime_mold_update(population, fitness, func)\n            else:\n                population, fitness = self.adaptive_mutation_update(population, fitness, func)\n            \n            #Dynamically switch phases. This is a simple switching rule.  More sophisticated switching\n            #rules could be implemented, such as tracking progress and switching based on stagnation.\n            if self.budget % 1000 < 500: #Switch every 500 evaluations\n                self.slime_mold_phase = True\n            else:\n                self.slime_mold_phase = False\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "The algorithm HybridSlimeAdaptive scored 0.480 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["2d086322-f43f-4b37-bed4-2235e826723e", "e28dc9c3-3d4b-4868-a051-3c9dc94d420f"], "operator": "crossover", "metadata": {"llm_response": "# Description: A hybrid algorithm combining Slime Mold's weighted averaging with Adaptive Mutation's diversity-based mutation strategy to balance exploration and exploitation.\n# Code: \n```python\nimport numpy as np\n\nclass HybridSlimeAdaptive:\n    def __init__(self, budget=10000, dim=10, population_size=30, w_decay=0.9, p_selection=0.2, initial_mutation_scale=1.0):\n        \"\"\"\n        Initializes the Hybrid Slime Mold Adaptive Mutation Optimizer.\n\n        Args:\n            budget (int): The maximum number of function evaluations.\n            dim (int): The dimensionality of the search space.\n            population_size (int): The number of slime mold agents / population size.\n            w_decay (float): Decay rate for the weight parameter in Slime Mold phase.\n            p_selection (float): Percentage of top solutions for weighted averaging in Slime Mold phase.\n            initial_mutation_scale (float): Initial mutation scale for Adaptive Mutation phase.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.population_size = population_size\n        self.w_decay = w_decay\n        self.p_selection = p_selection\n        self.lb = -5.0\n        self.ub = 5.0\n        self.mutation_prob_gaussian = 0.5\n        self.mutation_prob_cauchy = 0.5\n        self.mutation_scale = initial_mutation_scale\n        self.slime_mold_phase = True # Start with Slime Mold phase\n\n    def calculate_diversity(self, population):\n        \"\"\"Calculates diversity based on the average distance to the centroid.\"\"\"\n        centroid = np.mean(population, axis=0)\n        distances = np.linalg.norm(population - centroid, axis=1)\n        return np.mean(distances)\n\n    def mutate_gaussian(self, x):\n        return x + self.mutation_scale * np.random.normal(0, 1, size=self.dim)\n\n    def mutate_cauchy(self, x):\n        return x + self.mutation_scale * np.random.standard_cauchy(size=self.dim)\n\n    def correct_bounds(self, x):\n         return np.clip(x, self.lb, self.ub)\n    \n    def slime_mold_update(self, population, fitness, func):\n        \"\"\"Performs the Slime Mold update step.\"\"\"\n        # Sort population based on fitness\n        sorted_indices = np.argsort(fitness)\n        population = population[sorted_indices]\n        fitness = fitness[sorted_indices]\n        \n        # Select top solutions based on p_selection\n        num_selected = int(self.p_selection * self.population_size)\n        selected_population = population[:num_selected]\n        selected_fitness = fitness[:num_selected]\n            \n        # Calculate weights based on fitness rank. Give the better ones more weight\n        rank = np.arange(1, num_selected + 1)\n        weights_selected = np.exp(-rank / num_selected)\n        weights_selected /= np.sum(weights_selected)  # Normalize\n\n        # Update population using weighted average of selected solutions\n        for i in range(self.population_size):\n            if self.budget <= 0:\n                break\n            \n            # Small probability for random exploration\n            if np.random.rand() < 0.1:\n                x = np.random.uniform(self.lb, self.ub, size=self.dim)\n\n            else: # Weighted average of selected\n                x = np.zeros(self.dim)\n                for j in range(num_selected):\n                    x += weights_selected[j] * selected_population[j]\n\n                # Add small random perturbation\n                x += 0.1 * np.random.randn(self.dim)\n                x = np.clip(x, self.lb, self.ub)\n\n            f = func(x)\n            self.budget -= 1\n\n            if f < fitness[i]:\n                fitness[i] = f\n                population[i] = x.copy()\n\n                if f < self.f_opt:\n                    self.f_opt = f\n                    self.x_opt = x.copy()\n        \n        return population, fitness\n    \n    def adaptive_mutation_update(self, population, fitness, func):\n        \"\"\"Performs the Adaptive Mutation update step.\"\"\"\n        diversity = self.calculate_diversity(population)\n\n        # Adjust mutation probabilities based on diversity\n        if diversity > 1.0:  # High diversity, favor exploitation\n            self.mutation_prob_gaussian = 0.8\n            self.mutation_prob_cauchy = 0.2\n        else:  # Low diversity, favor exploration\n            self.mutation_prob_gaussian = 0.2\n            self.mutation_prob_cauchy = 0.8\n\n        new_population = np.copy(population)\n        for i in range(self.population_size):\n            if np.random.rand() < self.mutation_prob_gaussian:\n                new_population[i] = self.mutate_gaussian(population[i])\n            else:\n                new_population[i] = self.mutate_cauchy(population[i])\n\n            new_population[i] = self.correct_bounds(new_population[i])\n            \n            new_fitness = func(new_population[i])\n            self.budget -= 1\n\n            if new_fitness < fitness[i]:\n                fitness[i] = new_fitness\n                population[i] = new_population[i]\n\n                if new_fitness < self.f_opt:\n                    self.f_opt = new_fitness\n                    self.x_opt = new_population[i]\n        \n        # Adapt mutation scale\n        self.mutation_scale = 0.95 * self.mutation_scale # Decay mutation scale\n        \n        return population, fitness\n\n    def __call__(self, func):\n        \"\"\"\n        Optimizes the given function using the Hybrid Slime Mold Adaptive Mutation Optimizer.\n\n        Args:\n            func (callable): The black-box function to optimize.\n\n        Returns:\n            tuple: The best function value found and the corresponding solution.\n        \"\"\"\n        population = np.random.uniform(self.lb, self.ub, size=(self.population_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.population_size\n        \n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index].copy()\n\n        while self.budget > 0:\n            if self.slime_mold_phase:\n                population, fitness = self.slime_mold_update(population, fitness, func)\n            else:\n                population, fitness = self.adaptive_mutation_update(population, fitness, func)\n            \n            #Dynamically switch phases. This is a simple switching rule.  More sophisticated switching\n            #rules could be implemented, such as tracking progress and switching based on stagnation.\n            if self.budget % 1000 < 500: #Switch every 500 evaluations\n                self.slime_mold_phase = True\n            else:\n                self.slime_mold_phase = False\n\n\n        return self.f_opt, self.x_opt\n```", "aucs": [0.24717856819034667, 0.19101511524692294, 0.4663170313914178, 0.903243789948741, 0.42582242369161705, 0.5341959661920295, 0.3179219659334238, 0.42280143638865586, 0.45204837634799944, 0.20089483952296738, 0.8484246612125796, 0.998981366116266, 0.25481070329667665, 0.4410031991925184, 0.7607056315738262, 0.3598547720076063, 0.4080289449823782, 0.6364396947735591, 0.22601839047563788, 0.5112901491497026], "operator": "crossover", "theta_sampled": 0.04018859848213107, "reward": 0.057078602424176395, "generation": 2}, "task_prompt": ""}
{"id": "cf77202c-d840-4cba-816c-ced9204e2592", "fitness": 0.2775133445990459, "name": "AdaptiveHybridSlimeDE", "description": "Adaptive Hybrid algorithm combining Slime Mold, Differential Evolution, and a local search based on the best solution found so far.", "code": "import numpy as np\n\nclass AdaptiveHybridSlimeDE:\n    def __init__(self, budget=10000, dim=10, population_size=20, F=0.5, CR=0.7, local_search_prob=0.1, local_search_radius=0.1):\n        \"\"\"\n        Adaptive hybrid algorithm combining Slime Mold's weighted averaging,\n        Differential Evolution's mutation and crossover, and a local search around the best solution.\n\n        Args:\n            budget (int): The maximum number of function evaluations.\n            dim (int): The dimensionality of the search space.\n            population_size (int): The number of agents.\n            F (float): Initial scaling factor for DE mutation.\n            CR (float): Initial crossover rate for DE crossover.\n            local_search_prob (float): Probability of applying local search.\n            local_search_radius (float): Radius of the local search.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.population_size = population_size\n        self.F = F\n        self.CR = CR\n        self.lb = -5.0\n        self.ub = 5.0\n        self.archive_factor = 2.0\n        self.archive = []\n        self.local_search_prob = local_search_prob\n        self.local_search_radius = local_search_radius\n\n    def __call__(self, func):\n        \"\"\"\n        Optimizes the given function using the hybrid algorithm.\n\n        Args:\n            func (callable): The black-box function to optimize.\n\n        Returns:\n            tuple: The best function value found and the corresponding solution.\n        \"\"\"\n        population = np.random.uniform(self.lb, self.ub, size=(self.population_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.population_size\n\n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index].copy()\n\n        generation = 0\n        while self.budget > 0:\n            generation += 1\n\n            # Sort population based on fitness\n            sorted_indices = np.argsort(fitness)\n            population = population[sorted_indices]\n            fitness = fitness[sorted_indices]\n\n            # Adaptive DE parameters\n            F_i = np.clip(np.random.normal(self.F, 0.05), 0.1, 1.0)  # Reduced std dev for F\n            CR_i = np.clip(np.random.normal(self.CR, 0.05), 0.1, 1.0)  # Reduced std dev for CR\n\n\n            # Slime Mold inspired weighted averaging\n            num_selected = max(2, int(0.2 * self.population_size))  # Fixed selection percentage\n            selected_population = population[:num_selected]\n            rank = np.arange(1, num_selected + 1)\n            weights = np.exp(-rank / num_selected)\n            weights /= np.sum(weights)\n\n            # Local Search around best solution (applied probabilistically)\n            if np.random.rand() < self.local_search_prob:\n                x_local = self.x_opt + np.random.uniform(-self.local_search_radius, self.local_search_radius, size=self.dim)\n                x_local = np.clip(x_local, self.lb, self.ub)\n                f_local = func(x_local)\n                self.budget -= 1\n                if self.budget <= 0:\n                    break\n\n                if f_local < self.f_opt:\n                    self.f_opt = f_local\n                    self.x_opt = x_local.copy()\n                    \n            # DE Mutation and Crossover\n            for i in range(self.population_size):\n                if self.budget <= 0:\n                    break\n\n                idxs = np.random.choice(self.population_size, 3, replace=False)\n                x_1, x_2, x_3 = population[idxs]\n                mutant = x_1 + F_i * (x_2 - x_3)\n                mutant = np.clip(mutant, self.lb, self.ub)\n\n                cross_points = np.random.rand(self.dim) < CR_i\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial_vector_DE = np.where(cross_points, mutant, population[i])\n\n\n                # Slime Mold Perturbation\n\n                x_sm = np.zeros(self.dim)\n                for j in range(num_selected):\n                    x_sm += weights[j] * selected_population[j]\n\n                trial_vector = 0.6 * trial_vector_DE + 0.4 * x_sm  # Blend DE and SM\n\n\n                trial_vector = np.clip(trial_vector, self.lb, self.ub)\n\n                f_trial = func(trial_vector)\n                self.budget -= 1\n                if self.budget <= 0:\n                    break\n\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    population[i] = trial_vector.copy()\n\n                    #Update Archive\n                    self.archive.append(population[i].copy())\n                    if len(self.archive) > self.archive_factor * self.population_size:\n                        self.archive.pop(0)\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial_vector.copy()\n                        \n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "The algorithm AdaptiveHybridSlimeDE scored 0.278 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["125c52bb-8777-4feb-b57a-1eb805ed2e25"], "operator": "mutation", "metadata": {"llm_response": "# Description: Adaptive Hybrid algorithm combining Slime Mold, Differential Evolution, and a local search based on the best solution found so far.\n# Code: \n```python\nimport numpy as np\n\nclass AdaptiveHybridSlimeDE:\n    def __init__(self, budget=10000, dim=10, population_size=20, F=0.5, CR=0.7, local_search_prob=0.1, local_search_radius=0.1):\n        \"\"\"\n        Adaptive hybrid algorithm combining Slime Mold's weighted averaging,\n        Differential Evolution's mutation and crossover, and a local search around the best solution.\n\n        Args:\n            budget (int): The maximum number of function evaluations.\n            dim (int): The dimensionality of the search space.\n            population_size (int): The number of agents.\n            F (float): Initial scaling factor for DE mutation.\n            CR (float): Initial crossover rate for DE crossover.\n            local_search_prob (float): Probability of applying local search.\n            local_search_radius (float): Radius of the local search.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.population_size = population_size\n        self.F = F\n        self.CR = CR\n        self.lb = -5.0\n        self.ub = 5.0\n        self.archive_factor = 2.0\n        self.archive = []\n        self.local_search_prob = local_search_prob\n        self.local_search_radius = local_search_radius\n\n    def __call__(self, func):\n        \"\"\"\n        Optimizes the given function using the hybrid algorithm.\n\n        Args:\n            func (callable): The black-box function to optimize.\n\n        Returns:\n            tuple: The best function value found and the corresponding solution.\n        \"\"\"\n        population = np.random.uniform(self.lb, self.ub, size=(self.population_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.population_size\n\n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index].copy()\n\n        generation = 0\n        while self.budget > 0:\n            generation += 1\n\n            # Sort population based on fitness\n            sorted_indices = np.argsort(fitness)\n            population = population[sorted_indices]\n            fitness = fitness[sorted_indices]\n\n            # Adaptive DE parameters\n            F_i = np.clip(np.random.normal(self.F, 0.05), 0.1, 1.0)  # Reduced std dev for F\n            CR_i = np.clip(np.random.normal(self.CR, 0.05), 0.1, 1.0)  # Reduced std dev for CR\n\n\n            # Slime Mold inspired weighted averaging\n            num_selected = max(2, int(0.2 * self.population_size))  # Fixed selection percentage\n            selected_population = population[:num_selected]\n            rank = np.arange(1, num_selected + 1)\n            weights = np.exp(-rank / num_selected)\n            weights /= np.sum(weights)\n\n            # Local Search around best solution (applied probabilistically)\n            if np.random.rand() < self.local_search_prob:\n                x_local = self.x_opt + np.random.uniform(-self.local_search_radius, self.local_search_radius, size=self.dim)\n                x_local = np.clip(x_local, self.lb, self.ub)\n                f_local = func(x_local)\n                self.budget -= 1\n                if self.budget <= 0:\n                    break\n\n                if f_local < self.f_opt:\n                    self.f_opt = f_local\n                    self.x_opt = x_local.copy()\n                    \n            # DE Mutation and Crossover\n            for i in range(self.population_size):\n                if self.budget <= 0:\n                    break\n\n                idxs = np.random.choice(self.population_size, 3, replace=False)\n                x_1, x_2, x_3 = population[idxs]\n                mutant = x_1 + F_i * (x_2 - x_3)\n                mutant = np.clip(mutant, self.lb, self.ub)\n\n                cross_points = np.random.rand(self.dim) < CR_i\n                if not np.any(cross_points):\n                    cross_points[np.random.randint(0, self.dim)] = True\n                trial_vector_DE = np.where(cross_points, mutant, population[i])\n\n\n                # Slime Mold Perturbation\n\n                x_sm = np.zeros(self.dim)\n                for j in range(num_selected):\n                    x_sm += weights[j] * selected_population[j]\n\n                trial_vector = 0.6 * trial_vector_DE + 0.4 * x_sm  # Blend DE and SM\n\n\n                trial_vector = np.clip(trial_vector, self.lb, self.ub)\n\n                f_trial = func(trial_vector)\n                self.budget -= 1\n                if self.budget <= 0:\n                    break\n\n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    population[i] = trial_vector.copy()\n\n                    #Update Archive\n                    self.archive.append(population[i].copy())\n                    if len(self.archive) > self.archive_factor * self.population_size:\n                        self.archive.pop(0)\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial_vector.copy()\n                        \n\n        return self.f_opt, self.x_opt\n```", "aucs": [0.14086954101617521, 0.18865363068029906, 0.30713758986781725, 0.19831062132389443, 0.19234412917623844, 0.19975008526185356, 0.22161421314248297, 0.22282117131525792, 0.31592405130054513, 0.19198139405686865, 0.3114445074803236, 0.9973013008663184, 0.2916505312009128, 0.17435519506390706, 0.21315052270296442, 0.2610122975641094, 0.24885517051323502, 0.21344977469042492, 0.208335573029934, 0.4513055917273555], "operator": "mutation", "theta_sampled": 0.01971744318842762, "reward": 0.0, "generation": 2}, "task_prompt": ""}
{"id": "d131136a-fb1a-4620-b126-92577859ead1", "fitness": 0.0, "name": "HybridAdaptiveOptimizer", "description": "A hybrid algorithm combining adaptive mutation with a rank-based slime mold influence, adjusting mutation scales and population diversity for effective exploration and exploitation.", "code": "import numpy as np\n\nclass HybridAdaptiveOptimizer:\n    def __init__(self, budget=10000, dim=10, pop_size=50, stagnation_threshold=1000, p_selection=0.2, initial_mutation_scale=1.0):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lb = -5.0\n        self.ub = 5.0\n        self.mutation_prob_gaussian = 0.5\n        self.mutation_prob_cauchy = 0.5\n        self.mutation_scale = initial_mutation_scale\n        self.stagnation_threshold = stagnation_threshold\n        self.stagnation_counter = 0\n        self.best_fitness_history = []\n        self.p_selection = p_selection # Percentage of top solutions for slime mold influence\n\n    def initialize_population(self):\n        return np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n\n    def calculate_diversity(self, population):\n        centroid = np.mean(population, axis=0)\n        distances = np.linalg.norm(population - centroid, axis=1)\n        return np.mean(distances)\n\n    def mutate_gaussian(self, x):\n        return x + self.mutation_scale * np.random.normal(0, 1, size=self.dim)\n\n    def mutate_cauchy(self, x):\n        return x + self.mutation_scale * np.random.standard_cauchy(size=self.dim)\n\n    def correct_bounds(self, x):\n        return np.clip(x, self.lb, self.ub)\n\n    def rank_based_selection(self, population, fitness, num_parents):\n        ranked_indices = np.argsort(fitness)\n        parent_indices = ranked_indices[:num_parents]\n        return population[parent_indices]\n    \n    def adjust_population_size(self):\n        if self.stagnation_counter > self.stagnation_threshold:\n            self.pop_size = int(self.pop_size * 0.9)  # Reduce population size if stagnating\n            if self.pop_size < 10:\n                self.pop_size = 10 # Minimum population size\n\n            self.mutation_scale = 1.0 # Reset mutation scale\n            self.stagnation_counter = 0 # Reset stagnation counter\n            #print(f\"Stagnation detected, reducing pop size to {self.pop_size}\")\n\n    def slime_mold_influence(self, population, fitness):\n        \"\"\"Introduces slime mold-inspired movement based on rank.\"\"\"\n        num_selected = int(self.p_selection * self.pop_size)\n        ranked_indices = np.argsort(fitness)\n        selected_indices = ranked_indices[:num_selected]\n        \n        for i in range(self.pop_size):\n            if np.random.rand() < 0.2:  # Probability to apply slime mold influence\n                chosen_leader_index = np.random.choice(selected_indices)\n                population[i] = 0.8 * population[i] + 0.2 * population[chosen_leader_index] + 0.01 * np.random.randn(self.dim)  # Move towards a leader with some noise\n                population[i] = self.correct_bounds(population[i])\n\n        return population\n\n    def __call__(self, func):\n        population = self.initialize_population()\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n        \n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index]\n        self.best_fitness_history.append(self.f_opt)\n\n        while self.budget > 0:\n            diversity = self.calculate_diversity(population)\n\n            if diversity > 1.0:\n                self.mutation_prob_gaussian = 0.8\n                self.mutation_prob_cauchy = 0.2\n            else:\n                self.mutation_prob_gaussian = 0.2\n                self.mutation_prob_cauchy = 0.8\n\n            # Elitism: Keep the best individual\n            best_individual = np.copy(self.x_opt)\n            best_fitness = self.f_opt\n\n            # Rank-based selection of parents for mutation\n            num_parents = int(self.pop_size / 2)  # Select top half as parents\n            parents = self.rank_based_selection(population, fitness, num_parents)\n            \n            new_population = np.copy(population)\n            for i in range(self.pop_size):\n                # Choose a parent randomly\n                parent = parents[np.random.randint(0, num_parents)]\n                \n                if np.random.rand() < self.mutation_prob_gaussian:\n                    new_population[i] = self.mutate_gaussian(parent)\n                else:\n                    new_population[i] = self.mutate_cauchy(parent)\n\n                new_population[i] = self.correct_bounds(new_population[i])\n                \n                new_fitness = func(new_population[i])\n                self.budget -= 1\n\n                if new_fitness < fitness[i]:\n                    fitness[i] = new_fitness\n                    population[i] = new_population[i]\n\n                    if new_fitness < self.f_opt:\n                        self.f_opt = new_fitness\n                        self.x_opt = new_population[i]\n            \n            # Slime mold influence step\n            population = self.slime_mold_influence(population, fitness)\n            fitness = np.array([func(x) for x in population])\n            self.budget -= self.pop_size #Approximate cost of slime mold update, can remove if too costly\n            \n            # Replace worst individual with the best from previous generation (Elitism)\n            worst_index = np.argmax(fitness)\n            population[worst_index] = best_individual\n            fitness[worst_index] = best_fitness\n\n            # Adapt mutation scale\n            self.mutation_scale = 0.95 * self.mutation_scale\n\n            # Stagnation detection and population size adjustment\n            if len(self.best_fitness_history) > self.stagnation_threshold:\n                if abs(self.f_opt - self.best_fitness_history[-self.stagnation_threshold]) < 1e-5:\n                    self.stagnation_counter += self.pop_size\n                else:\n                    self.stagnation_counter = 0\n\n            self.best_fitness_history.append(self.f_opt)\n            self.adjust_population_size()\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "The algorithm HybridAdaptiveOptimizer scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["6c537388-4ccd-49e7-95d1-ae15369d9854", "19122a76-87b8-404d-86bd-2bda6f4e75cc"], "operator": "crossover", "metadata": {"llm_response": "# Description: A hybrid algorithm combining adaptive mutation with a rank-based slime mold influence, adjusting mutation scales and population diversity for effective exploration and exploitation.\n# Code: \n```python\nimport numpy as np\n\nclass HybridAdaptiveOptimizer:\n    def __init__(self, budget=10000, dim=10, pop_size=50, stagnation_threshold=1000, p_selection=0.2, initial_mutation_scale=1.0):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lb = -5.0\n        self.ub = 5.0\n        self.mutation_prob_gaussian = 0.5\n        self.mutation_prob_cauchy = 0.5\n        self.mutation_scale = initial_mutation_scale\n        self.stagnation_threshold = stagnation_threshold\n        self.stagnation_counter = 0\n        self.best_fitness_history = []\n        self.p_selection = p_selection # Percentage of top solutions for slime mold influence\n\n    def initialize_population(self):\n        return np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n\n    def calculate_diversity(self, population):\n        centroid = np.mean(population, axis=0)\n        distances = np.linalg.norm(population - centroid, axis=1)\n        return np.mean(distances)\n\n    def mutate_gaussian(self, x):\n        return x + self.mutation_scale * np.random.normal(0, 1, size=self.dim)\n\n    def mutate_cauchy(self, x):\n        return x + self.mutation_scale * np.random.standard_cauchy(size=self.dim)\n\n    def correct_bounds(self, x):\n        return np.clip(x, self.lb, self.ub)\n\n    def rank_based_selection(self, population, fitness, num_parents):\n        ranked_indices = np.argsort(fitness)\n        parent_indices = ranked_indices[:num_parents]\n        return population[parent_indices]\n    \n    def adjust_population_size(self):\n        if self.stagnation_counter > self.stagnation_threshold:\n            self.pop_size = int(self.pop_size * 0.9)  # Reduce population size if stagnating\n            if self.pop_size < 10:\n                self.pop_size = 10 # Minimum population size\n\n            self.mutation_scale = 1.0 # Reset mutation scale\n            self.stagnation_counter = 0 # Reset stagnation counter\n            #print(f\"Stagnation detected, reducing pop size to {self.pop_size}\")\n\n    def slime_mold_influence(self, population, fitness):\n        \"\"\"Introduces slime mold-inspired movement based on rank.\"\"\"\n        num_selected = int(self.p_selection * self.pop_size)\n        ranked_indices = np.argsort(fitness)\n        selected_indices = ranked_indices[:num_selected]\n        \n        for i in range(self.pop_size):\n            if np.random.rand() < 0.2:  # Probability to apply slime mold influence\n                chosen_leader_index = np.random.choice(selected_indices)\n                population[i] = 0.8 * population[i] + 0.2 * population[chosen_leader_index] + 0.01 * np.random.randn(self.dim)  # Move towards a leader with some noise\n                population[i] = self.correct_bounds(population[i])\n\n        return population\n\n    def __call__(self, func):\n        population = self.initialize_population()\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n        \n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index]\n        self.best_fitness_history.append(self.f_opt)\n\n        while self.budget > 0:\n            diversity = self.calculate_diversity(population)\n\n            if diversity > 1.0:\n                self.mutation_prob_gaussian = 0.8\n                self.mutation_prob_cauchy = 0.2\n            else:\n                self.mutation_prob_gaussian = 0.2\n                self.mutation_prob_cauchy = 0.8\n\n            # Elitism: Keep the best individual\n            best_individual = np.copy(self.x_opt)\n            best_fitness = self.f_opt\n\n            # Rank-based selection of parents for mutation\n            num_parents = int(self.pop_size / 2)  # Select top half as parents\n            parents = self.rank_based_selection(population, fitness, num_parents)\n            \n            new_population = np.copy(population)\n            for i in range(self.pop_size):\n                # Choose a parent randomly\n                parent = parents[np.random.randint(0, num_parents)]\n                \n                if np.random.rand() < self.mutation_prob_gaussian:\n                    new_population[i] = self.mutate_gaussian(parent)\n                else:\n                    new_population[i] = self.mutate_cauchy(parent)\n\n                new_population[i] = self.correct_bounds(new_population[i])\n                \n                new_fitness = func(new_population[i])\n                self.budget -= 1\n\n                if new_fitness < fitness[i]:\n                    fitness[i] = new_fitness\n                    population[i] = new_population[i]\n\n                    if new_fitness < self.f_opt:\n                        self.f_opt = new_fitness\n                        self.x_opt = new_population[i]\n            \n            # Slime mold influence step\n            population = self.slime_mold_influence(population, fitness)\n            fitness = np.array([func(x) for x in population])\n            self.budget -= self.pop_size #Approximate cost of slime mold update, can remove if too costly\n            \n            # Replace worst individual with the best from previous generation (Elitism)\n            worst_index = np.argmax(fitness)\n            population[worst_index] = best_individual\n            fitness[worst_index] = best_fitness\n\n            # Adapt mutation scale\n            self.mutation_scale = 0.95 * self.mutation_scale\n\n            # Stagnation detection and population size adjustment\n            if len(self.best_fitness_history) > self.stagnation_threshold:\n                if abs(self.f_opt - self.best_fitness_history[-self.stagnation_threshold]) < 1e-5:\n                    self.stagnation_counter += self.pop_size\n                else:\n                    self.stagnation_counter = 0\n\n            self.best_fitness_history.append(self.f_opt)\n            self.adjust_population_size()\n\n\n        return self.f_opt, self.x_opt\n```", "aucs": [0], "operator": "crossover", "theta_sampled": 0.0656649734006471, "reward": 0.0, "generation": 3}, "task_prompt": ""}
{"id": "b43c0c04-260d-479f-b678-193ed1a56638", "fitness": 0.4797748189308422, "name": "HybridAdaptiveSlime", "description": "Combines adaptive mutation with slime mold-inspired selection and population management for enhanced exploration and exploitation.", "code": "import numpy as np\n\nclass HybridAdaptiveSlime:\n    def __init__(self, budget=10000, dim=10, pop_size=50, stagnation_threshold=1000, p_selection=0.2, initial_mutation_scale=1.0):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lb = -5.0\n        self.ub = 5.0\n        self.mutation_prob_gaussian = 0.5\n        self.mutation_prob_cauchy = 0.5\n        self.mutation_scale = initial_mutation_scale\n        self.stagnation_threshold = stagnation_threshold\n        self.stagnation_counter = 0\n        self.best_fitness_history = []\n        self.p_selection = p_selection\n\n\n    def initialize_population(self):\n        return np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n\n    def calculate_diversity(self, population):\n        centroid = np.mean(population, axis=0)\n        distances = np.linalg.norm(population - centroid, axis=1)\n        return np.mean(distances)\n\n    def mutate_gaussian(self, x):\n        return x + self.mutation_scale * np.random.normal(0, 1, size=self.dim)\n\n    def mutate_cauchy(self, x):\n        return x + self.mutation_scale * np.random.standard_cauchy(size=self.dim)\n\n    def correct_bounds(self, x):\n        return np.clip(x, self.lb, self.ub)\n\n    def rank_based_selection(self, population, fitness, num_parents):\n        ranked_indices = np.argsort(fitness)\n        parent_indices = ranked_indices[:num_parents]\n        return population[parent_indices]\n\n    def slime_mold_selection(self, population, fitness):\n        \"\"\"Selects parents based on slime mold principles.\"\"\"\n        sorted_indices = np.argsort(fitness)\n        population = population[sorted_indices]\n        fitness = fitness[sorted_indices]\n\n        num_selected = int(self.p_selection * self.pop_size)\n        selected_population = population[:num_selected]\n\n        rank = np.arange(1, num_selected + 1)\n        weights_selected = np.exp(-rank / num_selected)\n        weights_selected /= np.sum(weights_selected)\n\n        return selected_population, weights_selected\n\n\n    def adjust_population_size(self):\n        if self.stagnation_counter > self.stagnation_threshold:\n            self.pop_size = int(self.pop_size * 0.9)\n            if self.pop_size < 10:\n                self.pop_size = 10\n            self.mutation_scale = self.initial_mutation_scale # Reset mutation scale\n            self.stagnation_counter = 0\n            print(f\"Stagnation detected, reducing pop size to {self.pop_size}\")\n\n\n    def __call__(self, func):\n        population = self.initialize_population()\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index]\n        self.best_fitness_history.append(self.f_opt)\n        self.initial_mutation_scale = self.mutation_scale #Initialize mutation scale\n\n        while self.budget > 0:\n            diversity = self.calculate_diversity(population)\n\n            if diversity > 1.0:\n                self.mutation_prob_gaussian = 0.8\n                self.mutation_prob_cauchy = 0.2\n            else:\n                self.mutation_prob_gaussian = 0.2\n                self.mutation_prob_cauchy = 0.8\n\n            # Elitism\n            best_individual = np.copy(self.x_opt)\n            best_fitness = self.f_opt\n\n            # Slime Mold inspired selection\n            selected_population, weights_selected = self.slime_mold_selection(population, fitness)\n\n            new_population = np.copy(population)\n            for i in range(self.pop_size):\n\n                #Probabilistic weighted averaging inspired by slime mold\n                if np.random.rand() < 0.5:\n                    x = np.zeros(self.dim)\n                    for j in range(len(selected_population)):\n                         x += weights_selected[j] * selected_population[j]\n                    # Add small random perturbation\n                    x += 0.1 * np.random.randn(self.dim)\n                    x = np.clip(x, self.lb, self.ub)\n                else: #Adaptive mutation\n                    if np.random.rand() < self.mutation_prob_gaussian:\n                        x = self.mutate_gaussian(population[i])\n                    else:\n                        x = self.mutate_cauchy(population[i])\n                    x = self.correct_bounds(x)\n\n                new_population[i] = x #Assign before evaluation\n                new_fitness = func(new_population[i])\n                self.budget -= 1\n\n                if new_fitness < fitness[i]:\n                    fitness[i] = new_fitness\n                    population[i] = new_population[i]\n\n                    if new_fitness < self.f_opt:\n                        self.f_opt = new_fitness\n                        self.x_opt = new_population[i]\n\n            worst_index = np.argmax(fitness)\n            population[worst_index] = best_individual\n            fitness[worst_index] = best_fitness\n\n            #Adapt mutation scale\n            self.mutation_scale = 0.95 * self.mutation_scale\n\n            if len(self.best_fitness_history) > self.stagnation_threshold:\n                if abs(self.f_opt - self.best_fitness_history[-self.stagnation_threshold]) < 1e-5:\n                    self.stagnation_counter += self.pop_size\n                else:\n                    self.stagnation_counter = 0\n\n            self.best_fitness_history.append(self.f_opt)\n            self.adjust_population_size()\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "The algorithm HybridAdaptiveSlime scored 0.480 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["6c537388-4ccd-49e7-95d1-ae15369d9854", "19122a76-87b8-404d-86bd-2bda6f4e75cc"], "operator": "crossover", "metadata": {"llm_response": "# Description: Combines adaptive mutation with slime mold-inspired selection and population management for enhanced exploration and exploitation.\n# Code: \n```python\nimport numpy as np\n\nclass HybridAdaptiveSlime:\n    def __init__(self, budget=10000, dim=10, pop_size=50, stagnation_threshold=1000, p_selection=0.2, initial_mutation_scale=1.0):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lb = -5.0\n        self.ub = 5.0\n        self.mutation_prob_gaussian = 0.5\n        self.mutation_prob_cauchy = 0.5\n        self.mutation_scale = initial_mutation_scale\n        self.stagnation_threshold = stagnation_threshold\n        self.stagnation_counter = 0\n        self.best_fitness_history = []\n        self.p_selection = p_selection\n\n\n    def initialize_population(self):\n        return np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n\n    def calculate_diversity(self, population):\n        centroid = np.mean(population, axis=0)\n        distances = np.linalg.norm(population - centroid, axis=1)\n        return np.mean(distances)\n\n    def mutate_gaussian(self, x):\n        return x + self.mutation_scale * np.random.normal(0, 1, size=self.dim)\n\n    def mutate_cauchy(self, x):\n        return x + self.mutation_scale * np.random.standard_cauchy(size=self.dim)\n\n    def correct_bounds(self, x):\n        return np.clip(x, self.lb, self.ub)\n\n    def rank_based_selection(self, population, fitness, num_parents):\n        ranked_indices = np.argsort(fitness)\n        parent_indices = ranked_indices[:num_parents]\n        return population[parent_indices]\n\n    def slime_mold_selection(self, population, fitness):\n        \"\"\"Selects parents based on slime mold principles.\"\"\"\n        sorted_indices = np.argsort(fitness)\n        population = population[sorted_indices]\n        fitness = fitness[sorted_indices]\n\n        num_selected = int(self.p_selection * self.pop_size)\n        selected_population = population[:num_selected]\n\n        rank = np.arange(1, num_selected + 1)\n        weights_selected = np.exp(-rank / num_selected)\n        weights_selected /= np.sum(weights_selected)\n\n        return selected_population, weights_selected\n\n\n    def adjust_population_size(self):\n        if self.stagnation_counter > self.stagnation_threshold:\n            self.pop_size = int(self.pop_size * 0.9)\n            if self.pop_size < 10:\n                self.pop_size = 10\n            self.mutation_scale = self.initial_mutation_scale # Reset mutation scale\n            self.stagnation_counter = 0\n            print(f\"Stagnation detected, reducing pop size to {self.pop_size}\")\n\n\n    def __call__(self, func):\n        population = self.initialize_population()\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index]\n        self.best_fitness_history.append(self.f_opt)\n        self.initial_mutation_scale = self.mutation_scale #Initialize mutation scale\n\n        while self.budget > 0:\n            diversity = self.calculate_diversity(population)\n\n            if diversity > 1.0:\n                self.mutation_prob_gaussian = 0.8\n                self.mutation_prob_cauchy = 0.2\n            else:\n                self.mutation_prob_gaussian = 0.2\n                self.mutation_prob_cauchy = 0.8\n\n            # Elitism\n            best_individual = np.copy(self.x_opt)\n            best_fitness = self.f_opt\n\n            # Slime Mold inspired selection\n            selected_population, weights_selected = self.slime_mold_selection(population, fitness)\n\n            new_population = np.copy(population)\n            for i in range(self.pop_size):\n\n                #Probabilistic weighted averaging inspired by slime mold\n                if np.random.rand() < 0.5:\n                    x = np.zeros(self.dim)\n                    for j in range(len(selected_population)):\n                         x += weights_selected[j] * selected_population[j]\n                    # Add small random perturbation\n                    x += 0.1 * np.random.randn(self.dim)\n                    x = np.clip(x, self.lb, self.ub)\n                else: #Adaptive mutation\n                    if np.random.rand() < self.mutation_prob_gaussian:\n                        x = self.mutate_gaussian(population[i])\n                    else:\n                        x = self.mutate_cauchy(population[i])\n                    x = self.correct_bounds(x)\n\n                new_population[i] = x #Assign before evaluation\n                new_fitness = func(new_population[i])\n                self.budget -= 1\n\n                if new_fitness < fitness[i]:\n                    fitness[i] = new_fitness\n                    population[i] = new_population[i]\n\n                    if new_fitness < self.f_opt:\n                        self.f_opt = new_fitness\n                        self.x_opt = new_population[i]\n\n            worst_index = np.argmax(fitness)\n            population[worst_index] = best_individual\n            fitness[worst_index] = best_fitness\n\n            #Adapt mutation scale\n            self.mutation_scale = 0.95 * self.mutation_scale\n\n            if len(self.best_fitness_history) > self.stagnation_threshold:\n                if abs(self.f_opt - self.best_fitness_history[-self.stagnation_threshold]) < 1e-5:\n                    self.stagnation_counter += self.pop_size\n                else:\n                    self.stagnation_counter = 0\n\n            self.best_fitness_history.append(self.f_opt)\n            self.adjust_population_size()\n\n\n        return self.f_opt, self.x_opt\n```", "aucs": [0.13470988526263428, 0.2265448746114862, 0.49646423998778777, 0.8104046792973393, 0.312611651196527, 0.6082739774586297, 0.3179091447297758, 0.46233420024077976, 0.5330917804197057, 0.23755356628228552, 0.8002879287015165, 0.9930472007500645, 0.25042593985560446, 0.5113670894685891, 0.7179641198988821, 0.346815303883071, 0.42606659207046327, 0.704965311611218, 0.20134735548974025, 0.5033115374007435], "operator": "crossover", "theta_sampled": 0.02987694194643527, "reward": 0.0, "generation": 3}, "task_prompt": ""}
{"id": "ddb255ad-fa53-4028-8bc5-cf4dcdf480e3", "fitness": 0.5282574644486615, "name": "HybridAdaptiveOptimizer", "description": "Combines adaptive mutation with a dynamic slime mold-inspired selection, adjusting population size and mutation scales based on stagnation and diversity.", "code": "import numpy as np\n\nclass HybridAdaptiveOptimizer:\n    def __init__(self, budget=10000, dim=10, pop_size=50, stagnation_threshold=1000, w_decay=0.9, p_selection=0.2, initial_mutation_scale=1.0):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lb = -5.0\n        self.ub = 5.0\n        self.mutation_prob_gaussian = 0.5\n        self.mutation_prob_cauchy = 0.5\n        self.mutation_scale = initial_mutation_scale\n        self.stagnation_threshold = stagnation_threshold\n        self.stagnation_counter = 0\n        self.best_fitness_history = []\n        self.w_decay = w_decay\n        self.p_selection = p_selection\n        self.slime_mold_phase = True # Start with Slime Mold phase\n        self.initial_pop_size = pop_size\n\n\n    def initialize_population(self):\n        return np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n\n    def calculate_diversity(self, population):\n        centroid = np.mean(population, axis=0)\n        distances = np.linalg.norm(population - centroid, axis=1)\n        return np.mean(distances)\n\n    def mutate_gaussian(self, x):\n        return x + self.mutation_scale * np.random.normal(0, 1, size=self.dim)\n\n    def mutate_cauchy(self, x):\n        return x + self.mutation_scale * np.random.standard_cauchy(size=self.dim)\n\n    def correct_bounds(self, x):\n        return np.clip(x, self.lb, self.ub)\n\n    def rank_based_selection(self, population, fitness, num_parents):\n        ranked_indices = np.argsort(fitness)\n        parent_indices = ranked_indices[:num_parents]\n        return population[parent_indices]\n    \n    def adjust_population_size(self):\n        if self.stagnation_counter > self.stagnation_threshold:\n            self.pop_size = int(self.pop_size * 0.9)  # Reduce population size if stagnating\n            if self.pop_size < 10:\n                self.pop_size = 10 # Minimum population size\n\n            self.mutation_scale = self.initial_mutation_scale # Reset mutation scale\n            self.stagnation_counter = 0 # Reset stagnation counter\n            print(f\"Stagnation detected, reducing pop size to {self.pop_size}\")\n        elif self.pop_size < self.initial_pop_size and len(self.best_fitness_history) > self.stagnation_threshold and abs(self.f_opt - self.best_fitness_history[-self.stagnation_threshold]) > 1e-5:\n            #Potentially increase the pop size if we are not stagnating. \n            self.pop_size = int(min(self.pop_size * 1.1, self.initial_pop_size))\n            print(f\"Progress detected, increasing pop size to {self.pop_size}\")\n\n    def slime_mold_selection(self, population, fitness):\n        \"\"\"Selects parents based on slime mold weighting.\"\"\"\n        sorted_indices = np.argsort(fitness)\n        population = population[sorted_indices]\n        fitness = fitness[sorted_indices]\n\n        num_selected = int(self.p_selection * self.pop_size)\n        selected_population = population[:num_selected]\n        selected_fitness = fitness[:num_selected]\n\n        rank = np.arange(1, num_selected + 1)\n        weights_selected = np.exp(-rank / num_selected)\n        weights_selected /= np.sum(weights_selected)\n        return selected_population, weights_selected\n    \n    def __call__(self, func):\n        population = self.initialize_population()\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n        \n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index]\n        self.best_fitness_history.append(self.f_opt)\n\n        while self.budget > 0:\n            diversity = self.calculate_diversity(population)\n\n            if diversity > 1.0:\n                self.mutation_prob_gaussian = 0.8\n                self.mutation_prob_cauchy = 0.2\n            else:\n                self.mutation_prob_gaussian = 0.2\n                self.mutation_prob_cauchy = 0.8\n\n            # Elitism: Keep the best individual\n            best_individual = np.copy(self.x_opt)\n            best_fitness = self.f_opt\n\n            # Slime mold inspired parent selection\n            parents, weights = self.slime_mold_selection(population, fitness)\n            num_parents = len(parents)\n            \n            new_population = np.copy(population)\n            for i in range(self.pop_size):\n                # Choose a parent based on slime mold weights\n                parent_index = np.random.choice(num_parents, p=weights)\n                parent = parents[parent_index]\n\n                if np.random.rand() < self.mutation_prob_gaussian:\n                    new_population[i] = self.mutate_gaussian(parent)\n                else:\n                    new_population[i] = self.mutate_cauchy(parent)\n\n                new_population[i] = self.correct_bounds(new_population[i])\n                \n                new_fitness = func(new_population[i])\n                self.budget -= 1\n\n                if new_fitness < fitness[i]:\n                    fitness[i] = new_fitness\n                    population[i] = new_population[i]\n\n                    if new_fitness < self.f_opt:\n                        self.f_opt = new_fitness\n                        self.x_opt = new_population[i]\n            \n            # Replace worst individual with the best from previous generation (Elitism)\n            worst_index = np.argmax(fitness)\n            population[worst_index] = best_individual\n            fitness[worst_index] = best_fitness\n\n            # Adapt mutation scale\n            self.mutation_scale = 0.95 * self.mutation_scale\n\n            # Stagnation detection and population size adjustment\n            if len(self.best_fitness_history) > self.stagnation_threshold:\n                if abs(self.f_opt - self.best_fitness_history[-self.stagnation_threshold]) < 1e-5:\n                    self.stagnation_counter += self.pop_size\n                else:\n                    self.stagnation_counter = 0\n\n            self.best_fitness_history.append(self.f_opt)\n            self.adjust_population_size()\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "The algorithm HybridAdaptiveOptimizer scored 0.528 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["6c537388-4ccd-49e7-95d1-ae15369d9854", "19122a76-87b8-404d-86bd-2bda6f4e75cc"], "operator": "crossover", "metadata": {"llm_response": "# Description: Combines adaptive mutation with a dynamic slime mold-inspired selection, adjusting population size and mutation scales based on stagnation and diversity.\n# Code: \n```python\nimport numpy as np\n\nclass HybridAdaptiveOptimizer:\n    def __init__(self, budget=10000, dim=10, pop_size=50, stagnation_threshold=1000, w_decay=0.9, p_selection=0.2, initial_mutation_scale=1.0):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lb = -5.0\n        self.ub = 5.0\n        self.mutation_prob_gaussian = 0.5\n        self.mutation_prob_cauchy = 0.5\n        self.mutation_scale = initial_mutation_scale\n        self.stagnation_threshold = stagnation_threshold\n        self.stagnation_counter = 0\n        self.best_fitness_history = []\n        self.w_decay = w_decay\n        self.p_selection = p_selection\n        self.slime_mold_phase = True # Start with Slime Mold phase\n        self.initial_pop_size = pop_size\n\n\n    def initialize_population(self):\n        return np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n\n    def calculate_diversity(self, population):\n        centroid = np.mean(population, axis=0)\n        distances = np.linalg.norm(population - centroid, axis=1)\n        return np.mean(distances)\n\n    def mutate_gaussian(self, x):\n        return x + self.mutation_scale * np.random.normal(0, 1, size=self.dim)\n\n    def mutate_cauchy(self, x):\n        return x + self.mutation_scale * np.random.standard_cauchy(size=self.dim)\n\n    def correct_bounds(self, x):\n        return np.clip(x, self.lb, self.ub)\n\n    def rank_based_selection(self, population, fitness, num_parents):\n        ranked_indices = np.argsort(fitness)\n        parent_indices = ranked_indices[:num_parents]\n        return population[parent_indices]\n    \n    def adjust_population_size(self):\n        if self.stagnation_counter > self.stagnation_threshold:\n            self.pop_size = int(self.pop_size * 0.9)  # Reduce population size if stagnating\n            if self.pop_size < 10:\n                self.pop_size = 10 # Minimum population size\n\n            self.mutation_scale = self.initial_mutation_scale # Reset mutation scale\n            self.stagnation_counter = 0 # Reset stagnation counter\n            print(f\"Stagnation detected, reducing pop size to {self.pop_size}\")\n        elif self.pop_size < self.initial_pop_size and len(self.best_fitness_history) > self.stagnation_threshold and abs(self.f_opt - self.best_fitness_history[-self.stagnation_threshold]) > 1e-5:\n            #Potentially increase the pop size if we are not stagnating. \n            self.pop_size = int(min(self.pop_size * 1.1, self.initial_pop_size))\n            print(f\"Progress detected, increasing pop size to {self.pop_size}\")\n\n    def slime_mold_selection(self, population, fitness):\n        \"\"\"Selects parents based on slime mold weighting.\"\"\"\n        sorted_indices = np.argsort(fitness)\n        population = population[sorted_indices]\n        fitness = fitness[sorted_indices]\n\n        num_selected = int(self.p_selection * self.pop_size)\n        selected_population = population[:num_selected]\n        selected_fitness = fitness[:num_selected]\n\n        rank = np.arange(1, num_selected + 1)\n        weights_selected = np.exp(-rank / num_selected)\n        weights_selected /= np.sum(weights_selected)\n        return selected_population, weights_selected\n    \n    def __call__(self, func):\n        population = self.initialize_population()\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n        \n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index]\n        self.best_fitness_history.append(self.f_opt)\n\n        while self.budget > 0:\n            diversity = self.calculate_diversity(population)\n\n            if diversity > 1.0:\n                self.mutation_prob_gaussian = 0.8\n                self.mutation_prob_cauchy = 0.2\n            else:\n                self.mutation_prob_gaussian = 0.2\n                self.mutation_prob_cauchy = 0.8\n\n            # Elitism: Keep the best individual\n            best_individual = np.copy(self.x_opt)\n            best_fitness = self.f_opt\n\n            # Slime mold inspired parent selection\n            parents, weights = self.slime_mold_selection(population, fitness)\n            num_parents = len(parents)\n            \n            new_population = np.copy(population)\n            for i in range(self.pop_size):\n                # Choose a parent based on slime mold weights\n                parent_index = np.random.choice(num_parents, p=weights)\n                parent = parents[parent_index]\n\n                if np.random.rand() < self.mutation_prob_gaussian:\n                    new_population[i] = self.mutate_gaussian(parent)\n                else:\n                    new_population[i] = self.mutate_cauchy(parent)\n\n                new_population[i] = self.correct_bounds(new_population[i])\n                \n                new_fitness = func(new_population[i])\n                self.budget -= 1\n\n                if new_fitness < fitness[i]:\n                    fitness[i] = new_fitness\n                    population[i] = new_population[i]\n\n                    if new_fitness < self.f_opt:\n                        self.f_opt = new_fitness\n                        self.x_opt = new_population[i]\n            \n            # Replace worst individual with the best from previous generation (Elitism)\n            worst_index = np.argmax(fitness)\n            population[worst_index] = best_individual\n            fitness[worst_index] = best_fitness\n\n            # Adapt mutation scale\n            self.mutation_scale = 0.95 * self.mutation_scale\n\n            # Stagnation detection and population size adjustment\n            if len(self.best_fitness_history) > self.stagnation_threshold:\n                if abs(self.f_opt - self.best_fitness_history[-self.stagnation_threshold]) < 1e-5:\n                    self.stagnation_counter += self.pop_size\n                else:\n                    self.stagnation_counter = 0\n\n            self.best_fitness_history.append(self.f_opt)\n            self.adjust_population_size()\n\n\n        return self.f_opt, self.x_opt\n```", "aucs": [0.22498913543383758, 0.22627030371692203, 0.5486292943076696, 0.8318821755595354, 0.5361692767457433, 0.6098712502935169, 0.3114790654719193, 0.48374178318142846, 0.5540509636244744, 0.5030649531996683, 0.7941454315565135, 0.9974115606038982, 0.28786808449064916, 0.43174276505605735, 0.782528204160168, 0.59327627259906, 0.3221155930657783, 0.7258036765634236, 0.2955406659318296, 0.5045688334111404], "operator": "crossover", "theta_sampled": 0.0314140070282935, "reward": 0.0, "generation": 3}, "task_prompt": ""}
{"id": "f0fc86fa-9f10-48e5-b92c-a4d1f243a829", "fitness": 0.4371963503836415, "name": "HybridAdaptiveOptimizer", "description": "A hybrid algorithm combining adaptive mutation with slime mold optimization, dynamically adjusting search behavior based on population diversity and stagnation detection.", "code": "import numpy as np\n\nclass HybridAdaptiveOptimizer:\n    def __init__(self, budget=10000, dim=10, pop_size=40, stagnation_threshold=750, p_selection=0.2, initial_mutation_scale=1.0):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lb = -5.0\n        self.ub = 5.0\n        self.mutation_prob_gaussian = 0.5\n        self.mutation_prob_cauchy = 0.5\n        self.mutation_scale = initial_mutation_scale\n        self.stagnation_threshold = stagnation_threshold\n        self.stagnation_counter = 0\n        self.best_fitness_history = []\n        self.p_selection = p_selection\n        self.slime_mold_weight = 0.5  # Initial weight for slime mold influence\n        self.w_decay = 0.95\n\n    def initialize_population(self):\n        return np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n\n    def calculate_diversity(self, population):\n        centroid = np.mean(population, axis=0)\n        distances = np.linalg.norm(population - centroid, axis=1)\n        return np.mean(distances)\n\n    def mutate_gaussian(self, x):\n        return x + self.mutation_scale * np.random.normal(0, 1, size=self.dim)\n\n    def mutate_cauchy(self, x):\n        return x + self.mutation_scale * np.random.standard_cauchy(size=self.dim)\n\n    def correct_bounds(self, x):\n        return np.clip(x, self.lb, self.ub)\n\n    def rank_based_selection(self, population, fitness, num_parents):\n        ranked_indices = np.argsort(fitness)\n        parent_indices = ranked_indices[:num_parents]\n        return population[parent_indices]\n    \n    def adjust_mutation_scale(self):\n         self.mutation_scale = 0.98 * self.mutation_scale\n    \n    def slime_mold_influence(self, population, fitness):\n        \"\"\"Apply slime mold influence to a portion of the population.\"\"\"\n        num_selected = int(self.p_selection * self.pop_size)\n        sorted_indices = np.argsort(fitness)\n        selected_population = population[sorted_indices[:num_selected]]\n\n        rank = np.arange(1, num_selected + 1)\n        weights_selected = np.exp(-rank / num_selected)\n        weights_selected /= np.sum(weights_selected)\n\n        new_individuals = []\n        for i in range(self.pop_size):\n            new_x = np.zeros(self.dim)\n            for j in range(num_selected):\n                new_x += weights_selected[j] * selected_population[j]\n            new_individuals.append(new_x)\n        return np.array(new_individuals)\n\n    def __call__(self, func):\n        population = self.initialize_population()\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n        \n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index]\n        self.best_fitness_history.append(self.f_opt)\n\n        while self.budget > 0:\n            diversity = self.calculate_diversity(population)\n\n            if diversity > 1.0:\n                self.mutation_prob_gaussian = 0.7\n                self.mutation_prob_cauchy = 0.3\n            else:\n                self.mutation_prob_gaussian = 0.3\n                self.mutation_prob_cauchy = 0.7\n\n            # Elitism: Keep the best individual\n            best_individual = np.copy(self.x_opt)\n            best_fitness = self.f_opt\n\n            # Rank-based selection of parents for mutation\n            num_parents = int(self.pop_size / 2)  # Select top half as parents\n            parents = self.rank_based_selection(population, fitness, num_parents)\n            \n            new_population = np.copy(population)\n            for i in range(self.pop_size):\n                # Choose a parent randomly\n                parent = parents[np.random.randint(0, num_parents)]\n                \n                if np.random.rand() < self.mutation_prob_gaussian:\n                    new_population[i] = self.mutate_gaussian(parent)\n                else:\n                    new_population[i] = self.mutate_cauchy(parent)\n\n                new_population[i] = self.correct_bounds(new_population[i])\n                \n                new_fitness = func(new_population[i])\n                self.budget -= 1\n\n                if new_fitness < fitness[i]:\n                    fitness[i] = new_fitness\n                    population[i] = new_population[i]\n\n                    if new_fitness < self.f_opt:\n                        self.f_opt = new_fitness\n                        self.x_opt = new_population[i]\n            \n            # Apply Slime Mold Influence\n            if np.random.rand() < self.slime_mold_weight:\n                slime_mold_individuals = self.slime_mold_influence(population, fitness)\n                for i in range(self.pop_size):\n                    population[i] = self.slime_mold_weight * slime_mold_individuals[i] + (1 - self.slime_mold_weight) * population[i]\n                    population[i] = self.correct_bounds(population[i])\n                    fitness[i] = func(population[i])  # Evaluate the modified individual\n                    self.budget -= 1\n                    if fitness[i] < self.f_opt:\n                        self.f_opt = fitness[i]\n                        self.x_opt = population[i]\n\n            # Replace worst individual with the best from previous generation (Elitism)\n            worst_index = np.argmax(fitness)\n            population[worst_index] = best_individual\n            fitness[worst_index] = best_fitness\n\n            # Adapt mutation scale\n            self.adjust_mutation_scale()\n            self.slime_mold_weight *= self.w_decay\n\n            # Stagnation detection and mutation scale adjustment\n            if len(self.best_fitness_history) > self.stagnation_threshold:\n                if abs(self.f_opt - self.best_fitness_history[-self.stagnation_threshold]) < 1e-5:\n                    self.stagnation_counter += 1\n                    if self.stagnation_counter > self.stagnation_threshold:\n                        self.mutation_scale = self.initial_mutation_scale #reset mutation scale\n                        self.stagnation_counter = 0\n                else:\n                    self.stagnation_counter = 0\n\n            self.best_fitness_history.append(self.f_opt)\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "The algorithm HybridAdaptiveOptimizer scored 0.437 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["6c537388-4ccd-49e7-95d1-ae15369d9854", "19122a76-87b8-404d-86bd-2bda6f4e75cc"], "operator": "crossover", "metadata": {"llm_response": "# Description: A hybrid algorithm combining adaptive mutation with slime mold optimization, dynamically adjusting search behavior based on population diversity and stagnation detection.\n# Code: \n```python\nimport numpy as np\n\nclass HybridAdaptiveOptimizer:\n    def __init__(self, budget=10000, dim=10, pop_size=40, stagnation_threshold=750, p_selection=0.2, initial_mutation_scale=1.0):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lb = -5.0\n        self.ub = 5.0\n        self.mutation_prob_gaussian = 0.5\n        self.mutation_prob_cauchy = 0.5\n        self.mutation_scale = initial_mutation_scale\n        self.stagnation_threshold = stagnation_threshold\n        self.stagnation_counter = 0\n        self.best_fitness_history = []\n        self.p_selection = p_selection\n        self.slime_mold_weight = 0.5  # Initial weight for slime mold influence\n        self.w_decay = 0.95\n\n    def initialize_population(self):\n        return np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n\n    def calculate_diversity(self, population):\n        centroid = np.mean(population, axis=0)\n        distances = np.linalg.norm(population - centroid, axis=1)\n        return np.mean(distances)\n\n    def mutate_gaussian(self, x):\n        return x + self.mutation_scale * np.random.normal(0, 1, size=self.dim)\n\n    def mutate_cauchy(self, x):\n        return x + self.mutation_scale * np.random.standard_cauchy(size=self.dim)\n\n    def correct_bounds(self, x):\n        return np.clip(x, self.lb, self.ub)\n\n    def rank_based_selection(self, population, fitness, num_parents):\n        ranked_indices = np.argsort(fitness)\n        parent_indices = ranked_indices[:num_parents]\n        return population[parent_indices]\n    \n    def adjust_mutation_scale(self):\n         self.mutation_scale = 0.98 * self.mutation_scale\n    \n    def slime_mold_influence(self, population, fitness):\n        \"\"\"Apply slime mold influence to a portion of the population.\"\"\"\n        num_selected = int(self.p_selection * self.pop_size)\n        sorted_indices = np.argsort(fitness)\n        selected_population = population[sorted_indices[:num_selected]]\n\n        rank = np.arange(1, num_selected + 1)\n        weights_selected = np.exp(-rank / num_selected)\n        weights_selected /= np.sum(weights_selected)\n\n        new_individuals = []\n        for i in range(self.pop_size):\n            new_x = np.zeros(self.dim)\n            for j in range(num_selected):\n                new_x += weights_selected[j] * selected_population[j]\n            new_individuals.append(new_x)\n        return np.array(new_individuals)\n\n    def __call__(self, func):\n        population = self.initialize_population()\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n        \n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index]\n        self.best_fitness_history.append(self.f_opt)\n\n        while self.budget > 0:\n            diversity = self.calculate_diversity(population)\n\n            if diversity > 1.0:\n                self.mutation_prob_gaussian = 0.7\n                self.mutation_prob_cauchy = 0.3\n            else:\n                self.mutation_prob_gaussian = 0.3\n                self.mutation_prob_cauchy = 0.7\n\n            # Elitism: Keep the best individual\n            best_individual = np.copy(self.x_opt)\n            best_fitness = self.f_opt\n\n            # Rank-based selection of parents for mutation\n            num_parents = int(self.pop_size / 2)  # Select top half as parents\n            parents = self.rank_based_selection(population, fitness, num_parents)\n            \n            new_population = np.copy(population)\n            for i in range(self.pop_size):\n                # Choose a parent randomly\n                parent = parents[np.random.randint(0, num_parents)]\n                \n                if np.random.rand() < self.mutation_prob_gaussian:\n                    new_population[i] = self.mutate_gaussian(parent)\n                else:\n                    new_population[i] = self.mutate_cauchy(parent)\n\n                new_population[i] = self.correct_bounds(new_population[i])\n                \n                new_fitness = func(new_population[i])\n                self.budget -= 1\n\n                if new_fitness < fitness[i]:\n                    fitness[i] = new_fitness\n                    population[i] = new_population[i]\n\n                    if new_fitness < self.f_opt:\n                        self.f_opt = new_fitness\n                        self.x_opt = new_population[i]\n            \n            # Apply Slime Mold Influence\n            if np.random.rand() < self.slime_mold_weight:\n                slime_mold_individuals = self.slime_mold_influence(population, fitness)\n                for i in range(self.pop_size):\n                    population[i] = self.slime_mold_weight * slime_mold_individuals[i] + (1 - self.slime_mold_weight) * population[i]\n                    population[i] = self.correct_bounds(population[i])\n                    fitness[i] = func(population[i])  # Evaluate the modified individual\n                    self.budget -= 1\n                    if fitness[i] < self.f_opt:\n                        self.f_opt = fitness[i]\n                        self.x_opt = population[i]\n\n            # Replace worst individual with the best from previous generation (Elitism)\n            worst_index = np.argmax(fitness)\n            population[worst_index] = best_individual\n            fitness[worst_index] = best_fitness\n\n            # Adapt mutation scale\n            self.adjust_mutation_scale()\n            self.slime_mold_weight *= self.w_decay\n\n            # Stagnation detection and mutation scale adjustment\n            if len(self.best_fitness_history) > self.stagnation_threshold:\n                if abs(self.f_opt - self.best_fitness_history[-self.stagnation_threshold]) < 1e-5:\n                    self.stagnation_counter += 1\n                    if self.stagnation_counter > self.stagnation_threshold:\n                        self.mutation_scale = self.initial_mutation_scale #reset mutation scale\n                        self.stagnation_counter = 0\n                else:\n                    self.stagnation_counter = 0\n\n            self.best_fitness_history.append(self.f_opt)\n\n\n        return self.f_opt, self.x_opt\n```", "aucs": [0.19764364454552485, 0.2285176991584671, 0.4340745701730909, 0.6743149324304542, 0.36654719304906047, 0.4435629439387855, 0.30309521885303625, 0.3557122642955165, 0.3727743388576128, 0.31840721037101816, 0.6230591122741058, 0.9915110521653723, 0.29043993372710264, 0.3885833175095934, 0.7819712699262216, 0.3549612080470136, 0.3716073536549569, 0.5223760929094254, 0.20834624423833603, 0.5164214075481343], "operator": "crossover", "theta_sampled": 0.017317599669264768, "reward": 0.0, "generation": 3}, "task_prompt": ""}
{"id": "02ef52c2-a68e-480d-98c2-1ff73f542f50", "fitness": 0.33987708668309435, "name": "AdaptiveHarmonySearch", "description": "Harmony Search with adaptive parameters, dynamic bandwidth, and a diversity-enhancing mutation strategy.", "code": "import numpy as np\n\nclass AdaptiveHarmonySearch:\n    def __init__(self, budget=10000, dim=10, HMCR=0.9, PAR=0.3, BW=1.0, HM_size=10):\n        self.budget = budget\n        self.dim = dim\n        self.HMCR = HMCR  # Harmony Memory Consideration Rate\n        self.PAR = PAR  # Pitch Adjustment Rate\n        self.BW = BW  # Bandwidth (initial value)\n        self.HM_size = HM_size # Harmony Memory size\n        self.HMCR_decay = 0.999 # Decay rate for HMCR\n        self.PAR_decay = 0.999 # Decay rate for PAR\n        self.BW_min = 0.0001 #Minimum bandwidth\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        \n        # Initialize Harmony Memory (HM)\n        HM = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.HM_size, self.dim))  # HM size: 10 harmonies\n        HM_fitness = np.array([func(HM[i]) for i in range(self.HM_size)])\n        \n        eval_count = self.HM_size  # Initialized HM requires evaluations\n\n        while eval_count < self.budget:\n            # Improvise a new harmony\n            new_harmony = np.zeros(self.dim)\n            for i in range(self.dim):\n                if np.random.rand() < self.HMCR:\n                    # Memory Consideration\n                    new_harmony[i] = HM[np.random.randint(0, self.HM_size), i]\n                    # Pitch Adjustment\n                    if np.random.rand() < self.PAR:\n                        new_harmony[i] += np.random.uniform(-self.BW, self.BW)\n                        new_harmony[i] = np.clip(new_harmony[i], func.bounds.lb[i], func.bounds.ub[i]) #clip each dimension\n                else:\n                    # Random Exploration\n                    new_harmony[i] = np.random.uniform(func.bounds.lb[i], func.bounds.ub[i])  # Access lb and ub by index\n            \n            # Apply a diversity-enhancing mutation\n            if np.random.rand() < 0.1: # 10% chance of mutation\n                mutation_dim = np.random.randint(0, self.dim)\n                new_harmony[mutation_dim] = np.random.uniform(func.bounds.lb[mutation_dim], func.bounds.ub[mutation_dim])\n\n            new_fitness = func(new_harmony)\n            eval_count += 1\n\n            # Update Harmony Memory\n            worst_index = np.argmax(HM_fitness)\n            if new_fitness < HM_fitness[worst_index]:\n                HM[worst_index] = new_harmony\n                HM_fitness[worst_index] = new_fitness\n\n            # Update best solution found so far\n            best_index = np.argmin(HM_fitness)\n            if HM_fitness[best_index] < self.f_opt:\n                self.f_opt = HM_fitness[best_index]\n                self.x_opt = HM[best_index]\n            \n            # Adaptive parameter control\n            self.HMCR *= self.HMCR_decay\n            self.PAR *= self.PAR_decay\n            self.BW = max(self.BW * self.HMCR_decay, self.BW_min) #BW decreases more aggressively when HMCR is low\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "The algorithm AdaptiveHarmonySearch scored 0.340 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["56ed699f-9a0a-4575-a57d-d7099e29a214"], "operator": "mutation", "metadata": {"llm_response": "# Description: Harmony Search with adaptive parameters, dynamic bandwidth, and a diversity-enhancing mutation strategy.\n# Code:\n```python\nimport numpy as np\n\nclass AdaptiveHarmonySearch:\n    def __init__(self, budget=10000, dim=10, HMCR=0.9, PAR=0.3, BW=1.0, HM_size=10):\n        self.budget = budget\n        self.dim = dim\n        self.HMCR = HMCR  # Harmony Memory Consideration Rate\n        self.PAR = PAR  # Pitch Adjustment Rate\n        self.BW = BW  # Bandwidth (initial value)\n        self.HM_size = HM_size # Harmony Memory size\n        self.HMCR_decay = 0.999 # Decay rate for HMCR\n        self.PAR_decay = 0.999 # Decay rate for PAR\n        self.BW_min = 0.0001 #Minimum bandwidth\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        \n        # Initialize Harmony Memory (HM)\n        HM = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.HM_size, self.dim))  # HM size: 10 harmonies\n        HM_fitness = np.array([func(HM[i]) for i in range(self.HM_size)])\n        \n        eval_count = self.HM_size  # Initialized HM requires evaluations\n\n        while eval_count < self.budget:\n            # Improvise a new harmony\n            new_harmony = np.zeros(self.dim)\n            for i in range(self.dim):\n                if np.random.rand() < self.HMCR:\n                    # Memory Consideration\n                    new_harmony[i] = HM[np.random.randint(0, self.HM_size), i]\n                    # Pitch Adjustment\n                    if np.random.rand() < self.PAR:\n                        new_harmony[i] += np.random.uniform(-self.BW, self.BW)\n                        new_harmony[i] = np.clip(new_harmony[i], func.bounds.lb[i], func.bounds.ub[i]) #clip each dimension\n                else:\n                    # Random Exploration\n                    new_harmony[i] = np.random.uniform(func.bounds.lb[i], func.bounds.ub[i])  # Access lb and ub by index\n            \n            # Apply a diversity-enhancing mutation\n            if np.random.rand() < 0.1: # 10% chance of mutation\n                mutation_dim = np.random.randint(0, self.dim)\n                new_harmony[mutation_dim] = np.random.uniform(func.bounds.lb[mutation_dim], func.bounds.ub[mutation_dim])\n\n            new_fitness = func(new_harmony)\n            eval_count += 1\n\n            # Update Harmony Memory\n            worst_index = np.argmax(HM_fitness)\n            if new_fitness < HM_fitness[worst_index]:\n                HM[worst_index] = new_harmony\n                HM_fitness[worst_index] = new_fitness\n\n            # Update best solution found so far\n            best_index = np.argmin(HM_fitness)\n            if HM_fitness[best_index] < self.f_opt:\n                self.f_opt = HM_fitness[best_index]\n                self.x_opt = HM[best_index]\n            \n            # Adaptive parameter control\n            self.HMCR *= self.HMCR_decay\n            self.PAR *= self.PAR_decay\n            self.BW = max(self.BW * self.HMCR_decay, self.BW_min) #BW decreases more aggressively when HMCR is low\n\n        return self.f_opt, self.x_opt\n```", "aucs": [0.12211052299876046, 0.18112203895167622, 0.33374430955069645, 0.35125653217133723, 0.23521224541834673, 0.37356227131211495, 0.2738309762108063, 0.2650728437362836, 0.25032457425672094, 0.183157792233425, 0.26695359814836395, 0.9963581503575922, 0.266479608014368, 0.27427191475803503, 0.7163020854587627, 0.3341542778950476, 0.2814839984447173, 0.43507868351425805, 0.1631770782774029, 0.4938882319531708], "operator": "mutation", "theta_sampled": 0.03816956863084347, "reward": 0.0, "generation": 3}, "task_prompt": ""}
{"id": "e8a11899-492c-463b-9412-67910c8f787b", "fitness": 0.5719738040293911, "name": "ShrinkingCauchyOptimizer", "description": "A novel algorithm that uses a population-based approach with Cauchy mutation and a shrinking search space based on the best-performing individuals.", "code": "import numpy as np\n\nclass ShrinkingCauchyOptimizer:\n    def __init__(self, budget=10000, dim=10, population_size=20, shrink_factor=0.95):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = population_size\n        self.shrink_factor = shrink_factor\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        population = np.random.uniform(lb, ub, size=(self.population_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.population_size\n\n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index]\n\n        search_range = ub - lb\n        center = self.x_opt\n\n        while self.budget > 0:\n            # Cauchy mutation around the best individual\n            for i in range(self.population_size):\n                if self.budget <= 0:\n                    break\n                mutation = center + search_range * np.random.standard_cauchy(size=self.dim) * 0.1  # Scale Cauchy distribution\n                mutation = np.clip(mutation, lb, ub)  # Clip values to stay within bounds\n\n                f_mutation = func(mutation)\n                self.budget -= 1\n\n                if f_mutation < fitness[i]:\n                    fitness[i] = f_mutation\n                    population[i] = mutation\n\n                    if f_mutation < self.f_opt:\n                        self.f_opt = f_mutation\n                        self.x_opt = mutation\n                        center = self.x_opt\n\n            # Shrink the search range around the best individual\n            search_range *= self.shrink_factor\n            lb = np.clip(center - search_range/2, func.bounds.lb, func.bounds.ub)\n            ub = np.clip(center + search_range/2, func.bounds.lb, func.bounds.ub)\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "The algorithm ShrinkingCauchyOptimizer scored 0.572 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": "random_new", "metadata": {"llm_response": "# Description: A novel algorithm that uses a population-based approach with Cauchy mutation and a shrinking search space based on the best-performing individuals.\n# Code: \n```python\nimport numpy as np\n\nclass ShrinkingCauchyOptimizer:\n    def __init__(self, budget=10000, dim=10, population_size=20, shrink_factor=0.95):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = population_size\n        self.shrink_factor = shrink_factor\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        population = np.random.uniform(lb, ub, size=(self.population_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.population_size\n\n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index]\n\n        search_range = ub - lb\n        center = self.x_opt\n\n        while self.budget > 0:\n            # Cauchy mutation around the best individual\n            for i in range(self.population_size):\n                if self.budget <= 0:\n                    break\n                mutation = center + search_range * np.random.standard_cauchy(size=self.dim) * 0.1  # Scale Cauchy distribution\n                mutation = np.clip(mutation, lb, ub)  # Clip values to stay within bounds\n\n                f_mutation = func(mutation)\n                self.budget -= 1\n\n                if f_mutation < fitness[i]:\n                    fitness[i] = f_mutation\n                    population[i] = mutation\n\n                    if f_mutation < self.f_opt:\n                        self.f_opt = f_mutation\n                        self.x_opt = mutation\n                        center = self.x_opt\n\n            # Shrink the search range around the best individual\n            search_range *= self.shrink_factor\n            lb = np.clip(center - search_range/2, func.bounds.lb, func.bounds.ub)\n            ub = np.clip(center + search_range/2, func.bounds.lb, func.bounds.ub)\n\n\n        return self.f_opt, self.x_opt\n```", "aucs": [0.20362328481796976, 0.2093555318093241, 0.7931797297982941, 0.9309072579186833, 0.8053719771612118, 0.8310843120926139, 0.3029913315195466, 0.5006030651376522, 0.33083675289422654, 0.2072435578376346, 0.9002074418199252, 0.9982109976479571, 0.31617769715647515, 0.40187321333783843, 0.8988237581893873, 0.8233274245660944, 0.4050090592296498, 0.8773926525392262, 0.19039635776995611, 0.5128606773441573], "operator": "random_new", "theta_sampled": 0.0068235389543569065, "reward": 0.04055002064504665, "generation": 3}, "task_prompt": ""}
{"id": "af686c9f-4eaa-44f3-9a84-d20f8e1cdff4", "fitness": 0.37076949703618356, "name": "WeightedNeighborhoodSearch", "description": "Population-based algorithm where individuals move towards a weighted average of the best solution and a random neighbor, with adaptive exploration based on population diversity.", "code": "import numpy as np\n\nclass WeightedNeighborhoodSearch:\n    def __init__(self, budget=10000, dim=10, population_size=20, weight_best=0.6, weight_neighbor=0.4, initial_exploration=0.5, exploration_decay=0.995):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = population_size\n        self.weight_best = weight_best\n        self.weight_neighbor = weight_neighbor\n        self.initial_exploration = initial_exploration\n        self.exploration_decay = exploration_decay\n\n    def __call__(self, func):\n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.population_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.population_size\n\n        # Find initial best\n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index]\n\n        exploration_rate = self.initial_exploration\n\n        while self.budget > 0:\n            new_population = np.zeros_like(population)\n            new_fitness = np.zeros_like(fitness)\n\n            for i in range(self.population_size):\n                # Select a random neighbor (excluding self)\n                neighbors = np.arange(self.population_size)\n                neighbors = neighbors[neighbors != i]\n                neighbor_index = np.random.choice(neighbors)\n\n                # Create a weighted average direction\n                direction = (self.weight_best * (self.x_opt - population[i]) +\n                             self.weight_neighbor * (population[neighbor_index] - population[i]))\n\n                # Add exploration noise\n                if np.random.rand() < exploration_rate:\n                    exploration_noise = np.random.uniform(-1, 1, size=self.dim)\n                    direction += exploration_rate * exploration_noise\n\n                # Move the individual\n                new_x = population[i] + direction\n                new_x = np.clip(new_x, func.bounds.lb, func.bounds.ub)  # Clip to bounds\n\n                new_f = func(new_x)\n                self.budget -= 1\n\n                if new_f < fitness[i]:\n                    new_population[i] = new_x\n                    new_fitness[i] = new_f\n                else:\n                    new_population[i] = population[i]\n                    new_fitness[i] = fitness[i]\n\n                # Update best if necessary\n                if new_f < self.f_opt:\n                    self.f_opt = new_f\n                    self.x_opt = new_x\n            \n            population = new_population\n            fitness = new_fitness\n\n            # Decay the exploration rate\n            exploration_rate *= self.exploration_decay\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "The algorithm WeightedNeighborhoodSearch scored 0.371 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": "random_new", "metadata": {"llm_response": "# Description: Population-based algorithm where individuals move towards a weighted average of the best solution and a random neighbor, with adaptive exploration based on population diversity.\n# Code: \n```python\nimport numpy as np\n\nclass WeightedNeighborhoodSearch:\n    def __init__(self, budget=10000, dim=10, population_size=20, weight_best=0.6, weight_neighbor=0.4, initial_exploration=0.5, exploration_decay=0.995):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = population_size\n        self.weight_best = weight_best\n        self.weight_neighbor = weight_neighbor\n        self.initial_exploration = initial_exploration\n        self.exploration_decay = exploration_decay\n\n    def __call__(self, func):\n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.population_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.population_size\n\n        # Find initial best\n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index]\n\n        exploration_rate = self.initial_exploration\n\n        while self.budget > 0:\n            new_population = np.zeros_like(population)\n            new_fitness = np.zeros_like(fitness)\n\n            for i in range(self.population_size):\n                # Select a random neighbor (excluding self)\n                neighbors = np.arange(self.population_size)\n                neighbors = neighbors[neighbors != i]\n                neighbor_index = np.random.choice(neighbors)\n\n                # Create a weighted average direction\n                direction = (self.weight_best * (self.x_opt - population[i]) +\n                             self.weight_neighbor * (population[neighbor_index] - population[i]))\n\n                # Add exploration noise\n                if np.random.rand() < exploration_rate:\n                    exploration_noise = np.random.uniform(-1, 1, size=self.dim)\n                    direction += exploration_rate * exploration_noise\n\n                # Move the individual\n                new_x = population[i] + direction\n                new_x = np.clip(new_x, func.bounds.lb, func.bounds.ub)  # Clip to bounds\n\n                new_f = func(new_x)\n                self.budget -= 1\n\n                if new_f < fitness[i]:\n                    new_population[i] = new_x\n                    new_fitness[i] = new_f\n                else:\n                    new_population[i] = population[i]\n                    new_fitness[i] = fitness[i]\n\n                # Update best if necessary\n                if new_f < self.f_opt:\n                    self.f_opt = new_f\n                    self.x_opt = new_x\n            \n            population = new_population\n            fitness = new_fitness\n\n            # Decay the exploration rate\n            exploration_rate *= self.exploration_decay\n\n        return self.f_opt, self.x_opt\n```", "aucs": [0.10296339075540384, 0.1779687305546468, 0.422463794586766, 0.20093486562329554, 0.2575400208905214, 0.4535138665697447, 0.2566102612474328, 0.359228600678851, 0.36579913871396663, 0.17911007766830667, 0.504257233930949, 0.9966891992835782, 0.25705349832194224, 0.2145951442574392, 0.7722505753881552, 0.3690040450371612, 0.3645340838151897, 0.5278530069205354, 0.15753987394985214, 0.4754805325299327], "operator": "random_new", "theta_sampled": 0.017462576738650093, "reward": 0.0, "generation": 3}, "task_prompt": ""}
{"id": "2ac84e2b-40f2-4ab8-b59d-74b98fcd5d9c", "fitness": 0.4482021339965129, "name": "EnhancedSlimeMoldOptimizer", "description": "Enhanced Slime Mold Optimizer with adaptive parameter control and a more robust exploration-exploitation balance.", "code": "import numpy as np\n\nclass EnhancedSlimeMoldOptimizer:\n    def __init__(self, budget=10000, dim=10, population_size=20, initial_w=0.9, p_selection=0.2, exploration_rate=0.1):\n        \"\"\"\n        Initializes the Enhanced Slime Mold Optimizer.\n\n        Args:\n            budget (int): The maximum number of function evaluations.\n            dim (int): The dimensionality of the search space.\n            population_size (int): The number of slime mold agents.\n            initial_w (float): Initial weight for averaging best solutions.\n            p_selection (float): Percentage of top solutions to consider for weighted averaging.\n            exploration_rate (float): Probability of random exploration.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.population_size = population_size\n        self.w = initial_w  # Initial averaging weight\n        self.p_selection = p_selection  # Percentage for best selection\n        self.exploration_rate = exploration_rate\n        self.lb = -5.0\n        self.ub = 5.0\n        self.best_fitness_history = []\n\n\n    def __call__(self, func):\n        \"\"\"\n        Optimizes the given function using the Enhanced Slime Mold Optimizer.\n\n        Args:\n            func (callable): The black-box function to optimize.\n\n        Returns:\n            tuple: The best function value found and the corresponding solution.\n        \"\"\"\n        population = np.random.uniform(self.lb, self.ub, size=(self.population_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.population_size\n        \n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index].copy()\n        self.best_fitness_history.append(self.f_opt)\n\n\n        while self.budget > 0:\n            # Sort population based on fitness\n            sorted_indices = np.argsort(fitness)\n            population = population[sorted_indices]\n            fitness = fitness[sorted_indices]\n            \n            # Select top solutions based on p_selection\n            num_selected = int(self.p_selection * self.population_size)\n            selected_population = population[:num_selected]\n            selected_fitness = fitness[:num_selected]\n                \n            # Calculate weights based on fitness rank. Give the better ones more weight\n            rank = np.arange(1, num_selected + 1)\n            weights_selected = np.exp(-rank / num_selected)\n            weights_selected /= np.sum(weights_selected)  # Normalize\n\n            # Adaptive parameter control for exploration rate and w based on convergence.\n            if len(self.best_fitness_history) > 5:\n                 if np.std(self.best_fitness_history[-5:]) < 1e-3: # If the best fitness is not changing, then increase exploration\n                      self.exploration_rate = min(0.5, self.exploration_rate * 1.2) # Increase exploration if stagnating\n                      self.w = min(0.99, self.w + 0.05) # Increase the averaging weight for better exploitation\n                 else:\n                      self.exploration_rate = max(0.01, self.exploration_rate * 0.8) # Otherwise decrease exploration to exploit\n                      self.w = max(0.1, self.w - 0.05) # Decrease averaging if not converging to allow the algorithm to explore other regions.\n            \n\n            # Update population using weighted average of selected solutions\n            for i in range(self.population_size):\n                if self.budget <= 0:\n                    break\n                \n                # Exploration phase\n                if np.random.rand() < self.exploration_rate:\n                    x = np.random.uniform(self.lb, self.ub, size=self.dim)\n\n                else: # Exploitation phase (weighted average of selected solutions)\n                    x = np.zeros(self.dim)\n                    for j in range(num_selected):\n                        x += weights_selected[j] * selected_population[j]\n                    \n                    # Perturbation proportional to current best solution distance\n                    distance_vector = self.x_opt - population[i] \n                    x += self.w * np.random.randn(self.dim) * distance_vector  # Scale perturbation\n\n                    x = np.clip(x, self.lb, self.ub)\n\n                f = func(x)\n                self.budget -= 1\n\n                if f < fitness[i]:\n                    fitness[i] = f\n                    population[i] = x.copy()\n\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = x.copy()\n                        \n            self.best_fitness_history.append(self.f_opt)\n        \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "The algorithm EnhancedSlimeMoldOptimizer scored 0.448 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["2d086322-f43f-4b37-bed4-2235e826723e"], "operator": "mutation", "metadata": {"llm_response": "# Description: Enhanced Slime Mold Optimizer with adaptive parameter control and a more robust exploration-exploitation balance.\n# Code: \n```python\nimport numpy as np\n\nclass EnhancedSlimeMoldOptimizer:\n    def __init__(self, budget=10000, dim=10, population_size=20, initial_w=0.9, p_selection=0.2, exploration_rate=0.1):\n        \"\"\"\n        Initializes the Enhanced Slime Mold Optimizer.\n\n        Args:\n            budget (int): The maximum number of function evaluations.\n            dim (int): The dimensionality of the search space.\n            population_size (int): The number of slime mold agents.\n            initial_w (float): Initial weight for averaging best solutions.\n            p_selection (float): Percentage of top solutions to consider for weighted averaging.\n            exploration_rate (float): Probability of random exploration.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.population_size = population_size\n        self.w = initial_w  # Initial averaging weight\n        self.p_selection = p_selection  # Percentage for best selection\n        self.exploration_rate = exploration_rate\n        self.lb = -5.0\n        self.ub = 5.0\n        self.best_fitness_history = []\n\n\n    def __call__(self, func):\n        \"\"\"\n        Optimizes the given function using the Enhanced Slime Mold Optimizer.\n\n        Args:\n            func (callable): The black-box function to optimize.\n\n        Returns:\n            tuple: The best function value found and the corresponding solution.\n        \"\"\"\n        population = np.random.uniform(self.lb, self.ub, size=(self.population_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.population_size\n        \n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index].copy()\n        self.best_fitness_history.append(self.f_opt)\n\n\n        while self.budget > 0:\n            # Sort population based on fitness\n            sorted_indices = np.argsort(fitness)\n            population = population[sorted_indices]\n            fitness = fitness[sorted_indices]\n            \n            # Select top solutions based on p_selection\n            num_selected = int(self.p_selection * self.population_size)\n            selected_population = population[:num_selected]\n            selected_fitness = fitness[:num_selected]\n                \n            # Calculate weights based on fitness rank. Give the better ones more weight\n            rank = np.arange(1, num_selected + 1)\n            weights_selected = np.exp(-rank / num_selected)\n            weights_selected /= np.sum(weights_selected)  # Normalize\n\n            # Adaptive parameter control for exploration rate and w based on convergence.\n            if len(self.best_fitness_history) > 5:\n                 if np.std(self.best_fitness_history[-5:]) < 1e-3: # If the best fitness is not changing, then increase exploration\n                      self.exploration_rate = min(0.5, self.exploration_rate * 1.2) # Increase exploration if stagnating\n                      self.w = min(0.99, self.w + 0.05) # Increase the averaging weight for better exploitation\n                 else:\n                      self.exploration_rate = max(0.01, self.exploration_rate * 0.8) # Otherwise decrease exploration to exploit\n                      self.w = max(0.1, self.w - 0.05) # Decrease averaging if not converging to allow the algorithm to explore other regions.\n            \n\n            # Update population using weighted average of selected solutions\n            for i in range(self.population_size):\n                if self.budget <= 0:\n                    break\n                \n                # Exploration phase\n                if np.random.rand() < self.exploration_rate:\n                    x = np.random.uniform(self.lb, self.ub, size=self.dim)\n\n                else: # Exploitation phase (weighted average of selected solutions)\n                    x = np.zeros(self.dim)\n                    for j in range(num_selected):\n                        x += weights_selected[j] * selected_population[j]\n                    \n                    # Perturbation proportional to current best solution distance\n                    distance_vector = self.x_opt - population[i] \n                    x += self.w * np.random.randn(self.dim) * distance_vector  # Scale perturbation\n\n                    x = np.clip(x, self.lb, self.ub)\n\n                f = func(x)\n                self.budget -= 1\n\n                if f < fitness[i]:\n                    fitness[i] = f\n                    population[i] = x.copy()\n\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = x.copy()\n                        \n            self.best_fitness_history.append(self.f_opt)\n        \n        return self.f_opt, self.x_opt\n```", "aucs": [0.153201575590319, 0.21214337049224385, 0.27921256473029366, 0.9697812539254593, 0.2968561951834541, 0.4449124071103794, 0.33672125190876667, 0.33189701496883195, 0.31586772863027013, 0.18419037367124513, 0.2994683320737932, 0.9986637546636442, 0.2561471093253761, 0.2622112546414792, 0.9261989568064343, 0.7359155570009472, 0.3186410785710787, 0.8936987438614643, 0.25372188297965004, 0.4945922737951267], "operator": "mutation", "theta_sampled": 0.018393465015341157, "reward": 0.0, "generation": 3}, "task_prompt": ""}
{"id": "9724a9db-9628-4a51-89b9-212121651f89", "fitness": 0.5594433274238504, "name": "HybridShrinkingAdaptiveOptimizer", "description": "Combines shrinking Cauchy exploration with adaptive mutation, adjusting population size and mutation strategy based on diversity and stagnation.", "code": "import numpy as np\n\nclass HybridShrinkingAdaptiveOptimizer:\n    def __init__(self, budget=10000, dim=10, initial_pop_size=50, shrink_factor=0.95, stagnation_threshold=1000, cauchy_scale=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.initial_pop_size = initial_pop_size\n        self.pop_size = initial_pop_size\n        self.shrink_factor = shrink_factor\n        self.stagnation_threshold = stagnation_threshold\n        self.cauchy_scale = cauchy_scale\n        self.lb = -5.0\n        self.ub = 5.0\n        self.mutation_prob_gaussian = 0.5\n        self.mutation_prob_cauchy = 0.5\n        self.mutation_scale = 1.0\n        self.stagnation_counter = 0\n        self.best_fitness_history = []\n\n\n    def initialize_population(self):\n        return np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n\n    def calculate_diversity(self, population):\n        centroid = np.mean(population, axis=0)\n        distances = np.linalg.norm(population - centroid, axis=1)\n        return np.mean(distances)\n\n    def mutate_gaussian(self, x):\n        return x + self.mutation_scale * np.random.normal(0, 1, size=self.dim)\n\n    def mutate_cauchy(self, x, search_range):\n        return x + search_range * np.random.standard_cauchy(size=self.dim) * self.cauchy_scale\n\n    def correct_bounds(self, x):\n        return np.clip(x, self.lb, self.ub)\n\n    def rank_based_selection(self, population, fitness, num_parents):\n        ranked_indices = np.argsort(fitness)\n        parent_indices = ranked_indices[:num_parents]\n        return population[parent_indices]\n\n    def adjust_population_size(self):\n        if self.stagnation_counter > self.stagnation_threshold:\n            self.pop_size = int(self.pop_size * 0.9)  # Reduce population size if stagnating\n            if self.pop_size < 10:\n                self.pop_size = 10 # Minimum population size\n\n            self.mutation_scale = 1.0 # Reset mutation scale\n            self.stagnation_counter = 0 # Reset stagnation counter\n            print(f\"Stagnation detected, reducing pop size to {self.pop_size}\")\n\n\n    def __call__(self, func):\n        population = self.initialize_population()\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n        \n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index]\n        self.best_fitness_history.append(self.f_opt)\n\n        search_range = self.ub - self.lb\n        center = self.x_opt\n\n        while self.budget > 0:\n            diversity = self.calculate_diversity(population)\n\n            if diversity > 1.0:\n                self.mutation_prob_gaussian = 0.8\n                self.mutation_prob_cauchy = 0.2\n            else:\n                self.mutation_prob_gaussian = 0.2\n                self.mutation_prob_cauchy = 0.8\n\n            # Elitism: Keep the best individual\n            best_individual = np.copy(self.x_opt)\n            best_fitness = self.f_opt\n\n            # Rank-based selection of parents for mutation\n            num_parents = int(self.pop_size / 2)  # Select top half as parents\n            parents = self.rank_based_selection(population, fitness, num_parents)\n            \n            new_population = np.copy(population)\n            for i in range(self.pop_size):\n                # Choose a parent randomly\n                parent = parents[np.random.randint(0, num_parents)]\n                \n                if np.random.rand() < self.mutation_prob_gaussian:\n                    new_population[i] = self.mutate_gaussian(parent)\n                else:\n                    new_population[i] = self.mutate_cauchy(parent, search_range)\n\n                new_population[i] = self.correct_bounds(new_population[i])\n                \n                new_fitness = func(new_population[i])\n                self.budget -= 1\n\n                if new_fitness < fitness[i]:\n                    fitness[i] = new_fitness\n                    population[i] = new_population[i]\n\n                    if new_fitness < self.f_opt:\n                        self.f_opt = new_fitness\n                        self.x_opt = new_population[i]\n                        center = self.x_opt\n            \n            # Replace worst individual with the best from previous generation (Elitism)\n            worst_index = np.argmax(fitness)\n            population[worst_index] = best_individual\n            fitness[worst_index] = best_fitness\n\n            # Adapt mutation scale\n            self.mutation_scale = 0.95 * self.mutation_scale\n\n            # Shrink the search range\n            search_range *= self.shrink_factor\n            lb = np.clip(center - search_range/2, self.lb, self.ub)\n            ub = np.clip(center + search_range/2, self.lb, self.ub)\n\n            # Stagnation detection and population size adjustment\n            if len(self.best_fitness_history) > self.stagnation_threshold:\n                if abs(self.f_opt - self.best_fitness_history[-self.stagnation_threshold]) < 1e-5:\n                    self.stagnation_counter += self.pop_size\n                else:\n                    self.stagnation_counter = 0\n\n            self.best_fitness_history.append(self.f_opt)\n            self.adjust_population_size()\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "The algorithm HybridShrinkingAdaptiveOptimizer scored 0.559 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["e8a11899-492c-463b-9412-67910c8f787b", "6c537388-4ccd-49e7-95d1-ae15369d9854"], "operator": "crossover", "metadata": {"llm_response": "# Description: Combines shrinking Cauchy exploration with adaptive mutation, adjusting population size and mutation strategy based on diversity and stagnation.\n# Code: \n```python\nimport numpy as np\n\nclass HybridShrinkingAdaptiveOptimizer:\n    def __init__(self, budget=10000, dim=10, initial_pop_size=50, shrink_factor=0.95, stagnation_threshold=1000, cauchy_scale=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.initial_pop_size = initial_pop_size\n        self.pop_size = initial_pop_size\n        self.shrink_factor = shrink_factor\n        self.stagnation_threshold = stagnation_threshold\n        self.cauchy_scale = cauchy_scale\n        self.lb = -5.0\n        self.ub = 5.0\n        self.mutation_prob_gaussian = 0.5\n        self.mutation_prob_cauchy = 0.5\n        self.mutation_scale = 1.0\n        self.stagnation_counter = 0\n        self.best_fitness_history = []\n\n\n    def initialize_population(self):\n        return np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n\n    def calculate_diversity(self, population):\n        centroid = np.mean(population, axis=0)\n        distances = np.linalg.norm(population - centroid, axis=1)\n        return np.mean(distances)\n\n    def mutate_gaussian(self, x):\n        return x + self.mutation_scale * np.random.normal(0, 1, size=self.dim)\n\n    def mutate_cauchy(self, x, search_range):\n        return x + search_range * np.random.standard_cauchy(size=self.dim) * self.cauchy_scale\n\n    def correct_bounds(self, x):\n        return np.clip(x, self.lb, self.ub)\n\n    def rank_based_selection(self, population, fitness, num_parents):\n        ranked_indices = np.argsort(fitness)\n        parent_indices = ranked_indices[:num_parents]\n        return population[parent_indices]\n\n    def adjust_population_size(self):\n        if self.stagnation_counter > self.stagnation_threshold:\n            self.pop_size = int(self.pop_size * 0.9)  # Reduce population size if stagnating\n            if self.pop_size < 10:\n                self.pop_size = 10 # Minimum population size\n\n            self.mutation_scale = 1.0 # Reset mutation scale\n            self.stagnation_counter = 0 # Reset stagnation counter\n            print(f\"Stagnation detected, reducing pop size to {self.pop_size}\")\n\n\n    def __call__(self, func):\n        population = self.initialize_population()\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n        \n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index]\n        self.best_fitness_history.append(self.f_opt)\n\n        search_range = self.ub - self.lb\n        center = self.x_opt\n\n        while self.budget > 0:\n            diversity = self.calculate_diversity(population)\n\n            if diversity > 1.0:\n                self.mutation_prob_gaussian = 0.8\n                self.mutation_prob_cauchy = 0.2\n            else:\n                self.mutation_prob_gaussian = 0.2\n                self.mutation_prob_cauchy = 0.8\n\n            # Elitism: Keep the best individual\n            best_individual = np.copy(self.x_opt)\n            best_fitness = self.f_opt\n\n            # Rank-based selection of parents for mutation\n            num_parents = int(self.pop_size / 2)  # Select top half as parents\n            parents = self.rank_based_selection(population, fitness, num_parents)\n            \n            new_population = np.copy(population)\n            for i in range(self.pop_size):\n                # Choose a parent randomly\n                parent = parents[np.random.randint(0, num_parents)]\n                \n                if np.random.rand() < self.mutation_prob_gaussian:\n                    new_population[i] = self.mutate_gaussian(parent)\n                else:\n                    new_population[i] = self.mutate_cauchy(parent, search_range)\n\n                new_population[i] = self.correct_bounds(new_population[i])\n                \n                new_fitness = func(new_population[i])\n                self.budget -= 1\n\n                if new_fitness < fitness[i]:\n                    fitness[i] = new_fitness\n                    population[i] = new_population[i]\n\n                    if new_fitness < self.f_opt:\n                        self.f_opt = new_fitness\n                        self.x_opt = new_population[i]\n                        center = self.x_opt\n            \n            # Replace worst individual with the best from previous generation (Elitism)\n            worst_index = np.argmax(fitness)\n            population[worst_index] = best_individual\n            fitness[worst_index] = best_fitness\n\n            # Adapt mutation scale\n            self.mutation_scale = 0.95 * self.mutation_scale\n\n            # Shrink the search range\n            search_range *= self.shrink_factor\n            lb = np.clip(center - search_range/2, self.lb, self.ub)\n            ub = np.clip(center + search_range/2, self.lb, self.ub)\n\n            # Stagnation detection and population size adjustment\n            if len(self.best_fitness_history) > self.stagnation_threshold:\n                if abs(self.f_opt - self.best_fitness_history[-self.stagnation_threshold]) < 1e-5:\n                    self.stagnation_counter += self.pop_size\n                else:\n                    self.stagnation_counter = 0\n\n            self.best_fitness_history.append(self.f_opt)\n            self.adjust_population_size()\n\n\n        return self.f_opt, self.x_opt\n```", "aucs": [0.3221597991996331, 0.5992461430372996, 0.5450407745264945, 0.8185075515829925, 0.5413484575961103, 0.6066331677539301, 0.3169871833676662, 0.46966124758318983, 0.5507698186336771, 0.5013679453562403, 0.780765303766404, 0.9980838386544796, 0.2460274774431208, 0.5451062068389156, 0.7491600162228891, 0.6044011004680538, 0.47619780454515725, 0.7180769012611581, 0.2968029841415394, 0.5025228264980577], "operator": "crossover", "theta_sampled": 0.01857040237101255, "reward": 0.0, "generation": 4}, "task_prompt": ""}
{"id": "d6179564-19c6-4fae-96aa-dfb89579ffa6", "fitness": 0.0, "name": "EnhancedAdaptiveMutationOptimizer", "description": "Combines Gaussian and Cauchy mutations with adaptive probabilities based on population diversity and introduces a local search component around the best solution.", "code": "import numpy as np\n\nclass EnhancedAdaptiveMutationOptimizer:\n    def __init__(self, budget=10000, dim=10, pop_size=50, stagnation_threshold=1000, local_search_prob=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lb = -5.0\n        self.ub = 5.0\n        self.mutation_prob_gaussian = 0.5\n        self.mutation_prob_cauchy = 0.5\n        self.mutation_scale = 1.0\n        self.stagnation_threshold = stagnation_threshold\n        self.stagnation_counter = 0\n        self.best_fitness_history = []\n        self.local_search_prob = local_search_prob\n\n    def initialize_population(self):\n        return np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n\n    def calculate_diversity(self, population):\n        centroid = np.mean(population, axis=0)\n        distances = np.linalg.norm(population - centroid, axis=1)\n        return np.mean(distances)\n\n    def mutate_gaussian(self, x):\n        return x + self.mutation_scale * np.random.normal(0, 1, size=self.dim)\n\n    def mutate_cauchy(self, x):\n        return x + self.mutation_scale * np.random.standard_cauchy(size=self.dim)\n\n    def correct_bounds(self, x):\n        return np.clip(x, self.lb, self.ub)\n\n    def rank_based_selection(self, population, fitness, num_parents):\n        ranked_indices = np.argsort(fitness)\n        parent_indices = ranked_indices[:num_parents]\n        return population[parent_indices]\n\n    def adjust_population_size(self):\n        if self.stagnation_counter > self.stagnation_threshold:\n            self.pop_size = int(self.pop_size * 0.9)  # Reduce population size if stagnating\n            if self.pop_size < 10:\n                self.pop_size = 10  # Minimum population size\n\n            self.mutation_scale = 1.0  # Reset mutation scale\n            self.stagnation_counter = 0  # Reset stagnation counter\n            print(f\"Stagnation detected, reducing pop size to {self.pop_size}\")\n\n    def local_search(self, func, x, num_steps=10, step_size=0.1):\n        best_x = x\n        best_f = func(x)\n        self.budget -= 1\n\n        for _ in range(num_steps):\n            # Generate a random direction\n            direction = np.random.normal(0, 1, size=self.dim)\n            direction = direction / np.linalg.norm(direction)  # Normalize\n\n            # Take a step in that direction\n            new_x = x + step_size * direction\n            new_x = self.correct_bounds(new_x)\n\n            new_f = func(new_x)\n            self.budget -= 1\n\n            if new_f < best_f:\n                best_f = new_f\n                best_x = new_x\n\n        return best_f, best_x\n\n    def __call__(self, func):\n        population = self.initialize_population()\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index]\n        self.best_fitness_history.append(self.f_opt)\n\n        while self.budget > 0:\n            diversity = self.calculate_diversity(population)\n\n            if diversity > 1.0:\n                self.mutation_prob_gaussian = 0.8\n                self.mutation_prob_cauchy = 0.2\n            else:\n                self.mutation_prob_gaussian = 0.2\n                self.mutation_prob_cauchy = 0.8\n\n            # Elitism: Keep the best individual\n            best_individual = np.copy(self.x_opt)\n            best_fitness = self.f_opt\n\n            # Rank-based selection of parents for mutation\n            num_parents = int(self.pop_size / 2)  # Select top half as parents\n            parents = self.rank_based_selection(population, fitness, num_parents)\n\n            new_population = np.copy(population)\n            for i in range(self.pop_size):\n                # Choose a parent randomly\n                parent = parents[np.random.randint(0, num_parents)]\n\n                if np.random.rand() < self.mutation_prob_gaussian:\n                    new_population[i] = self.mutate_gaussian(parent)\n                else:\n                    new_population[i] = self.mutate_cauchy(parent)\n\n                new_population[i] = self.correct_bounds(new_population[i])\n\n                new_fitness = func(new_population[i])\n                self.budget -= 1\n\n                if new_fitness < fitness[i]:\n                    fitness[i] = new_fitness\n                    population[i] = new_population[i]\n\n                    if new_fitness < self.f_opt:\n                        self.f_opt = new_fitness\n                        self.x_opt = new_population[i]\n                        # Local Search around the best solution\n                        if np.random.rand() < self.local_search_prob and self.budget > 10: # Ensure enough budget for local search\n                            local_f, local_x = self.local_search(func, self.x_opt)\n                            if local_f < self.f_opt:\n                                self.f_opt = local_f\n                                self.x_opt = local_x\n                                best_individual = np.copy(self.x_opt)\n                                best_fitness = self.f_opt\n\n\n            # Replace worst individual with the best from previous generation (Elitism)\n            worst_index = np.argmax(fitness)\n            population[worst_index] = best_individual\n            fitness[worst_index] = best_fitness\n\n            # Adapt mutation scale\n            self.mutation_scale = 0.95 * self.mutation_scale\n\n            # Stagnation detection and population size adjustment\n            if len(self.best_fitness_history) > self.stagnation_threshold:\n                if abs(self.f_opt - self.best_fitness_history[-self.stagnation_threshold]) < 1e-5:\n                    self.stagnation_counter += self.pop_size\n                else:\n                    self.stagnation_counter = 0\n\n            self.best_fitness_history.append(self.f_opt)\n            self.adjust_population_size()\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "The algorithm EnhancedAdaptiveMutationOptimizer scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["6c537388-4ccd-49e7-95d1-ae15369d9854"], "operator": "mutation", "metadata": {"llm_response": "# Description: Combines Gaussian and Cauchy mutations with adaptive probabilities based on population diversity and introduces a local search component around the best solution.\n\n# Code:\n```python\nimport numpy as np\n\nclass EnhancedAdaptiveMutationOptimizer:\n    def __init__(self, budget=10000, dim=10, pop_size=50, stagnation_threshold=1000, local_search_prob=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lb = -5.0\n        self.ub = 5.0\n        self.mutation_prob_gaussian = 0.5\n        self.mutation_prob_cauchy = 0.5\n        self.mutation_scale = 1.0\n        self.stagnation_threshold = stagnation_threshold\n        self.stagnation_counter = 0\n        self.best_fitness_history = []\n        self.local_search_prob = local_search_prob\n\n    def initialize_population(self):\n        return np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n\n    def calculate_diversity(self, population):\n        centroid = np.mean(population, axis=0)\n        distances = np.linalg.norm(population - centroid, axis=1)\n        return np.mean(distances)\n\n    def mutate_gaussian(self, x):\n        return x + self.mutation_scale * np.random.normal(0, 1, size=self.dim)\n\n    def mutate_cauchy(self, x):\n        return x + self.mutation_scale * np.random.standard_cauchy(size=self.dim)\n\n    def correct_bounds(self, x):\n        return np.clip(x, self.lb, self.ub)\n\n    def rank_based_selection(self, population, fitness, num_parents):\n        ranked_indices = np.argsort(fitness)\n        parent_indices = ranked_indices[:num_parents]\n        return population[parent_indices]\n\n    def adjust_population_size(self):\n        if self.stagnation_counter > self.stagnation_threshold:\n            self.pop_size = int(self.pop_size * 0.9)  # Reduce population size if stagnating\n            if self.pop_size < 10:\n                self.pop_size = 10  # Minimum population size\n\n            self.mutation_scale = 1.0  # Reset mutation scale\n            self.stagnation_counter = 0  # Reset stagnation counter\n            print(f\"Stagnation detected, reducing pop size to {self.pop_size}\")\n\n    def local_search(self, func, x, num_steps=10, step_size=0.1):\n        best_x = x\n        best_f = func(x)\n        self.budget -= 1\n\n        for _ in range(num_steps):\n            # Generate a random direction\n            direction = np.random.normal(0, 1, size=self.dim)\n            direction = direction / np.linalg.norm(direction)  # Normalize\n\n            # Take a step in that direction\n            new_x = x + step_size * direction\n            new_x = self.correct_bounds(new_x)\n\n            new_f = func(new_x)\n            self.budget -= 1\n\n            if new_f < best_f:\n                best_f = new_f\n                best_x = new_x\n\n        return best_f, best_x\n\n    def __call__(self, func):\n        population = self.initialize_population()\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index]\n        self.best_fitness_history.append(self.f_opt)\n\n        while self.budget > 0:\n            diversity = self.calculate_diversity(population)\n\n            if diversity > 1.0:\n                self.mutation_prob_gaussian = 0.8\n                self.mutation_prob_cauchy = 0.2\n            else:\n                self.mutation_prob_gaussian = 0.2\n                self.mutation_prob_cauchy = 0.8\n\n            # Elitism: Keep the best individual\n            best_individual = np.copy(self.x_opt)\n            best_fitness = self.f_opt\n\n            # Rank-based selection of parents for mutation\n            num_parents = int(self.pop_size / 2)  # Select top half as parents\n            parents = self.rank_based_selection(population, fitness, num_parents)\n\n            new_population = np.copy(population)\n            for i in range(self.pop_size):\n                # Choose a parent randomly\n                parent = parents[np.random.randint(0, num_parents)]\n\n                if np.random.rand() < self.mutation_prob_gaussian:\n                    new_population[i] = self.mutate_gaussian(parent)\n                else:\n                    new_population[i] = self.mutate_cauchy(parent)\n\n                new_population[i] = self.correct_bounds(new_population[i])\n\n                new_fitness = func(new_population[i])\n                self.budget -= 1\n\n                if new_fitness < fitness[i]:\n                    fitness[i] = new_fitness\n                    population[i] = new_population[i]\n\n                    if new_fitness < self.f_opt:\n                        self.f_opt = new_fitness\n                        self.x_opt = new_population[i]\n                        # Local Search around the best solution\n                        if np.random.rand() < self.local_search_prob and self.budget > 10: # Ensure enough budget for local search\n                            local_f, local_x = self.local_search(func, self.x_opt)\n                            if local_f < self.f_opt:\n                                self.f_opt = local_f\n                                self.x_opt = local_x\n                                best_individual = np.copy(self.x_opt)\n                                best_fitness = self.f_opt\n\n\n            # Replace worst individual with the best from previous generation (Elitism)\n            worst_index = np.argmax(fitness)\n            population[worst_index] = best_individual\n            fitness[worst_index] = best_fitness\n\n            # Adapt mutation scale\n            self.mutation_scale = 0.95 * self.mutation_scale\n\n            # Stagnation detection and population size adjustment\n            if len(self.best_fitness_history) > self.stagnation_threshold:\n                if abs(self.f_opt - self.best_fitness_history[-self.stagnation_threshold]) < 1e-5:\n                    self.stagnation_counter += self.pop_size\n                else:\n                    self.stagnation_counter = 0\n\n            self.best_fitness_history.append(self.f_opt)\n            self.adjust_population_size()\n\n        return self.f_opt, self.x_opt\n```", "aucs": [0], "operator": "mutation", "theta_sampled": 0.024510138461786762, "reward": 0.0, "generation": 4}, "task_prompt": ""}
{"id": "7b0eb6ab-2fe2-4725-8148-aca1813b4dbc", "fitness": 0.0, "name": "EnhancedHybridOptimizer", "description": "Integrates Slime Mold's cooperative search with a diversity-guided adaptive mutation strategy and a self-adaptive switching mechanism between these two search phases, alongside a population-based mutation scale adjustment and differential evolution inspired crossover.", "code": "import numpy as np\n\nclass EnhancedHybridOptimizer:\n    def __init__(self, budget=10000, dim=10, population_size=30, w_decay=0.9, p_selection=0.2, initial_mutation_scale=1.0, switch_prob=0.05):\n        \"\"\"\n        Initializes the Enhanced Hybrid Optimizer.\n\n        Args:\n            budget (int): The maximum number of function evaluations.\n            dim (int): The dimensionality of the search space.\n            population_size (int): The number of slime mold agents / population size.\n            w_decay (float): Decay rate for the weight parameter in Slime Mold phase.\n            p_selection (float): Percentage of top solutions for weighted averaging in Slime Mold phase.\n            initial_mutation_scale (float): Initial mutation scale for Adaptive Mutation phase.\n            switch_prob (float): Probability of switching between Slime Mold and Adaptive Mutation phases.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.population_size = population_size\n        self.w_decay = w_decay\n        self.p_selection = p_selection\n        self.lb = -5.0\n        self.ub = 5.0\n        self.mutation_prob_gaussian = 0.5\n        self.mutation_prob_cauchy = 0.5\n        self.mutation_scale = initial_mutation_scale\n        self.slime_mold_phase = True\n        self.switch_prob = switch_prob\n\n    def calculate_diversity(self, population):\n        \"\"\"Calculates diversity based on the average distance to the centroid.\"\"\"\n        centroid = np.mean(population, axis=0)\n        distances = np.linalg.norm(population - centroid, axis=1)\n        return np.mean(distances)\n\n    def mutate_gaussian(self, x):\n        return x + self.mutation_scale * np.random.normal(0, 1, size=self.dim)\n\n    def mutate_cauchy(self, x):\n        return x + self.mutation_scale * np.random.standard_cauchy(size=self.dim)\n\n    def correct_bounds(self, x):\n        return np.clip(x, self.lb, self.ub)\n\n    def slime_mold_update(self, population, fitness, func):\n        \"\"\"Performs the Slime Mold update step.\"\"\"\n        sorted_indices = np.argsort(fitness)\n        population = population[sorted_indices]\n        fitness = fitness[sorted_indices]\n\n        num_selected = int(self.p_selection * self.population_size)\n        selected_population = population[:num_selected]\n        selected_fitness = fitness[:num_selected]\n\n        rank = np.arange(1, num_selected + 1)\n        weights_selected = np.exp(-rank / num_selected)\n        weights_selected /= np.sum(weights_selected)\n\n        new_population = np.copy(population)\n\n        for i in range(self.population_size):\n            if self.budget <= 0:\n                break\n\n            if np.random.rand() < 0.1:\n                x = np.random.uniform(self.lb, self.ub, size=self.dim)\n\n            else:\n                x = np.zeros(self.dim)\n                for j in range(num_selected):\n                    x += weights_selected[j] * selected_population[j]\n\n                x += 0.1 * np.random.randn(self.dim)\n                x = np.clip(x, self.lb, self.ub)\n\n            # Differential evolution inspired crossover\n            if np.random.rand() < 0.3:\n                donor_indices = np.random.choice(self.population_size, 3, replace=False)\n                x_donor = population[donor_indices[0]] + 0.5 * (population[donor_indices[1]] - population[donor_indices[2]])\n                x = np.where(np.random.rand(self.dim) < 0.5, x, x_donor)  # Crossover\n\n            x = self.correct_bounds(x)\n            f = func(x)\n            self.budget -= 1\n\n            if f < fitness[i]:\n                fitness[i] = f\n                new_population[i] = x.copy()\n\n                if f < self.f_opt:\n                    self.f_opt = f\n                    self.x_opt = x.copy()\n\n        return new_population, fitness\n\n    def adaptive_mutation_update(self, population, fitness, func):\n        \"\"\"Performs the Adaptive Mutation update step.\"\"\"\n        diversity = self.calculate_diversity(population)\n\n        # Adjust mutation probabilities based on diversity\n        if diversity > 1.0:\n            self.mutation_prob_gaussian = 0.8\n            self.mutation_prob_cauchy = 0.2\n        else:\n            self.mutation_prob_gaussian = 0.2\n            self.mutation_prob_cauchy = 0.8\n\n        new_population = np.copy(population)\n        mutation_scales = np.zeros(self.population_size)\n        for i in range(self.population_size):\n            if np.random.rand() < self.mutation_prob_gaussian:\n                new_population[i] = self.mutate_gaussian(population[i])\n            else:\n                new_population[i] = self.mutate_cauchy(population[i])\n\n            new_population[i] = self.correct_bounds(new_population[i])\n            f = func(new_population[i])\n            self.budget -= 1\n\n            if f < fitness[i]:\n                fitness[i] = f\n                population[i] = new_population[i]\n\n                if f < self.f_opt:\n                    self.f_opt = f\n                    self.x_opt = new_population[i]\n                mutation_scales[i] = 0.95 #successfull mutation decay\n            else:\n                 mutation_scales[i] = 1.05 #unsuccesfull mutation increase\n        \n        self.mutation_scale = np.mean(mutation_scales) * self.mutation_scale\n        self.mutation_scale = np.clip(self.mutation_scale, 0.01, 1.0) # bound mutation scale\n\n\n        return population, fitness\n\n    def __call__(self, func):\n        \"\"\"\n        Optimizes the given function using the Enhanced Hybrid Optimizer.\n\n        Args:\n            func (callable): The black-box function to optimize.\n\n        Returns:\n            tuple: The best function value found and the corresponding solution.\n        \"\"\"\n        population = np.random.uniform(self.lb, self.ub, size=(self.population_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.population_size\n\n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index].copy()\n\n        while self.budget > 0:\n            if self.slime_mold_phase:\n                population, fitness = self.slime_mold_update(population, fitness, func)\n            else:\n                population, fitness = self.adaptive_mutation_update(population, fitness, func)\n\n            # Self-adaptive switching\n            if np.random.rand() < self.switch_prob:\n                self.slime_mold_phase = not self.slime_mold_phase\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "The algorithm EnhancedHybridOptimizer scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["19122a76-87b8-404d-86bd-2bda6f4e75cc"], "operator": "mutation", "metadata": {"llm_response": "# Description: Integrates Slime Mold's cooperative search with a diversity-guided adaptive mutation strategy and a self-adaptive switching mechanism between these two search phases, alongside a population-based mutation scale adjustment and differential evolution inspired crossover.\n# Code:\n```python\nimport numpy as np\n\nclass EnhancedHybridOptimizer:\n    def __init__(self, budget=10000, dim=10, population_size=30, w_decay=0.9, p_selection=0.2, initial_mutation_scale=1.0, switch_prob=0.05):\n        \"\"\"\n        Initializes the Enhanced Hybrid Optimizer.\n\n        Args:\n            budget (int): The maximum number of function evaluations.\n            dim (int): The dimensionality of the search space.\n            population_size (int): The number of slime mold agents / population size.\n            w_decay (float): Decay rate for the weight parameter in Slime Mold phase.\n            p_selection (float): Percentage of top solutions for weighted averaging in Slime Mold phase.\n            initial_mutation_scale (float): Initial mutation scale for Adaptive Mutation phase.\n            switch_prob (float): Probability of switching between Slime Mold and Adaptive Mutation phases.\n        \"\"\"\n        self.budget = budget\n        self.dim = dim\n        self.population_size = population_size\n        self.w_decay = w_decay\n        self.p_selection = p_selection\n        self.lb = -5.0\n        self.ub = 5.0\n        self.mutation_prob_gaussian = 0.5\n        self.mutation_prob_cauchy = 0.5\n        self.mutation_scale = initial_mutation_scale\n        self.slime_mold_phase = True\n        self.switch_prob = switch_prob\n\n    def calculate_diversity(self, population):\n        \"\"\"Calculates diversity based on the average distance to the centroid.\"\"\"\n        centroid = np.mean(population, axis=0)\n        distances = np.linalg.norm(population - centroid, axis=1)\n        return np.mean(distances)\n\n    def mutate_gaussian(self, x):\n        return x + self.mutation_scale * np.random.normal(0, 1, size=self.dim)\n\n    def mutate_cauchy(self, x):\n        return x + self.mutation_scale * np.random.standard_cauchy(size=self.dim)\n\n    def correct_bounds(self, x):\n        return np.clip(x, self.lb, self.ub)\n\n    def slime_mold_update(self, population, fitness, func):\n        \"\"\"Performs the Slime Mold update step.\"\"\"\n        sorted_indices = np.argsort(fitness)\n        population = population[sorted_indices]\n        fitness = fitness[sorted_indices]\n\n        num_selected = int(self.p_selection * self.population_size)\n        selected_population = population[:num_selected]\n        selected_fitness = fitness[:num_selected]\n\n        rank = np.arange(1, num_selected + 1)\n        weights_selected = np.exp(-rank / num_selected)\n        weights_selected /= np.sum(weights_selected)\n\n        new_population = np.copy(population)\n\n        for i in range(self.population_size):\n            if self.budget <= 0:\n                break\n\n            if np.random.rand() < 0.1:\n                x = np.random.uniform(self.lb, self.ub, size=self.dim)\n\n            else:\n                x = np.zeros(self.dim)\n                for j in range(num_selected):\n                    x += weights_selected[j] * selected_population[j]\n\n                x += 0.1 * np.random.randn(self.dim)\n                x = np.clip(x, self.lb, self.ub)\n\n            # Differential evolution inspired crossover\n            if np.random.rand() < 0.3:\n                donor_indices = np.random.choice(self.population_size, 3, replace=False)\n                x_donor = population[donor_indices[0]] + 0.5 * (population[donor_indices[1]] - population[donor_indices[2]])\n                x = np.where(np.random.rand(self.dim) < 0.5, x, x_donor)  # Crossover\n\n            x = self.correct_bounds(x)\n            f = func(x)\n            self.budget -= 1\n\n            if f < fitness[i]:\n                fitness[i] = f\n                new_population[i] = x.copy()\n\n                if f < self.f_opt:\n                    self.f_opt = f\n                    self.x_opt = x.copy()\n\n        return new_population, fitness\n\n    def adaptive_mutation_update(self, population, fitness, func):\n        \"\"\"Performs the Adaptive Mutation update step.\"\"\"\n        diversity = self.calculate_diversity(population)\n\n        # Adjust mutation probabilities based on diversity\n        if diversity > 1.0:\n            self.mutation_prob_gaussian = 0.8\n            self.mutation_prob_cauchy = 0.2\n        else:\n            self.mutation_prob_gaussian = 0.2\n            self.mutation_prob_cauchy = 0.8\n\n        new_population = np.copy(population)\n        mutation_scales = np.zeros(self.population_size)\n        for i in range(self.population_size):\n            if np.random.rand() < self.mutation_prob_gaussian:\n                new_population[i] = self.mutate_gaussian(population[i])\n            else:\n                new_population[i] = self.mutate_cauchy(population[i])\n\n            new_population[i] = self.correct_bounds(new_population[i])\n            f = func(new_population[i])\n            self.budget -= 1\n\n            if f < fitness[i]:\n                fitness[i] = f\n                population[i] = new_population[i]\n\n                if f < self.f_opt:\n                    self.f_opt = f\n                    self.x_opt = new_population[i]\n                mutation_scales[i] = 0.95 #successfull mutation decay\n            else:\n                 mutation_scales[i] = 1.05 #unsuccesfull mutation increase\n        \n        self.mutation_scale = np.mean(mutation_scales) * self.mutation_scale\n        self.mutation_scale = np.clip(self.mutation_scale, 0.01, 1.0) # bound mutation scale\n\n\n        return population, fitness\n\n    def __call__(self, func):\n        \"\"\"\n        Optimizes the given function using the Enhanced Hybrid Optimizer.\n\n        Args:\n            func (callable): The black-box function to optimize.\n\n        Returns:\n            tuple: The best function value found and the corresponding solution.\n        \"\"\"\n        population = np.random.uniform(self.lb, self.ub, size=(self.population_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.population_size\n\n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index].copy()\n\n        while self.budget > 0:\n            if self.slime_mold_phase:\n                population, fitness = self.slime_mold_update(population, fitness, func)\n            else:\n                population, fitness = self.adaptive_mutation_update(population, fitness, func)\n\n            # Self-adaptive switching\n            if np.random.rand() < self.switch_prob:\n                self.slime_mold_phase = not self.slime_mold_phase\n\n        return self.f_opt, self.x_opt\n```", "aucs": [0], "operator": "mutation", "theta_sampled": 0.018214010554139324, "reward": 0.0, "generation": 4}, "task_prompt": ""}
{"id": "1eae1175-fe6d-4da8-a4f7-63f806d104e8", "fitness": 0.0, "name": "AdaptiveHybridEvolutionarySearch", "description": "Population-based algorithm using a combination of differential evolution, covariance matrix adaptation, and a local search operator based on gradient estimation, dynamically adapting the strategy based on success rate.", "code": "import numpy as np\n\nclass AdaptiveHybridEvolutionarySearch:\n    def __init__(self, budget=10000, dim=10, pop_size=20, exploration_rate=0.3, local_search_probability=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.exploration_rate = exploration_rate\n        self.local_search_probability = local_search_probability\n        self.population = None\n        self.fitness = None\n        self.x_opt = None\n        self.f_opt = np.inf\n        self.success_history = []\n        self.cma_sigma = 0.1 #Initial step size\n        self.cma_C = np.eye(dim) #Initial covariance matrix\n        self.cma_damps = 1 #Damping factor\n        self.cma_mu = pop_size // 2\n        self.cma_weights = np.log(self.cma_mu + 0.5) - np.log(np.arange(1, self.cma_mu + 1))\n        self.cma_weights /= np.sum(self.cma_weights)\n        self.cma_cs = (self.cma_damps + 2) / (self.dim + self.cma_mu + 5) #Cumulation factor for step size\n        self.cma_cc = (4 + self.cma_mu / self.dim) / (self.dim + 4 + 2 * self.cma_mu / self.dim) #Cumulation factor for covariance matrix\n        self.cma_alpha_mu = 2\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n\n    def differential_evolution(self, func):\n        for i in range(self.pop_size):\n            idxs = [idx for idx in range(self.pop_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            mutant = a + self.exploration_rate * (b - c)\n            mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n            cross_points = np.random.rand(self.dim) < 0.9\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n\n            f = func(trial)\n            if f < self.fitness[i]:\n                self.population[i] = trial\n                self.fitness[i] = f\n                if f < self.f_opt:\n                    self.f_opt = f\n                    self.x_opt = trial\n\n    def covariance_matrix_adaptation(self, func):\n        # Select parents\n        indices = np.argsort(self.fitness)\n        parents = self.population[indices[:self.cma_mu]]\n        weights = self.cma_weights\n\n        # Recombination\n        mean = np.sum(parents * weights[:, np.newaxis], axis=0)\n\n        # Update evolution path\n        z = np.random.normal(0, 1, self.dim)\n        self.p_sigma = (1 - self.cma_cs) * getattr(self, 'p_sigma', np.zeros(self.dim)) + np.sqrt(self.cma_cs * (2 - self.cma_cs)) * np.dot(np.linalg.inv(np.linalg.cholesky(self.cma_C)), z)\n\n        # Update step size\n        self.cma_sigma *= np.exp((self.cma_cs / self.cma_damps) * (np.linalg.norm(self.p_sigma) / np.sqrt(self.dim) - 1))\n\n        # Update covariance matrix\n        y = (parents - self.population[indices[0]]) / self.cma_sigma\n        self.p_c = (1 - self.cma_cc) * getattr(self, 'p_c', np.zeros(self.dim)) + np.sqrt(self.cma_cc * (2 - self.cma_cc)) * np.sum(weights[:, np.newaxis] * y, axis=0)\n\n        self.cma_C = (1 - self.cma_alpha_mu) * self.cma_C + self.cma_alpha_mu * (np.outer(self.p_c, self.p_c) + self.cma_alpha_mu * np.eye(self.dim) / np.sum(weights**2))\n\n        # Sample new population\n        for i in range(self.pop_size):\n            z = np.random.normal(0, 1, self.dim)\n            new_x = mean + self.cma_sigma * np.dot(np.linalg.cholesky(self.cma_C), z)\n            new_x = np.clip(new_x, func.bounds.lb, func.bounds.ub)\n            f = func(new_x)\n            if f < self.fitness[i]:\n                self.population[i] = new_x\n                self.fitness[i] = f\n                if f < self.f_opt:\n                    self.f_opt = f\n                    self.x_opt = new_x\n\n\n    def local_search(self, func, x, step_size=0.01):\n        # Estimate gradient (simplified)\n        grad = np.zeros(self.dim)\n        for i in range(self.dim):\n            x_plus = x.copy()\n            x_minus = x.copy()\n            x_plus[i] += step_size\n            x_minus[i] -= step_size\n            x_plus[i] = np.clip(x_plus[i], func.bounds.lb[i], func.bounds.ub[i])\n            x_minus[i] = np.clip(x_minus[i], func.bounds.lb[i], func.bounds.ub[i])\n            grad[i] = (func(x_plus) - func(x_minus)) / (2 * step_size)\n\n        # Move against gradient\n        new_x = x - 0.1 * step_size * grad  # Adjust step based on gradient magnitude\n        new_x = np.clip(new_x, func.bounds.lb, func.bounds.ub)\n        f = func(new_x)\n        return f, new_x\n    \n    def should_use_local_search(self):\n        return np.random.rand() < self.local_search_probability\n\n    def update_strategy(self):\n        if len(self.success_history) > 10:\n            success_rate = np.mean(self.success_history[-10:])\n            self.exploration_rate = np.clip(self.exploration_rate * (1 + 0.2 * (success_rate - 0.5)), 0.1, 0.5)  # Dynamically adjust exploration rate\n            self.local_search_probability = np.clip(self.local_search_probability * (1 + 0.1 * (success_rate - 0.5)), 0.05, 0.2) #Dynamically adjust local search probability\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        eval_count = self.pop_size  # Account for initial population evaluations\n\n        while eval_count < self.budget:\n            old_f_opt = self.f_opt\n            self.differential_evolution(func)\n            eval_count += self.pop_size\n            if eval_count >= self.budget: break\n\n            self.covariance_matrix_adaptation(func)\n            eval_count += self.pop_size\n            if eval_count >= self.budget: break\n\n            #Local Search\n            for i in range(self.pop_size):\n                if self.should_use_local_search():\n                    f_local, x_local = self.local_search(func, self.population[i])\n                    eval_count += 1\n                    if eval_count >= self.budget: break\n                    if f_local < self.fitness[i]:\n                        self.population[i] = x_local\n                        self.fitness[i] = f_local\n                        if f_local < self.f_opt:\n                            self.f_opt = f_local\n                            self.x_opt = x_local\n\n            if eval_count >= self.budget: break\n            \n            success = self.f_opt < old_f_opt\n            self.success_history.append(success)\n            self.update_strategy()\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "", "error": "Evaluation error: Matrix is not positive definite", "parent_ids": [], "operator": "random_new", "metadata": {"llm_response": "# Description: Population-based algorithm using a combination of differential evolution, covariance matrix adaptation, and a local search operator based on gradient estimation, dynamically adapting the strategy based on success rate.\n# Code:\n```python\nimport numpy as np\n\nclass AdaptiveHybridEvolutionarySearch:\n    def __init__(self, budget=10000, dim=10, pop_size=20, exploration_rate=0.3, local_search_probability=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.exploration_rate = exploration_rate\n        self.local_search_probability = local_search_probability\n        self.population = None\n        self.fitness = None\n        self.x_opt = None\n        self.f_opt = np.inf\n        self.success_history = []\n        self.cma_sigma = 0.1 #Initial step size\n        self.cma_C = np.eye(dim) #Initial covariance matrix\n        self.cma_damps = 1 #Damping factor\n        self.cma_mu = pop_size // 2\n        self.cma_weights = np.log(self.cma_mu + 0.5) - np.log(np.arange(1, self.cma_mu + 1))\n        self.cma_weights /= np.sum(self.cma_weights)\n        self.cma_cs = (self.cma_damps + 2) / (self.dim + self.cma_mu + 5) #Cumulation factor for step size\n        self.cma_cc = (4 + self.cma_mu / self.dim) / (self.dim + 4 + 2 * self.cma_mu / self.dim) #Cumulation factor for covariance matrix\n        self.cma_alpha_mu = 2\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n\n    def differential_evolution(self, func):\n        for i in range(self.pop_size):\n            idxs = [idx for idx in range(self.pop_size) if idx != i]\n            a, b, c = self.population[np.random.choice(idxs, 3, replace=False)]\n            mutant = a + self.exploration_rate * (b - c)\n            mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n            cross_points = np.random.rand(self.dim) < 0.9\n            if not np.any(cross_points):\n                cross_points[np.random.randint(0, self.dim)] = True\n            trial = np.where(cross_points, mutant, self.population[i])\n\n            f = func(trial)\n            if f < self.fitness[i]:\n                self.population[i] = trial\n                self.fitness[i] = f\n                if f < self.f_opt:\n                    self.f_opt = f\n                    self.x_opt = trial\n\n    def covariance_matrix_adaptation(self, func):\n        # Select parents\n        indices = np.argsort(self.fitness)\n        parents = self.population[indices[:self.cma_mu]]\n        weights = self.cma_weights\n\n        # Recombination\n        mean = np.sum(parents * weights[:, np.newaxis], axis=0)\n\n        # Update evolution path\n        z = np.random.normal(0, 1, self.dim)\n        self.p_sigma = (1 - self.cma_cs) * getattr(self, 'p_sigma', np.zeros(self.dim)) + np.sqrt(self.cma_cs * (2 - self.cma_cs)) * np.dot(np.linalg.inv(np.linalg.cholesky(self.cma_C)), z)\n\n        # Update step size\n        self.cma_sigma *= np.exp((self.cma_cs / self.cma_damps) * (np.linalg.norm(self.p_sigma) / np.sqrt(self.dim) - 1))\n\n        # Update covariance matrix\n        y = (parents - self.population[indices[0]]) / self.cma_sigma\n        self.p_c = (1 - self.cma_cc) * getattr(self, 'p_c', np.zeros(self.dim)) + np.sqrt(self.cma_cc * (2 - self.cma_cc)) * np.sum(weights[:, np.newaxis] * y, axis=0)\n\n        self.cma_C = (1 - self.cma_alpha_mu) * self.cma_C + self.cma_alpha_mu * (np.outer(self.p_c, self.p_c) + self.cma_alpha_mu * np.eye(self.dim) / np.sum(weights**2))\n\n        # Sample new population\n        for i in range(self.pop_size):\n            z = np.random.normal(0, 1, self.dim)\n            new_x = mean + self.cma_sigma * np.dot(np.linalg.cholesky(self.cma_C), z)\n            new_x = np.clip(new_x, func.bounds.lb, func.bounds.ub)\n            f = func(new_x)\n            if f < self.fitness[i]:\n                self.population[i] = new_x\n                self.fitness[i] = f\n                if f < self.f_opt:\n                    self.f_opt = f\n                    self.x_opt = new_x\n\n\n    def local_search(self, func, x, step_size=0.01):\n        # Estimate gradient (simplified)\n        grad = np.zeros(self.dim)\n        for i in range(self.dim):\n            x_plus = x.copy()\n            x_minus = x.copy()\n            x_plus[i] += step_size\n            x_minus[i] -= step_size\n            x_plus[i] = np.clip(x_plus[i], func.bounds.lb[i], func.bounds.ub[i])\n            x_minus[i] = np.clip(x_minus[i], func.bounds.lb[i], func.bounds.ub[i])\n            grad[i] = (func(x_plus) - func(x_minus)) / (2 * step_size)\n\n        # Move against gradient\n        new_x = x - 0.1 * step_size * grad  # Adjust step based on gradient magnitude\n        new_x = np.clip(new_x, func.bounds.lb, func.bounds.ub)\n        f = func(new_x)\n        return f, new_x\n    \n    def should_use_local_search(self):\n        return np.random.rand() < self.local_search_probability\n\n    def update_strategy(self):\n        if len(self.success_history) > 10:\n            success_rate = np.mean(self.success_history[-10:])\n            self.exploration_rate = np.clip(self.exploration_rate * (1 + 0.2 * (success_rate - 0.5)), 0.1, 0.5)  # Dynamically adjust exploration rate\n            self.local_search_probability = np.clip(self.local_search_probability * (1 + 0.1 * (success_rate - 0.5)), 0.05, 0.2) #Dynamically adjust local search probability\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        eval_count = self.pop_size  # Account for initial population evaluations\n\n        while eval_count < self.budget:\n            old_f_opt = self.f_opt\n            self.differential_evolution(func)\n            eval_count += self.pop_size\n            if eval_count >= self.budget: break\n\n            self.covariance_matrix_adaptation(func)\n            eval_count += self.pop_size\n            if eval_count >= self.budget: break\n\n            #Local Search\n            for i in range(self.pop_size):\n                if self.should_use_local_search():\n                    f_local, x_local = self.local_search(func, self.population[i])\n                    eval_count += 1\n                    if eval_count >= self.budget: break\n                    if f_local < self.fitness[i]:\n                        self.population[i] = x_local\n                        self.fitness[i] = f_local\n                        if f_local < self.f_opt:\n                            self.f_opt = f_local\n                            self.x_opt = x_local\n\n            if eval_count >= self.budget: break\n            \n            success = self.f_opt < old_f_opt\n            self.success_history.append(success)\n            self.update_strategy()\n\n\n        return self.f_opt, self.x_opt\n```", "operator": "random_new", "theta_sampled": 0.01098419447060927, "reward": 0.0, "generation": 4}, "task_prompt": ""}
{"id": "fde3f481-fa2f-42d0-973e-ab488c0337a1", "fitness": 0.0, "name": "AdaptiveCMAESOptimizer", "description": "Combines adaptive population sizing with CMA-ES inspired mutation and selection, enhanced with stagnation detection and restart mechanisms.", "code": "import numpy as np\n\nclass AdaptiveCMAESOptimizer:\n    def __init__(self, budget=10000, dim=10, pop_size=None, stagnation_threshold=1000, initial_mutation_scale=1.0, restart_strategy=\"shrink\"):\n        self.budget = budget\n        self.dim = dim\n        self.lb = -5.0\n        self.ub = 5.0\n        if pop_size is None:\n          self.pop_size = 4 + int(3 * np.log(dim)) #Default CMA-ES population size\n        else:\n          self.pop_size = pop_size\n        self.initial_pop_size = self.pop_size  # Store initial population size\n        self.mutation_scale = initial_mutation_scale\n        self.stagnation_threshold = stagnation_threshold\n        self.stagnation_counter = 0\n        self.best_fitness_history = []\n        self.mean = np.random.uniform(self.lb, self.ub, size=self.dim)\n        self.covariance = np.eye(self.dim)\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.restart_strategy = restart_strategy #Can be \"shrink\" or \"random\"\n\n    def sample_population(self):\n        return np.random.multivariate_normal(self.mean, self.mutation_scale**2 * self.covariance, size=self.pop_size)\n\n    def correct_bounds(self, x):\n        return np.clip(x, self.lb, self.ub)\n\n    def update_mean_covariance(self, population, fitness):\n        sorted_indices = np.argsort(fitness)\n        best_individuals = population[sorted_indices[:self.pop_size // 2]] # Use top 50% for adaptation\n        self.mean = np.mean(best_individuals, axis=0)\n        \n        #Simplified covariance update (can be improved with more sophisticated CMA-ES techniques)\n        diff = best_individuals - self.mean\n        self.covariance = np.cov(diff.T)\n        if np.isnan(self.covariance).any() or np.isinf(self.covariance).any():\n            self.covariance = np.eye(self.dim) #Reset if covariance becomes invalid\n\n\n    def adjust_population_size(self):\n      #Adjustments performed only when necessary.\n        if self.stagnation_counter > self.stagnation_threshold:\n            self.pop_size = int(self.pop_size * 0.8)  # Reduce population size if stagnating\n            if self.pop_size < 4: # CMA-ES requires pop size > 1\n                self.pop_size = 4 # Minimum population size\n\n            self.mutation_scale = 0.8 * self.mutation_scale #Reduce mutation scale\n            self.stagnation_counter = 0  # Reset stagnation counter\n            print(f\"Stagnation detected, reducing pop size to {self.pop_size}, mutation scale to {self.mutation_scale}\")\n            \n            if self.restart_strategy == \"shrink\":\n              self.mean = np.random.uniform(self.lb, self.ub, size=self.dim)\n            elif self.restart_strategy == \"random\":\n              self.mean = self.x_opt + np.random.normal(0, self.mutation_scale, size=self.dim) #Restart around current best\n              self.mean = self.correct_bounds(self.mean)\n            \n            self.covariance = np.eye(self.dim) #Reset covariance\n\n        elif self.pop_size < self.initial_pop_size and len(self.best_fitness_history) > self.stagnation_threshold and abs(self.f_opt - self.best_fitness_history[-self.stagnation_threshold]) > 1e-5:\n            self.pop_size = int(min(self.pop_size * 1.1, self.initial_pop_size))\n            print(f\"Progress detected, increasing pop size to {self.pop_size}\")\n\n    def __call__(self, func):\n        while self.budget > 0:\n            population = self.sample_population()\n            population = np.array([self.correct_bounds(x) for x in population]) #Correct bounds for all individuals\n            \n            fitness = np.array([func(x) for x in population])\n            self.budget -= self.pop_size\n\n            best_index = np.argmin(fitness)\n            if fitness[best_index] < self.f_opt:\n                self.f_opt = fitness[best_index]\n                self.x_opt = population[best_index]\n\n            self.best_fitness_history.append(self.f_opt)\n            self.update_mean_covariance(population, fitness)\n\n            # Stagnation detection\n            if len(self.best_fitness_history) > self.stagnation_threshold:\n                if abs(self.f_opt - self.best_fitness_history[-self.stagnation_threshold]) < 1e-5:\n                    self.stagnation_counter += self.pop_size\n                else:\n                    self.stagnation_counter = 0\n\n            self.adjust_population_size()\n            if self.budget <= 0:\n                break\n            \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "The algorithm AdaptiveCMAESOptimizer scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["ddb255ad-fa53-4028-8bc5-cf4dcdf480e3"], "operator": "mutation", "metadata": {"llm_response": "# Description: Combines adaptive population sizing with CMA-ES inspired mutation and selection, enhanced with stagnation detection and restart mechanisms.\n# Code: \n```python\nimport numpy as np\n\nclass AdaptiveCMAESOptimizer:\n    def __init__(self, budget=10000, dim=10, pop_size=None, stagnation_threshold=1000, initial_mutation_scale=1.0, restart_strategy=\"shrink\"):\n        self.budget = budget\n        self.dim = dim\n        self.lb = -5.0\n        self.ub = 5.0\n        if pop_size is None:\n          self.pop_size = 4 + int(3 * np.log(dim)) #Default CMA-ES population size\n        else:\n          self.pop_size = pop_size\n        self.initial_pop_size = self.pop_size  # Store initial population size\n        self.mutation_scale = initial_mutation_scale\n        self.stagnation_threshold = stagnation_threshold\n        self.stagnation_counter = 0\n        self.best_fitness_history = []\n        self.mean = np.random.uniform(self.lb, self.ub, size=self.dim)\n        self.covariance = np.eye(self.dim)\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.restart_strategy = restart_strategy #Can be \"shrink\" or \"random\"\n\n    def sample_population(self):\n        return np.random.multivariate_normal(self.mean, self.mutation_scale**2 * self.covariance, size=self.pop_size)\n\n    def correct_bounds(self, x):\n        return np.clip(x, self.lb, self.ub)\n\n    def update_mean_covariance(self, population, fitness):\n        sorted_indices = np.argsort(fitness)\n        best_individuals = population[sorted_indices[:self.pop_size // 2]] # Use top 50% for adaptation\n        self.mean = np.mean(best_individuals, axis=0)\n        \n        #Simplified covariance update (can be improved with more sophisticated CMA-ES techniques)\n        diff = best_individuals - self.mean\n        self.covariance = np.cov(diff.T)\n        if np.isnan(self.covariance).any() or np.isinf(self.covariance).any():\n            self.covariance = np.eye(self.dim) #Reset if covariance becomes invalid\n\n\n    def adjust_population_size(self):\n      #Adjustments performed only when necessary.\n        if self.stagnation_counter > self.stagnation_threshold:\n            self.pop_size = int(self.pop_size * 0.8)  # Reduce population size if stagnating\n            if self.pop_size < 4: # CMA-ES requires pop size > 1\n                self.pop_size = 4 # Minimum population size\n\n            self.mutation_scale = 0.8 * self.mutation_scale #Reduce mutation scale\n            self.stagnation_counter = 0  # Reset stagnation counter\n            print(f\"Stagnation detected, reducing pop size to {self.pop_size}, mutation scale to {self.mutation_scale}\")\n            \n            if self.restart_strategy == \"shrink\":\n              self.mean = np.random.uniform(self.lb, self.ub, size=self.dim)\n            elif self.restart_strategy == \"random\":\n              self.mean = self.x_opt + np.random.normal(0, self.mutation_scale, size=self.dim) #Restart around current best\n              self.mean = self.correct_bounds(self.mean)\n            \n            self.covariance = np.eye(self.dim) #Reset covariance\n\n        elif self.pop_size < self.initial_pop_size and len(self.best_fitness_history) > self.stagnation_threshold and abs(self.f_opt - self.best_fitness_history[-self.stagnation_threshold]) > 1e-5:\n            self.pop_size = int(min(self.pop_size * 1.1, self.initial_pop_size))\n            print(f\"Progress detected, increasing pop size to {self.pop_size}\")\n\n    def __call__(self, func):\n        while self.budget > 0:\n            population = self.sample_population()\n            population = np.array([self.correct_bounds(x) for x in population]) #Correct bounds for all individuals\n            \n            fitness = np.array([func(x) for x in population])\n            self.budget -= self.pop_size\n\n            best_index = np.argmin(fitness)\n            if fitness[best_index] < self.f_opt:\n                self.f_opt = fitness[best_index]\n                self.x_opt = population[best_index]\n\n            self.best_fitness_history.append(self.f_opt)\n            self.update_mean_covariance(population, fitness)\n\n            # Stagnation detection\n            if len(self.best_fitness_history) > self.stagnation_threshold:\n                if abs(self.f_opt - self.best_fitness_history[-self.stagnation_threshold]) < 1e-5:\n                    self.stagnation_counter += self.pop_size\n                else:\n                    self.stagnation_counter = 0\n\n            self.adjust_population_size()\n            if self.budget <= 0:\n                break\n            \n        return self.f_opt, self.x_opt\n```", "aucs": [0], "operator": "mutation", "theta_sampled": 0.020743241120209383, "reward": 0.0, "generation": 4}, "task_prompt": ""}
{"id": "a590d495-1243-4ddf-81e2-8450998352af", "fitness": 0.0, "name": "AdaptiveSwarmEvolution", "description": "An adaptive population-based algorithm that combines aspects of particle swarm optimization and differential evolution, with a focus on local search intensification using a shrinking neighborhood around promising solutions.", "code": "import numpy as np\n\nclass AdaptiveSwarmEvolution:\n    def __init__(self, budget=10000, dim=10, pop_size=20, inertia=0.7, c1=1.5, c2=1.5, de_rate=0.1, local_search_radius=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.inertia = inertia\n        self.c1 = c1\n        self.c2 = c2\n        self.de_rate = de_rate\n        self.local_search_radius = local_search_radius\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        \n        # Initialize population and velocities\n        population = np.random.uniform(lb, ub, size=(self.pop_size, self.dim))\n        velocities = np.random.uniform(-0.1 * (ub - lb), 0.1 * (ub - lb), size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        # Initialize best positions\n        personal_best_positions = population.copy()\n        personal_best_fitness = fitness.copy()\n        global_best_index = np.argmin(fitness)\n        global_best_position = population[global_best_index].copy()\n        global_best_fitness = fitness[global_best_index]\n        \n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Update velocity using PSO-like update\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                velocities[i] = (self.inertia * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - population[i]) +\n                                 self.c2 * r2 * (global_best_position - population[i]))\n\n                # Differential Evolution component\n                if np.random.rand() < self.de_rate:\n                    idxs = np.random.choice(self.pop_size, 3, replace=False)\n                    v = population[idxs[0]] + 0.5 * (population[idxs[1]] - population[idxs[2]])\n                    \n                    # Crossover\n                    cross_points = np.random.rand(self.dim) < 0.9\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n                    velocities[i] = np.where(cross_points, v - population[i], velocities[i])\n\n\n                # Update position\n                population[i] = population[i] + velocities[i]\n\n                # Boundary handling\n                population[i] = np.clip(population[i], lb, ub)\n                \n\n                # Evaluate fitness\n                f = func(population[i])\n                self.budget -= 1\n\n                if f < personal_best_fitness[i]:\n                    personal_best_fitness[i] = f\n                    personal_best_positions[i] = population[i].copy()\n\n                    if f < global_best_fitness:\n                        global_best_fitness = f\n                        global_best_position = population[i].copy()\n\n            # Local search intensification: shrink search space around global best\n            lb_local = np.maximum(lb, global_best_position - self.local_search_radius)\n            ub_local = np.minimum(ub, global_best_position + self.local_search_radius)\n\n            # Move a subset of particles to the shrunken search space\n            num_local_search_particles = int(self.pop_size * 0.2)\n            local_search_indices = np.random.choice(self.pop_size, num_local_search_particles, replace=False)\n            population[local_search_indices] = np.random.uniform(lb_local, ub_local, size=(num_local_search_particles, self.dim))\n            \n            # Re-evaluate the fitness for these particles.\n            for i in local_search_indices:\n                f = func(population[i])\n                self.budget -= 1\n                if f < personal_best_fitness[i]:\n                    personal_best_fitness[i] = f\n                    personal_best_positions[i] = population[i].copy()\n\n                    if f < global_best_fitness:\n                        global_best_fitness = f\n                        global_best_position = population[i].copy()\n            \n            if self.budget <= 0:\n                break\n        \n        return global_best_fitness, global_best_position", "configspace": "", "generation": 4, "feedback": "The algorithm AdaptiveSwarmEvolution scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": "random_new", "metadata": {"llm_response": "# Description: An adaptive population-based algorithm that combines aspects of particle swarm optimization and differential evolution, with a focus on local search intensification using a shrinking neighborhood around promising solutions.\n# Code: \n```python\nimport numpy as np\n\nclass AdaptiveSwarmEvolution:\n    def __init__(self, budget=10000, dim=10, pop_size=20, inertia=0.7, c1=1.5, c2=1.5, de_rate=0.1, local_search_radius=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.inertia = inertia\n        self.c1 = c1\n        self.c2 = c2\n        self.de_rate = de_rate\n        self.local_search_radius = local_search_radius\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        \n        # Initialize population and velocities\n        population = np.random.uniform(lb, ub, size=(self.pop_size, self.dim))\n        velocities = np.random.uniform(-0.1 * (ub - lb), 0.1 * (ub - lb), size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        # Initialize best positions\n        personal_best_positions = population.copy()\n        personal_best_fitness = fitness.copy()\n        global_best_index = np.argmin(fitness)\n        global_best_position = population[global_best_index].copy()\n        global_best_fitness = fitness[global_best_index]\n        \n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Update velocity using PSO-like update\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n                velocities[i] = (self.inertia * velocities[i] +\n                                 self.c1 * r1 * (personal_best_positions[i] - population[i]) +\n                                 self.c2 * r2 * (global_best_position - population[i]))\n\n                # Differential Evolution component\n                if np.random.rand() < self.de_rate:\n                    idxs = np.random.choice(self.pop_size, 3, replace=False)\n                    v = population[idxs[0]] + 0.5 * (population[idxs[1]] - population[idxs[2]])\n                    \n                    # Crossover\n                    cross_points = np.random.rand(self.dim) < 0.9\n                    if not np.any(cross_points):\n                        cross_points[np.random.randint(0, self.dim)] = True\n                    velocities[i] = np.where(cross_points, v - population[i], velocities[i])\n\n\n                # Update position\n                population[i] = population[i] + velocities[i]\n\n                # Boundary handling\n                population[i] = np.clip(population[i], lb, ub)\n                \n\n                # Evaluate fitness\n                f = func(population[i])\n                self.budget -= 1\n\n                if f < personal_best_fitness[i]:\n                    personal_best_fitness[i] = f\n                    personal_best_positions[i] = population[i].copy()\n\n                    if f < global_best_fitness:\n                        global_best_fitness = f\n                        global_best_position = population[i].copy()\n\n            # Local search intensification: shrink search space around global best\n            lb_local = np.maximum(lb, global_best_position - self.local_search_radius)\n            ub_local = np.minimum(ub, global_best_position + self.local_search_radius)\n\n            # Move a subset of particles to the shrunken search space\n            num_local_search_particles = int(self.pop_size * 0.2)\n            local_search_indices = np.random.choice(self.pop_size, num_local_search_particles, replace=False)\n            population[local_search_indices] = np.random.uniform(lb_local, ub_local, size=(num_local_search_particles, self.dim))\n            \n            # Re-evaluate the fitness for these particles.\n            for i in local_search_indices:\n                f = func(population[i])\n                self.budget -= 1\n                if f < personal_best_fitness[i]:\n                    personal_best_fitness[i] = f\n                    personal_best_positions[i] = population[i].copy()\n\n                    if f < global_best_fitness:\n                        global_best_fitness = f\n                        global_best_position = population[i].copy()\n            \n            if self.budget <= 0:\n                break\n        \n        return global_best_fitness, global_best_position\n```", "aucs": [0], "operator": "random_new", "theta_sampled": 0.014324396610869365, "reward": 0.0, "generation": 4}, "task_prompt": ""}
{"id": "d902aa05-14d9-4781-8493-aa676f2685a5", "fitness": 0.0, "name": "AdaptiveCauchyShrinkOptimizer", "description": "Adaptive Cauchy mutation with shrinking search range and dynamic population size adjustment based on stagnation and diversity.", "code": "import numpy as np\n\nclass AdaptiveCauchyShrinkOptimizer:\n    def __init__(self, budget=10000, dim=10, initial_pop_size=50, shrink_factor=0.95, stagnation_threshold=1000, diversity_threshold=1.0):\n        self.budget = budget\n        self.dim = dim\n        self.initial_pop_size = initial_pop_size\n        self.pop_size = initial_pop_size # Current population size\n        self.lb = -5.0\n        self.ub = 5.0\n        self.shrink_factor = shrink_factor\n        self.stagnation_threshold = stagnation_threshold\n        self.diversity_threshold = diversity_threshold\n        self.stagnation_counter = 0\n        self.best_fitness_history = []\n        self.mutation_scale = 1.0\n\n\n    def initialize_population(self):\n        return np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n\n    def calculate_diversity(self, population):\n        centroid = np.mean(population, axis=0)\n        distances = np.linalg.norm(population - centroid, axis=1)\n        return np.mean(distances)\n\n    def mutate_cauchy(self, x, search_range):\n         return x + self.mutation_scale * search_range * np.random.standard_cauchy(size=self.dim) * 0.1\n\n    def correct_bounds(self, x):\n        return np.clip(x, self.lb, self.ub)\n\n    def adjust_population_size(self):\n        if self.stagnation_counter > self.stagnation_threshold:\n            self.pop_size = int(self.pop_size * 0.9)  # Reduce population size if stagnating\n            if self.pop_size < 10:\n                self.pop_size = 10 # Minimum population size\n\n            self.mutation_scale = 1.0 # Reset mutation scale\n            self.stagnation_counter = 0 # Reset stagnation counter\n            print(f\"Stagnation detected, reducing pop size to {self.pop_size}\")\n        elif self.calculate_diversity(self.population) < self.diversity_threshold and self.pop_size < self.initial_pop_size * 2:\n            self.pop_size = int(self.pop_size * 1.1)  # Increase population size if low diversity\n            if self.pop_size > self.initial_pop_size * 2:\n                self.pop_size = int(self.initial_pop_size * 2)\n            print(f\"Low diversity, increasing pop size to {self.pop_size}\")\n\n\n\n    def __call__(self, func):\n        self.population = self.initialize_population()\n        fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        \n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = self.population[best_index]\n        self.best_fitness_history.append(self.f_opt)\n\n        search_range = self.ub - self.lb\n        center = self.x_opt\n\n        while self.budget > 0:\n            diversity = self.calculate_diversity(self.population)\n            \n            # Cauchy mutation around the best individual\n            new_population = np.copy(self.population)\n            for i in range(self.pop_size):\n                if self.budget <= 0:\n                    break\n\n                mutation = self.mutate_cauchy(center, search_range)\n                mutation = self.correct_bounds(mutation)\n\n                f_mutation = func(mutation)\n                self.budget -= 1\n\n                if f_mutation < fitness[i]:\n                    fitness[i] = f_mutation\n                    new_population[i] = mutation\n\n                    if f_mutation < self.f_opt:\n                        self.f_opt = f_mutation\n                        self.x_opt = mutation\n                        center = self.x_opt\n            self.population = new_population\n            # Shrink the search range around the best individual\n            search_range *= self.shrink_factor\n            lb = np.clip(center - search_range/2, func.bounds.lb, func.bounds.ub)\n            ub = np.clip(center + search_range/2, func.bounds.lb, func.bounds.ub)\n\n            self.lb = lb\n            self.ub = ub\n\n            # Adapt mutation scale\n            self.mutation_scale = 0.95 * self.mutation_scale\n\n            # Stagnation detection and population size adjustment\n            if len(self.best_fitness_history) > self.stagnation_threshold:\n                if abs(self.f_opt - self.best_fitness_history[-self.stagnation_threshold]) < 1e-5:\n                    self.stagnation_counter += self.pop_size\n                else:\n                    self.stagnation_counter = 0\n\n            self.best_fitness_history.append(self.f_opt)\n            self.adjust_population_size()\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "", "error": "Evaluation error: index 50 is out of bounds for axis 0 with size 50", "parent_ids": ["e8a11899-492c-463b-9412-67910c8f787b", "6c537388-4ccd-49e7-95d1-ae15369d9854"], "operator": "crossover", "metadata": {"llm_response": "# Description: Adaptive Cauchy mutation with shrinking search range and dynamic population size adjustment based on stagnation and diversity.\n# Code: \n```python\nimport numpy as np\n\nclass AdaptiveCauchyShrinkOptimizer:\n    def __init__(self, budget=10000, dim=10, initial_pop_size=50, shrink_factor=0.95, stagnation_threshold=1000, diversity_threshold=1.0):\n        self.budget = budget\n        self.dim = dim\n        self.initial_pop_size = initial_pop_size\n        self.pop_size = initial_pop_size # Current population size\n        self.lb = -5.0\n        self.ub = 5.0\n        self.shrink_factor = shrink_factor\n        self.stagnation_threshold = stagnation_threshold\n        self.diversity_threshold = diversity_threshold\n        self.stagnation_counter = 0\n        self.best_fitness_history = []\n        self.mutation_scale = 1.0\n\n\n    def initialize_population(self):\n        return np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n\n    def calculate_diversity(self, population):\n        centroid = np.mean(population, axis=0)\n        distances = np.linalg.norm(population - centroid, axis=1)\n        return np.mean(distances)\n\n    def mutate_cauchy(self, x, search_range):\n         return x + self.mutation_scale * search_range * np.random.standard_cauchy(size=self.dim) * 0.1\n\n    def correct_bounds(self, x):\n        return np.clip(x, self.lb, self.ub)\n\n    def adjust_population_size(self):\n        if self.stagnation_counter > self.stagnation_threshold:\n            self.pop_size = int(self.pop_size * 0.9)  # Reduce population size if stagnating\n            if self.pop_size < 10:\n                self.pop_size = 10 # Minimum population size\n\n            self.mutation_scale = 1.0 # Reset mutation scale\n            self.stagnation_counter = 0 # Reset stagnation counter\n            print(f\"Stagnation detected, reducing pop size to {self.pop_size}\")\n        elif self.calculate_diversity(self.population) < self.diversity_threshold and self.pop_size < self.initial_pop_size * 2:\n            self.pop_size = int(self.pop_size * 1.1)  # Increase population size if low diversity\n            if self.pop_size > self.initial_pop_size * 2:\n                self.pop_size = int(self.initial_pop_size * 2)\n            print(f\"Low diversity, increasing pop size to {self.pop_size}\")\n\n\n\n    def __call__(self, func):\n        self.population = self.initialize_population()\n        fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        \n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = self.population[best_index]\n        self.best_fitness_history.append(self.f_opt)\n\n        search_range = self.ub - self.lb\n        center = self.x_opt\n\n        while self.budget > 0:\n            diversity = self.calculate_diversity(self.population)\n            \n            # Cauchy mutation around the best individual\n            new_population = np.copy(self.population)\n            for i in range(self.pop_size):\n                if self.budget <= 0:\n                    break\n\n                mutation = self.mutate_cauchy(center, search_range)\n                mutation = self.correct_bounds(mutation)\n\n                f_mutation = func(mutation)\n                self.budget -= 1\n\n                if f_mutation < fitness[i]:\n                    fitness[i] = f_mutation\n                    new_population[i] = mutation\n\n                    if f_mutation < self.f_opt:\n                        self.f_opt = f_mutation\n                        self.x_opt = mutation\n                        center = self.x_opt\n            self.population = new_population\n            # Shrink the search range around the best individual\n            search_range *= self.shrink_factor\n            lb = np.clip(center - search_range/2, func.bounds.lb, func.bounds.ub)\n            ub = np.clip(center + search_range/2, func.bounds.lb, func.bounds.ub)\n\n            self.lb = lb\n            self.ub = ub\n\n            # Adapt mutation scale\n            self.mutation_scale = 0.95 * self.mutation_scale\n\n            # Stagnation detection and population size adjustment\n            if len(self.best_fitness_history) > self.stagnation_threshold:\n                if abs(self.f_opt - self.best_fitness_history[-self.stagnation_threshold]) < 1e-5:\n                    self.stagnation_counter += self.pop_size\n                else:\n                    self.stagnation_counter = 0\n\n            self.best_fitness_history.append(self.f_opt)\n            self.adjust_population_size()\n\n\n        return self.f_opt, self.x_opt\n```", "operator": "crossover", "theta_sampled": 0.01811737224597937, "reward": 0.0, "generation": 4}, "task_prompt": ""}
{"id": "75faba1c-3c57-44f0-a7de-e868d809509f", "fitness": 0.0, "name": "SelfAdjustingMutationOptimizer", "description": "A self-adjusting mutation strategy optimizer that dynamically adapts mutation probabilities, scale, and population size based on diversity, stagnation detection, and successful mutation rate.", "code": "import numpy as np\n\nclass SelfAdjustingMutationOptimizer:\n    def __init__(self, budget=10000, dim=10, pop_size=50, stagnation_threshold=1000, success_threshold=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lb = -5.0\n        self.ub = 5.0\n        self.mutation_prob_gaussian = 0.5\n        self.mutation_prob_cauchy = 0.5\n        self.mutation_scale = 1.0\n        self.stagnation_threshold = stagnation_threshold\n        self.stagnation_counter = 0\n        self.best_fitness_history = []\n        self.success_threshold = success_threshold\n        self.success_rate = 0.0\n        self.success_counter = 0\n        self.mutation_type_weights = np.array([0.5, 0.5]) # Gaussian, Cauchy\n\n    def initialize_population(self):\n        return np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n\n    def calculate_diversity(self, population):\n        centroid = np.mean(population, axis=0)\n        distances = np.linalg.norm(population - centroid, axis=1)\n        return np.mean(distances)\n\n    def mutate_gaussian(self, x):\n        return x + self.mutation_scale * np.random.normal(0, 1, size=self.dim)\n\n    def mutate_cauchy(self, x):\n        return x + self.mutation_scale * np.random.standard_cauchy(size=self.dim)\n\n    def correct_bounds(self, x):\n        return np.clip(x, self.lb, self.ub)\n\n    def rank_based_selection(self, population, fitness, num_parents):\n        ranked_indices = np.argsort(fitness)\n        parent_indices = ranked_indices[:num_parents]\n        return population[parent_indices]\n\n    def adjust_population_size(self):\n        if self.stagnation_counter > self.stagnation_threshold:\n            self.pop_size = int(self.pop_size * 0.9)  # Reduce population size if stagnating\n            if self.pop_size < 10:\n                self.pop_size = 10 # Minimum population size\n\n            self.mutation_scale = 1.0 # Reset mutation scale\n            self.stagnation_counter = 0 # Reset stagnation counter\n            print(f\"Stagnation detected, reducing pop size to {self.pop_size}\")\n            # Increase diversity by re-initializing some individuals\n            num_reinit = int(self.pop_size * 0.2)\n            for i in range(num_reinit):\n                self.population[np.random.randint(0, self.pop_size)] = np.random.uniform(self.lb, self.ub, size=self.dim)\n\n    def adjust_mutation_probabilities(self):\n        # Adjust mutation probabilities based on success rate\n        if self.success_rate > self.success_threshold:\n            self.mutation_scale *= 1.05  # Increase mutation scale if successful\n            self.success_threshold *= 1.01  # Make success harder to achieve\n        else:\n            self.mutation_scale *= 0.95  # Decrease mutation scale if unsuccessful\n            self.success_threshold *= 0.99  # Make success easier to achieve\n        \n        # Adjust probabilities based on which mutation type is more successful\n        self.mutation_type_weights /= np.sum(self.mutation_type_weights) # Normalize weights\n\n    def __call__(self, func):\n        self.population = self.initialize_population()\n        fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        \n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = self.population[best_index]\n        self.best_fitness_history.append(self.f_opt)\n\n        while self.budget > 0:\n            diversity = self.calculate_diversity(self.population)\n\n            if diversity > 1.0:\n                self.mutation_prob_gaussian = 0.8\n                self.mutation_prob_cauchy = 0.2\n            else:\n                self.mutation_prob_gaussian = 0.2\n                self.mutation_prob_cauchy = 0.8\n\n            # Elitism: Keep the best individual\n            best_individual = np.copy(self.x_opt)\n            best_fitness = self.f_opt\n\n            # Rank-based selection of parents for mutation\n            num_parents = int(self.pop_size / 2)  # Select top half as parents\n            parents = self.rank_based_selection(self.population, fitness, num_parents)\n            \n            new_population = np.copy(self.population)\n            success_count = 0\n\n            for i in range(self.pop_size):\n                # Choose a parent randomly\n                parent = parents[np.random.randint(0, num_parents)]\n\n                # Choose mutation type based on weights\n                mutation_type = np.random.choice([0, 1], p=self.mutation_type_weights)  # 0: Gaussian, 1: Cauchy\n\n                if mutation_type == 0:\n                    new_population[i] = self.mutate_gaussian(parent)\n                else:\n                    new_population[i] = self.mutate_cauchy(parent)\n\n                new_population[i] = self.correct_bounds(new_population[i])\n                \n                new_fitness = func(new_population[i])\n                self.budget -= 1\n\n                if new_fitness < fitness[i]:\n                    fitness[i] = new_fitness\n                    self.population[i] = new_population[i]\n                    success_count += 1\n\n                    if new_fitness < self.f_opt:\n                        self.f_opt = new_fitness\n                        self.x_opt = new_population[i]\n                        \n                        # Update weights in favor of successful mutation type\n                        self.mutation_type_weights[mutation_type] *= 1.1 # Increase weight of successful type\n                else:\n                    if mutation_type == 0:\n                        self.mutation_type_weights[mutation_type] *= 0.9\n                    else:\n                        self.mutation_type_weights[mutation_type] *= 0.9\n            \n            # Replace worst individual with the best from previous generation (Elitism)\n            worst_index = np.argmax(fitness)\n            self.population[worst_index] = best_individual\n            fitness[worst_index] = best_fitness\n\n            # Adapt mutation scale\n            #self.mutation_scale = 0.95 * self.mutation_scale\n            self.success_rate = success_count / self.pop_size\n            self.adjust_mutation_probabilities()\n            # Stagnation detection and population size adjustment\n            if len(self.best_fitness_history) > self.stagnation_threshold:\n                if abs(self.f_opt - self.best_fitness_history[-self.stagnation_threshold]) < 1e-5:\n                    self.stagnation_counter += self.pop_size\n                else:\n                    self.stagnation_counter = 0\n\n            self.best_fitness_history.append(self.f_opt)\n            self.adjust_population_size()\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "", "error": "Evaluation error: probabilities do not sum to 1", "parent_ids": ["6c537388-4ccd-49e7-95d1-ae15369d9854"], "operator": "mutation", "metadata": {"llm_response": "# Description: A self-adjusting mutation strategy optimizer that dynamically adapts mutation probabilities, scale, and population size based on diversity, stagnation detection, and successful mutation rate.\n# Code: \n```python\nimport numpy as np\n\nclass SelfAdjustingMutationOptimizer:\n    def __init__(self, budget=10000, dim=10, pop_size=50, stagnation_threshold=1000, success_threshold=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lb = -5.0\n        self.ub = 5.0\n        self.mutation_prob_gaussian = 0.5\n        self.mutation_prob_cauchy = 0.5\n        self.mutation_scale = 1.0\n        self.stagnation_threshold = stagnation_threshold\n        self.stagnation_counter = 0\n        self.best_fitness_history = []\n        self.success_threshold = success_threshold\n        self.success_rate = 0.0\n        self.success_counter = 0\n        self.mutation_type_weights = np.array([0.5, 0.5]) # Gaussian, Cauchy\n\n    def initialize_population(self):\n        return np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n\n    def calculate_diversity(self, population):\n        centroid = np.mean(population, axis=0)\n        distances = np.linalg.norm(population - centroid, axis=1)\n        return np.mean(distances)\n\n    def mutate_gaussian(self, x):\n        return x + self.mutation_scale * np.random.normal(0, 1, size=self.dim)\n\n    def mutate_cauchy(self, x):\n        return x + self.mutation_scale * np.random.standard_cauchy(size=self.dim)\n\n    def correct_bounds(self, x):\n        return np.clip(x, self.lb, self.ub)\n\n    def rank_based_selection(self, population, fitness, num_parents):\n        ranked_indices = np.argsort(fitness)\n        parent_indices = ranked_indices[:num_parents]\n        return population[parent_indices]\n\n    def adjust_population_size(self):\n        if self.stagnation_counter > self.stagnation_threshold:\n            self.pop_size = int(self.pop_size * 0.9)  # Reduce population size if stagnating\n            if self.pop_size < 10:\n                self.pop_size = 10 # Minimum population size\n\n            self.mutation_scale = 1.0 # Reset mutation scale\n            self.stagnation_counter = 0 # Reset stagnation counter\n            print(f\"Stagnation detected, reducing pop size to {self.pop_size}\")\n            # Increase diversity by re-initializing some individuals\n            num_reinit = int(self.pop_size * 0.2)\n            for i in range(num_reinit):\n                self.population[np.random.randint(0, self.pop_size)] = np.random.uniform(self.lb, self.ub, size=self.dim)\n\n    def adjust_mutation_probabilities(self):\n        # Adjust mutation probabilities based on success rate\n        if self.success_rate > self.success_threshold:\n            self.mutation_scale *= 1.05  # Increase mutation scale if successful\n            self.success_threshold *= 1.01  # Make success harder to achieve\n        else:\n            self.mutation_scale *= 0.95  # Decrease mutation scale if unsuccessful\n            self.success_threshold *= 0.99  # Make success easier to achieve\n        \n        # Adjust probabilities based on which mutation type is more successful\n        self.mutation_type_weights /= np.sum(self.mutation_type_weights) # Normalize weights\n\n    def __call__(self, func):\n        self.population = self.initialize_population()\n        fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n        \n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = self.population[best_index]\n        self.best_fitness_history.append(self.f_opt)\n\n        while self.budget > 0:\n            diversity = self.calculate_diversity(self.population)\n\n            if diversity > 1.0:\n                self.mutation_prob_gaussian = 0.8\n                self.mutation_prob_cauchy = 0.2\n            else:\n                self.mutation_prob_gaussian = 0.2\n                self.mutation_prob_cauchy = 0.8\n\n            # Elitism: Keep the best individual\n            best_individual = np.copy(self.x_opt)\n            best_fitness = self.f_opt\n\n            # Rank-based selection of parents for mutation\n            num_parents = int(self.pop_size / 2)  # Select top half as parents\n            parents = self.rank_based_selection(self.population, fitness, num_parents)\n            \n            new_population = np.copy(self.population)\n            success_count = 0\n\n            for i in range(self.pop_size):\n                # Choose a parent randomly\n                parent = parents[np.random.randint(0, num_parents)]\n\n                # Choose mutation type based on weights\n                mutation_type = np.random.choice([0, 1], p=self.mutation_type_weights)  # 0: Gaussian, 1: Cauchy\n\n                if mutation_type == 0:\n                    new_population[i] = self.mutate_gaussian(parent)\n                else:\n                    new_population[i] = self.mutate_cauchy(parent)\n\n                new_population[i] = self.correct_bounds(new_population[i])\n                \n                new_fitness = func(new_population[i])\n                self.budget -= 1\n\n                if new_fitness < fitness[i]:\n                    fitness[i] = new_fitness\n                    self.population[i] = new_population[i]\n                    success_count += 1\n\n                    if new_fitness < self.f_opt:\n                        self.f_opt = new_fitness\n                        self.x_opt = new_population[i]\n                        \n                        # Update weights in favor of successful mutation type\n                        self.mutation_type_weights[mutation_type] *= 1.1 # Increase weight of successful type\n                else:\n                    if mutation_type == 0:\n                        self.mutation_type_weights[mutation_type] *= 0.9\n                    else:\n                        self.mutation_type_weights[mutation_type] *= 0.9\n            \n            # Replace worst individual with the best from previous generation (Elitism)\n            worst_index = np.argmax(fitness)\n            self.population[worst_index] = best_individual\n            fitness[worst_index] = best_fitness\n\n            # Adapt mutation scale\n            #self.mutation_scale = 0.95 * self.mutation_scale\n            self.success_rate = success_count / self.pop_size\n            self.adjust_mutation_probabilities()\n            # Stagnation detection and population size adjustment\n            if len(self.best_fitness_history) > self.stagnation_threshold:\n                if abs(self.f_opt - self.best_fitness_history[-self.stagnation_threshold]) < 1e-5:\n                    self.stagnation_counter += self.pop_size\n                else:\n                    self.stagnation_counter = 0\n\n            self.best_fitness_history.append(self.f_opt)\n            self.adjust_population_size()\n\n        return self.f_opt, self.x_opt\n```", "operator": "mutation", "theta_sampled": 0.01640467814296835, "reward": 0.0, "generation": 4}, "task_prompt": ""}
{"id": "d4ee1588-aca4-4fbb-b2a4-6cc5a3b7e2d9", "fitness": 0.0, "name": "GradientEnhancedPopulationSearch", "description": "An algorithm that combines a population-based approach with gradient estimation using finite differences for exploration and exploitation.", "code": "import numpy as np\n\nclass GradientEnhancedPopulationSearch:\n    def __init__(self, budget=10000, dim=10, pop_size=20, step_size=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.step_size = step_size # Step size for gradient estimation\n        self.population = None\n        self.fitness = None\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n\n    def estimate_gradient(self, func, x):\n        \"\"\"Estimates the gradient using finite differences.\"\"\"\n        gradient = np.zeros(self.dim)\n        for i in range(self.dim):\n            x_plus = x.copy()\n            x_minus = x.copy()\n            x_plus[i] += self.step_size\n            x_minus[i] -= self.step_size\n\n            # Ensure bounds are respected\n            x_plus[i] = np.clip(x_plus[i], func.bounds.lb[i], func.bounds.ub[i])\n            x_minus[i] = np.clip(x_minus[i], func.bounds.lb[i], func.bounds.ub[i])\n            \n            f_plus = func(x_plus)\n            f_minus = func(x_minus)\n            \n            gradient[i] = (f_plus - f_minus) / (2 * self.step_size)\n        return gradient\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        # Initialize the population\n        self.initialize_population(func)\n        best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_index]\n        self.x_opt = self.population[best_index]\n\n        while self.budget > 0:\n            # Select a random individual from the population\n            idx = np.random.randint(self.pop_size)\n            x = self.population[idx]\n\n            # Estimate the gradient at the selected individual\n            gradient = self.estimate_gradient(func, x)\n           \n            # Move in the direction of the negative gradient\n            x_new = x - self.step_size * gradient\n\n            # Ensure bounds are respected\n            x_new = np.clip(x_new, func.bounds.lb, func.bounds.ub)\n            \n            # Evaluate the new solution\n            f_new = func(x_new)\n            self.budget -= 1\n\n            # Update the population if the new solution is better\n            if f_new < self.fitness[idx]:\n                self.fitness[idx] = f_new\n                self.population[idx] = x_new\n\n                # Update the best solution found so far\n                if f_new < self.f_opt:\n                    self.f_opt = f_new\n                    self.x_opt = x_new\n            else:\n                 # Random walk for exploration if no improvement\n                x_rand = np.random.uniform(func.bounds.lb, func.bounds.ub)\n                f_rand = func(x_rand)\n                self.budget -= 1\n                \n                worst_index = np.argmax(self.fitness)\n                if f_rand < self.fitness[worst_index]:\n                    self.fitness[worst_index] = f_rand\n                    self.population[worst_index] = x_rand\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "The algorithm GradientEnhancedPopulationSearch scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": "random_new", "metadata": {"llm_response": "# Description: An algorithm that combines a population-based approach with gradient estimation using finite differences for exploration and exploitation.\n# Code:\n```python\nimport numpy as np\n\nclass GradientEnhancedPopulationSearch:\n    def __init__(self, budget=10000, dim=10, pop_size=20, step_size=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.step_size = step_size # Step size for gradient estimation\n        self.population = None\n        self.fitness = None\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.budget -= self.pop_size\n\n    def estimate_gradient(self, func, x):\n        \"\"\"Estimates the gradient using finite differences.\"\"\"\n        gradient = np.zeros(self.dim)\n        for i in range(self.dim):\n            x_plus = x.copy()\n            x_minus = x.copy()\n            x_plus[i] += self.step_size\n            x_minus[i] -= self.step_size\n\n            # Ensure bounds are respected\n            x_plus[i] = np.clip(x_plus[i], func.bounds.lb[i], func.bounds.ub[i])\n            x_minus[i] = np.clip(x_minus[i], func.bounds.lb[i], func.bounds.ub[i])\n            \n            f_plus = func(x_plus)\n            f_minus = func(x_minus)\n            \n            gradient[i] = (f_plus - f_minus) / (2 * self.step_size)\n        return gradient\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        # Initialize the population\n        self.initialize_population(func)\n        best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_index]\n        self.x_opt = self.population[best_index]\n\n        while self.budget > 0:\n            # Select a random individual from the population\n            idx = np.random.randint(self.pop_size)\n            x = self.population[idx]\n\n            # Estimate the gradient at the selected individual\n            gradient = self.estimate_gradient(func, x)\n           \n            # Move in the direction of the negative gradient\n            x_new = x - self.step_size * gradient\n\n            # Ensure bounds are respected\n            x_new = np.clip(x_new, func.bounds.lb, func.bounds.ub)\n            \n            # Evaluate the new solution\n            f_new = func(x_new)\n            self.budget -= 1\n\n            # Update the population if the new solution is better\n            if f_new < self.fitness[idx]:\n                self.fitness[idx] = f_new\n                self.population[idx] = x_new\n\n                # Update the best solution found so far\n                if f_new < self.f_opt:\n                    self.f_opt = f_new\n                    self.x_opt = x_new\n            else:\n                 # Random walk for exploration if no improvement\n                x_rand = np.random.uniform(func.bounds.lb, func.bounds.ub)\n                f_rand = func(x_rand)\n                self.budget -= 1\n                \n                worst_index = np.argmax(self.fitness)\n                if f_rand < self.fitness[worst_index]:\n                    self.fitness[worst_index] = f_rand\n                    self.population[worst_index] = x_rand\n\n        return self.f_opt, self.x_opt\n```", "aucs": [0], "operator": "random_new", "theta_sampled": 0.005920232156566521, "reward": 0.0, "generation": 5}, "task_prompt": ""}
{"id": "e895f6e9-5aa1-47fa-ada6-1eee4a9fb371", "fitness": 0.5300705380453716, "name": "HybridAdaptiveOptimizerV2", "description": "Dynamically adjusts mutation probabilities and exploration-exploitation balance using a combination of slime mold-inspired selection and an aging mechanism for diversity preservation and escape from local optima.", "code": "import numpy as np\n\nclass HybridAdaptiveOptimizerV2:\n    def __init__(self, budget=10000, dim=10, pop_size=50, stagnation_threshold=1000, w_decay=0.9, p_selection=0.2, initial_mutation_scale=1.0, age_threshold=50):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lb = -5.0\n        self.ub = 5.0\n        self.mutation_prob_gaussian = 0.5\n        self.mutation_prob_cauchy = 0.5\n        self.mutation_scale = initial_mutation_scale\n        self.stagnation_threshold = stagnation_threshold\n        self.stagnation_counter = 0\n        self.best_fitness_history = []\n        self.w_decay = w_decay\n        self.p_selection = p_selection\n        self.initial_pop_size = pop_size\n        self.age_threshold = age_threshold  # Threshold for individual age\n        self.ages = np.zeros(pop_size)  # Initialize ages for each individual\n\n    def initialize_population(self):\n        return np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n\n    def calculate_diversity(self, population):\n        centroid = np.mean(population, axis=0)\n        distances = np.linalg.norm(population - centroid, axis=1)\n        return np.mean(distances)\n\n    def mutate_gaussian(self, x):\n        return x + self.mutation_scale * np.random.normal(0, 1, size=self.dim)\n\n    def mutate_cauchy(self, x):\n        return x + self.mutation_scale * np.random.standard_cauchy(size=self.dim)\n\n    def correct_bounds(self, x):\n        return np.clip(x, self.lb, self.ub)\n\n    def rank_based_selection(self, population, fitness, num_parents):\n        ranked_indices = np.argsort(fitness)\n        parent_indices = ranked_indices[:num_parents]\n        return population[parent_indices]\n    \n    def adjust_population_size(self):\n        if self.stagnation_counter > self.stagnation_threshold:\n            self.pop_size = int(self.pop_size * 0.9)  # Reduce population size if stagnating\n            if self.pop_size < 10:\n                self.pop_size = 10 # Minimum population size\n\n            self.mutation_scale = self.initial_mutation_scale # Reset mutation scale\n            self.stagnation_counter = 0 # Reset stagnation counter\n            self.ages = np.zeros(self.pop_size) #Reset ages\n            print(f\"Stagnation detected, reducing pop size to {self.pop_size}\")\n        elif self.pop_size < self.initial_pop_size and len(self.best_fitness_history) > self.stagnation_threshold and abs(self.f_opt - self.best_fitness_history[-self.stagnation_threshold]) > 1e-5:\n            #Potentially increase the pop size if we are not stagnating. \n            self.pop_size = int(min(self.pop_size * 1.1, self.initial_pop_size))\n            print(f\"Progress detected, increasing pop size to {self.pop_size}\")\n\n    def slime_mold_selection(self, population, fitness):\n        \"\"\"Selects parents based on slime mold weighting.\"\"\"\n        sorted_indices = np.argsort(fitness)\n        population = population[sorted_indices]\n        fitness = fitness[sorted_indices]\n\n        num_selected = int(self.p_selection * self.pop_size)\n        selected_population = population[:num_selected]\n        selected_fitness = fitness[:num_selected]\n\n        rank = np.arange(1, num_selected + 1)\n        weights_selected = np.exp(-rank / num_selected)\n        weights_selected /= np.sum(weights_selected)\n        return selected_population, weights_selected\n    \n    def __call__(self, func):\n        population = self.initialize_population()\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n        \n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index]\n        self.best_fitness_history.append(self.f_opt)\n\n        while self.budget > 0:\n            diversity = self.calculate_diversity(population)\n\n            if diversity > 1.0:\n                self.mutation_prob_gaussian = 0.8\n                self.mutation_prob_cauchy = 0.2\n            else:\n                self.mutation_prob_gaussian = 0.2\n                self.mutation_prob_cauchy = 0.8\n\n            # Elitism: Keep the best individual\n            best_individual = np.copy(self.x_opt)\n            best_fitness = self.f_opt\n\n            # Slime mold inspired parent selection\n            parents, weights = self.slime_mold_selection(population, fitness)\n            num_parents = len(parents)\n            \n            new_population = np.copy(population)\n            for i in range(self.pop_size):\n                # Choose a parent based on slime mold weights\n                parent_index = np.random.choice(num_parents, p=weights)\n                parent = parents[parent_index]\n\n                # Aging mechanism: increase age if not improving\n                if fitness[i] == np.min(fitness):\n                    self.ages[i] = 0  # Reset age if individual is the best\n                else:\n                    self.ages[i] += 1\n\n                # If an individual is \"old,\" force Cauchy mutation for exploration\n                if self.ages[i] > self.age_threshold:\n                    new_population[i] = self.mutate_cauchy(parent)\n                elif np.random.rand() < self.mutation_prob_gaussian:\n                    new_population[i] = self.mutate_gaussian(parent)\n                else:\n                    new_population[i] = self.mutate_cauchy(parent)\n\n                new_population[i] = self.correct_bounds(new_population[i])\n                \n                new_fitness = func(new_population[i])\n                self.budget -= 1\n\n                if new_fitness < fitness[i]:\n                    fitness[i] = new_fitness\n                    population[i] = new_population[i]\n                    self.ages[i] = 0 #Reset age\n\n                    if new_fitness < self.f_opt:\n                        self.f_opt = new_fitness\n                        self.x_opt = new_population[i]\n            \n            # Replace worst individual with the best from previous generation (Elitism)\n            worst_index = np.argmax(fitness)\n            population[worst_index] = best_individual\n            fitness[worst_index] = best_fitness\n            self.ages[worst_index] = 0 #Reset age for elitism\n\n            # Adapt mutation scale\n            self.mutation_scale = 0.95 * self.mutation_scale\n\n            # Stagnation detection and population size adjustment\n            if len(self.best_fitness_history) > self.stagnation_threshold:\n                if abs(self.f_opt - self.best_fitness_history[-self.stagnation_threshold]) < 1e-5:\n                    self.stagnation_counter += self.pop_size\n                else:\n                    self.stagnation_counter = 0\n\n            self.best_fitness_history.append(self.f_opt)\n            self.adjust_population_size()\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "The algorithm HybridAdaptiveOptimizerV2 scored 0.530 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["ddb255ad-fa53-4028-8bc5-cf4dcdf480e3"], "operator": "mutation", "metadata": {"llm_response": "# Description: Dynamically adjusts mutation probabilities and exploration-exploitation balance using a combination of slime mold-inspired selection and an aging mechanism for diversity preservation and escape from local optima.\n# Code: \n```python\nimport numpy as np\n\nclass HybridAdaptiveOptimizerV2:\n    def __init__(self, budget=10000, dim=10, pop_size=50, stagnation_threshold=1000, w_decay=0.9, p_selection=0.2, initial_mutation_scale=1.0, age_threshold=50):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lb = -5.0\n        self.ub = 5.0\n        self.mutation_prob_gaussian = 0.5\n        self.mutation_prob_cauchy = 0.5\n        self.mutation_scale = initial_mutation_scale\n        self.stagnation_threshold = stagnation_threshold\n        self.stagnation_counter = 0\n        self.best_fitness_history = []\n        self.w_decay = w_decay\n        self.p_selection = p_selection\n        self.initial_pop_size = pop_size\n        self.age_threshold = age_threshold  # Threshold for individual age\n        self.ages = np.zeros(pop_size)  # Initialize ages for each individual\n\n    def initialize_population(self):\n        return np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n\n    def calculate_diversity(self, population):\n        centroid = np.mean(population, axis=0)\n        distances = np.linalg.norm(population - centroid, axis=1)\n        return np.mean(distances)\n\n    def mutate_gaussian(self, x):\n        return x + self.mutation_scale * np.random.normal(0, 1, size=self.dim)\n\n    def mutate_cauchy(self, x):\n        return x + self.mutation_scale * np.random.standard_cauchy(size=self.dim)\n\n    def correct_bounds(self, x):\n        return np.clip(x, self.lb, self.ub)\n\n    def rank_based_selection(self, population, fitness, num_parents):\n        ranked_indices = np.argsort(fitness)\n        parent_indices = ranked_indices[:num_parents]\n        return population[parent_indices]\n    \n    def adjust_population_size(self):\n        if self.stagnation_counter > self.stagnation_threshold:\n            self.pop_size = int(self.pop_size * 0.9)  # Reduce population size if stagnating\n            if self.pop_size < 10:\n                self.pop_size = 10 # Minimum population size\n\n            self.mutation_scale = self.initial_mutation_scale # Reset mutation scale\n            self.stagnation_counter = 0 # Reset stagnation counter\n            self.ages = np.zeros(self.pop_size) #Reset ages\n            print(f\"Stagnation detected, reducing pop size to {self.pop_size}\")\n        elif self.pop_size < self.initial_pop_size and len(self.best_fitness_history) > self.stagnation_threshold and abs(self.f_opt - self.best_fitness_history[-self.stagnation_threshold]) > 1e-5:\n            #Potentially increase the pop size if we are not stagnating. \n            self.pop_size = int(min(self.pop_size * 1.1, self.initial_pop_size))\n            print(f\"Progress detected, increasing pop size to {self.pop_size}\")\n\n    def slime_mold_selection(self, population, fitness):\n        \"\"\"Selects parents based on slime mold weighting.\"\"\"\n        sorted_indices = np.argsort(fitness)\n        population = population[sorted_indices]\n        fitness = fitness[sorted_indices]\n\n        num_selected = int(self.p_selection * self.pop_size)\n        selected_population = population[:num_selected]\n        selected_fitness = fitness[:num_selected]\n\n        rank = np.arange(1, num_selected + 1)\n        weights_selected = np.exp(-rank / num_selected)\n        weights_selected /= np.sum(weights_selected)\n        return selected_population, weights_selected\n    \n    def __call__(self, func):\n        population = self.initialize_population()\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n        \n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index]\n        self.best_fitness_history.append(self.f_opt)\n\n        while self.budget > 0:\n            diversity = self.calculate_diversity(population)\n\n            if diversity > 1.0:\n                self.mutation_prob_gaussian = 0.8\n                self.mutation_prob_cauchy = 0.2\n            else:\n                self.mutation_prob_gaussian = 0.2\n                self.mutation_prob_cauchy = 0.8\n\n            # Elitism: Keep the best individual\n            best_individual = np.copy(self.x_opt)\n            best_fitness = self.f_opt\n\n            # Slime mold inspired parent selection\n            parents, weights = self.slime_mold_selection(population, fitness)\n            num_parents = len(parents)\n            \n            new_population = np.copy(population)\n            for i in range(self.pop_size):\n                # Choose a parent based on slime mold weights\n                parent_index = np.random.choice(num_parents, p=weights)\n                parent = parents[parent_index]\n\n                # Aging mechanism: increase age if not improving\n                if fitness[i] == np.min(fitness):\n                    self.ages[i] = 0  # Reset age if individual is the best\n                else:\n                    self.ages[i] += 1\n\n                # If an individual is \"old,\" force Cauchy mutation for exploration\n                if self.ages[i] > self.age_threshold:\n                    new_population[i] = self.mutate_cauchy(parent)\n                elif np.random.rand() < self.mutation_prob_gaussian:\n                    new_population[i] = self.mutate_gaussian(parent)\n                else:\n                    new_population[i] = self.mutate_cauchy(parent)\n\n                new_population[i] = self.correct_bounds(new_population[i])\n                \n                new_fitness = func(new_population[i])\n                self.budget -= 1\n\n                if new_fitness < fitness[i]:\n                    fitness[i] = new_fitness\n                    population[i] = new_population[i]\n                    self.ages[i] = 0 #Reset age\n\n                    if new_fitness < self.f_opt:\n                        self.f_opt = new_fitness\n                        self.x_opt = new_population[i]\n            \n            # Replace worst individual with the best from previous generation (Elitism)\n            worst_index = np.argmax(fitness)\n            population[worst_index] = best_individual\n            fitness[worst_index] = best_fitness\n            self.ages[worst_index] = 0 #Reset age for elitism\n\n            # Adapt mutation scale\n            self.mutation_scale = 0.95 * self.mutation_scale\n\n            # Stagnation detection and population size adjustment\n            if len(self.best_fitness_history) > self.stagnation_threshold:\n                if abs(self.f_opt - self.best_fitness_history[-self.stagnation_threshold]) < 1e-5:\n                    self.stagnation_counter += self.pop_size\n                else:\n                    self.stagnation_counter = 0\n\n            self.best_fitness_history.append(self.f_opt)\n            self.adjust_population_size()\n\n\n        return self.f_opt, self.x_opt\n```", "aucs": [0.25090630826801197, 0.3265163670695299, 0.5474040547600134, 0.8408809729328885, 0.5477323804223084, 0.6091059457455896, 0.3172903546763609, 0.4776853461640469, 0.5541136320277102, 0.290166227212293, 0.826241341166006, 0.9950397800482796, 0.22559337230068044, 0.29978669667306934, 0.859032103851312, 0.5971372775904453, 0.5087428318888082, 0.7224770852470641, 0.2826307051045628, 0.5229279777584532], "operator": "mutation", "theta_sampled": 0.02796781150600238, "reward": 0.0, "generation": 5}, "task_prompt": ""}
{"id": "2490d4be-f4cf-4c94-836d-98a90f0777d2", "fitness": 0.6021983328044098, "name": "AdaptiveCauchyOptimizer", "description": "Combines Cauchy mutation with adaptive population size and diversity-based mutation probability for enhanced exploration and exploitation.", "code": "import numpy as np\n\nclass AdaptiveCauchyOptimizer:\n    def __init__(self, budget=10000, dim=10, initial_pop_size=20, shrink_factor=0.95, stagnation_threshold=1000, cauchy_scale=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.initial_pop_size = initial_pop_size\n        self.pop_size = initial_pop_size\n        self.shrink_factor = shrink_factor\n        self.stagnation_threshold = stagnation_threshold\n        self.cauchy_scale = cauchy_scale\n        self.lb = -5.0\n        self.ub = 5.0\n        self.mutation_prob_gaussian = 0.5\n        self.mutation_prob_cauchy = 0.5\n        self.stagnation_counter = 0\n        self.best_fitness_history = []\n        self.search_range = self.ub - self.lb\n        self.mutation_scale = 1.0 # Scale for Gaussian mutation\n\n\n    def initialize_population(self):\n        return np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n\n    def calculate_diversity(self, population):\n        centroid = np.mean(population, axis=0)\n        distances = np.linalg.norm(population - centroid, axis=1)\n        return np.mean(distances)\n\n    def mutate_gaussian(self, x):\n        return x + self.mutation_scale * np.random.normal(0, 1, size=self.dim)\n\n    def mutate_cauchy(self, x):\n         return x + self.search_range * np.random.standard_cauchy(size=self.dim) * self.cauchy_scale\n\n    def correct_bounds(self, x):\n        return np.clip(x, self.lb, self.ub)\n\n    def adjust_population_size(self):\n        if self.stagnation_counter > self.stagnation_threshold:\n            self.pop_size = int(self.pop_size * 0.9)  # Reduce population size if stagnating\n            if self.pop_size < 10:\n                self.pop_size = 10 # Minimum population size\n            self.stagnation_counter = 0 # Reset stagnation counter\n            self.mutation_scale = 1.0\n            print(f\"Stagnation detected, reducing pop size to {self.pop_size}\")\n\n\n    def __call__(self, func):\n        population = self.initialize_population()\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n        \n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index]\n        self.best_fitness_history.append(self.f_opt)\n\n        center = self.x_opt\n\n\n        while self.budget > 0:\n            diversity = self.calculate_diversity(population)\n\n            if diversity > 1.0:\n                self.mutation_prob_gaussian = 0.8\n                self.mutation_prob_cauchy = 0.2\n            else:\n                self.mutation_prob_gaussian = 0.2\n                self.mutation_prob_cauchy = 0.8\n\n            new_population = np.copy(population)\n            new_fitness = np.copy(fitness)\n\n            for i in range(self.pop_size):\n                if np.random.rand() < self.mutation_prob_gaussian:\n                    mutated_individual = self.mutate_gaussian(population[i])\n                else:\n                    mutated_individual = self.mutate_cauchy(population[i])\n\n                mutated_individual = self.correct_bounds(mutated_individual)\n\n                f_mutated = func(mutated_individual)\n                self.budget -= 1\n\n                if f_mutated < new_fitness[i]:\n                  new_fitness[i] = f_mutated\n                  new_population[i] = mutated_individual\n                  if f_mutated < self.f_opt:\n                    self.f_opt = f_mutated\n                    self.x_opt = mutated_individual\n                    center = self.x_opt\n\n\n            # Replace the current population with the new one\n            population = new_population\n            fitness = new_fitness\n\n\n            # Shrink the search range around the best individual\n            self.search_range *= self.shrink_factor\n            self.lb = np.clip(center - self.search_range/2, -5.0, 5.0)\n            self.ub = np.clip(center + self.search_range/2, -5.0, 5.0)\n\n            # Adapt mutation scale\n            self.mutation_scale = 0.95 * self.mutation_scale\n\n            # Stagnation detection and population size adjustment\n            if len(self.best_fitness_history) > self.stagnation_threshold:\n                if abs(self.f_opt - self.best_fitness_history[-self.stagnation_threshold]) < 1e-5:\n                    self.stagnation_counter += self.pop_size\n                else:\n                    self.stagnation_counter = 0\n\n            self.best_fitness_history.append(self.f_opt)\n            self.adjust_population_size()\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "The algorithm AdaptiveCauchyOptimizer scored 0.602 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["e8a11899-492c-463b-9412-67910c8f787b", "9724a9db-9628-4a51-89b9-212121651f89"], "operator": "crossover", "metadata": {"llm_response": "# Description: Combines Cauchy mutation with adaptive population size and diversity-based mutation probability for enhanced exploration and exploitation.\n# Code: \n```python\nimport numpy as np\n\nclass AdaptiveCauchyOptimizer:\n    def __init__(self, budget=10000, dim=10, initial_pop_size=20, shrink_factor=0.95, stagnation_threshold=1000, cauchy_scale=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.initial_pop_size = initial_pop_size\n        self.pop_size = initial_pop_size\n        self.shrink_factor = shrink_factor\n        self.stagnation_threshold = stagnation_threshold\n        self.cauchy_scale = cauchy_scale\n        self.lb = -5.0\n        self.ub = 5.0\n        self.mutation_prob_gaussian = 0.5\n        self.mutation_prob_cauchy = 0.5\n        self.stagnation_counter = 0\n        self.best_fitness_history = []\n        self.search_range = self.ub - self.lb\n        self.mutation_scale = 1.0 # Scale for Gaussian mutation\n\n\n    def initialize_population(self):\n        return np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n\n    def calculate_diversity(self, population):\n        centroid = np.mean(population, axis=0)\n        distances = np.linalg.norm(population - centroid, axis=1)\n        return np.mean(distances)\n\n    def mutate_gaussian(self, x):\n        return x + self.mutation_scale * np.random.normal(0, 1, size=self.dim)\n\n    def mutate_cauchy(self, x):\n         return x + self.search_range * np.random.standard_cauchy(size=self.dim) * self.cauchy_scale\n\n    def correct_bounds(self, x):\n        return np.clip(x, self.lb, self.ub)\n\n    def adjust_population_size(self):\n        if self.stagnation_counter > self.stagnation_threshold:\n            self.pop_size = int(self.pop_size * 0.9)  # Reduce population size if stagnating\n            if self.pop_size < 10:\n                self.pop_size = 10 # Minimum population size\n            self.stagnation_counter = 0 # Reset stagnation counter\n            self.mutation_scale = 1.0\n            print(f\"Stagnation detected, reducing pop size to {self.pop_size}\")\n\n\n    def __call__(self, func):\n        population = self.initialize_population()\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n        \n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index]\n        self.best_fitness_history.append(self.f_opt)\n\n        center = self.x_opt\n\n\n        while self.budget > 0:\n            diversity = self.calculate_diversity(population)\n\n            if diversity > 1.0:\n                self.mutation_prob_gaussian = 0.8\n                self.mutation_prob_cauchy = 0.2\n            else:\n                self.mutation_prob_gaussian = 0.2\n                self.mutation_prob_cauchy = 0.8\n\n            new_population = np.copy(population)\n            new_fitness = np.copy(fitness)\n\n            for i in range(self.pop_size):\n                if np.random.rand() < self.mutation_prob_gaussian:\n                    mutated_individual = self.mutate_gaussian(population[i])\n                else:\n                    mutated_individual = self.mutate_cauchy(population[i])\n\n                mutated_individual = self.correct_bounds(mutated_individual)\n\n                f_mutated = func(mutated_individual)\n                self.budget -= 1\n\n                if f_mutated < new_fitness[i]:\n                  new_fitness[i] = f_mutated\n                  new_population[i] = mutated_individual\n                  if f_mutated < self.f_opt:\n                    self.f_opt = f_mutated\n                    self.x_opt = mutated_individual\n                    center = self.x_opt\n\n\n            # Replace the current population with the new one\n            population = new_population\n            fitness = new_fitness\n\n\n            # Shrink the search range around the best individual\n            self.search_range *= self.shrink_factor\n            self.lb = np.clip(center - self.search_range/2, -5.0, 5.0)\n            self.ub = np.clip(center + self.search_range/2, -5.0, 5.0)\n\n            # Adapt mutation scale\n            self.mutation_scale = 0.95 * self.mutation_scale\n\n            # Stagnation detection and population size adjustment\n            if len(self.best_fitness_history) > self.stagnation_threshold:\n                if abs(self.f_opt - self.best_fitness_history[-self.stagnation_threshold]) < 1e-5:\n                    self.stagnation_counter += self.pop_size\n                else:\n                    self.stagnation_counter = 0\n\n            self.best_fitness_history.append(self.f_opt)\n            self.adjust_population_size()\n\n        return self.f_opt, self.x_opt\n```", "aucs": [0.1808384374889539, 0.3317070511233189, 0.7657718583101127, 0.8936634544063817, 0.7837479110218428, 0.8044225665086684, 0.3580859367192013, 0.7249021578443643, 0.7755722322645945, 0.23081982352631025, 0.8832565285175099, 0.9919838907486118, 0.2470899059475652, 0.7603600404086786, 0.7465423629193257, 0.37863920348712254, 0.4394791170702531, 0.862048383492049, 0.37844820849283245, 0.5065875857904998], "operator": "crossover", "theta_sampled": 0.004405583107996319, "reward": 0.03022452877501869, "generation": 5}, "task_prompt": ""}
{"id": "85807d97-1f52-4cd8-a29a-cbab289180b4", "fitness": 0.0, "name": "HypersphereDifferentialEvolution", "description": "A population-based algorithm that uses a combination of differential evolution and a shrinking hypersphere to focus the search.", "code": "import numpy as np\n\nclass HypersphereDifferentialEvolution:\n    def __init__(self, budget=10000, dim=10, pop_size=20, F=0.7, CR=0.9, shrink_rate=0.99):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.shrink_rate = shrink_rate\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        \n        # Initialize population within hypersphere\n        radius = (ub - lb) / 2\n        center = (ub + lb) / 2  # Initialize at the center of the search space\n        self.population = np.zeros((self.pop_size, self.dim))\n        for i in range(self.pop_size):\n            # Generate random point inside hypersphere\n            while True:\n                x = np.random.uniform(-1, 1, self.dim)\n                if np.linalg.norm(x) <= 1:\n                    break\n            self.population[i] = center + radius * x\n\n        self.fitness = np.array([func(self.population[i]) for i in range(self.pop_size)])\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n        eval_count = self.pop_size\n\n        while eval_count < self.budget:\n            # Shrink the hypersphere\n            radius *= self.shrink_rate\n            \n            for i in range(self.pop_size):\n                # Differential Evolution\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = np.random.choice(idxs, 3, replace=False)\n                \n                # Ensure population is inside shrinking hypersphere\n                x_trial = self.population[i].copy()\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR:\n                        x_trial[j] = self.population[a, j] + self.F * (self.population[b, j] - self.population[c, j])\n                    \n                # Bound repair and hypersphere check\n                x_trial = np.clip(x_trial, lb, ub)\n                \n                # Project back inside the shrunken hypersphere if it is outside\n                direction = x_trial - center\n                if np.linalg.norm(direction) > radius:\n                    x_trial = center + radius * direction / np.linalg.norm(direction)\n                \n\n                f_trial = func(x_trial)\n                eval_count += 1\n                \n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.population[i] = x_trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = x_trial\n\n                if eval_count >= self.budget:\n                    break\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "", "error": "Evaluation error: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()", "parent_ids": [], "operator": "random_new", "metadata": {"llm_response": "# Description: A population-based algorithm that uses a combination of differential evolution and a shrinking hypersphere to focus the search.\n\n# Code: \n```python\nimport numpy as np\n\nclass HypersphereDifferentialEvolution:\n    def __init__(self, budget=10000, dim=10, pop_size=20, F=0.7, CR=0.9, shrink_rate=0.99):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.shrink_rate = shrink_rate\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        \n        # Initialize population within hypersphere\n        radius = (ub - lb) / 2\n        center = (ub + lb) / 2  # Initialize at the center of the search space\n        self.population = np.zeros((self.pop_size, self.dim))\n        for i in range(self.pop_size):\n            # Generate random point inside hypersphere\n            while True:\n                x = np.random.uniform(-1, 1, self.dim)\n                if np.linalg.norm(x) <= 1:\n                    break\n            self.population[i] = center + radius * x\n\n        self.fitness = np.array([func(self.population[i]) for i in range(self.pop_size)])\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n        eval_count = self.pop_size\n\n        while eval_count < self.budget:\n            # Shrink the hypersphere\n            radius *= self.shrink_rate\n            \n            for i in range(self.pop_size):\n                # Differential Evolution\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = np.random.choice(idxs, 3, replace=False)\n                \n                # Ensure population is inside shrinking hypersphere\n                x_trial = self.population[i].copy()\n                for j in range(self.dim):\n                    if np.random.rand() < self.CR:\n                        x_trial[j] = self.population[a, j] + self.F * (self.population[b, j] - self.population[c, j])\n                    \n                # Bound repair and hypersphere check\n                x_trial = np.clip(x_trial, lb, ub)\n                \n                # Project back inside the shrunken hypersphere if it is outside\n                direction = x_trial - center\n                if np.linalg.norm(direction) > radius:\n                    x_trial = center + radius * direction / np.linalg.norm(direction)\n                \n\n                f_trial = func(x_trial)\n                eval_count += 1\n                \n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.population[i] = x_trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = x_trial\n\n                if eval_count >= self.budget:\n                    break\n        return self.f_opt, self.x_opt\n```", "operator": "random_new", "theta_sampled": 0.016714203116656556, "reward": 0.0, "generation": 5}, "task_prompt": ""}
{"id": "14141113-7977-460c-9182-d881a514090d", "fitness": 0.6173029967372639, "name": "AdaptiveShrinkingCauchyOptimizer", "description": "An adaptive shrinking Cauchy optimizer that adjusts the Cauchy distribution scale based on the success rate of mutations and uses a local search to fine-tune the best solution.", "code": "import numpy as np\n\nclass AdaptiveShrinkingCauchyOptimizer:\n    def __init__(self, budget=10000, dim=10, population_size=20, shrink_factor=0.95, initial_cauchy_scale=0.1, local_search_iterations=5):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = population_size\n        self.shrink_factor = shrink_factor\n        self.cauchy_scale = initial_cauchy_scale\n        self.success_rate = 0.5  # Initialize success rate\n        self.local_search_iterations = local_search_iterations\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        population = np.random.uniform(lb, ub, size=(self.population_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.population_size\n\n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index]\n\n        search_range = ub - lb\n        center = self.x_opt\n\n        successes = 0\n        iterations = 0\n\n        while self.budget > 0:\n            # Cauchy mutation around the best individual\n            for i in range(self.population_size):\n                if self.budget <= 0:\n                    break\n                mutation = center + search_range * np.random.standard_cauchy(size=self.dim) * self.cauchy_scale  # Scale Cauchy distribution\n                mutation = np.clip(mutation, lb, ub)  # Clip values to stay within bounds\n\n                f_mutation = func(mutation)\n                self.budget -= 1\n                iterations += 1\n\n                if f_mutation < fitness[i]:\n                    fitness[i] = f_mutation\n                    population[i] = mutation\n                    successes += 1\n\n                    if f_mutation < self.f_opt:\n                        self.f_opt = f_mutation\n                        self.x_opt = mutation\n                        center = self.x_opt\n\n            # Shrink the search range around the best individual\n            search_range *= self.shrink_factor\n            lb = np.clip(center - search_range/2, func.bounds.lb, func.bounds.ub)\n            ub = np.clip(center + search_range/2, func.bounds.lb, func.bounds.ub)\n\n            # Adjust Cauchy scale based on success rate\n            if iterations > 0:\n                self.success_rate = successes / iterations\n                if self.success_rate > 0.2:\n                    self.cauchy_scale *= 1.1  # Increase scale if successful\n                else:\n                    self.cauchy_scale *= 0.9  # Decrease scale if not successful\n                successes = 0\n                iterations = 0\n\n            self.cauchy_scale = np.clip(self.cauchy_scale, 0.01, 1.0) # Limit Cauchy scale\n        \n        # Local Search around best solution\n        for _ in range(min(self.local_search_iterations, self.budget)):\n             if self.budget <= 0:\n                 break\n\n             perturbation = np.random.uniform(-search_range/100, search_range/100, self.dim)\n             local_x = np.clip(self.x_opt + perturbation, lb, ub)\n             local_f = func(local_x)\n             self.budget -=1\n\n             if local_f < self.f_opt:\n                  self.f_opt = local_f\n                  self.x_opt = local_x\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "The algorithm AdaptiveShrinkingCauchyOptimizer scored 0.617 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["e8a11899-492c-463b-9412-67910c8f787b"], "operator": "mutation", "metadata": {"llm_response": "# Description: An adaptive shrinking Cauchy optimizer that adjusts the Cauchy distribution scale based on the success rate of mutations and uses a local search to fine-tune the best solution.\n# Code:\n```python\nimport numpy as np\n\nclass AdaptiveShrinkingCauchyOptimizer:\n    def __init__(self, budget=10000, dim=10, population_size=20, shrink_factor=0.95, initial_cauchy_scale=0.1, local_search_iterations=5):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = population_size\n        self.shrink_factor = shrink_factor\n        self.cauchy_scale = initial_cauchy_scale\n        self.success_rate = 0.5  # Initialize success rate\n        self.local_search_iterations = local_search_iterations\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        population = np.random.uniform(lb, ub, size=(self.population_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.population_size\n\n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index]\n\n        search_range = ub - lb\n        center = self.x_opt\n\n        successes = 0\n        iterations = 0\n\n        while self.budget > 0:\n            # Cauchy mutation around the best individual\n            for i in range(self.population_size):\n                if self.budget <= 0:\n                    break\n                mutation = center + search_range * np.random.standard_cauchy(size=self.dim) * self.cauchy_scale  # Scale Cauchy distribution\n                mutation = np.clip(mutation, lb, ub)  # Clip values to stay within bounds\n\n                f_mutation = func(mutation)\n                self.budget -= 1\n                iterations += 1\n\n                if f_mutation < fitness[i]:\n                    fitness[i] = f_mutation\n                    population[i] = mutation\n                    successes += 1\n\n                    if f_mutation < self.f_opt:\n                        self.f_opt = f_mutation\n                        self.x_opt = mutation\n                        center = self.x_opt\n\n            # Shrink the search range around the best individual\n            search_range *= self.shrink_factor\n            lb = np.clip(center - search_range/2, func.bounds.lb, func.bounds.ub)\n            ub = np.clip(center + search_range/2, func.bounds.lb, func.bounds.ub)\n\n            # Adjust Cauchy scale based on success rate\n            if iterations > 0:\n                self.success_rate = successes / iterations\n                if self.success_rate > 0.2:\n                    self.cauchy_scale *= 1.1  # Increase scale if successful\n                else:\n                    self.cauchy_scale *= 0.9  # Decrease scale if not successful\n                successes = 0\n                iterations = 0\n\n            self.cauchy_scale = np.clip(self.cauchy_scale, 0.01, 1.0) # Limit Cauchy scale\n        \n        # Local Search around best solution\n        for _ in range(min(self.local_search_iterations, self.budget)):\n             if self.budget <= 0:\n                 break\n\n             perturbation = np.random.uniform(-search_range/100, search_range/100, self.dim)\n             local_x = np.clip(self.x_opt + perturbation, lb, ub)\n             local_f = func(local_x)\n             self.budget -=1\n\n             if local_f < self.f_opt:\n                  self.f_opt = local_f\n                  self.x_opt = local_x\n\n\n        return self.f_opt, self.x_opt\n```", "aucs": [0.1606031075761173, 0.2514694359098345, 0.8342383616214157, 0.9287187688703, 0.8539387382564494, 0.8776723756831445, 0.329492255031239, 0.7024943131870516, 0.8463311847034654, 0.8445525684766195, 0.8384797972923864, 0.9987422744532402, 0.21765699393837712, 0.30891703307200846, 0.8145857031434983, 0.47725262363248244, 0.3656188097666153, 0.9168179046923491, 0.26004733336605124, 0.5184303520726314], "operator": "mutation", "theta_sampled": 0.010804744330423149, "reward": 0.04532919270787272, "generation": 5}, "task_prompt": ""}
{"id": "81c4333c-0d55-4a9f-9170-68690646402b", "fitness": 0.5789966400224694, "name": "DynamicShrinkingCauchyOptimizer", "description": "Shrinks the search space and uses a Cauchy mutation with dynamically adjusted scaling based on the algorithm's progress.", "code": "import numpy as np\n\nclass DynamicShrinkingCauchyOptimizer:\n    def __init__(self, budget=10000, dim=10, population_size=20, shrink_factor=0.95, initial_cauchy_scale=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = population_size\n        self.shrink_factor = shrink_factor\n        self.initial_cauchy_scale = initial_cauchy_scale\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        population = np.random.uniform(lb, ub, size=(self.population_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.population_size\n\n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index]\n\n        search_range = ub - lb\n        center = self.x_opt\n        cauchy_scale = self.initial_cauchy_scale # Initialize the Cauchy scale\n\n        iteration = 0\n        while self.budget > 0:\n            iteration += 1\n            # Cauchy mutation around the best individual\n            for i in range(self.population_size):\n                if self.budget <= 0:\n                    break\n\n                # Dynamically adjust Cauchy scale. Reduce it over time.\n                current_cauchy_scale = cauchy_scale * (1 - iteration/ (self.budget // self.population_size + 1))\n\n                mutation = center + search_range * np.random.standard_cauchy(size=self.dim) * current_cauchy_scale  # Scale Cauchy distribution\n                mutation = np.clip(mutation, lb, ub)  # Clip values to stay within bounds\n\n                f_mutation = func(mutation)\n                self.budget -= 1\n\n                if f_mutation < fitness[i]:\n                    fitness[i] = f_mutation\n                    population[i] = mutation\n\n                    if f_mutation < self.f_opt:\n                        self.f_opt = f_mutation\n                        self.x_opt = mutation\n                        center = self.x_opt\n\n            # Shrink the search range around the best individual\n            search_range *= self.shrink_factor\n            lb = np.clip(center - search_range/2, func.bounds.lb, func.bounds.ub)\n            ub = np.clip(center + search_range/2, func.bounds.lb, func.bounds.ub)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "The algorithm DynamicShrinkingCauchyOptimizer scored 0.579 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["e8a11899-492c-463b-9412-67910c8f787b"], "operator": "mutation", "metadata": {"llm_response": "# Description: Shrinks the search space and uses a Cauchy mutation with dynamically adjusted scaling based on the algorithm's progress.\n# Code: \n```python\nimport numpy as np\n\nclass DynamicShrinkingCauchyOptimizer:\n    def __init__(self, budget=10000, dim=10, population_size=20, shrink_factor=0.95, initial_cauchy_scale=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = population_size\n        self.shrink_factor = shrink_factor\n        self.initial_cauchy_scale = initial_cauchy_scale\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        population = np.random.uniform(lb, ub, size=(self.population_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.population_size\n\n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index]\n\n        search_range = ub - lb\n        center = self.x_opt\n        cauchy_scale = self.initial_cauchy_scale # Initialize the Cauchy scale\n\n        iteration = 0\n        while self.budget > 0:\n            iteration += 1\n            # Cauchy mutation around the best individual\n            for i in range(self.population_size):\n                if self.budget <= 0:\n                    break\n\n                # Dynamically adjust Cauchy scale. Reduce it over time.\n                current_cauchy_scale = cauchy_scale * (1 - iteration/ (self.budget // self.population_size + 1))\n\n                mutation = center + search_range * np.random.standard_cauchy(size=self.dim) * current_cauchy_scale  # Scale Cauchy distribution\n                mutation = np.clip(mutation, lb, ub)  # Clip values to stay within bounds\n\n                f_mutation = func(mutation)\n                self.budget -= 1\n\n                if f_mutation < fitness[i]:\n                    fitness[i] = f_mutation\n                    population[i] = mutation\n\n                    if f_mutation < self.f_opt:\n                        self.f_opt = f_mutation\n                        self.x_opt = mutation\n                        center = self.x_opt\n\n            # Shrink the search range around the best individual\n            search_range *= self.shrink_factor\n            lb = np.clip(center - search_range/2, func.bounds.lb, func.bounds.ub)\n            ub = np.clip(center + search_range/2, func.bounds.lb, func.bounds.ub)\n\n        return self.f_opt, self.x_opt\n```", "aucs": [0.18316705449749493, 0.671787325058009, 0.8075640446516603, 0.9204939652989748, 0.2518105519494396, 0.8372647589317667, 0.30825975503159475, 0.533586661153306, 0.8169081668056943, 0.1996373217650479, 0.9266338927541001, 0.9959606538907636, 0.24531512976254422, 0.7625233309025025, 0.7774819162170379, 0.38290147478266723, 0.3302783519118856, 0.883728641741721, 0.23752678499855728, 0.5071030183446203], "operator": "mutation", "theta_sampled": 0.022713261267001374, "reward": 0.007022835993078291, "generation": 5}, "task_prompt": ""}
{"id": "26935051-61ec-4274-b73a-8dc1cf8a174c", "fitness": 0.0, "name": "AdaptiveShrinkingCauchyOptimizer", "description": "Shrinking Cauchy Optimizer with Adaptive Shrink Factor and Momentum-based Center Update for faster convergence.", "code": "import numpy as np\n\nclass AdaptiveShrinkingCauchyOptimizer:\n    def __init__(self, budget=10000, dim=10, population_size=20, initial_shrink_factor=0.95, learning_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = population_size\n        self.shrink_factor = initial_shrink_factor\n        self.initial_shrink_factor = initial_shrink_factor\n        self.learning_rate = learning_rate\n        self.momentum = 0.9\n        self.velocity = np.zeros(dim)\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        population = np.random.uniform(lb, ub, size=(self.population_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.population_size\n\n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index]\n\n        search_range = ub - lb\n        center = self.x_opt.copy()\n\n        iteration = 0\n        while self.budget > 0:\n            iteration += 1\n            # Cauchy mutation around the best individual\n            for i in range(self.population_size):\n                if self.budget <= 0:\n                    break\n\n                # Adaptive shrink factor based on iteration\n                adaptive_scale = search_range * (1 + np.sin(2 * np.pi * iteration / (self.budget//self.population_size+1)))/2\n                mutation = center + adaptive_scale * np.random.standard_cauchy(size=self.dim) * 0.1  # Scale Cauchy distribution\n                mutation = np.clip(mutation, lb, ub)  # Clip values to stay within bounds\n\n                f_mutation = func(mutation)\n                self.budget -= 1\n\n                if f_mutation < fitness[i]:\n                    fitness[i] = f_mutation\n                    population[i] = mutation\n\n                    if f_mutation < self.f_opt:\n                        self.f_opt = f_mutation\n                        \n                        # Momentum-based center update\n                        new_direction = mutation - self.x_opt\n                        self.velocity = self.momentum * self.velocity + self.learning_rate * new_direction\n                        self.x_opt = mutation.copy()\n                        center = self.x_opt + self.velocity\n                        center = np.clip(center, lb, ub)\n                       \n            # Shrink the search range around the best individual\n            self.shrink_factor = self.initial_shrink_factor * (1 - iteration/(self.budget//self.population_size + 1))**0.1 #Slowly decrease shrink factor\n            search_range *= self.shrink_factor\n            lb = np.clip(center - search_range/2, func.bounds.lb, func.bounds.ub)\n            ub = np.clip(center + search_range/2, func.bounds.lb, func.bounds.ub)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "", "error": "Evaluation error: Cannot cast ufunc 'multiply' output from dtype('complex128') to dtype('float64') with casting rule 'same_kind'", "parent_ids": ["e8a11899-492c-463b-9412-67910c8f787b"], "operator": "mutation", "metadata": {"llm_response": "# Description: Shrinking Cauchy Optimizer with Adaptive Shrink Factor and Momentum-based Center Update for faster convergence.\n# Code: \n```python\nimport numpy as np\n\nclass AdaptiveShrinkingCauchyOptimizer:\n    def __init__(self, budget=10000, dim=10, population_size=20, initial_shrink_factor=0.95, learning_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = population_size\n        self.shrink_factor = initial_shrink_factor\n        self.initial_shrink_factor = initial_shrink_factor\n        self.learning_rate = learning_rate\n        self.momentum = 0.9\n        self.velocity = np.zeros(dim)\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        population = np.random.uniform(lb, ub, size=(self.population_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.population_size\n\n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index]\n\n        search_range = ub - lb\n        center = self.x_opt.copy()\n\n        iteration = 0\n        while self.budget > 0:\n            iteration += 1\n            # Cauchy mutation around the best individual\n            for i in range(self.population_size):\n                if self.budget <= 0:\n                    break\n\n                # Adaptive shrink factor based on iteration\n                adaptive_scale = search_range * (1 + np.sin(2 * np.pi * iteration / (self.budget//self.population_size+1)))/2\n                mutation = center + adaptive_scale * np.random.standard_cauchy(size=self.dim) * 0.1  # Scale Cauchy distribution\n                mutation = np.clip(mutation, lb, ub)  # Clip values to stay within bounds\n\n                f_mutation = func(mutation)\n                self.budget -= 1\n\n                if f_mutation < fitness[i]:\n                    fitness[i] = f_mutation\n                    population[i] = mutation\n\n                    if f_mutation < self.f_opt:\n                        self.f_opt = f_mutation\n                        \n                        # Momentum-based center update\n                        new_direction = mutation - self.x_opt\n                        self.velocity = self.momentum * self.velocity + self.learning_rate * new_direction\n                        self.x_opt = mutation.copy()\n                        center = self.x_opt + self.velocity\n                        center = np.clip(center, lb, ub)\n                       \n            # Shrink the search range around the best individual\n            self.shrink_factor = self.initial_shrink_factor * (1 - iteration/(self.budget//self.population_size + 1))**0.1 #Slowly decrease shrink factor\n            search_range *= self.shrink_factor\n            lb = np.clip(center - search_range/2, func.bounds.lb, func.bounds.ub)\n            ub = np.clip(center + search_range/2, func.bounds.lb, func.bounds.ub)\n\n        return self.f_opt, self.x_opt\n```", "operator": "mutation", "theta_sampled": 0.02051940257505546, "reward": 0.0, "generation": 5}, "task_prompt": ""}
{"id": "02b67a6d-acf0-458b-8ac8-ebf644e52ac1", "fitness": 0.45067922396543053, "name": "AdaptiveVelocityOptimizer", "description": "Implements a self-adaptive hybrid optimization strategy that blends Gaussian and Cauchy mutations, adapts population size based on stagnation detection, employs a slime mold-inspired selection mechanism, and integrates a velocity-based exploration strategy to dynamically adjust search behavior.", "code": "import numpy as np\n\nclass AdaptiveVelocityOptimizer:\n    def __init__(self, budget=10000, dim=10, pop_size=50, stagnation_threshold=1000, w_decay=0.9, p_selection=0.2, initial_mutation_scale=1.0, velocity_scale=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lb = -5.0\n        self.ub = 5.0\n        self.mutation_prob_gaussian = 0.5\n        self.mutation_prob_cauchy = 0.5\n        self.mutation_scale = initial_mutation_scale\n        self.stagnation_threshold = stagnation_threshold\n        self.stagnation_counter = 0\n        self.best_fitness_history = []\n        self.w_decay = w_decay\n        self.p_selection = p_selection\n        self.initial_pop_size = pop_size\n        self.velocity = np.zeros((pop_size, dim))\n        self.velocity_scale = velocity_scale\n        self.inertia_weight = 0.7 # Inertia weight for velocity update\n\n    def initialize_population(self):\n        return np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n\n    def calculate_diversity(self, population):\n        centroid = np.mean(population, axis=0)\n        distances = np.linalg.norm(population - centroid, axis=1)\n        return np.mean(distances)\n\n    def mutate_gaussian(self, x):\n        return x + self.mutation_scale * np.random.normal(0, 1, size=self.dim)\n\n    def mutate_cauchy(self, x):\n        return x + self.mutation_scale * np.random.standard_cauchy(size=self.dim)\n\n    def correct_bounds(self, x):\n        return np.clip(x, self.lb, self.ub)\n\n    def rank_based_selection(self, population, fitness, num_parents):\n        ranked_indices = np.argsort(fitness)\n        parent_indices = ranked_indices[:num_parents]\n        return population[parent_indices]\n    \n    def adjust_population_size(self):\n        if self.stagnation_counter > self.stagnation_threshold:\n            self.pop_size = int(self.pop_size * 0.9)  # Reduce population size if stagnating\n            if self.pop_size < 10:\n                self.pop_size = 10 # Minimum population size\n\n            self.mutation_scale = self.initial_mutation_scale # Reset mutation scale\n            self.velocity_scale = self.velocity_scale * 0.9 # Reduce velocity scale\n            self.stagnation_counter = 0 # Reset stagnation counter\n            print(f\"Stagnation detected, reducing pop size to {self.pop_size}, reducing velocity to {self.velocity_scale}\")\n        elif self.pop_size < self.initial_pop_size and len(self.best_fitness_history) > self.stagnation_threshold and abs(self.f_opt - self.best_fitness_history[-self.stagnation_threshold]) > 1e-5:\n            #Potentially increase the pop size if we are not stagnating. \n            self.pop_size = int(min(self.pop_size * 1.1, self.initial_pop_size))\n            self.velocity_scale = min(self.velocity_scale * 1.1, 0.2) # Increase velocity scale\n            print(f\"Progress detected, increasing pop size to {self.pop_size}, increasing velocity to {self.velocity_scale}\")\n\n    def slime_mold_selection(self, population, fitness):\n        \"\"\"Selects parents based on slime mold weighting.\"\"\"\n        sorted_indices = np.argsort(fitness)\n        population = population[sorted_indices]\n        fitness = fitness[sorted_indices]\n\n        num_selected = int(self.p_selection * self.pop_size)\n        selected_population = population[:num_selected]\n        selected_fitness = fitness[:num_selected]\n\n        rank = np.arange(1, num_selected + 1)\n        weights_selected = np.exp(-rank / num_selected)\n        weights_selected /= np.sum(weights_selected)\n        return selected_population, weights_selected\n    \n    def __call__(self, func):\n        population = self.initialize_population()\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n        \n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index]\n        self.best_fitness_history.append(self.f_opt)\n\n        while self.budget > 0:\n            diversity = self.calculate_diversity(population)\n\n            if diversity > 1.0:\n                self.mutation_prob_gaussian = 0.8\n                self.mutation_prob_cauchy = 0.2\n            else:\n                self.mutation_prob_gaussian = 0.2\n                self.mutation_prob_cauchy = 0.8\n\n            # Elitism: Keep the best individual\n            best_individual = np.copy(self.x_opt)\n            best_fitness = self.f_opt\n\n            # Slime mold inspired parent selection\n            parents, weights = self.slime_mold_selection(population, fitness)\n            num_parents = len(parents)\n            \n            new_population = np.copy(population)\n            for i in range(self.pop_size):\n                # Choose a parent based on slime mold weights\n                parent_index = np.random.choice(num_parents, p=weights)\n                parent = parents[parent_index]\n\n                #Update Velocity\n                self.velocity[i] = self.inertia_weight * self.velocity[i] + self.velocity_scale * np.random.uniform(-1, 1, size=self.dim)\n\n                if np.random.rand() < self.mutation_prob_gaussian:\n                    new_population[i] = self.mutate_gaussian(parent) + self.velocity[i]\n                else:\n                    new_population[i] = self.mutate_cauchy(parent) + self.velocity[i]\n\n                new_population[i] = self.correct_bounds(new_population[i])\n                \n                new_fitness = func(new_population[i])\n                self.budget -= 1\n\n                if new_fitness < fitness[i]:\n                    fitness[i] = new_fitness\n                    population[i] = new_population[i]\n\n                    if new_fitness < self.f_opt:\n                        self.f_opt = new_fitness\n                        self.x_opt = new_population[i]\n            \n            # Replace worst individual with the best from previous generation (Elitism)\n            worst_index = np.argmax(fitness)\n            population[worst_index] = best_individual\n            fitness[worst_index] = best_fitness\n\n            # Adapt mutation scale\n            self.mutation_scale = 0.95 * self.mutation_scale\n\n            # Stagnation detection and population size adjustment\n            if len(self.best_fitness_history) > self.stagnation_threshold:\n                if abs(self.f_opt - self.best_fitness_history[-self.stagnation_threshold]) < 1e-5:\n                    self.stagnation_counter += self.pop_size\n                else:\n                    self.stagnation_counter = 0\n\n            self.best_fitness_history.append(self.f_opt)\n            self.adjust_population_size()\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "The algorithm AdaptiveVelocityOptimizer scored 0.451 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["ddb255ad-fa53-4028-8bc5-cf4dcdf480e3"], "operator": "mutation", "metadata": {"llm_response": "# Description: Implements a self-adaptive hybrid optimization strategy that blends Gaussian and Cauchy mutations, adapts population size based on stagnation detection, employs a slime mold-inspired selection mechanism, and integrates a velocity-based exploration strategy to dynamically adjust search behavior.\n# Code: \n```python\nimport numpy as np\n\nclass AdaptiveVelocityOptimizer:\n    def __init__(self, budget=10000, dim=10, pop_size=50, stagnation_threshold=1000, w_decay=0.9, p_selection=0.2, initial_mutation_scale=1.0, velocity_scale=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.lb = -5.0\n        self.ub = 5.0\n        self.mutation_prob_gaussian = 0.5\n        self.mutation_prob_cauchy = 0.5\n        self.mutation_scale = initial_mutation_scale\n        self.stagnation_threshold = stagnation_threshold\n        self.stagnation_counter = 0\n        self.best_fitness_history = []\n        self.w_decay = w_decay\n        self.p_selection = p_selection\n        self.initial_pop_size = pop_size\n        self.velocity = np.zeros((pop_size, dim))\n        self.velocity_scale = velocity_scale\n        self.inertia_weight = 0.7 # Inertia weight for velocity update\n\n    def initialize_population(self):\n        return np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n\n    def calculate_diversity(self, population):\n        centroid = np.mean(population, axis=0)\n        distances = np.linalg.norm(population - centroid, axis=1)\n        return np.mean(distances)\n\n    def mutate_gaussian(self, x):\n        return x + self.mutation_scale * np.random.normal(0, 1, size=self.dim)\n\n    def mutate_cauchy(self, x):\n        return x + self.mutation_scale * np.random.standard_cauchy(size=self.dim)\n\n    def correct_bounds(self, x):\n        return np.clip(x, self.lb, self.ub)\n\n    def rank_based_selection(self, population, fitness, num_parents):\n        ranked_indices = np.argsort(fitness)\n        parent_indices = ranked_indices[:num_parents]\n        return population[parent_indices]\n    \n    def adjust_population_size(self):\n        if self.stagnation_counter > self.stagnation_threshold:\n            self.pop_size = int(self.pop_size * 0.9)  # Reduce population size if stagnating\n            if self.pop_size < 10:\n                self.pop_size = 10 # Minimum population size\n\n            self.mutation_scale = self.initial_mutation_scale # Reset mutation scale\n            self.velocity_scale = self.velocity_scale * 0.9 # Reduce velocity scale\n            self.stagnation_counter = 0 # Reset stagnation counter\n            print(f\"Stagnation detected, reducing pop size to {self.pop_size}, reducing velocity to {self.velocity_scale}\")\n        elif self.pop_size < self.initial_pop_size and len(self.best_fitness_history) > self.stagnation_threshold and abs(self.f_opt - self.best_fitness_history[-self.stagnation_threshold]) > 1e-5:\n            #Potentially increase the pop size if we are not stagnating. \n            self.pop_size = int(min(self.pop_size * 1.1, self.initial_pop_size))\n            self.velocity_scale = min(self.velocity_scale * 1.1, 0.2) # Increase velocity scale\n            print(f\"Progress detected, increasing pop size to {self.pop_size}, increasing velocity to {self.velocity_scale}\")\n\n    def slime_mold_selection(self, population, fitness):\n        \"\"\"Selects parents based on slime mold weighting.\"\"\"\n        sorted_indices = np.argsort(fitness)\n        population = population[sorted_indices]\n        fitness = fitness[sorted_indices]\n\n        num_selected = int(self.p_selection * self.pop_size)\n        selected_population = population[:num_selected]\n        selected_fitness = fitness[:num_selected]\n\n        rank = np.arange(1, num_selected + 1)\n        weights_selected = np.exp(-rank / num_selected)\n        weights_selected /= np.sum(weights_selected)\n        return selected_population, weights_selected\n    \n    def __call__(self, func):\n        population = self.initialize_population()\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n        \n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index]\n        self.best_fitness_history.append(self.f_opt)\n\n        while self.budget > 0:\n            diversity = self.calculate_diversity(population)\n\n            if diversity > 1.0:\n                self.mutation_prob_gaussian = 0.8\n                self.mutation_prob_cauchy = 0.2\n            else:\n                self.mutation_prob_gaussian = 0.2\n                self.mutation_prob_cauchy = 0.8\n\n            # Elitism: Keep the best individual\n            best_individual = np.copy(self.x_opt)\n            best_fitness = self.f_opt\n\n            # Slime mold inspired parent selection\n            parents, weights = self.slime_mold_selection(population, fitness)\n            num_parents = len(parents)\n            \n            new_population = np.copy(population)\n            for i in range(self.pop_size):\n                # Choose a parent based on slime mold weights\n                parent_index = np.random.choice(num_parents, p=weights)\n                parent = parents[parent_index]\n\n                #Update Velocity\n                self.velocity[i] = self.inertia_weight * self.velocity[i] + self.velocity_scale * np.random.uniform(-1, 1, size=self.dim)\n\n                if np.random.rand() < self.mutation_prob_gaussian:\n                    new_population[i] = self.mutate_gaussian(parent) + self.velocity[i]\n                else:\n                    new_population[i] = self.mutate_cauchy(parent) + self.velocity[i]\n\n                new_population[i] = self.correct_bounds(new_population[i])\n                \n                new_fitness = func(new_population[i])\n                self.budget -= 1\n\n                if new_fitness < fitness[i]:\n                    fitness[i] = new_fitness\n                    population[i] = new_population[i]\n\n                    if new_fitness < self.f_opt:\n                        self.f_opt = new_fitness\n                        self.x_opt = new_population[i]\n            \n            # Replace worst individual with the best from previous generation (Elitism)\n            worst_index = np.argmax(fitness)\n            population[worst_index] = best_individual\n            fitness[worst_index] = best_fitness\n\n            # Adapt mutation scale\n            self.mutation_scale = 0.95 * self.mutation_scale\n\n            # Stagnation detection and population size adjustment\n            if len(self.best_fitness_history) > self.stagnation_threshold:\n                if abs(self.f_opt - self.best_fitness_history[-self.stagnation_threshold]) < 1e-5:\n                    self.stagnation_counter += self.pop_size\n                else:\n                    self.stagnation_counter = 0\n\n            self.best_fitness_history.append(self.f_opt)\n            self.adjust_population_size()\n\n\n        return self.f_opt, self.x_opt\n```", "aucs": [0.20851094517401303, 0.21580774539764003, 0.4352660316669831, 0.7384819264813525, 0.3664066865148139, 0.45237173120497187, 0.31482181247269814, 0.3663933395568171, 0.35979286725356674, 0.2250111520723732, 0.7114676235424258, 0.9955769343965722, 0.2901239830813258, 0.3613793684783422, 0.840843092209792, 0.4640200198841483, 0.36150392456830815, 0.5393607592981379, 0.2639399899618172, 0.5025045460925093], "operator": "mutation", "theta_sampled": 0.010903055144720826, "reward": 0.0, "generation": 5}, "task_prompt": ""}
{"id": "6e82eff0-53ca-4ceb-8b23-00663284f63e", "fitness": 0.5349576487784719, "name": "RankDifferentialEvolution", "description": "A population-based algorithm that combines elements of differential evolution with a self-adaptive parameter control mechanism based on fitness ranking and success rate.", "code": "import numpy as np\n\nclass RankDifferentialEvolution:\n    def __init__(self, budget=10000, dim=10, pop_size=20, F=0.5, CR=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F  # Initial scaling factor\n        self.CR = CR # Initial crossover rate\n        self.F_memory = np.ones(self.pop_size) * self.F\n        self.CR_memory = np.ones(self.pop_size) * self.CR\n        self.archive_factor = 2 #Archive size relative to pop_size\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size #Reduce budget after evaluation\n\n        archive = []  # Archive for storing discarded solutions\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation\n                indices = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = population[indices]\n                mutant = population[i] + self.F_memory[i] * (x_r2 - x_r3)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_mask = np.random.rand(self.dim) < self.CR_memory[i]\n                trial_vector = np.where(cross_mask, mutant, population[i])\n\n                # Selection\n                f_trial = func(trial_vector)\n                self.budget -= 1 #Reduce budget after evaluation\n                if f_trial < fitness[i]:\n                    if len(archive) > self.archive_factor * self.pop_size:\n                         archive.pop(np.random.randint(0, len(archive)))\n                    archive.append(population[i])\n                    population[i] = trial_vector\n                    fitness[i] = f_trial\n\n                    # Success History Adaptation: Modify F and CR based on success\n                    self.F_memory[i] = np.clip(np.random.normal(self.F, 0.1), 0.1, 1.0)\n                    self.CR_memory[i] = np.clip(np.random.normal(self.CR, 0.1), 0.0, 1.0)\n\n                # Update global best\n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = trial_vector\n                if fitness[i] < self.f_opt:\n                    self.f_opt = fitness[i]\n                    self.x_opt = population[i]\n                \n                if self.budget <= 0:\n                   break\n\n\n            #Rank-based parameter adjustment: Apply at the end of each generation\n\n            ranked_indices = np.argsort(fitness)\n            top_indices = ranked_indices[:self.pop_size // 4] #Top quarter of the population\n\n            # Adjust F and CR based on the best performing individuals\n            self.F = np.mean(self.F_memory[top_indices])\n            self.CR = np.mean(self.CR_memory[top_indices])\n\n            # Ensure some diversity in the population\n            if len(archive) > 0 and np.random.rand() < 0.1: #10% chance\n                idx = np.random.randint(0, self.pop_size)\n                population[idx] = archive[np.random.randint(0, len(archive))]\n                fitness[idx] = func(population[idx])\n                self.budget -= 1\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "The algorithm RankDifferentialEvolution scored 0.535 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": "random_new", "metadata": {"llm_response": "# Description: A population-based algorithm that combines elements of differential evolution with a self-adaptive parameter control mechanism based on fitness ranking and success rate.\n# Code:\n```python\nimport numpy as np\n\nclass RankDifferentialEvolution:\n    def __init__(self, budget=10000, dim=10, pop_size=20, F=0.5, CR=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F  # Initial scaling factor\n        self.CR = CR # Initial crossover rate\n        self.F_memory = np.ones(self.pop_size) * self.F\n        self.CR_memory = np.ones(self.pop_size) * self.CR\n        self.archive_factor = 2 #Archive size relative to pop_size\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size #Reduce budget after evaluation\n\n        archive = []  # Archive for storing discarded solutions\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Mutation\n                indices = np.random.choice(self.pop_size, 3, replace=False)\n                x_r1, x_r2, x_r3 = population[indices]\n                mutant = population[i] + self.F_memory[i] * (x_r2 - x_r3)\n                mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n\n                # Crossover\n                cross_mask = np.random.rand(self.dim) < self.CR_memory[i]\n                trial_vector = np.where(cross_mask, mutant, population[i])\n\n                # Selection\n                f_trial = func(trial_vector)\n                self.budget -= 1 #Reduce budget after evaluation\n                if f_trial < fitness[i]:\n                    if len(archive) > self.archive_factor * self.pop_size:\n                         archive.pop(np.random.randint(0, len(archive)))\n                    archive.append(population[i])\n                    population[i] = trial_vector\n                    fitness[i] = f_trial\n\n                    # Success History Adaptation: Modify F and CR based on success\n                    self.F_memory[i] = np.clip(np.random.normal(self.F, 0.1), 0.1, 1.0)\n                    self.CR_memory[i] = np.clip(np.random.normal(self.CR, 0.1), 0.0, 1.0)\n\n                # Update global best\n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = trial_vector\n                if fitness[i] < self.f_opt:\n                    self.f_opt = fitness[i]\n                    self.x_opt = population[i]\n                \n                if self.budget <= 0:\n                   break\n\n\n            #Rank-based parameter adjustment: Apply at the end of each generation\n\n            ranked_indices = np.argsort(fitness)\n            top_indices = ranked_indices[:self.pop_size // 4] #Top quarter of the population\n\n            # Adjust F and CR based on the best performing individuals\n            self.F = np.mean(self.F_memory[top_indices])\n            self.CR = np.mean(self.CR_memory[top_indices])\n\n            # Ensure some diversity in the population\n            if len(archive) > 0 and np.random.rand() < 0.1: #10% chance\n                idx = np.random.randint(0, self.pop_size)\n                population[idx] = archive[np.random.randint(0, len(archive))]\n                fitness[idx] = func(population[idx])\n                self.budget -= 1\n\n\n        return self.f_opt, self.x_opt\n```", "aucs": [0.187375130079797, 0.37654184878756036, 0.6071993474387911, 0.8024898429912708, 0.383829550038176, 0.7352473354478626, 0.3812795259823559, 0.5423482389111923, 0.6562826701703911, 0.2534697361127748, 0.8003774624041575, 0.9901619464131286, 0.3089975652508091, 0.5261929063472353, 0.8850023780853963, 0.6574845447731228, 0], "operator": "random_new", "theta_sampled": 0.012139562791727088, "reward": 0.0, "generation": 6}, "task_prompt": ""}
{"id": "ecc70942-b0a7-4f55-9c5c-b847d4827960", "fitness": 0.6265208142224229, "name": "BirdSwarmOptimization", "description": "Population-based algorithm that simulates a flock of birds searching for food, using a combination of global exploration and local exploitation, influenced by inertia and social interaction with neighbors.", "code": "import numpy as np\n\nclass BirdSwarmOptimization:\n    def __init__(self, budget=10000, dim=10, swarm_size=20, inertia_weight=0.7, cognitive_coeff=1.4, social_coeff=1.4, neighborhood_size=3):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = swarm_size\n        self.inertia_weight = inertia_weight\n        self.cognitive_coeff = cognitive_coeff\n        self.social_coeff = social_coeff\n        self.neighborhood_size = neighborhood_size\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        \n        # Initialize swarm positions and velocities\n        swarm_positions = np.random.uniform(lb, ub, size=(self.swarm_size, self.dim))\n        swarm_velocities = np.random.uniform(-abs(ub-lb)/10, abs(ub-lb)/10, size=(self.swarm_size, self.dim))  #Initialize with small velocities\n        swarm_fitness = np.array([func(x) for x in swarm_positions])\n        self.budget -= self.swarm_size #Update the remaining budget\n        \n        personal_best_positions = swarm_positions.copy()\n        personal_best_fitness = swarm_fitness.copy()\n        \n        global_best_index = np.argmin(personal_best_fitness)\n        global_best_position = personal_best_positions[global_best_index].copy()\n        global_best_fitness = personal_best_fitness[global_best_index]\n        \n        while self.budget > 0:\n            for i in range(self.swarm_size):\n                # Update velocity\n                inertia = self.inertia_weight * swarm_velocities[i]\n                cognitive = self.cognitive_coeff * np.random.rand(self.dim) * (personal_best_positions[i] - swarm_positions[i])\n                \n                # Social component: find best neighbor\n                neighbors_indices = np.random.choice(self.swarm_size, size=self.neighborhood_size, replace=False)  # Select random neighbors\n                neighbors_fitness = personal_best_fitness[neighbors_indices]\n                best_neighbor_index = neighbors_indices[np.argmin(neighbors_fitness)]\n                social = self.social_coeff * np.random.rand(self.dim) * (personal_best_positions[best_neighbor_index] - swarm_positions[i])\n                \n                swarm_velocities[i] = inertia + cognitive + social\n                \n                # Update position\n                new_position = swarm_positions[i] + swarm_velocities[i]\n                \n                # Boundary handling (clip the solution, if out of bounds)\n                new_position = np.clip(new_position, lb, ub)\n                \n                new_fitness = func(new_position)\n                self.budget -= 1\n                \n                if new_fitness < personal_best_fitness[i]:\n                    personal_best_fitness[i] = new_fitness\n                    personal_best_positions[i] = new_position.copy()\n                    \n                    if new_fitness < global_best_fitness:\n                        global_best_fitness = new_fitness\n                        global_best_position = new_position.copy()\n                \n                swarm_positions[i] = new_position\n            \n        return global_best_fitness, global_best_position", "configspace": "", "generation": 6, "feedback": "The algorithm BirdSwarmOptimization scored 0.627 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": "random_new", "metadata": {"llm_response": "# Description: Population-based algorithm that simulates a flock of birds searching for food, using a combination of global exploration and local exploitation, influenced by inertia and social interaction with neighbors.\n# Code: \n```python\nimport numpy as np\n\nclass BirdSwarmOptimization:\n    def __init__(self, budget=10000, dim=10, swarm_size=20, inertia_weight=0.7, cognitive_coeff=1.4, social_coeff=1.4, neighborhood_size=3):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = swarm_size\n        self.inertia_weight = inertia_weight\n        self.cognitive_coeff = cognitive_coeff\n        self.social_coeff = social_coeff\n        self.neighborhood_size = neighborhood_size\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        \n        # Initialize swarm positions and velocities\n        swarm_positions = np.random.uniform(lb, ub, size=(self.swarm_size, self.dim))\n        swarm_velocities = np.random.uniform(-abs(ub-lb)/10, abs(ub-lb)/10, size=(self.swarm_size, self.dim))  #Initialize with small velocities\n        swarm_fitness = np.array([func(x) for x in swarm_positions])\n        self.budget -= self.swarm_size #Update the remaining budget\n        \n        personal_best_positions = swarm_positions.copy()\n        personal_best_fitness = swarm_fitness.copy()\n        \n        global_best_index = np.argmin(personal_best_fitness)\n        global_best_position = personal_best_positions[global_best_index].copy()\n        global_best_fitness = personal_best_fitness[global_best_index]\n        \n        while self.budget > 0:\n            for i in range(self.swarm_size):\n                # Update velocity\n                inertia = self.inertia_weight * swarm_velocities[i]\n                cognitive = self.cognitive_coeff * np.random.rand(self.dim) * (personal_best_positions[i] - swarm_positions[i])\n                \n                # Social component: find best neighbor\n                neighbors_indices = np.random.choice(self.swarm_size, size=self.neighborhood_size, replace=False)  # Select random neighbors\n                neighbors_fitness = personal_best_fitness[neighbors_indices]\n                best_neighbor_index = neighbors_indices[np.argmin(neighbors_fitness)]\n                social = self.social_coeff * np.random.rand(self.dim) * (personal_best_positions[best_neighbor_index] - swarm_positions[i])\n                \n                swarm_velocities[i] = inertia + cognitive + social\n                \n                # Update position\n                new_position = swarm_positions[i] + swarm_velocities[i]\n                \n                # Boundary handling (clip the solution, if out of bounds)\n                new_position = np.clip(new_position, lb, ub)\n                \n                new_fitness = func(new_position)\n                self.budget -= 1\n                \n                if new_fitness < personal_best_fitness[i]:\n                    personal_best_fitness[i] = new_fitness\n                    personal_best_positions[i] = new_position.copy()\n                    \n                    if new_fitness < global_best_fitness:\n                        global_best_fitness = new_fitness\n                        global_best_position = new_position.copy()\n                \n                swarm_positions[i] = new_position\n            \n        return global_best_fitness, global_best_position\n```", "aucs": [0.21912944314679572, 0.6267088491246919, 0.6991188727249468, 0.8996370732319131, 0.7700654474988055, 0.8313562365219547, 0.3774610756616159, 0.6238013284869012, 0.7748101140820358, 0.6944166983616329, 0.8322804026270882, 0.9967832239873411, 0.26118170673908925, 0.27503331364590156, 0.7315704794461665, 0.7968489635997027, 0.5097547773686406, 0.8554626880332356, 0.224261700601326, 0.5307338895586724], "operator": "random_new", "theta_sampled": 0.005651036516106045, "reward": 0.009217817485159041, "generation": 6}, "task_prompt": ""}
{"id": "69141d6c-264a-49de-8fb5-c932503d6693", "fitness": 0.5844019634237753, "name": "HybridAdaptiveCauchyOptimizer", "description": "A hybrid Cauchy optimizer that combines adaptive shrinking, diversity-based mutation probabilities, and dynamic population size adjustments.", "code": "import numpy as np\n\nclass HybridAdaptiveCauchyOptimizer:\n    def __init__(self, budget=10000, dim=10, initial_pop_size=20, shrink_factor=0.95, stagnation_threshold=1000, initial_cauchy_scale=0.1, local_search_iterations=5):\n        self.budget = budget\n        self.dim = dim\n        self.initial_pop_size = initial_pop_size\n        self.pop_size = initial_pop_size\n        self.shrink_factor = shrink_factor\n        self.stagnation_threshold = stagnation_threshold\n        self.cauchy_scale = initial_cauchy_scale\n        self.lb = -5.0\n        self.ub = 5.0\n        self.mutation_prob_gaussian = 0.5\n        self.mutation_prob_cauchy = 0.5\n        self.stagnation_counter = 0\n        self.best_fitness_history = []\n        self.search_range = self.ub - self.lb\n        self.mutation_scale = 1.0 # Scale for Gaussian mutation\n        self.local_search_iterations = local_search_iterations\n        self.success_rate = 0.5  # Initialize success rate\n        self.iterations_since_last_success = 0\n\n    def initialize_population(self):\n        return np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n\n    def calculate_diversity(self, population):\n        centroid = np.mean(population, axis=0)\n        distances = np.linalg.norm(population - centroid, axis=1)\n        return np.mean(distances)\n\n    def mutate_gaussian(self, x):\n        return x + self.mutation_scale * np.random.normal(0, 1, size=self.dim)\n\n    def mutate_cauchy(self, x):\n         return x + self.search_range * np.random.standard_cauchy(size=self.dim) * self.cauchy_scale\n\n    def correct_bounds(self, x):\n        return np.clip(x, self.lb, self.ub)\n\n    def adjust_population_size(self):\n        if self.stagnation_counter > self.stagnation_threshold:\n            self.pop_size = int(self.pop_size * 0.9)  # Reduce population size if stagnating\n            if self.pop_size < 10:\n                self.pop_size = 10 # Minimum population size\n            self.stagnation_counter = 0 # Reset stagnation counter\n            self.mutation_scale = 1.0\n            self.cauchy_scale = self.initial_cauchy_scale\n            print(f\"Stagnation detected, reducing pop size to {self.pop_size}\")\n\n    def __call__(self, func):\n        population = self.initialize_population()\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n        \n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index]\n        self.best_fitness_history.append(self.f_opt)\n\n        center = self.x_opt\n        successes = 0\n        iterations = 0\n\n\n        while self.budget > 0:\n            diversity = self.calculate_diversity(population)\n\n            if diversity > 1.0:\n                self.mutation_prob_gaussian = 0.8\n                self.mutation_prob_cauchy = 0.2\n            else:\n                self.mutation_prob_gaussian = 0.2\n                self.mutation_prob_cauchy = 0.8\n\n            new_population = np.copy(population)\n            new_fitness = np.copy(fitness)\n\n            for i in range(self.pop_size):\n                if self.budget <= 0:\n                    break\n\n                if np.random.rand() < self.mutation_prob_gaussian:\n                    mutated_individual = self.mutate_gaussian(population[i])\n                else:\n                    mutated_individual = self.mutate_cauchy(population[i])\n\n                mutated_individual = self.correct_bounds(mutated_individual)\n\n                f_mutated = func(mutated_individual)\n                self.budget -= 1\n                iterations += 1\n\n                if f_mutated < new_fitness[i]:\n                  new_fitness[i] = f_mutated\n                  new_population[i] = mutated_individual\n                  successes +=1\n                  if f_mutated < self.f_opt:\n                    self.f_opt = f_mutated\n                    self.x_opt = mutated_individual\n                    center = self.x_opt\n                    self.iterations_since_last_success = 0\n                else:\n                    self.iterations_since_last_success +=1\n\n            # Replace the current population with the new one\n            population = new_population\n            fitness = new_fitness\n\n            # Shrink the search range around the best individual\n            self.search_range *= self.shrink_factor\n            self.lb = np.clip(center - self.search_range/2, -5.0, 5.0)\n            self.ub = np.clip(center + self.search_range/2, -5.0, 5.0)\n\n            # Adapt mutation scale\n            self.mutation_scale = 0.95 * self.mutation_scale\n\n             # Adjust Cauchy scale based on success rate\n            if iterations > 0:\n                self.success_rate = successes / iterations\n                if self.success_rate > 0.2:\n                    self.cauchy_scale *= 1.1  # Increase scale if successful\n                else:\n                    self.cauchy_scale *= 0.9  # Decrease scale if not successful\n                successes = 0\n                iterations = 0\n\n            self.cauchy_scale = np.clip(self.cauchy_scale, 0.01, 1.0) # Limit Cauchy scale\n\n            # Stagnation detection and population size adjustment\n            if len(self.best_fitness_history) > self.stagnation_threshold:\n                if abs(self.f_opt - self.best_fitness_history[-self.stagnation_threshold]) < 1e-5:\n                    self.stagnation_counter += self.pop_size\n                else:\n                    self.stagnation_counter = 0\n\n            self.best_fitness_history.append(self.f_opt)\n            self.adjust_population_size()\n\n        # Local Search around best solution\n        search_range_local = self.ub - self.lb\n        for _ in range(min(self.local_search_iterations, self.budget)):\n             if self.budget <= 0:\n                 break\n\n             perturbation = np.random.uniform(-search_range_local/100, search_range_local/100, self.dim)\n             local_x = np.clip(self.x_opt + perturbation, self.lb, self.ub)\n             local_f = func(local_x)\n             self.budget -=1\n\n             if local_f < self.f_opt:\n                  self.f_opt = local_f\n                  self.x_opt = local_x\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "The algorithm HybridAdaptiveCauchyOptimizer scored 0.584 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["14141113-7977-460c-9182-d881a514090d", "2490d4be-f4cf-4c94-836d-98a90f0777d2"], "operator": "crossover", "metadata": {"llm_response": "# Description: A hybrid Cauchy optimizer that combines adaptive shrinking, diversity-based mutation probabilities, and dynamic population size adjustments.\n# Code: \n```python\nimport numpy as np\n\nclass HybridAdaptiveCauchyOptimizer:\n    def __init__(self, budget=10000, dim=10, initial_pop_size=20, shrink_factor=0.95, stagnation_threshold=1000, initial_cauchy_scale=0.1, local_search_iterations=5):\n        self.budget = budget\n        self.dim = dim\n        self.initial_pop_size = initial_pop_size\n        self.pop_size = initial_pop_size\n        self.shrink_factor = shrink_factor\n        self.stagnation_threshold = stagnation_threshold\n        self.cauchy_scale = initial_cauchy_scale\n        self.lb = -5.0\n        self.ub = 5.0\n        self.mutation_prob_gaussian = 0.5\n        self.mutation_prob_cauchy = 0.5\n        self.stagnation_counter = 0\n        self.best_fitness_history = []\n        self.search_range = self.ub - self.lb\n        self.mutation_scale = 1.0 # Scale for Gaussian mutation\n        self.local_search_iterations = local_search_iterations\n        self.success_rate = 0.5  # Initialize success rate\n        self.iterations_since_last_success = 0\n\n    def initialize_population(self):\n        return np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n\n    def calculate_diversity(self, population):\n        centroid = np.mean(population, axis=0)\n        distances = np.linalg.norm(population - centroid, axis=1)\n        return np.mean(distances)\n\n    def mutate_gaussian(self, x):\n        return x + self.mutation_scale * np.random.normal(0, 1, size=self.dim)\n\n    def mutate_cauchy(self, x):\n         return x + self.search_range * np.random.standard_cauchy(size=self.dim) * self.cauchy_scale\n\n    def correct_bounds(self, x):\n        return np.clip(x, self.lb, self.ub)\n\n    def adjust_population_size(self):\n        if self.stagnation_counter > self.stagnation_threshold:\n            self.pop_size = int(self.pop_size * 0.9)  # Reduce population size if stagnating\n            if self.pop_size < 10:\n                self.pop_size = 10 # Minimum population size\n            self.stagnation_counter = 0 # Reset stagnation counter\n            self.mutation_scale = 1.0\n            self.cauchy_scale = self.initial_cauchy_scale\n            print(f\"Stagnation detected, reducing pop size to {self.pop_size}\")\n\n    def __call__(self, func):\n        population = self.initialize_population()\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n        \n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index]\n        self.best_fitness_history.append(self.f_opt)\n\n        center = self.x_opt\n        successes = 0\n        iterations = 0\n\n\n        while self.budget > 0:\n            diversity = self.calculate_diversity(population)\n\n            if diversity > 1.0:\n                self.mutation_prob_gaussian = 0.8\n                self.mutation_prob_cauchy = 0.2\n            else:\n                self.mutation_prob_gaussian = 0.2\n                self.mutation_prob_cauchy = 0.8\n\n            new_population = np.copy(population)\n            new_fitness = np.copy(fitness)\n\n            for i in range(self.pop_size):\n                if self.budget <= 0:\n                    break\n\n                if np.random.rand() < self.mutation_prob_gaussian:\n                    mutated_individual = self.mutate_gaussian(population[i])\n                else:\n                    mutated_individual = self.mutate_cauchy(population[i])\n\n                mutated_individual = self.correct_bounds(mutated_individual)\n\n                f_mutated = func(mutated_individual)\n                self.budget -= 1\n                iterations += 1\n\n                if f_mutated < new_fitness[i]:\n                  new_fitness[i] = f_mutated\n                  new_population[i] = mutated_individual\n                  successes +=1\n                  if f_mutated < self.f_opt:\n                    self.f_opt = f_mutated\n                    self.x_opt = mutated_individual\n                    center = self.x_opt\n                    self.iterations_since_last_success = 0\n                else:\n                    self.iterations_since_last_success +=1\n\n            # Replace the current population with the new one\n            population = new_population\n            fitness = new_fitness\n\n            # Shrink the search range around the best individual\n            self.search_range *= self.shrink_factor\n            self.lb = np.clip(center - self.search_range/2, -5.0, 5.0)\n            self.ub = np.clip(center + self.search_range/2, -5.0, 5.0)\n\n            # Adapt mutation scale\n            self.mutation_scale = 0.95 * self.mutation_scale\n\n             # Adjust Cauchy scale based on success rate\n            if iterations > 0:\n                self.success_rate = successes / iterations\n                if self.success_rate > 0.2:\n                    self.cauchy_scale *= 1.1  # Increase scale if successful\n                else:\n                    self.cauchy_scale *= 0.9  # Decrease scale if not successful\n                successes = 0\n                iterations = 0\n\n            self.cauchy_scale = np.clip(self.cauchy_scale, 0.01, 1.0) # Limit Cauchy scale\n\n            # Stagnation detection and population size adjustment\n            if len(self.best_fitness_history) > self.stagnation_threshold:\n                if abs(self.f_opt - self.best_fitness_history[-self.stagnation_threshold]) < 1e-5:\n                    self.stagnation_counter += self.pop_size\n                else:\n                    self.stagnation_counter = 0\n\n            self.best_fitness_history.append(self.f_opt)\n            self.adjust_population_size()\n\n        # Local Search around best solution\n        search_range_local = self.ub - self.lb\n        for _ in range(min(self.local_search_iterations, self.budget)):\n             if self.budget <= 0:\n                 break\n\n             perturbation = np.random.uniform(-search_range_local/100, search_range_local/100, self.dim)\n             local_x = np.clip(self.x_opt + perturbation, self.lb, self.ub)\n             local_f = func(local_x)\n             self.budget -=1\n\n             if local_f < self.f_opt:\n                  self.f_opt = local_f\n                  self.x_opt = local_x\n\n\n        return self.f_opt, self.x_opt\n```", "aucs": [0.14028615098781483, 0.2175484419996746, 0.7636542505286976, 0.8840574903982403, 0.7691436609796775, 0.7926168451705687, 0.2983697872595843, 0.713615375011406, 0.7676448724589966, 0.1742015291793172, 0.8824809291047674, 0.997777639389362, 0.2726682110059818, 0.7816630160983697, 0.7442314035227999, 0.3970020808911219, 0.5783461827603344, 0.8458781355852519, 0.1850602989259641, 0.48179296721757214], "operator": "crossover", "theta_sampled": 0.03656153081783696, "reward": 0.0, "generation": 6}, "task_prompt": ""}
{"id": "04fdb57e-863b-4cf4-945b-434ce4453081", "fitness": 0.5360443479920326, "name": "AdaptiveShrinkingCauchyOptimizer", "description": "Shrinking Cauchy Optimizer with dynamic population size adjustment and adaptive step size.", "code": "import numpy as np\n\nclass AdaptiveShrinkingCauchyOptimizer:\n    def __init__(self, budget=10000, dim=10, initial_population_size=20, shrink_factor=0.95, step_size_factor=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = initial_population_size\n        self.population_size = initial_population_size\n        self.shrink_factor = shrink_factor\n        self.step_size_factor = step_size_factor\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        \n        population = np.random.uniform(lb, ub, size=(self.population_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.population_size\n\n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index]\n\n        search_range = ub - lb\n        center = self.x_opt\n        \n        iteration = 0\n\n        while self.budget > 0:\n            iteration += 1\n            # Cauchy mutation around the best individual\n            for i in range(self.population_size):\n                if self.budget <= 0:\n                    break\n\n                # Adaptive step size based on the shrinking search range\n                adaptive_step_size = self.step_size_factor * search_range\n\n                mutation = center + adaptive_step_size * np.random.standard_cauchy(size=self.dim)\n                mutation = np.clip(mutation, lb, ub)  # Clip values to stay within bounds\n\n                f_mutation = func(mutation)\n                self.budget -= 1\n\n                if f_mutation < fitness[i]:\n                    fitness[i] = f_mutation\n                    population[i] = mutation\n\n                    if f_mutation < self.f_opt:\n                        self.f_opt = f_mutation\n                        self.x_opt = mutation\n                        center = self.x_opt\n\n            # Shrink the search range around the best individual\n            search_range *= self.shrink_factor\n            lb = np.clip(center - search_range/2, func.bounds.lb, func.bounds.ub)\n            ub = np.clip(center + search_range/2, func.bounds.lb, func.bounds.ub)\n            \n            # Dynamically adjust population size\n            if iteration % 5 == 0:  # Adjust population size every 5 iterations\n                if self.f_opt < np.mean(fitness):\n                    self.population_size = min(self.initial_population_size * 2, self.budget) # Increase if doing well\n                else:\n                    self.population_size = max(self.initial_population_size // 2, 1) # Decrease if stagnant\n                \n                # Regenerate population if changed\n                if self.population_size != len(population):\n                    population = np.random.uniform(lb, ub, size=(self.population_size, self.dim))\n                    fitness = np.array([func(x) for x in population])\n                    self.budget -= self.population_size\n\n                    best_index = np.argmin(fitness)\n                    if fitness[best_index] < self.f_opt:\n                        self.f_opt = fitness[best_index]\n                        self.x_opt = population[best_index]\n                        center = self.x_opt\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "The algorithm AdaptiveShrinkingCauchyOptimizer scored 0.536 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["e8a11899-492c-463b-9412-67910c8f787b"], "operator": "mutation", "metadata": {"llm_response": "# Description: Shrinking Cauchy Optimizer with dynamic population size adjustment and adaptive step size.\n# Code: \n```python\nimport numpy as np\n\nclass AdaptiveShrinkingCauchyOptimizer:\n    def __init__(self, budget=10000, dim=10, initial_population_size=20, shrink_factor=0.95, step_size_factor=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = initial_population_size\n        self.population_size = initial_population_size\n        self.shrink_factor = shrink_factor\n        self.step_size_factor = step_size_factor\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        \n        population = np.random.uniform(lb, ub, size=(self.population_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.population_size\n\n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index]\n\n        search_range = ub - lb\n        center = self.x_opt\n        \n        iteration = 0\n\n        while self.budget > 0:\n            iteration += 1\n            # Cauchy mutation around the best individual\n            for i in range(self.population_size):\n                if self.budget <= 0:\n                    break\n\n                # Adaptive step size based on the shrinking search range\n                adaptive_step_size = self.step_size_factor * search_range\n\n                mutation = center + adaptive_step_size * np.random.standard_cauchy(size=self.dim)\n                mutation = np.clip(mutation, lb, ub)  # Clip values to stay within bounds\n\n                f_mutation = func(mutation)\n                self.budget -= 1\n\n                if f_mutation < fitness[i]:\n                    fitness[i] = f_mutation\n                    population[i] = mutation\n\n                    if f_mutation < self.f_opt:\n                        self.f_opt = f_mutation\n                        self.x_opt = mutation\n                        center = self.x_opt\n\n            # Shrink the search range around the best individual\n            search_range *= self.shrink_factor\n            lb = np.clip(center - search_range/2, func.bounds.lb, func.bounds.ub)\n            ub = np.clip(center + search_range/2, func.bounds.lb, func.bounds.ub)\n            \n            # Dynamically adjust population size\n            if iteration % 5 == 0:  # Adjust population size every 5 iterations\n                if self.f_opt < np.mean(fitness):\n                    self.population_size = min(self.initial_population_size * 2, self.budget) # Increase if doing well\n                else:\n                    self.population_size = max(self.initial_population_size // 2, 1) # Decrease if stagnant\n                \n                # Regenerate population if changed\n                if self.population_size != len(population):\n                    population = np.random.uniform(lb, ub, size=(self.population_size, self.dim))\n                    fitness = np.array([func(x) for x in population])\n                    self.budget -= self.population_size\n\n                    best_index = np.argmin(fitness)\n                    if fitness[best_index] < self.f_opt:\n                        self.f_opt = fitness[best_index]\n                        self.x_opt = population[best_index]\n                        center = self.x_opt\n\n\n        return self.f_opt, self.x_opt\n```", "aucs": [0.1763260774118679, 0.18924603230133252, 0.6130025231986513, 0.8754096585007438, 0.6250984160908009, 0.6751715877245728, 0.33289785100666536, 0.5381118589130863, 0.6267704539577849, 0.18405981518116388, 0.8208832367685022, 0.9973870724958807, 0.2390222254984381, 0.6112580185226442, 0.8550872820243725, 0.4174772019371904, 0.4211432467781818, 0.7802067057107814, 0.23717943206973424, 0.5051482637482547], "operator": "mutation", "theta_sampled": 0.006287696960626061, "reward": 0.0, "generation": 6}, "task_prompt": ""}
{"id": "479fc4c0-646b-443b-8ea6-4c6cd0fcdcc9", "fitness": 0.56651260701277, "name": "HybridAdaptiveOptimizer", "description": "A hybrid optimizer that combines adaptive Cauchy mutation with Gaussian mutation, dynamic population sizing, and local search for improved exploration and exploitation.", "code": "import numpy as np\n\nclass HybridAdaptiveOptimizer:\n    def __init__(self, budget=10000, dim=10, initial_pop_size=20, shrink_factor=0.95, stagnation_threshold=1000, initial_cauchy_scale=0.1, local_search_iterations=5, diversity_threshold=1.0):\n        self.budget = budget\n        self.dim = dim\n        self.initial_pop_size = initial_pop_size\n        self.pop_size = initial_pop_size\n        self.shrink_factor = shrink_factor\n        self.stagnation_threshold = stagnation_threshold\n        self.cauchy_scale = initial_cauchy_scale\n        self.lb = -5.0\n        self.ub = 5.0\n        self.mutation_prob_gaussian = 0.5\n        self.mutation_prob_cauchy = 0.5\n        self.stagnation_counter = 0\n        self.best_fitness_history = []\n        self.search_range = self.ub - self.lb\n        self.mutation_scale = 1.0 # Scale for Gaussian mutation\n        self.local_search_iterations = local_search_iterations\n        self.success_rate = 0.5\n        self.diversity_threshold = diversity_threshold\n\n    def initialize_population(self):\n        return np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n\n    def calculate_diversity(self, population):\n        centroid = np.mean(population, axis=0)\n        distances = np.linalg.norm(population - centroid, axis=1)\n        return np.mean(distances)\n\n    def mutate_gaussian(self, self_x):\n        return self_x + self.mutation_scale * np.random.normal(0, 1, size=self.dim)\n\n    def mutate_cauchy(self, self_x):\n        return self_x + self.search_range * np.random.standard_cauchy(size=self.dim) * self.cauchy_scale\n\n    def correct_bounds(self, self_x):\n        return np.clip(self_x, self.lb, self.ub)\n\n    def adjust_population_size(self):\n        if self.stagnation_counter > self.stagnation_threshold:\n            self.pop_size = int(self.pop_size * 0.9)  # Reduce population size if stagnating\n            if self.pop_size < 10:\n                self.pop_size = 10 # Minimum population size\n            self.stagnation_counter = 0 # Reset stagnation counter\n            self.mutation_scale = 1.0\n            self.cauchy_scale = 0.1 # Reset cauchy scale, as we shrink the population size\n            print(f\"Stagnation detected, reducing pop size to {self.pop_size}\")\n\n    def __call__(self, func):\n        population = self.initialize_population()\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n        \n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index]\n        self.best_fitness_history.append(self.f_opt)\n\n        center = self.x_opt\n        successes = 0\n        iterations = 0\n\n\n        while self.budget > 0:\n            diversity = self.calculate_diversity(population)\n\n            if diversity > self.diversity_threshold:\n                self.mutation_prob_gaussian = 0.7\n                self.mutation_prob_cauchy = 0.3\n            else:\n                self.mutation_prob_gaussian = 0.3\n                self.mutation_prob_cauchy = 0.7\n\n            new_population = np.copy(population)\n            new_fitness = np.copy(fitness)\n\n            for i in range(self.pop_size):\n                if self.budget <=0:\n                    break\n                if np.random.rand() < self.mutation_prob_gaussian:\n                    mutated_individual = self.mutate_gaussian(population[i])\n                else:\n                    mutated_individual = self.mutate_cauchy(population[i])\n\n                mutated_individual = self.correct_bounds(mutated_individual)\n\n                f_mutated = func(mutated_individual)\n                self.budget -= 1\n                iterations += 1\n\n                if f_mutated < new_fitness[i]:\n                  new_fitness[i] = f_mutated\n                  new_population[i] = mutated_individual\n                  successes +=1\n                  if f_mutated < self.f_opt:\n                    self.f_opt = f_mutated\n                    self.x_opt = mutated_individual\n                    center = self.x_opt\n\n\n            # Replace the current population with the new one\n            population = new_population\n            fitness = new_fitness\n\n\n            # Shrink the search range around the best individual\n            self.search_range *= self.shrink_factor\n            self.lb = np.clip(center - self.search_range/2, -5.0, 5.0)\n            self.ub = np.clip(center + self.search_range/2, -5.0, 5.0)\n\n            # Adapt mutation scale\n            self.mutation_scale = 0.95 * self.mutation_scale\n\n            # Adjust Cauchy scale based on success rate\n            if iterations > 0:\n                self.success_rate = successes / iterations\n                if self.success_rate > 0.2:\n                    self.cauchy_scale *= 1.1  # Increase scale if successful\n                else:\n                    self.cauchy_scale *= 0.9  # Decrease scale if not successful\n                successes = 0\n                iterations = 0\n\n            self.cauchy_scale = np.clip(self.cauchy_scale, 0.01, 1.0) # Limit Cauchy scale\n\n            # Stagnation detection and population size adjustment\n            if len(self.best_fitness_history) > self.stagnation_threshold:\n                if abs(self.f_opt - self.best_fitness_history[-self.stagnation_threshold]) < 1e-5:\n                    self.stagnation_counter += self.pop_size\n                else:\n                    self.stagnation_counter = 0\n\n            self.best_fitness_history.append(self.f_opt)\n            self.adjust_population_size()\n        \n\n        # Local Search around best solution\n        search_range_local = self.ub - self.lb # Use the shrunk search range for the local search\n        for _ in range(min(self.local_search_iterations, self.budget)):\n             if self.budget <= 0:\n                 break\n\n             perturbation = np.random.uniform(-search_range_local/100, search_range_local/100, self.dim)\n             local_x = np.clip(self.x_opt + perturbation, self.lb, self.ub)\n             local_f = func(local_x)\n             self.budget -=1\n\n             if local_f < self.f_opt:\n                  self.f_opt = local_f\n                  self.x_opt = local_x\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "The algorithm HybridAdaptiveOptimizer scored 0.567 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["14141113-7977-460c-9182-d881a514090d", "2490d4be-f4cf-4c94-836d-98a90f0777d2"], "operator": "crossover", "metadata": {"llm_response": "# Description: A hybrid optimizer that combines adaptive Cauchy mutation with Gaussian mutation, dynamic population sizing, and local search for improved exploration and exploitation.\n# Code: \n```python\nimport numpy as np\n\nclass HybridAdaptiveOptimizer:\n    def __init__(self, budget=10000, dim=10, initial_pop_size=20, shrink_factor=0.95, stagnation_threshold=1000, initial_cauchy_scale=0.1, local_search_iterations=5, diversity_threshold=1.0):\n        self.budget = budget\n        self.dim = dim\n        self.initial_pop_size = initial_pop_size\n        self.pop_size = initial_pop_size\n        self.shrink_factor = shrink_factor\n        self.stagnation_threshold = stagnation_threshold\n        self.cauchy_scale = initial_cauchy_scale\n        self.lb = -5.0\n        self.ub = 5.0\n        self.mutation_prob_gaussian = 0.5\n        self.mutation_prob_cauchy = 0.5\n        self.stagnation_counter = 0\n        self.best_fitness_history = []\n        self.search_range = self.ub - self.lb\n        self.mutation_scale = 1.0 # Scale for Gaussian mutation\n        self.local_search_iterations = local_search_iterations\n        self.success_rate = 0.5\n        self.diversity_threshold = diversity_threshold\n\n    def initialize_population(self):\n        return np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n\n    def calculate_diversity(self, population):\n        centroid = np.mean(population, axis=0)\n        distances = np.linalg.norm(population - centroid, axis=1)\n        return np.mean(distances)\n\n    def mutate_gaussian(self, self_x):\n        return self_x + self.mutation_scale * np.random.normal(0, 1, size=self.dim)\n\n    def mutate_cauchy(self, self_x):\n        return self_x + self.search_range * np.random.standard_cauchy(size=self.dim) * self.cauchy_scale\n\n    def correct_bounds(self, self_x):\n        return np.clip(self_x, self.lb, self.ub)\n\n    def adjust_population_size(self):\n        if self.stagnation_counter > self.stagnation_threshold:\n            self.pop_size = int(self.pop_size * 0.9)  # Reduce population size if stagnating\n            if self.pop_size < 10:\n                self.pop_size = 10 # Minimum population size\n            self.stagnation_counter = 0 # Reset stagnation counter\n            self.mutation_scale = 1.0\n            self.cauchy_scale = 0.1 # Reset cauchy scale, as we shrink the population size\n            print(f\"Stagnation detected, reducing pop size to {self.pop_size}\")\n\n    def __call__(self, func):\n        population = self.initialize_population()\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n        \n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index]\n        self.best_fitness_history.append(self.f_opt)\n\n        center = self.x_opt\n        successes = 0\n        iterations = 0\n\n\n        while self.budget > 0:\n            diversity = self.calculate_diversity(population)\n\n            if diversity > self.diversity_threshold:\n                self.mutation_prob_gaussian = 0.7\n                self.mutation_prob_cauchy = 0.3\n            else:\n                self.mutation_prob_gaussian = 0.3\n                self.mutation_prob_cauchy = 0.7\n\n            new_population = np.copy(population)\n            new_fitness = np.copy(fitness)\n\n            for i in range(self.pop_size):\n                if self.budget <=0:\n                    break\n                if np.random.rand() < self.mutation_prob_gaussian:\n                    mutated_individual = self.mutate_gaussian(population[i])\n                else:\n                    mutated_individual = self.mutate_cauchy(population[i])\n\n                mutated_individual = self.correct_bounds(mutated_individual)\n\n                f_mutated = func(mutated_individual)\n                self.budget -= 1\n                iterations += 1\n\n                if f_mutated < new_fitness[i]:\n                  new_fitness[i] = f_mutated\n                  new_population[i] = mutated_individual\n                  successes +=1\n                  if f_mutated < self.f_opt:\n                    self.f_opt = f_mutated\n                    self.x_opt = mutated_individual\n                    center = self.x_opt\n\n\n            # Replace the current population with the new one\n            population = new_population\n            fitness = new_fitness\n\n\n            # Shrink the search range around the best individual\n            self.search_range *= self.shrink_factor\n            self.lb = np.clip(center - self.search_range/2, -5.0, 5.0)\n            self.ub = np.clip(center + self.search_range/2, -5.0, 5.0)\n\n            # Adapt mutation scale\n            self.mutation_scale = 0.95 * self.mutation_scale\n\n            # Adjust Cauchy scale based on success rate\n            if iterations > 0:\n                self.success_rate = successes / iterations\n                if self.success_rate > 0.2:\n                    self.cauchy_scale *= 1.1  # Increase scale if successful\n                else:\n                    self.cauchy_scale *= 0.9  # Decrease scale if not successful\n                successes = 0\n                iterations = 0\n\n            self.cauchy_scale = np.clip(self.cauchy_scale, 0.01, 1.0) # Limit Cauchy scale\n\n            # Stagnation detection and population size adjustment\n            if len(self.best_fitness_history) > self.stagnation_threshold:\n                if abs(self.f_opt - self.best_fitness_history[-self.stagnation_threshold]) < 1e-5:\n                    self.stagnation_counter += self.pop_size\n                else:\n                    self.stagnation_counter = 0\n\n            self.best_fitness_history.append(self.f_opt)\n            self.adjust_population_size()\n        \n\n        # Local Search around best solution\n        search_range_local = self.ub - self.lb # Use the shrunk search range for the local search\n        for _ in range(min(self.local_search_iterations, self.budget)):\n             if self.budget <= 0:\n                 break\n\n             perturbation = np.random.uniform(-search_range_local/100, search_range_local/100, self.dim)\n             local_x = np.clip(self.x_opt + perturbation, self.lb, self.ub)\n             local_f = func(local_x)\n             self.budget -=1\n\n             if local_f < self.f_opt:\n                  self.f_opt = local_f\n                  self.x_opt = local_x\n\n        return self.f_opt, self.x_opt\n```", "aucs": [0.1730123674407561, 0.24321458716906164, 0.7610562231523439, 0.8802987657353796, 0.33384554466123095, 0.7963529616890428, 0.3140574293050752, 0.7152595750822943, 0.7717534729126543, 0.2021204156249199, 0.8959405223817811, 0.9982388204352526, 0.263421421570453, 0.7748040637071302, 0.7463231638297423, 0.6409460044290978, 0.2933407414536341, 0.8487963584600571, 0.17697212386464467, 0.5004975773508514], "operator": "crossover", "theta_sampled": 0.007766515989407986, "reward": 0.0, "generation": 6}, "task_prompt": ""}
{"id": "4ec628c7-fa87-4c4d-9ec2-f8c68947a102", "fitness": 0.562581393036963, "name": "AdaptiveDynamicShrinkingCauchyOptimizer", "description": "Adaptive Dynamic Shrinking Cauchy Optimizer with momentum and velocity clipping to improve exploration and convergence.", "code": "import numpy as np\n\nclass AdaptiveDynamicShrinkingCauchyOptimizer:\n    def __init__(self, budget=10000, dim=10, population_size=20, shrink_factor=0.95, initial_cauchy_scale=0.1, momentum_factor=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = population_size\n        self.shrink_factor = shrink_factor\n        self.initial_cauchy_scale = initial_cauchy_scale\n        self.momentum_factor = momentum_factor\n        self.velocities = np.zeros((population_size, dim)) # Initialize velocities for momentum\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        population = np.random.uniform(lb, ub, size=(self.population_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.population_size\n\n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index]\n\n        search_range = ub - lb\n        center = self.x_opt\n        cauchy_scale = self.initial_cauchy_scale # Initialize the Cauchy scale\n\n        iteration = 0\n        while self.budget > 0:\n            iteration += 1\n            # Cauchy mutation around the best individual\n            for i in range(self.population_size):\n                if self.budget <= 0:\n                    break\n\n                # Dynamically adjust Cauchy scale. Reduce it over time.\n                current_cauchy_scale = cauchy_scale * (1 - iteration/ (self.budget // self.population_size + 1))\n\n                # Momentum-based update\n                cauchy_mutation = np.random.standard_cauchy(size=self.dim) * current_cauchy_scale\n                self.velocities[i] = self.momentum_factor * self.velocities[i] + (1 - self.momentum_factor) * cauchy_mutation\n\n                # Clip the velocity to prevent large jumps\n                velocity_clipping = 0.1 * search_range #clip to a percentage of the search range\n                self.velocities[i] = np.clip(self.velocities[i], -velocity_clipping, velocity_clipping)\n                \n                mutation = center + search_range * self.velocities[i]  # Scale Cauchy distribution\n                mutation = np.clip(mutation, lb, ub)  # Clip values to stay within bounds\n\n                f_mutation = func(mutation)\n                self.budget -= 1\n\n                if f_mutation < fitness[i]:\n                    fitness[i] = f_mutation\n                    population[i] = mutation\n\n                    if f_mutation < self.f_opt:\n                        self.f_opt = f_mutation\n                        self.x_opt = mutation\n                        center = self.x_opt\n\n            # Shrink the search range around the best individual\n            search_range *= self.shrink_factor\n            lb = np.clip(center - search_range/2, func.bounds.lb, func.bounds.ub)\n            ub = np.clip(center + search_range/2, func.bounds.lb, func.bounds.ub)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "The algorithm AdaptiveDynamicShrinkingCauchyOptimizer scored 0.563 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["81c4333c-0d55-4a9f-9170-68690646402b"], "operator": "mutation", "metadata": {"llm_response": "# Description: Adaptive Dynamic Shrinking Cauchy Optimizer with momentum and velocity clipping to improve exploration and convergence.\n# Code: \n```python\nimport numpy as np\n\nclass AdaptiveDynamicShrinkingCauchyOptimizer:\n    def __init__(self, budget=10000, dim=10, population_size=20, shrink_factor=0.95, initial_cauchy_scale=0.1, momentum_factor=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.population_size = population_size\n        self.shrink_factor = shrink_factor\n        self.initial_cauchy_scale = initial_cauchy_scale\n        self.momentum_factor = momentum_factor\n        self.velocities = np.zeros((population_size, dim)) # Initialize velocities for momentum\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        population = np.random.uniform(lb, ub, size=(self.population_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.population_size\n\n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index]\n\n        search_range = ub - lb\n        center = self.x_opt\n        cauchy_scale = self.initial_cauchy_scale # Initialize the Cauchy scale\n\n        iteration = 0\n        while self.budget > 0:\n            iteration += 1\n            # Cauchy mutation around the best individual\n            for i in range(self.population_size):\n                if self.budget <= 0:\n                    break\n\n                # Dynamically adjust Cauchy scale. Reduce it over time.\n                current_cauchy_scale = cauchy_scale * (1 - iteration/ (self.budget // self.population_size + 1))\n\n                # Momentum-based update\n                cauchy_mutation = np.random.standard_cauchy(size=self.dim) * current_cauchy_scale\n                self.velocities[i] = self.momentum_factor * self.velocities[i] + (1 - self.momentum_factor) * cauchy_mutation\n\n                # Clip the velocity to prevent large jumps\n                velocity_clipping = 0.1 * search_range #clip to a percentage of the search range\n                self.velocities[i] = np.clip(self.velocities[i], -velocity_clipping, velocity_clipping)\n                \n                mutation = center + search_range * self.velocities[i]  # Scale Cauchy distribution\n                mutation = np.clip(mutation, lb, ub)  # Clip values to stay within bounds\n\n                f_mutation = func(mutation)\n                self.budget -= 1\n\n                if f_mutation < fitness[i]:\n                    fitness[i] = f_mutation\n                    population[i] = mutation\n\n                    if f_mutation < self.f_opt:\n                        self.f_opt = f_mutation\n                        self.x_opt = mutation\n                        center = self.x_opt\n\n            # Shrink the search range around the best individual\n            search_range *= self.shrink_factor\n            lb = np.clip(center - search_range/2, func.bounds.lb, func.bounds.ub)\n            ub = np.clip(center + search_range/2, func.bounds.lb, func.bounds.ub)\n\n        return self.f_opt, self.x_opt\n```", "aucs": [0.1580932486571215, 0.19654631937119116, 0.866078044100408, 0.9346713063737317, 0.25907549541816666, 0.882137064903664, 0.3171727532774936, 0.839648118406018, 0.8638335468561014, 0.22273412810005744, 0.6256627671461882, 0.9963183245376864, 0.233473673891488, 0.7007852526418004, 0.8144076637916848, 0.3582391418031283, 0.37693471059847605, 0.9118591657629369, 0.21792373591351732, 0.47603339918839904], "operator": "mutation", "theta_sampled": 0.004607755231382569, "reward": 0.0, "generation": 6}, "task_prompt": ""}
{"id": "b07d0cab-02dd-4c16-9fa3-791ad7fa23f1", "fitness": 0.5331521798940868, "name": "AdaptiveCauchyMomentumOptimizer", "description": "Adaptive Cauchy optimizer with momentum, dynamic population size, and a restart mechanism for improved exploration and exploitation.", "code": "import numpy as np\n\nclass AdaptiveCauchyMomentumOptimizer:\n    def __init__(self, budget=10000, dim=10, initial_population_size=20, shrink_factor=0.95, initial_cauchy_scale=0.1, local_search_iterations=5, momentum=0.1, restart_probability=0.01):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = initial_population_size\n        self.population_size = initial_population_size  # Dynamic population size\n        self.shrink_factor = shrink_factor\n        self.cauchy_scale = initial_cauchy_scale\n        self.success_rate = 0.5  # Initialize success rate\n        self.local_search_iterations = local_search_iterations\n        self.momentum = momentum\n        self.restart_probability = restart_probability\n        self.velocity = np.zeros(dim)\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        population = np.random.uniform(lb, ub, size=(self.population_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.population_size\n\n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index]\n\n        search_range = ub - lb\n        center = self.x_opt\n\n        successes = 0\n        iterations = 0\n\n        while self.budget > 0:\n            # Cauchy mutation with momentum around the best individual\n            for i in range(self.population_size):\n                if self.budget <= 0:\n                    break\n\n                # Cauchy mutation with momentum\n                cauchy_step = np.random.standard_cauchy(size=self.dim) * self.cauchy_scale * search_range\n                self.velocity = self.momentum * self.velocity + (1 - self.momentum) * cauchy_step\n                mutation = center + self.velocity  # Scale Cauchy distribution\n                mutation = np.clip(mutation, lb, ub)  # Clip values to stay within bounds\n\n                f_mutation = func(mutation)\n                self.budget -= 1\n                iterations += 1\n\n                if f_mutation < fitness[i]:\n                    fitness[i] = f_mutation\n                    population[i] = mutation\n                    successes += 1\n\n                    if f_mutation < self.f_opt:\n                        self.f_opt = f_mutation\n                        self.x_opt = mutation\n                        center = self.x_opt\n\n            # Shrink the search range around the best individual\n            search_range *= self.shrink_factor\n            lb = np.clip(center - search_range/2, func.bounds.lb, func.bounds.ub)\n            ub = np.clip(center + search_range/2, func.bounds.lb, func.bounds.ub)\n\n            # Adjust Cauchy scale based on success rate\n            if iterations > 0:\n                self.success_rate = successes / iterations\n                if self.success_rate > 0.2:\n                    self.cauchy_scale *= 1.1  # Increase scale if successful\n                else:\n                    self.cauchy_scale *= 0.9  # Decrease scale if not successful\n                successes = 0\n                iterations = 0\n\n            self.cauchy_scale = np.clip(self.cauchy_scale, 0.01, 1.0) # Limit Cauchy scale\n            \n            # Dynamic population size adjustment\n            if self.success_rate > 0.3 and self.population_size < 2 * self.initial_population_size:\n                self.population_size += 1\n                population = np.vstack((population, np.random.uniform(lb, ub, size=(1, self.dim))))\n                new_fitness = np.array([func(x) for x in population[-1:]])\n                self.budget -= 1\n                fitness = np.concatenate((fitness, new_fitness))\n                if new_fitness[0] < self.f_opt:\n                    self.f_opt = new_fitness[0]\n                    self.x_opt = population[-1]\n                    center = self.x_opt\n                    \n            elif self.success_rate < 0.1 and self.population_size > self.initial_population_size // 2:\n                self.population_size -= 1\n                population = population[:-1]\n                fitness = fitness[:-1]\n                best_index = np.argmin(fitness)\n                self.f_opt = fitness[best_index]\n                self.x_opt = population[best_index]\n                center = self.x_opt\n\n            # Restart mechanism for exploration\n            if np.random.rand() < self.restart_probability:\n                center = np.random.uniform(lb, ub, size=self.dim)\n                self.x_opt = center\n                population = np.random.uniform(lb, ub, size=(self.population_size, self.dim))\n                fitness = np.array([func(x) for x in population])\n                self.budget -= self.population_size\n\n                best_index = np.argmin(fitness)\n                self.f_opt = fitness[best_index]\n                self.x_opt = population[best_index]\n\n\n        # Local Search around best solution\n        for _ in range(min(self.local_search_iterations, self.budget)):\n             if self.budget <= 0:\n                 break\n\n             perturbation = np.random.uniform(-search_range/100, search_range/100, self.dim)\n             local_x = np.clip(self.x_opt + perturbation, lb, ub)\n             local_f = func(local_x)\n             self.budget -=1\n\n             if local_f < self.f_opt:\n                  self.f_opt = local_f\n                  self.x_opt = local_x\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "The algorithm AdaptiveCauchyMomentumOptimizer scored 0.533 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["14141113-7977-460c-9182-d881a514090d"], "operator": "mutation", "metadata": {"llm_response": "# Description: Adaptive Cauchy optimizer with momentum, dynamic population size, and a restart mechanism for improved exploration and exploitation.\n# Code:\n```python\nimport numpy as np\n\nclass AdaptiveCauchyMomentumOptimizer:\n    def __init__(self, budget=10000, dim=10, initial_population_size=20, shrink_factor=0.95, initial_cauchy_scale=0.1, local_search_iterations=5, momentum=0.1, restart_probability=0.01):\n        self.budget = budget\n        self.dim = dim\n        self.initial_population_size = initial_population_size\n        self.population_size = initial_population_size  # Dynamic population size\n        self.shrink_factor = shrink_factor\n        self.cauchy_scale = initial_cauchy_scale\n        self.success_rate = 0.5  # Initialize success rate\n        self.local_search_iterations = local_search_iterations\n        self.momentum = momentum\n        self.restart_probability = restart_probability\n        self.velocity = np.zeros(dim)\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        population = np.random.uniform(lb, ub, size=(self.population_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.population_size\n\n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index]\n\n        search_range = ub - lb\n        center = self.x_opt\n\n        successes = 0\n        iterations = 0\n\n        while self.budget > 0:\n            # Cauchy mutation with momentum around the best individual\n            for i in range(self.population_size):\n                if self.budget <= 0:\n                    break\n\n                # Cauchy mutation with momentum\n                cauchy_step = np.random.standard_cauchy(size=self.dim) * self.cauchy_scale * search_range\n                self.velocity = self.momentum * self.velocity + (1 - self.momentum) * cauchy_step\n                mutation = center + self.velocity  # Scale Cauchy distribution\n                mutation = np.clip(mutation, lb, ub)  # Clip values to stay within bounds\n\n                f_mutation = func(mutation)\n                self.budget -= 1\n                iterations += 1\n\n                if f_mutation < fitness[i]:\n                    fitness[i] = f_mutation\n                    population[i] = mutation\n                    successes += 1\n\n                    if f_mutation < self.f_opt:\n                        self.f_opt = f_mutation\n                        self.x_opt = mutation\n                        center = self.x_opt\n\n            # Shrink the search range around the best individual\n            search_range *= self.shrink_factor\n            lb = np.clip(center - search_range/2, func.bounds.lb, func.bounds.ub)\n            ub = np.clip(center + search_range/2, func.bounds.lb, func.bounds.ub)\n\n            # Adjust Cauchy scale based on success rate\n            if iterations > 0:\n                self.success_rate = successes / iterations\n                if self.success_rate > 0.2:\n                    self.cauchy_scale *= 1.1  # Increase scale if successful\n                else:\n                    self.cauchy_scale *= 0.9  # Decrease scale if not successful\n                successes = 0\n                iterations = 0\n\n            self.cauchy_scale = np.clip(self.cauchy_scale, 0.01, 1.0) # Limit Cauchy scale\n            \n            # Dynamic population size adjustment\n            if self.success_rate > 0.3 and self.population_size < 2 * self.initial_population_size:\n                self.population_size += 1\n                population = np.vstack((population, np.random.uniform(lb, ub, size=(1, self.dim))))\n                new_fitness = np.array([func(x) for x in population[-1:]])\n                self.budget -= 1\n                fitness = np.concatenate((fitness, new_fitness))\n                if new_fitness[0] < self.f_opt:\n                    self.f_opt = new_fitness[0]\n                    self.x_opt = population[-1]\n                    center = self.x_opt\n                    \n            elif self.success_rate < 0.1 and self.population_size > self.initial_population_size // 2:\n                self.population_size -= 1\n                population = population[:-1]\n                fitness = fitness[:-1]\n                best_index = np.argmin(fitness)\n                self.f_opt = fitness[best_index]\n                self.x_opt = population[best_index]\n                center = self.x_opt\n\n            # Restart mechanism for exploration\n            if np.random.rand() < self.restart_probability:\n                center = np.random.uniform(lb, ub, size=self.dim)\n                self.x_opt = center\n                population = np.random.uniform(lb, ub, size=(self.population_size, self.dim))\n                fitness = np.array([func(x) for x in population])\n                self.budget -= self.population_size\n\n                best_index = np.argmin(fitness)\n                self.f_opt = fitness[best_index]\n                self.x_opt = population[best_index]\n\n\n        # Local Search around best solution\n        for _ in range(min(self.local_search_iterations, self.budget)):\n             if self.budget <= 0:\n                 break\n\n             perturbation = np.random.uniform(-search_range/100, search_range/100, self.dim)\n             local_x = np.clip(self.x_opt + perturbation, lb, ub)\n             local_f = func(local_x)\n             self.budget -=1\n\n             if local_f < self.f_opt:\n                  self.f_opt = local_f\n                  self.x_opt = local_x\n\n\n        return self.f_opt, self.x_opt\n```", "aucs": [0.22819449650615042, 0.17339820245690063, 0.84639656168081, 0.9357338220107547, 0.8449258099247352, 0.7952204547414339, 0.3407603097837638, 0.6337399619422324, 0], "operator": "mutation", "theta_sampled": 0.006357856575858489, "reward": 0.0, "generation": 6}, "task_prompt": ""}
{"id": "744797f2-bd27-486a-9e52-c6cd25db5306", "fitness": 0.5698075874446881, "name": "HybridAdaptiveCauchyOptimizer", "description": "A hybrid Cauchy optimizer that adapts mutation strategy based on diversity and stagnation, shrinks search space, and adjusts population size dynamically.", "code": "import numpy as np\n\nclass HybridAdaptiveCauchyOptimizer:\n    def __init__(self, budget=10000, dim=10, initial_pop_size=20, shrink_factor=0.95, stagnation_threshold=1000, cauchy_scale=0.1, local_search_iterations=5):\n        self.budget = budget\n        self.dim = dim\n        self.initial_pop_size = initial_pop_size\n        self.pop_size = initial_pop_size\n        self.shrink_factor = shrink_factor\n        self.stagnation_threshold = stagnation_threshold\n        self.cauchy_scale = cauchy_scale\n        self.lb = -5.0\n        self.ub = 5.0\n        self.mutation_prob_gaussian = 0.5\n        self.mutation_prob_cauchy = 0.5\n        self.stagnation_counter = 0\n        self.best_fitness_history = []\n        self.search_range = self.ub - self.lb\n        self.mutation_scale = 1.0 # Scale for Gaussian mutation\n        self.local_search_iterations = local_search_iterations\n        self.success_rate = 0.5\n        self.iterations = 0\n        self.successes = 0\n\n\n    def initialize_population(self):\n        return np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n\n    def calculate_diversity(self, population):\n        centroid = np.mean(population, axis=0)\n        distances = np.linalg.norm(population - centroid, axis=1)\n        return np.mean(distances)\n\n    def mutate_gaussian(self, x):\n        return x + self.mutation_scale * np.random.normal(0, 1, size=self.dim)\n\n    def mutate_cauchy(self, x):\n         return x + self.search_range * np.random.standard_cauchy(size=self.dim) * self.cauchy_scale\n\n    def correct_bounds(self, x):\n        return np.clip(x, self.lb, self.ub)\n\n    def adjust_population_size(self):\n        if self.stagnation_counter > self.stagnation_threshold:\n            self.pop_size = int(self.pop_size * 0.9)  # Reduce population size if stagnating\n            if self.pop_size < 10:\n                self.pop_size = 10 # Minimum population size\n            self.stagnation_counter = 0 # Reset stagnation counter\n            self.mutation_scale = 1.0\n            self.cauchy_scale = 0.1\n            print(f\"Stagnation detected, reducing pop size to {self.pop_size}\")\n\n\n    def __call__(self, func):\n        population = self.initialize_population()\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n        \n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index]\n        self.best_fitness_history.append(self.f_opt)\n\n        center = self.x_opt\n\n\n        while self.budget > 0:\n            diversity = self.calculate_diversity(population)\n\n            if diversity > 1.0:\n                self.mutation_prob_gaussian = 0.8\n                self.mutation_prob_cauchy = 0.2\n            else:\n                self.mutation_prob_gaussian = 0.2\n                self.mutation_prob_cauchy = 0.8\n\n            new_population = np.copy(population)\n            new_fitness = np.copy(fitness)\n\n            for i in range(self.pop_size):\n                if self.budget <= 0:\n                    break\n\n                if np.random.rand() < self.mutation_prob_gaussian:\n                    mutated_individual = self.mutate_gaussian(population[i])\n                else:\n                    mutated_individual = self.mutate_cauchy(population[i])\n\n                mutated_individual = self.correct_bounds(mutated_individual)\n\n                f_mutated = func(mutated_individual)\n                self.budget -= 1\n                self.iterations +=1\n\n                if f_mutated < new_fitness[i]:\n                  new_fitness[i] = f_mutated\n                  new_population[i] = mutated_individual\n                  self.successes +=1\n\n                  if f_mutated < self.f_opt:\n                    self.f_opt = f_mutated\n                    self.x_opt = mutated_individual\n                    center = self.x_opt\n\n\n            # Replace the current population with the new one\n            population = new_population\n            fitness = new_fitness\n\n\n            # Shrink the search range around the best individual\n            self.search_range *= self.shrink_factor\n            self.lb = np.clip(center - self.search_range/2, -5.0, 5.0)\n            self.ub = np.clip(center + self.search_range/2, -5.0, 5.0)\n\n            # Adapt mutation scale\n            self.mutation_scale = 0.95 * self.mutation_scale\n\n            # Stagnation detection and population size adjustment\n            if len(self.best_fitness_history) > self.stagnation_threshold:\n                if abs(self.f_opt - self.best_fitness_history[-self.stagnation_threshold]) < 1e-5:\n                    self.stagnation_counter += self.pop_size\n                else:\n                    self.stagnation_counter = 0\n\n            self.best_fitness_history.append(self.f_opt)\n            self.adjust_population_size()\n\n            # Adjust Cauchy scale based on success rate\n            if self.iterations > 0:\n                self.success_rate = self.successes / self.iterations\n                if self.success_rate > 0.2:\n                    self.cauchy_scale *= 1.1  # Increase scale if successful\n                else:\n                    self.cauchy_scale *= 0.9  # Decrease scale if not successful\n                self.successes = 0\n                self.iterations = 0\n\n            self.cauchy_scale = np.clip(self.cauchy_scale, 0.01, 1.0) # Limit Cauchy scale\n\n        # Local Search around best solution\n        for _ in range(min(self.local_search_iterations, self.budget)):\n             if self.budget <= 0:\n                 break\n\n             perturbation = np.random.uniform(-self.search_range/100, self.search_range/100, self.dim)\n             local_x = np.clip(self.x_opt + perturbation, self.lb, self.ub)\n             local_f = func(local_x)\n             self.budget -=1\n\n             if local_f < self.f_opt:\n                  self.f_opt = local_f\n                  self.x_opt = local_x\n\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "The algorithm HybridAdaptiveCauchyOptimizer scored 0.570 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["14141113-7977-460c-9182-d881a514090d", "2490d4be-f4cf-4c94-836d-98a90f0777d2"], "operator": "crossover", "metadata": {"llm_response": "# Description: A hybrid Cauchy optimizer that adapts mutation strategy based on diversity and stagnation, shrinks search space, and adjusts population size dynamically.\n# Code: \n```python\nimport numpy as np\n\nclass HybridAdaptiveCauchyOptimizer:\n    def __init__(self, budget=10000, dim=10, initial_pop_size=20, shrink_factor=0.95, stagnation_threshold=1000, cauchy_scale=0.1, local_search_iterations=5):\n        self.budget = budget\n        self.dim = dim\n        self.initial_pop_size = initial_pop_size\n        self.pop_size = initial_pop_size\n        self.shrink_factor = shrink_factor\n        self.stagnation_threshold = stagnation_threshold\n        self.cauchy_scale = cauchy_scale\n        self.lb = -5.0\n        self.ub = 5.0\n        self.mutation_prob_gaussian = 0.5\n        self.mutation_prob_cauchy = 0.5\n        self.stagnation_counter = 0\n        self.best_fitness_history = []\n        self.search_range = self.ub - self.lb\n        self.mutation_scale = 1.0 # Scale for Gaussian mutation\n        self.local_search_iterations = local_search_iterations\n        self.success_rate = 0.5\n        self.iterations = 0\n        self.successes = 0\n\n\n    def initialize_population(self):\n        return np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n\n    def calculate_diversity(self, population):\n        centroid = np.mean(population, axis=0)\n        distances = np.linalg.norm(population - centroid, axis=1)\n        return np.mean(distances)\n\n    def mutate_gaussian(self, x):\n        return x + self.mutation_scale * np.random.normal(0, 1, size=self.dim)\n\n    def mutate_cauchy(self, x):\n         return x + self.search_range * np.random.standard_cauchy(size=self.dim) * self.cauchy_scale\n\n    def correct_bounds(self, x):\n        return np.clip(x, self.lb, self.ub)\n\n    def adjust_population_size(self):\n        if self.stagnation_counter > self.stagnation_threshold:\n            self.pop_size = int(self.pop_size * 0.9)  # Reduce population size if stagnating\n            if self.pop_size < 10:\n                self.pop_size = 10 # Minimum population size\n            self.stagnation_counter = 0 # Reset stagnation counter\n            self.mutation_scale = 1.0\n            self.cauchy_scale = 0.1\n            print(f\"Stagnation detected, reducing pop size to {self.pop_size}\")\n\n\n    def __call__(self, func):\n        population = self.initialize_population()\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n        \n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index]\n        self.best_fitness_history.append(self.f_opt)\n\n        center = self.x_opt\n\n\n        while self.budget > 0:\n            diversity = self.calculate_diversity(population)\n\n            if diversity > 1.0:\n                self.mutation_prob_gaussian = 0.8\n                self.mutation_prob_cauchy = 0.2\n            else:\n                self.mutation_prob_gaussian = 0.2\n                self.mutation_prob_cauchy = 0.8\n\n            new_population = np.copy(population)\n            new_fitness = np.copy(fitness)\n\n            for i in range(self.pop_size):\n                if self.budget <= 0:\n                    break\n\n                if np.random.rand() < self.mutation_prob_gaussian:\n                    mutated_individual = self.mutate_gaussian(population[i])\n                else:\n                    mutated_individual = self.mutate_cauchy(population[i])\n\n                mutated_individual = self.correct_bounds(mutated_individual)\n\n                f_mutated = func(mutated_individual)\n                self.budget -= 1\n                self.iterations +=1\n\n                if f_mutated < new_fitness[i]:\n                  new_fitness[i] = f_mutated\n                  new_population[i] = mutated_individual\n                  self.successes +=1\n\n                  if f_mutated < self.f_opt:\n                    self.f_opt = f_mutated\n                    self.x_opt = mutated_individual\n                    center = self.x_opt\n\n\n            # Replace the current population with the new one\n            population = new_population\n            fitness = new_fitness\n\n\n            # Shrink the search range around the best individual\n            self.search_range *= self.shrink_factor\n            self.lb = np.clip(center - self.search_range/2, -5.0, 5.0)\n            self.ub = np.clip(center + self.search_range/2, -5.0, 5.0)\n\n            # Adapt mutation scale\n            self.mutation_scale = 0.95 * self.mutation_scale\n\n            # Stagnation detection and population size adjustment\n            if len(self.best_fitness_history) > self.stagnation_threshold:\n                if abs(self.f_opt - self.best_fitness_history[-self.stagnation_threshold]) < 1e-5:\n                    self.stagnation_counter += self.pop_size\n                else:\n                    self.stagnation_counter = 0\n\n            self.best_fitness_history.append(self.f_opt)\n            self.adjust_population_size()\n\n            # Adjust Cauchy scale based on success rate\n            if self.iterations > 0:\n                self.success_rate = self.successes / self.iterations\n                if self.success_rate > 0.2:\n                    self.cauchy_scale *= 1.1  # Increase scale if successful\n                else:\n                    self.cauchy_scale *= 0.9  # Decrease scale if not successful\n                self.successes = 0\n                self.iterations = 0\n\n            self.cauchy_scale = np.clip(self.cauchy_scale, 0.01, 1.0) # Limit Cauchy scale\n\n        # Local Search around best solution\n        for _ in range(min(self.local_search_iterations, self.budget)):\n             if self.budget <= 0:\n                 break\n\n             perturbation = np.random.uniform(-self.search_range/100, self.search_range/100, self.dim)\n             local_x = np.clip(self.x_opt + perturbation, self.lb, self.ub)\n             local_f = func(local_x)\n             self.budget -=1\n\n             if local_f < self.f_opt:\n                  self.f_opt = local_f\n                  self.x_opt = local_x\n\n\n\n        return self.f_opt, self.x_opt\n```", "aucs": [0.21434887277007153, 0.2137553515742252, 0.7648591181832527, 0.8798996776765675, 0.7655974288764805, 0.7948241973499504, 0.24567694960475106, 0.45675869744188347, 0.7667285035103574, 0.17839546278499807, 0.8862799278266067, 0.9925956224529331, 0.2617305704891929, 0.7773065032310731, 0.7177138314940231, 0.3214532431818502, 0.5847812505317385, 0.846946830367963, 0.219255169436398, 0.5072445401094441], "operator": "crossover", "theta_sampled": 0.009123658414707796, "reward": 0.0, "generation": 6}, "task_prompt": ""}
{"id": "d2cd291f-54d2-47bf-8505-070d6e28ee1a", "fitness": 0.5180800041807725, "name": "AdaptiveCauchyOptimizerVelocity", "description": "Adaptive Cauchy Optimizer with velocity-based exploration, dynamic population size adjustment and learning rate annealing for Cauchy mutation.", "code": "import numpy as np\n\nclass AdaptiveCauchyOptimizerVelocity:\n    def __init__(self, budget=10000, dim=10, initial_pop_size=20, shrink_factor=0.95, stagnation_threshold=1000, cauchy_scale=0.1, velocity_decay=0.9, lr_cauchy=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.initial_pop_size = initial_pop_size\n        self.pop_size = initial_pop_size\n        self.shrink_factor = shrink_factor\n        self.stagnation_threshold = stagnation_threshold\n        self.cauchy_scale = cauchy_scale\n        self.lb = -5.0\n        self.ub = 5.0\n        self.mutation_prob_gaussian = 0.5\n        self.mutation_prob_cauchy = 0.5\n        self.stagnation_counter = 0\n        self.best_fitness_history = []\n        self.search_range = self.ub - self.lb\n        self.mutation_scale = 1.0 # Scale for Gaussian mutation\n        self.velocity = np.zeros((self.initial_pop_size, self.dim))\n        self.velocity_decay = velocity_decay\n        self.lr_cauchy = lr_cauchy # Learning rate for Cauchy mutation\n        self.lr_cauchy_decay = 0.99\n\n\n    def initialize_population(self):\n        return np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n\n    def calculate_diversity(self, population):\n        centroid = np.mean(population, axis=0)\n        distances = np.linalg.norm(population - centroid, axis=1)\n        return np.mean(distances)\n\n    def mutate_gaussian(self, x):\n        return x + self.mutation_scale * np.random.normal(0, 1, size=self.dim)\n\n    def mutate_cauchy(self, x):\n         return x + self.lr_cauchy * self.search_range * np.random.standard_cauchy(size=self.dim)\n\n    def correct_bounds(self, x):\n        return np.clip(x, self.lb, self.ub)\n\n    def adjust_population_size(self):\n        if self.stagnation_counter > self.stagnation_threshold:\n            self.pop_size = int(self.pop_size * 0.9)  # Reduce population size if stagnating\n            if self.pop_size < 10:\n                self.pop_size = 10 # Minimum population size\n            self.stagnation_counter = 0 # Reset stagnation counter\n            self.mutation_scale = 1.0\n            self.velocity = np.zeros((self.pop_size, self.dim)) # Reinitialize velocity\n            print(f\"Stagnation detected, reducing pop size to {self.pop_size}\")\n        elif len(self.best_fitness_history) > self.stagnation_threshold/2:\n            if abs(self.f_opt - self.best_fitness_history[-int(self.stagnation_threshold/2)]) < 1e-4 and self.pop_size < self.initial_pop_size * 2:\n                self.pop_size = int(self.pop_size * 1.1)\n                if self.pop_size > self.initial_pop_size * 2:\n                    self.pop_size = int(self.initial_pop_size * 2)\n                self.velocity = np.concatenate((self.velocity, np.zeros((self.pop_size - self.velocity.shape[0], self.dim))), axis=0)\n                print(f\"Improvement detected, increasing pop size to {self.pop_size}\")\n\n\n    def __call__(self, func):\n        population = self.initialize_population()\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n        \n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index]\n        self.best_fitness_history.append(self.f_opt)\n\n        center = self.x_opt\n\n\n        while self.budget > 0:\n            diversity = self.calculate_diversity(population)\n\n            if diversity > 1.0:\n                self.mutation_prob_gaussian = 0.8\n                self.mutation_prob_cauchy = 0.2\n            else:\n                self.mutation_prob_gaussian = 0.2\n                self.mutation_prob_cauchy = 0.8\n\n            new_population = np.copy(population)\n            new_fitness = np.copy(fitness)\n\n            for i in range(self.pop_size):\n                # Velocity update\n                self.velocity[i] = self.velocity_decay * self.velocity[i] + np.random.uniform(-0.1, 0.1, size=self.dim)\n\n                if np.random.rand() < self.mutation_prob_gaussian:\n                    mutated_individual = self.mutate_gaussian(population[i] + self.velocity[i]) # Incorporate velocity\n                else:\n                    mutated_individual = self.mutate_cauchy(population[i] + self.velocity[i]) # Incorporate velocity\n\n                mutated_individual = self.correct_bounds(mutated_individual)\n\n                f_mutated = func(mutated_individual)\n                self.budget -= 1\n\n                if f_mutated < new_fitness[i]:\n                  new_fitness[i] = f_mutated\n                  new_population[i] = mutated_individual\n                  if f_mutated < self.f_opt:\n                    self.f_opt = f_mutated\n                    self.x_opt = mutated_individual\n                    center = self.x_opt\n\n\n            # Replace the current population with the new one\n            population = new_population\n            fitness = new_fitness\n\n\n            # Shrink the search range around the best individual\n            self.search_range *= self.shrink_factor\n            self.lb = np.clip(center - self.search_range/2, -5.0, 5.0)\n            self.ub = np.clip(center + self.search_range/2, -5.0, 5.0)\n\n            # Adapt mutation scale\n            self.mutation_scale = 0.95 * self.mutation_scale\n\n            # Stagnation detection and population size adjustment\n            if len(self.best_fitness_history) > self.stagnation_threshold:\n                if abs(self.f_opt - self.best_fitness_history[-self.stagnation_threshold]) < 1e-5:\n                    self.stagnation_counter += self.pop_size\n                else:\n                    self.stagnation_counter = 0\n\n            self.best_fitness_history.append(self.f_opt)\n            self.adjust_population_size()\n            self.lr_cauchy *= self.lr_cauchy_decay\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "The algorithm AdaptiveCauchyOptimizerVelocity scored 0.518 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["2490d4be-f4cf-4c94-836d-98a90f0777d2"], "operator": "mutation", "metadata": {"llm_response": "# Description: Adaptive Cauchy Optimizer with velocity-based exploration, dynamic population size adjustment and learning rate annealing for Cauchy mutation.\n# Code: \n```python\nimport numpy as np\n\nclass AdaptiveCauchyOptimizerVelocity:\n    def __init__(self, budget=10000, dim=10, initial_pop_size=20, shrink_factor=0.95, stagnation_threshold=1000, cauchy_scale=0.1, velocity_decay=0.9, lr_cauchy=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.initial_pop_size = initial_pop_size\n        self.pop_size = initial_pop_size\n        self.shrink_factor = shrink_factor\n        self.stagnation_threshold = stagnation_threshold\n        self.cauchy_scale = cauchy_scale\n        self.lb = -5.0\n        self.ub = 5.0\n        self.mutation_prob_gaussian = 0.5\n        self.mutation_prob_cauchy = 0.5\n        self.stagnation_counter = 0\n        self.best_fitness_history = []\n        self.search_range = self.ub - self.lb\n        self.mutation_scale = 1.0 # Scale for Gaussian mutation\n        self.velocity = np.zeros((self.initial_pop_size, self.dim))\n        self.velocity_decay = velocity_decay\n        self.lr_cauchy = lr_cauchy # Learning rate for Cauchy mutation\n        self.lr_cauchy_decay = 0.99\n\n\n    def initialize_population(self):\n        return np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n\n    def calculate_diversity(self, population):\n        centroid = np.mean(population, axis=0)\n        distances = np.linalg.norm(population - centroid, axis=1)\n        return np.mean(distances)\n\n    def mutate_gaussian(self, x):\n        return x + self.mutation_scale * np.random.normal(0, 1, size=self.dim)\n\n    def mutate_cauchy(self, x):\n         return x + self.lr_cauchy * self.search_range * np.random.standard_cauchy(size=self.dim)\n\n    def correct_bounds(self, x):\n        return np.clip(x, self.lb, self.ub)\n\n    def adjust_population_size(self):\n        if self.stagnation_counter > self.stagnation_threshold:\n            self.pop_size = int(self.pop_size * 0.9)  # Reduce population size if stagnating\n            if self.pop_size < 10:\n                self.pop_size = 10 # Minimum population size\n            self.stagnation_counter = 0 # Reset stagnation counter\n            self.mutation_scale = 1.0\n            self.velocity = np.zeros((self.pop_size, self.dim)) # Reinitialize velocity\n            print(f\"Stagnation detected, reducing pop size to {self.pop_size}\")\n        elif len(self.best_fitness_history) > self.stagnation_threshold/2:\n            if abs(self.f_opt - self.best_fitness_history[-int(self.stagnation_threshold/2)]) < 1e-4 and self.pop_size < self.initial_pop_size * 2:\n                self.pop_size = int(self.pop_size * 1.1)\n                if self.pop_size > self.initial_pop_size * 2:\n                    self.pop_size = int(self.initial_pop_size * 2)\n                self.velocity = np.concatenate((self.velocity, np.zeros((self.pop_size - self.velocity.shape[0], self.dim))), axis=0)\n                print(f\"Improvement detected, increasing pop size to {self.pop_size}\")\n\n\n    def __call__(self, func):\n        population = self.initialize_population()\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n        \n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index]\n        self.best_fitness_history.append(self.f_opt)\n\n        center = self.x_opt\n\n\n        while self.budget > 0:\n            diversity = self.calculate_diversity(population)\n\n            if diversity > 1.0:\n                self.mutation_prob_gaussian = 0.8\n                self.mutation_prob_cauchy = 0.2\n            else:\n                self.mutation_prob_gaussian = 0.2\n                self.mutation_prob_cauchy = 0.8\n\n            new_population = np.copy(population)\n            new_fitness = np.copy(fitness)\n\n            for i in range(self.pop_size):\n                # Velocity update\n                self.velocity[i] = self.velocity_decay * self.velocity[i] + np.random.uniform(-0.1, 0.1, size=self.dim)\n\n                if np.random.rand() < self.mutation_prob_gaussian:\n                    mutated_individual = self.mutate_gaussian(population[i] + self.velocity[i]) # Incorporate velocity\n                else:\n                    mutated_individual = self.mutate_cauchy(population[i] + self.velocity[i]) # Incorporate velocity\n\n                mutated_individual = self.correct_bounds(mutated_individual)\n\n                f_mutated = func(mutated_individual)\n                self.budget -= 1\n\n                if f_mutated < new_fitness[i]:\n                  new_fitness[i] = f_mutated\n                  new_population[i] = mutated_individual\n                  if f_mutated < self.f_opt:\n                    self.f_opt = f_mutated\n                    self.x_opt = mutated_individual\n                    center = self.x_opt\n\n\n            # Replace the current population with the new one\n            population = new_population\n            fitness = new_fitness\n\n\n            # Shrink the search range around the best individual\n            self.search_range *= self.shrink_factor\n            self.lb = np.clip(center - self.search_range/2, -5.0, 5.0)\n            self.ub = np.clip(center + self.search_range/2, -5.0, 5.0)\n\n            # Adapt mutation scale\n            self.mutation_scale = 0.95 * self.mutation_scale\n\n            # Stagnation detection and population size adjustment\n            if len(self.best_fitness_history) > self.stagnation_threshold:\n                if abs(self.f_opt - self.best_fitness_history[-self.stagnation_threshold]) < 1e-5:\n                    self.stagnation_counter += self.pop_size\n                else:\n                    self.stagnation_counter = 0\n\n            self.best_fitness_history.append(self.f_opt)\n            self.adjust_population_size()\n            self.lr_cauchy *= self.lr_cauchy_decay\n\n\n        return self.f_opt, self.x_opt\n```", "aucs": [0.16692658964091422, 0.18575065988846584, 0.7503922637967746, 0.8801824053229972, 0.3540769336348071, 0.7837041530843779, 0.3124793679900679, 0.496122636203709, 0.7519645345137642, 0.19260770423419038, 0.8868256346955012, 0.9961224130250612, 0.2333198837447641, 0.3895731254114949, 0.7224041875038862, 0.3567089276537556, 0.33247755905384346, 0.8385584285365448, 0.22886002895537472, 0.5025426467251538], "operator": "mutation", "theta_sampled": 0.005717192677596294, "reward": 0.0, "generation": 7}, "task_prompt": ""}
{"id": "6ce32006-df0d-4c03-acd8-91120fb82594", "fitness": 0.624217183060662, "name": "AdaptiveCauchyOptimizerV2", "description": "Adaptive Cauchy Optimizer with velocity-based exploration and dynamic parameter adjustments for enhanced search diversification and intensification.", "code": "import numpy as np\n\nclass AdaptiveCauchyOptimizerV2:\n    def __init__(self, budget=10000, dim=10, initial_pop_size=20, shrink_factor=0.95, stagnation_threshold=1000, cauchy_scale=0.1, velocity_factor=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.initial_pop_size = initial_pop_size\n        self.pop_size = initial_pop_size\n        self.shrink_factor = shrink_factor\n        self.stagnation_threshold = stagnation_threshold\n        self.cauchy_scale = cauchy_scale\n        self.velocity_factor = velocity_factor\n        self.lb = -5.0\n        self.ub = 5.0\n        self.mutation_prob_gaussian = 0.5\n        self.mutation_prob_cauchy = 0.5\n        self.stagnation_counter = 0\n        self.best_fitness_history = []\n        self.search_range = self.ub - self.lb\n        self.mutation_scale = 1.0 # Scale for Gaussian mutation\n        self.velocities = None\n\n\n    def initialize_population(self):\n        return np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n\n    def calculate_diversity(self, population):\n        centroid = np.mean(population, axis=0)\n        distances = np.linalg.norm(population - centroid, axis=1)\n        return np.mean(distances)\n\n    def mutate_gaussian(self, x):\n        return x + self.mutation_scale * np.random.normal(0, 1, size=self.dim)\n\n    def mutate_cauchy(self, x):\n         return x + self.search_range * np.random.standard_cauchy(size=self.dim) * self.cauchy_scale\n\n    def correct_bounds(self, x):\n        return np.clip(x, self.lb, self.ub)\n\n    def adjust_population_size(self):\n        if self.stagnation_counter > self.stagnation_threshold:\n            self.pop_size = int(self.pop_size * 0.9)  # Reduce population size if stagnating\n            if self.pop_size < 10:\n                self.pop_size = 10 # Minimum population size\n            self.stagnation_counter = 0 # Reset stagnation counter\n            self.mutation_scale = 1.0\n            print(f\"Stagnation detected, reducing pop size to {self.pop_size}\")\n            self.velocities = np.zeros_like(self.velocities[:self.pop_size])  # Re-initialize velocities\n\n\n    def __call__(self, func):\n        population = self.initialize_population()\n        self.velocities = np.zeros_like(population)  # Initialize velocities\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n        \n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index]\n        self.best_fitness_history.append(self.f_opt)\n\n        center = self.x_opt\n\n\n        while self.budget > 0:\n            diversity = self.calculate_diversity(population)\n\n            if diversity > 1.0:\n                self.mutation_prob_gaussian = 0.8\n                self.mutation_prob_cauchy = 0.2\n            else:\n                self.mutation_prob_gaussian = 0.2\n                self.mutation_prob_cauchy = 0.8\n\n            new_population = np.copy(population)\n            new_fitness = np.copy(fitness)\n\n            for i in range(self.pop_size):\n                # Velocity update\n                self.velocities[i] = self.velocity_factor * self.velocities[i] + np.random.uniform(0, 1) * (self.x_opt - population[i])\n                mutated_individual = population[i] + self.velocities[i]\n\n\n                if np.random.rand() < self.mutation_prob_gaussian:\n                    mutated_individual = self.mutate_gaussian(mutated_individual)\n                else:\n                    mutated_individual = self.mutate_cauchy(mutated_individual)\n\n                mutated_individual = self.correct_bounds(mutated_individual)\n\n                f_mutated = func(mutated_individual)\n                self.budget -= 1\n\n                if f_mutated < new_fitness[i]:\n                  new_fitness[i] = f_mutated\n                  new_population[i] = mutated_individual\n                  if f_mutated < self.f_opt:\n                    self.f_opt = f_mutated\n                    self.x_opt = mutated_individual\n                    center = self.x_opt\n\n\n            # Replace the current population with the new one\n            population = new_population\n            fitness = new_fitness\n\n\n            # Shrink the search range around the best individual\n            self.search_range *= self.shrink_factor\n            self.lb = np.clip(center - self.search_range/2, -5.0, 5.0)\n            self.ub = np.clip(center + self.search_range/2, -5.0, 5.0)\n\n            # Adapt mutation scale\n            self.mutation_scale = 0.95 * self.mutation_scale\n\n            # Stagnation detection and population size adjustment\n            if len(self.best_fitness_history) > self.stagnation_threshold:\n                if abs(self.f_opt - self.best_fitness_history[-self.stagnation_threshold]) < 1e-5:\n                    self.stagnation_counter += self.pop_size\n                else:\n                    self.stagnation_counter = 0\n\n            self.best_fitness_history.append(self.f_opt)\n            self.adjust_population_size()\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "The algorithm AdaptiveCauchyOptimizerV2 scored 0.624 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["2490d4be-f4cf-4c94-836d-98a90f0777d2"], "operator": "mutation", "metadata": {"llm_response": "# Description: Adaptive Cauchy Optimizer with velocity-based exploration and dynamic parameter adjustments for enhanced search diversification and intensification.\n# Code:\n```python\nimport numpy as np\n\nclass AdaptiveCauchyOptimizerV2:\n    def __init__(self, budget=10000, dim=10, initial_pop_size=20, shrink_factor=0.95, stagnation_threshold=1000, cauchy_scale=0.1, velocity_factor=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.initial_pop_size = initial_pop_size\n        self.pop_size = initial_pop_size\n        self.shrink_factor = shrink_factor\n        self.stagnation_threshold = stagnation_threshold\n        self.cauchy_scale = cauchy_scale\n        self.velocity_factor = velocity_factor\n        self.lb = -5.0\n        self.ub = 5.0\n        self.mutation_prob_gaussian = 0.5\n        self.mutation_prob_cauchy = 0.5\n        self.stagnation_counter = 0\n        self.best_fitness_history = []\n        self.search_range = self.ub - self.lb\n        self.mutation_scale = 1.0 # Scale for Gaussian mutation\n        self.velocities = None\n\n\n    def initialize_population(self):\n        return np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n\n    def calculate_diversity(self, population):\n        centroid = np.mean(population, axis=0)\n        distances = np.linalg.norm(population - centroid, axis=1)\n        return np.mean(distances)\n\n    def mutate_gaussian(self, x):\n        return x + self.mutation_scale * np.random.normal(0, 1, size=self.dim)\n\n    def mutate_cauchy(self, x):\n         return x + self.search_range * np.random.standard_cauchy(size=self.dim) * self.cauchy_scale\n\n    def correct_bounds(self, x):\n        return np.clip(x, self.lb, self.ub)\n\n    def adjust_population_size(self):\n        if self.stagnation_counter > self.stagnation_threshold:\n            self.pop_size = int(self.pop_size * 0.9)  # Reduce population size if stagnating\n            if self.pop_size < 10:\n                self.pop_size = 10 # Minimum population size\n            self.stagnation_counter = 0 # Reset stagnation counter\n            self.mutation_scale = 1.0\n            print(f\"Stagnation detected, reducing pop size to {self.pop_size}\")\n            self.velocities = np.zeros_like(self.velocities[:self.pop_size])  # Re-initialize velocities\n\n\n    def __call__(self, func):\n        population = self.initialize_population()\n        self.velocities = np.zeros_like(population)  # Initialize velocities\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n        \n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index]\n        self.best_fitness_history.append(self.f_opt)\n\n        center = self.x_opt\n\n\n        while self.budget > 0:\n            diversity = self.calculate_diversity(population)\n\n            if diversity > 1.0:\n                self.mutation_prob_gaussian = 0.8\n                self.mutation_prob_cauchy = 0.2\n            else:\n                self.mutation_prob_gaussian = 0.2\n                self.mutation_prob_cauchy = 0.8\n\n            new_population = np.copy(population)\n            new_fitness = np.copy(fitness)\n\n            for i in range(self.pop_size):\n                # Velocity update\n                self.velocities[i] = self.velocity_factor * self.velocities[i] + np.random.uniform(0, 1) * (self.x_opt - population[i])\n                mutated_individual = population[i] + self.velocities[i]\n\n\n                if np.random.rand() < self.mutation_prob_gaussian:\n                    mutated_individual = self.mutate_gaussian(mutated_individual)\n                else:\n                    mutated_individual = self.mutate_cauchy(mutated_individual)\n\n                mutated_individual = self.correct_bounds(mutated_individual)\n\n                f_mutated = func(mutated_individual)\n                self.budget -= 1\n\n                if f_mutated < new_fitness[i]:\n                  new_fitness[i] = f_mutated\n                  new_population[i] = mutated_individual\n                  if f_mutated < self.f_opt:\n                    self.f_opt = f_mutated\n                    self.x_opt = mutated_individual\n                    center = self.x_opt\n\n\n            # Replace the current population with the new one\n            population = new_population\n            fitness = new_fitness\n\n\n            # Shrink the search range around the best individual\n            self.search_range *= self.shrink_factor\n            self.lb = np.clip(center - self.search_range/2, -5.0, 5.0)\n            self.ub = np.clip(center + self.search_range/2, -5.0, 5.0)\n\n            # Adapt mutation scale\n            self.mutation_scale = 0.95 * self.mutation_scale\n\n            # Stagnation detection and population size adjustment\n            if len(self.best_fitness_history) > self.stagnation_threshold:\n                if abs(self.f_opt - self.best_fitness_history[-self.stagnation_threshold]) < 1e-5:\n                    self.stagnation_counter += self.pop_size\n                else:\n                    self.stagnation_counter = 0\n\n            self.best_fitness_history.append(self.f_opt)\n            self.adjust_population_size()\n\n        return self.f_opt, self.x_opt\n```", "aucs": [0.2269870001304749, 0.833282779641962, 0.7963357054703595, 0.1996493022439486, 0.8057549426221089, 0.83490931557855, 0.2994658395477765, 0.7491955560964497, 0.8039020749471968, 0.2129909601158252, 0.9259272364164353, 0.9998265547156309, 0.2599733559545415, 0.8117883269751076, 0.8046002139040676, 0.6541468138622539, 0.660673168317454, 0.8819977484350687, 0.21481902110811235, 0.5081177451299158], "operator": "mutation", "theta_sampled": 0.010544765727289837, "reward": 0.0, "generation": 7}, "task_prompt": ""}
{"id": "5a5b4ea3-56ab-41f9-a2ca-efb45b241d1b", "fitness": 0.6494911466710953, "name": "HybridBirdSwarmCauchyOptimizer", "description": "A hybrid algorithm combining Bird Swarm Optimization's social learning with Adaptive Shrinking Cauchy Optimizer's adaptive mutation and local search.", "code": "import numpy as np\n\nclass HybridBirdSwarmCauchyOptimizer:\n    def __init__(self, budget=10000, dim=10, swarm_size=20, inertia_weight=0.7, cognitive_coeff=1.4, social_coeff=1.4, neighborhood_size=3,\n                 shrink_factor=0.95, initial_cauchy_scale=0.1, local_search_iterations=5, cauchy_probability = 0.2):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = swarm_size\n        self.inertia_weight = inertia_weight\n        self.cognitive_coeff = cognitive_coeff\n        self.social_coeff = social_coeff\n        self.neighborhood_size = neighborhood_size\n        self.shrink_factor = shrink_factor\n        self.cauchy_scale = initial_cauchy_scale\n        self.local_search_iterations = local_search_iterations\n        self.cauchy_probability = cauchy_probability\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        \n        # Initialize swarm positions and velocities\n        swarm_positions = np.random.uniform(lb, ub, size=(self.swarm_size, self.dim))\n        swarm_velocities = np.random.uniform(-abs(ub-lb)/10, abs(ub-lb)/10, size=(self.swarm_size, self.dim))  #Initialize with small velocities\n        swarm_fitness = np.array([func(x) for x in swarm_positions])\n        self.budget -= self.swarm_size #Update the remaining budget\n        \n        personal_best_positions = swarm_positions.copy()\n        personal_best_fitness = swarm_fitness.copy()\n        \n        global_best_index = np.argmin(personal_best_fitness)\n        global_best_position = personal_best_positions[global_best_index].copy()\n        global_best_fitness = personal_best_fitness[global_best_index]\n\n        search_range = ub - lb #Added Search Range\n\n        successes = 0 #Added Success Rate\n        iterations = 0 #Added Success Rate\n        success_rate = 0.5 #Added Success Rate\n        \n        while self.budget > 0:\n            for i in range(self.swarm_size):\n                # Decide whether to use BSO or Cauchy mutation\n                if np.random.rand() < self.cauchy_probability:\n                    # Adaptive Cauchy Mutation\n                    mutation = global_best_position + search_range * np.random.standard_cauchy(size=self.dim) * self.cauchy_scale\n                    mutation = np.clip(mutation, lb, ub)\n                    \n                    new_fitness = func(mutation)\n                    self.budget -= 1\n                    iterations += 1\n\n                    if new_fitness < personal_best_fitness[i]:\n                        personal_best_fitness[i] = new_fitness\n                        personal_best_positions[i] = mutation.copy()\n                        successes += 1\n                        \n                        if new_fitness < global_best_fitness:\n                            global_best_fitness = new_fitness\n                            global_best_position = mutation.copy()\n                    swarm_positions[i] = mutation #Update swarm position\n                \n                else:\n                    # Bird Swarm Optimization Update\n                    # Update velocity\n                    inertia = self.inertia_weight * swarm_velocities[i]\n                    cognitive = self.cognitive_coeff * np.random.rand(self.dim) * (personal_best_positions[i] - swarm_positions[i])\n                    \n                    # Social component: find best neighbor\n                    neighbors_indices = np.random.choice(self.swarm_size, size=self.neighborhood_size, replace=False)  # Select random neighbors\n                    neighbors_fitness = personal_best_fitness[neighbors_indices]\n                    best_neighbor_index = neighbors_indices[np.argmin(neighbors_fitness)]\n                    social = self.social_coeff * np.random.rand(self.dim) * (personal_best_positions[best_neighbor_index] - swarm_positions[i])\n                    \n                    swarm_velocities[i] = inertia + cognitive + social\n                    \n                    # Update position\n                    new_position = swarm_positions[i] + swarm_velocities[i]\n                    \n                    # Boundary handling (clip the solution, if out of bounds)\n                    new_position = np.clip(new_position, lb, ub)\n                    \n                    new_fitness = func(new_position)\n                    self.budget -= 1\n                    \n                    if new_fitness < personal_best_fitness[i]:\n                        personal_best_fitness[i] = new_fitness\n                        personal_best_positions[i] = new_position.copy()\n                        \n                        if new_fitness < global_best_fitness:\n                            global_best_fitness = new_fitness\n                            global_best_position = new_position.copy()\n                    \n                    swarm_positions[i] = new_position\n            # Shrink the search range around the best individual\n            search_range *= self.shrink_factor\n           # lb = np.clip(global_best_position - search_range/2, func.bounds.lb, func.bounds.ub) #Shrink lb\n           # ub = np.clip(global_best_position + search_range/2, func.bounds.lb, func.bounds.ub) #Shrink ub\n\n            # Adjust Cauchy scale based on success rate\n            if iterations > 0:\n                success_rate = successes / iterations\n                if success_rate > 0.2:\n                    self.cauchy_scale *= 1.1  # Increase scale if successful\n                else:\n                    self.cauchy_scale *= 0.9  # Decrease scale if not successful\n                successes = 0\n                iterations = 0\n\n            self.cauchy_scale = np.clip(self.cauchy_scale, 0.01, 1.0) # Limit Cauchy scale\n        \n        # Local Search around best solution\n        for _ in range(min(self.local_search_iterations, self.budget)):\n             if self.budget <= 0:\n                 break\n\n             perturbation = np.random.uniform(-search_range/100, search_range/100, self.dim)\n             local_x = np.clip(global_best_position + perturbation, lb, ub)\n             local_f = func(local_x)\n             self.budget -=1\n\n             if local_f < global_best_fitness:\n                  global_best_fitness = local_f\n                  global_best_position = local_x\n\n\n        return global_best_fitness, global_best_position", "configspace": "", "generation": 7, "feedback": "The algorithm HybridBirdSwarmCauchyOptimizer scored 0.649 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["ecc70942-b0a7-4f55-9c5c-b847d4827960", "14141113-7977-460c-9182-d881a514090d"], "operator": "crossover", "metadata": {"llm_response": "# Description: A hybrid algorithm combining Bird Swarm Optimization's social learning with Adaptive Shrinking Cauchy Optimizer's adaptive mutation and local search.\n# Code: \n```python\nimport numpy as np\n\nclass HybridBirdSwarmCauchyOptimizer:\n    def __init__(self, budget=10000, dim=10, swarm_size=20, inertia_weight=0.7, cognitive_coeff=1.4, social_coeff=1.4, neighborhood_size=3,\n                 shrink_factor=0.95, initial_cauchy_scale=0.1, local_search_iterations=5, cauchy_probability = 0.2):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = swarm_size\n        self.inertia_weight = inertia_weight\n        self.cognitive_coeff = cognitive_coeff\n        self.social_coeff = social_coeff\n        self.neighborhood_size = neighborhood_size\n        self.shrink_factor = shrink_factor\n        self.cauchy_scale = initial_cauchy_scale\n        self.local_search_iterations = local_search_iterations\n        self.cauchy_probability = cauchy_probability\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        \n        # Initialize swarm positions and velocities\n        swarm_positions = np.random.uniform(lb, ub, size=(self.swarm_size, self.dim))\n        swarm_velocities = np.random.uniform(-abs(ub-lb)/10, abs(ub-lb)/10, size=(self.swarm_size, self.dim))  #Initialize with small velocities\n        swarm_fitness = np.array([func(x) for x in swarm_positions])\n        self.budget -= self.swarm_size #Update the remaining budget\n        \n        personal_best_positions = swarm_positions.copy()\n        personal_best_fitness = swarm_fitness.copy()\n        \n        global_best_index = np.argmin(personal_best_fitness)\n        global_best_position = personal_best_positions[global_best_index].copy()\n        global_best_fitness = personal_best_fitness[global_best_index]\n\n        search_range = ub - lb #Added Search Range\n\n        successes = 0 #Added Success Rate\n        iterations = 0 #Added Success Rate\n        success_rate = 0.5 #Added Success Rate\n        \n        while self.budget > 0:\n            for i in range(self.swarm_size):\n                # Decide whether to use BSO or Cauchy mutation\n                if np.random.rand() < self.cauchy_probability:\n                    # Adaptive Cauchy Mutation\n                    mutation = global_best_position + search_range * np.random.standard_cauchy(size=self.dim) * self.cauchy_scale\n                    mutation = np.clip(mutation, lb, ub)\n                    \n                    new_fitness = func(mutation)\n                    self.budget -= 1\n                    iterations += 1\n\n                    if new_fitness < personal_best_fitness[i]:\n                        personal_best_fitness[i] = new_fitness\n                        personal_best_positions[i] = mutation.copy()\n                        successes += 1\n                        \n                        if new_fitness < global_best_fitness:\n                            global_best_fitness = new_fitness\n                            global_best_position = mutation.copy()\n                    swarm_positions[i] = mutation #Update swarm position\n                \n                else:\n                    # Bird Swarm Optimization Update\n                    # Update velocity\n                    inertia = self.inertia_weight * swarm_velocities[i]\n                    cognitive = self.cognitive_coeff * np.random.rand(self.dim) * (personal_best_positions[i] - swarm_positions[i])\n                    \n                    # Social component: find best neighbor\n                    neighbors_indices = np.random.choice(self.swarm_size, size=self.neighborhood_size, replace=False)  # Select random neighbors\n                    neighbors_fitness = personal_best_fitness[neighbors_indices]\n                    best_neighbor_index = neighbors_indices[np.argmin(neighbors_fitness)]\n                    social = self.social_coeff * np.random.rand(self.dim) * (personal_best_positions[best_neighbor_index] - swarm_positions[i])\n                    \n                    swarm_velocities[i] = inertia + cognitive + social\n                    \n                    # Update position\n                    new_position = swarm_positions[i] + swarm_velocities[i]\n                    \n                    # Boundary handling (clip the solution, if out of bounds)\n                    new_position = np.clip(new_position, lb, ub)\n                    \n                    new_fitness = func(new_position)\n                    self.budget -= 1\n                    \n                    if new_fitness < personal_best_fitness[i]:\n                        personal_best_fitness[i] = new_fitness\n                        personal_best_positions[i] = new_position.copy()\n                        \n                        if new_fitness < global_best_fitness:\n                            global_best_fitness = new_fitness\n                            global_best_position = new_position.copy()\n                    \n                    swarm_positions[i] = new_position\n            # Shrink the search range around the best individual\n            search_range *= self.shrink_factor\n           # lb = np.clip(global_best_position - search_range/2, func.bounds.lb, func.bounds.ub) #Shrink lb\n           # ub = np.clip(global_best_position + search_range/2, func.bounds.lb, func.bounds.ub) #Shrink ub\n\n            # Adjust Cauchy scale based on success rate\n            if iterations > 0:\n                success_rate = successes / iterations\n                if success_rate > 0.2:\n                    self.cauchy_scale *= 1.1  # Increase scale if successful\n                else:\n                    self.cauchy_scale *= 0.9  # Decrease scale if not successful\n                successes = 0\n                iterations = 0\n\n            self.cauchy_scale = np.clip(self.cauchy_scale, 0.01, 1.0) # Limit Cauchy scale\n        \n        # Local Search around best solution\n        for _ in range(min(self.local_search_iterations, self.budget)):\n             if self.budget <= 0:\n                 break\n\n             perturbation = np.random.uniform(-search_range/100, search_range/100, self.dim)\n             local_x = np.clip(global_best_position + perturbation, lb, ub)\n             local_f = func(local_x)\n             self.budget -=1\n\n             if local_f < global_best_fitness:\n                  global_best_fitness = local_f\n                  global_best_position = local_x\n\n\n        return global_best_fitness, global_best_position\n```", "aucs": [0.216798655564298, 0.7709292837497989, 0.7384448447077837, 0.8716251876312378, 0.7738080502128736, 0.7993143873623803, 0.31217794009764455, 0.6888417695524459, 0.738620027279116, 0.20850969949541054, 0.9050823415275936, 0.9975979557738299, 0.2413598392510582, 0.7689113737231484, 0.8994906815627456, 0.7904639750223359, 0.6570651366606237, 0.8499939897081703, 0.23946340337411343, 0.5213243911652968], "operator": "crossover", "theta_sampled": 0.0019374296711667815, "reward": 0.022970332448672415, "generation": 7}, "task_prompt": ""}
{"id": "fb0dcad7-cdec-4109-8703-278f0ae547d4", "fitness": 0.0, "name": "HybridBirdSwarmCauchy", "description": "Combines Bird Swarm's social learning with Adaptive Cauchy's shrinking search space, dynamically adjusting parameters based on performance.", "code": "import numpy as np\n\nclass HybridBirdSwarmCauchy:\n    def __init__(self, budget=10000, dim=10, swarm_size=20, inertia_weight=0.7, cognitive_coeff=1.4, social_coeff=1.4, neighborhood_size=3, shrink_factor=0.95, initial_cauchy_scale=0.1, local_search_iterations=5):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = swarm_size\n        self.inertia_weight = inertia_weight\n        self.cognitive_coeff = cognitive_coeff\n        self.social_coeff = social_coeff\n        self.neighborhood_size = neighborhood_size\n        self.shrink_factor = shrink_factor\n        self.cauchy_scale = initial_cauchy_scale\n        self.success_rate = 0.5  # Initialize success rate\n        self.local_search_iterations = local_search_iterations\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        \n        # Initialize swarm positions and velocities\n        swarm_positions = np.random.uniform(lb, ub, size=(self.swarm_size, self.dim))\n        swarm_velocities = np.random.uniform(-abs(ub-lb)/10, abs(ub-lb)/10, size=(self.swarm_size, self.dim))  #Initialize with small velocities\n        swarm_fitness = np.array([func(x) for x in swarm_positions])\n        self.budget -= self.swarm_size #Update the remaining budget\n        \n        personal_best_positions = swarm_positions.copy()\n        personal_best_fitness = swarm_fitness.copy()\n        \n        global_best_index = np.argmin(personal_best_fitness)\n        global_best_position = personal_best_positions[global_best_index].copy()\n        global_best_fitness = personal_best_fitness[global_best_index]\n        \n        search_range = ub - lb\n        center = global_best_position\n\n        successes = 0\n        iterations = 0\n\n        while self.budget > 0:\n            for i in range(self.swarm_size):\n                # Update velocity (Bird Swarm part)\n                inertia = self.inertia_weight * swarm_velocities[i]\n                cognitive = self.cognitive_coeff * np.random.rand(self.dim) * (personal_best_positions[i] - swarm_positions[i])\n                \n                # Social component: find best neighbor\n                neighbors_indices = np.random.choice(self.swarm_size, size=self.neighborhood_size, replace=False)  # Select random neighbors\n                neighbors_fitness = personal_best_fitness[neighbors_indices]\n                best_neighbor_index = neighbors_indices[np.argmin(neighbors_fitness)]\n                social = self.social_coeff * np.random.rand(self.dim) * (personal_best_positions[best_neighbor_index] - swarm_positions[i])\n                \n                swarm_velocities[i] = inertia + cognitive + social\n                \n                # Update position\n                new_position = swarm_positions[i] + swarm_velocities[i]\n\n                # Cauchy mutation around the best individual (Cauchy Part integrated with Bird Swarm)\n                mutation = center + search_range * np.random.standard_cauchy(size=self.dim) * self.cauchy_scale  # Scale Cauchy distribution\n                mutation = np.clip(mutation, lb, ub)  # Clip values to stay within bounds\n\n                # Boundary handling (clip the solution, if out of bounds)\n                new_position = np.clip(new_position, lb, ub)\n                \n                new_fitness = func(new_position)\n                f_mutation = func(mutation)\n\n                self.budget -= 2 # Update budget for function evaluations\n                iterations += 1\n                \n                if f_mutation < new_fitness:\n                    new_fitness = f_mutation\n                    new_position = mutation\n\n                if new_fitness < personal_best_fitness[i]:\n                    personal_best_fitness[i] = new_fitness\n                    personal_best_positions[i] = new_position.copy()\n                    successes += 1\n                    \n                    if new_fitness < global_best_fitness:\n                        global_best_fitness = new_fitness\n                        global_best_position = new_position.copy()\n                        center = global_best_position\n                \n                swarm_positions[i] = new_position\n            \n             # Shrink the search range around the best individual\n            search_range *= self.shrink_factor\n            lb = np.clip(center - search_range/2, func.bounds.lb, func.bounds.ub)\n            ub = np.clip(center + search_range/2, func.bounds.lb, func.bounds.ub)\n\n            # Adjust Cauchy scale based on success rate\n            if iterations > 0:\n                self.success_rate = successes / iterations\n                if self.success_rate > 0.2:\n                    self.cauchy_scale *= 1.1  # Increase scale if successful\n                else:\n                    self.cauchy_scale *= 0.9  # Decrease scale if not successful\n                successes = 0\n                iterations = 0\n\n            self.cauchy_scale = np.clip(self.cauchy_scale, 0.01, 1.0) # Limit Cauchy scale\n        \n        # Local Search around best solution\n        for _ in range(min(self.local_search_iterations, self.budget)):\n             if self.budget <= 0:\n                 break\n\n             perturbation = np.random.uniform(-search_range/100, search_range/100, self.dim)\n             local_x = np.clip(global_best_position + perturbation, lb, ub)\n             local_f = func(local_x)\n             self.budget -=1\n\n             if local_f < global_best_fitness:\n                  global_best_fitness = local_f\n                  global_best_position = local_x\n\n\n        return global_best_fitness, global_best_position", "configspace": "", "generation": 7, "feedback": "The algorithm HybridBirdSwarmCauchy scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["ecc70942-b0a7-4f55-9c5c-b847d4827960", "14141113-7977-460c-9182-d881a514090d"], "operator": "crossover", "metadata": {"llm_response": "# Description: Combines Bird Swarm's social learning with Adaptive Cauchy's shrinking search space, dynamically adjusting parameters based on performance.\n# Code: \n```python\nimport numpy as np\n\nclass HybridBirdSwarmCauchy:\n    def __init__(self, budget=10000, dim=10, swarm_size=20, inertia_weight=0.7, cognitive_coeff=1.4, social_coeff=1.4, neighborhood_size=3, shrink_factor=0.95, initial_cauchy_scale=0.1, local_search_iterations=5):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = swarm_size\n        self.inertia_weight = inertia_weight\n        self.cognitive_coeff = cognitive_coeff\n        self.social_coeff = social_coeff\n        self.neighborhood_size = neighborhood_size\n        self.shrink_factor = shrink_factor\n        self.cauchy_scale = initial_cauchy_scale\n        self.success_rate = 0.5  # Initialize success rate\n        self.local_search_iterations = local_search_iterations\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        \n        # Initialize swarm positions and velocities\n        swarm_positions = np.random.uniform(lb, ub, size=(self.swarm_size, self.dim))\n        swarm_velocities = np.random.uniform(-abs(ub-lb)/10, abs(ub-lb)/10, size=(self.swarm_size, self.dim))  #Initialize with small velocities\n        swarm_fitness = np.array([func(x) for x in swarm_positions])\n        self.budget -= self.swarm_size #Update the remaining budget\n        \n        personal_best_positions = swarm_positions.copy()\n        personal_best_fitness = swarm_fitness.copy()\n        \n        global_best_index = np.argmin(personal_best_fitness)\n        global_best_position = personal_best_positions[global_best_index].copy()\n        global_best_fitness = personal_best_fitness[global_best_index]\n        \n        search_range = ub - lb\n        center = global_best_position\n\n        successes = 0\n        iterations = 0\n\n        while self.budget > 0:\n            for i in range(self.swarm_size):\n                # Update velocity (Bird Swarm part)\n                inertia = self.inertia_weight * swarm_velocities[i]\n                cognitive = self.cognitive_coeff * np.random.rand(self.dim) * (personal_best_positions[i] - swarm_positions[i])\n                \n                # Social component: find best neighbor\n                neighbors_indices = np.random.choice(self.swarm_size, size=self.neighborhood_size, replace=False)  # Select random neighbors\n                neighbors_fitness = personal_best_fitness[neighbors_indices]\n                best_neighbor_index = neighbors_indices[np.argmin(neighbors_fitness)]\n                social = self.social_coeff * np.random.rand(self.dim) * (personal_best_positions[best_neighbor_index] - swarm_positions[i])\n                \n                swarm_velocities[i] = inertia + cognitive + social\n                \n                # Update position\n                new_position = swarm_positions[i] + swarm_velocities[i]\n\n                # Cauchy mutation around the best individual (Cauchy Part integrated with Bird Swarm)\n                mutation = center + search_range * np.random.standard_cauchy(size=self.dim) * self.cauchy_scale  # Scale Cauchy distribution\n                mutation = np.clip(mutation, lb, ub)  # Clip values to stay within bounds\n\n                # Boundary handling (clip the solution, if out of bounds)\n                new_position = np.clip(new_position, lb, ub)\n                \n                new_fitness = func(new_position)\n                f_mutation = func(mutation)\n\n                self.budget -= 2 # Update budget for function evaluations\n                iterations += 1\n                \n                if f_mutation < new_fitness:\n                    new_fitness = f_mutation\n                    new_position = mutation\n\n                if new_fitness < personal_best_fitness[i]:\n                    personal_best_fitness[i] = new_fitness\n                    personal_best_positions[i] = new_position.copy()\n                    successes += 1\n                    \n                    if new_fitness < global_best_fitness:\n                        global_best_fitness = new_fitness\n                        global_best_position = new_position.copy()\n                        center = global_best_position\n                \n                swarm_positions[i] = new_position\n            \n             # Shrink the search range around the best individual\n            search_range *= self.shrink_factor\n            lb = np.clip(center - search_range/2, func.bounds.lb, func.bounds.ub)\n            ub = np.clip(center + search_range/2, func.bounds.lb, func.bounds.ub)\n\n            # Adjust Cauchy scale based on success rate\n            if iterations > 0:\n                self.success_rate = successes / iterations\n                if self.success_rate > 0.2:\n                    self.cauchy_scale *= 1.1  # Increase scale if successful\n                else:\n                    self.cauchy_scale *= 0.9  # Decrease scale if not successful\n                successes = 0\n                iterations = 0\n\n            self.cauchy_scale = np.clip(self.cauchy_scale, 0.01, 1.0) # Limit Cauchy scale\n        \n        # Local Search around best solution\n        for _ in range(min(self.local_search_iterations, self.budget)):\n             if self.budget <= 0:\n                 break\n\n             perturbation = np.random.uniform(-search_range/100, search_range/100, self.dim)\n             local_x = np.clip(global_best_position + perturbation, lb, ub)\n             local_f = func(local_x)\n             self.budget -=1\n\n             if local_f < global_best_fitness:\n                  global_best_fitness = local_f\n                  global_best_position = local_x\n\n\n        return global_best_fitness, global_best_position\n```", "aucs": [0], "operator": "crossover", "theta_sampled": 0.015074585816888895, "reward": 0.0, "generation": 7}, "task_prompt": ""}
{"id": "5253e78a-9a13-4bb8-8b13-8cfcc617f2a2", "fitness": 0.0, "name": "AdaptiveHybridOptimizer", "description": "Adaptive population size and mutation strategy based on success rate and diversity, with a Cauchy-Gaussian hybrid mutation and a local search refinement.", "code": "import numpy as np\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget=10000, dim=10, initial_pop_size=20, shrink_factor=0.95, stagnation_threshold=1000, initial_cauchy_scale=0.1, local_search_iterations=5, exploration_prob = 0.2):\n        self.budget = budget\n        self.dim = dim\n        self.initial_pop_size = initial_pop_size\n        self.pop_size = initial_pop_size\n        self.shrink_factor = shrink_factor\n        self.stagnation_threshold = stagnation_threshold\n        self.cauchy_scale = initial_cauchy_scale\n        self.lb = -5.0\n        self.ub = 5.0\n        self.mutation_prob_gaussian = 0.5\n        self.mutation_prob_cauchy = 0.5\n        self.stagnation_counter = 0\n        self.best_fitness_history = []\n        self.search_range = self.ub - self.lb\n        self.mutation_scale = 1.0 # Scale for Gaussian mutation\n        self.local_search_iterations = local_search_iterations\n        self.success_rate = 0.5  # Initialize success rate\n        self.iterations_since_last_success = 0\n        self.exploration_prob = exploration_prob  # Probability of exploration\n\n    def initialize_population(self):\n        return np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n\n    def calculate_diversity(self, population):\n        centroid = np.mean(population, axis=0)\n        distances = np.linalg.norm(population - centroid, axis=1)\n        return np.mean(distances)\n\n    def mutate_gaussian(self, x):\n        return x + self.mutation_scale * np.random.normal(0, 1, size=self.dim)\n\n    def mutate_cauchy(self, x):\n         return x + self.search_range * np.random.standard_cauchy(size=self.dim) * self.cauchy_scale\n\n    def correct_bounds(self, x):\n        return np.clip(x, self.lb, self.ub)\n\n    def adjust_population_size(self):\n        if self.stagnation_counter > self.stagnation_threshold:\n            self.pop_size = int(self.pop_size * 0.9)  # Reduce population size if stagnating\n            if self.pop_size < 10:\n                self.pop_size = 10 # Minimum population size\n            self.stagnation_counter = 0 # Reset stagnation counter\n            self.mutation_scale = 1.0\n            self.cauchy_scale = self.initial_cauchy_scale\n            print(f\"Stagnation detected, reducing pop size to {self.pop_size}\")\n        elif self.iterations_since_last_success > 5000 and self.pop_size < self.initial_pop_size * 2:  #Increase population if no improvement is seen after long\n              self.pop_size = int(self.pop_size * 1.1)\n              print(f\"No improvement, increasing pop size to {self.pop_size}\")\n\n\n    def __call__(self, func):\n        population = self.initialize_population()\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n        \n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index]\n        self.best_fitness_history.append(self.f_opt)\n\n        center = self.x_opt\n        successes = 0\n        iterations = 0\n\n\n        while self.budget > 0:\n            diversity = self.calculate_diversity(population)\n\n            if diversity > 1.0:\n                self.mutation_prob_gaussian = 0.8\n                self.mutation_prob_cauchy = 0.2\n            else:\n                self.mutation_prob_gaussian = 0.2\n                self.mutation_prob_cauchy = 0.8\n\n            new_population = np.copy(population)\n            new_fitness = np.copy(fitness)\n\n            for i in range(self.pop_size):\n                if self.budget <= 0:\n                    break\n\n                # Exploration phase: random jump with probability exploration_prob\n                if np.random.rand() < self.exploration_prob:\n                    mutated_individual = np.random.uniform(self.lb, self.ub, size=self.dim)\n                elif np.random.rand() < self.mutation_prob_gaussian:\n                    mutated_individual = self.mutate_gaussian(population[i])\n                else:\n                    mutated_individual = self.mutate_cauchy(population[i])\n\n                mutated_individual = self.correct_bounds(mutated_individual)\n\n                f_mutated = func(mutated_individual)\n                self.budget -= 1\n                iterations += 1\n\n                if f_mutated < new_fitness[i]:\n                  new_fitness[i] = f_mutated\n                  new_population[i] = mutated_individual\n                  successes +=1\n                  if f_mutated < self.f_opt:\n                    self.f_opt = f_mutated\n                    self.x_opt = mutated_individual\n                    center = self.x_opt\n                    self.iterations_since_last_success = 0\n                else:\n                    self.iterations_since_last_success +=1\n\n            # Replace the current population with the new one\n            population = new_population\n            fitness = new_fitness\n\n            # Shrink the search range around the best individual\n            self.search_range *= self.shrink_factor\n            self.lb = np.clip(center - self.search_range/2, -5.0, 5.0)\n            self.ub = np.clip(center + self.search_range/2, -5.0, 5.0)\n\n            # Adapt mutation scale\n            self.mutation_scale = 0.95 * self.mutation_scale\n\n             # Adjust Cauchy scale based on success rate\n            if iterations > 0:\n                self.success_rate = successes / iterations\n                if self.success_rate > 0.2:\n                    self.cauchy_scale *= 1.1  # Increase scale if successful\n                else:\n                    self.cauchy_scale *= 0.9  # Decrease scale if not successful\n                successes = 0\n                iterations = 0\n\n            self.cauchy_scale = np.clip(self.cauchy_scale, 0.01, 1.0) # Limit Cauchy scale\n\n            # Stagnation detection and population size adjustment\n            if len(self.best_fitness_history) > self.stagnation_threshold:\n                if abs(self.f_opt - self.best_fitness_history[-self.stagnation_threshold]) < 1e-5:\n                    self.stagnation_counter += self.pop_size\n                else:\n                    self.stagnation_counter = 0\n\n            self.best_fitness_history.append(self.f_opt)\n            self.adjust_population_size()\n\n        # Local Search around best solution\n        search_range_local = self.ub - self.lb\n        for _ in range(min(self.local_search_iterations, self.budget)):\n             if self.budget <= 0:\n                 break\n\n             perturbation = np.random.uniform(-search_range_local/100, search_range_local/100, self.dim)\n             local_x = np.clip(self.x_opt + perturbation, self.lb, self.ub)\n             local_f = func(local_x)\n             self.budget -=1\n\n             if local_f < self.f_opt:\n                  self.f_opt = local_f\n                  self.x_opt = local_x\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "", "error": "Evaluation error: index 20 is out of bounds for axis 0 with size 20", "parent_ids": ["69141d6c-264a-49de-8fb5-c932503d6693"], "operator": "mutation", "metadata": {"llm_response": "# Description: Adaptive population size and mutation strategy based on success rate and diversity, with a Cauchy-Gaussian hybrid mutation and a local search refinement.\n# Code: \n```python\nimport numpy as np\n\nclass AdaptiveHybridOptimizer:\n    def __init__(self, budget=10000, dim=10, initial_pop_size=20, shrink_factor=0.95, stagnation_threshold=1000, initial_cauchy_scale=0.1, local_search_iterations=5, exploration_prob = 0.2):\n        self.budget = budget\n        self.dim = dim\n        self.initial_pop_size = initial_pop_size\n        self.pop_size = initial_pop_size\n        self.shrink_factor = shrink_factor\n        self.stagnation_threshold = stagnation_threshold\n        self.cauchy_scale = initial_cauchy_scale\n        self.lb = -5.0\n        self.ub = 5.0\n        self.mutation_prob_gaussian = 0.5\n        self.mutation_prob_cauchy = 0.5\n        self.stagnation_counter = 0\n        self.best_fitness_history = []\n        self.search_range = self.ub - self.lb\n        self.mutation_scale = 1.0 # Scale for Gaussian mutation\n        self.local_search_iterations = local_search_iterations\n        self.success_rate = 0.5  # Initialize success rate\n        self.iterations_since_last_success = 0\n        self.exploration_prob = exploration_prob  # Probability of exploration\n\n    def initialize_population(self):\n        return np.random.uniform(self.lb, self.ub, size=(self.pop_size, self.dim))\n\n    def calculate_diversity(self, population):\n        centroid = np.mean(population, axis=0)\n        distances = np.linalg.norm(population - centroid, axis=1)\n        return np.mean(distances)\n\n    def mutate_gaussian(self, x):\n        return x + self.mutation_scale * np.random.normal(0, 1, size=self.dim)\n\n    def mutate_cauchy(self, x):\n         return x + self.search_range * np.random.standard_cauchy(size=self.dim) * self.cauchy_scale\n\n    def correct_bounds(self, x):\n        return np.clip(x, self.lb, self.ub)\n\n    def adjust_population_size(self):\n        if self.stagnation_counter > self.stagnation_threshold:\n            self.pop_size = int(self.pop_size * 0.9)  # Reduce population size if stagnating\n            if self.pop_size < 10:\n                self.pop_size = 10 # Minimum population size\n            self.stagnation_counter = 0 # Reset stagnation counter\n            self.mutation_scale = 1.0\n            self.cauchy_scale = self.initial_cauchy_scale\n            print(f\"Stagnation detected, reducing pop size to {self.pop_size}\")\n        elif self.iterations_since_last_success > 5000 and self.pop_size < self.initial_pop_size * 2:  #Increase population if no improvement is seen after long\n              self.pop_size = int(self.pop_size * 1.1)\n              print(f\"No improvement, increasing pop size to {self.pop_size}\")\n\n\n    def __call__(self, func):\n        population = self.initialize_population()\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n        \n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index]\n        self.best_fitness_history.append(self.f_opt)\n\n        center = self.x_opt\n        successes = 0\n        iterations = 0\n\n\n        while self.budget > 0:\n            diversity = self.calculate_diversity(population)\n\n            if diversity > 1.0:\n                self.mutation_prob_gaussian = 0.8\n                self.mutation_prob_cauchy = 0.2\n            else:\n                self.mutation_prob_gaussian = 0.2\n                self.mutation_prob_cauchy = 0.8\n\n            new_population = np.copy(population)\n            new_fitness = np.copy(fitness)\n\n            for i in range(self.pop_size):\n                if self.budget <= 0:\n                    break\n\n                # Exploration phase: random jump with probability exploration_prob\n                if np.random.rand() < self.exploration_prob:\n                    mutated_individual = np.random.uniform(self.lb, self.ub, size=self.dim)\n                elif np.random.rand() < self.mutation_prob_gaussian:\n                    mutated_individual = self.mutate_gaussian(population[i])\n                else:\n                    mutated_individual = self.mutate_cauchy(population[i])\n\n                mutated_individual = self.correct_bounds(mutated_individual)\n\n                f_mutated = func(mutated_individual)\n                self.budget -= 1\n                iterations += 1\n\n                if f_mutated < new_fitness[i]:\n                  new_fitness[i] = f_mutated\n                  new_population[i] = mutated_individual\n                  successes +=1\n                  if f_mutated < self.f_opt:\n                    self.f_opt = f_mutated\n                    self.x_opt = mutated_individual\n                    center = self.x_opt\n                    self.iterations_since_last_success = 0\n                else:\n                    self.iterations_since_last_success +=1\n\n            # Replace the current population with the new one\n            population = new_population\n            fitness = new_fitness\n\n            # Shrink the search range around the best individual\n            self.search_range *= self.shrink_factor\n            self.lb = np.clip(center - self.search_range/2, -5.0, 5.0)\n            self.ub = np.clip(center + self.search_range/2, -5.0, 5.0)\n\n            # Adapt mutation scale\n            self.mutation_scale = 0.95 * self.mutation_scale\n\n             # Adjust Cauchy scale based on success rate\n            if iterations > 0:\n                self.success_rate = successes / iterations\n                if self.success_rate > 0.2:\n                    self.cauchy_scale *= 1.1  # Increase scale if successful\n                else:\n                    self.cauchy_scale *= 0.9  # Decrease scale if not successful\n                successes = 0\n                iterations = 0\n\n            self.cauchy_scale = np.clip(self.cauchy_scale, 0.01, 1.0) # Limit Cauchy scale\n\n            # Stagnation detection and population size adjustment\n            if len(self.best_fitness_history) > self.stagnation_threshold:\n                if abs(self.f_opt - self.best_fitness_history[-self.stagnation_threshold]) < 1e-5:\n                    self.stagnation_counter += self.pop_size\n                else:\n                    self.stagnation_counter = 0\n\n            self.best_fitness_history.append(self.f_opt)\n            self.adjust_population_size()\n\n        # Local Search around best solution\n        search_range_local = self.ub - self.lb\n        for _ in range(min(self.local_search_iterations, self.budget)):\n             if self.budget <= 0:\n                 break\n\n             perturbation = np.random.uniform(-search_range_local/100, search_range_local/100, self.dim)\n             local_x = np.clip(self.x_opt + perturbation, self.lb, self.ub)\n             local_f = func(local_x)\n             self.budget -=1\n\n             if local_f < self.f_opt:\n                  self.f_opt = local_f\n                  self.x_opt = local_x\n\n\n        return self.f_opt, self.x_opt\n```", "operator": "mutation", "theta_sampled": 0.005780389983706077, "reward": 0.0, "generation": 7}, "task_prompt": ""}
{"id": "df8a52a3-838e-4304-b60e-76e5afa6152f", "fitness": 0.6778071678754112, "name": "HybridBirdCauchyOptimizer", "description": "A hybrid algorithm combining bird swarm optimization's social learning and adaptive shrinking Cauchy mutation for enhanced exploration and exploitation.", "code": "import numpy as np\n\nclass HybridBirdCauchyOptimizer:\n    def __init__(self, budget=10000, dim=10, swarm_size=20, inertia_weight=0.7, cognitive_coeff=1.4, social_coeff=1.4, neighborhood_size=3, shrink_factor=0.95, initial_cauchy_scale=0.1, local_search_iterations=5, cauchy_probability=0.2):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = swarm_size\n        self.inertia_weight = inertia_weight\n        self.cognitive_coeff = cognitive_coeff\n        self.social_coeff = social_coeff\n        self.neighborhood_size = neighborhood_size\n        self.shrink_factor = shrink_factor\n        self.cauchy_scale = initial_cauchy_scale\n        self.success_rate = 0.5\n        self.local_search_iterations = local_search_iterations\n        self.cauchy_probability = cauchy_probability  #Probability of applying Cauchy mutation\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        \n        # Initialize swarm positions and velocities\n        swarm_positions = np.random.uniform(lb, ub, size=(self.swarm_size, self.dim))\n        swarm_velocities = np.random.uniform(-abs(ub-lb)/10, abs(ub-lb)/10, size=(self.swarm_size, self.dim))\n        swarm_fitness = np.array([func(x) for x in swarm_positions])\n        self.budget -= self.swarm_size\n        \n        personal_best_positions = swarm_positions.copy()\n        personal_best_fitness = swarm_fitness.copy()\n        \n        global_best_index = np.argmin(personal_best_fitness)\n        global_best_position = personal_best_positions[global_best_index].copy()\n        global_best_fitness = personal_best_fitness[global_best_index]\n        \n        search_range = ub - lb\n        center = global_best_position #Use global best as center for Cauchy\n\n        successes = 0\n        iterations = 0\n        \n        while self.budget > 0:\n            for i in range(self.swarm_size):\n                if self.budget <= 0:\n                    break\n                # Bird Swarm Optimization part\n                inertia = self.inertia_weight * swarm_velocities[i]\n                cognitive = self.cognitive_coeff * np.random.rand(self.dim) * (personal_best_positions[i] - swarm_positions[i])\n                \n                # Social component: find best neighbor\n                neighbors_indices = np.random.choice(self.swarm_size, size=self.neighborhood_size, replace=False)\n                neighbors_fitness = personal_best_fitness[neighbors_indices]\n                best_neighbor_index = neighbors_indices[np.argmin(neighbors_fitness)]\n                social = self.social_coeff * np.random.rand(self.dim) * (personal_best_positions[best_neighbor_index] - swarm_positions[i])\n                \n                swarm_velocities[i] = inertia + cognitive + social\n                \n                # Update position\n                new_position = swarm_positions[i] + swarm_velocities[i]\n                \n                # Cauchy mutation with probability cauchy_probability\n                if np.random.rand() < self.cauchy_probability:\n                    mutation = center + search_range * np.random.standard_cauchy(size=self.dim) * self.cauchy_scale  # Scale Cauchy distribution\n                    new_position = mutation\n                \n                # Boundary handling\n                new_position = np.clip(new_position, lb, ub)\n                \n                new_fitness = func(new_position)\n                self.budget -= 1\n                iterations += 1\n                \n                if new_fitness < personal_best_fitness[i]:\n                    personal_best_fitness[i] = new_fitness\n                    personal_best_positions[i] = new_position.copy()\n                    successes += 1\n                    \n                    if new_fitness < global_best_fitness:\n                        global_best_fitness = new_fitness\n                        global_best_position = new_position.copy()\n                        center = global_best_position #Update center\n\n                swarm_positions[i] = new_position\n                \n            # Shrink search range\n            search_range *= self.shrink_factor\n            lb = np.clip(center - search_range/2, func.bounds.lb, func.bounds.ub)\n            ub = np.clip(center + search_range/2, func.bounds.lb, func.bounds.ub)\n            \n            # Adjust Cauchy scale based on success rate\n            if iterations > 0:\n                self.success_rate = successes / iterations\n                if self.success_rate > 0.2:\n                    self.cauchy_scale *= 1.1  # Increase scale if successful\n                else:\n                    self.cauchy_scale *= 0.9  # Decrease scale if not successful\n                successes = 0\n                iterations = 0\n\n            self.cauchy_scale = np.clip(self.cauchy_scale, 0.01, 1.0)  # Limit Cauchy scale\n        \n        # Local Search\n        for _ in range(min(self.local_search_iterations, self.budget)):\n             if self.budget <= 0:\n                 break\n\n             perturbation = np.random.uniform(-search_range/100, search_range/100, self.dim)\n             local_x = np.clip(global_best_position + perturbation, lb, ub)\n             local_f = func(local_x)\n             self.budget -=1\n\n             if local_f < global_best_fitness:\n                  global_best_fitness = local_f\n                  global_best_position = local_x\n\n\n        return global_best_fitness, global_best_position", "configspace": "", "generation": 7, "feedback": "The algorithm HybridBirdCauchyOptimizer scored 0.678 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["ecc70942-b0a7-4f55-9c5c-b847d4827960", "14141113-7977-460c-9182-d881a514090d"], "operator": "crossover", "metadata": {"llm_response": "# Description: A hybrid algorithm combining bird swarm optimization's social learning and adaptive shrinking Cauchy mutation for enhanced exploration and exploitation.\n# Code: \n```python\nimport numpy as np\n\nclass HybridBirdCauchyOptimizer:\n    def __init__(self, budget=10000, dim=10, swarm_size=20, inertia_weight=0.7, cognitive_coeff=1.4, social_coeff=1.4, neighborhood_size=3, shrink_factor=0.95, initial_cauchy_scale=0.1, local_search_iterations=5, cauchy_probability=0.2):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = swarm_size\n        self.inertia_weight = inertia_weight\n        self.cognitive_coeff = cognitive_coeff\n        self.social_coeff = social_coeff\n        self.neighborhood_size = neighborhood_size\n        self.shrink_factor = shrink_factor\n        self.cauchy_scale = initial_cauchy_scale\n        self.success_rate = 0.5\n        self.local_search_iterations = local_search_iterations\n        self.cauchy_probability = cauchy_probability  #Probability of applying Cauchy mutation\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        \n        # Initialize swarm positions and velocities\n        swarm_positions = np.random.uniform(lb, ub, size=(self.swarm_size, self.dim))\n        swarm_velocities = np.random.uniform(-abs(ub-lb)/10, abs(ub-lb)/10, size=(self.swarm_size, self.dim))\n        swarm_fitness = np.array([func(x) for x in swarm_positions])\n        self.budget -= self.swarm_size\n        \n        personal_best_positions = swarm_positions.copy()\n        personal_best_fitness = swarm_fitness.copy()\n        \n        global_best_index = np.argmin(personal_best_fitness)\n        global_best_position = personal_best_positions[global_best_index].copy()\n        global_best_fitness = personal_best_fitness[global_best_index]\n        \n        search_range = ub - lb\n        center = global_best_position #Use global best as center for Cauchy\n\n        successes = 0\n        iterations = 0\n        \n        while self.budget > 0:\n            for i in range(self.swarm_size):\n                if self.budget <= 0:\n                    break\n                # Bird Swarm Optimization part\n                inertia = self.inertia_weight * swarm_velocities[i]\n                cognitive = self.cognitive_coeff * np.random.rand(self.dim) * (personal_best_positions[i] - swarm_positions[i])\n                \n                # Social component: find best neighbor\n                neighbors_indices = np.random.choice(self.swarm_size, size=self.neighborhood_size, replace=False)\n                neighbors_fitness = personal_best_fitness[neighbors_indices]\n                best_neighbor_index = neighbors_indices[np.argmin(neighbors_fitness)]\n                social = self.social_coeff * np.random.rand(self.dim) * (personal_best_positions[best_neighbor_index] - swarm_positions[i])\n                \n                swarm_velocities[i] = inertia + cognitive + social\n                \n                # Update position\n                new_position = swarm_positions[i] + swarm_velocities[i]\n                \n                # Cauchy mutation with probability cauchy_probability\n                if np.random.rand() < self.cauchy_probability:\n                    mutation = center + search_range * np.random.standard_cauchy(size=self.dim) * self.cauchy_scale  # Scale Cauchy distribution\n                    new_position = mutation\n                \n                # Boundary handling\n                new_position = np.clip(new_position, lb, ub)\n                \n                new_fitness = func(new_position)\n                self.budget -= 1\n                iterations += 1\n                \n                if new_fitness < personal_best_fitness[i]:\n                    personal_best_fitness[i] = new_fitness\n                    personal_best_positions[i] = new_position.copy()\n                    successes += 1\n                    \n                    if new_fitness < global_best_fitness:\n                        global_best_fitness = new_fitness\n                        global_best_position = new_position.copy()\n                        center = global_best_position #Update center\n\n                swarm_positions[i] = new_position\n                \n            # Shrink search range\n            search_range *= self.shrink_factor\n            lb = np.clip(center - search_range/2, func.bounds.lb, func.bounds.ub)\n            ub = np.clip(center + search_range/2, func.bounds.lb, func.bounds.ub)\n            \n            # Adjust Cauchy scale based on success rate\n            if iterations > 0:\n                self.success_rate = successes / iterations\n                if self.success_rate > 0.2:\n                    self.cauchy_scale *= 1.1  # Increase scale if successful\n                else:\n                    self.cauchy_scale *= 0.9  # Decrease scale if not successful\n                successes = 0\n                iterations = 0\n\n            self.cauchy_scale = np.clip(self.cauchy_scale, 0.01, 1.0)  # Limit Cauchy scale\n        \n        # Local Search\n        for _ in range(min(self.local_search_iterations, self.budget)):\n             if self.budget <= 0:\n                 break\n\n             perturbation = np.random.uniform(-search_range/100, search_range/100, self.dim)\n             local_x = np.clip(global_best_position + perturbation, lb, ub)\n             local_f = func(local_x)\n             self.budget -=1\n\n             if local_f < global_best_fitness:\n                  global_best_fitness = local_f\n                  global_best_position = local_x\n\n\n        return global_best_fitness, global_best_position\n```", "aucs": [0.26817260157720324, 0.39209660016235315, 0.8208648434119855, 0.9308093909606897, 0.8349848084730477, 0.8592204237967452, 0.34676791570659216, 0.7746289910361102, 0.8287485211253525, 0.8310009536932828, 0.9173143903007298, 0.9971345781751523, 0.26391895547721833, 0.8310644593383512, 0.7421116703270598, 0.8546663524747603, 0.4139677715085449, 0.8950802837087142, 0.2343137888430996, 0.5192760574112323], "operator": "crossover", "theta_sampled": 0.0051685700896461845, "reward": 0.05128635365298828, "generation": 7}, "task_prompt": ""}
{"id": "5ed072b3-d57e-4c1e-8fc4-1c2b4f7cf697", "fitness": 0.685698677101927, "name": "HybridBirdSwarmCauchy", "description": "A hybrid approach combining the exploration of Bird Swarm Optimization with the adaptive shrinking and Cauchy mutation of Adaptive Shrinking Cauchy Optimizer.", "code": "import numpy as np\n\nclass HybridBirdSwarmCauchy:\n    def __init__(self, budget=10000, dim=10, swarm_size=20, inertia_weight=0.7, cognitive_coeff=1.4, social_coeff=1.4, neighborhood_size=3, shrink_factor=0.95, initial_cauchy_scale=0.1, local_search_iterations=5, cauchy_probability=0.2):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = swarm_size\n        self.inertia_weight = inertia_weight\n        self.cognitive_coeff = cognitive_coeff\n        self.social_coeff = social_coeff\n        self.neighborhood_size = neighborhood_size\n        self.shrink_factor = shrink_factor\n        self.cauchy_scale = initial_cauchy_scale\n        self.local_search_iterations = local_search_iterations\n        self.cauchy_probability = cauchy_probability  # Probability of applying Cauchy mutation\n        self.success_rate = 0.5  # Initialize success rate\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        \n        # Initialize swarm positions and velocities\n        swarm_positions = np.random.uniform(lb, ub, size=(self.swarm_size, self.dim))\n        swarm_velocities = np.random.uniform(-abs(ub-lb)/10, abs(ub-lb)/10, size=(self.swarm_size, self.dim))  #Initialize with small velocities\n        swarm_fitness = np.array([func(x) for x in swarm_positions])\n        self.budget -= self.swarm_size #Update the remaining budget\n        \n        personal_best_positions = swarm_positions.copy()\n        personal_best_fitness = swarm_fitness.copy()\n        \n        global_best_index = np.argmin(personal_best_fitness)\n        global_best_position = personal_best_positions[global_best_index].copy()\n        global_best_fitness = personal_best_fitness[global_best_index]\n        \n        search_range = ub - lb\n        center = global_best_position\n\n        successes = 0\n        iterations = 0\n\n\n        while self.budget > 0:\n            for i in range(self.swarm_size):\n                # Bird Swarm Optimization part\n                inertia = self.inertia_weight * swarm_velocities[i]\n                cognitive = self.cognitive_coeff * np.random.rand(self.dim) * (personal_best_positions[i] - swarm_positions[i])\n                \n                # Social component: find best neighbor\n                neighbors_indices = np.random.choice(self.swarm_size, size=self.neighborhood_size, replace=False)  # Select random neighbors\n                neighbors_fitness = personal_best_fitness[neighbors_indices]\n                best_neighbor_index = neighbors_indices[np.argmin(neighbors_fitness)]\n                social = self.social_coeff * np.random.rand(self.dim) * (personal_best_positions[best_neighbor_index] - swarm_positions[i])\n                \n                swarm_velocities[i] = inertia + cognitive + social\n                \n                # Update position\n                new_position = swarm_positions[i] + swarm_velocities[i]\n                \n                # Boundary handling (clip the solution, if out of bounds)\n                new_position = np.clip(new_position, lb, ub)\n\n                # Cauchy mutation with probability\n                if np.random.rand() < self.cauchy_probability:\n                    mutation = center + search_range * np.random.standard_cauchy(size=self.dim) * self.cauchy_scale  # Scale Cauchy distribution\n                    mutation = np.clip(mutation, lb, ub)  # Clip values to stay within bounds\n                    new_position = mutation\n                \n                new_fitness = func(new_position)\n                self.budget -= 1\n                iterations += 1\n                \n                if new_fitness < personal_best_fitness[i]:\n                    personal_best_fitness[i] = new_fitness\n                    personal_best_positions[i] = new_position.copy()\n                    successes += 1\n                    \n                    if new_fitness < global_best_fitness:\n                        global_best_fitness = new_fitness\n                        global_best_position = new_position.copy()\n                        center = global_best_position\n                \n                swarm_positions[i] = new_position\n\n            # Shrink the search range around the best individual\n            search_range *= self.shrink_factor\n            lb = np.clip(center - search_range/2, func.bounds.lb, func.bounds.ub)\n            ub = np.clip(center + search_range/2, func.bounds.lb, func.bounds.ub)\n\n             # Adjust Cauchy scale based on success rate\n            if iterations > 0:\n                self.success_rate = successes / iterations\n                if self.success_rate > 0.2:\n                    self.cauchy_scale *= 1.1  # Increase scale if successful\n                else:\n                    self.cauchy_scale *= 0.9  # Decrease scale if not successful\n                successes = 0\n                iterations = 0\n\n            self.cauchy_scale = np.clip(self.cauchy_scale, 0.01, 1.0) # Limit Cauchy scale\n            \n        \n        # Local Search around best solution\n        for _ in range(min(self.local_search_iterations, self.budget)):\n             if self.budget <= 0:\n                 break\n\n             perturbation = np.random.uniform(-search_range/100, search_range/100, self.dim)\n             local_x = np.clip(global_best_position + perturbation, lb, ub)\n             local_f = func(local_x)\n             self.budget -=1\n\n             if local_f < global_best_fitness:\n                  global_best_fitness = local_f\n                  global_best_position = local_x\n        \n        return global_best_fitness, global_best_position", "configspace": "", "generation": 7, "feedback": "The algorithm HybridBirdSwarmCauchy scored 0.686 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["ecc70942-b0a7-4f55-9c5c-b847d4827960", "14141113-7977-460c-9182-d881a514090d"], "operator": "crossover", "metadata": {"llm_response": "# Description: A hybrid approach combining the exploration of Bird Swarm Optimization with the adaptive shrinking and Cauchy mutation of Adaptive Shrinking Cauchy Optimizer.\n# Code: \n```python\nimport numpy as np\n\nclass HybridBirdSwarmCauchy:\n    def __init__(self, budget=10000, dim=10, swarm_size=20, inertia_weight=0.7, cognitive_coeff=1.4, social_coeff=1.4, neighborhood_size=3, shrink_factor=0.95, initial_cauchy_scale=0.1, local_search_iterations=5, cauchy_probability=0.2):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = swarm_size\n        self.inertia_weight = inertia_weight\n        self.cognitive_coeff = cognitive_coeff\n        self.social_coeff = social_coeff\n        self.neighborhood_size = neighborhood_size\n        self.shrink_factor = shrink_factor\n        self.cauchy_scale = initial_cauchy_scale\n        self.local_search_iterations = local_search_iterations\n        self.cauchy_probability = cauchy_probability  # Probability of applying Cauchy mutation\n        self.success_rate = 0.5  # Initialize success rate\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        \n        # Initialize swarm positions and velocities\n        swarm_positions = np.random.uniform(lb, ub, size=(self.swarm_size, self.dim))\n        swarm_velocities = np.random.uniform(-abs(ub-lb)/10, abs(ub-lb)/10, size=(self.swarm_size, self.dim))  #Initialize with small velocities\n        swarm_fitness = np.array([func(x) for x in swarm_positions])\n        self.budget -= self.swarm_size #Update the remaining budget\n        \n        personal_best_positions = swarm_positions.copy()\n        personal_best_fitness = swarm_fitness.copy()\n        \n        global_best_index = np.argmin(personal_best_fitness)\n        global_best_position = personal_best_positions[global_best_index].copy()\n        global_best_fitness = personal_best_fitness[global_best_index]\n        \n        search_range = ub - lb\n        center = global_best_position\n\n        successes = 0\n        iterations = 0\n\n\n        while self.budget > 0:\n            for i in range(self.swarm_size):\n                # Bird Swarm Optimization part\n                inertia = self.inertia_weight * swarm_velocities[i]\n                cognitive = self.cognitive_coeff * np.random.rand(self.dim) * (personal_best_positions[i] - swarm_positions[i])\n                \n                # Social component: find best neighbor\n                neighbors_indices = np.random.choice(self.swarm_size, size=self.neighborhood_size, replace=False)  # Select random neighbors\n                neighbors_fitness = personal_best_fitness[neighbors_indices]\n                best_neighbor_index = neighbors_indices[np.argmin(neighbors_fitness)]\n                social = self.social_coeff * np.random.rand(self.dim) * (personal_best_positions[best_neighbor_index] - swarm_positions[i])\n                \n                swarm_velocities[i] = inertia + cognitive + social\n                \n                # Update position\n                new_position = swarm_positions[i] + swarm_velocities[i]\n                \n                # Boundary handling (clip the solution, if out of bounds)\n                new_position = np.clip(new_position, lb, ub)\n\n                # Cauchy mutation with probability\n                if np.random.rand() < self.cauchy_probability:\n                    mutation = center + search_range * np.random.standard_cauchy(size=self.dim) * self.cauchy_scale  # Scale Cauchy distribution\n                    mutation = np.clip(mutation, lb, ub)  # Clip values to stay within bounds\n                    new_position = mutation\n                \n                new_fitness = func(new_position)\n                self.budget -= 1\n                iterations += 1\n                \n                if new_fitness < personal_best_fitness[i]:\n                    personal_best_fitness[i] = new_fitness\n                    personal_best_positions[i] = new_position.copy()\n                    successes += 1\n                    \n                    if new_fitness < global_best_fitness:\n                        global_best_fitness = new_fitness\n                        global_best_position = new_position.copy()\n                        center = global_best_position\n                \n                swarm_positions[i] = new_position\n\n            # Shrink the search range around the best individual\n            search_range *= self.shrink_factor\n            lb = np.clip(center - search_range/2, func.bounds.lb, func.bounds.ub)\n            ub = np.clip(center + search_range/2, func.bounds.lb, func.bounds.ub)\n\n             # Adjust Cauchy scale based on success rate\n            if iterations > 0:\n                self.success_rate = successes / iterations\n                if self.success_rate > 0.2:\n                    self.cauchy_scale *= 1.1  # Increase scale if successful\n                else:\n                    self.cauchy_scale *= 0.9  # Decrease scale if not successful\n                successes = 0\n                iterations = 0\n\n            self.cauchy_scale = np.clip(self.cauchy_scale, 0.01, 1.0) # Limit Cauchy scale\n            \n        \n        # Local Search around best solution\n        for _ in range(min(self.local_search_iterations, self.budget)):\n             if self.budget <= 0:\n                 break\n\n             perturbation = np.random.uniform(-search_range/100, search_range/100, self.dim)\n             local_x = np.clip(global_best_position + perturbation, lb, ub)\n             local_f = func(local_x)\n             self.budget -=1\n\n             if local_f < global_best_fitness:\n                  global_best_fitness = local_f\n                  global_best_position = local_x\n        \n        return global_best_fitness, global_best_position\n```", "aucs": [0.2580509279111509, 0.3627555955295172, 0.8149942946374649, 0.9203940910023426, 0.8367920512162574, 0.866473875329143, 0.3332982461019155, 0.786076606482359, 0.8406001717868912, 0.8325773000301482, 0.8859008658739419, 0.9973081065177224, 0.23913256140984585, 0.5183400884903783, 0.8253743613924076, 0.8540361838122562, 0.706079784227434, 0.8786129474048688, 0.43445013133457655, 0.5227253515479169], "operator": "crossover", "theta_sampled": 0.016395520766705567, "reward": 0.059177862879504106, "generation": 7}, "task_prompt": ""}
{"id": "2d4a0885-d5b7-4ac2-ac2a-9dc4a9dd7174", "fitness": 0.0, "name": "HybridBirdCauchyOptimizer", "description": "Hybrid algorithm combining Bird Swarm Optimization's social learning with Adaptive Shrinking Cauchy Optimizer's mutation and shrinking search space.", "code": "import numpy as np\n\nclass HybridBirdCauchyOptimizer:\n    def __init__(self, budget=10000, dim=10, swarm_size=20, inertia_weight=0.7, cognitive_coeff=1.4, social_coeff=1.4, neighborhood_size=3, shrink_factor=0.95, initial_cauchy_scale=0.1, local_search_iterations=5):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = swarm_size\n        self.inertia_weight = inertia_weight\n        self.cognitive_coeff = cognitive_coeff\n        self.social_coeff = social_coeff\n        self.neighborhood_size = neighborhood_size\n        self.shrink_factor = shrink_factor\n        self.cauchy_scale = initial_cauchy_scale\n        self.success_rate = 0.5  # Initialize success rate\n        self.local_search_iterations = local_search_iterations\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        \n        # Initialize swarm positions and velocities\n        swarm_positions = np.random.uniform(lb, ub, size=(self.swarm_size, self.dim))\n        swarm_velocities = np.random.uniform(-abs(ub-lb)/10, abs(ub-lb)/10, size=(self.swarm_size, self.dim))  #Initialize with small velocities\n        swarm_fitness = np.array([func(x) for x in swarm_positions])\n        self.budget -= self.swarm_size #Update the remaining budget\n        \n        personal_best_positions = swarm_positions.copy()\n        personal_best_fitness = swarm_fitness.copy()\n        \n        global_best_index = np.argmin(personal_best_fitness)\n        global_best_position = personal_best_positions[global_best_index].copy()\n        global_best_fitness = personal_best_fitness[global_best_index]\n        \n        search_range = ub - lb\n        center = global_best_position\n\n        successes = 0\n        iterations = 0\n        \n        while self.budget > 0:\n            for i in range(self.swarm_size):\n                # Cauchy mutation with shrinking search range\n                mutation = center + search_range * np.random.standard_cauchy(size=self.dim) * self.cauchy_scale\n                mutation = np.clip(mutation, lb, ub)\n                \n                f_mutation = func(mutation)\n                self.budget -= 1\n                iterations += 1\n                \n                if f_mutation < swarm_fitness[i]:  #compare to current fitness\n                    swarm_fitness[i] = f_mutation\n                    swarm_positions[i] = mutation\n                    successes += 1\n                    \n                    if f_mutation < personal_best_fitness[i]:\n                        personal_best_fitness[i] = f_mutation\n                        personal_best_positions[i] = mutation.copy()\n                        \n                        if f_mutation < global_best_fitness:\n                            global_best_fitness = f_mutation\n                            global_best_position = mutation.copy()\n                            center = global_best_position #Update center\n\n                # Social component: find best neighbor - use personal best fitness to determine neighbor\n                neighbors_indices = np.random.choice(self.swarm_size, size=self.neighborhood_size, replace=False)  # Select random neighbors\n                neighbors_fitness = personal_best_fitness[neighbors_indices]\n                best_neighbor_index = neighbors_indices[np.argmin(neighbors_fitness)]\n                social = self.social_coeff * np.random.rand(self.dim) * (personal_best_positions[best_neighbor_index] - swarm_positions[i])\n                \n                # Update velocity using the social component\n                swarm_velocities[i] = self.inertia_weight * swarm_velocities[i] + self.cognitive_coeff * np.random.rand(self.dim) * (personal_best_positions[i] - swarm_positions[i]) + social #use inertia, cognitive, and social component\n\n                # Update position\n                new_position = swarm_positions[i] + swarm_velocities[i]\n\n                # Boundary handling (clip the solution, if out of bounds)\n                new_position = np.clip(new_position, lb, ub)\n\n                new_fitness = func(new_position)\n                self.budget -= 1\n\n                if new_fitness < swarm_fitness[i]: #Compare to current fitness\n                    swarm_fitness[i] = new_fitness\n                    swarm_positions[i] = new_position.copy()\n\n                    if new_fitness < personal_best_fitness[i]:\n                        personal_best_fitness[i] = new_fitness\n                        personal_best_positions[i] = new_position.copy()\n\n                        if new_fitness < global_best_fitness:\n                            global_best_fitness = new_fitness\n                            global_best_position = new_position.copy()\n                            center = global_best_position #Update center\n            \n            # Shrink the search range around the best individual\n            search_range *= self.shrink_factor\n            lb = np.clip(center - search_range/2, func.bounds.lb, func.bounds.ub)\n            ub = np.clip(center + search_range/2, func.bounds.lb, func.bounds.ub)\n        \n            # Adjust Cauchy scale based on success rate\n            if iterations > 0:\n                self.success_rate = successes / iterations\n                if self.success_rate > 0.2:\n                    self.cauchy_scale *= 1.1  # Increase scale if successful\n                else:\n                    self.cauchy_scale *= 0.9  # Decrease scale if not successful\n                successes = 0\n                iterations = 0\n\n            self.cauchy_scale = np.clip(self.cauchy_scale, 0.01, 1.0) # Limit Cauchy scale\n\n        # Local Search around best solution\n        for _ in range(min(self.local_search_iterations, self.budget)):\n            if self.budget <= 0:\n                break\n\n            perturbation = np.random.uniform(-search_range/100, search_range/100, self.dim)\n            local_x = np.clip(global_best_position + perturbation, lb, ub)\n            local_f = func(local_x)\n            self.budget -=1\n\n            if local_f < global_best_fitness:\n                global_best_fitness = local_f\n                global_best_position = local_x\n                center = global_best_position\n\n\n        return global_best_fitness, global_best_position", "configspace": "", "generation": 7, "feedback": "The algorithm HybridBirdCauchyOptimizer scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["ecc70942-b0a7-4f55-9c5c-b847d4827960", "14141113-7977-460c-9182-d881a514090d"], "operator": "crossover", "metadata": {"llm_response": "# Description: Hybrid algorithm combining Bird Swarm Optimization's social learning with Adaptive Shrinking Cauchy Optimizer's mutation and shrinking search space.\n# Code: \n```python\nimport numpy as np\n\nclass HybridBirdCauchyOptimizer:\n    def __init__(self, budget=10000, dim=10, swarm_size=20, inertia_weight=0.7, cognitive_coeff=1.4, social_coeff=1.4, neighborhood_size=3, shrink_factor=0.95, initial_cauchy_scale=0.1, local_search_iterations=5):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = swarm_size\n        self.inertia_weight = inertia_weight\n        self.cognitive_coeff = cognitive_coeff\n        self.social_coeff = social_coeff\n        self.neighborhood_size = neighborhood_size\n        self.shrink_factor = shrink_factor\n        self.cauchy_scale = initial_cauchy_scale\n        self.success_rate = 0.5  # Initialize success rate\n        self.local_search_iterations = local_search_iterations\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        \n        # Initialize swarm positions and velocities\n        swarm_positions = np.random.uniform(lb, ub, size=(self.swarm_size, self.dim))\n        swarm_velocities = np.random.uniform(-abs(ub-lb)/10, abs(ub-lb)/10, size=(self.swarm_size, self.dim))  #Initialize with small velocities\n        swarm_fitness = np.array([func(x) for x in swarm_positions])\n        self.budget -= self.swarm_size #Update the remaining budget\n        \n        personal_best_positions = swarm_positions.copy()\n        personal_best_fitness = swarm_fitness.copy()\n        \n        global_best_index = np.argmin(personal_best_fitness)\n        global_best_position = personal_best_positions[global_best_index].copy()\n        global_best_fitness = personal_best_fitness[global_best_index]\n        \n        search_range = ub - lb\n        center = global_best_position\n\n        successes = 0\n        iterations = 0\n        \n        while self.budget > 0:\n            for i in range(self.swarm_size):\n                # Cauchy mutation with shrinking search range\n                mutation = center + search_range * np.random.standard_cauchy(size=self.dim) * self.cauchy_scale\n                mutation = np.clip(mutation, lb, ub)\n                \n                f_mutation = func(mutation)\n                self.budget -= 1\n                iterations += 1\n                \n                if f_mutation < swarm_fitness[i]:  #compare to current fitness\n                    swarm_fitness[i] = f_mutation\n                    swarm_positions[i] = mutation\n                    successes += 1\n                    \n                    if f_mutation < personal_best_fitness[i]:\n                        personal_best_fitness[i] = f_mutation\n                        personal_best_positions[i] = mutation.copy()\n                        \n                        if f_mutation < global_best_fitness:\n                            global_best_fitness = f_mutation\n                            global_best_position = mutation.copy()\n                            center = global_best_position #Update center\n\n                # Social component: find best neighbor - use personal best fitness to determine neighbor\n                neighbors_indices = np.random.choice(self.swarm_size, size=self.neighborhood_size, replace=False)  # Select random neighbors\n                neighbors_fitness = personal_best_fitness[neighbors_indices]\n                best_neighbor_index = neighbors_indices[np.argmin(neighbors_fitness)]\n                social = self.social_coeff * np.random.rand(self.dim) * (personal_best_positions[best_neighbor_index] - swarm_positions[i])\n                \n                # Update velocity using the social component\n                swarm_velocities[i] = self.inertia_weight * swarm_velocities[i] + self.cognitive_coeff * np.random.rand(self.dim) * (personal_best_positions[i] - swarm_positions[i]) + social #use inertia, cognitive, and social component\n\n                # Update position\n                new_position = swarm_positions[i] + swarm_velocities[i]\n\n                # Boundary handling (clip the solution, if out of bounds)\n                new_position = np.clip(new_position, lb, ub)\n\n                new_fitness = func(new_position)\n                self.budget -= 1\n\n                if new_fitness < swarm_fitness[i]: #Compare to current fitness\n                    swarm_fitness[i] = new_fitness\n                    swarm_positions[i] = new_position.copy()\n\n                    if new_fitness < personal_best_fitness[i]:\n                        personal_best_fitness[i] = new_fitness\n                        personal_best_positions[i] = new_position.copy()\n\n                        if new_fitness < global_best_fitness:\n                            global_best_fitness = new_fitness\n                            global_best_position = new_position.copy()\n                            center = global_best_position #Update center\n            \n            # Shrink the search range around the best individual\n            search_range *= self.shrink_factor\n            lb = np.clip(center - search_range/2, func.bounds.lb, func.bounds.ub)\n            ub = np.clip(center + search_range/2, func.bounds.lb, func.bounds.ub)\n        \n            # Adjust Cauchy scale based on success rate\n            if iterations > 0:\n                self.success_rate = successes / iterations\n                if self.success_rate > 0.2:\n                    self.cauchy_scale *= 1.1  # Increase scale if successful\n                else:\n                    self.cauchy_scale *= 0.9  # Decrease scale if not successful\n                successes = 0\n                iterations = 0\n\n            self.cauchy_scale = np.clip(self.cauchy_scale, 0.01, 1.0) # Limit Cauchy scale\n\n        # Local Search around best solution\n        for _ in range(min(self.local_search_iterations, self.budget)):\n            if self.budget <= 0:\n                break\n\n            perturbation = np.random.uniform(-search_range/100, search_range/100, self.dim)\n            local_x = np.clip(global_best_position + perturbation, lb, ub)\n            local_f = func(local_x)\n            self.budget -=1\n\n            if local_f < global_best_fitness:\n                global_best_fitness = local_f\n                global_best_position = local_x\n                center = global_best_position\n\n\n        return global_best_fitness, global_best_position\n```", "aucs": [0], "operator": "crossover", "theta_sampled": 0.025215638683476992, "reward": 0.0, "generation": 7}, "task_prompt": ""}
{"id": "f993ee8a-5f7e-4691-8169-9f5d87366a0b", "fitness": 0.6549302209664012, "name": "HybridBirdSwarmCauchyNM", "description": "Combines Bird Swarm Optimization with Cauchy mutation, adaptive scaling, and Nelder-Mead local search for enhanced exploration and exploitation.", "code": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridBirdSwarmCauchyNM:\n    def __init__(self, budget=10000, dim=10, swarm_size=20, inertia_weight=0.7, cognitive_coeff=1.4, social_coeff=1.4, neighborhood_size=3, shrink_factor=0.95, initial_cauchy_scale=0.1, cauchy_probability=0.2, nm_iterations=5):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = swarm_size\n        self.inertia_weight = inertia_weight\n        self.cognitive_coeff = cognitive_coeff\n        self.social_coeff = social_coeff\n        self.neighborhood_size = neighborhood_size\n        self.shrink_factor = shrink_factor\n        self.cauchy_scale = initial_cauchy_scale\n        self.cauchy_probability = cauchy_probability\n        self.nm_iterations = nm_iterations\n        self.success_rate = 0.5\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        \n        # Initialize swarm positions and velocities\n        swarm_positions = np.random.uniform(lb, ub, size=(self.swarm_size, self.dim))\n        swarm_velocities = np.random.uniform(-abs(ub-lb)/10, abs(ub-lb)/10, size=(self.swarm_size, self.dim))\n        swarm_fitness = np.array([func(x) for x in swarm_positions])\n        self.budget -= self.swarm_size\n        \n        personal_best_positions = swarm_positions.copy()\n        personal_best_fitness = swarm_fitness.copy()\n        \n        global_best_index = np.argmin(personal_best_fitness)\n        global_best_position = personal_best_positions[global_best_index].copy()\n        global_best_fitness = personal_best_fitness[global_best_index]\n        \n        search_range = ub - lb\n        center = global_best_position\n\n        successes = 0\n        iterations = 0\n\n        while self.budget > 0:\n            for i in range(self.swarm_size):\n                if self.budget <= 0:\n                    break\n\n                # Bird Swarm Optimization part\n                inertia = self.inertia_weight * swarm_velocities[i]\n                cognitive = self.cognitive_coeff * np.random.rand(self.dim) * (personal_best_positions[i] - swarm_positions[i])\n                \n                # Social component: find best neighbor\n                neighbors_indices = np.random.choice(self.swarm_size, size=self.neighborhood_size, replace=False)\n                neighbors_fitness = personal_best_fitness[neighbors_indices]\n                best_neighbor_index = neighbors_indices[np.argmin(neighbors_fitness)]\n                social = self.social_coeff * np.random.rand(self.dim) * (personal_best_positions[best_neighbor_index] - swarm_positions[i])\n                \n                swarm_velocities[i] = inertia + cognitive + social\n                \n                # Update position\n                new_position = swarm_positions[i] + swarm_velocities[i]\n                \n                # Cauchy mutation with probability\n                if np.random.rand() < self.cauchy_probability:\n                    mutation = center + search_range * np.random.standard_cauchy(size=self.dim) * self.cauchy_scale\n                    new_position = mutation\n                \n                # Boundary handling\n                new_position = np.clip(new_position, lb, ub)\n                \n                new_fitness = func(new_position)\n                self.budget -= 1\n                iterations += 1\n                \n                if new_fitness < personal_best_fitness[i]:\n                    personal_best_fitness[i] = new_fitness\n                    personal_best_positions[i] = new_position.copy()\n                    successes += 1\n                    \n                    if new_fitness < global_best_fitness:\n                        global_best_fitness = new_fitness\n                        global_best_position = new_position.copy()\n                        center = global_best_position #Update center\n\n                swarm_positions[i] = new_position\n                \n            # Shrink search range\n            search_range *= self.shrink_factor\n            lb = np.clip(center - search_range/2, func.bounds.lb, func.bounds.ub)\n            ub = np.clip(center + search_range/2, func.bounds.lb, func.bounds.ub)\n\n            # Adjust Cauchy scale based on success rate\n            if iterations > 0:\n                self.success_rate = successes / iterations\n                if self.success_rate > 0.2:\n                    self.cauchy_scale *= 1.1  # Increase scale if successful\n                else:\n                    self.cauchy_scale *= 0.9  # Decrease scale if not successful\n                successes = 0\n                iterations = 0\n\n            self.cauchy_scale = np.clip(self.cauchy_scale, 0.01, 1.0)\n        \n        # Nelder-Mead local search\n        if self.budget > 0:\n            nm_budget = min(self.nm_iterations, self.budget)\n            result = minimize(func, global_best_position, method='Nelder-Mead', bounds=np.column_stack((func.bounds.lb, func.bounds.ub)), options={'maxfev': nm_budget})\n            if result.fun < global_best_fitness:\n                global_best_fitness = result.fun\n                global_best_position = result.x\n            self.budget -= result.nfev  # Update budget based on Nelder-Mead evaluations\n\n\n        return global_best_fitness, global_best_position", "configspace": "", "generation": 8, "feedback": "The algorithm HybridBirdSwarmCauchyNM scored 0.655 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["5ed072b3-d57e-4c1e-8fc4-1c2b4f7cf697", "df8a52a3-838e-4304-b60e-76e5afa6152f"], "operator": "crossover", "metadata": {"llm_response": "# Description: Combines Bird Swarm Optimization with Cauchy mutation, adaptive scaling, and Nelder-Mead local search for enhanced exploration and exploitation.\n# Code: \n```python\nimport numpy as np\nfrom scipy.optimize import minimize\n\nclass HybridBirdSwarmCauchyNM:\n    def __init__(self, budget=10000, dim=10, swarm_size=20, inertia_weight=0.7, cognitive_coeff=1.4, social_coeff=1.4, neighborhood_size=3, shrink_factor=0.95, initial_cauchy_scale=0.1, cauchy_probability=0.2, nm_iterations=5):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = swarm_size\n        self.inertia_weight = inertia_weight\n        self.cognitive_coeff = cognitive_coeff\n        self.social_coeff = social_coeff\n        self.neighborhood_size = neighborhood_size\n        self.shrink_factor = shrink_factor\n        self.cauchy_scale = initial_cauchy_scale\n        self.cauchy_probability = cauchy_probability\n        self.nm_iterations = nm_iterations\n        self.success_rate = 0.5\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        \n        # Initialize swarm positions and velocities\n        swarm_positions = np.random.uniform(lb, ub, size=(self.swarm_size, self.dim))\n        swarm_velocities = np.random.uniform(-abs(ub-lb)/10, abs(ub-lb)/10, size=(self.swarm_size, self.dim))\n        swarm_fitness = np.array([func(x) for x in swarm_positions])\n        self.budget -= self.swarm_size\n        \n        personal_best_positions = swarm_positions.copy()\n        personal_best_fitness = swarm_fitness.copy()\n        \n        global_best_index = np.argmin(personal_best_fitness)\n        global_best_position = personal_best_positions[global_best_index].copy()\n        global_best_fitness = personal_best_fitness[global_best_index]\n        \n        search_range = ub - lb\n        center = global_best_position\n\n        successes = 0\n        iterations = 0\n\n        while self.budget > 0:\n            for i in range(self.swarm_size):\n                if self.budget <= 0:\n                    break\n\n                # Bird Swarm Optimization part\n                inertia = self.inertia_weight * swarm_velocities[i]\n                cognitive = self.cognitive_coeff * np.random.rand(self.dim) * (personal_best_positions[i] - swarm_positions[i])\n                \n                # Social component: find best neighbor\n                neighbors_indices = np.random.choice(self.swarm_size, size=self.neighborhood_size, replace=False)\n                neighbors_fitness = personal_best_fitness[neighbors_indices]\n                best_neighbor_index = neighbors_indices[np.argmin(neighbors_fitness)]\n                social = self.social_coeff * np.random.rand(self.dim) * (personal_best_positions[best_neighbor_index] - swarm_positions[i])\n                \n                swarm_velocities[i] = inertia + cognitive + social\n                \n                # Update position\n                new_position = swarm_positions[i] + swarm_velocities[i]\n                \n                # Cauchy mutation with probability\n                if np.random.rand() < self.cauchy_probability:\n                    mutation = center + search_range * np.random.standard_cauchy(size=self.dim) * self.cauchy_scale\n                    new_position = mutation\n                \n                # Boundary handling\n                new_position = np.clip(new_position, lb, ub)\n                \n                new_fitness = func(new_position)\n                self.budget -= 1\n                iterations += 1\n                \n                if new_fitness < personal_best_fitness[i]:\n                    personal_best_fitness[i] = new_fitness\n                    personal_best_positions[i] = new_position.copy()\n                    successes += 1\n                    \n                    if new_fitness < global_best_fitness:\n                        global_best_fitness = new_fitness\n                        global_best_position = new_position.copy()\n                        center = global_best_position #Update center\n\n                swarm_positions[i] = new_position\n                \n            # Shrink search range\n            search_range *= self.shrink_factor\n            lb = np.clip(center - search_range/2, func.bounds.lb, func.bounds.ub)\n            ub = np.clip(center + search_range/2, func.bounds.lb, func.bounds.ub)\n\n            # Adjust Cauchy scale based on success rate\n            if iterations > 0:\n                self.success_rate = successes / iterations\n                if self.success_rate > 0.2:\n                    self.cauchy_scale *= 1.1  # Increase scale if successful\n                else:\n                    self.cauchy_scale *= 0.9  # Decrease scale if not successful\n                successes = 0\n                iterations = 0\n\n            self.cauchy_scale = np.clip(self.cauchy_scale, 0.01, 1.0)\n        \n        # Nelder-Mead local search\n        if self.budget > 0:\n            nm_budget = min(self.nm_iterations, self.budget)\n            result = minimize(func, global_best_position, method='Nelder-Mead', bounds=np.column_stack((func.bounds.lb, func.bounds.ub)), options={'maxfev': nm_budget})\n            if result.fun < global_best_fitness:\n                global_best_fitness = result.fun\n                global_best_position = result.x\n            self.budget -= result.nfev  # Update budget based on Nelder-Mead evaluations\n\n\n        return global_best_fitness, global_best_position\n```", "aucs": [0.24254112301787756, 0.20611702590142034, 0.8153975794137709, 0.9193830403291238, 0.8394286893568945, 0.856901791336462, 0.3455649702376681, 0.6269753052335247, 0.8167809739375684, 0.8295229028286876, 0.9259922743538741, 0.995868046167059, 0.2699487499348363, 0.692789461307624, 0.842026233165935, 0.8562530684278059, 0.407491868621511, 0.9014548185485356, 0.200699090654737, 0.5074674065531095], "operator": "crossover", "theta_sampled": 0.027389671532490664, "reward": 0.0, "generation": 8}, "task_prompt": ""}
{"id": "474b8cdb-8489-4fb5-9b40-e10d198b4047", "fitness": 0.0, "name": "EnhancedHybridBirdCauchy", "description": "Combines Bird Swarm Optimization with Cauchy mutation centered around the global best, adaptive Cauchy scale, and a shrinking search range, further enhanced by a diversity maintenance mechanism.", "code": "import numpy as np\n\nclass EnhancedHybridBirdCauchy:\n    def __init__(self, budget=10000, dim=10, swarm_size=20, inertia_weight=0.7, cognitive_coeff=1.4, social_coeff=1.4, neighborhood_size=3, shrink_factor=0.95, initial_cauchy_scale=0.1, local_search_iterations=5, cauchy_probability=0.2, diversity_threshold=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = swarm_size\n        self.inertia_weight = inertia_weight\n        self.cognitive_coeff = cognitive_coeff\n        self.social_coeff = social_coeff\n        self.neighborhood_size = neighborhood_size\n        self.shrink_factor = shrink_factor\n        self.cauchy_scale = initial_cauchy_scale\n        self.local_search_iterations = local_search_iterations\n        self.cauchy_probability = cauchy_probability\n        self.diversity_threshold = diversity_threshold #Threshold for triggering diversity mechanism\n        self.success_rate = 0.5  # Initialize success rate\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        \n        # Initialize swarm positions and velocities\n        swarm_positions = np.random.uniform(lb, ub, size=(self.swarm_size, self.dim))\n        swarm_velocities = np.random.uniform(-abs(ub-lb)/10, abs(ub-lb)/10, size=(self.swarm_size, self.dim))\n        swarm_fitness = np.array([func(x) for x in swarm_positions])\n        self.budget -= self.swarm_size\n        \n        personal_best_positions = swarm_positions.copy()\n        personal_best_fitness = swarm_fitness.copy()\n        \n        global_best_index = np.argmin(personal_best_fitness)\n        global_best_position = personal_best_positions[global_best_index].copy()\n        global_best_fitness = personal_best_fitness[global_best_index]\n        \n        search_range = ub - lb\n        center = global_best_position\n\n        successes = 0\n        iterations = 0\n\n        def calculate_diversity(positions):\n            \"\"\"Calculates the average distance of each particle from the swarm's center.\"\"\"\n            center_of_swarm = np.mean(positions, axis=0)\n            distances = np.linalg.norm(positions - center_of_swarm, axis=1)\n            return np.mean(distances)\n        \n        def diversify_swarm(positions, lb, ub):\n            \"\"\"Perturbs the swarm to increase diversity.\"\"\"\n            for i in range(len(positions)):\n                for j in range(self.dim):\n                    if np.random.rand() < 0.1: #Low probability to prevent excessive disruption\n                        positions[i, j] = np.random.uniform(lb, ub)\n            return positions\n\n        while self.budget > 0:\n            for i in range(self.swarm_size):\n                if self.budget <= 0:\n                    break\n                # Bird Swarm Optimization part\n                inertia = self.inertia_weight * swarm_velocities[i]\n                cognitive = self.cognitive_coeff * np.random.rand(self.dim) * (personal_best_positions[i] - swarm_positions[i])\n                \n                # Social component: find best neighbor\n                neighbors_indices = np.random.choice(self.swarm_size, size=self.neighborhood_size, replace=False)\n                neighbors_fitness = personal_best_fitness[neighbors_indices]\n                best_neighbor_index = neighbors_indices[np.argmin(neighbors_fitness)]\n                social = self.social_coeff * np.random.rand(self.dim) * (personal_best_positions[best_neighbor_index] - swarm_positions[i])\n                \n                swarm_velocities[i] = inertia + cognitive + social\n                \n                # Update position\n                new_position = swarm_positions[i] + swarm_velocities[i]\n                \n                # Cauchy mutation with probability\n                if np.random.rand() < self.cauchy_probability:\n                    mutation = center + search_range * np.random.standard_cauchy(size=self.dim) * self.cauchy_scale\n                    new_position = mutation\n\n                # Boundary handling\n                new_position = np.clip(new_position, lb, ub)\n                \n                new_fitness = func(new_position)\n                self.budget -= 1\n                iterations += 1\n                \n                if new_fitness < personal_best_fitness[i]:\n                    personal_best_fitness[i] = new_fitness\n                    personal_best_positions[i] = new_position.copy()\n                    successes += 1\n                    \n                    if new_fitness < global_best_fitness:\n                        global_best_fitness = new_fitness\n                        global_best_position = new_position.copy()\n                        center = global_best_position\n\n                swarm_positions[i] = new_position\n                \n            # Shrink search range\n            search_range *= self.shrink_factor\n            lb = np.clip(center - search_range/2, func.bounds.lb, func.bounds.ub)\n            ub = np.clip(center + search_range/2, func.bounds.lb, func.bounds.ub)\n\n            # Diversity check and maintenance\n            diversity = calculate_diversity(swarm_positions)\n            if diversity < self.diversity_threshold:\n                swarm_positions = diversify_swarm(swarm_positions, func.bounds.lb, func.bounds.ub)\n                # Re-evaluate fitness after diversification\n                swarm_fitness = np.array([func(x) for x in swarm_positions])\n                self.budget -= self.swarm_size  # Account for re-evaluation\n\n                personal_best_positions = swarm_positions.copy()\n                personal_best_fitness = swarm_fitness.copy()\n\n                global_best_index = np.argmin(personal_best_fitness)\n                global_best_position = personal_best_positions[global_best_index].copy()\n                global_best_fitness = personal_best_fitness[global_best_index]\n                center = global_best_position\n\n            # Adjust Cauchy scale based on success rate\n            if iterations > 0:\n                self.success_rate = successes / iterations\n                if self.success_rate > 0.2:\n                    self.cauchy_scale *= 1.1\n                else:\n                    self.cauchy_scale *= 0.9\n                successes = 0\n                iterations = 0\n\n            self.cauchy_scale = np.clip(self.cauchy_scale, 0.01, 1.0)\n\n        # Local Search\n        for _ in range(min(self.local_search_iterations, self.budget)):\n             if self.budget <= 0:\n                 break\n\n             perturbation = np.random.uniform(-search_range/100, search_range/100, self.dim)\n             local_x = np.clip(global_best_position + perturbation, lb, ub)\n             local_f = func(local_x)\n             self.budget -=1\n\n             if local_f < global_best_fitness:\n                  global_best_fitness = local_f\n                  global_best_position = local_x\n\n        return global_best_fitness, global_best_position", "configspace": "", "generation": 8, "feedback": "", "error": "Evaluation error: setting an array element with a sequence.", "parent_ids": ["5ed072b3-d57e-4c1e-8fc4-1c2b4f7cf697", "df8a52a3-838e-4304-b60e-76e5afa6152f"], "operator": "crossover", "metadata": {"llm_response": "# Description: Combines Bird Swarm Optimization with Cauchy mutation centered around the global best, adaptive Cauchy scale, and a shrinking search range, further enhanced by a diversity maintenance mechanism.\n# Code: \n```python\nimport numpy as np\n\nclass EnhancedHybridBirdCauchy:\n    def __init__(self, budget=10000, dim=10, swarm_size=20, inertia_weight=0.7, cognitive_coeff=1.4, social_coeff=1.4, neighborhood_size=3, shrink_factor=0.95, initial_cauchy_scale=0.1, local_search_iterations=5, cauchy_probability=0.2, diversity_threshold=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = swarm_size\n        self.inertia_weight = inertia_weight\n        self.cognitive_coeff = cognitive_coeff\n        self.social_coeff = social_coeff\n        self.neighborhood_size = neighborhood_size\n        self.shrink_factor = shrink_factor\n        self.cauchy_scale = initial_cauchy_scale\n        self.local_search_iterations = local_search_iterations\n        self.cauchy_probability = cauchy_probability\n        self.diversity_threshold = diversity_threshold #Threshold for triggering diversity mechanism\n        self.success_rate = 0.5  # Initialize success rate\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        \n        # Initialize swarm positions and velocities\n        swarm_positions = np.random.uniform(lb, ub, size=(self.swarm_size, self.dim))\n        swarm_velocities = np.random.uniform(-abs(ub-lb)/10, abs(ub-lb)/10, size=(self.swarm_size, self.dim))\n        swarm_fitness = np.array([func(x) for x in swarm_positions])\n        self.budget -= self.swarm_size\n        \n        personal_best_positions = swarm_positions.copy()\n        personal_best_fitness = swarm_fitness.copy()\n        \n        global_best_index = np.argmin(personal_best_fitness)\n        global_best_position = personal_best_positions[global_best_index].copy()\n        global_best_fitness = personal_best_fitness[global_best_index]\n        \n        search_range = ub - lb\n        center = global_best_position\n\n        successes = 0\n        iterations = 0\n\n        def calculate_diversity(positions):\n            \"\"\"Calculates the average distance of each particle from the swarm's center.\"\"\"\n            center_of_swarm = np.mean(positions, axis=0)\n            distances = np.linalg.norm(positions - center_of_swarm, axis=1)\n            return np.mean(distances)\n        \n        def diversify_swarm(positions, lb, ub):\n            \"\"\"Perturbs the swarm to increase diversity.\"\"\"\n            for i in range(len(positions)):\n                for j in range(self.dim):\n                    if np.random.rand() < 0.1: #Low probability to prevent excessive disruption\n                        positions[i, j] = np.random.uniform(lb, ub)\n            return positions\n\n        while self.budget > 0:\n            for i in range(self.swarm_size):\n                if self.budget <= 0:\n                    break\n                # Bird Swarm Optimization part\n                inertia = self.inertia_weight * swarm_velocities[i]\n                cognitive = self.cognitive_coeff * np.random.rand(self.dim) * (personal_best_positions[i] - swarm_positions[i])\n                \n                # Social component: find best neighbor\n                neighbors_indices = np.random.choice(self.swarm_size, size=self.neighborhood_size, replace=False)\n                neighbors_fitness = personal_best_fitness[neighbors_indices]\n                best_neighbor_index = neighbors_indices[np.argmin(neighbors_fitness)]\n                social = self.social_coeff * np.random.rand(self.dim) * (personal_best_positions[best_neighbor_index] - swarm_positions[i])\n                \n                swarm_velocities[i] = inertia + cognitive + social\n                \n                # Update position\n                new_position = swarm_positions[i] + swarm_velocities[i]\n                \n                # Cauchy mutation with probability\n                if np.random.rand() < self.cauchy_probability:\n                    mutation = center + search_range * np.random.standard_cauchy(size=self.dim) * self.cauchy_scale\n                    new_position = mutation\n\n                # Boundary handling\n                new_position = np.clip(new_position, lb, ub)\n                \n                new_fitness = func(new_position)\n                self.budget -= 1\n                iterations += 1\n                \n                if new_fitness < personal_best_fitness[i]:\n                    personal_best_fitness[i] = new_fitness\n                    personal_best_positions[i] = new_position.copy()\n                    successes += 1\n                    \n                    if new_fitness < global_best_fitness:\n                        global_best_fitness = new_fitness\n                        global_best_position = new_position.copy()\n                        center = global_best_position\n\n                swarm_positions[i] = new_position\n                \n            # Shrink search range\n            search_range *= self.shrink_factor\n            lb = np.clip(center - search_range/2, func.bounds.lb, func.bounds.ub)\n            ub = np.clip(center + search_range/2, func.bounds.lb, func.bounds.ub)\n\n            # Diversity check and maintenance\n            diversity = calculate_diversity(swarm_positions)\n            if diversity < self.diversity_threshold:\n                swarm_positions = diversify_swarm(swarm_positions, func.bounds.lb, func.bounds.ub)\n                # Re-evaluate fitness after diversification\n                swarm_fitness = np.array([func(x) for x in swarm_positions])\n                self.budget -= self.swarm_size  # Account for re-evaluation\n\n                personal_best_positions = swarm_positions.copy()\n                personal_best_fitness = swarm_fitness.copy()\n\n                global_best_index = np.argmin(personal_best_fitness)\n                global_best_position = personal_best_positions[global_best_index].copy()\n                global_best_fitness = personal_best_fitness[global_best_index]\n                center = global_best_position\n\n            # Adjust Cauchy scale based on success rate\n            if iterations > 0:\n                self.success_rate = successes / iterations\n                if self.success_rate > 0.2:\n                    self.cauchy_scale *= 1.1\n                else:\n                    self.cauchy_scale *= 0.9\n                successes = 0\n                iterations = 0\n\n            self.cauchy_scale = np.clip(self.cauchy_scale, 0.01, 1.0)\n\n        # Local Search\n        for _ in range(min(self.local_search_iterations, self.budget)):\n             if self.budget <= 0:\n                 break\n\n             perturbation = np.random.uniform(-search_range/100, search_range/100, self.dim)\n             local_x = np.clip(global_best_position + perturbation, lb, ub)\n             local_f = func(local_x)\n             self.budget -=1\n\n             if local_f < global_best_fitness:\n                  global_best_fitness = local_f\n                  global_best_position = local_x\n\n        return global_best_fitness, global_best_position\n```", "operator": "crossover", "theta_sampled": 0.015091592340404868, "reward": 0.0, "generation": 8}, "task_prompt": ""}
{"id": "01ff5a81-5d74-4915-9216-b5983d0dd845", "fitness": 0.0, "name": "AdaptiveHybridBirdSwarm", "description": "An adaptive hybrid algorithm combining Bird Swarm Optimization with Cauchy mutation and Simulated Annealing-inspired acceptance to escape local optima, with dynamic parameter adjustments based on success rate and temperature.", "code": "import numpy as np\n\nclass AdaptiveHybridBirdSwarm:\n    def __init__(self, budget=10000, dim=10, swarm_size=20, inertia_weight=0.7, cognitive_coeff=1.4, social_coeff=1.4,\n                 neighborhood_size=3, initial_cauchy_scale=0.1, cauchy_probability=0.2, initial_temp=1.0, cooling_rate=0.95):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = swarm_size\n        self.inertia_weight = inertia_weight\n        self.cognitive_coeff = cognitive_coeff\n        self.social_coeff = social_coeff\n        self.neighborhood_size = neighborhood_size\n        self.cauchy_scale = initial_cauchy_scale\n        self.cauchy_probability = cauchy_probability\n        self.temp = initial_temp\n        self.cooling_rate = cooling_rate\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        search_range = ub - lb\n\n        # Initialize swarm\n        swarm_positions = np.random.uniform(lb, ub, size=(self.swarm_size, self.dim))\n        swarm_velocities = np.random.uniform(-abs(ub-lb)/10, abs(ub-lb)/10, size=(self.swarm_size, self.dim))\n        swarm_fitness = np.array([func(x) for x in swarm_positions])\n        self.budget -= self.swarm_size\n\n        personal_best_positions = swarm_positions.copy()\n        personal_best_fitness = swarm_fitness.copy()\n\n        global_best_index = np.argmin(personal_best_fitness)\n        global_best_position = swarm_positions[global_best_index].copy()\n        global_best_fitness = personal_best_fitness[global_best_index]\n\n        successes = 0\n        iterations = 0\n        success_rate = 0.5\n        adaptation_period = 10 #Parameter to control when adaptation starts\n\n        while self.budget > 0:\n            for i in range(self.swarm_size):\n                if np.random.rand() < self.cauchy_probability:\n                    # Adaptive Cauchy Mutation with Simulated Annealing-inspired Acceptance\n                    mutation = global_best_position + search_range * np.random.standard_cauchy(size=self.dim) * self.cauchy_scale\n                    mutation = np.clip(mutation, lb, ub)\n\n                    new_fitness = func(mutation)\n                    self.budget -= 1\n                    iterations += 1\n\n                    delta = new_fitness - personal_best_fitness[i]\n                    if delta < 0 or np.random.rand() < np.exp(-delta / self.temp):  #SA-like acceptance\n                        personal_best_fitness[i] = new_fitness\n                        personal_best_positions[i] = mutation.copy()\n                        successes += 1\n\n                        if new_fitness < global_best_fitness:\n                            global_best_fitness = new_fitness\n                            global_best_position = mutation.copy()\n                    swarm_positions[i] = mutation\n\n                else:\n                    # Bird Swarm Optimization Update\n                    inertia = self.inertia_weight * swarm_velocities[i]\n                    cognitive = self.cognitive_coeff * np.random.rand(self.dim) * (personal_best_positions[i] - swarm_positions[i])\n\n                    neighbors_indices = np.random.choice(self.swarm_size, size=self.neighborhood_size, replace=False)\n                    neighbors_fitness = personal_best_fitness[neighbors_indices]\n                    best_neighbor_index = neighbors_indices[np.argmin(neighbors_fitness)]\n                    social = self.social_coeff * np.random.rand(self.dim) * (personal_best_positions[best_neighbor_index] - swarm_positions[i])\n\n                    swarm_velocities[i] = inertia + cognitive + social\n\n                    new_position = swarm_positions[i] + swarm_velocities[i]\n                    new_position = np.clip(new_position, lb, ub)\n\n                    new_fitness = func(new_position)\n                    self.budget -= 1\n\n                    delta = new_fitness - personal_best_fitness[i]\n                    if delta < 0 or np.random.rand() < np.exp(-delta / self.temp):\n                        personal_best_fitness[i] = new_fitness\n                        personal_best_positions[i] = new_position.copy()\n\n                        if new_fitness < global_best_fitness:\n                            global_best_fitness = new_fitness\n                            global_best_position = new_position.copy()\n                    swarm_positions[i] = new_position\n\n            # Parameter Adaptation (only after an initial period)\n            if self.budget < budget - adaptation_period * self.swarm_size: #Wait to adapt.\n                if iterations > 0:\n                    success_rate = successes / iterations\n                    if success_rate > 0.3:\n                        self.cauchy_scale *= 1.05\n                        self.inertia_weight = min(self.inertia_weight * 1.01, 0.9)  #Encourage exploration\n                    else:\n                        self.cauchy_scale *= 0.95\n                        self.inertia_weight = max(self.inertia_weight * 0.99, 0.1)   #Exploitation\n\n                    successes = 0\n                    iterations = 0\n                self.cauchy_scale = np.clip(self.cauchy_scale, 0.01, 1.0)  # Limit Cauchy scale\n                self.inertia_weight = np.clip(self.inertia_weight, 0.1, 0.9)\n                self.temp *= self.cooling_rate #Cooling\n                self.temp = max(self.temp, 0.01)\n\n            search_range *= 0.99 # Gradual search space reduction.\n\n        return global_best_fitness, global_best_position", "configspace": "", "generation": 8, "feedback": "", "error": "Evaluation error: name 'budget' is not defined", "parent_ids": ["5a5b4ea3-56ab-41f9-a2ca-efb45b241d1b"], "operator": "mutation", "metadata": {"llm_response": "# Description: An adaptive hybrid algorithm combining Bird Swarm Optimization with Cauchy mutation and Simulated Annealing-inspired acceptance to escape local optima, with dynamic parameter adjustments based on success rate and temperature.\n# Code: \n```python\nimport numpy as np\n\nclass AdaptiveHybridBirdSwarm:\n    def __init__(self, budget=10000, dim=10, swarm_size=20, inertia_weight=0.7, cognitive_coeff=1.4, social_coeff=1.4,\n                 neighborhood_size=3, initial_cauchy_scale=0.1, cauchy_probability=0.2, initial_temp=1.0, cooling_rate=0.95):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = swarm_size\n        self.inertia_weight = inertia_weight\n        self.cognitive_coeff = cognitive_coeff\n        self.social_coeff = social_coeff\n        self.neighborhood_size = neighborhood_size\n        self.cauchy_scale = initial_cauchy_scale\n        self.cauchy_probability = cauchy_probability\n        self.temp = initial_temp\n        self.cooling_rate = cooling_rate\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        search_range = ub - lb\n\n        # Initialize swarm\n        swarm_positions = np.random.uniform(lb, ub, size=(self.swarm_size, self.dim))\n        swarm_velocities = np.random.uniform(-abs(ub-lb)/10, abs(ub-lb)/10, size=(self.swarm_size, self.dim))\n        swarm_fitness = np.array([func(x) for x in swarm_positions])\n        self.budget -= self.swarm_size\n\n        personal_best_positions = swarm_positions.copy()\n        personal_best_fitness = swarm_fitness.copy()\n\n        global_best_index = np.argmin(personal_best_fitness)\n        global_best_position = swarm_positions[global_best_index].copy()\n        global_best_fitness = personal_best_fitness[global_best_index]\n\n        successes = 0\n        iterations = 0\n        success_rate = 0.5\n        adaptation_period = 10 #Parameter to control when adaptation starts\n\n        while self.budget > 0:\n            for i in range(self.swarm_size):\n                if np.random.rand() < self.cauchy_probability:\n                    # Adaptive Cauchy Mutation with Simulated Annealing-inspired Acceptance\n                    mutation = global_best_position + search_range * np.random.standard_cauchy(size=self.dim) * self.cauchy_scale\n                    mutation = np.clip(mutation, lb, ub)\n\n                    new_fitness = func(mutation)\n                    self.budget -= 1\n                    iterations += 1\n\n                    delta = new_fitness - personal_best_fitness[i]\n                    if delta < 0 or np.random.rand() < np.exp(-delta / self.temp):  #SA-like acceptance\n                        personal_best_fitness[i] = new_fitness\n                        personal_best_positions[i] = mutation.copy()\n                        successes += 1\n\n                        if new_fitness < global_best_fitness:\n                            global_best_fitness = new_fitness\n                            global_best_position = mutation.copy()\n                    swarm_positions[i] = mutation\n\n                else:\n                    # Bird Swarm Optimization Update\n                    inertia = self.inertia_weight * swarm_velocities[i]\n                    cognitive = self.cognitive_coeff * np.random.rand(self.dim) * (personal_best_positions[i] - swarm_positions[i])\n\n                    neighbors_indices = np.random.choice(self.swarm_size, size=self.neighborhood_size, replace=False)\n                    neighbors_fitness = personal_best_fitness[neighbors_indices]\n                    best_neighbor_index = neighbors_indices[np.argmin(neighbors_fitness)]\n                    social = self.social_coeff * np.random.rand(self.dim) * (personal_best_positions[best_neighbor_index] - swarm_positions[i])\n\n                    swarm_velocities[i] = inertia + cognitive + social\n\n                    new_position = swarm_positions[i] + swarm_velocities[i]\n                    new_position = np.clip(new_position, lb, ub)\n\n                    new_fitness = func(new_position)\n                    self.budget -= 1\n\n                    delta = new_fitness - personal_best_fitness[i]\n                    if delta < 0 or np.random.rand() < np.exp(-delta / self.temp):\n                        personal_best_fitness[i] = new_fitness\n                        personal_best_positions[i] = new_position.copy()\n\n                        if new_fitness < global_best_fitness:\n                            global_best_fitness = new_fitness\n                            global_best_position = new_position.copy()\n                    swarm_positions[i] = new_position\n\n            # Parameter Adaptation (only after an initial period)\n            if self.budget < budget - adaptation_period * self.swarm_size: #Wait to adapt.\n                if iterations > 0:\n                    success_rate = successes / iterations\n                    if success_rate > 0.3:\n                        self.cauchy_scale *= 1.05\n                        self.inertia_weight = min(self.inertia_weight * 1.01, 0.9)  #Encourage exploration\n                    else:\n                        self.cauchy_scale *= 0.95\n                        self.inertia_weight = max(self.inertia_weight * 0.99, 0.1)   #Exploitation\n\n                    successes = 0\n                    iterations = 0\n                self.cauchy_scale = np.clip(self.cauchy_scale, 0.01, 1.0)  # Limit Cauchy scale\n                self.inertia_weight = np.clip(self.inertia_weight, 0.1, 0.9)\n                self.temp *= self.cooling_rate #Cooling\n                self.temp = max(self.temp, 0.01)\n\n            search_range *= 0.99 # Gradual search space reduction.\n\n        return global_best_fitness, global_best_position\n```", "operator": "mutation", "theta_sampled": 0.005421797390066605, "reward": 0.0, "generation": 8}, "task_prompt": ""}
{"id": "a5b61a20-1389-4908-aff1-63b377601d9f", "fitness": 0.6614344698955289, "name": "EnhancedHybridBirdSwarm", "description": "A hybrid bird swarm optimization algorithm with adaptive Cauchy mutation and dynamic parameter adjustment, incorporating a local search phase for refinement.", "code": "import numpy as np\n\nclass EnhancedHybridBirdSwarm:\n    def __init__(self, budget=10000, dim=10, swarm_size=20, inertia_weight=0.7, cognitive_coeff=1.4, social_coeff=1.4, neighborhood_size=3, shrink_factor=0.95, initial_cauchy_scale=0.1, local_search_iterations=5, cauchy_probability=0.2, stagnation_threshold=10):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = swarm_size\n        self.inertia_weight = inertia_weight\n        self.cognitive_coeff = cognitive_coeff\n        self.social_coeff = social_coeff\n        self.neighborhood_size = neighborhood_size\n        self.shrink_factor = shrink_factor\n        self.cauchy_scale = initial_cauchy_scale\n        self.local_search_iterations = local_search_iterations\n        self.cauchy_probability = cauchy_probability\n        self.success_rate = 0.5\n        self.stagnation_threshold = stagnation_threshold\n        self.stagnation_counter = 0\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        \n        # Initialize swarm positions and velocities\n        swarm_positions = np.random.uniform(lb, ub, size=(self.swarm_size, self.dim))\n        swarm_velocities = np.random.uniform(-abs(ub-lb)/10, abs(ub-lb)/10, size=(self.swarm_size, self.dim))\n        swarm_fitness = np.array([func(x) for x in swarm_positions])\n        self.budget -= self.swarm_size\n        \n        personal_best_positions = swarm_positions.copy()\n        personal_best_fitness = swarm_fitness.copy()\n        \n        global_best_index = np.argmin(personal_best_fitness)\n        global_best_position = personal_best_positions[global_best_index].copy()\n        global_best_fitness = personal_best_fitness[global_best_index]\n        \n        search_range = ub - lb\n        center = global_best_position\n\n        successes = 0\n        iterations = 0\n\n        previous_best_fitness = global_best_fitness\n        \n        while self.budget > 0:\n            for i in range(self.swarm_size):\n                if self.budget <= 0:\n                    break\n                # Bird Swarm Optimization part\n                inertia = self.inertia_weight * swarm_velocities[i]\n                cognitive = self.cognitive_coeff * np.random.rand(self.dim) * (personal_best_positions[i] - swarm_positions[i])\n                \n                # Social component: find best neighbor\n                neighbors_indices = np.random.choice(self.swarm_size, size=self.neighborhood_size, replace=False)\n                neighbors_fitness = personal_best_fitness[neighbors_indices]\n                best_neighbor_index = neighbors_indices[np.argmin(neighbors_fitness)]\n                social = self.social_coeff * np.random.rand(self.dim) * (personal_best_positions[best_neighbor_index] - swarm_positions[i])\n                \n                swarm_velocities[i] = inertia + cognitive + social\n                \n                # Update position\n                new_position = swarm_positions[i] + swarm_velocities[i]\n\n                # Adaptive Cauchy mutation\n                if np.random.rand() < self.cauchy_probability:\n                    mutation = center + search_range * np.random.standard_cauchy(size=self.dim) * self.cauchy_scale\n                    new_position = mutation\n\n                # Boundary handling\n                new_position = np.clip(new_position, lb, ub)\n                \n                new_fitness = func(new_position)\n                self.budget -= 1\n                iterations += 1\n                \n                if new_fitness < personal_best_fitness[i]:\n                    personal_best_fitness[i] = new_fitness\n                    personal_best_positions[i] = new_position.copy()\n                    successes += 1\n                    \n                    if new_fitness < global_best_fitness:\n                        global_best_fitness = new_fitness\n                        global_best_position = new_position.copy()\n                        center = global_best_position\n\n                swarm_positions[i] = new_position\n\n            # Shrink search range\n            search_range *= self.shrink_factor\n            lb = np.clip(center - search_range/2, func.bounds.lb, func.bounds.ub)\n            ub = np.clip(center + search_range/2, func.bounds.lb, func.bounds.ub)\n            \n            # Adjust Cauchy scale based on success rate\n            if iterations > 0:\n                self.success_rate = successes / iterations\n                if self.success_rate > 0.2:\n                    self.cauchy_scale *= 1.1\n                else:\n                    self.cauchy_scale *= 0.9\n                successes = 0\n                iterations = 0\n\n            self.cauchy_scale = np.clip(self.cauchy_scale, 0.01, 1.0)\n\n            # Dynamic parameter adjustment based on stagnation\n            if abs(global_best_fitness - previous_best_fitness) < 1e-6:\n                self.stagnation_counter += 1\n                if self.stagnation_counter > self.stagnation_threshold:\n                    # Increase exploration: reset Cauchy scale, increase mutation probability\n                    self.cauchy_scale = 0.2  # Reset cauchy scale\n                    self.cauchy_probability = min(self.cauchy_probability * 1.2, 0.5)  # Increase probability\n                    # Perturb the swarm to escape local optima\n                    for i in range(self.swarm_size):\n                         swarm_positions[i] = np.clip(swarm_positions[i] + np.random.uniform(-search_range/5, search_range/5, self.dim), lb, ub) #Large perturbation\n                    \n                    self.stagnation_counter = 0  # Reset stagnation counter\n            else:\n                self.stagnation_counter = 0\n\n            previous_best_fitness = global_best_fitness\n        \n        # Local Search with more intensity around the best\n        local_search_range = search_range / 100 #Start small\n        for _ in range(min(self.local_search_iterations, self.budget)):\n             if self.budget <= 0:\n                 break\n\n             perturbation = np.random.uniform(-local_search_range, local_search_range, self.dim)\n             local_x = np.clip(global_best_position + perturbation, lb, ub)\n             local_f = func(local_x)\n             self.budget -=1\n\n             if local_f < global_best_fitness:\n                  global_best_fitness = local_f\n                  global_best_position = local_x\n\n             local_search_range *= 0.9 #Reduce range each iteration\n\n        return global_best_fitness, global_best_position", "configspace": "", "generation": 8, "feedback": "The algorithm EnhancedHybridBirdSwarm scored 0.661 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["5ed072b3-d57e-4c1e-8fc4-1c2b4f7cf697", "df8a52a3-838e-4304-b60e-76e5afa6152f"], "operator": "crossover", "metadata": {"llm_response": "# Description: A hybrid bird swarm optimization algorithm with adaptive Cauchy mutation and dynamic parameter adjustment, incorporating a local search phase for refinement.\n# Code: \n```python\nimport numpy as np\n\nclass EnhancedHybridBirdSwarm:\n    def __init__(self, budget=10000, dim=10, swarm_size=20, inertia_weight=0.7, cognitive_coeff=1.4, social_coeff=1.4, neighborhood_size=3, shrink_factor=0.95, initial_cauchy_scale=0.1, local_search_iterations=5, cauchy_probability=0.2, stagnation_threshold=10):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = swarm_size\n        self.inertia_weight = inertia_weight\n        self.cognitive_coeff = cognitive_coeff\n        self.social_coeff = social_coeff\n        self.neighborhood_size = neighborhood_size\n        self.shrink_factor = shrink_factor\n        self.cauchy_scale = initial_cauchy_scale\n        self.local_search_iterations = local_search_iterations\n        self.cauchy_probability = cauchy_probability\n        self.success_rate = 0.5\n        self.stagnation_threshold = stagnation_threshold\n        self.stagnation_counter = 0\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        \n        # Initialize swarm positions and velocities\n        swarm_positions = np.random.uniform(lb, ub, size=(self.swarm_size, self.dim))\n        swarm_velocities = np.random.uniform(-abs(ub-lb)/10, abs(ub-lb)/10, size=(self.swarm_size, self.dim))\n        swarm_fitness = np.array([func(x) for x in swarm_positions])\n        self.budget -= self.swarm_size\n        \n        personal_best_positions = swarm_positions.copy()\n        personal_best_fitness = swarm_fitness.copy()\n        \n        global_best_index = np.argmin(personal_best_fitness)\n        global_best_position = personal_best_positions[global_best_index].copy()\n        global_best_fitness = personal_best_fitness[global_best_index]\n        \n        search_range = ub - lb\n        center = global_best_position\n\n        successes = 0\n        iterations = 0\n\n        previous_best_fitness = global_best_fitness\n        \n        while self.budget > 0:\n            for i in range(self.swarm_size):\n                if self.budget <= 0:\n                    break\n                # Bird Swarm Optimization part\n                inertia = self.inertia_weight * swarm_velocities[i]\n                cognitive = self.cognitive_coeff * np.random.rand(self.dim) * (personal_best_positions[i] - swarm_positions[i])\n                \n                # Social component: find best neighbor\n                neighbors_indices = np.random.choice(self.swarm_size, size=self.neighborhood_size, replace=False)\n                neighbors_fitness = personal_best_fitness[neighbors_indices]\n                best_neighbor_index = neighbors_indices[np.argmin(neighbors_fitness)]\n                social = self.social_coeff * np.random.rand(self.dim) * (personal_best_positions[best_neighbor_index] - swarm_positions[i])\n                \n                swarm_velocities[i] = inertia + cognitive + social\n                \n                # Update position\n                new_position = swarm_positions[i] + swarm_velocities[i]\n\n                # Adaptive Cauchy mutation\n                if np.random.rand() < self.cauchy_probability:\n                    mutation = center + search_range * np.random.standard_cauchy(size=self.dim) * self.cauchy_scale\n                    new_position = mutation\n\n                # Boundary handling\n                new_position = np.clip(new_position, lb, ub)\n                \n                new_fitness = func(new_position)\n                self.budget -= 1\n                iterations += 1\n                \n                if new_fitness < personal_best_fitness[i]:\n                    personal_best_fitness[i] = new_fitness\n                    personal_best_positions[i] = new_position.copy()\n                    successes += 1\n                    \n                    if new_fitness < global_best_fitness:\n                        global_best_fitness = new_fitness\n                        global_best_position = new_position.copy()\n                        center = global_best_position\n\n                swarm_positions[i] = new_position\n\n            # Shrink search range\n            search_range *= self.shrink_factor\n            lb = np.clip(center - search_range/2, func.bounds.lb, func.bounds.ub)\n            ub = np.clip(center + search_range/2, func.bounds.lb, func.bounds.ub)\n            \n            # Adjust Cauchy scale based on success rate\n            if iterations > 0:\n                self.success_rate = successes / iterations\n                if self.success_rate > 0.2:\n                    self.cauchy_scale *= 1.1\n                else:\n                    self.cauchy_scale *= 0.9\n                successes = 0\n                iterations = 0\n\n            self.cauchy_scale = np.clip(self.cauchy_scale, 0.01, 1.0)\n\n            # Dynamic parameter adjustment based on stagnation\n            if abs(global_best_fitness - previous_best_fitness) < 1e-6:\n                self.stagnation_counter += 1\n                if self.stagnation_counter > self.stagnation_threshold:\n                    # Increase exploration: reset Cauchy scale, increase mutation probability\n                    self.cauchy_scale = 0.2  # Reset cauchy scale\n                    self.cauchy_probability = min(self.cauchy_probability * 1.2, 0.5)  # Increase probability\n                    # Perturb the swarm to escape local optima\n                    for i in range(self.swarm_size):\n                         swarm_positions[i] = np.clip(swarm_positions[i] + np.random.uniform(-search_range/5, search_range/5, self.dim), lb, ub) #Large perturbation\n                    \n                    self.stagnation_counter = 0  # Reset stagnation counter\n            else:\n                self.stagnation_counter = 0\n\n            previous_best_fitness = global_best_fitness\n        \n        # Local Search with more intensity around the best\n        local_search_range = search_range / 100 #Start small\n        for _ in range(min(self.local_search_iterations, self.budget)):\n             if self.budget <= 0:\n                 break\n\n             perturbation = np.random.uniform(-local_search_range, local_search_range, self.dim)\n             local_x = np.clip(global_best_position + perturbation, lb, ub)\n             local_f = func(local_x)\n             self.budget -=1\n\n             if local_f < global_best_fitness:\n                  global_best_fitness = local_f\n                  global_best_position = local_x\n\n             local_search_range *= 0.9 #Reduce range each iteration\n\n        return global_best_fitness, global_best_position\n```", "aucs": [0.33520136946583456, 0.20386706849716707, 0.8089002123491895, 0.9186264002554038, 0.821848015584695, 0.8197312470934255, 0.32960535758027676, 0.7464640863046994, 0.8069515098535361, 0.8109561561232634, 0.9215150391813016, 0.996326048677885, 0.3277954180318541, 0.40269420053216265, 0.846309316729385, 0.8409733126574405, 0.6777800786613306, 0.8817084527377346, 0.23235903331241892, 0.4990770742815759], "operator": "crossover", "theta_sampled": 0.021404976558947797, "reward": 0.0, "generation": 8}, "task_prompt": ""}
{"id": "a59e4969-8f47-4293-8622-3e4ef39ee479", "fitness": 0.6816655513857703, "name": "AdaptiveHybridBirdSwarm", "description": "A hybrid bird swarm optimization algorithm with dynamic Cauchy mutation and adaptive parameter control for enhanced exploration and exploitation.", "code": "import numpy as np\n\nclass AdaptiveHybridBirdSwarm:\n    def __init__(self, budget=10000, dim=10, swarm_size=20, inertia_weight=0.7, cognitive_coeff=1.4, social_coeff=1.4, neighborhood_size=3, shrink_factor=0.95, initial_cauchy_scale=0.1, local_search_iterations=5, cauchy_probability=0.2, exploration_probability=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = swarm_size\n        self.inertia_weight = inertia_weight\n        self.cognitive_coeff = cognitive_coeff\n        self.social_coeff = social_coeff\n        self.neighborhood_size = neighborhood_size\n        self.shrink_factor = shrink_factor\n        self.cauchy_scale = initial_cauchy_scale\n        self.local_search_iterations = local_search_iterations\n        self.cauchy_probability = cauchy_probability\n        self.exploration_probability = exploration_probability\n        self.success_rate = 0.5\n        self.inertia_weight_decay = 0.995  # Decay factor for inertia weight\n        self.cognitive_coeff_increase = 1.005  # Increase factor for cognitive coefficient\n        self.social_coeff_decrease = 0.995  # Decrease factor for social coefficient\n        self.min_inertia_weight = 0.4 #Minimum intertia weight value\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        \n        # Initialize swarm positions and velocities\n        swarm_positions = np.random.uniform(lb, ub, size=(self.swarm_size, self.dim))\n        swarm_velocities = np.random.uniform(-abs(ub-lb)/10, abs(ub-lb)/10, size=(self.swarm_size, self.dim))\n        swarm_fitness = np.array([func(x) for x in swarm_positions])\n        self.budget -= self.swarm_size\n        \n        personal_best_positions = swarm_positions.copy()\n        personal_best_fitness = swarm_fitness.copy()\n        \n        global_best_index = np.argmin(personal_best_fitness)\n        global_best_position = personal_best_positions[global_best_index].copy()\n        global_best_fitness = personal_best_fitness[global_best_index]\n        \n        search_range = ub - lb\n        center = global_best_position #Use global best as center for Cauchy\n\n        successes = 0\n        iterations = 0\n\n        while self.budget > 0:\n            for i in range(self.swarm_size):\n                if self.budget <= 0:\n                    break\n\n                # Exploration phase: Random jump with a probability\n                if np.random.rand() < self.exploration_probability:\n                    new_position = np.random.uniform(lb, ub, size=self.dim)\n                else:\n                    # Bird Swarm Optimization part\n                    inertia = self.inertia_weight * swarm_velocities[i]\n                    cognitive = self.cognitive_coeff * np.random.rand(self.dim) * (personal_best_positions[i] - swarm_positions[i])\n                    \n                    # Social component: find best neighbor\n                    neighbors_indices = np.random.choice(self.swarm_size, size=self.neighborhood_size, replace=False)\n                    neighbors_fitness = personal_best_fitness[neighbors_indices]\n                    best_neighbor_index = neighbors_indices[np.argmin(neighbors_fitness)]\n                    social = self.social_coeff * np.random.rand(self.dim) * (personal_best_positions[best_neighbor_index] - swarm_positions[i])\n                    \n                    swarm_velocities[i] = inertia + cognitive + social\n                    \n                    # Update position\n                    new_position = swarm_positions[i] + swarm_velocities[i]\n\n                # Cauchy mutation with probability cauchy_probability\n                if np.random.rand() < self.cauchy_probability:\n                    mutation = center + search_range * np.random.standard_cauchy(size=self.dim) * self.cauchy_scale  # Scale Cauchy distribution\n                    new_position = mutation\n                \n                # Boundary handling\n                new_position = np.clip(new_position, lb, ub)\n                \n                new_fitness = func(new_position)\n                self.budget -= 1\n                iterations += 1\n                \n                if new_fitness < personal_best_fitness[i]:\n                    personal_best_fitness[i] = new_fitness\n                    personal_best_positions[i] = new_position.copy()\n                    successes += 1\n                    \n                    if new_fitness < global_best_fitness:\n                        global_best_fitness = new_fitness\n                        global_best_position = new_position.copy()\n                        center = global_best_position #Update center\n\n                swarm_positions[i] = new_position\n                \n            # Shrink search range\n            search_range *= self.shrink_factor\n            lb = np.clip(center - search_range/2, func.bounds.lb, func.bounds.ub)\n            ub = np.clip(center + search_range/2, func.bounds.lb, func.bounds.ub)\n\n            # Dynamic Parameter Control\n            self.inertia_weight = max(self.inertia_weight * self.inertia_weight_decay, self.min_inertia_weight)\n            self.cognitive_coeff *= self.cognitive_coeff_increase\n            self.social_coeff *= self.social_coeff_decrease\n            \n            # Adjust Cauchy scale based on success rate\n            if iterations > 0:\n                self.success_rate = successes / iterations\n                if self.success_rate > 0.2:\n                    self.cauchy_scale *= 1.1  # Increase scale if successful\n                else:\n                    self.cauchy_scale *= 0.9  # Decrease scale if not successful\n                successes = 0\n                iterations = 0\n\n            self.cauchy_scale = np.clip(self.cauchy_scale, 0.01, 1.0)  # Limit Cauchy scale\n        \n        # Local Search\n        for _ in range(min(self.local_search_iterations, self.budget)):\n             if self.budget <= 0:\n                 break\n\n             perturbation = np.random.uniform(-search_range/100, search_range/100, self.dim)\n             local_x = np.clip(global_best_position + perturbation, lb, ub)\n             local_f = func(local_x)\n             self.budget -=1\n\n             if local_f < global_best_fitness:\n                  global_best_fitness = local_f\n                  global_best_position = local_x\n\n\n        return global_best_fitness, global_best_position", "configspace": "", "generation": 8, "feedback": "The algorithm AdaptiveHybridBirdSwarm scored 0.682 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["5ed072b3-d57e-4c1e-8fc4-1c2b4f7cf697", "df8a52a3-838e-4304-b60e-76e5afa6152f"], "operator": "crossover", "metadata": {"llm_response": "# Description: A hybrid bird swarm optimization algorithm with dynamic Cauchy mutation and adaptive parameter control for enhanced exploration and exploitation.\n# Code: \n```python\nimport numpy as np\n\nclass AdaptiveHybridBirdSwarm:\n    def __init__(self, budget=10000, dim=10, swarm_size=20, inertia_weight=0.7, cognitive_coeff=1.4, social_coeff=1.4, neighborhood_size=3, shrink_factor=0.95, initial_cauchy_scale=0.1, local_search_iterations=5, cauchy_probability=0.2, exploration_probability=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = swarm_size\n        self.inertia_weight = inertia_weight\n        self.cognitive_coeff = cognitive_coeff\n        self.social_coeff = social_coeff\n        self.neighborhood_size = neighborhood_size\n        self.shrink_factor = shrink_factor\n        self.cauchy_scale = initial_cauchy_scale\n        self.local_search_iterations = local_search_iterations\n        self.cauchy_probability = cauchy_probability\n        self.exploration_probability = exploration_probability\n        self.success_rate = 0.5\n        self.inertia_weight_decay = 0.995  # Decay factor for inertia weight\n        self.cognitive_coeff_increase = 1.005  # Increase factor for cognitive coefficient\n        self.social_coeff_decrease = 0.995  # Decrease factor for social coefficient\n        self.min_inertia_weight = 0.4 #Minimum intertia weight value\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        \n        # Initialize swarm positions and velocities\n        swarm_positions = np.random.uniform(lb, ub, size=(self.swarm_size, self.dim))\n        swarm_velocities = np.random.uniform(-abs(ub-lb)/10, abs(ub-lb)/10, size=(self.swarm_size, self.dim))\n        swarm_fitness = np.array([func(x) for x in swarm_positions])\n        self.budget -= self.swarm_size\n        \n        personal_best_positions = swarm_positions.copy()\n        personal_best_fitness = swarm_fitness.copy()\n        \n        global_best_index = np.argmin(personal_best_fitness)\n        global_best_position = personal_best_positions[global_best_index].copy()\n        global_best_fitness = personal_best_fitness[global_best_index]\n        \n        search_range = ub - lb\n        center = global_best_position #Use global best as center for Cauchy\n\n        successes = 0\n        iterations = 0\n\n        while self.budget > 0:\n            for i in range(self.swarm_size):\n                if self.budget <= 0:\n                    break\n\n                # Exploration phase: Random jump with a probability\n                if np.random.rand() < self.exploration_probability:\n                    new_position = np.random.uniform(lb, ub, size=self.dim)\n                else:\n                    # Bird Swarm Optimization part\n                    inertia = self.inertia_weight * swarm_velocities[i]\n                    cognitive = self.cognitive_coeff * np.random.rand(self.dim) * (personal_best_positions[i] - swarm_positions[i])\n                    \n                    # Social component: find best neighbor\n                    neighbors_indices = np.random.choice(self.swarm_size, size=self.neighborhood_size, replace=False)\n                    neighbors_fitness = personal_best_fitness[neighbors_indices]\n                    best_neighbor_index = neighbors_indices[np.argmin(neighbors_fitness)]\n                    social = self.social_coeff * np.random.rand(self.dim) * (personal_best_positions[best_neighbor_index] - swarm_positions[i])\n                    \n                    swarm_velocities[i] = inertia + cognitive + social\n                    \n                    # Update position\n                    new_position = swarm_positions[i] + swarm_velocities[i]\n\n                # Cauchy mutation with probability cauchy_probability\n                if np.random.rand() < self.cauchy_probability:\n                    mutation = center + search_range * np.random.standard_cauchy(size=self.dim) * self.cauchy_scale  # Scale Cauchy distribution\n                    new_position = mutation\n                \n                # Boundary handling\n                new_position = np.clip(new_position, lb, ub)\n                \n                new_fitness = func(new_position)\n                self.budget -= 1\n                iterations += 1\n                \n                if new_fitness < personal_best_fitness[i]:\n                    personal_best_fitness[i] = new_fitness\n                    personal_best_positions[i] = new_position.copy()\n                    successes += 1\n                    \n                    if new_fitness < global_best_fitness:\n                        global_best_fitness = new_fitness\n                        global_best_position = new_position.copy()\n                        center = global_best_position #Update center\n\n                swarm_positions[i] = new_position\n                \n            # Shrink search range\n            search_range *= self.shrink_factor\n            lb = np.clip(center - search_range/2, func.bounds.lb, func.bounds.ub)\n            ub = np.clip(center + search_range/2, func.bounds.lb, func.bounds.ub)\n\n            # Dynamic Parameter Control\n            self.inertia_weight = max(self.inertia_weight * self.inertia_weight_decay, self.min_inertia_weight)\n            self.cognitive_coeff *= self.cognitive_coeff_increase\n            self.social_coeff *= self.social_coeff_decrease\n            \n            # Adjust Cauchy scale based on success rate\n            if iterations > 0:\n                self.success_rate = successes / iterations\n                if self.success_rate > 0.2:\n                    self.cauchy_scale *= 1.1  # Increase scale if successful\n                else:\n                    self.cauchy_scale *= 0.9  # Decrease scale if not successful\n                successes = 0\n                iterations = 0\n\n            self.cauchy_scale = np.clip(self.cauchy_scale, 0.01, 1.0)  # Limit Cauchy scale\n        \n        # Local Search\n        for _ in range(min(self.local_search_iterations, self.budget)):\n             if self.budget <= 0:\n                 break\n\n             perturbation = np.random.uniform(-search_range/100, search_range/100, self.dim)\n             local_x = np.clip(global_best_position + perturbation, lb, ub)\n             local_f = func(local_x)\n             self.budget -=1\n\n             if local_f < global_best_fitness:\n                  global_best_fitness = local_f\n                  global_best_position = local_x\n\n\n        return global_best_fitness, global_best_position\n```", "aucs": [0.24133993510560525, 0.4284715119113536, 0.8127141047104244, 0.9231422597047734, 0.8392656541387715, 0.8509597286130017, 0.3741454453196187, 0.7637628191773513, 0.8415484402148762, 0.8301278161765946, 0.9352102644648592, 0.9958299258780289, 0.2437452449355283, 0.8436836135950028, 0.8056590342999776, 0.8530744692389032, 0.3841581096044334, 0.9043579666933761, 0.23603765883158512, 0.5260770251013434], "operator": "crossover", "theta_sampled": 0.02124562636347333, "reward": 0.0, "generation": 8}, "task_prompt": ""}
{"id": "8b2e8c51-7960-42f2-ac3a-7504b36f5176", "fitness": 0.0, "name": "GaussianProcessOptimization", "description": "A population-based algorithm employing a Gaussian process surrogate model to guide exploration and exploitation.", "code": "import numpy as np\nfrom sklearn.gaussian_process import GaussianProcessRegressor\nfrom sklearn.gaussian_process.kernels import RBF, ConstantKernel as C\n\nclass GaussianProcessOptimization:\n    def __init__(self, budget=10000, dim=10, n_initial_samples=10, exploration_weight=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.n_initial_samples = n_initial_samples\n        self.exploration_weight = exploration_weight\n        self.X = None\n        self.y = None\n        self.gpr = None\n\n    def initialize(self, func):\n        # Generate initial samples randomly\n        X_init = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.n_initial_samples, self.dim))\n        y_init = np.array([func(x) for x in X_init])\n\n        self.X = X_init\n        self.y = y_init\n        \n    def train_gpr(self):\n        # Define Gaussian Process Regressor\n        kernel = C(1.0, constant_value_bounds=\"fixed\") * RBF(1.0, length_scale_bounds=\"fixed\")\n        self.gpr = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=5)\n        self.gpr.fit(self.X, self.y)\n\n    def acquisition_function(self, x):\n        # Expected Improvement acquisition function\n        mu, sigma = self.gpr.predict(x.reshape(1, -1), return_std=True)\n        \n        # Add exploration by scaling sigma\n        return -mu[0] + self.exploration_weight * sigma[0]\n\n    def find_next_sample(self, func):\n        # Find the next sample by optimizing the acquisition function\n        best_x = None\n        best_acq = np.inf\n        \n        for _ in range(100): # Run local search multiple times\n            x0 = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n            \n            #Simple Gradient Descent for acquisition func, clip to bounds\n            learning_rate = 0.01 # small learning rate\n            \n            x = x0.copy()\n            for _ in range(10): # run gradient descent a few steps\n                grad = self.approximate_gradient(self.acquisition_function, x)\n                x -= learning_rate * grad\n                x = np.clip(x, func.bounds.lb, func.bounds.ub) # clip the bounds\n\n            acq = self.acquisition_function(x)\n            if acq < best_acq:\n                best_acq = acq\n                best_x = x\n        \n        return best_x\n\n    def approximate_gradient(self, func, x, epsilon=1e-5):\n        # Approximate the gradient using finite differences\n        grad = np.zeros_like(x)\n        for i in range(len(x)):\n            x_plus = x.copy()\n            x_minus = x.copy()\n            x_plus[i] += epsilon\n            x_minus[i] -= epsilon\n            grad[i] = (func(x_plus) - func(x_minus)) / (2 * epsilon)\n        return grad\n        \n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        \n        # Initialize\n        self.initialize(func)\n        \n        remaining_budget = self.budget - self.n_initial_samples\n        \n        for i in range(remaining_budget):\n            # Train Gaussian Process Regressor\n            self.train_gpr()\n            \n            # Find next sample\n            x_next = self.find_next_sample(func)\n            \n            # Evaluate the function\n            f_next = func(x_next)\n            \n            # Update the best solution\n            if f_next < self.f_opt:\n                self.f_opt = f_next\n                self.x_opt = x_next\n            \n            # Add the new sample to the data\n            self.X = np.vstack((self.X, x_next))\n            self.y = np.append(self.y, f_next)\n        \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "", "error": "Evaluation error: name 'C' is not defined", "parent_ids": [], "operator": "random_new", "metadata": {"llm_response": "# Description: A population-based algorithm employing a Gaussian process surrogate model to guide exploration and exploitation.\n\n# Code:\n```python\nimport numpy as np\nfrom sklearn.gaussian_process import GaussianProcessRegressor\nfrom sklearn.gaussian_process.kernels import RBF, ConstantKernel as C\n\nclass GaussianProcessOptimization:\n    def __init__(self, budget=10000, dim=10, n_initial_samples=10, exploration_weight=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.n_initial_samples = n_initial_samples\n        self.exploration_weight = exploration_weight\n        self.X = None\n        self.y = None\n        self.gpr = None\n\n    def initialize(self, func):\n        # Generate initial samples randomly\n        X_init = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.n_initial_samples, self.dim))\n        y_init = np.array([func(x) for x in X_init])\n\n        self.X = X_init\n        self.y = y_init\n        \n    def train_gpr(self):\n        # Define Gaussian Process Regressor\n        kernel = C(1.0, constant_value_bounds=\"fixed\") * RBF(1.0, length_scale_bounds=\"fixed\")\n        self.gpr = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=5)\n        self.gpr.fit(self.X, self.y)\n\n    def acquisition_function(self, x):\n        # Expected Improvement acquisition function\n        mu, sigma = self.gpr.predict(x.reshape(1, -1), return_std=True)\n        \n        # Add exploration by scaling sigma\n        return -mu[0] + self.exploration_weight * sigma[0]\n\n    def find_next_sample(self, func):\n        # Find the next sample by optimizing the acquisition function\n        best_x = None\n        best_acq = np.inf\n        \n        for _ in range(100): # Run local search multiple times\n            x0 = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n            \n            #Simple Gradient Descent for acquisition func, clip to bounds\n            learning_rate = 0.01 # small learning rate\n            \n            x = x0.copy()\n            for _ in range(10): # run gradient descent a few steps\n                grad = self.approximate_gradient(self.acquisition_function, x)\n                x -= learning_rate * grad\n                x = np.clip(x, func.bounds.lb, func.bounds.ub) # clip the bounds\n\n            acq = self.acquisition_function(x)\n            if acq < best_acq:\n                best_acq = acq\n                best_x = x\n        \n        return best_x\n\n    def approximate_gradient(self, func, x, epsilon=1e-5):\n        # Approximate the gradient using finite differences\n        grad = np.zeros_like(x)\n        for i in range(len(x)):\n            x_plus = x.copy()\n            x_minus = x.copy()\n            x_plus[i] += epsilon\n            x_minus[i] -= epsilon\n            grad[i] = (func(x_plus) - func(x_minus)) / (2 * epsilon)\n        return grad\n        \n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        \n        # Initialize\n        self.initialize(func)\n        \n        remaining_budget = self.budget - self.n_initial_samples\n        \n        for i in range(remaining_budget):\n            # Train Gaussian Process Regressor\n            self.train_gpr()\n            \n            # Find next sample\n            x_next = self.find_next_sample(func)\n            \n            # Evaluate the function\n            f_next = func(x_next)\n            \n            # Update the best solution\n            if f_next < self.f_opt:\n                self.f_opt = f_next\n                self.x_opt = x_next\n            \n            # Add the new sample to the data\n            self.X = np.vstack((self.X, x_next))\n            self.y = np.append(self.y, f_next)\n        \n        return self.f_opt, self.x_opt\n```", "operator": "random_new", "theta_sampled": 0.013488553147752255, "reward": 0.0, "generation": 8}, "task_prompt": ""}
{"id": "7b60ed66-28c8-47b8-b838-0ec826de6eb7", "fitness": 0.0, "name": "HybridBirdSwarmCauchyOrthogonal", "description": "Combines Bird Swarm Optimization with adaptive Cauchy mutation and orthogonal learning for enhanced exploration and exploitation.", "code": "import numpy as np\n\nclass HybridBirdSwarmCauchyOrthogonal:\n    def __init__(self, budget=10000, dim=10, swarm_size=20, inertia_weight=0.7, cognitive_coeff=1.4, social_coeff=1.4, neighborhood_size=3, shrink_factor=0.95, initial_cauchy_scale=0.1, local_search_iterations=5, cauchy_probability=0.2, orthogonal_probability=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = swarm_size\n        self.inertia_weight = inertia_weight\n        self.cognitive_coeff = cognitive_coeff\n        self.social_coeff = social_coeff\n        self.neighborhood_size = neighborhood_size\n        self.shrink_factor = shrink_factor\n        self.cauchy_scale = initial_cauchy_scale\n        self.local_search_iterations = local_search_iterations\n        self.cauchy_probability = cauchy_probability\n        self.orthogonal_probability = orthogonal_probability\n        self.success_rate = 0.5\n\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        \n        # Initialize swarm positions and velocities\n        swarm_positions = np.random.uniform(lb, ub, size=(self.swarm_size, self.dim))\n        swarm_velocities = np.random.uniform(-abs(ub-lb)/10, abs(ub-lb)/10, size=(self.swarm_size, self.dim))\n        swarm_fitness = np.array([func(x) for x in swarm_positions])\n        self.budget -= self.swarm_size\n        \n        personal_best_positions = swarm_positions.copy()\n        personal_best_fitness = swarm_fitness.copy()\n        \n        global_best_index = np.argmin(personal_best_fitness)\n        global_best_position = personal_best_positions[global_best_index].copy()\n        global_best_fitness = personal_best_fitness[global_best_index]\n        \n        search_range = ub - lb\n        center = global_best_position\n\n        successes = 0\n        iterations = 0\n\n        while self.budget > 0:\n            for i in range(self.swarm_size):\n                if self.budget <= 0:\n                    break\n                # Bird Swarm Optimization part\n                inertia = self.inertia_weight * swarm_velocities[i]\n                cognitive = self.cognitive_coeff * np.random.rand(self.dim) * (personal_best_positions[i] - swarm_positions[i])\n                \n                # Social component: find best neighbor\n                neighbors_indices = np.random.choice(self.swarm_size, size=self.neighborhood_size, replace=False)\n                neighbors_fitness = personal_best_fitness[neighbors_indices]\n                best_neighbor_index = neighbors_indices[np.argmin(neighbors_fitness)]\n                social = self.social_coeff * np.random.rand(self.dim) * (personal_best_positions[best_neighbor_index] - swarm_positions[i])\n                \n                swarm_velocities[i] = inertia + cognitive + social\n                \n                # Update position\n                new_position = swarm_positions[i] + swarm_velocities[i]\n                \n                # Boundary handling\n                new_position = np.clip(new_position, lb, ub)\n                \n                # Cauchy mutation with probability\n                if np.random.rand() < self.cauchy_probability:\n                    mutation = center + search_range * np.random.standard_cauchy(size=self.dim) * self.cauchy_scale\n                    mutation = np.clip(mutation, lb, ub)\n                    new_position = mutation\n\n                # Orthogonal Learning\n                if np.random.rand() < self.orthogonal_probability:\n                    # Create orthogonal array (simplified for demonstration)\n                    levels = 3  # Example: low, medium, high\n                    orthogonal_array = np.array([[-1, -1], [-1, 1], [1, -1], [1, 1]]) #Simplifed 2-factor orthogonal array\n\n                    # Select two dimensions randomly\n                    dims = np.random.choice(self.dim, 2, replace=False)\n                    \n                    # Create potential solutions based on orthogonal array\n                    potential_solutions = np.zeros((len(orthogonal_array), self.dim))\n                    for j in range(len(orthogonal_array)):\n                        potential_solutions[j] = new_position.copy()\n                        potential_solutions[j][dims[0]] = np.clip(new_position[dims[0]] + orthogonal_array[j][0] * search_range/10, lb[dims[0]], ub[dims[0]]) #Adjust\n                        potential_solutions[j][dims[1]] = np.clip(new_position[dims[1]] + orthogonal_array[j][1] * search_range/10, lb[dims[1]], ub[dims[1]]) #Adjust\n\n                    # Evaluate potential solutions (only if budget allows)\n                    potential_fitness = []\n                    for sol in potential_solutions:\n                        if self.budget > 0:\n                            potential_fitness.append(func(sol))\n                            self.budget -= 1\n                        else:\n                            potential_fitness.append(np.inf)\n\n                    # Choose the best solution from orthogonal learning\n                    best_orthogonal_index = np.argmin(potential_fitness)\n                    if potential_fitness[best_orthogonal_index] < func(new_position):\n                        new_position = potential_solutions[best_orthogonal_index].copy()\n                \n                new_fitness = func(new_position)\n                self.budget -= 1\n                iterations += 1\n                \n                if new_fitness < personal_best_fitness[i]:\n                    personal_best_fitness[i] = new_fitness\n                    personal_best_positions[i] = new_position.copy()\n                    successes += 1\n                    \n                    if new_fitness < global_best_fitness:\n                        global_best_fitness = new_fitness\n                        global_best_position = new_position.copy()\n                        center = global_best_position\n\n                swarm_positions[i] = new_position\n                \n            # Shrink search range\n            search_range *= self.shrink_factor\n            lb = np.clip(center - search_range/2, func.bounds.lb, func.bounds.ub)\n            ub = np.clip(center + search_range/2, func.bounds.lb, func.bounds.ub)\n\n            # Adjust Cauchy scale based on success rate\n            if iterations > 0:\n                self.success_rate = successes / iterations\n                if self.success_rate > 0.2:\n                    self.cauchy_scale *= 1.1\n                else:\n                    self.cauchy_scale *= 0.9\n                successes = 0\n                iterations = 0\n\n            self.cauchy_scale = np.clip(self.cauchy_scale, 0.01, 1.0)\n        \n        # Local Search\n        for _ in range(min(self.local_search_iterations, self.budget)):\n             if self.budget <= 0:\n                 break\n             perturbation = np.random.uniform(-search_range/100, search_range/100, self.dim)\n             local_x = np.clip(global_best_position + perturbation, lb, ub)\n             local_f = func(local_x)\n             self.budget -=1\n             if local_f < global_best_fitness:\n                  global_best_fitness = local_f\n                  global_best_position = local_x\n\n        return global_best_fitness, global_best_position", "configspace": "", "generation": 8, "feedback": "", "error": "Evaluation error: setting an array element with a sequence.", "parent_ids": ["5ed072b3-d57e-4c1e-8fc4-1c2b4f7cf697", "df8a52a3-838e-4304-b60e-76e5afa6152f"], "operator": "crossover", "metadata": {"llm_response": "# Description: Combines Bird Swarm Optimization with adaptive Cauchy mutation and orthogonal learning for enhanced exploration and exploitation.\n# Code: \n```python\nimport numpy as np\n\nclass HybridBirdSwarmCauchyOrthogonal:\n    def __init__(self, budget=10000, dim=10, swarm_size=20, inertia_weight=0.7, cognitive_coeff=1.4, social_coeff=1.4, neighborhood_size=3, shrink_factor=0.95, initial_cauchy_scale=0.1, local_search_iterations=5, cauchy_probability=0.2, orthogonal_probability=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = swarm_size\n        self.inertia_weight = inertia_weight\n        self.cognitive_coeff = cognitive_coeff\n        self.social_coeff = social_coeff\n        self.neighborhood_size = neighborhood_size\n        self.shrink_factor = shrink_factor\n        self.cauchy_scale = initial_cauchy_scale\n        self.local_search_iterations = local_search_iterations\n        self.cauchy_probability = cauchy_probability\n        self.orthogonal_probability = orthogonal_probability\n        self.success_rate = 0.5\n\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        \n        # Initialize swarm positions and velocities\n        swarm_positions = np.random.uniform(lb, ub, size=(self.swarm_size, self.dim))\n        swarm_velocities = np.random.uniform(-abs(ub-lb)/10, abs(ub-lb)/10, size=(self.swarm_size, self.dim))\n        swarm_fitness = np.array([func(x) for x in swarm_positions])\n        self.budget -= self.swarm_size\n        \n        personal_best_positions = swarm_positions.copy()\n        personal_best_fitness = swarm_fitness.copy()\n        \n        global_best_index = np.argmin(personal_best_fitness)\n        global_best_position = personal_best_positions[global_best_index].copy()\n        global_best_fitness = personal_best_fitness[global_best_index]\n        \n        search_range = ub - lb\n        center = global_best_position\n\n        successes = 0\n        iterations = 0\n\n        while self.budget > 0:\n            for i in range(self.swarm_size):\n                if self.budget <= 0:\n                    break\n                # Bird Swarm Optimization part\n                inertia = self.inertia_weight * swarm_velocities[i]\n                cognitive = self.cognitive_coeff * np.random.rand(self.dim) * (personal_best_positions[i] - swarm_positions[i])\n                \n                # Social component: find best neighbor\n                neighbors_indices = np.random.choice(self.swarm_size, size=self.neighborhood_size, replace=False)\n                neighbors_fitness = personal_best_fitness[neighbors_indices]\n                best_neighbor_index = neighbors_indices[np.argmin(neighbors_fitness)]\n                social = self.social_coeff * np.random.rand(self.dim) * (personal_best_positions[best_neighbor_index] - swarm_positions[i])\n                \n                swarm_velocities[i] = inertia + cognitive + social\n                \n                # Update position\n                new_position = swarm_positions[i] + swarm_velocities[i]\n                \n                # Boundary handling\n                new_position = np.clip(new_position, lb, ub)\n                \n                # Cauchy mutation with probability\n                if np.random.rand() < self.cauchy_probability:\n                    mutation = center + search_range * np.random.standard_cauchy(size=self.dim) * self.cauchy_scale\n                    mutation = np.clip(mutation, lb, ub)\n                    new_position = mutation\n\n                # Orthogonal Learning\n                if np.random.rand() < self.orthogonal_probability:\n                    # Create orthogonal array (simplified for demonstration)\n                    levels = 3  # Example: low, medium, high\n                    orthogonal_array = np.array([[-1, -1], [-1, 1], [1, -1], [1, 1]]) #Simplifed 2-factor orthogonal array\n\n                    # Select two dimensions randomly\n                    dims = np.random.choice(self.dim, 2, replace=False)\n                    \n                    # Create potential solutions based on orthogonal array\n                    potential_solutions = np.zeros((len(orthogonal_array), self.dim))\n                    for j in range(len(orthogonal_array)):\n                        potential_solutions[j] = new_position.copy()\n                        potential_solutions[j][dims[0]] = np.clip(new_position[dims[0]] + orthogonal_array[j][0] * search_range/10, lb[dims[0]], ub[dims[0]]) #Adjust\n                        potential_solutions[j][dims[1]] = np.clip(new_position[dims[1]] + orthogonal_array[j][1] * search_range/10, lb[dims[1]], ub[dims[1]]) #Adjust\n\n                    # Evaluate potential solutions (only if budget allows)\n                    potential_fitness = []\n                    for sol in potential_solutions:\n                        if self.budget > 0:\n                            potential_fitness.append(func(sol))\n                            self.budget -= 1\n                        else:\n                            potential_fitness.append(np.inf)\n\n                    # Choose the best solution from orthogonal learning\n                    best_orthogonal_index = np.argmin(potential_fitness)\n                    if potential_fitness[best_orthogonal_index] < func(new_position):\n                        new_position = potential_solutions[best_orthogonal_index].copy()\n                \n                new_fitness = func(new_position)\n                self.budget -= 1\n                iterations += 1\n                \n                if new_fitness < personal_best_fitness[i]:\n                    personal_best_fitness[i] = new_fitness\n                    personal_best_positions[i] = new_position.copy()\n                    successes += 1\n                    \n                    if new_fitness < global_best_fitness:\n                        global_best_fitness = new_fitness\n                        global_best_position = new_position.copy()\n                        center = global_best_position\n\n                swarm_positions[i] = new_position\n                \n            # Shrink search range\n            search_range *= self.shrink_factor\n            lb = np.clip(center - search_range/2, func.bounds.lb, func.bounds.ub)\n            ub = np.clip(center + search_range/2, func.bounds.lb, func.bounds.ub)\n\n            # Adjust Cauchy scale based on success rate\n            if iterations > 0:\n                self.success_rate = successes / iterations\n                if self.success_rate > 0.2:\n                    self.cauchy_scale *= 1.1\n                else:\n                    self.cauchy_scale *= 0.9\n                successes = 0\n                iterations = 0\n\n            self.cauchy_scale = np.clip(self.cauchy_scale, 0.01, 1.0)\n        \n        # Local Search\n        for _ in range(min(self.local_search_iterations, self.budget)):\n             if self.budget <= 0:\n                 break\n             perturbation = np.random.uniform(-search_range/100, search_range/100, self.dim)\n             local_x = np.clip(global_best_position + perturbation, lb, ub)\n             local_f = func(local_x)\n             self.budget -=1\n             if local_f < global_best_fitness:\n                  global_best_fitness = local_f\n                  global_best_position = local_x\n\n        return global_best_fitness, global_best_position\n```", "operator": "crossover", "theta_sampled": 0.007847888940405388, "reward": 0.0, "generation": 8}, "task_prompt": ""}
{"id": "31d009e6-d94e-4a5d-ab61-a13707862669", "fitness": 0.0, "name": "AdaptiveHybridBirdSwarm", "description": "Adaptive Hybrid Bird Swarm Optimizer with Cauchy mutation, neighborhood shrinking, and dynamic parameter adaptation, incorporating a restart mechanism for improved exploration and exploitation.", "code": "import numpy as np\n\nclass AdaptiveHybridBirdSwarm:\n    def __init__(self, budget=10000, dim=10, swarm_size=20, inertia_weight=0.7, cognitive_coeff=1.4, social_coeff=1.4, neighborhood_size=3, shrink_factor=0.95, initial_cauchy_scale=0.1, local_search_iterations=5, cauchy_probability=0.2, restart_probability=0.05, stagnation_threshold=100):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = swarm_size\n        self.inertia_weight = inertia_weight\n        self.cognitive_coeff = cognitive_coeff\n        self.social_coeff = social_coeff\n        self.neighborhood_size = neighborhood_size\n        self.shrink_factor = shrink_factor\n        self.cauchy_scale = initial_cauchy_scale\n        self.local_search_iterations = local_search_iterations\n        self.cauchy_probability = cauchy_probability\n        self.restart_probability = restart_probability\n        self.stagnation_threshold = stagnation_threshold  # Number of iterations without improvement before restart\n        self.success_rate = 0.5\n        self.stagnation_counter = 0\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        \n        # Initialize swarm positions and velocities\n        swarm_positions = np.random.uniform(lb, ub, size=(self.swarm_size, self.dim))\n        swarm_velocities = np.random.uniform(-abs(ub-lb)/10, abs(ub-lb)/10, size=(self.swarm_size, self.dim))\n        swarm_fitness = np.array([func(x) for x in swarm_positions])\n        self.budget -= self.swarm_size\n        \n        personal_best_positions = swarm_positions.copy()\n        personal_best_fitness = swarm_fitness.copy()\n        \n        global_best_index = np.argmin(personal_best_fitness)\n        global_best_position = personal_best_positions[global_best_index].copy()\n        global_best_fitness = personal_best_fitness[global_best_index]\n        \n        search_range = ub - lb\n        center = global_best_position\n\n        successes = 0\n        iterations = 0\n        \n        while self.budget > 0:\n            for i in range(self.swarm_size):\n                if self.budget <= 0:\n                    break\n                # Bird Swarm Optimization part\n                inertia = self.inertia_weight * swarm_velocities[i]\n                cognitive = self.cognitive_coeff * np.random.rand(self.dim) * (personal_best_positions[i] - swarm_positions[i])\n                \n                # Social component: find best neighbor\n                neighbors_indices = np.random.choice(self.swarm_size, size=self.neighborhood_size, replace=False)\n                neighbors_fitness = personal_best_fitness[neighbors_indices]\n                best_neighbor_index = neighbors_indices[np.argmin(neighbors_fitness)]\n                social = self.social_coeff * np.random.rand(self.dim) * (personal_best_positions[best_neighbor_index] - swarm_positions[i])\n                \n                swarm_velocities[i] = inertia + cognitive + social\n                \n                # Update position\n                new_position = swarm_positions[i] + swarm_velocities[i]\n                \n                # Cauchy mutation with probability\n                if np.random.rand() < self.cauchy_probability:\n                    mutation = center + search_range * np.random.standard_cauchy(size=self.dim) * self.cauchy_scale\n                    new_position = mutation\n\n                # Boundary handling\n                new_position = np.clip(new_position, lb, ub)\n                \n                new_fitness = func(new_position)\n                self.budget -= 1\n                iterations += 1\n                \n                if new_fitness < personal_best_fitness[i]:\n                    personal_best_fitness[i] = new_fitness\n                    personal_best_positions[i] = new_position.copy()\n                    successes += 1\n                    \n                    if new_fitness < global_best_fitness:\n                        global_best_fitness = new_fitness\n                        global_best_position = new_position.copy()\n                        center = global_best_position\n                        self.stagnation_counter = 0 # Reset stagnation counter\n                else:\n                    self.stagnation_counter += 1\n                    \n                swarm_positions[i] = new_position\n                \n            # Shrink search range\n            search_range *= self.shrink_factor\n            lb = np.clip(center - search_range/2, func.bounds.lb, func.bounds.ub)\n            ub = np.clip(center + search_range/2, func.bounds.lb, func.bounds.ub)\n            \n            # Adjust Cauchy scale based on success rate\n            if iterations > 0:\n                self.success_rate = successes / iterations\n                if self.success_rate > 0.2:\n                    self.cauchy_scale *= 1.1  # Increase scale if successful\n                else:\n                    self.cauchy_scale *= 0.9  # Decrease scale if not successful\n                successes = 0\n                iterations = 0\n\n            self.cauchy_scale = np.clip(self.cauchy_scale, 0.01, 1.0)\n\n            # Adaptive Inertia Weight: linearly decrease inertia weight\n            self.inertia_weight = 0.9 - (0.9 - 0.4) * (1 - self.budget / self.budget) # Linearly decrease from 0.9 to 0.4\n\n            # Restart Mechanism\n            if self.stagnation_counter > self.stagnation_threshold or np.random.rand() < self.restart_probability:\n                 # Re-initialize a portion of the swarm\n                num_to_restart = int(self.swarm_size * 0.2)  # Restart 20% of the swarm\n                indices_to_restart = np.random.choice(self.swarm_size, num_to_restart, replace=False)\n\n                for i in indices_to_restart:\n                    swarm_positions[i] = np.random.uniform(lb, ub, size=self.dim)\n                    swarm_velocities[i] = np.random.uniform(-abs(ub - lb) / 10, abs(ub - lb) / 10, size=self.dim)\n                    swarm_fitness[i] = func(swarm_positions[i])\n                    self.budget -= 1\n                    personal_best_positions[i] = swarm_positions[i].copy()\n                    personal_best_fitness[i] = swarm_fitness[i]\n\n                # Update global best\n                global_best_index = np.argmin(personal_best_fitness)\n                global_best_position = personal_best_positions[global_best_index].copy()\n                global_best_fitness = personal_best_fitness[global_best_index]\n                center = global_best_position\n                self.stagnation_counter = 0\n\n        # Local Search\n        for _ in range(min(self.local_search_iterations, self.budget)):\n             if self.budget <= 0:\n                 break\n\n             perturbation = np.random.uniform(-search_range/100, search_range/100, self.dim)\n             local_x = np.clip(global_best_position + perturbation, lb, ub)\n             local_f = func(local_x)\n             self.budget -=1\n\n             if local_f < global_best_fitness:\n                  global_best_fitness = local_f\n                  global_best_position = local_x\n\n        return global_best_fitness, global_best_position", "configspace": "", "generation": 8, "feedback": "", "error": "Evaluation error: division by zero", "parent_ids": ["5ed072b3-d57e-4c1e-8fc4-1c2b4f7cf697", "df8a52a3-838e-4304-b60e-76e5afa6152f"], "operator": "crossover", "metadata": {"llm_response": "# Description: Adaptive Hybrid Bird Swarm Optimizer with Cauchy mutation, neighborhood shrinking, and dynamic parameter adaptation, incorporating a restart mechanism for improved exploration and exploitation.\n# Code: \n```python\nimport numpy as np\n\nclass AdaptiveHybridBirdSwarm:\n    def __init__(self, budget=10000, dim=10, swarm_size=20, inertia_weight=0.7, cognitive_coeff=1.4, social_coeff=1.4, neighborhood_size=3, shrink_factor=0.95, initial_cauchy_scale=0.1, local_search_iterations=5, cauchy_probability=0.2, restart_probability=0.05, stagnation_threshold=100):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = swarm_size\n        self.inertia_weight = inertia_weight\n        self.cognitive_coeff = cognitive_coeff\n        self.social_coeff = social_coeff\n        self.neighborhood_size = neighborhood_size\n        self.shrink_factor = shrink_factor\n        self.cauchy_scale = initial_cauchy_scale\n        self.local_search_iterations = local_search_iterations\n        self.cauchy_probability = cauchy_probability\n        self.restart_probability = restart_probability\n        self.stagnation_threshold = stagnation_threshold  # Number of iterations without improvement before restart\n        self.success_rate = 0.5\n        self.stagnation_counter = 0\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        \n        # Initialize swarm positions and velocities\n        swarm_positions = np.random.uniform(lb, ub, size=(self.swarm_size, self.dim))\n        swarm_velocities = np.random.uniform(-abs(ub-lb)/10, abs(ub-lb)/10, size=(self.swarm_size, self.dim))\n        swarm_fitness = np.array([func(x) for x in swarm_positions])\n        self.budget -= self.swarm_size\n        \n        personal_best_positions = swarm_positions.copy()\n        personal_best_fitness = swarm_fitness.copy()\n        \n        global_best_index = np.argmin(personal_best_fitness)\n        global_best_position = personal_best_positions[global_best_index].copy()\n        global_best_fitness = personal_best_fitness[global_best_index]\n        \n        search_range = ub - lb\n        center = global_best_position\n\n        successes = 0\n        iterations = 0\n        \n        while self.budget > 0:\n            for i in range(self.swarm_size):\n                if self.budget <= 0:\n                    break\n                # Bird Swarm Optimization part\n                inertia = self.inertia_weight * swarm_velocities[i]\n                cognitive = self.cognitive_coeff * np.random.rand(self.dim) * (personal_best_positions[i] - swarm_positions[i])\n                \n                # Social component: find best neighbor\n                neighbors_indices = np.random.choice(self.swarm_size, size=self.neighborhood_size, replace=False)\n                neighbors_fitness = personal_best_fitness[neighbors_indices]\n                best_neighbor_index = neighbors_indices[np.argmin(neighbors_fitness)]\n                social = self.social_coeff * np.random.rand(self.dim) * (personal_best_positions[best_neighbor_index] - swarm_positions[i])\n                \n                swarm_velocities[i] = inertia + cognitive + social\n                \n                # Update position\n                new_position = swarm_positions[i] + swarm_velocities[i]\n                \n                # Cauchy mutation with probability\n                if np.random.rand() < self.cauchy_probability:\n                    mutation = center + search_range * np.random.standard_cauchy(size=self.dim) * self.cauchy_scale\n                    new_position = mutation\n\n                # Boundary handling\n                new_position = np.clip(new_position, lb, ub)\n                \n                new_fitness = func(new_position)\n                self.budget -= 1\n                iterations += 1\n                \n                if new_fitness < personal_best_fitness[i]:\n                    personal_best_fitness[i] = new_fitness\n                    personal_best_positions[i] = new_position.copy()\n                    successes += 1\n                    \n                    if new_fitness < global_best_fitness:\n                        global_best_fitness = new_fitness\n                        global_best_position = new_position.copy()\n                        center = global_best_position\n                        self.stagnation_counter = 0 # Reset stagnation counter\n                else:\n                    self.stagnation_counter += 1\n                    \n                swarm_positions[i] = new_position\n                \n            # Shrink search range\n            search_range *= self.shrink_factor\n            lb = np.clip(center - search_range/2, func.bounds.lb, func.bounds.ub)\n            ub = np.clip(center + search_range/2, func.bounds.lb, func.bounds.ub)\n            \n            # Adjust Cauchy scale based on success rate\n            if iterations > 0:\n                self.success_rate = successes / iterations\n                if self.success_rate > 0.2:\n                    self.cauchy_scale *= 1.1  # Increase scale if successful\n                else:\n                    self.cauchy_scale *= 0.9  # Decrease scale if not successful\n                successes = 0\n                iterations = 0\n\n            self.cauchy_scale = np.clip(self.cauchy_scale, 0.01, 1.0)\n\n            # Adaptive Inertia Weight: linearly decrease inertia weight\n            self.inertia_weight = 0.9 - (0.9 - 0.4) * (1 - self.budget / self.budget) # Linearly decrease from 0.9 to 0.4\n\n            # Restart Mechanism\n            if self.stagnation_counter > self.stagnation_threshold or np.random.rand() < self.restart_probability:\n                 # Re-initialize a portion of the swarm\n                num_to_restart = int(self.swarm_size * 0.2)  # Restart 20% of the swarm\n                indices_to_restart = np.random.choice(self.swarm_size, num_to_restart, replace=False)\n\n                for i in indices_to_restart:\n                    swarm_positions[i] = np.random.uniform(lb, ub, size=self.dim)\n                    swarm_velocities[i] = np.random.uniform(-abs(ub - lb) / 10, abs(ub - lb) / 10, size=self.dim)\n                    swarm_fitness[i] = func(swarm_positions[i])\n                    self.budget -= 1\n                    personal_best_positions[i] = swarm_positions[i].copy()\n                    personal_best_fitness[i] = swarm_fitness[i]\n\n                # Update global best\n                global_best_index = np.argmin(personal_best_fitness)\n                global_best_position = personal_best_positions[global_best_index].copy()\n                global_best_fitness = personal_best_fitness[global_best_index]\n                center = global_best_position\n                self.stagnation_counter = 0\n\n        # Local Search\n        for _ in range(min(self.local_search_iterations, self.budget)):\n             if self.budget <= 0:\n                 break\n\n             perturbation = np.random.uniform(-search_range/100, search_range/100, self.dim)\n             local_x = np.clip(global_best_position + perturbation, lb, ub)\n             local_f = func(local_x)\n             self.budget -=1\n\n             if local_f < global_best_fitness:\n                  global_best_fitness = local_f\n                  global_best_position = local_x\n\n        return global_best_fitness, global_best_position\n```", "operator": "crossover", "theta_sampled": 0.00612363502996735, "reward": 0.0, "generation": 8}, "task_prompt": ""}
{"id": "7d85d6b8-f571-4284-84a0-b5013342b143", "fitness": 0.0, "name": "AdaptiveLevyBirdSwarm", "description": "Adaptive Bird Swarm Optimization with Lvy flights for enhanced exploration and a self-adaptive Cauchy mutation strategy for efficient exploitation.", "code": "import numpy as np\n\nclass AdaptiveLevyBirdSwarm:\n    def __init__(self, budget=10000, dim=10, swarm_size=20, inertia_weight=0.7, cognitive_coeff=1.4, social_coeff=1.4, neighborhood_size=3, shrink_factor=0.95, initial_cauchy_scale=0.1, local_search_iterations=5, cauchy_probability=0.2, stagnation_threshold=10, levy_flight_coefficient=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = swarm_size\n        self.inertia_weight = inertia_weight\n        self.cognitive_coeff = cognitive_coeff\n        self.social_coeff = social_coeff\n        self.neighborhood_size = neighborhood_size\n        self.shrink_factor = shrink_factor\n        self.cauchy_scale = initial_cauchy_scale\n        self.local_search_iterations = local_search_iterations\n        self.cauchy_probability = cauchy_probability\n        self.success_rate = 0.5\n        self.stagnation_threshold = stagnation_threshold\n        self.stagnation_counter = 0\n        self.levy_flight_coefficient = levy_flight_coefficient\n\n    def levy_flight(self, dim, beta=1.5):\n        \"\"\"\n        Generates Lvy flight steps.\n        \"\"\"\n        sigma = (np.math.gamma(1 + beta) * np.sin(np.pi * beta / 2) / (np.math.gamma((1 + beta) / 2) * beta * (2 ** ((beta - 1) / 2)))) ** (1 / beta)\n        u = np.random.normal(0, sigma, size=dim)\n        v = np.random.normal(0, 1, size=dim)\n        step = u / (abs(v) ** (1 / beta))\n        return step\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        \n        # Initialize swarm positions and velocities\n        swarm_positions = np.random.uniform(lb, ub, size=(self.swarm_size, self.dim))\n        swarm_velocities = np.random.uniform(-abs(ub-lb)/10, abs(ub-lb)/10, size=(self.swarm_size, self.dim))\n        swarm_fitness = np.array([func(x) for x in swarm_positions])\n        self.budget -= self.swarm_size\n        \n        personal_best_positions = swarm_positions.copy()\n        personal_best_fitness = swarm_fitness.copy()\n        \n        global_best_index = np.argmin(personal_best_fitness)\n        global_best_position = personal_best_positions[global_best_index].copy()\n        global_best_fitness = personal_best_fitness[global_best_index]\n        \n        search_range = ub - lb\n        center = global_best_position\n\n        successes = 0\n        iterations = 0\n\n        previous_best_fitness = global_best_fitness\n        \n        while self.budget > 0:\n            for i in range(self.swarm_size):\n                if self.budget <= 0:\n                    break\n                # Bird Swarm Optimization part\n                inertia = self.inertia_weight * swarm_velocities[i]\n                cognitive = self.cognitive_coeff * np.random.rand(self.dim) * (personal_best_positions[i] - swarm_positions[i])\n                \n                # Social component: find best neighbor\n                neighbors_indices = np.random.choice(self.swarm_size, size=self.neighborhood_size, replace=False)\n                neighbors_fitness = personal_best_fitness[neighbors_indices]\n                best_neighbor_index = neighbors_indices[np.argmin(neighbors_fitness)]\n                social = self.social_coeff * np.random.rand(self.dim) * (personal_best_positions[best_neighbor_index] - swarm_positions[i])\n                \n                swarm_velocities[i] = inertia + cognitive + social\n                \n                # Update position\n                new_position = swarm_positions[i] + swarm_velocities[i]\n\n                # Adaptive Cauchy mutation\n                if np.random.rand() < self.cauchy_probability:\n                    # Self-adaptive Cauchy scale based on individual success\n                    individual_success = (personal_best_fitness[i] < swarm_fitness[i])\n                    cauchy_scale_adaptive = self.cauchy_scale * (1.2 if individual_success else 0.8)\n                    mutation = center + search_range * np.random.standard_cauchy(size=self.dim) * cauchy_scale_adaptive\n                    new_position = mutation\n\n                # Lvy Flight for Exploration\n                if np.random.rand() < 0.1:  # Low probability for Levy flight to avoid disrupting exploitation\n                    levy_steps = self.levy_flight(self.dim)\n                    new_position = swarm_positions[i] + self.levy_flight_coefficient * levy_steps * search_range  # Scale the steps\n\n                # Boundary handling\n                new_position = np.clip(new_position, lb, ub)\n                \n                new_fitness = func(new_position)\n                self.budget -= 1\n                iterations += 1\n                \n                if new_fitness < personal_best_fitness[i]:\n                    personal_best_fitness[i] = new_fitness\n                    personal_best_positions[i] = new_position.copy()\n                    successes += 1\n                    \n                    if new_fitness < global_best_fitness:\n                        global_best_fitness = new_fitness\n                        global_best_position = new_position.copy()\n                        center = global_best_position\n\n                swarm_positions[i] = new_position\n\n            # Shrink search range\n            search_range *= self.shrink_factor\n            lb = np.clip(center - search_range/2, func.bounds.lb, func.bounds.ub)\n            ub = np.clip(center + search_range/2, func.bounds.lb, func.bounds.ub)\n            \n            # Adjust Cauchy scale based on success rate\n            if iterations > 0:\n                self.success_rate = successes / iterations\n                if self.success_rate > 0.2:\n                    self.cauchy_scale *= 1.1\n                else:\n                    self.cauchy_scale *= 0.9\n                successes = 0\n                iterations = 0\n\n            self.cauchy_scale = np.clip(self.cauchy_scale, 0.01, 1.0)\n\n            # Dynamic parameter adjustment based on stagnation\n            if abs(global_best_fitness - previous_best_fitness) < 1e-6:\n                self.stagnation_counter += 1\n                if self.stagnation_counter > self.stagnation_threshold:\n                    # Increase exploration: reset Cauchy scale, increase mutation probability\n                    self.cauchy_scale = 0.2  # Reset cauchy scale\n                    self.cauchy_probability = min(self.cauchy_probability * 1.2, 0.5)  # Increase probability\n                    # Perturb the swarm to escape local optima using levy flight\n                    for i in range(self.swarm_size):\n                         levy_steps = self.levy_flight(self.dim)\n                         swarm_positions[i] = np.clip(swarm_positions[i] + self.levy_flight_coefficient * levy_steps * search_range, lb, ub) #Perturbation using levy\n                    \n                    self.stagnation_counter = 0  # Reset stagnation counter\n            else:\n                self.stagnation_counter = 0\n\n            previous_best_fitness = global_best_fitness\n        \n        # Local Search with more intensity around the best\n        local_search_range = search_range / 100 #Start small\n        for _ in range(min(self.local_search_iterations, self.budget)):\n             if self.budget <= 0:\n                 break\n\n             perturbation = np.random.uniform(-local_search_range, local_search_range, self.dim)\n             local_x = np.clip(global_best_position + perturbation, lb, ub)\n             local_f = func(local_x)\n             self.budget -=1\n\n             if local_f < global_best_fitness:\n                  global_best_fitness = local_f\n                  global_best_position = local_x\n\n             local_search_range *= 0.9 #Reduce range each iteration\n\n        return global_best_fitness, global_best_position", "configspace": "", "generation": 9, "feedback": "", "error": "Evaluation error: module 'numpy' has no attribute 'math'", "parent_ids": ["a5b61a20-1389-4908-aff1-63b377601d9f"], "operator": "mutation", "metadata": {"llm_response": "# Description: Adaptive Bird Swarm Optimization with Lvy flights for enhanced exploration and a self-adaptive Cauchy mutation strategy for efficient exploitation.\n# Code: \n```python\nimport numpy as np\n\nclass AdaptiveLevyBirdSwarm:\n    def __init__(self, budget=10000, dim=10, swarm_size=20, inertia_weight=0.7, cognitive_coeff=1.4, social_coeff=1.4, neighborhood_size=3, shrink_factor=0.95, initial_cauchy_scale=0.1, local_search_iterations=5, cauchy_probability=0.2, stagnation_threshold=10, levy_flight_coefficient=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = swarm_size\n        self.inertia_weight = inertia_weight\n        self.cognitive_coeff = cognitive_coeff\n        self.social_coeff = social_coeff\n        self.neighborhood_size = neighborhood_size\n        self.shrink_factor = shrink_factor\n        self.cauchy_scale = initial_cauchy_scale\n        self.local_search_iterations = local_search_iterations\n        self.cauchy_probability = cauchy_probability\n        self.success_rate = 0.5\n        self.stagnation_threshold = stagnation_threshold\n        self.stagnation_counter = 0\n        self.levy_flight_coefficient = levy_flight_coefficient\n\n    def levy_flight(self, dim, beta=1.5):\n        \"\"\"\n        Generates Lvy flight steps.\n        \"\"\"\n        sigma = (np.math.gamma(1 + beta) * np.sin(np.pi * beta / 2) / (np.math.gamma((1 + beta) / 2) * beta * (2 ** ((beta - 1) / 2)))) ** (1 / beta)\n        u = np.random.normal(0, sigma, size=dim)\n        v = np.random.normal(0, 1, size=dim)\n        step = u / (abs(v) ** (1 / beta))\n        return step\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        \n        # Initialize swarm positions and velocities\n        swarm_positions = np.random.uniform(lb, ub, size=(self.swarm_size, self.dim))\n        swarm_velocities = np.random.uniform(-abs(ub-lb)/10, abs(ub-lb)/10, size=(self.swarm_size, self.dim))\n        swarm_fitness = np.array([func(x) for x in swarm_positions])\n        self.budget -= self.swarm_size\n        \n        personal_best_positions = swarm_positions.copy()\n        personal_best_fitness = swarm_fitness.copy()\n        \n        global_best_index = np.argmin(personal_best_fitness)\n        global_best_position = personal_best_positions[global_best_index].copy()\n        global_best_fitness = personal_best_fitness[global_best_index]\n        \n        search_range = ub - lb\n        center = global_best_position\n\n        successes = 0\n        iterations = 0\n\n        previous_best_fitness = global_best_fitness\n        \n        while self.budget > 0:\n            for i in range(self.swarm_size):\n                if self.budget <= 0:\n                    break\n                # Bird Swarm Optimization part\n                inertia = self.inertia_weight * swarm_velocities[i]\n                cognitive = self.cognitive_coeff * np.random.rand(self.dim) * (personal_best_positions[i] - swarm_positions[i])\n                \n                # Social component: find best neighbor\n                neighbors_indices = np.random.choice(self.swarm_size, size=self.neighborhood_size, replace=False)\n                neighbors_fitness = personal_best_fitness[neighbors_indices]\n                best_neighbor_index = neighbors_indices[np.argmin(neighbors_fitness)]\n                social = self.social_coeff * np.random.rand(self.dim) * (personal_best_positions[best_neighbor_index] - swarm_positions[i])\n                \n                swarm_velocities[i] = inertia + cognitive + social\n                \n                # Update position\n                new_position = swarm_positions[i] + swarm_velocities[i]\n\n                # Adaptive Cauchy mutation\n                if np.random.rand() < self.cauchy_probability:\n                    # Self-adaptive Cauchy scale based on individual success\n                    individual_success = (personal_best_fitness[i] < swarm_fitness[i])\n                    cauchy_scale_adaptive = self.cauchy_scale * (1.2 if individual_success else 0.8)\n                    mutation = center + search_range * np.random.standard_cauchy(size=self.dim) * cauchy_scale_adaptive\n                    new_position = mutation\n\n                # Lvy Flight for Exploration\n                if np.random.rand() < 0.1:  # Low probability for Levy flight to avoid disrupting exploitation\n                    levy_steps = self.levy_flight(self.dim)\n                    new_position = swarm_positions[i] + self.levy_flight_coefficient * levy_steps * search_range  # Scale the steps\n\n                # Boundary handling\n                new_position = np.clip(new_position, lb, ub)\n                \n                new_fitness = func(new_position)\n                self.budget -= 1\n                iterations += 1\n                \n                if new_fitness < personal_best_fitness[i]:\n                    personal_best_fitness[i] = new_fitness\n                    personal_best_positions[i] = new_position.copy()\n                    successes += 1\n                    \n                    if new_fitness < global_best_fitness:\n                        global_best_fitness = new_fitness\n                        global_best_position = new_position.copy()\n                        center = global_best_position\n\n                swarm_positions[i] = new_position\n\n            # Shrink search range\n            search_range *= self.shrink_factor\n            lb = np.clip(center - search_range/2, func.bounds.lb, func.bounds.ub)\n            ub = np.clip(center + search_range/2, func.bounds.lb, func.bounds.ub)\n            \n            # Adjust Cauchy scale based on success rate\n            if iterations > 0:\n                self.success_rate = successes / iterations\n                if self.success_rate > 0.2:\n                    self.cauchy_scale *= 1.1\n                else:\n                    self.cauchy_scale *= 0.9\n                successes = 0\n                iterations = 0\n\n            self.cauchy_scale = np.clip(self.cauchy_scale, 0.01, 1.0)\n\n            # Dynamic parameter adjustment based on stagnation\n            if abs(global_best_fitness - previous_best_fitness) < 1e-6:\n                self.stagnation_counter += 1\n                if self.stagnation_counter > self.stagnation_threshold:\n                    # Increase exploration: reset Cauchy scale, increase mutation probability\n                    self.cauchy_scale = 0.2  # Reset cauchy scale\n                    self.cauchy_probability = min(self.cauchy_probability * 1.2, 0.5)  # Increase probability\n                    # Perturb the swarm to escape local optima using levy flight\n                    for i in range(self.swarm_size):\n                         levy_steps = self.levy_flight(self.dim)\n                         swarm_positions[i] = np.clip(swarm_positions[i] + self.levy_flight_coefficient * levy_steps * search_range, lb, ub) #Perturbation using levy\n                    \n                    self.stagnation_counter = 0  # Reset stagnation counter\n            else:\n                self.stagnation_counter = 0\n\n            previous_best_fitness = global_best_fitness\n        \n        # Local Search with more intensity around the best\n        local_search_range = search_range / 100 #Start small\n        for _ in range(min(self.local_search_iterations, self.budget)):\n             if self.budget <= 0:\n                 break\n\n             perturbation = np.random.uniform(-local_search_range, local_search_range, self.dim)\n             local_x = np.clip(global_best_position + perturbation, lb, ub)\n             local_f = func(local_x)\n             self.budget -=1\n\n             if local_f < global_best_fitness:\n                  global_best_fitness = local_f\n                  global_best_position = local_x\n\n             local_search_range *= 0.9 #Reduce range each iteration\n\n        return global_best_fitness, global_best_position\n```", "operator": "mutation", "theta_sampled": 0.006967043517692794, "reward": 0.0, "generation": 9}, "task_prompt": ""}
{"id": "6c9581a1-65e5-4b1c-a293-376d55ced2b8", "fitness": 0.2859402604233722, "name": "FitnessWeightedSampling", "description": "A population-based algorithm that uses a combination of global exploration and local exploitation based on a fitness-weighted sampling strategy.", "code": "import numpy as np\n\nclass FitnessWeightedSampling:\n    def __init__(self, budget=10000, dim=10, pop_size=20, exploration_prob=0.3):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.exploration_prob = exploration_prob\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        \n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        # Main optimization loop\n        while self.budget > 0:\n            # Find the best individual in the current population\n            best_index = np.argmin(fitness)\n            best_x = population[best_index]\n            best_f = fitness[best_index]\n\n            if best_f < self.f_opt:\n                self.f_opt = best_f\n                self.x_opt = best_x\n\n            # Calculate selection probabilities based on fitness\n            fitness_min = np.min(fitness)\n            if fitness_min < 0:\n                adjusted_fitness = fitness - fitness_min + 1e-9  # shift to positive values and add small constant to avoid zero division\n            else:\n                adjusted_fitness = fitness + 1e-9 # avoid zero division\n            \n            probabilities = (np.max(adjusted_fitness) - adjusted_fitness) / np.sum(np.max(adjusted_fitness) - adjusted_fitness) # higher prob for better fitness\n\n            # Generate new population\n            new_population = np.zeros_like(population)\n            for i in range(self.pop_size):\n                if np.random.rand() < self.exploration_prob:\n                    # Exploration: Sample randomly from the search space\n                    new_population[i] = np.random.uniform(func.bounds.lb, func.bounds.ub)\n                else:\n                    # Exploitation: Sample from the current population based on fitness weights\n                    parent_index = np.random.choice(self.pop_size, p=probabilities)\n                    parent = population[parent_index]\n                    \n                    # Apply a small mutation to the parent to create the offspring\n                    mutation = np.random.normal(0, 0.1, size=self.dim) # Gaussian mutation\n                    new_population[i] = np.clip(parent + mutation, func.bounds.lb, func.bounds.ub) # Clip to bounds\n\n            # Evaluate the new population\n            new_fitness = np.array([func(x) for x in new_population])\n            self.budget -= self.pop_size\n\n            # Update the population\n            population = new_population\n            fitness = new_fitness\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 9, "feedback": "The algorithm FitnessWeightedSampling scored 0.286 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": "random_new", "metadata": {"llm_response": "# Description: A population-based algorithm that uses a combination of global exploration and local exploitation based on a fitness-weighted sampling strategy.\n# Code: \n```python\nimport numpy as np\n\nclass FitnessWeightedSampling:\n    def __init__(self, budget=10000, dim=10, pop_size=20, exploration_prob=0.3):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.exploration_prob = exploration_prob\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        \n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        # Main optimization loop\n        while self.budget > 0:\n            # Find the best individual in the current population\n            best_index = np.argmin(fitness)\n            best_x = population[best_index]\n            best_f = fitness[best_index]\n\n            if best_f < self.f_opt:\n                self.f_opt = best_f\n                self.x_opt = best_x\n\n            # Calculate selection probabilities based on fitness\n            fitness_min = np.min(fitness)\n            if fitness_min < 0:\n                adjusted_fitness = fitness - fitness_min + 1e-9  # shift to positive values and add small constant to avoid zero division\n            else:\n                adjusted_fitness = fitness + 1e-9 # avoid zero division\n            \n            probabilities = (np.max(adjusted_fitness) - adjusted_fitness) / np.sum(np.max(adjusted_fitness) - adjusted_fitness) # higher prob for better fitness\n\n            # Generate new population\n            new_population = np.zeros_like(population)\n            for i in range(self.pop_size):\n                if np.random.rand() < self.exploration_prob:\n                    # Exploration: Sample randomly from the search space\n                    new_population[i] = np.random.uniform(func.bounds.lb, func.bounds.ub)\n                else:\n                    # Exploitation: Sample from the current population based on fitness weights\n                    parent_index = np.random.choice(self.pop_size, p=probabilities)\n                    parent = population[parent_index]\n                    \n                    # Apply a small mutation to the parent to create the offspring\n                    mutation = np.random.normal(0, 0.1, size=self.dim) # Gaussian mutation\n                    new_population[i] = np.clip(parent + mutation, func.bounds.lb, func.bounds.ub) # Clip to bounds\n\n            # Evaluate the new population\n            new_fitness = np.array([func(x) for x in new_population])\n            self.budget -= self.pop_size\n\n            # Update the population\n            population = new_population\n            fitness = new_fitness\n\n        return self.f_opt, self.x_opt\n```", "aucs": [0.11819291645154084, 0.17868283915924954, 0.3014062710787868, 0.19642576530641798, 0.2151664107790483, 0.21277854517378203, 0.24118341475257454, 0.2097602437495516, 0.22477813239516742, 0.15993257955772755, 0.22434794137157676, 0.9944563836210415, 0.2826859285387012, 0.19924227020374752, 0.5430588808162731, 0.26633068610191535, 0.21902851801447942, 0.2807664735388956, 0.19295611886643715, 0.45762488899052856], "operator": "random_new", "theta_sampled": 0.0018787765224470205, "reward": 0.0, "generation": 9}, "task_prompt": ""}
{"id": "a24f1b90-71aa-4a38-9811-b486e7109a45", "fitness": 0.0, "name": "AHBOC", "description": "Combines Bird Swarm Optimization with Cauchy mutation, adaptive parameter control, and orthogonal learning to enhance exploration and exploitation.", "code": "import numpy as np\n\nclass AHBOC:\n    def __init__(self, budget=10000, dim=10, swarm_size=20, inertia_weight=0.7, cognitive_coeff=1.4, social_coeff=1.4, neighborhood_size=3, shrink_factor=0.95, initial_cauchy_scale=0.1, local_search_iterations=5, cauchy_probability=0.2, orthogonal_learning_probability=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = swarm_size\n        self.inertia_weight = inertia_weight\n        self.cognitive_coeff = cognitive_coeff\n        self.social_coeff = social_coeff\n        self.neighborhood_size = neighborhood_size\n        self.shrink_factor = shrink_factor\n        self.cauchy_scale = initial_cauchy_scale\n        self.local_search_iterations = local_search_iterations\n        self.cauchy_probability = cauchy_probability  # Probability of applying Cauchy mutation\n        self.orthogonal_learning_probability = orthogonal_learning_probability\n        self.success_rate = 0.5  # Initialize success rate\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        \n        # Initialize swarm positions and velocities\n        swarm_positions = np.random.uniform(lb, ub, size=(self.swarm_size, self.dim))\n        swarm_velocities = np.random.uniform(-abs(ub-lb)/10, abs(ub-lb)/10, size=(self.swarm_size, self.dim))  #Initialize with small velocities\n        swarm_fitness = np.array([func(x) for x in swarm_positions])\n        self.budget -= self.swarm_size #Update the remaining budget\n        \n        personal_best_positions = swarm_positions.copy()\n        personal_best_fitness = swarm_fitness.copy()\n        \n        global_best_index = np.argmin(personal_best_fitness)\n        global_best_position = personal_best_positions[global_best_index].copy()\n        global_best_fitness = personal_best_fitness[global_best_index]\n        \n        search_range = ub - lb\n        center = global_best_position\n\n        successes = 0\n        iterations = 0\n\n        # Adaptive Inertia Weight\n        inertia_weight_max = 0.9\n        inertia_weight_min = 0.2\n\n\n        while self.budget > 0:\n            # Adaptive Inertia Weight update\n            inertia_weight = inertia_weight_max - (inertia_weight_max - inertia_weight_min) * (iterations / self.budget)\n\n\n            for i in range(self.swarm_size):\n                # Bird Swarm Optimization part\n                inertia = inertia_weight * swarm_velocities[i]\n                cognitive = self.cognitive_coeff * np.random.rand(self.dim) * (personal_best_positions[i] - swarm_positions[i])\n                \n                # Social component: find best neighbor\n                neighbors_indices = np.random.choice(self.swarm_size, size=self.neighborhood_size, replace=False)  # Select random neighbors\n                neighbors_fitness = personal_best_fitness[neighbors_indices]\n                best_neighbor_index = neighbors_indices[np.argmin(neighbors_fitness)]\n                social = self.social_coeff * np.random.rand(self.dim) * (personal_best_positions[best_neighbor_index] - swarm_positions[i])\n                \n                swarm_velocities[i] = inertia + cognitive + social\n                \n                # Update position\n                new_position = swarm_positions[i] + swarm_velocities[i]\n                \n                # Boundary handling (clip the solution, if out of bounds)\n                new_position = np.clip(new_position, lb, ub)\n\n                # Cauchy mutation with probability\n                if np.random.rand() < self.cauchy_probability:\n                    mutation = center + search_range * np.random.standard_cauchy(size=self.dim) * self.cauchy_scale  # Scale Cauchy distribution\n                    mutation = np.clip(mutation, lb, ub)  # Clip values to stay within bounds\n                    new_position = mutation\n                \n                # Orthogonal Learning\n                if np.random.rand() < self.orthogonal_learning_probability:\n                    levels = 3  # Number of levels for orthogonal design\n                    indices = np.random.choice(self.dim, size=2, replace=False)\n                    x1_index, x2_index = indices[0], indices[1]\n\n                    x1_range = np.linspace(lb[x1_index], ub[x1_index], levels)\n                    x2_range = np.linspace(lb[x2_index], ub[x2_index], levels)\n                    \n                    orthogonal_design = np.array([[x1_range[0], x2_range[0]],\n                                                    [x1_range[1], x2_range[2]],\n                                                    [x1_range[2], x2_range[1]]])\n                    \n                    fitness_values = []\n                    for k in range(levels):\n                        temp_position = new_position.copy()\n                        temp_position[x1_index] = orthogonal_design[k, 0]\n                        temp_position[x2_index] = orthogonal_design[k, 1]\n                        fitness_values.append(func(temp_position))\n                        self.budget -= 1\n                        iterations += 1\n                        if self.budget <= 0:\n                           break\n\n                    best_index = np.argmin(fitness_values)\n                    new_position[x1_index] = orthogonal_design[best_index, 0]\n                    new_position[x2_index] = orthogonal_design[best_index, 1]\n                    \n                \n                new_fitness = func(new_position)\n                self.budget -= 1\n                iterations += 1\n                \n                if new_fitness < personal_best_fitness[i]:\n                    personal_best_fitness[i] = new_fitness\n                    personal_best_positions[i] = new_position.copy()\n                    successes += 1\n                    \n                    if new_fitness < global_best_fitness:\n                        global_best_fitness = new_fitness\n                        global_best_position = new_position.copy()\n                        center = global_best_position\n                \n                swarm_positions[i] = new_position\n\n            # Shrink the search range around the best individual\n            search_range *= self.shrink_factor\n            lb = np.clip(center - search_range/2, func.bounds.lb, func.bounds.ub)\n            ub = np.clip(center + search_range/2, func.bounds.lb, func.bounds.ub)\n\n             # Adjust Cauchy scale based on success rate\n            if iterations > 0:\n                self.success_rate = successes / iterations\n                if self.success_rate > 0.2:\n                    self.cauchy_scale *= 1.1  # Increase scale if successful\n                else:\n                    self.cauchy_scale *= 0.9  # Decrease scale if not successful\n                successes = 0\n                iterations = 0\n\n            self.cauchy_scale = np.clip(self.cauchy_scale, 0.01, 1.0) # Limit Cauchy scale\n            \n        \n        # Local Search around best solution\n        for _ in range(min(self.local_search_iterations, self.budget)):\n             if self.budget <= 0:\n                 break\n\n             perturbation = np.random.uniform(-search_range/100, search_range/100, self.dim)\n             local_x = np.clip(global_best_position + perturbation, lb, ub)\n             local_f = func(local_x)\n             self.budget -=1\n\n             if local_f < global_best_fitness:\n                  global_best_fitness = local_f\n                  global_best_position = local_x\n        \n        return global_best_fitness, global_best_position", "configspace": "", "generation": 9, "feedback": "The algorithm AHBOC scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["5ed072b3-d57e-4c1e-8fc4-1c2b4f7cf697"], "operator": "mutation", "metadata": {"llm_response": "# Description: Combines Bird Swarm Optimization with Cauchy mutation, adaptive parameter control, and orthogonal learning to enhance exploration and exploitation.\n# Code: \n```python\nimport numpy as np\n\nclass AHBOC:\n    def __init__(self, budget=10000, dim=10, swarm_size=20, inertia_weight=0.7, cognitive_coeff=1.4, social_coeff=1.4, neighborhood_size=3, shrink_factor=0.95, initial_cauchy_scale=0.1, local_search_iterations=5, cauchy_probability=0.2, orthogonal_learning_probability=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = swarm_size\n        self.inertia_weight = inertia_weight\n        self.cognitive_coeff = cognitive_coeff\n        self.social_coeff = social_coeff\n        self.neighborhood_size = neighborhood_size\n        self.shrink_factor = shrink_factor\n        self.cauchy_scale = initial_cauchy_scale\n        self.local_search_iterations = local_search_iterations\n        self.cauchy_probability = cauchy_probability  # Probability of applying Cauchy mutation\n        self.orthogonal_learning_probability = orthogonal_learning_probability\n        self.success_rate = 0.5  # Initialize success rate\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        \n        # Initialize swarm positions and velocities\n        swarm_positions = np.random.uniform(lb, ub, size=(self.swarm_size, self.dim))\n        swarm_velocities = np.random.uniform(-abs(ub-lb)/10, abs(ub-lb)/10, size=(self.swarm_size, self.dim))  #Initialize with small velocities\n        swarm_fitness = np.array([func(x) for x in swarm_positions])\n        self.budget -= self.swarm_size #Update the remaining budget\n        \n        personal_best_positions = swarm_positions.copy()\n        personal_best_fitness = swarm_fitness.copy()\n        \n        global_best_index = np.argmin(personal_best_fitness)\n        global_best_position = personal_best_positions[global_best_index].copy()\n        global_best_fitness = personal_best_fitness[global_best_index]\n        \n        search_range = ub - lb\n        center = global_best_position\n\n        successes = 0\n        iterations = 0\n\n        # Adaptive Inertia Weight\n        inertia_weight_max = 0.9\n        inertia_weight_min = 0.2\n\n\n        while self.budget > 0:\n            # Adaptive Inertia Weight update\n            inertia_weight = inertia_weight_max - (inertia_weight_max - inertia_weight_min) * (iterations / self.budget)\n\n\n            for i in range(self.swarm_size):\n                # Bird Swarm Optimization part\n                inertia = inertia_weight * swarm_velocities[i]\n                cognitive = self.cognitive_coeff * np.random.rand(self.dim) * (personal_best_positions[i] - swarm_positions[i])\n                \n                # Social component: find best neighbor\n                neighbors_indices = np.random.choice(self.swarm_size, size=self.neighborhood_size, replace=False)  # Select random neighbors\n                neighbors_fitness = personal_best_fitness[neighbors_indices]\n                best_neighbor_index = neighbors_indices[np.argmin(neighbors_fitness)]\n                social = self.social_coeff * np.random.rand(self.dim) * (personal_best_positions[best_neighbor_index] - swarm_positions[i])\n                \n                swarm_velocities[i] = inertia + cognitive + social\n                \n                # Update position\n                new_position = swarm_positions[i] + swarm_velocities[i]\n                \n                # Boundary handling (clip the solution, if out of bounds)\n                new_position = np.clip(new_position, lb, ub)\n\n                # Cauchy mutation with probability\n                if np.random.rand() < self.cauchy_probability:\n                    mutation = center + search_range * np.random.standard_cauchy(size=self.dim) * self.cauchy_scale  # Scale Cauchy distribution\n                    mutation = np.clip(mutation, lb, ub)  # Clip values to stay within bounds\n                    new_position = mutation\n                \n                # Orthogonal Learning\n                if np.random.rand() < self.orthogonal_learning_probability:\n                    levels = 3  # Number of levels for orthogonal design\n                    indices = np.random.choice(self.dim, size=2, replace=False)\n                    x1_index, x2_index = indices[0], indices[1]\n\n                    x1_range = np.linspace(lb[x1_index], ub[x1_index], levels)\n                    x2_range = np.linspace(lb[x2_index], ub[x2_index], levels)\n                    \n                    orthogonal_design = np.array([[x1_range[0], x2_range[0]],\n                                                    [x1_range[1], x2_range[2]],\n                                                    [x1_range[2], x2_range[1]]])\n                    \n                    fitness_values = []\n                    for k in range(levels):\n                        temp_position = new_position.copy()\n                        temp_position[x1_index] = orthogonal_design[k, 0]\n                        temp_position[x2_index] = orthogonal_design[k, 1]\n                        fitness_values.append(func(temp_position))\n                        self.budget -= 1\n                        iterations += 1\n                        if self.budget <= 0:\n                           break\n\n                    best_index = np.argmin(fitness_values)\n                    new_position[x1_index] = orthogonal_design[best_index, 0]\n                    new_position[x2_index] = orthogonal_design[best_index, 1]\n                    \n                \n                new_fitness = func(new_position)\n                self.budget -= 1\n                iterations += 1\n                \n                if new_fitness < personal_best_fitness[i]:\n                    personal_best_fitness[i] = new_fitness\n                    personal_best_positions[i] = new_position.copy()\n                    successes += 1\n                    \n                    if new_fitness < global_best_fitness:\n                        global_best_fitness = new_fitness\n                        global_best_position = new_position.copy()\n                        center = global_best_position\n                \n                swarm_positions[i] = new_position\n\n            # Shrink the search range around the best individual\n            search_range *= self.shrink_factor\n            lb = np.clip(center - search_range/2, func.bounds.lb, func.bounds.ub)\n            ub = np.clip(center + search_range/2, func.bounds.lb, func.bounds.ub)\n\n             # Adjust Cauchy scale based on success rate\n            if iterations > 0:\n                self.success_rate = successes / iterations\n                if self.success_rate > 0.2:\n                    self.cauchy_scale *= 1.1  # Increase scale if successful\n                else:\n                    self.cauchy_scale *= 0.9  # Decrease scale if not successful\n                successes = 0\n                iterations = 0\n\n            self.cauchy_scale = np.clip(self.cauchy_scale, 0.01, 1.0) # Limit Cauchy scale\n            \n        \n        # Local Search around best solution\n        for _ in range(min(self.local_search_iterations, self.budget)):\n             if self.budget <= 0:\n                 break\n\n             perturbation = np.random.uniform(-search_range/100, search_range/100, self.dim)\n             local_x = np.clip(global_best_position + perturbation, lb, ub)\n             local_f = func(local_x)\n             self.budget -=1\n\n             if local_f < global_best_fitness:\n                  global_best_fitness = local_f\n                  global_best_position = local_x\n        \n        return global_best_fitness, global_best_position\n```", "aucs": [0], "operator": "mutation", "theta_sampled": 0.004885846915289518, "reward": 0.0, "generation": 9}, "task_prompt": ""}
{"id": "513e0e8e-0f6b-4eb6-ba0f-825e76b3b484", "fitness": 0.0, "name": "AdaptiveGravitationalSearch", "description": "An Adaptive Gravitational Search Algorithm with a dynamically adjusted gravitational constant and velocity clamping.", "code": "import numpy as np\n\nclass AdaptiveGravitationalSearch:\n    def __init__(self, budget=10000, dim=10, G_initial=100, alpha=20):\n        self.budget = budget\n        self.dim = dim\n        self.G_initial = G_initial  # Initial gravitational constant\n        self.alpha = alpha          # Decay rate for G\n        self.population_size = int(np.ceil(np.log(dim))) * 10 # Population size, scaled to dimension\n        self.positions = np.random.uniform(-5, 5, size=(self.population_size, self.dim))\n        self.velocities = np.zeros_like(self.positions)\n        self.fitness = np.full(self.population_size, np.inf)\n        self.best_fitness = np.inf\n        self.best_position = None\n        self.eval_count = 0\n\n    def __call__(self, func):\n        while self.eval_count < self.budget:\n            # Evaluate fitness\n            for i in range(self.population_size):\n                if self.eval_count < self.budget:\n                    f = func(self.positions[i])\n                    self.eval_count += 1\n                    self.fitness[i] = f\n                else:\n                    break\n\n            # Update best solution\n            best_index = np.argmin(self.fitness)\n            if self.fitness[best_index] < self.best_fitness:\n                self.best_fitness = self.fitness[best_index]\n                self.best_position = self.positions[best_index].copy()\n\n            # Calculate gravitational constant\n            G = self.G_initial * np.exp(-self.alpha * (self.eval_count / self.budget))\n\n            # Calculate masses\n            masses = np.exp(-self.fitness / np.mean(self.fitness))\n            masses = masses / np.sum(masses)\n\n            # Calculate total force on each particle\n            total_force = np.zeros_like(self.positions)\n            for i in range(self.population_size):\n                for j in range(self.population_size):\n                    if i != j:\n                        R = np.linalg.norm(self.positions[j] - self.positions[i]) + 1e-9 # Avoid division by zero\n                        force = G * masses[i] * masses[j] / R * (self.positions[j] - self.positions[i])\n                        total_force[i] += force\n\n            # Update velocities and positions\n            for i in range(self.population_size):\n                self.velocities[i] = 0.5 * self.velocities[i] + np.random.rand(self.dim) * total_force[i]  # Keep some inertia\n                \n                #Velocity clamping\n                v_max = 0.1 * (func.bounds.ub - func.bounds.lb)\n                self.velocities[i] = np.clip(self.velocities[i], -v_max, v_max)\n                \n                self.positions[i] += self.velocities[i]\n\n                # Boundary handling (reflecting)\n                for d in range(self.dim):\n                    if self.positions[i, d] < func.bounds.lb:\n                        self.positions[i, d] = func.bounds.lb + (func.bounds.lb - self.positions[i, d])\n                        self.velocities[i,d] = -self.velocities[i,d]\n                    elif self.positions[i, d] > func.bounds.ub:\n                        self.positions[i, d] = func.bounds.ub - (self.positions[i, d] - func.bounds.ub)\n                        self.velocities[i,d] = -self.velocities[i,d]\n\n        return self.best_fitness, self.best_position", "configspace": "", "generation": 9, "feedback": "", "error": "Evaluation error: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()", "parent_ids": [], "operator": "random_new", "metadata": {"llm_response": "# Description: An Adaptive Gravitational Search Algorithm with a dynamically adjusted gravitational constant and velocity clamping.\n# Code: \n```python\nimport numpy as np\n\nclass AdaptiveGravitationalSearch:\n    def __init__(self, budget=10000, dim=10, G_initial=100, alpha=20):\n        self.budget = budget\n        self.dim = dim\n        self.G_initial = G_initial  # Initial gravitational constant\n        self.alpha = alpha          # Decay rate for G\n        self.population_size = int(np.ceil(np.log(dim))) * 10 # Population size, scaled to dimension\n        self.positions = np.random.uniform(-5, 5, size=(self.population_size, self.dim))\n        self.velocities = np.zeros_like(self.positions)\n        self.fitness = np.full(self.population_size, np.inf)\n        self.best_fitness = np.inf\n        self.best_position = None\n        self.eval_count = 0\n\n    def __call__(self, func):\n        while self.eval_count < self.budget:\n            # Evaluate fitness\n            for i in range(self.population_size):\n                if self.eval_count < self.budget:\n                    f = func(self.positions[i])\n                    self.eval_count += 1\n                    self.fitness[i] = f\n                else:\n                    break\n\n            # Update best solution\n            best_index = np.argmin(self.fitness)\n            if self.fitness[best_index] < self.best_fitness:\n                self.best_fitness = self.fitness[best_index]\n                self.best_position = self.positions[best_index].copy()\n\n            # Calculate gravitational constant\n            G = self.G_initial * np.exp(-self.alpha * (self.eval_count / self.budget))\n\n            # Calculate masses\n            masses = np.exp(-self.fitness / np.mean(self.fitness))\n            masses = masses / np.sum(masses)\n\n            # Calculate total force on each particle\n            total_force = np.zeros_like(self.positions)\n            for i in range(self.population_size):\n                for j in range(self.population_size):\n                    if i != j:\n                        R = np.linalg.norm(self.positions[j] - self.positions[i]) + 1e-9 # Avoid division by zero\n                        force = G * masses[i] * masses[j] / R * (self.positions[j] - self.positions[i])\n                        total_force[i] += force\n\n            # Update velocities and positions\n            for i in range(self.population_size):\n                self.velocities[i] = 0.5 * self.velocities[i] + np.random.rand(self.dim) * total_force[i]  # Keep some inertia\n                \n                #Velocity clamping\n                v_max = 0.1 * (func.bounds.ub - func.bounds.lb)\n                self.velocities[i] = np.clip(self.velocities[i], -v_max, v_max)\n                \n                self.positions[i] += self.velocities[i]\n\n                # Boundary handling (reflecting)\n                for d in range(self.dim):\n                    if self.positions[i, d] < func.bounds.lb:\n                        self.positions[i, d] = func.bounds.lb + (func.bounds.lb - self.positions[i, d])\n                        self.velocities[i,d] = -self.velocities[i,d]\n                    elif self.positions[i, d] > func.bounds.ub:\n                        self.positions[i, d] = func.bounds.ub - (self.positions[i, d] - func.bounds.ub)\n                        self.velocities[i,d] = -self.velocities[i,d]\n\n        return self.best_fitness, self.best_position\n```", "operator": "random_new", "theta_sampled": 0.0034785480503511375, "reward": 0.0, "generation": 9}, "task_prompt": ""}
{"id": "da14a9a2-a7e3-4237-9cd2-913262058f3b", "fitness": 0.0, "name": "AdaptiveHybridBirdSwarmLevy", "description": "An adaptive hybrid bird swarm optimization with dynamic parameter control, Cauchy mutation and Lvy flight, combining global exploration with local exploitation for efficient optimization.", "code": "import numpy as np\n\nclass AdaptiveHybridBirdSwarmLevy:\n    def __init__(self, budget=10000, dim=10, swarm_size=20, inertia_weight=0.7, cognitive_coeff=1.4, social_coeff=1.4, neighborhood_size=3, shrink_factor=0.95, initial_cauchy_scale=0.1, local_search_iterations=5, cauchy_probability=0.2, exploration_probability=0.1, levy_flight_probability=0.05, success_rate_threshold=0.2):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = swarm_size\n        self.inertia_weight = inertia_weight\n        self.cognitive_coeff = cognitive_coeff\n        self.social_coeff = social_coeff\n        self.neighborhood_size = neighborhood_size\n        self.shrink_factor = shrink_factor\n        self.cauchy_scale = initial_cauchy_scale\n        self.local_search_iterations = local_search_iterations\n        self.cauchy_probability = cauchy_probability\n        self.exploration_probability = exploration_probability\n        self.levy_flight_probability = levy_flight_probability\n        self.success_rate_threshold = success_rate_threshold\n        self.success_rate = 0.5\n        self.inertia_weight_decay = 0.995\n        self.cognitive_coeff_increase = 1.005\n        self.social_coeff_decrease = 0.995\n        self.min_inertia_weight = 0.4\n\n    def levy_flight(self, beta=1.5):\n        sigma = (np.math.gamma(1 + beta) * np.sin(np.pi * beta / 2) / (np.math.gamma((1 + beta) / 2) * beta * (2 ** ((beta - 1) / 2)))) ** (1 / beta)\n        u = np.random.normal(0, sigma, size=self.dim)\n        v = np.random.normal(0, 1, size=self.dim)\n        step = u / abs(v) ** (1 / beta)\n        return step\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        \n        # Initialize swarm positions and velocities\n        swarm_positions = np.random.uniform(lb, ub, size=(self.swarm_size, self.dim))\n        swarm_velocities = np.random.uniform(-abs(ub-lb)/10, abs(ub-lb)/10, size=(self.swarm_size, self.dim))\n        swarm_fitness = np.array([func(x) for x in swarm_positions])\n        self.budget -= self.swarm_size\n        \n        personal_best_positions = swarm_positions.copy()\n        personal_best_fitness = swarm_fitness.copy()\n        \n        global_best_index = np.argmin(personal_best_fitness)\n        global_best_position = personal_best_positions[global_best_index].copy()\n        global_best_fitness = personal_best_fitness[global_best_index]\n        \n        search_range = ub - lb\n        center = global_best_position\n\n        successes = 0\n        iterations = 0\n\n        while self.budget > 0:\n            for i in range(self.swarm_size):\n                if self.budget <= 0:\n                    break\n\n                # Exploration phase: Random jump with a probability\n                if np.random.rand() < self.exploration_probability:\n                    new_position = np.random.uniform(lb, ub, size=self.dim)\n                else:\n                    # Bird Swarm Optimization part\n                    inertia = self.inertia_weight * swarm_velocities[i]\n                    cognitive = self.cognitive_coeff * np.random.rand(self.dim) * (personal_best_positions[i] - swarm_positions[i])\n                    \n                    # Social component: find best neighbor\n                    neighbors_indices = np.random.choice(self.swarm_size, size=self.neighborhood_size, replace=False)\n                    neighbors_fitness = personal_best_fitness[neighbors_indices]\n                    best_neighbor_index = neighbors_indices[np.argmin(neighbors_fitness)]\n                    social = self.social_coeff * np.random.rand(self.dim) * (personal_best_positions[best_neighbor_index] - swarm_positions[i])\n                    \n                    swarm_velocities[i] = inertia + cognitive + social\n                    \n                    # Update position\n                    new_position = swarm_positions[i] + swarm_velocities[i]\n                \n                #Levy Flight Mutation\n                if np.random.rand() < self.levy_flight_probability:\n                  step_size = 0.01 * search_range\n                  levy_step = self.levy_flight()\n                  new_position = swarm_positions[i] + step_size * levy_step\n                  \n\n                # Cauchy mutation with probability cauchy_probability\n                if np.random.rand() < self.cauchy_probability:\n                    mutation = center + search_range * np.random.standard_cauchy(size=self.dim) * self.cauchy_scale  # Scale Cauchy distribution\n                    new_position = mutation\n                \n                # Boundary handling\n                new_position = np.clip(new_position, lb, ub)\n                \n                new_fitness = func(new_position)\n                self.budget -= 1\n                iterations += 1\n                \n                if new_fitness < personal_best_fitness[i]:\n                    personal_best_fitness[i] = new_fitness\n                    personal_best_positions[i] = new_position.copy()\n                    successes += 1\n                    \n                    if new_fitness < global_best_fitness:\n                        global_best_fitness = new_fitness\n                        global_best_position = new_position.copy()\n                        center = global_best_position\n\n                swarm_positions[i] = new_position\n                \n            # Shrink search range\n            search_range *= self.shrink_factor\n            lb = np.clip(center - search_range/2, func.bounds.lb, func.bounds.ub)\n            ub = np.clip(center + search_range/2, func.bounds.lb, func.bounds.ub)\n\n            # Dynamic Parameter Control\n            self.inertia_weight = max(self.inertia_weight * self.inertia_weight_decay, self.min_inertia_weight)\n            self.cognitive_coeff *= self.cognitive_coeff_increase\n            self.social_coeff *= self.social_coeff_decrease\n            \n            # Adjust Cauchy scale based on success rate\n            if iterations > 0:\n                self.success_rate = successes / iterations\n                if self.success_rate > self.success_rate_threshold:\n                    self.cauchy_scale *= 1.1\n                else:\n                    self.cauchy_scale *= 0.9\n                successes = 0\n                iterations = 0\n\n            self.cauchy_scale = np.clip(self.cauchy_scale, 0.01, 1.0)\n        \n        # Local Search\n        for _ in range(min(self.local_search_iterations, self.budget)):\n             if self.budget <= 0:\n                 break\n\n             perturbation = np.random.uniform(-search_range/100, search_range/100, self.dim)\n             local_x = np.clip(global_best_position + perturbation, lb, ub)\n             local_f = func(local_x)\n             self.budget -=1\n\n             if local_f < global_best_fitness:\n                  global_best_fitness = local_f\n                  global_best_position = local_x\n\n        return global_best_fitness, global_best_position", "configspace": "", "generation": 9, "feedback": "", "error": "Evaluation error: module 'numpy' has no attribute 'math'", "parent_ids": ["5ed072b3-d57e-4c1e-8fc4-1c2b4f7cf697", "a59e4969-8f47-4293-8622-3e4ef39ee479"], "operator": "crossover", "metadata": {"llm_response": "# Description: An adaptive hybrid bird swarm optimization with dynamic parameter control, Cauchy mutation and Lvy flight, combining global exploration with local exploitation for efficient optimization.\n# Code: \n```python\nimport numpy as np\n\nclass AdaptiveHybridBirdSwarmLevy:\n    def __init__(self, budget=10000, dim=10, swarm_size=20, inertia_weight=0.7, cognitive_coeff=1.4, social_coeff=1.4, neighborhood_size=3, shrink_factor=0.95, initial_cauchy_scale=0.1, local_search_iterations=5, cauchy_probability=0.2, exploration_probability=0.1, levy_flight_probability=0.05, success_rate_threshold=0.2):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = swarm_size\n        self.inertia_weight = inertia_weight\n        self.cognitive_coeff = cognitive_coeff\n        self.social_coeff = social_coeff\n        self.neighborhood_size = neighborhood_size\n        self.shrink_factor = shrink_factor\n        self.cauchy_scale = initial_cauchy_scale\n        self.local_search_iterations = local_search_iterations\n        self.cauchy_probability = cauchy_probability\n        self.exploration_probability = exploration_probability\n        self.levy_flight_probability = levy_flight_probability\n        self.success_rate_threshold = success_rate_threshold\n        self.success_rate = 0.5\n        self.inertia_weight_decay = 0.995\n        self.cognitive_coeff_increase = 1.005\n        self.social_coeff_decrease = 0.995\n        self.min_inertia_weight = 0.4\n\n    def levy_flight(self, beta=1.5):\n        sigma = (np.math.gamma(1 + beta) * np.sin(np.pi * beta / 2) / (np.math.gamma((1 + beta) / 2) * beta * (2 ** ((beta - 1) / 2)))) ** (1 / beta)\n        u = np.random.normal(0, sigma, size=self.dim)\n        v = np.random.normal(0, 1, size=self.dim)\n        step = u / abs(v) ** (1 / beta)\n        return step\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        \n        # Initialize swarm positions and velocities\n        swarm_positions = np.random.uniform(lb, ub, size=(self.swarm_size, self.dim))\n        swarm_velocities = np.random.uniform(-abs(ub-lb)/10, abs(ub-lb)/10, size=(self.swarm_size, self.dim))\n        swarm_fitness = np.array([func(x) for x in swarm_positions])\n        self.budget -= self.swarm_size\n        \n        personal_best_positions = swarm_positions.copy()\n        personal_best_fitness = swarm_fitness.copy()\n        \n        global_best_index = np.argmin(personal_best_fitness)\n        global_best_position = personal_best_positions[global_best_index].copy()\n        global_best_fitness = personal_best_fitness[global_best_index]\n        \n        search_range = ub - lb\n        center = global_best_position\n\n        successes = 0\n        iterations = 0\n\n        while self.budget > 0:\n            for i in range(self.swarm_size):\n                if self.budget <= 0:\n                    break\n\n                # Exploration phase: Random jump with a probability\n                if np.random.rand() < self.exploration_probability:\n                    new_position = np.random.uniform(lb, ub, size=self.dim)\n                else:\n                    # Bird Swarm Optimization part\n                    inertia = self.inertia_weight * swarm_velocities[i]\n                    cognitive = self.cognitive_coeff * np.random.rand(self.dim) * (personal_best_positions[i] - swarm_positions[i])\n                    \n                    # Social component: find best neighbor\n                    neighbors_indices = np.random.choice(self.swarm_size, size=self.neighborhood_size, replace=False)\n                    neighbors_fitness = personal_best_fitness[neighbors_indices]\n                    best_neighbor_index = neighbors_indices[np.argmin(neighbors_fitness)]\n                    social = self.social_coeff * np.random.rand(self.dim) * (personal_best_positions[best_neighbor_index] - swarm_positions[i])\n                    \n                    swarm_velocities[i] = inertia + cognitive + social\n                    \n                    # Update position\n                    new_position = swarm_positions[i] + swarm_velocities[i]\n                \n                #Levy Flight Mutation\n                if np.random.rand() < self.levy_flight_probability:\n                  step_size = 0.01 * search_range\n                  levy_step = self.levy_flight()\n                  new_position = swarm_positions[i] + step_size * levy_step\n                  \n\n                # Cauchy mutation with probability cauchy_probability\n                if np.random.rand() < self.cauchy_probability:\n                    mutation = center + search_range * np.random.standard_cauchy(size=self.dim) * self.cauchy_scale  # Scale Cauchy distribution\n                    new_position = mutation\n                \n                # Boundary handling\n                new_position = np.clip(new_position, lb, ub)\n                \n                new_fitness = func(new_position)\n                self.budget -= 1\n                iterations += 1\n                \n                if new_fitness < personal_best_fitness[i]:\n                    personal_best_fitness[i] = new_fitness\n                    personal_best_positions[i] = new_position.copy()\n                    successes += 1\n                    \n                    if new_fitness < global_best_fitness:\n                        global_best_fitness = new_fitness\n                        global_best_position = new_position.copy()\n                        center = global_best_position\n\n                swarm_positions[i] = new_position\n                \n            # Shrink search range\n            search_range *= self.shrink_factor\n            lb = np.clip(center - search_range/2, func.bounds.lb, func.bounds.ub)\n            ub = np.clip(center + search_range/2, func.bounds.lb, func.bounds.ub)\n\n            # Dynamic Parameter Control\n            self.inertia_weight = max(self.inertia_weight * self.inertia_weight_decay, self.min_inertia_weight)\n            self.cognitive_coeff *= self.cognitive_coeff_increase\n            self.social_coeff *= self.social_coeff_decrease\n            \n            # Adjust Cauchy scale based on success rate\n            if iterations > 0:\n                self.success_rate = successes / iterations\n                if self.success_rate > self.success_rate_threshold:\n                    self.cauchy_scale *= 1.1\n                else:\n                    self.cauchy_scale *= 0.9\n                successes = 0\n                iterations = 0\n\n            self.cauchy_scale = np.clip(self.cauchy_scale, 0.01, 1.0)\n        \n        # Local Search\n        for _ in range(min(self.local_search_iterations, self.budget)):\n             if self.budget <= 0:\n                 break\n\n             perturbation = np.random.uniform(-search_range/100, search_range/100, self.dim)\n             local_x = np.clip(global_best_position + perturbation, lb, ub)\n             local_f = func(local_x)\n             self.budget -=1\n\n             if local_f < global_best_fitness:\n                  global_best_fitness = local_f\n                  global_best_position = local_x\n\n        return global_best_fitness, global_best_position\n```", "operator": "crossover", "theta_sampled": 0.007183359145928472, "reward": 0.0, "generation": 9}, "task_prompt": ""}
{"id": "df361d92-4468-4dee-9bea-cf16fff9ebcc", "fitness": 0.0, "name": "AdaptiveCauchyBirdSwarm", "description": "Combines Adaptive Bird Swarm with Cauchy mutation, dynamic parameter control, and a success-based mutation strategy for enhanced exploration and exploitation.", "code": "import numpy as np\n\nclass AdaptiveCauchyBirdSwarm:\n    def __init__(self, budget=10000, dim=10, swarm_size=20, inertia_weight=0.7, cognitive_coeff=1.4, social_coeff=1.4, neighborhood_size=3, shrink_factor=0.95, initial_cauchy_scale=0.1, local_search_iterations=5, cauchy_probability=0.2, exploration_probability=0.1, stagnation_threshold=10):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = swarm_size\n        self.inertia_weight = inertia_weight\n        self.cognitive_coeff = cognitive_coeff\n        self.social_coeff = social_coeff\n        self.neighborhood_size = neighborhood_size\n        self.shrink_factor = shrink_factor\n        self.cauchy_scale = initial_cauchy_scale\n        self.local_search_iterations = local_search_iterations\n        self.cauchy_probability = cauchy_probability\n        self.exploration_probability = exploration_probability\n        self.success_rate = 0.5\n        self.inertia_weight_decay = 0.995\n        self.cognitive_coeff_increase = 1.005\n        self.social_coeff_decrease = 0.995\n        self.min_inertia_weight = 0.4\n        self.stagnation_threshold = stagnation_threshold\n        self.stagnation_counter = 0\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        \n        # Initialize swarm positions and velocities\n        swarm_positions = np.random.uniform(lb, ub, size=(self.swarm_size, self.dim))\n        swarm_velocities = np.random.uniform(-abs(ub-lb)/10, abs(ub-lb)/10, size=(self.swarm_size, self.dim))\n        swarm_fitness = np.array([func(x) for x in swarm_positions])\n        self.budget -= self.swarm_size\n        \n        personal_best_positions = swarm_positions.copy()\n        personal_best_fitness = swarm_fitness.copy()\n        \n        global_best_index = np.argmin(personal_best_fitness)\n        global_best_position = personal_best_positions[global_best_index].copy()\n        global_best_fitness = personal_best_fitness[global_best_index]\n        \n        search_range = ub - lb\n        center = global_best_position\n\n        successes = 0\n        iterations = 0\n\n        while self.budget > 0:\n            previous_best_fitness = global_best_fitness  # Store the previous best fitness\n\n            for i in range(self.swarm_size):\n                if self.budget <= 0:\n                    break\n\n                # Exploration phase\n                if np.random.rand() < self.exploration_probability:\n                    new_position = np.random.uniform(lb, ub, size=self.dim)\n                else:\n                    # Bird Swarm Optimization part\n                    inertia = self.inertia_weight * swarm_velocities[i]\n                    cognitive = self.cognitive_coeff * np.random.rand(self.dim) * (personal_best_positions[i] - swarm_positions[i])\n                    \n                    # Social component\n                    neighbors_indices = np.random.choice(self.swarm_size, size=self.neighborhood_size, replace=False)\n                    neighbors_fitness = personal_best_fitness[neighbors_indices]\n                    best_neighbor_index = neighbors_indices[np.argmin(neighbors_fitness)]\n                    social = self.social_coeff * np.random.rand(self.dim) * (personal_best_positions[best_neighbor_index] - swarm_positions[i])\n                    \n                    swarm_velocities[i] = inertia + cognitive + social\n                    \n                    # Update position\n                    new_position = swarm_positions[i] + swarm_velocities[i]\n\n                # Cauchy mutation with probability\n                if np.random.rand() < self.cauchy_probability:\n                    # Apply Cauchy mutation, scaling dynamically with search range and success rate\n                    mutation = center + search_range * np.random.standard_cauchy(size=self.dim) * self.cauchy_scale\n                    new_position = mutation\n                \n                # Boundary handling\n                new_position = np.clip(new_position, lb, ub)\n                \n                new_fitness = func(new_position)\n                self.budget -= 1\n                iterations += 1\n                \n                if new_fitness < personal_best_fitness[i]:\n                    personal_best_fitness[i] = new_fitness\n                    personal_best_positions[i] = new_position.copy()\n                    successes += 1\n                    \n                    if new_fitness < global_best_fitness:\n                        global_best_fitness = new_fitness\n                        global_best_position = new_position.copy()\n                        center = global_best_position\n\n                swarm_positions[i] = new_position\n                \n            # Shrink search range\n            search_range *= self.shrink_factor\n            lb = np.clip(center - search_range/2, func.bounds.lb, func.bounds.ub)\n            ub = np.clip(center + search_range/2, func.bounds.lb, func.bounds.ub)\n\n            # Dynamic Parameter Control\n            self.inertia_weight = max(self.inertia_weight * self.inertia_weight_decay, self.min_inertia_weight)\n            self.cognitive_coeff *= self.cognitive_coeff_increase\n            self.social_coeff *= self.social_coeff_decrease\n\n            # Adjust Cauchy scale based on success rate\n            if iterations > 0:\n                self.success_rate = successes / iterations\n                if self.success_rate > 0.2:\n                    self.cauchy_scale *= 1.1\n                else:\n                    self.cauchy_scale *= 0.9\n                successes = 0\n                iterations = 0\n\n            self.cauchy_scale = np.clip(self.cauchy_scale, 0.01, 1.0)\n\n            # Stagnation detection and reset\n            if global_best_fitness >= previous_best_fitness:\n                self.stagnation_counter += 1\n            else:\n                self.stagnation_counter = 0\n\n            if self.stagnation_counter > self.stagnation_threshold:\n                # Reset a fraction of the swarm to random positions to escape stagnation\n                reset_fraction = 0.2\n                num_to_reset = int(self.swarm_size * reset_fraction)\n                indices_to_reset = np.random.choice(self.swarm_size, num_to_reset, replace=False)\n                swarm_positions[indices_to_reset] = np.random.uniform(lb, ub, size=(num_to_reset, self.dim))\n                swarm_fitness = np.array([func(x) for x in swarm_positions])\n                self.budget -= num_to_reset\n                personal_best_positions[indices_to_reset] = swarm_positions[indices_to_reset].copy()\n                personal_best_fitness[indices_to_reset] = swarm_fitness[indices_to_reset].copy()\n\n                global_best_index = np.argmin(personal_best_fitness)\n                global_best_position = personal_best_positions[global_best_index].copy()\n                global_best_fitness = personal_best_fitness[global_best_index]\n                center = global_best_position\n                self.stagnation_counter = 0\n\n        # Local Search\n        for _ in range(min(self.local_search_iterations, self.budget)):\n            if self.budget <= 0:\n                break\n\n            perturbation = np.random.uniform(-search_range/100, search_range/100, self.dim)\n            local_x = np.clip(global_best_position + perturbation, lb, ub)\n            local_f = func(local_x)\n            self.budget -= 1\n\n            if local_f < global_best_fitness:\n                global_best_fitness = local_f\n                global_best_position = local_x\n\n        return global_best_fitness, global_best_position", "configspace": "", "generation": 9, "feedback": "The algorithm AdaptiveCauchyBirdSwarm scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["5ed072b3-d57e-4c1e-8fc4-1c2b4f7cf697", "a59e4969-8f47-4293-8622-3e4ef39ee479"], "operator": "crossover", "metadata": {"llm_response": "# Description: Combines Adaptive Bird Swarm with Cauchy mutation, dynamic parameter control, and a success-based mutation strategy for enhanced exploration and exploitation.\n# Code: \n```python\nimport numpy as np\n\nclass AdaptiveCauchyBirdSwarm:\n    def __init__(self, budget=10000, dim=10, swarm_size=20, inertia_weight=0.7, cognitive_coeff=1.4, social_coeff=1.4, neighborhood_size=3, shrink_factor=0.95, initial_cauchy_scale=0.1, local_search_iterations=5, cauchy_probability=0.2, exploration_probability=0.1, stagnation_threshold=10):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = swarm_size\n        self.inertia_weight = inertia_weight\n        self.cognitive_coeff = cognitive_coeff\n        self.social_coeff = social_coeff\n        self.neighborhood_size = neighborhood_size\n        self.shrink_factor = shrink_factor\n        self.cauchy_scale = initial_cauchy_scale\n        self.local_search_iterations = local_search_iterations\n        self.cauchy_probability = cauchy_probability\n        self.exploration_probability = exploration_probability\n        self.success_rate = 0.5\n        self.inertia_weight_decay = 0.995\n        self.cognitive_coeff_increase = 1.005\n        self.social_coeff_decrease = 0.995\n        self.min_inertia_weight = 0.4\n        self.stagnation_threshold = stagnation_threshold\n        self.stagnation_counter = 0\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        \n        # Initialize swarm positions and velocities\n        swarm_positions = np.random.uniform(lb, ub, size=(self.swarm_size, self.dim))\n        swarm_velocities = np.random.uniform(-abs(ub-lb)/10, abs(ub-lb)/10, size=(self.swarm_size, self.dim))\n        swarm_fitness = np.array([func(x) for x in swarm_positions])\n        self.budget -= self.swarm_size\n        \n        personal_best_positions = swarm_positions.copy()\n        personal_best_fitness = swarm_fitness.copy()\n        \n        global_best_index = np.argmin(personal_best_fitness)\n        global_best_position = personal_best_positions[global_best_index].copy()\n        global_best_fitness = personal_best_fitness[global_best_index]\n        \n        search_range = ub - lb\n        center = global_best_position\n\n        successes = 0\n        iterations = 0\n\n        while self.budget > 0:\n            previous_best_fitness = global_best_fitness  # Store the previous best fitness\n\n            for i in range(self.swarm_size):\n                if self.budget <= 0:\n                    break\n\n                # Exploration phase\n                if np.random.rand() < self.exploration_probability:\n                    new_position = np.random.uniform(lb, ub, size=self.dim)\n                else:\n                    # Bird Swarm Optimization part\n                    inertia = self.inertia_weight * swarm_velocities[i]\n                    cognitive = self.cognitive_coeff * np.random.rand(self.dim) * (personal_best_positions[i] - swarm_positions[i])\n                    \n                    # Social component\n                    neighbors_indices = np.random.choice(self.swarm_size, size=self.neighborhood_size, replace=False)\n                    neighbors_fitness = personal_best_fitness[neighbors_indices]\n                    best_neighbor_index = neighbors_indices[np.argmin(neighbors_fitness)]\n                    social = self.social_coeff * np.random.rand(self.dim) * (personal_best_positions[best_neighbor_index] - swarm_positions[i])\n                    \n                    swarm_velocities[i] = inertia + cognitive + social\n                    \n                    # Update position\n                    new_position = swarm_positions[i] + swarm_velocities[i]\n\n                # Cauchy mutation with probability\n                if np.random.rand() < self.cauchy_probability:\n                    # Apply Cauchy mutation, scaling dynamically with search range and success rate\n                    mutation = center + search_range * np.random.standard_cauchy(size=self.dim) * self.cauchy_scale\n                    new_position = mutation\n                \n                # Boundary handling\n                new_position = np.clip(new_position, lb, ub)\n                \n                new_fitness = func(new_position)\n                self.budget -= 1\n                iterations += 1\n                \n                if new_fitness < personal_best_fitness[i]:\n                    personal_best_fitness[i] = new_fitness\n                    personal_best_positions[i] = new_position.copy()\n                    successes += 1\n                    \n                    if new_fitness < global_best_fitness:\n                        global_best_fitness = new_fitness\n                        global_best_position = new_position.copy()\n                        center = global_best_position\n\n                swarm_positions[i] = new_position\n                \n            # Shrink search range\n            search_range *= self.shrink_factor\n            lb = np.clip(center - search_range/2, func.bounds.lb, func.bounds.ub)\n            ub = np.clip(center + search_range/2, func.bounds.lb, func.bounds.ub)\n\n            # Dynamic Parameter Control\n            self.inertia_weight = max(self.inertia_weight * self.inertia_weight_decay, self.min_inertia_weight)\n            self.cognitive_coeff *= self.cognitive_coeff_increase\n            self.social_coeff *= self.social_coeff_decrease\n\n            # Adjust Cauchy scale based on success rate\n            if iterations > 0:\n                self.success_rate = successes / iterations\n                if self.success_rate > 0.2:\n                    self.cauchy_scale *= 1.1\n                else:\n                    self.cauchy_scale *= 0.9\n                successes = 0\n                iterations = 0\n\n            self.cauchy_scale = np.clip(self.cauchy_scale, 0.01, 1.0)\n\n            # Stagnation detection and reset\n            if global_best_fitness >= previous_best_fitness:\n                self.stagnation_counter += 1\n            else:\n                self.stagnation_counter = 0\n\n            if self.stagnation_counter > self.stagnation_threshold:\n                # Reset a fraction of the swarm to random positions to escape stagnation\n                reset_fraction = 0.2\n                num_to_reset = int(self.swarm_size * reset_fraction)\n                indices_to_reset = np.random.choice(self.swarm_size, num_to_reset, replace=False)\n                swarm_positions[indices_to_reset] = np.random.uniform(lb, ub, size=(num_to_reset, self.dim))\n                swarm_fitness = np.array([func(x) for x in swarm_positions])\n                self.budget -= num_to_reset\n                personal_best_positions[indices_to_reset] = swarm_positions[indices_to_reset].copy()\n                personal_best_fitness[indices_to_reset] = swarm_fitness[indices_to_reset].copy()\n\n                global_best_index = np.argmin(personal_best_fitness)\n                global_best_position = personal_best_positions[global_best_index].copy()\n                global_best_fitness = personal_best_fitness[global_best_index]\n                center = global_best_position\n                self.stagnation_counter = 0\n\n        # Local Search\n        for _ in range(min(self.local_search_iterations, self.budget)):\n            if self.budget <= 0:\n                break\n\n            perturbation = np.random.uniform(-search_range/100, search_range/100, self.dim)\n            local_x = np.clip(global_best_position + perturbation, lb, ub)\n            local_f = func(local_x)\n            self.budget -= 1\n\n            if local_f < global_best_fitness:\n                global_best_fitness = local_f\n                global_best_position = local_x\n\n        return global_best_fitness, global_best_position\n```", "aucs": [0], "operator": "crossover", "theta_sampled": 0.01977829717803389, "reward": 0.0, "generation": 9}, "task_prompt": ""}
{"id": "7f39ad48-10bd-4693-bf48-a08aea65c982", "fitness": 0.7072327143670395, "name": "AdaptiveDifferentialEvolution", "description": "An adaptive differential evolution strategy that adjusts mutation and crossover rates based on success, combined with a restart mechanism for stagnation.", "code": "import numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, CR=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F  # Mutation factor\n        self.CR = CR  # Crossover rate\n        self.lb = -5.0\n        self.ub = 5.0\n        self.stagnation_counter = 0\n        self.stagnation_threshold = budget // 10 #Restart if no improvement\n        self.best_fitness_history = []\n\n    def __call__(self, func):\n        self.population = np.random.uniform(self.lb, self.ub, (self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n        self.eval_count = self.pop_size # initial population evaluation\n\n        while self.eval_count < self.budget:\n            for i in range(self.pop_size):\n                # Adaptive F and CR\n                adaptive_F = self.F * (1 + 0.1 * np.random.randn())\n                adaptive_CR = self.CR * (1 + 0.1 * np.random.randn())\n                adaptive_F = np.clip(adaptive_F, 0.1, 1.0)\n                adaptive_CR = np.clip(adaptive_CR, 0.1, 1.0)\n                \n                # Mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = np.random.choice(idxs, 3, replace=False)\n                mutant = self.population[a] + adaptive_F * (self.population[b] - self.population[c])\n                mutant = np.clip(mutant, self.lb, self.ub)\n\n                # Crossover\n                trial = np.copy(self.population[i])\n                mask = np.random.rand(self.dim) < adaptive_CR\n                trial[mask] = mutant[mask]\n\n                # Selection\n                f_trial = func(trial)\n                self.eval_count += 1\n                \n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n            \n            #Stagnation Check and Restart\n            self.best_fitness_history.append(self.f_opt)\n            if len(self.best_fitness_history) > self.stagnation_threshold:\n                if self.best_fitness_history[-1] >= self.best_fitness_history[-self.stagnation_threshold]:\n                    self.stagnation_counter += 1\n                else:\n                    self.stagnation_counter = 0\n\n            if self.stagnation_counter >= self.stagnation_threshold:\n                #Restart the population\n                self.population = np.random.uniform(self.lb, self.ub, (self.pop_size, self.dim))\n                self.fitness = np.array([func(x) for x in self.population])\n                self.f_opt = np.min(self.fitness)\n                self.x_opt = self.population[np.argmin(self.fitness)]\n                self.eval_count += self.pop_size - (self.budget-self.eval_count) if self.eval_count + self.pop_size > self.budget else self.pop_size\n                self.stagnation_counter = 0\n                self.best_fitness_history = [self.f_opt] #Reset history to current best\n\n            if self.eval_count >= self.budget:\n                break\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 9, "feedback": "The algorithm AdaptiveDifferentialEvolution scored 0.707 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": "random_new", "metadata": {"llm_response": "# Description: An adaptive differential evolution strategy that adjusts mutation and crossover rates based on success, combined with a restart mechanism for stagnation.\n# Code: \n```python\nimport numpy as np\n\nclass AdaptiveDifferentialEvolution:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, CR=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F  # Mutation factor\n        self.CR = CR  # Crossover rate\n        self.lb = -5.0\n        self.ub = 5.0\n        self.stagnation_counter = 0\n        self.stagnation_threshold = budget // 10 #Restart if no improvement\n        self.best_fitness_history = []\n\n    def __call__(self, func):\n        self.population = np.random.uniform(self.lb, self.ub, (self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n        self.eval_count = self.pop_size # initial population evaluation\n\n        while self.eval_count < self.budget:\n            for i in range(self.pop_size):\n                # Adaptive F and CR\n                adaptive_F = self.F * (1 + 0.1 * np.random.randn())\n                adaptive_CR = self.CR * (1 + 0.1 * np.random.randn())\n                adaptive_F = np.clip(adaptive_F, 0.1, 1.0)\n                adaptive_CR = np.clip(adaptive_CR, 0.1, 1.0)\n                \n                # Mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = np.random.choice(idxs, 3, replace=False)\n                mutant = self.population[a] + adaptive_F * (self.population[b] - self.population[c])\n                mutant = np.clip(mutant, self.lb, self.ub)\n\n                # Crossover\n                trial = np.copy(self.population[i])\n                mask = np.random.rand(self.dim) < adaptive_CR\n                trial[mask] = mutant[mask]\n\n                # Selection\n                f_trial = func(trial)\n                self.eval_count += 1\n                \n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n            \n            #Stagnation Check and Restart\n            self.best_fitness_history.append(self.f_opt)\n            if len(self.best_fitness_history) > self.stagnation_threshold:\n                if self.best_fitness_history[-1] >= self.best_fitness_history[-self.stagnation_threshold]:\n                    self.stagnation_counter += 1\n                else:\n                    self.stagnation_counter = 0\n\n            if self.stagnation_counter >= self.stagnation_threshold:\n                #Restart the population\n                self.population = np.random.uniform(self.lb, self.ub, (self.pop_size, self.dim))\n                self.fitness = np.array([func(x) for x in self.population])\n                self.f_opt = np.min(self.fitness)\n                self.x_opt = self.population[np.argmin(self.fitness)]\n                self.eval_count += self.pop_size - (self.budget-self.eval_count) if self.eval_count + self.pop_size > self.budget else self.pop_size\n                self.stagnation_counter = 0\n                self.best_fitness_history = [self.f_opt] #Reset history to current best\n\n            if self.eval_count >= self.budget:\n                break\n        return self.f_opt, self.x_opt\n```", "aucs": [0.27937147149836694, 0.6773762012738789, 0.711009880023793, 0.8824885505666981, 0.7232685510664003, 0.7941869886633128, 0.6717684263079495, 0.6303245145480237, 0.7440397189872876, 0.7026632825485273, 0.8513207446702062, 0.9981781695822808, 0.6625393067373352, 0.7371967365131389, 0.9121957665521858, 0.7918359439329965, 0.6490615400478882, 0.8316338616130621, 0.28687829903518236, 0.6073163331722767], "operator": "random_new", "theta_sampled": 0.002785474894467179, "reward": 0.021534037265112493, "generation": 9}, "task_prompt": ""}
{"id": "d492dee7-3880-4f0f-b230-0ec0a1ba36da", "fitness": 0.0, "name": "AdaptiveHybridBirdSwarmLevy", "description": "An adaptive hybrid bird swarm optimizer with dynamic parameter adjustment and enhanced exploration using Lvy flights.", "code": "import numpy as np\n\nclass AdaptiveHybridBirdSwarmLevy:\n    def __init__(self, budget=10000, dim=10, swarm_size=20, inertia_weight=0.7, cognitive_coeff=1.4, social_coeff=1.4, neighborhood_size=3, shrink_factor=0.95, initial_cauchy_scale=0.1, local_search_iterations=5, cauchy_probability=0.2, exploration_probability=0.1, levy_flight_probability=0.05):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = swarm_size\n        self.inertia_weight = inertia_weight\n        self.cognitive_coeff = cognitive_coeff\n        self.social_coeff = social_coeff\n        self.neighborhood_size = neighborhood_size\n        self.shrink_factor = shrink_factor\n        self.cauchy_scale = initial_cauchy_scale\n        self.local_search_iterations = local_search_iterations\n        self.cauchy_probability = cauchy_probability\n        self.exploration_probability = exploration_probability\n        self.levy_flight_probability = levy_flight_probability  # Probability of applying Levy flight\n        self.success_rate = 0.5\n        self.inertia_weight_decay = 0.995\n        self.cognitive_coeff_increase = 1.005\n        self.social_coeff_decrease = 0.995\n        self.min_inertia_weight = 0.4\n        self.levy_exponent = 1.5  # Levy flight exponent (adjust for different characteristics)\n\n    def levy_flight(self, beta, dim):\n        sigma = (np.math.gamma(1 + beta) * np.sin(np.pi * beta / 2) / (np.math.gamma((1 + beta) / 2) * beta * (2 ** ((beta - 1) / 2)))) ** (1 / beta)\n        u = np.random.randn(dim) * sigma\n        v = np.random.randn(dim)\n        step = u / abs(v) ** (1 / beta)\n        return step\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        \n        # Initialize swarm positions and velocities\n        swarm_positions = np.random.uniform(lb, ub, size=(self.swarm_size, self.dim))\n        swarm_velocities = np.random.uniform(-abs(ub-lb)/10, abs(ub-lb)/10, size=(self.swarm_size, self.dim))\n        swarm_fitness = np.array([func(x) for x in swarm_positions])\n        self.budget -= self.swarm_size\n        \n        personal_best_positions = swarm_positions.copy()\n        personal_best_fitness = swarm_fitness.copy()\n        \n        global_best_index = np.argmin(personal_best_fitness)\n        global_best_position = personal_best_positions[global_best_index].copy()\n        global_best_fitness = personal_best_fitness[global_best_index]\n        \n        search_range = ub - lb\n        center = global_best_position\n\n        successes = 0\n        iterations = 0\n\n        while self.budget > 0:\n            for i in range(self.swarm_size):\n                if self.budget <= 0:\n                    break\n\n                # Exploration phase: Random jump with a probability\n                if np.random.rand() < self.exploration_probability:\n                    new_position = np.random.uniform(lb, ub, size=self.dim)\n                else:\n                    # Bird Swarm Optimization part\n                    inertia = self.inertia_weight * swarm_velocities[i]\n                    cognitive = self.cognitive_coeff * np.random.rand(self.dim) * (personal_best_positions[i] - swarm_positions[i])\n                    \n                    # Social component: find best neighbor\n                    neighbors_indices = np.random.choice(self.swarm_size, size=self.neighborhood_size, replace=False)\n                    neighbors_fitness = personal_best_fitness[neighbors_indices]\n                    best_neighbor_index = neighbors_indices[np.argmin(neighbors_fitness)]\n                    social = self.social_coeff * np.random.rand(self.dim) * (personal_best_positions[best_neighbor_index] - swarm_positions[i])\n                    \n                    swarm_velocities[i] = inertia + cognitive + social\n                    \n                    # Update position\n                    new_position = swarm_positions[i] + swarm_velocities[i]\n\n                # Cauchy mutation with probability cauchy_probability\n                if np.random.rand() < self.cauchy_probability:\n                    mutation = center + search_range * np.random.standard_cauchy(size=self.dim) * self.cauchy_scale\n                    new_position = mutation\n                    \n                # Levy Flight with Probability\n                if np.random.rand() < self.levy_flight_probability:\n                    levy_step = self.levy_flight(self.levy_exponent, self.dim)\n                    new_position = new_position + 0.01 * search_range * levy_step #0.01 scaling factor\n                \n                # Boundary handling\n                new_position = np.clip(new_position, lb, ub)\n                \n                new_fitness = func(new_position)\n                self.budget -= 1\n                iterations += 1\n                \n                if new_fitness < personal_best_fitness[i]:\n                    personal_best_fitness[i] = new_fitness\n                    personal_best_positions[i] = new_position.copy()\n                    successes += 1\n                    \n                    if new_fitness < global_best_fitness:\n                        global_best_fitness = new_fitness\n                        global_best_position = new_position.copy()\n                        center = global_best_position\n\n                swarm_positions[i] = new_position\n                \n            # Shrink search range\n            search_range *= self.shrink_factor\n            lb = np.clip(center - search_range/2, func.bounds.lb, func.bounds.ub)\n            ub = np.clip(center + search_range/2, func.bounds.lb, func.bounds.ub)\n\n            # Dynamic Parameter Control\n            self.inertia_weight = max(self.inertia_weight * self.inertia_weight_decay, self.min_inertia_weight)\n            self.cognitive_coeff *= self.cognitive_coeff_increase\n            self.social_coeff *= self.social_coeff_decrease\n            \n            # Adjust Cauchy scale based on success rate\n            if iterations > 0:\n                self.success_rate = successes / iterations\n                if self.success_rate > 0.2:\n                    self.cauchy_scale *= 1.1\n                else:\n                    self.cauchy_scale *= 0.9\n                successes = 0\n                iterations = 0\n\n            self.cauchy_scale = np.clip(self.cauchy_scale, 0.01, 1.0)\n        \n        # Local Search\n        for _ in range(min(self.local_search_iterations, self.budget)):\n             if self.budget <= 0:\n                 break\n\n             perturbation = np.random.uniform(-search_range/100, search_range/100, self.dim)\n             local_x = np.clip(global_best_position + perturbation, lb, ub)\n             local_f = func(local_x)\n             self.budget -=1\n\n             if local_f < global_best_fitness:\n                  global_best_fitness = local_f\n                  global_best_position = local_x\n\n        return global_best_fitness, global_best_position", "configspace": "", "generation": 9, "feedback": "", "error": "Evaluation error: module 'numpy' has no attribute 'math'", "parent_ids": ["5ed072b3-d57e-4c1e-8fc4-1c2b4f7cf697", "a59e4969-8f47-4293-8622-3e4ef39ee479"], "operator": "crossover", "metadata": {"llm_response": "# Description: An adaptive hybrid bird swarm optimizer with dynamic parameter adjustment and enhanced exploration using Lvy flights.\n# Code: \n```python\nimport numpy as np\n\nclass AdaptiveHybridBirdSwarmLevy:\n    def __init__(self, budget=10000, dim=10, swarm_size=20, inertia_weight=0.7, cognitive_coeff=1.4, social_coeff=1.4, neighborhood_size=3, shrink_factor=0.95, initial_cauchy_scale=0.1, local_search_iterations=5, cauchy_probability=0.2, exploration_probability=0.1, levy_flight_probability=0.05):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = swarm_size\n        self.inertia_weight = inertia_weight\n        self.cognitive_coeff = cognitive_coeff\n        self.social_coeff = social_coeff\n        self.neighborhood_size = neighborhood_size\n        self.shrink_factor = shrink_factor\n        self.cauchy_scale = initial_cauchy_scale\n        self.local_search_iterations = local_search_iterations\n        self.cauchy_probability = cauchy_probability\n        self.exploration_probability = exploration_probability\n        self.levy_flight_probability = levy_flight_probability  # Probability of applying Levy flight\n        self.success_rate = 0.5\n        self.inertia_weight_decay = 0.995\n        self.cognitive_coeff_increase = 1.005\n        self.social_coeff_decrease = 0.995\n        self.min_inertia_weight = 0.4\n        self.levy_exponent = 1.5  # Levy flight exponent (adjust for different characteristics)\n\n    def levy_flight(self, beta, dim):\n        sigma = (np.math.gamma(1 + beta) * np.sin(np.pi * beta / 2) / (np.math.gamma((1 + beta) / 2) * beta * (2 ** ((beta - 1) / 2)))) ** (1 / beta)\n        u = np.random.randn(dim) * sigma\n        v = np.random.randn(dim)\n        step = u / abs(v) ** (1 / beta)\n        return step\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        \n        # Initialize swarm positions and velocities\n        swarm_positions = np.random.uniform(lb, ub, size=(self.swarm_size, self.dim))\n        swarm_velocities = np.random.uniform(-abs(ub-lb)/10, abs(ub-lb)/10, size=(self.swarm_size, self.dim))\n        swarm_fitness = np.array([func(x) for x in swarm_positions])\n        self.budget -= self.swarm_size\n        \n        personal_best_positions = swarm_positions.copy()\n        personal_best_fitness = swarm_fitness.copy()\n        \n        global_best_index = np.argmin(personal_best_fitness)\n        global_best_position = personal_best_positions[global_best_index].copy()\n        global_best_fitness = personal_best_fitness[global_best_index]\n        \n        search_range = ub - lb\n        center = global_best_position\n\n        successes = 0\n        iterations = 0\n\n        while self.budget > 0:\n            for i in range(self.swarm_size):\n                if self.budget <= 0:\n                    break\n\n                # Exploration phase: Random jump with a probability\n                if np.random.rand() < self.exploration_probability:\n                    new_position = np.random.uniform(lb, ub, size=self.dim)\n                else:\n                    # Bird Swarm Optimization part\n                    inertia = self.inertia_weight * swarm_velocities[i]\n                    cognitive = self.cognitive_coeff * np.random.rand(self.dim) * (personal_best_positions[i] - swarm_positions[i])\n                    \n                    # Social component: find best neighbor\n                    neighbors_indices = np.random.choice(self.swarm_size, size=self.neighborhood_size, replace=False)\n                    neighbors_fitness = personal_best_fitness[neighbors_indices]\n                    best_neighbor_index = neighbors_indices[np.argmin(neighbors_fitness)]\n                    social = self.social_coeff * np.random.rand(self.dim) * (personal_best_positions[best_neighbor_index] - swarm_positions[i])\n                    \n                    swarm_velocities[i] = inertia + cognitive + social\n                    \n                    # Update position\n                    new_position = swarm_positions[i] + swarm_velocities[i]\n\n                # Cauchy mutation with probability cauchy_probability\n                if np.random.rand() < self.cauchy_probability:\n                    mutation = center + search_range * np.random.standard_cauchy(size=self.dim) * self.cauchy_scale\n                    new_position = mutation\n                    \n                # Levy Flight with Probability\n                if np.random.rand() < self.levy_flight_probability:\n                    levy_step = self.levy_flight(self.levy_exponent, self.dim)\n                    new_position = new_position + 0.01 * search_range * levy_step #0.01 scaling factor\n                \n                # Boundary handling\n                new_position = np.clip(new_position, lb, ub)\n                \n                new_fitness = func(new_position)\n                self.budget -= 1\n                iterations += 1\n                \n                if new_fitness < personal_best_fitness[i]:\n                    personal_best_fitness[i] = new_fitness\n                    personal_best_positions[i] = new_position.copy()\n                    successes += 1\n                    \n                    if new_fitness < global_best_fitness:\n                        global_best_fitness = new_fitness\n                        global_best_position = new_position.copy()\n                        center = global_best_position\n\n                swarm_positions[i] = new_position\n                \n            # Shrink search range\n            search_range *= self.shrink_factor\n            lb = np.clip(center - search_range/2, func.bounds.lb, func.bounds.ub)\n            ub = np.clip(center + search_range/2, func.bounds.lb, func.bounds.ub)\n\n            # Dynamic Parameter Control\n            self.inertia_weight = max(self.inertia_weight * self.inertia_weight_decay, self.min_inertia_weight)\n            self.cognitive_coeff *= self.cognitive_coeff_increase\n            self.social_coeff *= self.social_coeff_decrease\n            \n            # Adjust Cauchy scale based on success rate\n            if iterations > 0:\n                self.success_rate = successes / iterations\n                if self.success_rate > 0.2:\n                    self.cauchy_scale *= 1.1\n                else:\n                    self.cauchy_scale *= 0.9\n                successes = 0\n                iterations = 0\n\n            self.cauchy_scale = np.clip(self.cauchy_scale, 0.01, 1.0)\n        \n        # Local Search\n        for _ in range(min(self.local_search_iterations, self.budget)):\n             if self.budget <= 0:\n                 break\n\n             perturbation = np.random.uniform(-search_range/100, search_range/100, self.dim)\n             local_x = np.clip(global_best_position + perturbation, lb, ub)\n             local_f = func(local_x)\n             self.budget -=1\n\n             if local_f < global_best_fitness:\n                  global_best_fitness = local_f\n                  global_best_position = local_x\n\n        return global_best_fitness, global_best_position\n```", "operator": "crossover", "theta_sampled": 0.011706675147102488, "reward": 0.0, "generation": 9}, "task_prompt": ""}
{"id": "a09c17d3-3646-46c1-a1d1-68fa4dcb54d2", "fitness": 0.380083019371896, "name": "AdaptiveNeighborhoodDE", "description": "An adaptive population-based algorithm that combines elements of differential evolution with a neighborhood-based search strategy, dynamically adjusting parameters based on observed performance.", "code": "import numpy as np\n\nclass AdaptiveNeighborhoodDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, Cr=0.9, neighborhood_size=5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F  # Mutation factor\n        self.Cr = Cr # Crossover rate\n        self.neighborhood_size = neighborhood_size\n        self.population = None\n        self.fitness = None\n        self.best_idx = None\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.best_idx = np.argmin(self.fitness)\n\n    def mutate(self, idx, func):\n        # Neighborhood-based selection of parents\n        neighborhood_indices = np.random.choice(self.pop_size, self.neighborhood_size, replace=False)\n        neighborhood_indices = np.setdiff1d(neighborhood_indices, [idx])\n        if len(neighborhood_indices) < 2:\n            available_indices = np.setdiff1d(np.arange(self.pop_size), [idx])\n            replace_count = 2 - len(neighborhood_indices)\n            additional_indices = np.random.choice(available_indices, replace_count, replace=False)\n            neighborhood_indices = np.concatenate([neighborhood_indices, additional_indices])\n\n\n        a, b = np.random.choice(neighborhood_indices, 2, replace=False)\n        \n        mutant = self.population[idx] + self.F * (self.population[a] - self.population[b])\n        mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n        return mutant\n\n    def crossover(self, mutant, target):\n        trial = np.copy(target)\n        j_rand = np.random.randint(self.dim)\n        for j in range(self.dim):\n            if np.random.rand() < self.Cr or j == j_rand:\n                trial[j] = mutant[j]\n        return trial\n\n    def selection(self, trial, idx, func):\n        f_trial = func(trial)\n        if f_trial < self.fitness[idx]:\n            self.population[idx] = trial\n            self.fitness[idx] = f_trial\n            if f_trial < self.fitness[self.best_idx]:\n                self.best_idx = idx\n            return True\n        return False\n\n    def adapt_parameters(self, success_rate):\n        # Adaptive adjustment of F and Cr based on success rate\n        self.F = np.clip(self.F + 0.1 * (success_rate - 0.5), 0.1, 1.0)\n        self.Cr = np.clip(self.Cr + 0.1 * (success_rate - 0.5), 0.1, 1.0)\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        success_count = 0\n        evaluation_count = self.pop_size  # Account for initial population evaluation\n        \n        while evaluation_count < self.budget:\n            for i in range(self.pop_size):\n                if evaluation_count >= self.budget:\n                    break  # Stop if budget is exceeded within the inner loop\n\n                mutant = self.mutate(i, func)\n                trial = self.crossover(mutant, self.population[i])\n\n                success = self.selection(trial, i, func)\n                evaluation_count += 1  # Increment the evaluation count after each function evaluation within selection\n                if success:\n                    success_count += 1\n\n            # Parameter adaptation\n            if evaluation_count > self.pop_size:  # Ensure enough data for adaptation\n                success_rate = success_count / self.pop_size\n                self.adapt_parameters(success_rate)\n                success_count = 0  # Reset the success counter after adaptation\n\n        return self.fitness[self.best_idx], self.population[self.best_idx]", "configspace": "", "generation": 10, "feedback": "The algorithm AdaptiveNeighborhoodDE scored 0.380 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": "random_new", "metadata": {"llm_response": "# Description: An adaptive population-based algorithm that combines elements of differential evolution with a neighborhood-based search strategy, dynamically adjusting parameters based on observed performance.\n# Code: \n```python\nimport numpy as np\n\nclass AdaptiveNeighborhoodDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, Cr=0.9, neighborhood_size=5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F  # Mutation factor\n        self.Cr = Cr # Crossover rate\n        self.neighborhood_size = neighborhood_size\n        self.population = None\n        self.fitness = None\n        self.best_idx = None\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.best_idx = np.argmin(self.fitness)\n\n    def mutate(self, idx, func):\n        # Neighborhood-based selection of parents\n        neighborhood_indices = np.random.choice(self.pop_size, self.neighborhood_size, replace=False)\n        neighborhood_indices = np.setdiff1d(neighborhood_indices, [idx])\n        if len(neighborhood_indices) < 2:\n            available_indices = np.setdiff1d(np.arange(self.pop_size), [idx])\n            replace_count = 2 - len(neighborhood_indices)\n            additional_indices = np.random.choice(available_indices, replace_count, replace=False)\n            neighborhood_indices = np.concatenate([neighborhood_indices, additional_indices])\n\n\n        a, b = np.random.choice(neighborhood_indices, 2, replace=False)\n        \n        mutant = self.population[idx] + self.F * (self.population[a] - self.population[b])\n        mutant = np.clip(mutant, func.bounds.lb, func.bounds.ub)\n        return mutant\n\n    def crossover(self, mutant, target):\n        trial = np.copy(target)\n        j_rand = np.random.randint(self.dim)\n        for j in range(self.dim):\n            if np.random.rand() < self.Cr or j == j_rand:\n                trial[j] = mutant[j]\n        return trial\n\n    def selection(self, trial, idx, func):\n        f_trial = func(trial)\n        if f_trial < self.fitness[idx]:\n            self.population[idx] = trial\n            self.fitness[idx] = f_trial\n            if f_trial < self.fitness[self.best_idx]:\n                self.best_idx = idx\n            return True\n        return False\n\n    def adapt_parameters(self, success_rate):\n        # Adaptive adjustment of F and Cr based on success rate\n        self.F = np.clip(self.F + 0.1 * (success_rate - 0.5), 0.1, 1.0)\n        self.Cr = np.clip(self.Cr + 0.1 * (success_rate - 0.5), 0.1, 1.0)\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        success_count = 0\n        evaluation_count = self.pop_size  # Account for initial population evaluation\n        \n        while evaluation_count < self.budget:\n            for i in range(self.pop_size):\n                if evaluation_count >= self.budget:\n                    break  # Stop if budget is exceeded within the inner loop\n\n                mutant = self.mutate(i, func)\n                trial = self.crossover(mutant, self.population[i])\n\n                success = self.selection(trial, i, func)\n                evaluation_count += 1  # Increment the evaluation count after each function evaluation within selection\n                if success:\n                    success_count += 1\n\n            # Parameter adaptation\n            if evaluation_count > self.pop_size:  # Ensure enough data for adaptation\n                success_rate = success_count / self.pop_size\n                self.adapt_parameters(success_rate)\n                success_count = 0  # Reset the success counter after adaptation\n\n        return self.fitness[self.best_idx], self.population[self.best_idx]\n```", "aucs": [0.13740209858464547, 0.2886724397195387, 0.3893438772791695, 0.38452637101885234, 0.30392582210654984, 0.4053778244275742, 0.28381104886082, 0.3349839376907825, 0.29804238727828536, 0.20219823038573226, 0.43832390353684814, 0.9945143526431299, 0.3244726187699697, 0.2833169301444607, 0.7096120195098539, 0.3821587273959176, 0.286476413131421, 0.4511293135582113, 0.20561000845772315, 0.49776206293843483], "operator": "random_new", "theta_sampled": 0.011267211810249367, "reward": 0.0, "generation": 10}, "task_prompt": ""}
{"id": "9cc3c4fb-f36b-46e6-b83f-f886cf8ee991", "fitness": 0.0, "name": "AdaptiveDEBirdSwarmCauchy", "description": "A hybrid algorithm combining Adaptive Differential Evolution's mutation and crossover with a shrinking Cauchy-mutation based Bird Swarm Optimization for efficient exploration and exploitation.", "code": "import numpy as np\n\nclass AdaptiveDEBirdSwarmCauchy:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, CR=0.9, swarm_size=20, inertia_weight=0.7, cognitive_coeff=1.4, social_coeff=1.4, neighborhood_size=3, shrink_factor=0.95, initial_cauchy_scale=0.1, local_search_iterations=5, cauchy_probability=0.2):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F  # Mutation factor\n        self.CR = CR  # Crossover rate\n        self.lb = -5.0\n        self.ub = 5.0\n        self.stagnation_counter = 0\n        self.stagnation_threshold = budget // 10 #Restart if no improvement\n\n        self.swarm_size = swarm_size\n        self.inertia_weight = inertia_weight\n        self.cognitive_coeff = cognitive_coeff\n        self.social_coeff = social_coeff\n        self.neighborhood_size = neighborhood_size\n        self.shrink_factor = shrink_factor\n        self.cauchy_scale = initial_cauchy_scale\n        self.local_search_iterations = local_search_iterations\n        self.cauchy_probability = cauchy_probability  # Probability of applying Cauchy mutation\n        self.success_rate = 0.5  # Initialize success rate\n        self.best_fitness_history = []\n\n    def __call__(self, func):\n        # Initialize population for DE\n        self.population = np.random.uniform(self.lb, self.ub, (self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n        self.eval_count = self.pop_size # initial population evaluation\n\n        # Initialize swarm for BSO (using DE's best)\n        swarm_positions = np.random.uniform(self.lb, self.ub, size=(self.swarm_size, self.dim))\n        swarm_fitness = np.array([func(x) for x in swarm_positions])\n        self.eval_count += self.swarm_size\n        \n        personal_best_positions = swarm_positions.copy()\n        personal_best_fitness = swarm_fitness.copy()\n        \n        global_best_index = np.argmin(personal_best_fitness)\n        global_best_position = personal_best_positions[global_best_index].copy()\n        global_best_fitness = personal_best_fitness[global_best_index]\n\n        search_range = self.ub - self.lb\n        center = global_best_position\n\n        successes = 0\n        iterations = 0\n        \n        while self.eval_count < self.budget:\n            # Adaptive Differential Evolution step\n            for i in range(self.pop_size):\n                # Adaptive F and CR\n                adaptive_F = self.F * (1 + 0.1 * np.random.randn())\n                adaptive_CR = self.CR * (1 + 0.1 * np.random.randn())\n                adaptive_F = np.clip(adaptive_F, 0.1, 1.0)\n                adaptive_CR = np.clip(adaptive_CR, 0.1, 1.0)\n                \n                # Mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = np.random.choice(idxs, 3, replace=False)\n                mutant = self.population[a] + adaptive_F * (self.population[b] - self.population[c])\n                mutant = np.clip(mutant, self.lb, self.ub)\n\n                # Crossover\n                trial = np.copy(self.population[i])\n                mask = np.random.rand(self.dim) < adaptive_CR\n                trial[mask] = mutant[mask]\n\n                # Selection\n                f_trial = func(trial)\n                self.eval_count += 1\n                \n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                        global_best_fitness = f_trial\n                        global_best_position = trial\n            \n            # Bird Swarm Optimization step with shrinking search range and Cauchy mutation\n            for i in range(self.swarm_size):\n                # Bird Swarm Optimization part\n                inertia = self.inertia_weight * np.random.uniform(-1,1, self.dim) #swarm_velocities[i]\n                cognitive = self.cognitive_coeff * np.random.rand(self.dim) * (personal_best_positions[i] - swarm_positions[i])\n                \n                # Social component: find best neighbor\n                neighbors_indices = np.random.choice(self.swarm_size, size=self.neighborhood_size, replace=False)  # Select random neighbors\n                neighbors_fitness = personal_best_fitness[neighbors_indices]\n                best_neighbor_index = neighbors_indices[np.argmin(neighbors_fitness)]\n                social = self.social_coeff * np.random.rand(self.dim) * (personal_best_positions[best_neighbor_index] - swarm_positions[i])\n                \n                velocity = inertia + cognitive + social\n                \n                # Update position\n                new_position = swarm_positions[i] + velocity\n                \n                # Boundary handling (clip the solution, if out of bounds)\n                new_position = np.clip(new_position, self.lb, self.ub)\n\n                # Cauchy mutation with probability\n                if np.random.rand() < self.cauchy_probability:\n                    mutation = center + search_range * np.random.standard_cauchy(size=self.dim) * self.cauchy_scale  # Scale Cauchy distribution\n                    mutation = np.clip(mutation, self.lb, self.ub)  # Clip values to stay within bounds\n                    new_position = mutation\n                \n                new_fitness = func(new_position)\n                self.eval_count += 1\n                iterations += 1\n                \n                if new_fitness < personal_best_fitness[i]:\n                    personal_best_fitness[i] = new_fitness\n                    personal_best_positions[i] = new_position.copy()\n                    successes += 1\n                    \n                    if new_fitness < global_best_fitness:\n                        global_best_fitness = new_fitness\n                        global_best_position = new_position.copy()\n                        center = global_best_position\n                        self.f_opt = global_best_fitness\n                        self.x_opt = global_best_position\n\n                swarm_positions[i] = new_position\n\n            # Shrink the search range around the best individual\n            search_range *= self.shrink_factor\n            self.lb = np.clip(center - search_range/2, -5.0, 5.0)\n            self.ub = np.clip(center + search_range/2, -5.0, 5.0)\n\n             # Adjust Cauchy scale based on success rate\n            if iterations > 0:\n                self.success_rate = successes / iterations\n                if self.success_rate > 0.2:\n                    self.cauchy_scale *= 1.1  # Increase scale if successful\n                else:\n                    self.cauchy_scale *= 0.9  # Decrease scale if not successful\n                successes = 0\n                iterations = 0\n\n            self.cauchy_scale = np.clip(self.cauchy_scale, 0.01, 1.0) # Limit Cauchy scale\n\n            #Stagnation Check and Restart (DE part)\n            self.best_fitness_history.append(self.f_opt)\n            if len(self.best_fitness_history) > self.stagnation_threshold:\n                if self.best_fitness_history[-1] >= self.best_fitness_history[-self.stagnation_threshold]:\n                    self.stagnation_counter += 1\n                else:\n                    self.stagnation_counter = 0\n\n            if self.stagnation_counter >= self.stagnation_threshold:\n                #Restart the population\n                self.population = np.random.uniform(self.lb, self.ub, (self.pop_size, self.dim))\n                self.fitness = np.array([func(x) for x in self.population])\n                self.f_opt = np.min(self.fitness)\n                self.x_opt = self.population[np.argmin(self.fitness)]\n                self.eval_count += self.pop_size - (self.budget-self.eval_count) if self.eval_count + self.pop_size > self.budget else self.pop_size\n                self.stagnation_counter = 0\n                self.best_fitness_history = [self.f_opt] #Reset history to current best\n                \n                #Restart the swarm\n                swarm_positions = np.random.uniform(self.lb, self.ub, size=(self.swarm_size, self.dim))\n                swarm_fitness = np.array([func(x) for x in swarm_positions])\n                self.eval_count += self.swarm_size - (self.budget-self.eval_count) if self.eval_count + self.swarm_size > self.budget else self.swarm_size\n                personal_best_positions = swarm_positions.copy()\n                personal_best_fitness = swarm_fitness.copy()\n                global_best_index = np.argmin(personal_best_fitness)\n                global_best_position = personal_best_positions[global_best_index].copy()\n                global_best_fitness = personal_best_fitness[global_best_index]\n                center = global_best_position\n\n\n            if self.eval_count >= self.budget:\n                break\n                \n        # Local Search around best solution\n        for _ in range(min(self.local_search_iterations, self.budget-self.eval_count)):\n            if self.eval_count >= self.budget:\n                break\n\n            perturbation = np.random.uniform(-search_range/100, search_range/100, self.dim)\n            local_x = np.clip(global_best_position + perturbation, self.lb, self.ub)\n            local_f = func(local_x)\n            self.eval_count +=1\n\n            if local_f < global_best_fitness:\n                global_best_fitness = local_f\n                global_best_position = local_x\n                self.f_opt = global_best_fitness\n                self.x_opt = global_best_position\n        \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 10, "feedback": "The algorithm AdaptiveDEBirdSwarmCauchy scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["7f39ad48-10bd-4693-bf48-a08aea65c982", "5ed072b3-d57e-4c1e-8fc4-1c2b4f7cf697"], "operator": "crossover", "metadata": {"llm_response": "# Description: A hybrid algorithm combining Adaptive Differential Evolution's mutation and crossover with a shrinking Cauchy-mutation based Bird Swarm Optimization for efficient exploration and exploitation.\n# Code: \n```python\nimport numpy as np\n\nclass AdaptiveDEBirdSwarmCauchy:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, CR=0.9, swarm_size=20, inertia_weight=0.7, cognitive_coeff=1.4, social_coeff=1.4, neighborhood_size=3, shrink_factor=0.95, initial_cauchy_scale=0.1, local_search_iterations=5, cauchy_probability=0.2):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F  # Mutation factor\n        self.CR = CR  # Crossover rate\n        self.lb = -5.0\n        self.ub = 5.0\n        self.stagnation_counter = 0\n        self.stagnation_threshold = budget // 10 #Restart if no improvement\n\n        self.swarm_size = swarm_size\n        self.inertia_weight = inertia_weight\n        self.cognitive_coeff = cognitive_coeff\n        self.social_coeff = social_coeff\n        self.neighborhood_size = neighborhood_size\n        self.shrink_factor = shrink_factor\n        self.cauchy_scale = initial_cauchy_scale\n        self.local_search_iterations = local_search_iterations\n        self.cauchy_probability = cauchy_probability  # Probability of applying Cauchy mutation\n        self.success_rate = 0.5  # Initialize success rate\n        self.best_fitness_history = []\n\n    def __call__(self, func):\n        # Initialize population for DE\n        self.population = np.random.uniform(self.lb, self.ub, (self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n        self.eval_count = self.pop_size # initial population evaluation\n\n        # Initialize swarm for BSO (using DE's best)\n        swarm_positions = np.random.uniform(self.lb, self.ub, size=(self.swarm_size, self.dim))\n        swarm_fitness = np.array([func(x) for x in swarm_positions])\n        self.eval_count += self.swarm_size\n        \n        personal_best_positions = swarm_positions.copy()\n        personal_best_fitness = swarm_fitness.copy()\n        \n        global_best_index = np.argmin(personal_best_fitness)\n        global_best_position = personal_best_positions[global_best_index].copy()\n        global_best_fitness = personal_best_fitness[global_best_index]\n\n        search_range = self.ub - self.lb\n        center = global_best_position\n\n        successes = 0\n        iterations = 0\n        \n        while self.eval_count < self.budget:\n            # Adaptive Differential Evolution step\n            for i in range(self.pop_size):\n                # Adaptive F and CR\n                adaptive_F = self.F * (1 + 0.1 * np.random.randn())\n                adaptive_CR = self.CR * (1 + 0.1 * np.random.randn())\n                adaptive_F = np.clip(adaptive_F, 0.1, 1.0)\n                adaptive_CR = np.clip(adaptive_CR, 0.1, 1.0)\n                \n                # Mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = np.random.choice(idxs, 3, replace=False)\n                mutant = self.population[a] + adaptive_F * (self.population[b] - self.population[c])\n                mutant = np.clip(mutant, self.lb, self.ub)\n\n                # Crossover\n                trial = np.copy(self.population[i])\n                mask = np.random.rand(self.dim) < adaptive_CR\n                trial[mask] = mutant[mask]\n\n                # Selection\n                f_trial = func(trial)\n                self.eval_count += 1\n                \n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                        global_best_fitness = f_trial\n                        global_best_position = trial\n            \n            # Bird Swarm Optimization step with shrinking search range and Cauchy mutation\n            for i in range(self.swarm_size):\n                # Bird Swarm Optimization part\n                inertia = self.inertia_weight * np.random.uniform(-1,1, self.dim) #swarm_velocities[i]\n                cognitive = self.cognitive_coeff * np.random.rand(self.dim) * (personal_best_positions[i] - swarm_positions[i])\n                \n                # Social component: find best neighbor\n                neighbors_indices = np.random.choice(self.swarm_size, size=self.neighborhood_size, replace=False)  # Select random neighbors\n                neighbors_fitness = personal_best_fitness[neighbors_indices]\n                best_neighbor_index = neighbors_indices[np.argmin(neighbors_fitness)]\n                social = self.social_coeff * np.random.rand(self.dim) * (personal_best_positions[best_neighbor_index] - swarm_positions[i])\n                \n                velocity = inertia + cognitive + social\n                \n                # Update position\n                new_position = swarm_positions[i] + velocity\n                \n                # Boundary handling (clip the solution, if out of bounds)\n                new_position = np.clip(new_position, self.lb, self.ub)\n\n                # Cauchy mutation with probability\n                if np.random.rand() < self.cauchy_probability:\n                    mutation = center + search_range * np.random.standard_cauchy(size=self.dim) * self.cauchy_scale  # Scale Cauchy distribution\n                    mutation = np.clip(mutation, self.lb, self.ub)  # Clip values to stay within bounds\n                    new_position = mutation\n                \n                new_fitness = func(new_position)\n                self.eval_count += 1\n                iterations += 1\n                \n                if new_fitness < personal_best_fitness[i]:\n                    personal_best_fitness[i] = new_fitness\n                    personal_best_positions[i] = new_position.copy()\n                    successes += 1\n                    \n                    if new_fitness < global_best_fitness:\n                        global_best_fitness = new_fitness\n                        global_best_position = new_position.copy()\n                        center = global_best_position\n                        self.f_opt = global_best_fitness\n                        self.x_opt = global_best_position\n\n                swarm_positions[i] = new_position\n\n            # Shrink the search range around the best individual\n            search_range *= self.shrink_factor\n            self.lb = np.clip(center - search_range/2, -5.0, 5.0)\n            self.ub = np.clip(center + search_range/2, -5.0, 5.0)\n\n             # Adjust Cauchy scale based on success rate\n            if iterations > 0:\n                self.success_rate = successes / iterations\n                if self.success_rate > 0.2:\n                    self.cauchy_scale *= 1.1  # Increase scale if successful\n                else:\n                    self.cauchy_scale *= 0.9  # Decrease scale if not successful\n                successes = 0\n                iterations = 0\n\n            self.cauchy_scale = np.clip(self.cauchy_scale, 0.01, 1.0) # Limit Cauchy scale\n\n            #Stagnation Check and Restart (DE part)\n            self.best_fitness_history.append(self.f_opt)\n            if len(self.best_fitness_history) > self.stagnation_threshold:\n                if self.best_fitness_history[-1] >= self.best_fitness_history[-self.stagnation_threshold]:\n                    self.stagnation_counter += 1\n                else:\n                    self.stagnation_counter = 0\n\n            if self.stagnation_counter >= self.stagnation_threshold:\n                #Restart the population\n                self.population = np.random.uniform(self.lb, self.ub, (self.pop_size, self.dim))\n                self.fitness = np.array([func(x) for x in self.population])\n                self.f_opt = np.min(self.fitness)\n                self.x_opt = self.population[np.argmin(self.fitness)]\n                self.eval_count += self.pop_size - (self.budget-self.eval_count) if self.eval_count + self.pop_size > self.budget else self.pop_size\n                self.stagnation_counter = 0\n                self.best_fitness_history = [self.f_opt] #Reset history to current best\n                \n                #Restart the swarm\n                swarm_positions = np.random.uniform(self.lb, self.ub, size=(self.swarm_size, self.dim))\n                swarm_fitness = np.array([func(x) for x in swarm_positions])\n                self.eval_count += self.swarm_size - (self.budget-self.eval_count) if self.eval_count + self.swarm_size > self.budget else self.swarm_size\n                personal_best_positions = swarm_positions.copy()\n                personal_best_fitness = swarm_fitness.copy()\n                global_best_index = np.argmin(personal_best_fitness)\n                global_best_position = personal_best_positions[global_best_index].copy()\n                global_best_fitness = personal_best_fitness[global_best_index]\n                center = global_best_position\n\n\n            if self.eval_count >= self.budget:\n                break\n                \n        # Local Search around best solution\n        for _ in range(min(self.local_search_iterations, self.budget-self.eval_count)):\n            if self.eval_count >= self.budget:\n                break\n\n            perturbation = np.random.uniform(-search_range/100, search_range/100, self.dim)\n            local_x = np.clip(global_best_position + perturbation, self.lb, self.ub)\n            local_f = func(local_x)\n            self.eval_count +=1\n\n            if local_f < global_best_fitness:\n                global_best_fitness = local_f\n                global_best_position = local_x\n                self.f_opt = global_best_fitness\n                self.x_opt = global_best_position\n        \n        return self.f_opt, self.x_opt\n```", "aucs": [0], "operator": "crossover", "theta_sampled": 0.017715759362687822, "reward": 0.0, "generation": 10}, "task_prompt": ""}
{"id": "f7eb0ee3-44b3-4b3e-a885-87fb2aee3dda", "fitness": 0.3991927227961251, "name": "DifferentialEvolutionLocalSearch", "description": "A population-based algorithm that combines aspects of differential evolution with a local search component guided by a shrinking neighborhood radius.", "code": "import numpy as np\n\nclass DifferentialEvolutionLocalSearch:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.7, Cr=0.9, local_search_radius=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.Cr = Cr\n        self.local_search_radius = local_search_radius\n        self.lb = -5.0\n        self.ub = 5.0\n\n    def __call__(self, func):\n        # Initialization\n        population = np.random.uniform(self.lb, self.ub, (self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        evaluations = self.pop_size\n        \n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index]\n\n        while evaluations < self.budget:\n            for i in range(self.pop_size):\n                # Differential Evolution mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = a + self.F * (b - c)\n                mutant = np.clip(mutant, self.lb, self.ub)\n\n                # Crossover\n                crossover_mask = np.random.rand(self.dim) < self.Cr\n                trial_vector = np.where(crossover_mask, mutant, population[i])\n\n                # Evaluate trial vector\n                f_trial = func(trial_vector)\n                evaluations += 1\n                \n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    population[i] = trial_vector\n\n                    # Local Search around the improved solution\n                    x_local_search = np.clip(trial_vector + np.random.uniform(-self.local_search_radius, self.local_search_radius, self.dim), self.lb, self.ub)\n                    f_local_search = func(x_local_search)\n                    evaluations += 1\n                    if f_local_search < fitness[i]:\n                        fitness[i] = f_local_search\n                        population[i] = x_local_search\n                \n                if fitness[i] < self.f_opt:\n                    self.f_opt = fitness[i]\n                    self.x_opt = population[i]\n\n                if evaluations >= self.budget:\n                    break\n            \n            #Reduce the radius of local search as the search progresses\n            self.local_search_radius *= 0.995 #shrink the radius to focus around local minima.\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 10, "feedback": "The algorithm DifferentialEvolutionLocalSearch scored 0.399 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": "random_new", "metadata": {"llm_response": "# Description: A population-based algorithm that combines aspects of differential evolution with a local search component guided by a shrinking neighborhood radius.\n# Code: \n```python\nimport numpy as np\n\nclass DifferentialEvolutionLocalSearch:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.7, Cr=0.9, local_search_radius=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.Cr = Cr\n        self.local_search_radius = local_search_radius\n        self.lb = -5.0\n        self.ub = 5.0\n\n    def __call__(self, func):\n        # Initialization\n        population = np.random.uniform(self.lb, self.ub, (self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        evaluations = self.pop_size\n        \n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index]\n\n        while evaluations < self.budget:\n            for i in range(self.pop_size):\n                # Differential Evolution mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = population[np.random.choice(idxs, 3, replace=False)]\n                mutant = a + self.F * (b - c)\n                mutant = np.clip(mutant, self.lb, self.ub)\n\n                # Crossover\n                crossover_mask = np.random.rand(self.dim) < self.Cr\n                trial_vector = np.where(crossover_mask, mutant, population[i])\n\n                # Evaluate trial vector\n                f_trial = func(trial_vector)\n                evaluations += 1\n                \n                if f_trial < fitness[i]:\n                    fitness[i] = f_trial\n                    population[i] = trial_vector\n\n                    # Local Search around the improved solution\n                    x_local_search = np.clip(trial_vector + np.random.uniform(-self.local_search_radius, self.local_search_radius, self.dim), self.lb, self.ub)\n                    f_local_search = func(x_local_search)\n                    evaluations += 1\n                    if f_local_search < fitness[i]:\n                        fitness[i] = f_local_search\n                        population[i] = x_local_search\n                \n                if fitness[i] < self.f_opt:\n                    self.f_opt = fitness[i]\n                    self.x_opt = population[i]\n\n                if evaluations >= self.budget:\n                    break\n            \n            #Reduce the radius of local search as the search progresses\n            self.local_search_radius *= 0.995 #shrink the radius to focus around local minima.\n\n        return self.f_opt, self.x_opt\n```", "aucs": [0.20027908491980506, 0.4428760225630811, 0.4954684100270622, 0.7660807388329762, 0.49045208043382604, 0], "operator": "random_new", "theta_sampled": 0.006175222606942605, "reward": 0.0, "generation": 10}, "task_prompt": ""}
{"id": "3d72add0-bb95-4cf9-978c-9744880e2171", "fitness": 0.0, "name": "AdaptiveBirdSwarmLevy", "description": "Adaptive Bird Swarm with Lvy flight and dynamic parameter adjustments for enhanced exploration and exploitation.", "code": "import numpy as np\n\nclass AdaptiveBirdSwarmLevy:\n    def __init__(self, budget=10000, dim=10, swarm_size=20, inertia_weight=0.7, cognitive_coeff=1.4, social_coeff=1.4, neighborhood_size=3, levy_exponent=1.5, shrink_factor=0.95, initial_levy_scale=0.1, local_search_iterations=5, levy_probability=0.2, velocity_damping=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = swarm_size\n        self.inertia_weight = inertia_weight\n        self.cognitive_coeff = cognitive_coeff\n        self.social_coeff = social_coeff\n        self.neighborhood_size = neighborhood_size\n        self.levy_exponent = levy_exponent\n        self.shrink_factor = shrink_factor\n        self.levy_scale = initial_levy_scale\n        self.local_search_iterations = local_search_iterations\n        self.levy_probability = levy_probability  # Probability of applying Levy flight mutation\n        self.velocity_damping = velocity_damping  # Damping factor for velocities\n        self.success_rate = 0.5  # Initialize success rate\n\n    def levy_flight(self, size):\n        \"\"\"Generate Levy distribution samples.\"\"\"\n        u = np.random.randn(size)\n        v = np.random.randn(size)\n        sigma = (np.math.gamma(1 + self.levy_exponent) * np.sin(np.pi * self.levy_exponent / 2) / (np.math.gamma((1 + self.levy_exponent) / 2) * self.levy_exponent * (2 ** ((self.levy_exponent - 1) / 2)))) ** (1 / self.levy_exponent)\n        step = sigma * u / (np.abs(v) ** (1 / self.levy_exponent))\n        return step\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        \n        # Initialize swarm positions and velocities\n        swarm_positions = np.random.uniform(lb, ub, size=(self.swarm_size, self.dim))\n        swarm_velocities = np.random.uniform(-abs(ub-lb)/10, abs(ub-lb)/10, size=(self.swarm_size, self.dim))  #Initialize with small velocities\n        swarm_fitness = np.array([func(x) for x in swarm_positions])\n        self.budget -= self.swarm_size #Update the remaining budget\n        \n        personal_best_positions = swarm_positions.copy()\n        personal_best_fitness = swarm_fitness.copy()\n        \n        global_best_index = np.argmin(personal_best_fitness)\n        global_best_position = personal_best_positions[global_best_index].copy()\n        global_best_fitness = personal_best_fitness[global_best_index]\n        \n        search_range = ub - lb\n        center = global_best_position\n\n        successes = 0\n        iterations = 0\n        \n        # Adaptive parameter adjustments\n        inertia_weight = self.inertia_weight\n        cognitive_coeff = self.cognitive_coeff\n        social_coeff = self.social_coeff\n\n\n        while self.budget > 0:\n            for i in range(self.swarm_size):\n                # Bird Swarm Optimization part\n                inertia = inertia_weight * swarm_velocities[i]\n                cognitive = cognitive_coeff * np.random.rand(self.dim) * (personal_best_positions[i] - swarm_positions[i])\n                \n                # Social component: find best neighbor\n                neighbors_indices = np.random.choice(self.swarm_size, size=self.neighborhood_size, replace=False)  # Select random neighbors\n                neighbors_fitness = personal_best_fitness[neighbors_indices]\n                best_neighbor_index = neighbors_indices[np.argmin(neighbors_fitness)]\n                social = social_coeff * np.random.rand(self.dim) * (personal_best_positions[best_neighbor_index] - swarm_positions[i])\n                \n                swarm_velocities[i] = self.velocity_damping * (inertia + cognitive + social)\n                \n                # Update position\n                new_position = swarm_positions[i] + swarm_velocities[i]\n                \n                # Boundary handling (clip the solution, if out of bounds)\n                new_position = np.clip(new_position, lb, ub)\n\n                # Levy flight mutation with probability\n                if np.random.rand() < self.levy_probability:\n                    levy_steps = self.levy_flight(self.dim)\n                    mutation = center + search_range * levy_steps * self.levy_scale  # Scale Levy distribution\n                    mutation = np.clip(mutation, lb, ub)  # Clip values to stay within bounds\n                    new_position = mutation\n                \n                new_fitness = func(new_position)\n                self.budget -= 1\n                iterations += 1\n                \n                if new_fitness < personal_best_fitness[i]:\n                    personal_best_fitness[i] = new_fitness\n                    personal_best_positions[i] = new_position.copy()\n                    successes += 1\n                    \n                    if new_fitness < global_best_fitness:\n                        global_best_fitness = new_fitness\n                        global_best_position = new_position.copy()\n                        center = global_best_position\n                \n                swarm_positions[i] = new_position\n\n            # Shrink the search range around the best individual\n            search_range *= self.shrink_factor\n            lb = np.clip(center - search_range/2, func.bounds.lb, func.bounds.ub)\n            ub = np.clip(center + search_range/2, func.bounds.lb, func.bounds.ub)\n\n            # Adjust Levy scale based on success rate\n            if iterations > 0:\n                self.success_rate = successes / iterations\n                if self.success_rate > 0.2:\n                    self.levy_scale *= 1.1  # Increase scale if successful\n                    inertia_weight = min(inertia_weight + 0.05, 0.9) #Adapt inertia weight\n                    cognitive_coeff = max(cognitive_coeff - 0.05, 1.0) #Adapt cognitive coeff\n                    social_coeff = max(social_coeff - 0.05, 1.0) #Adapt social coeff\n                else:\n                    self.levy_scale *= 0.9  # Decrease scale if not successful\n                    inertia_weight = max(inertia_weight - 0.05, 0.4) #Adapt inertia weight\n                    cognitive_coeff = min(cognitive_coeff + 0.05, 2.0) #Adapt cognitive coeff\n                    social_coeff = min(social_coeff + 0.05, 2.0) #Adapt social coeff\n                successes = 0\n                iterations = 0\n\n            self.levy_scale = np.clip(self.levy_scale, 0.01, 1.0) # Limit Levy scale\n            \n        \n        # Local Search around best solution\n        for _ in range(min(self.local_search_iterations, self.budget)):\n             if self.budget <= 0:\n                 break\n\n             perturbation = np.random.uniform(-search_range/100, search_range/100, self.dim)\n             local_x = np.clip(global_best_position + perturbation, lb, ub)\n             local_f = func(local_x)\n             self.budget -=1\n\n             if local_f < global_best_fitness:\n                  global_best_fitness = local_f\n                  global_best_position = local_x\n        \n        return global_best_fitness, global_best_position", "configspace": "", "generation": 10, "feedback": "", "error": "Evaluation error: module 'numpy' has no attribute 'math'", "parent_ids": ["5ed072b3-d57e-4c1e-8fc4-1c2b4f7cf697"], "operator": "mutation", "metadata": {"llm_response": "# Description: Adaptive Bird Swarm with Lvy flight and dynamic parameter adjustments for enhanced exploration and exploitation.\n# Code: \n```python\nimport numpy as np\n\nclass AdaptiveBirdSwarmLevy:\n    def __init__(self, budget=10000, dim=10, swarm_size=20, inertia_weight=0.7, cognitive_coeff=1.4, social_coeff=1.4, neighborhood_size=3, levy_exponent=1.5, shrink_factor=0.95, initial_levy_scale=0.1, local_search_iterations=5, levy_probability=0.2, velocity_damping=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = swarm_size\n        self.inertia_weight = inertia_weight\n        self.cognitive_coeff = cognitive_coeff\n        self.social_coeff = social_coeff\n        self.neighborhood_size = neighborhood_size\n        self.levy_exponent = levy_exponent\n        self.shrink_factor = shrink_factor\n        self.levy_scale = initial_levy_scale\n        self.local_search_iterations = local_search_iterations\n        self.levy_probability = levy_probability  # Probability of applying Levy flight mutation\n        self.velocity_damping = velocity_damping  # Damping factor for velocities\n        self.success_rate = 0.5  # Initialize success rate\n\n    def levy_flight(self, size):\n        \"\"\"Generate Levy distribution samples.\"\"\"\n        u = np.random.randn(size)\n        v = np.random.randn(size)\n        sigma = (np.math.gamma(1 + self.levy_exponent) * np.sin(np.pi * self.levy_exponent / 2) / (np.math.gamma((1 + self.levy_exponent) / 2) * self.levy_exponent * (2 ** ((self.levy_exponent - 1) / 2)))) ** (1 / self.levy_exponent)\n        step = sigma * u / (np.abs(v) ** (1 / self.levy_exponent))\n        return step\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        \n        # Initialize swarm positions and velocities\n        swarm_positions = np.random.uniform(lb, ub, size=(self.swarm_size, self.dim))\n        swarm_velocities = np.random.uniform(-abs(ub-lb)/10, abs(ub-lb)/10, size=(self.swarm_size, self.dim))  #Initialize with small velocities\n        swarm_fitness = np.array([func(x) for x in swarm_positions])\n        self.budget -= self.swarm_size #Update the remaining budget\n        \n        personal_best_positions = swarm_positions.copy()\n        personal_best_fitness = swarm_fitness.copy()\n        \n        global_best_index = np.argmin(personal_best_fitness)\n        global_best_position = personal_best_positions[global_best_index].copy()\n        global_best_fitness = personal_best_fitness[global_best_index]\n        \n        search_range = ub - lb\n        center = global_best_position\n\n        successes = 0\n        iterations = 0\n        \n        # Adaptive parameter adjustments\n        inertia_weight = self.inertia_weight\n        cognitive_coeff = self.cognitive_coeff\n        social_coeff = self.social_coeff\n\n\n        while self.budget > 0:\n            for i in range(self.swarm_size):\n                # Bird Swarm Optimization part\n                inertia = inertia_weight * swarm_velocities[i]\n                cognitive = cognitive_coeff * np.random.rand(self.dim) * (personal_best_positions[i] - swarm_positions[i])\n                \n                # Social component: find best neighbor\n                neighbors_indices = np.random.choice(self.swarm_size, size=self.neighborhood_size, replace=False)  # Select random neighbors\n                neighbors_fitness = personal_best_fitness[neighbors_indices]\n                best_neighbor_index = neighbors_indices[np.argmin(neighbors_fitness)]\n                social = social_coeff * np.random.rand(self.dim) * (personal_best_positions[best_neighbor_index] - swarm_positions[i])\n                \n                swarm_velocities[i] = self.velocity_damping * (inertia + cognitive + social)\n                \n                # Update position\n                new_position = swarm_positions[i] + swarm_velocities[i]\n                \n                # Boundary handling (clip the solution, if out of bounds)\n                new_position = np.clip(new_position, lb, ub)\n\n                # Levy flight mutation with probability\n                if np.random.rand() < self.levy_probability:\n                    levy_steps = self.levy_flight(self.dim)\n                    mutation = center + search_range * levy_steps * self.levy_scale  # Scale Levy distribution\n                    mutation = np.clip(mutation, lb, ub)  # Clip values to stay within bounds\n                    new_position = mutation\n                \n                new_fitness = func(new_position)\n                self.budget -= 1\n                iterations += 1\n                \n                if new_fitness < personal_best_fitness[i]:\n                    personal_best_fitness[i] = new_fitness\n                    personal_best_positions[i] = new_position.copy()\n                    successes += 1\n                    \n                    if new_fitness < global_best_fitness:\n                        global_best_fitness = new_fitness\n                        global_best_position = new_position.copy()\n                        center = global_best_position\n                \n                swarm_positions[i] = new_position\n\n            # Shrink the search range around the best individual\n            search_range *= self.shrink_factor\n            lb = np.clip(center - search_range/2, func.bounds.lb, func.bounds.ub)\n            ub = np.clip(center + search_range/2, func.bounds.lb, func.bounds.ub)\n\n            # Adjust Levy scale based on success rate\n            if iterations > 0:\n                self.success_rate = successes / iterations\n                if self.success_rate > 0.2:\n                    self.levy_scale *= 1.1  # Increase scale if successful\n                    inertia_weight = min(inertia_weight + 0.05, 0.9) #Adapt inertia weight\n                    cognitive_coeff = max(cognitive_coeff - 0.05, 1.0) #Adapt cognitive coeff\n                    social_coeff = max(social_coeff - 0.05, 1.0) #Adapt social coeff\n                else:\n                    self.levy_scale *= 0.9  # Decrease scale if not successful\n                    inertia_weight = max(inertia_weight - 0.05, 0.4) #Adapt inertia weight\n                    cognitive_coeff = min(cognitive_coeff + 0.05, 2.0) #Adapt cognitive coeff\n                    social_coeff = min(social_coeff + 0.05, 2.0) #Adapt social coeff\n                successes = 0\n                iterations = 0\n\n            self.levy_scale = np.clip(self.levy_scale, 0.01, 1.0) # Limit Levy scale\n            \n        \n        # Local Search around best solution\n        for _ in range(min(self.local_search_iterations, self.budget)):\n             if self.budget <= 0:\n                 break\n\n             perturbation = np.random.uniform(-search_range/100, search_range/100, self.dim)\n             local_x = np.clip(global_best_position + perturbation, lb, ub)\n             local_f = func(local_x)\n             self.budget -=1\n\n             if local_f < global_best_fitness:\n                  global_best_fitness = local_f\n                  global_best_position = local_x\n        \n        return global_best_fitness, global_best_position\n```", "operator": "mutation", "theta_sampled": 0.013097295900566843, "reward": 0.0, "generation": 10}, "task_prompt": ""}
{"id": "7c78c926-5b8e-462d-a0f7-0e6314e48163", "fitness": 0.0, "name": "HybridCMAESLBFGS", "description": "This algorithm combines the strengths of both gradient-free optimization (CMA-ES) for global exploration and local gradient-based optimization (L-BFGS-B) for exploitation, adaptively switching between the two based on the function evaluation budget.", "code": "import numpy as np\nfrom scipy.optimize import minimize\nimport cma\n\nclass HybridCMAESLBFGS:\n    def __init__(self, budget=10000, dim=10, cmaes_sigma=0.5, lbfgs_max_eval=50):\n        self.budget = budget\n        self.dim = dim\n        self.cmaes_sigma = cmaes_sigma\n        self.lbfgs_max_eval = lbfgs_max_eval\n        self.cmaes_evals = 0\n        self.lbfgs_evals = 0\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        \n        # Initial CMA-ES parameters\n        x0 = np.random.uniform(func.bounds.lb, func.bounds.ub, self.dim)\n        es = cma.CMAEvolutionStrategy(x0, self.cmaes_sigma,\n                                      {'bounds': [func.bounds.lb, func.bounds.ub],\n                                       'verbose': -9})\n\n        # Adaptive Switching Strategy\n        cmaes_fraction = 0.7  # Fraction of budget for CMA-ES\n\n        while self.cmaes_evals + self.lbfgs_evals < self.budget:\n            # CMA-ES Phase\n            if (self.cmaes_evals / self.budget) < cmaes_fraction and self.cmaes_evals < self.budget:\n                solutions = es.ask()\n                for x in solutions:\n                    f = func(x)\n                    self.cmaes_evals += 1\n                    es.tell(x, f)\n\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = x\n                es.logger.add()  # write data to disc to be plotted\n                es.disp()\n\n            # L-BFGS-B Phase\n            else:\n                if self.x_opt is not None: # Ensure there is a starting point.\n                    res = minimize(func, self.x_opt, method='L-BFGS-B', \n                                   bounds=[(func.bounds.lb, func.bounds.ub)] * self.dim,\n                                   options={'maxfun': self.lbfgs_max_eval})\n                    self.lbfgs_evals += res.nfev\n                    \n                    if res.fun < self.f_opt:\n                        self.f_opt = res.fun\n                        self.x_opt = res.x\n                \n                else: # If CMA-ES did not improve, use a random x\n                    x = np.random.uniform(func.bounds.lb, func.bounds.ub, self.dim)\n                    res = minimize(func, x, method='L-BFGS-B', \n                                   bounds=[(func.bounds.lb, func.bounds.ub)] * self.dim,\n                                   options={'maxfun': self.lbfgs_max_eval})\n                    self.lbfgs_evals += res.nfev\n                    if res.fun < self.f_opt:\n                        self.f_opt = res.fun\n                        self.x_opt = res.x\n\n            if self.cmaes_evals + self.lbfgs_evals >= self.budget:\n                break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 10, "feedback": "", "error": "Evaluation error: No module named 'cma'", "parent_ids": [], "operator": "random_new", "metadata": {"llm_response": "# Description: This algorithm combines the strengths of both gradient-free optimization (CMA-ES) for global exploration and local gradient-based optimization (L-BFGS-B) for exploitation, adaptively switching between the two based on the function evaluation budget.\n\n# Code:\n```python\nimport numpy as np\nfrom scipy.optimize import minimize\nimport cma\n\nclass HybridCMAESLBFGS:\n    def __init__(self, budget=10000, dim=10, cmaes_sigma=0.5, lbfgs_max_eval=50):\n        self.budget = budget\n        self.dim = dim\n        self.cmaes_sigma = cmaes_sigma\n        self.lbfgs_max_eval = lbfgs_max_eval\n        self.cmaes_evals = 0\n        self.lbfgs_evals = 0\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        \n        # Initial CMA-ES parameters\n        x0 = np.random.uniform(func.bounds.lb, func.bounds.ub, self.dim)\n        es = cma.CMAEvolutionStrategy(x0, self.cmaes_sigma,\n                                      {'bounds': [func.bounds.lb, func.bounds.ub],\n                                       'verbose': -9})\n\n        # Adaptive Switching Strategy\n        cmaes_fraction = 0.7  # Fraction of budget for CMA-ES\n\n        while self.cmaes_evals + self.lbfgs_evals < self.budget:\n            # CMA-ES Phase\n            if (self.cmaes_evals / self.budget) < cmaes_fraction and self.cmaes_evals < self.budget:\n                solutions = es.ask()\n                for x in solutions:\n                    f = func(x)\n                    self.cmaes_evals += 1\n                    es.tell(x, f)\n\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = x\n                es.logger.add()  # write data to disc to be plotted\n                es.disp()\n\n            # L-BFGS-B Phase\n            else:\n                if self.x_opt is not None: # Ensure there is a starting point.\n                    res = minimize(func, self.x_opt, method='L-BFGS-B', \n                                   bounds=[(func.bounds.lb, func.bounds.ub)] * self.dim,\n                                   options={'maxfun': self.lbfgs_max_eval})\n                    self.lbfgs_evals += res.nfev\n                    \n                    if res.fun < self.f_opt:\n                        self.f_opt = res.fun\n                        self.x_opt = res.x\n                \n                else: # If CMA-ES did not improve, use a random x\n                    x = np.random.uniform(func.bounds.lb, func.bounds.ub, self.dim)\n                    res = minimize(func, x, method='L-BFGS-B', \n                                   bounds=[(func.bounds.lb, func.bounds.ub)] * self.dim,\n                                   options={'maxfun': self.lbfgs_max_eval})\n                    self.lbfgs_evals += res.nfev\n                    if res.fun < self.f_opt:\n                        self.f_opt = res.fun\n                        self.x_opt = res.x\n\n            if self.cmaes_evals + self.lbfgs_evals >= self.budget:\n                break\n\n        return self.f_opt, self.x_opt\n```", "operator": "random_new", "theta_sampled": 0.013776366760816469, "reward": 0.0, "generation": 10}, "task_prompt": ""}
{"id": "5b77f9fb-407a-4344-aa0a-a95d9f7e8690", "fitness": 0.733614602664464, "name": "AdaptiveDEBirdSwarm", "description": "A hybrid algorithm combining adaptive Differential Evolution with Bird Swarm Optimization's neighborhood search and Cauchy mutation for enhanced exploration and exploitation.", "code": "import numpy as np\n\nclass AdaptiveDEBirdSwarm:\n    def __init__(self, budget=10000, dim=10, pop_size=40, F=0.5, CR=0.7, neighborhood_size=5, cauchy_probability=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F  # Mutation factor\n        self.CR = CR  # Crossover rate\n        self.lb = -5.0\n        self.ub = 5.0\n        self.stagnation_counter = 0\n        self.stagnation_threshold = budget // 10\n        self.best_fitness_history = []\n        self.neighborhood_size = neighborhood_size\n        self.cauchy_probability = cauchy_probability\n        self.cauchy_scale = 0.1\n        self.shrink_factor = 0.95\n\n    def __call__(self, func):\n        self.population = np.random.uniform(self.lb, self.ub, (self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n        self.eval_count = self.pop_size\n\n        while self.eval_count < self.budget:\n            for i in range(self.pop_size):\n                # Adaptive F and CR\n                adaptive_F = self.F * (1 + 0.1 * np.random.randn())\n                adaptive_CR = self.CR * (1 + 0.1 * np.random.randn())\n                adaptive_F = np.clip(adaptive_F, 0.1, 1.0)\n                adaptive_CR = np.clip(adaptive_CR, 0.1, 1.0)\n\n                # Mutation (DE part)\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = np.random.choice(idxs, 3, replace=False)\n                mutant = self.population[a] + adaptive_F * (self.population[b] - self.population[c])\n                mutant = np.clip(mutant, self.lb, self.ub)\n\n                # Crossover\n                trial = np.copy(self.population[i])\n                mask = np.random.rand(self.dim) < adaptive_CR\n                trial[mask] = mutant[mask]\n                trial = np.clip(trial, self.lb, self.ub)\n\n                # Bird Swarm Inspired Local Search and Cauchy Mutation\n                if np.random.rand() < 0.5: # Apply local search with 50% probability\n                    # Neighborhood Search\n                    neighbors_indices = np.random.choice(self.pop_size, size=self.neighborhood_size, replace=False)\n                    neighbors = self.population[neighbors_indices]\n                    neighbors_fitness = self.fitness[neighbors_indices]\n                    best_neighbor_index = neighbors_indices[np.argmin(neighbors_fitness)]\n                    best_neighbor = self.population[best_neighbor_index]\n\n                    # Move towards the best neighbor\n                    trial = trial + 0.1 * (best_neighbor - trial)\n                    trial = np.clip(trial, self.lb, self.ub)\n\n                    # Cauchy Mutation\n                    if np.random.rand() < self.cauchy_probability:\n                        mutation = trial + (self.ub - self.lb) * np.random.standard_cauchy(size=self.dim) * self.cauchy_scale\n                        mutation = np.clip(mutation, self.lb, self.ub)\n                        trial = mutation\n\n                # Selection\n                f_trial = func(trial)\n                self.eval_count += 1\n\n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n            # Stagnation Check and Restart\n            self.best_fitness_history.append(self.f_opt)\n            if len(self.best_fitness_history) > self.stagnation_threshold:\n                if self.best_fitness_history[-1] >= self.best_fitness_history[-self.stagnation_threshold]:\n                    self.stagnation_counter += 1\n                else:\n                    self.stagnation_counter = 0\n\n            if self.stagnation_counter >= self.stagnation_threshold:\n                #Restart the population\n                self.population = np.random.uniform(self.lb, self.ub, (self.pop_size, self.dim))\n                self.fitness = np.array([func(x) for x in self.population])\n                self.f_opt = np.min(self.fitness)\n                self.x_opt = self.population[np.argmin(self.fitness)]\n                self.eval_count += self.pop_size - (self.budget-self.eval_count) if self.eval_count + self.pop_size > self.budget else self.pop_size\n                self.stagnation_counter = 0\n                self.best_fitness_history = [self.f_opt]\n\n            if self.eval_count >= self.budget:\n                break\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 10, "feedback": "The algorithm AdaptiveDEBirdSwarm scored 0.734 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["7f39ad48-10bd-4693-bf48-a08aea65c982", "5ed072b3-d57e-4c1e-8fc4-1c2b4f7cf697"], "operator": "crossover", "metadata": {"llm_response": "# Description: A hybrid algorithm combining adaptive Differential Evolution with Bird Swarm Optimization's neighborhood search and Cauchy mutation for enhanced exploration and exploitation.\n# Code: \n```python\nimport numpy as np\n\nclass AdaptiveDEBirdSwarm:\n    def __init__(self, budget=10000, dim=10, pop_size=40, F=0.5, CR=0.7, neighborhood_size=5, cauchy_probability=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F  # Mutation factor\n        self.CR = CR  # Crossover rate\n        self.lb = -5.0\n        self.ub = 5.0\n        self.stagnation_counter = 0\n        self.stagnation_threshold = budget // 10\n        self.best_fitness_history = []\n        self.neighborhood_size = neighborhood_size\n        self.cauchy_probability = cauchy_probability\n        self.cauchy_scale = 0.1\n        self.shrink_factor = 0.95\n\n    def __call__(self, func):\n        self.population = np.random.uniform(self.lb, self.ub, (self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n        self.eval_count = self.pop_size\n\n        while self.eval_count < self.budget:\n            for i in range(self.pop_size):\n                # Adaptive F and CR\n                adaptive_F = self.F * (1 + 0.1 * np.random.randn())\n                adaptive_CR = self.CR * (1 + 0.1 * np.random.randn())\n                adaptive_F = np.clip(adaptive_F, 0.1, 1.0)\n                adaptive_CR = np.clip(adaptive_CR, 0.1, 1.0)\n\n                # Mutation (DE part)\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = np.random.choice(idxs, 3, replace=False)\n                mutant = self.population[a] + adaptive_F * (self.population[b] - self.population[c])\n                mutant = np.clip(mutant, self.lb, self.ub)\n\n                # Crossover\n                trial = np.copy(self.population[i])\n                mask = np.random.rand(self.dim) < adaptive_CR\n                trial[mask] = mutant[mask]\n                trial = np.clip(trial, self.lb, self.ub)\n\n                # Bird Swarm Inspired Local Search and Cauchy Mutation\n                if np.random.rand() < 0.5: # Apply local search with 50% probability\n                    # Neighborhood Search\n                    neighbors_indices = np.random.choice(self.pop_size, size=self.neighborhood_size, replace=False)\n                    neighbors = self.population[neighbors_indices]\n                    neighbors_fitness = self.fitness[neighbors_indices]\n                    best_neighbor_index = neighbors_indices[np.argmin(neighbors_fitness)]\n                    best_neighbor = self.population[best_neighbor_index]\n\n                    # Move towards the best neighbor\n                    trial = trial + 0.1 * (best_neighbor - trial)\n                    trial = np.clip(trial, self.lb, self.ub)\n\n                    # Cauchy Mutation\n                    if np.random.rand() < self.cauchy_probability:\n                        mutation = trial + (self.ub - self.lb) * np.random.standard_cauchy(size=self.dim) * self.cauchy_scale\n                        mutation = np.clip(mutation, self.lb, self.ub)\n                        trial = mutation\n\n                # Selection\n                f_trial = func(trial)\n                self.eval_count += 1\n\n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n            # Stagnation Check and Restart\n            self.best_fitness_history.append(self.f_opt)\n            if len(self.best_fitness_history) > self.stagnation_threshold:\n                if self.best_fitness_history[-1] >= self.best_fitness_history[-self.stagnation_threshold]:\n                    self.stagnation_counter += 1\n                else:\n                    self.stagnation_counter = 0\n\n            if self.stagnation_counter >= self.stagnation_threshold:\n                #Restart the population\n                self.population = np.random.uniform(self.lb, self.ub, (self.pop_size, self.dim))\n                self.fitness = np.array([func(x) for x in self.population])\n                self.f_opt = np.min(self.fitness)\n                self.x_opt = self.population[np.argmin(self.fitness)]\n                self.eval_count += self.pop_size - (self.budget-self.eval_count) if self.eval_count + self.pop_size > self.budget else self.pop_size\n                self.stagnation_counter = 0\n                self.best_fitness_history = [self.f_opt]\n\n            if self.eval_count >= self.budget:\n                break\n        return self.f_opt, self.x_opt\n```", "aucs": [0.2822259974612914, 0.6688890995207211, 0.7475605394162931, 0.8929455740115426, 0.8035827376425364, 0.8358699818258939, 0.740242633016669, 0.7389479263641497, 0.8010779517339284, 0.7766985286372584, 0.8463038304718614, 0.9891587422953577, 0.42561126795995174, 0.7774972058941574, 0.924635952046161, 0.8250373262940898, 0.6407770091478557, 0.8654570013869505, 0.5657686637463073, 0.5240040844163039], "operator": "crossover", "theta_sampled": 0.0033115248246760993, "reward": 0.026381888297424494, "generation": 10}, "task_prompt": ""}
{"id": "59841b17-f214-4de6-b2eb-4a6600ae9994", "fitness": 0.6409287280726581, "name": "AdaptiveHybridDE_BSO", "description": "A hybrid algorithm combining Adaptive Differential Evolution's mutation and crossover with Bird Swarm Optimization's social learning and Cauchy mutation, incorporating adaptive parameter control and a shrinking search range.", "code": "import numpy as np\n\nclass AdaptiveHybridDE_BSO:\n    def __init__(self, budget=10000, dim=10, pop_size=40, F=0.5, CR=0.9, inertia_weight=0.7, cognitive_coeff=1.4, social_coeff=1.4, neighborhood_size=3, shrink_factor=0.95, initial_cauchy_scale=0.1, cauchy_probability=0.2):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F  # Mutation factor\n        self.CR = CR  # Crossover rate\n        self.inertia_weight = inertia_weight\n        self.cognitive_coeff = cognitive_coeff\n        self.social_coeff = social_coeff\n        self.neighborhood_size = neighborhood_size\n        self.shrink_factor = shrink_factor\n        self.cauchy_scale = initial_cauchy_scale\n        self.cauchy_probability = cauchy_probability\n        self.lb = -5.0\n        self.ub = 5.0\n        self.stagnation_counter = 0\n        self.stagnation_threshold = budget // 10\n        self.best_fitness_history = []\n        self.success_rate = 0.5\n\n    def __call__(self, func):\n        self.population = np.random.uniform(self.lb, self.ub, (self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n        self.eval_count = self.pop_size\n        self.personal_best_positions = self.population.copy()\n        self.personal_best_fitness = self.fitness.copy()\n        self.global_best_index = np.argmin(self.personal_best_fitness)\n        self.global_best_position = self.personal_best_positions[self.global_best_index].copy()\n        self.global_best_fitness = self.personal_best_fitness[self.global_best_index]\n        self.search_range = self.ub - self.lb\n        self.center = self.global_best_position\n        successes = 0\n        iterations = 0\n\n        while self.eval_count < self.budget:\n            for i in range(self.pop_size):\n                # Adaptive F and CR\n                adaptive_F = self.F * (1 + 0.1 * np.random.randn())\n                adaptive_CR = self.CR * (1 + 0.1 * np.random.randn())\n                adaptive_F = np.clip(adaptive_F, 0.1, 1.0)\n                adaptive_CR = np.clip(adaptive_CR, 0.1, 1.0)\n\n                # Mutation (DE part)\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = np.random.choice(idxs, 3, replace=False)\n                mutant = self.population[a] + adaptive_F * (self.population[b] - self.population[c])\n                mutant = np.clip(mutant, self.lb, self.ub)\n\n                # Crossover (DE part)\n                trial = np.copy(self.population[i])\n                mask = np.random.rand(self.dim) < adaptive_CR\n                trial[mask] = mutant[mask]\n\n                # Bird Swarm Optimization - Social component\n                neighbors_indices = np.random.choice(self.pop_size, size=self.neighborhood_size, replace=False)\n                neighbors_fitness = self.personal_best_fitness[neighbors_indices]\n                best_neighbor_index = neighbors_indices[np.argmin(neighbors_fitness)]\n                social = self.social_coeff * np.random.rand(self.dim) * (self.personal_best_positions[best_neighbor_index] - self.population[i])\n                trial = trial + social\n\n                 # Cauchy mutation with probability\n                if np.random.rand() < self.cauchy_probability:\n                    mutation = self.center + self.search_range * np.random.standard_cauchy(size=self.dim) * self.cauchy_scale  # Scale Cauchy distribution\n                    mutation = np.clip(mutation, self.lb, self.ub)  # Clip values to stay within bounds\n                    trial = mutation\n\n\n                trial = np.clip(trial, self.lb, self.ub)\n\n                # Selection\n                f_trial = func(trial)\n                self.eval_count += 1\n                iterations += 1\n\n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n                    if f_trial < self.personal_best_fitness[i]:\n                        self.personal_best_fitness[i] = f_trial\n                        self.personal_best_positions[i] = trial.copy()\n                        successes += 1\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                        self.global_best_fitness = f_trial\n                        self.global_best_position = trial.copy()\n                        self.center = self.global_best_position\n\n            # Shrink the search range around the best individual\n            self.search_range *= self.shrink_factor\n            self.lb = np.clip(self.center - self.search_range/2, -5.0, 5.0)\n            self.ub = np.clip(self.center + self.search_range/2, -5.0, 5.0)\n\n             # Adjust Cauchy scale based on success rate\n            if iterations > 0:\n                self.success_rate = successes / iterations\n                if self.success_rate > 0.2:\n                    self.cauchy_scale *= 1.1  # Increase scale if successful\n                else:\n                    self.cauchy_scale *= 0.9  # Decrease scale if not successful\n                successes = 0\n                iterations = 0\n\n            self.cauchy_scale = np.clip(self.cauchy_scale, 0.01, 1.0) # Limit Cauchy scale\n            \n            #Stagnation Check and Restart\n            self.best_fitness_history.append(self.f_opt)\n            if len(self.best_fitness_history) > self.stagnation_threshold:\n                if self.best_fitness_history[-1] >= self.best_fitness_history[-self.stagnation_threshold]:\n                    self.stagnation_counter += 1\n                else:\n                    self.stagnation_counter = 0\n\n            if self.stagnation_counter >= self.stagnation_threshold:\n                #Restart the population\n                self.population = np.random.uniform(self.lb, self.ub, (self.pop_size, self.dim))\n                self.fitness = np.array([func(x) for x in self.population])\n                self.f_opt = np.min(self.fitness)\n                self.x_opt = self.population[np.argmin(self.fitness)]\n                self.personal_best_positions = self.population.copy()\n                self.personal_best_fitness = self.fitness.copy()\n                self.global_best_index = np.argmin(self.personal_best_fitness)\n                self.global_best_position = self.personal_best_positions[self.global_best_index].copy()\n                self.global_best_fitness = self.personal_best_fitness[self.global_best_index]\n                self.eval_count += self.pop_size - (self.budget-self.eval_count) if self.eval_count + self.pop_size > self.budget else self.pop_size\n                self.stagnation_counter = 0\n                self.best_fitness_history = [self.f_opt]\n\n            if self.eval_count >= self.budget:\n                break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 10, "feedback": "The algorithm AdaptiveHybridDE_BSO scored 0.641 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["7f39ad48-10bd-4693-bf48-a08aea65c982", "5ed072b3-d57e-4c1e-8fc4-1c2b4f7cf697"], "operator": "crossover", "metadata": {"llm_response": "# Description: A hybrid algorithm combining Adaptive Differential Evolution's mutation and crossover with Bird Swarm Optimization's social learning and Cauchy mutation, incorporating adaptive parameter control and a shrinking search range.\n# Code: \n```python\nimport numpy as np\n\nclass AdaptiveHybridDE_BSO:\n    def __init__(self, budget=10000, dim=10, pop_size=40, F=0.5, CR=0.9, inertia_weight=0.7, cognitive_coeff=1.4, social_coeff=1.4, neighborhood_size=3, shrink_factor=0.95, initial_cauchy_scale=0.1, cauchy_probability=0.2):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F  # Mutation factor\n        self.CR = CR  # Crossover rate\n        self.inertia_weight = inertia_weight\n        self.cognitive_coeff = cognitive_coeff\n        self.social_coeff = social_coeff\n        self.neighborhood_size = neighborhood_size\n        self.shrink_factor = shrink_factor\n        self.cauchy_scale = initial_cauchy_scale\n        self.cauchy_probability = cauchy_probability\n        self.lb = -5.0\n        self.ub = 5.0\n        self.stagnation_counter = 0\n        self.stagnation_threshold = budget // 10\n        self.best_fitness_history = []\n        self.success_rate = 0.5\n\n    def __call__(self, func):\n        self.population = np.random.uniform(self.lb, self.ub, (self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n        self.eval_count = self.pop_size\n        self.personal_best_positions = self.population.copy()\n        self.personal_best_fitness = self.fitness.copy()\n        self.global_best_index = np.argmin(self.personal_best_fitness)\n        self.global_best_position = self.personal_best_positions[self.global_best_index].copy()\n        self.global_best_fitness = self.personal_best_fitness[self.global_best_index]\n        self.search_range = self.ub - self.lb\n        self.center = self.global_best_position\n        successes = 0\n        iterations = 0\n\n        while self.eval_count < self.budget:\n            for i in range(self.pop_size):\n                # Adaptive F and CR\n                adaptive_F = self.F * (1 + 0.1 * np.random.randn())\n                adaptive_CR = self.CR * (1 + 0.1 * np.random.randn())\n                adaptive_F = np.clip(adaptive_F, 0.1, 1.0)\n                adaptive_CR = np.clip(adaptive_CR, 0.1, 1.0)\n\n                # Mutation (DE part)\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = np.random.choice(idxs, 3, replace=False)\n                mutant = self.population[a] + adaptive_F * (self.population[b] - self.population[c])\n                mutant = np.clip(mutant, self.lb, self.ub)\n\n                # Crossover (DE part)\n                trial = np.copy(self.population[i])\n                mask = np.random.rand(self.dim) < adaptive_CR\n                trial[mask] = mutant[mask]\n\n                # Bird Swarm Optimization - Social component\n                neighbors_indices = np.random.choice(self.pop_size, size=self.neighborhood_size, replace=False)\n                neighbors_fitness = self.personal_best_fitness[neighbors_indices]\n                best_neighbor_index = neighbors_indices[np.argmin(neighbors_fitness)]\n                social = self.social_coeff * np.random.rand(self.dim) * (self.personal_best_positions[best_neighbor_index] - self.population[i])\n                trial = trial + social\n\n                 # Cauchy mutation with probability\n                if np.random.rand() < self.cauchy_probability:\n                    mutation = self.center + self.search_range * np.random.standard_cauchy(size=self.dim) * self.cauchy_scale  # Scale Cauchy distribution\n                    mutation = np.clip(mutation, self.lb, self.ub)  # Clip values to stay within bounds\n                    trial = mutation\n\n\n                trial = np.clip(trial, self.lb, self.ub)\n\n                # Selection\n                f_trial = func(trial)\n                self.eval_count += 1\n                iterations += 1\n\n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n                    if f_trial < self.personal_best_fitness[i]:\n                        self.personal_best_fitness[i] = f_trial\n                        self.personal_best_positions[i] = trial.copy()\n                        successes += 1\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                        self.global_best_fitness = f_trial\n                        self.global_best_position = trial.copy()\n                        self.center = self.global_best_position\n\n            # Shrink the search range around the best individual\n            self.search_range *= self.shrink_factor\n            self.lb = np.clip(self.center - self.search_range/2, -5.0, 5.0)\n            self.ub = np.clip(self.center + self.search_range/2, -5.0, 5.0)\n\n             # Adjust Cauchy scale based on success rate\n            if iterations > 0:\n                self.success_rate = successes / iterations\n                if self.success_rate > 0.2:\n                    self.cauchy_scale *= 1.1  # Increase scale if successful\n                else:\n                    self.cauchy_scale *= 0.9  # Decrease scale if not successful\n                successes = 0\n                iterations = 0\n\n            self.cauchy_scale = np.clip(self.cauchy_scale, 0.01, 1.0) # Limit Cauchy scale\n            \n            #Stagnation Check and Restart\n            self.best_fitness_history.append(self.f_opt)\n            if len(self.best_fitness_history) > self.stagnation_threshold:\n                if self.best_fitness_history[-1] >= self.best_fitness_history[-self.stagnation_threshold]:\n                    self.stagnation_counter += 1\n                else:\n                    self.stagnation_counter = 0\n\n            if self.stagnation_counter >= self.stagnation_threshold:\n                #Restart the population\n                self.population = np.random.uniform(self.lb, self.ub, (self.pop_size, self.dim))\n                self.fitness = np.array([func(x) for x in self.population])\n                self.f_opt = np.min(self.fitness)\n                self.x_opt = self.population[np.argmin(self.fitness)]\n                self.personal_best_positions = self.population.copy()\n                self.personal_best_fitness = self.fitness.copy()\n                self.global_best_index = np.argmin(self.personal_best_fitness)\n                self.global_best_position = self.personal_best_positions[self.global_best_index].copy()\n                self.global_best_fitness = self.personal_best_fitness[self.global_best_index]\n                self.eval_count += self.pop_size - (self.budget-self.eval_count) if self.eval_count + self.pop_size > self.budget else self.pop_size\n                self.stagnation_counter = 0\n                self.best_fitness_history = [self.f_opt]\n\n            if self.eval_count >= self.budget:\n                break\n\n        return self.f_opt, self.x_opt\n```", "aucs": [0.36322285221654804, 0.4288746180669143, 0.6394019369787971, 0.862146918333406, 0.6792637113002572, 0.6992893820241959, 0.3574982740435012, 0.5718013360670775, 0.635550816021313, 0.6820085133401965, 0.843681467592561, 0.996535592718107, 0.2849279588311818, 0.6986969448388531, 0.9279994488460488, 0.7035618574851232, 0.5981246633453143, 0.7967604626314198, 0.5553108008719567, 0.4939170059003867], "operator": "crossover", "theta_sampled": 0.018275676659238686, "reward": 0.0, "generation": 10}, "task_prompt": ""}
{"id": "eaad2602-73f4-4c0b-8ada-ba4645f6baf6", "fitness": 0.0, "name": "AdaptiveStepSizeOptimizer", "description": "A population-based algorithm that adapts step sizes using a combination of historical success rates and covariance matrix adaptation inspired by CMA-ES, while also employing a restart mechanism to escape local optima.", "code": "import numpy as np\n\nclass AdaptiveStepSizeOptimizer:\n    def __init__(self, budget=10000, dim=10, pop_size=None, initial_step_size=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size if pop_size is not None else 4 + int(3 * np.log(dim))\n        self.initial_step_size = initial_step_size\n        self.mu = self.pop_size // 2  # Number of parents\n        self.c_sigma = (self.mu + 2) / (self.dim + self.mu + 5)  # Learning rate for step-size control\n        self.d_sigma = 1 + 2 * max(0, np.sqrt((self.mu - 1) / (self.dim + 1)) - 1) + self.c_sigma\n        self.c_cov = 2 / ((self.dim + np.sqrt(2))**2 + self.mu) # Learning rate for covariance matrix\n        self.c_cov_mu = self.c_cov\n        self.c_cov_min = 0.1\n        \n        self.success_rate = 0.5  # Initialize success rate\n        self.success_history = []\n        self.history_length = 10\n        self.restart_trigger = 10 # Restart trigger \n        self.restart_counter = 0 \n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        \n        # Initialization\n        mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)  # Initialize mean within bounds\n        step_size = self.initial_step_size\n        C = np.eye(self.dim)  # Covariance matrix\n        B = np.eye(self.dim) # Eigenvectors\n        D = np.ones(self.dim) # Eigenvalues\n\n        eval_count = 0\n\n        while eval_count < self.budget:\n            # Generate population\n            z = np.random.normal(0, 1, size=(self.pop_size, self.dim))\n            x = mean + step_size * (B @ (D * z.T)).T  # Apply transformation\n\n            # Clip individuals to bounds\n            x = np.clip(x, func.bounds.lb, func.bounds.ub)\n\n            # Evaluate population\n            f = np.array([func(xi) for xi in x])\n            eval_count += self.pop_size\n\n            # Sort population\n            idx = np.argsort(f)\n            x = x[idx]\n            f = f[idx]\n\n            # Update optimal solution\n            if f[0] < self.f_opt:\n                self.f_opt = f[0]\n                self.x_opt = x[0]\n            \n            # Update mean\n            mean_old = mean.copy()\n            mean = np.mean(x[:self.mu], axis=0)\n            \n            # Cumulation for step-size control\n            C_sigma = np.zeros(self.dim)\n            C_sigma = (1-self.c_sigma) * C_sigma + np.sqrt(self.c_sigma * (2-self.c_sigma)) * (B @ z[idx[0],:].T)\n\n\n            # Update step size\n            norm_C_sigma = np.linalg.norm(C_sigma)\n            step_size *= np.exp(self.c_sigma / self.d_sigma * (norm_C_sigma/np.sqrt(self.dim) - 1))\n            step_size = max(step_size, 1e-10)\n            \n            # Update covariance matrix\n            y = (x[:self.mu] - mean_old)\n            C_diff = y.T @ y\n            C = (1-self.c_cov)*C + self.c_cov/self.mu * C_diff\n\n            try:\n                D, B = np.linalg.eigh(C)\n                D = np.sqrt(np.maximum(D, 1e-10)) # Prevent negative eigenvalues\n            except np.linalg.LinAlgError:\n                # Covariance matrix is not positive definite, restart\n                mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)  # Initialize mean within bounds\n                step_size = self.initial_step_size\n                C = np.eye(self.dim)  # Covariance matrix\n                B = np.eye(self.dim) # Eigenvectors\n                D = np.ones(self.dim) # Eigenvalues\n                \n            # Restart mechanism\n            self.restart_counter += 1\n\n            if self.restart_counter > self.restart_trigger:\n                mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                step_size = self.initial_step_size\n                C = np.eye(self.dim)\n                B = np.eye(self.dim)\n                D = np.ones(self.dim)\n                self.restart_counter = 0 # Reset the counter\n\n            if eval_count >= self.budget:\n                eval_count = self.budget\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 10, "feedback": "", "error": "Evaluation error: operands could not be broadcast together with shapes (2,) (2,6) ", "parent_ids": [], "operator": "random_new", "metadata": {"llm_response": "# Description: A population-based algorithm that adapts step sizes using a combination of historical success rates and covariance matrix adaptation inspired by CMA-ES, while also employing a restart mechanism to escape local optima.\n# Code: \n```python\nimport numpy as np\n\nclass AdaptiveStepSizeOptimizer:\n    def __init__(self, budget=10000, dim=10, pop_size=None, initial_step_size=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size if pop_size is not None else 4 + int(3 * np.log(dim))\n        self.initial_step_size = initial_step_size\n        self.mu = self.pop_size // 2  # Number of parents\n        self.c_sigma = (self.mu + 2) / (self.dim + self.mu + 5)  # Learning rate for step-size control\n        self.d_sigma = 1 + 2 * max(0, np.sqrt((self.mu - 1) / (self.dim + 1)) - 1) + self.c_sigma\n        self.c_cov = 2 / ((self.dim + np.sqrt(2))**2 + self.mu) # Learning rate for covariance matrix\n        self.c_cov_mu = self.c_cov\n        self.c_cov_min = 0.1\n        \n        self.success_rate = 0.5  # Initialize success rate\n        self.success_history = []\n        self.history_length = 10\n        self.restart_trigger = 10 # Restart trigger \n        self.restart_counter = 0 \n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        \n        # Initialization\n        mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)  # Initialize mean within bounds\n        step_size = self.initial_step_size\n        C = np.eye(self.dim)  # Covariance matrix\n        B = np.eye(self.dim) # Eigenvectors\n        D = np.ones(self.dim) # Eigenvalues\n\n        eval_count = 0\n\n        while eval_count < self.budget:\n            # Generate population\n            z = np.random.normal(0, 1, size=(self.pop_size, self.dim))\n            x = mean + step_size * (B @ (D * z.T)).T  # Apply transformation\n\n            # Clip individuals to bounds\n            x = np.clip(x, func.bounds.lb, func.bounds.ub)\n\n            # Evaluate population\n            f = np.array([func(xi) for xi in x])\n            eval_count += self.pop_size\n\n            # Sort population\n            idx = np.argsort(f)\n            x = x[idx]\n            f = f[idx]\n\n            # Update optimal solution\n            if f[0] < self.f_opt:\n                self.f_opt = f[0]\n                self.x_opt = x[0]\n            \n            # Update mean\n            mean_old = mean.copy()\n            mean = np.mean(x[:self.mu], axis=0)\n            \n            # Cumulation for step-size control\n            C_sigma = np.zeros(self.dim)\n            C_sigma = (1-self.c_sigma) * C_sigma + np.sqrt(self.c_sigma * (2-self.c_sigma)) * (B @ z[idx[0],:].T)\n\n\n            # Update step size\n            norm_C_sigma = np.linalg.norm(C_sigma)\n            step_size *= np.exp(self.c_sigma / self.d_sigma * (norm_C_sigma/np.sqrt(self.dim) - 1))\n            step_size = max(step_size, 1e-10)\n            \n            # Update covariance matrix\n            y = (x[:self.mu] - mean_old)\n            C_diff = y.T @ y\n            C = (1-self.c_cov)*C + self.c_cov/self.mu * C_diff\n\n            try:\n                D, B = np.linalg.eigh(C)\n                D = np.sqrt(np.maximum(D, 1e-10)) # Prevent negative eigenvalues\n            except np.linalg.LinAlgError:\n                # Covariance matrix is not positive definite, restart\n                mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)  # Initialize mean within bounds\n                step_size = self.initial_step_size\n                C = np.eye(self.dim)  # Covariance matrix\n                B = np.eye(self.dim) # Eigenvectors\n                D = np.ones(self.dim) # Eigenvalues\n                \n            # Restart mechanism\n            self.restart_counter += 1\n\n            if self.restart_counter > self.restart_trigger:\n                mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                step_size = self.initial_step_size\n                C = np.eye(self.dim)\n                B = np.eye(self.dim)\n                D = np.ones(self.dim)\n                self.restart_counter = 0 # Reset the counter\n\n            if eval_count >= self.budget:\n                eval_count = self.budget\n\n        return self.f_opt, self.x_opt\n```", "operator": "random_new", "theta_sampled": 0.0035762815244338273, "reward": 0.0, "generation": 10}, "task_prompt": ""}
{"id": "340276af-3b52-4353-87ce-92b4bf206c53", "fitness": 0.0, "name": "HybridCMAESPSO", "description": "A hybrid algorithm combining aspects of CMA-ES and a simplified PSO, using CMA-ES for individual learning and PSO for social learning among a population of CMA-ES individuals.", "code": "import numpy as np\nimport cma\n\nclass HybridCMAESPSO:\n    def __init__(self, budget=10000, dim=10, popsize=10):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = popsize\n        self.inertia = 0.7\n        self.cognitive_coeff = 1.5\n        self.social_coeff = 1.5\n        self.es_list = []\n\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        # Initialize population of CMA-ES individuals\n        x0 = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        sigma0 = 0.5  # Initial step size\n        for _ in range(self.popsize):\n          es = cma.purecma.CMAES(x0, sigma0,\n                                 {'bounds': [func.bounds.lb, func.bounds.ub],\n                                  'popsize': self.budget // self.popsize + 1}) # Ensure budget usage\n          self.es_list.append(es)\n\n        # Initialize personal bests\n        pbest_x = [np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim) for _ in range(self.popsize)]\n        pbest_f = [np.inf] * self.popsize\n        velocities = [np.zeros(self.dim) for _ in range(self.popsize)]\n\n        eval_count = 0\n        while eval_count < self.budget:\n            for i in range(self.popsize):\n                # CMA-ES step\n                solutions = self.es_list[i].ask()\n                fitness_values = []\n\n                for x in solutions:\n                    if eval_count < self.budget:\n                        f = func(x)\n                        eval_count += 1\n                        fitness_values.append(f)\n                        if f < self.f_opt:\n                            self.f_opt = f\n                            self.x_opt = x\n\n                        # Update personal best\n                        if f < pbest_f[i]:\n                            pbest_f[i] = f\n                            pbest_x[i] = x.copy()\n                    else:\n                        break\n\n                if not fitness_values:\n                    break  # Budget exhausted\n\n                self.es_list[i].tell(solutions[:len(fitness_values)], fitness_values)\n\n\n            # Simplified PSO update based on personal bests, using best individual in population as global best\n            best_index = np.argmin(pbest_f)\n            gbest_x = pbest_x[best_index]\n\n            for i in range(self.popsize):\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n\n                velocities[i] = (self.inertia * velocities[i] +\n                                 self.cognitive_coeff * r1 * (pbest_x[i] - self.es_list[i].mean) +\n                                 self.social_coeff * r2 * (gbest_x - self.es_list[i].mean))\n\n                # Update the mean of the CMA-ES distribution based on PSO velocity, but limit the change\n                max_velocity_norm = 0.1 * np.linalg.norm(func.bounds.ub - func.bounds.lb) # Limit speed to 10% of the search space\n                velocity_norm = np.linalg.norm(velocities[i])\n                if velocity_norm > max_velocity_norm:\n                  velocities[i] = velocities[i] * (max_velocity_norm / velocity_norm)  # Normalize if exceeds limit\n\n                self.es_list[i].mean += velocities[i]\n                self.es_list[i].mean = np.clip(self.es_list[i].mean, func.bounds.lb, func.bounds.ub)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 11, "feedback": "", "error": "Evaluation error: No module named 'cma'", "parent_ids": [], "operator": "random_new", "metadata": {"llm_response": "# Description: A hybrid algorithm combining aspects of CMA-ES and a simplified PSO, using CMA-ES for individual learning and PSO for social learning among a population of CMA-ES individuals.\n# Code:\n```python\nimport numpy as np\nimport cma\n\nclass HybridCMAESPSO:\n    def __init__(self, budget=10000, dim=10, popsize=10):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = popsize\n        self.inertia = 0.7\n        self.cognitive_coeff = 1.5\n        self.social_coeff = 1.5\n        self.es_list = []\n\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n\n        # Initialize population of CMA-ES individuals\n        x0 = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        sigma0 = 0.5  # Initial step size\n        for _ in range(self.popsize):\n          es = cma.purecma.CMAES(x0, sigma0,\n                                 {'bounds': [func.bounds.lb, func.bounds.ub],\n                                  'popsize': self.budget // self.popsize + 1}) # Ensure budget usage\n          self.es_list.append(es)\n\n        # Initialize personal bests\n        pbest_x = [np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim) for _ in range(self.popsize)]\n        pbest_f = [np.inf] * self.popsize\n        velocities = [np.zeros(self.dim) for _ in range(self.popsize)]\n\n        eval_count = 0\n        while eval_count < self.budget:\n            for i in range(self.popsize):\n                # CMA-ES step\n                solutions = self.es_list[i].ask()\n                fitness_values = []\n\n                for x in solutions:\n                    if eval_count < self.budget:\n                        f = func(x)\n                        eval_count += 1\n                        fitness_values.append(f)\n                        if f < self.f_opt:\n                            self.f_opt = f\n                            self.x_opt = x\n\n                        # Update personal best\n                        if f < pbest_f[i]:\n                            pbest_f[i] = f\n                            pbest_x[i] = x.copy()\n                    else:\n                        break\n\n                if not fitness_values:\n                    break  # Budget exhausted\n\n                self.es_list[i].tell(solutions[:len(fitness_values)], fitness_values)\n\n\n            # Simplified PSO update based on personal bests, using best individual in population as global best\n            best_index = np.argmin(pbest_f)\n            gbest_x = pbest_x[best_index]\n\n            for i in range(self.popsize):\n                r1 = np.random.rand(self.dim)\n                r2 = np.random.rand(self.dim)\n\n                velocities[i] = (self.inertia * velocities[i] +\n                                 self.cognitive_coeff * r1 * (pbest_x[i] - self.es_list[i].mean) +\n                                 self.social_coeff * r2 * (gbest_x - self.es_list[i].mean))\n\n                # Update the mean of the CMA-ES distribution based on PSO velocity, but limit the change\n                max_velocity_norm = 0.1 * np.linalg.norm(func.bounds.ub - func.bounds.lb) # Limit speed to 10% of the search space\n                velocity_norm = np.linalg.norm(velocities[i])\n                if velocity_norm > max_velocity_norm:\n                  velocities[i] = velocities[i] * (max_velocity_norm / velocity_norm)  # Normalize if exceeds limit\n\n                self.es_list[i].mean += velocities[i]\n                self.es_list[i].mean = np.clip(self.es_list[i].mean, func.bounds.lb, func.bounds.ub)\n\n        return self.f_opt, self.x_opt\n```", "operator": "random_new", "theta_sampled": 0.0054152852891059975, "reward": 0.0, "generation": 11}, "task_prompt": ""}
{"id": "4ac4bf05-a2f0-4f45-8c53-cb3113c226e7", "fitness": 0.7206813248660546, "name": "AdaptiveDEBirdSwarmCauchyHybrid", "description": "Combines Adaptive Differential Evolution with a Bird Swarm inspired neighborhood search and a novel adaptive Cauchy mutation, with an adaptive population size based on stagnation.", "code": "import numpy as np\n\nclass AdaptiveDEBirdSwarmCauchyHybrid:\n    def __init__(self, budget=10000, dim=10, initial_pop_size=40, F=0.5, CR=0.7, neighborhood_size=5, cauchy_probability=0.1, cauchy_scale=0.1, pop_size_adapt_freq=500):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = initial_pop_size\n        self.initial_pop_size = initial_pop_size\n        self.F = F  # Mutation factor\n        self.CR = CR  # Crossover rate\n        self.lb = -5.0\n        self.ub = 5.0\n        self.stagnation_counter = 0\n        self.stagnation_threshold = budget // 10\n        self.best_fitness_history = []\n        self.neighborhood_size = neighborhood_size\n        self.cauchy_probability = cauchy_probability\n        self.cauchy_scale = cauchy_scale\n        self.pop_size_adapt_freq = pop_size_adapt_freq\n        self.shrink_factor = 0.95\n\n    def __call__(self, func):\n        self.population = np.random.uniform(self.lb, self.ub, (self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n        self.eval_count = self.pop_size\n\n        while self.eval_count < self.budget:\n            for i in range(self.pop_size):\n                # Adaptive F and CR\n                adaptive_F = self.F * (1 + 0.1 * np.random.randn())\n                adaptive_CR = self.CR * (1 + 0.1 * np.random.randn())\n                adaptive_F = np.clip(adaptive_F, 0.1, 1.0)\n                adaptive_CR = np.clip(adaptive_CR, 0.1, 1.0)\n\n                # Mutation (DE part)\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = np.random.choice(idxs, 3, replace=False)\n                mutant = self.population[a] + adaptive_F * (self.population[b] - self.population[c])\n                mutant = np.clip(mutant, self.lb, self.ub)\n\n                # Crossover\n                trial = np.copy(self.population[i])\n                mask = np.random.rand(self.dim) < adaptive_CR\n                trial[mask] = mutant[mask]\n                trial = np.clip(trial, self.lb, self.ub)\n\n                # Bird Swarm Inspired Local Search and Adaptive Cauchy Mutation\n                if np.random.rand() < 0.5: # Apply local search with 50% probability\n                    # Neighborhood Search\n                    neighbors_indices = np.random.choice(self.pop_size, size=self.neighborhood_size, replace=False)\n                    neighbors = self.population[neighbors_indices]\n                    neighbors_fitness = self.fitness[neighbors_indices]\n                    best_neighbor_index = neighbors_indices[np.argmin(neighbors_fitness)]\n                    best_neighbor = self.population[best_neighbor_index]\n\n                    # Move towards the best neighbor\n                    trial = trial + 0.1 * (best_neighbor - trial)\n                    trial = np.clip(trial, self.lb, self.ub)\n\n                    # Adaptive Cauchy Mutation: Scale adapts based on current best fitness\n                    cauchy_scale_adaptive = self.cauchy_scale * np.abs(self.f_opt)  #Scale down when close to optimum\n                    if np.random.rand() < self.cauchy_probability:\n                        mutation = trial + (self.ub - self.lb) * np.random.standard_cauchy(size=self.dim) * cauchy_scale_adaptive\n                        mutation = np.clip(mutation, self.lb, self.ub)\n                        trial = mutation\n\n                # Selection\n                f_trial = func(trial)\n                self.eval_count += 1\n\n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n            # Stagnation Check and Restart\n            self.best_fitness_history.append(self.f_opt)\n            if len(self.best_fitness_history) > self.stagnation_threshold:\n                if self.best_fitness_history[-1] >= self.best_fitness_history[-self.stagnation_threshold]:\n                    self.stagnation_counter += 1\n                else:\n                    self.stagnation_counter = 0\n\n            if self.stagnation_counter >= self.stagnation_threshold:\n                #Restart the population, adapt population size\n                if self.pop_size > 10:\n                    self.pop_size = int(self.pop_size * self.shrink_factor)\n                else:\n                    self.pop_size = self.initial_pop_size #Reset population size if it becomes too small\n\n                self.population = np.random.uniform(self.lb, self.ub, (self.pop_size, self.dim))\n                self.fitness = np.array([func(x) for x in self.population])\n                self.f_opt = np.min(self.fitness)\n                self.x_opt = self.population[np.argmin(self.fitness)]\n                self.eval_count += self.pop_size - (self.budget-self.eval_count) if self.eval_count + self.pop_size > self.budget else self.pop_size\n                self.stagnation_counter = 0\n                self.best_fitness_history = [self.f_opt]\n\n            if self.eval_count >= self.budget:\n                break\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 11, "feedback": "The algorithm AdaptiveDEBirdSwarmCauchyHybrid scored 0.721 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["5b77f9fb-407a-4344-aa0a-a95d9f7e8690", "7f39ad48-10bd-4693-bf48-a08aea65c982"], "operator": "crossover", "metadata": {"llm_response": "# Description: Combines Adaptive Differential Evolution with a Bird Swarm inspired neighborhood search and a novel adaptive Cauchy mutation, with an adaptive population size based on stagnation.\n# Code: \n```python\nimport numpy as np\n\nclass AdaptiveDEBirdSwarmCauchyHybrid:\n    def __init__(self, budget=10000, dim=10, initial_pop_size=40, F=0.5, CR=0.7, neighborhood_size=5, cauchy_probability=0.1, cauchy_scale=0.1, pop_size_adapt_freq=500):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = initial_pop_size\n        self.initial_pop_size = initial_pop_size\n        self.F = F  # Mutation factor\n        self.CR = CR  # Crossover rate\n        self.lb = -5.0\n        self.ub = 5.0\n        self.stagnation_counter = 0\n        self.stagnation_threshold = budget // 10\n        self.best_fitness_history = []\n        self.neighborhood_size = neighborhood_size\n        self.cauchy_probability = cauchy_probability\n        self.cauchy_scale = cauchy_scale\n        self.pop_size_adapt_freq = pop_size_adapt_freq\n        self.shrink_factor = 0.95\n\n    def __call__(self, func):\n        self.population = np.random.uniform(self.lb, self.ub, (self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n        self.eval_count = self.pop_size\n\n        while self.eval_count < self.budget:\n            for i in range(self.pop_size):\n                # Adaptive F and CR\n                adaptive_F = self.F * (1 + 0.1 * np.random.randn())\n                adaptive_CR = self.CR * (1 + 0.1 * np.random.randn())\n                adaptive_F = np.clip(adaptive_F, 0.1, 1.0)\n                adaptive_CR = np.clip(adaptive_CR, 0.1, 1.0)\n\n                # Mutation (DE part)\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = np.random.choice(idxs, 3, replace=False)\n                mutant = self.population[a] + adaptive_F * (self.population[b] - self.population[c])\n                mutant = np.clip(mutant, self.lb, self.ub)\n\n                # Crossover\n                trial = np.copy(self.population[i])\n                mask = np.random.rand(self.dim) < adaptive_CR\n                trial[mask] = mutant[mask]\n                trial = np.clip(trial, self.lb, self.ub)\n\n                # Bird Swarm Inspired Local Search and Adaptive Cauchy Mutation\n                if np.random.rand() < 0.5: # Apply local search with 50% probability\n                    # Neighborhood Search\n                    neighbors_indices = np.random.choice(self.pop_size, size=self.neighborhood_size, replace=False)\n                    neighbors = self.population[neighbors_indices]\n                    neighbors_fitness = self.fitness[neighbors_indices]\n                    best_neighbor_index = neighbors_indices[np.argmin(neighbors_fitness)]\n                    best_neighbor = self.population[best_neighbor_index]\n\n                    # Move towards the best neighbor\n                    trial = trial + 0.1 * (best_neighbor - trial)\n                    trial = np.clip(trial, self.lb, self.ub)\n\n                    # Adaptive Cauchy Mutation: Scale adapts based on current best fitness\n                    cauchy_scale_adaptive = self.cauchy_scale * np.abs(self.f_opt)  #Scale down when close to optimum\n                    if np.random.rand() < self.cauchy_probability:\n                        mutation = trial + (self.ub - self.lb) * np.random.standard_cauchy(size=self.dim) * cauchy_scale_adaptive\n                        mutation = np.clip(mutation, self.lb, self.ub)\n                        trial = mutation\n\n                # Selection\n                f_trial = func(trial)\n                self.eval_count += 1\n\n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n            # Stagnation Check and Restart\n            self.best_fitness_history.append(self.f_opt)\n            if len(self.best_fitness_history) > self.stagnation_threshold:\n                if self.best_fitness_history[-1] >= self.best_fitness_history[-self.stagnation_threshold]:\n                    self.stagnation_counter += 1\n                else:\n                    self.stagnation_counter = 0\n\n            if self.stagnation_counter >= self.stagnation_threshold:\n                #Restart the population, adapt population size\n                if self.pop_size > 10:\n                    self.pop_size = int(self.pop_size * self.shrink_factor)\n                else:\n                    self.pop_size = self.initial_pop_size #Reset population size if it becomes too small\n\n                self.population = np.random.uniform(self.lb, self.ub, (self.pop_size, self.dim))\n                self.fitness = np.array([func(x) for x in self.population])\n                self.f_opt = np.min(self.fitness)\n                self.x_opt = self.population[np.argmin(self.fitness)]\n                self.eval_count += self.pop_size - (self.budget-self.eval_count) if self.eval_count + self.pop_size > self.budget else self.pop_size\n                self.stagnation_counter = 0\n                self.best_fitness_history = [self.f_opt]\n\n            if self.eval_count >= self.budget:\n                break\n        return self.f_opt, self.x_opt\n```", "aucs": [0.27916051340896864, 0.6611872373911544, 0.7003965687174845, 0.8949178772943506, 0.7859544068092142, 0.8343545120354448, 0.7308497515488115, 0.73297478814988, 0.7923106652832462, 0.7699134595361599, 0.8964043269035125, 0.9926175515521769, 0.2926232811575197, 0.7707396731266929, 0.9124883438859364, 0.8403018407325376, 0.651809597493437, 0.8814505161205983, 0.46871377029206784, 0.5244578158818997], "operator": "crossover", "theta_sampled": 0.008444944810722107, "reward": 0.0, "generation": 11}, "task_prompt": ""}
{"id": "d25d99b5-3750-48d8-b413-d596dbc36df9", "fitness": 0.7233789035888688, "name": "AdaptiveDEBirdSwarmArchive", "description": "Combines Adaptive Differential Evolution with Bird Swarm inspired local search, Cauchy mutation, and an archive to enhance exploration and exploitation.", "code": "import numpy as np\n\nclass AdaptiveDEBirdSwarmArchive:\n    def __init__(self, budget=10000, dim=10, pop_size=40, F=0.5, CR=0.7, neighborhood_size=5, cauchy_probability=0.1, archive_size=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F  # Mutation factor\n        self.CR = CR  # Crossover rate\n        self.lb = -5.0\n        self.ub = 5.0\n        self.stagnation_counter = 0\n        self.stagnation_threshold = budget // 10\n        self.best_fitness_history = []\n        self.neighborhood_size = neighborhood_size\n        self.cauchy_probability = cauchy_probability\n        self.cauchy_scale = 0.1\n        self.archive_size = archive_size\n        self.archive = []\n        self.archive_fitness = []\n\n    def __call__(self, func):\n        self.population = np.random.uniform(self.lb, self.ub, (self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n        self.eval_count = self.pop_size\n\n        # Initialize Archive\n        for i in range(min(self.archive_size, self.pop_size)):\n            self.archive.append(self.population[i].copy())\n            self.archive_fitness.append(self.fitness[i])\n\n        while self.eval_count < self.budget:\n            for i in range(self.pop_size):\n                # Adaptive F and CR\n                adaptive_F = self.F * (1 + 0.1 * np.random.randn())\n                adaptive_CR = self.CR * (1 + 0.1 * np.random.randn())\n                adaptive_F = np.clip(adaptive_F, 0.1, 1.0)\n                adaptive_CR = np.clip(adaptive_CR, 0.1, 1.0)\n\n                # Mutation (DE part)\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = np.random.choice(idxs, 3, replace=False)\n                mutant = self.population[a] + adaptive_F * (self.population[b] - self.population[c])\n                mutant = np.clip(mutant, self.lb, self.ub)\n\n                # Crossover\n                trial = np.copy(self.population[i])\n                mask = np.random.rand(self.dim) < adaptive_CR\n                trial[mask] = mutant[mask]\n                trial = np.clip(trial, self.lb, self.ub)\n\n                # Bird Swarm Inspired Local Search and Cauchy Mutation\n                if np.random.rand() < 0.5: # Apply local search with 50% probability\n                    # Neighborhood Search\n                    neighbors_indices = np.random.choice(self.pop_size, size=self.neighborhood_size, replace=False)\n                    neighbors = self.population[neighbors_indices]\n                    neighbors_fitness = self.fitness[neighbors_indices]\n                    best_neighbor_index = neighbors_indices[np.argmin(neighbors_fitness)]\n                    best_neighbor = self.population[best_neighbor_index]\n\n                    # Move towards the best neighbor\n                    trial = trial + 0.1 * (best_neighbor - trial)\n                    trial = np.clip(trial, self.lb, self.ub)\n\n                    # Cauchy Mutation\n                    if np.random.rand() < self.cauchy_probability:\n                        mutation = trial + (self.ub - self.lb) * np.random.standard_cauchy(size=self.dim) * self.cauchy_scale\n                        mutation = np.clip(mutation, self.lb, self.ub)\n                        trial = mutation\n\n                # Selection\n                f_trial = func(trial)\n                self.eval_count += 1\n\n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                \n                # Update Archive\n                if f_trial < np.max(self.archive_fitness):\n                    worst_index = np.argmax(self.archive_fitness)\n                    self.archive[worst_index] = trial.copy()\n                    self.archive_fitness[worst_index] = f_trial\n\n            # Stagnation Check and Restart\n            self.best_fitness_history.append(self.f_opt)\n            if len(self.best_fitness_history) > self.stagnation_threshold:\n                if self.best_fitness_history[-1] >= self.best_fitness_history[-self.stagnation_threshold]:\n                    self.stagnation_counter += 1\n                else:\n                    self.stagnation_counter = 0\n\n            if self.stagnation_counter >= self.stagnation_threshold:\n                #Restart the population, partially from archive\n                num_archive = min(self.pop_size // 2, len(self.archive))\n                new_pop = np.random.uniform(self.lb, self.ub, (self.pop_size - num_archive, self.dim))\n                \n                # Sort archive by fitness\n                sorted_indices = np.argsort(self.archive_fitness)\n                archive_subset = [self.archive[i] for i in sorted_indices[:num_archive]]\n                \n                self.population = np.concatenate((new_pop, np.array(archive_subset)), axis=0)\n                self.fitness = np.array([func(x) for x in self.population])\n                self.f_opt = np.min(self.fitness)\n                self.x_opt = self.population[np.argmin(self.fitness)]\n                self.eval_count += self.pop_size - (self.budget-self.eval_count) if self.eval_count + self.pop_size > self.budget else self.pop_size\n                self.stagnation_counter = 0\n                self.best_fitness_history = [self.f_opt]\n                #Re-initialize Archive\n                self.archive = []\n                self.archive_fitness = []\n                for i in range(min(self.archive_size, self.pop_size)):\n                    self.archive.append(self.population[i].copy())\n                    self.archive_fitness.append(self.fitness[i])\n\n            if self.eval_count >= self.budget:\n                break\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 11, "feedback": "The algorithm AdaptiveDEBirdSwarmArchive scored 0.723 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["5b77f9fb-407a-4344-aa0a-a95d9f7e8690", "7f39ad48-10bd-4693-bf48-a08aea65c982"], "operator": "crossover", "metadata": {"llm_response": "# Description: Combines Adaptive Differential Evolution with Bird Swarm inspired local search, Cauchy mutation, and an archive to enhance exploration and exploitation.\n# Code: \n```python\nimport numpy as np\n\nclass AdaptiveDEBirdSwarmArchive:\n    def __init__(self, budget=10000, dim=10, pop_size=40, F=0.5, CR=0.7, neighborhood_size=5, cauchy_probability=0.1, archive_size=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F  # Mutation factor\n        self.CR = CR  # Crossover rate\n        self.lb = -5.0\n        self.ub = 5.0\n        self.stagnation_counter = 0\n        self.stagnation_threshold = budget // 10\n        self.best_fitness_history = []\n        self.neighborhood_size = neighborhood_size\n        self.cauchy_probability = cauchy_probability\n        self.cauchy_scale = 0.1\n        self.archive_size = archive_size\n        self.archive = []\n        self.archive_fitness = []\n\n    def __call__(self, func):\n        self.population = np.random.uniform(self.lb, self.ub, (self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n        self.eval_count = self.pop_size\n\n        # Initialize Archive\n        for i in range(min(self.archive_size, self.pop_size)):\n            self.archive.append(self.population[i].copy())\n            self.archive_fitness.append(self.fitness[i])\n\n        while self.eval_count < self.budget:\n            for i in range(self.pop_size):\n                # Adaptive F and CR\n                adaptive_F = self.F * (1 + 0.1 * np.random.randn())\n                adaptive_CR = self.CR * (1 + 0.1 * np.random.randn())\n                adaptive_F = np.clip(adaptive_F, 0.1, 1.0)\n                adaptive_CR = np.clip(adaptive_CR, 0.1, 1.0)\n\n                # Mutation (DE part)\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = np.random.choice(idxs, 3, replace=False)\n                mutant = self.population[a] + adaptive_F * (self.population[b] - self.population[c])\n                mutant = np.clip(mutant, self.lb, self.ub)\n\n                # Crossover\n                trial = np.copy(self.population[i])\n                mask = np.random.rand(self.dim) < adaptive_CR\n                trial[mask] = mutant[mask]\n                trial = np.clip(trial, self.lb, self.ub)\n\n                # Bird Swarm Inspired Local Search and Cauchy Mutation\n                if np.random.rand() < 0.5: # Apply local search with 50% probability\n                    # Neighborhood Search\n                    neighbors_indices = np.random.choice(self.pop_size, size=self.neighborhood_size, replace=False)\n                    neighbors = self.population[neighbors_indices]\n                    neighbors_fitness = self.fitness[neighbors_indices]\n                    best_neighbor_index = neighbors_indices[np.argmin(neighbors_fitness)]\n                    best_neighbor = self.population[best_neighbor_index]\n\n                    # Move towards the best neighbor\n                    trial = trial + 0.1 * (best_neighbor - trial)\n                    trial = np.clip(trial, self.lb, self.ub)\n\n                    # Cauchy Mutation\n                    if np.random.rand() < self.cauchy_probability:\n                        mutation = trial + (self.ub - self.lb) * np.random.standard_cauchy(size=self.dim) * self.cauchy_scale\n                        mutation = np.clip(mutation, self.lb, self.ub)\n                        trial = mutation\n\n                # Selection\n                f_trial = func(trial)\n                self.eval_count += 1\n\n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                \n                # Update Archive\n                if f_trial < np.max(self.archive_fitness):\n                    worst_index = np.argmax(self.archive_fitness)\n                    self.archive[worst_index] = trial.copy()\n                    self.archive_fitness[worst_index] = f_trial\n\n            # Stagnation Check and Restart\n            self.best_fitness_history.append(self.f_opt)\n            if len(self.best_fitness_history) > self.stagnation_threshold:\n                if self.best_fitness_history[-1] >= self.best_fitness_history[-self.stagnation_threshold]:\n                    self.stagnation_counter += 1\n                else:\n                    self.stagnation_counter = 0\n\n            if self.stagnation_counter >= self.stagnation_threshold:\n                #Restart the population, partially from archive\n                num_archive = min(self.pop_size // 2, len(self.archive))\n                new_pop = np.random.uniform(self.lb, self.ub, (self.pop_size - num_archive, self.dim))\n                \n                # Sort archive by fitness\n                sorted_indices = np.argsort(self.archive_fitness)\n                archive_subset = [self.archive[i] for i in sorted_indices[:num_archive]]\n                \n                self.population = np.concatenate((new_pop, np.array(archive_subset)), axis=0)\n                self.fitness = np.array([func(x) for x in self.population])\n                self.f_opt = np.min(self.fitness)\n                self.x_opt = self.population[np.argmin(self.fitness)]\n                self.eval_count += self.pop_size - (self.budget-self.eval_count) if self.eval_count + self.pop_size > self.budget else self.pop_size\n                self.stagnation_counter = 0\n                self.best_fitness_history = [self.f_opt]\n                #Re-initialize Archive\n                self.archive = []\n                self.archive_fitness = []\n                for i in range(min(self.archive_size, self.pop_size)):\n                    self.archive.append(self.population[i].copy())\n                    self.archive_fitness.append(self.fitness[i])\n\n            if self.eval_count >= self.budget:\n                break\n        return self.f_opt, self.x_opt\n```", "aucs": [0.2377519272609948, 0.5978609194882811, 0.6861488968549923, 0.8912699684535079, 0.8037136268330305, 0.8300624096537047, 0.7268634170148403, 0.704307242161027, 0.8014312972134943, 0.7517991741842404, 0.8711670474913201, 0.9916636883913138, 0.3997728109550922, 0.7896409305702708, 0.9143372946321608, 0.8308437967509348, 0.666247127369008, 0.8802782991050927, 0.5687533247488279, 0.5236648726452422], "operator": "crossover", "theta_sampled": 0.007941777403052103, "reward": 0.0, "generation": 11}, "task_prompt": ""}
{"id": "3b387073-58a6-4ee0-9faf-ba6fbc4ee604", "fitness": 0.7238277862335515, "name": "AdaptiveHybridDEBirdSwarmPlus", "description": "Combines Adaptive Differential Evolution with Bird Swarm Optimization's neighborhood search and Cauchy mutation, alongside a success-history based adaptation of F and CR, and dynamic population resizing for enhanced exploration and exploitation.", "code": "import numpy as np\n\nclass AdaptiveHybridDEBirdSwarmPlus:\n    def __init__(self, budget=10000, dim=10, pop_size=40, F=0.5, CR=0.7, neighborhood_size=5, cauchy_probability=0.1, success_history_size=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = popsize = int(np.clip(pop_size, 10, 100))  # Constrain pop_size\n        self.F = F  # Mutation factor\n        self.CR = CR  # Crossover rate\n        self.lb = -5.0\n        self.ub = 5.0\n        self.stagnation_counter = 0\n        self.stagnation_threshold = budget // 10\n        self.best_fitness_history = []\n        self.neighborhood_size = neighborhood_size\n        self.cauchy_probability = cauchy_probability\n        self.cauchy_scale = 0.1\n        self.success_history_size = success_history_size\n        self.success_f = []\n        self.success_cr = []\n        self.min_pop_size = 10\n        self.max_pop_size = 100\n        self.pop_resize_interval = budget // 20\n\n    def __call__(self, func):\n        self.population = np.random.uniform(self.lb, self.ub, (self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n        self.eval_count = self.pop_size\n        self.generation = 0\n\n        while self.eval_count < self.budget:\n            self.generation += 1\n\n            for i in range(self.pop_size):\n                # Adaptive F and CR using success history\n                if self.success_f and self.success_cr:\n                    adaptive_F = np.random.choice(self.success_f)\n                    adaptive_CR = np.random.choice(self.success_cr)\n                else:\n                    adaptive_F = self.F * (1 + 0.1 * np.random.randn())\n                    adaptive_CR = self.CR * (1 + 0.1 * np.random.randn())\n\n                adaptive_F = np.clip(adaptive_F, 0.1, 1.0)\n                adaptive_CR = np.clip(adaptive_CR, 0.1, 1.0)\n\n                # Mutation (DE part)\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = np.random.choice(idxs, 3, replace=False)\n                mutant = self.population[a] + adaptive_F * (self.population[b] - self.population[c])\n                mutant = np.clip(mutant, self.lb, self.ub)\n\n                # Crossover\n                trial = np.copy(self.population[i])\n                mask = np.random.rand(self.dim) < adaptive_CR\n                trial[mask] = mutant[mask]\n                trial = np.clip(trial, self.lb, self.ub)\n\n                # Bird Swarm Inspired Local Search and Cauchy Mutation\n                if np.random.rand() < 0.5: # Apply local search with 50% probability\n                    # Neighborhood Search\n                    neighbors_indices = np.random.choice(self.pop_size, size=self.neighborhood_size, replace=False)\n                    neighbors = self.population[neighbors_indices]\n                    neighbors_fitness = self.fitness[neighbors_indices]\n                    best_neighbor_index = neighbors_indices[np.argmin(neighbors_fitness)]\n                    best_neighbor = self.population[best_neighbor_index]\n\n                    # Move towards the best neighbor\n                    trial = trial + 0.1 * (best_neighbor - trial)\n                    trial = np.clip(trial, self.lb, self.ub)\n\n                    # Cauchy Mutation\n                    if np.random.rand() < self.cauchy_probability:\n                        mutation = trial + (self.ub - self.lb) * np.random.standard_cauchy(size=self.dim) * self.cauchy_scale\n                        mutation = np.clip(mutation, self.lb, self.ub)\n                        trial = mutation\n\n                # Selection\n                f_trial = func(trial)\n                self.eval_count += 1\n\n                if f_trial < self.fitness[i]:\n                    # Update success history\n                    self.success_f.append(adaptive_F)\n                    self.success_cr.append(adaptive_CR)\n                    if len(self.success_f) > self.success_history_size:\n                        self.success_f.pop(0)\n                        self.success_cr.pop(0)\n\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n            # Stagnation Check and Restart/Population Size Adjustment\n            self.best_fitness_history.append(self.f_opt)\n            if len(self.best_fitness_history) > self.stagnation_threshold:\n                if self.best_fitness_history[-1] >= self.best_fitness_history[-self.stagnation_threshold]:\n                    self.stagnation_counter += 1\n                else:\n                    self.stagnation_counter = 0\n\n            if self.stagnation_counter >= self.stagnation_threshold:\n                #Restart the population with possible population size adjustment\n                if self.generation % self.pop_resize_interval == 0:\n                    if np.random.rand() < 0.5: #shrink population\n                        self.pop_size = int(max(self.pop_size * 0.9, self.min_pop_size))\n                    else: #grow population\n                        self.pop_size = int(min(self.pop_size * 1.1, self.max_pop_size))\n\n                self.population = np.random.uniform(self.lb, self.ub, (self.pop_size, self.dim))\n                self.fitness = np.array([func(x) for x in self.population])\n                self.f_opt = np.min(self.fitness)\n                self.x_opt = self.population[np.argmin(self.fitness)]\n                self.eval_count += self.pop_size - (self.budget-self.eval_count) if self.eval_count + self.pop_size > self.budget else self.pop_size\n                self.stagnation_counter = 0\n                self.best_fitness_history = [self.f_opt]\n\n            if self.eval_count >= self.budget:\n                break\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 11, "feedback": "The algorithm AdaptiveHybridDEBirdSwarmPlus scored 0.724 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["5b77f9fb-407a-4344-aa0a-a95d9f7e8690", "7f39ad48-10bd-4693-bf48-a08aea65c982"], "operator": "crossover", "metadata": {"llm_response": "# Description: Combines Adaptive Differential Evolution with Bird Swarm Optimization's neighborhood search and Cauchy mutation, alongside a success-history based adaptation of F and CR, and dynamic population resizing for enhanced exploration and exploitation.\n# Code: \n```python\nimport numpy as np\n\nclass AdaptiveHybridDEBirdSwarmPlus:\n    def __init__(self, budget=10000, dim=10, pop_size=40, F=0.5, CR=0.7, neighborhood_size=5, cauchy_probability=0.1, success_history_size=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = popsize = int(np.clip(pop_size, 10, 100))  # Constrain pop_size\n        self.F = F  # Mutation factor\n        self.CR = CR  # Crossover rate\n        self.lb = -5.0\n        self.ub = 5.0\n        self.stagnation_counter = 0\n        self.stagnation_threshold = budget // 10\n        self.best_fitness_history = []\n        self.neighborhood_size = neighborhood_size\n        self.cauchy_probability = cauchy_probability\n        self.cauchy_scale = 0.1\n        self.success_history_size = success_history_size\n        self.success_f = []\n        self.success_cr = []\n        self.min_pop_size = 10\n        self.max_pop_size = 100\n        self.pop_resize_interval = budget // 20\n\n    def __call__(self, func):\n        self.population = np.random.uniform(self.lb, self.ub, (self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n        self.eval_count = self.pop_size\n        self.generation = 0\n\n        while self.eval_count < self.budget:\n            self.generation += 1\n\n            for i in range(self.pop_size):\n                # Adaptive F and CR using success history\n                if self.success_f and self.success_cr:\n                    adaptive_F = np.random.choice(self.success_f)\n                    adaptive_CR = np.random.choice(self.success_cr)\n                else:\n                    adaptive_F = self.F * (1 + 0.1 * np.random.randn())\n                    adaptive_CR = self.CR * (1 + 0.1 * np.random.randn())\n\n                adaptive_F = np.clip(adaptive_F, 0.1, 1.0)\n                adaptive_CR = np.clip(adaptive_CR, 0.1, 1.0)\n\n                # Mutation (DE part)\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = np.random.choice(idxs, 3, replace=False)\n                mutant = self.population[a] + adaptive_F * (self.population[b] - self.population[c])\n                mutant = np.clip(mutant, self.lb, self.ub)\n\n                # Crossover\n                trial = np.copy(self.population[i])\n                mask = np.random.rand(self.dim) < adaptive_CR\n                trial[mask] = mutant[mask]\n                trial = np.clip(trial, self.lb, self.ub)\n\n                # Bird Swarm Inspired Local Search and Cauchy Mutation\n                if np.random.rand() < 0.5: # Apply local search with 50% probability\n                    # Neighborhood Search\n                    neighbors_indices = np.random.choice(self.pop_size, size=self.neighborhood_size, replace=False)\n                    neighbors = self.population[neighbors_indices]\n                    neighbors_fitness = self.fitness[neighbors_indices]\n                    best_neighbor_index = neighbors_indices[np.argmin(neighbors_fitness)]\n                    best_neighbor = self.population[best_neighbor_index]\n\n                    # Move towards the best neighbor\n                    trial = trial + 0.1 * (best_neighbor - trial)\n                    trial = np.clip(trial, self.lb, self.ub)\n\n                    # Cauchy Mutation\n                    if np.random.rand() < self.cauchy_probability:\n                        mutation = trial + (self.ub - self.lb) * np.random.standard_cauchy(size=self.dim) * self.cauchy_scale\n                        mutation = np.clip(mutation, self.lb, self.ub)\n                        trial = mutation\n\n                # Selection\n                f_trial = func(trial)\n                self.eval_count += 1\n\n                if f_trial < self.fitness[i]:\n                    # Update success history\n                    self.success_f.append(adaptive_F)\n                    self.success_cr.append(adaptive_CR)\n                    if len(self.success_f) > self.success_history_size:\n                        self.success_f.pop(0)\n                        self.success_cr.pop(0)\n\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n            # Stagnation Check and Restart/Population Size Adjustment\n            self.best_fitness_history.append(self.f_opt)\n            if len(self.best_fitness_history) > self.stagnation_threshold:\n                if self.best_fitness_history[-1] >= self.best_fitness_history[-self.stagnation_threshold]:\n                    self.stagnation_counter += 1\n                else:\n                    self.stagnation_counter = 0\n\n            if self.stagnation_counter >= self.stagnation_threshold:\n                #Restart the population with possible population size adjustment\n                if self.generation % self.pop_resize_interval == 0:\n                    if np.random.rand() < 0.5: #shrink population\n                        self.pop_size = int(max(self.pop_size * 0.9, self.min_pop_size))\n                    else: #grow population\n                        self.pop_size = int(min(self.pop_size * 1.1, self.max_pop_size))\n\n                self.population = np.random.uniform(self.lb, self.ub, (self.pop_size, self.dim))\n                self.fitness = np.array([func(x) for x in self.population])\n                self.f_opt = np.min(self.fitness)\n                self.x_opt = self.population[np.argmin(self.fitness)]\n                self.eval_count += self.pop_size - (self.budget-self.eval_count) if self.eval_count + self.pop_size > self.budget else self.pop_size\n                self.stagnation_counter = 0\n                self.best_fitness_history = [self.f_opt]\n\n            if self.eval_count >= self.budget:\n                break\n        return self.f_opt, self.x_opt\n```", "aucs": [0.2875320483123115, 0.5633815774108817, 0.7149513280250065, 0.885061517284435, 0.7816056962466014, 0.836243810635068, 0.7204112145005162, 0.7499908949606949, 0.8093691443253197, 0.752266446300286, 0.8690737975055634, 0.9953538936267381, 0.5587019044553825, 0.7889944422023746, 0.9170142154790316, 0.846441942852743, 0.5681852860879932, 0.8737170683091892, 0.4303214654119353, 0.5279380307389585], "operator": "crossover", "theta_sampled": 0.009024161535554215, "reward": 0.0, "generation": 11}, "task_prompt": ""}
{"id": "bf0c98a5-9594-4d2d-af82-24a057cd54d4", "fitness": 0.6844272743941361, "name": "AdaptiveHybridBirdSwarmTrend", "description": "A self-adjusting hybrid Bird Swarm Optimizer with adaptive Cauchy mutation and a local search fine-tuning step, enhanced with a trend-following mechanism for improved exploration and exploitation.", "code": "import numpy as np\n\nclass AdaptiveHybridBirdSwarmTrend:\n    def __init__(self, budget=10000, dim=10, swarm_size=20, inertia_weight=0.7, cognitive_coeff=1.4, social_coeff=1.4, neighborhood_size=3, shrink_factor=0.95, initial_cauchy_scale=0.1, local_search_iterations=5, cauchy_probability=0.2, exploration_probability=0.1, trend_factor=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = swarm_size\n        self.inertia_weight = inertia_weight\n        self.cognitive_coeff = cognitive_coeff\n        self.social_coeff = social_coeff\n        self.neighborhood_size = neighborhood_size\n        self.shrink_factor = shrink_factor\n        self.cauchy_scale = initial_cauchy_scale\n        self.local_search_iterations = local_search_iterations\n        self.cauchy_probability = cauchy_probability\n        self.exploration_probability = exploration_probability\n        self.success_rate = 0.5\n        self.inertia_weight_decay = 0.995  # Decay factor for inertia weight\n        self.cognitive_coeff_increase = 1.005  # Increase factor for cognitive coefficient\n        self.social_coeff_decrease = 0.995  # Decrease factor for social coefficient\n        self.min_inertia_weight = 0.4 #Minimum intertia weight value\n        self.trend_factor = trend_factor #Strength of the trend following\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        \n        # Initialize swarm positions and velocities\n        swarm_positions = np.random.uniform(lb, ub, size=(self.swarm_size, self.dim))\n        swarm_velocities = np.random.uniform(-abs(ub-lb)/10, abs(ub-lb)/10, size=(self.swarm_size, self.dim))\n        swarm_fitness = np.array([func(x) for x in swarm_positions])\n        self.budget -= self.swarm_size\n        \n        personal_best_positions = swarm_positions.copy()\n        personal_best_fitness = swarm_fitness.copy()\n        \n        global_best_index = np.argmin(personal_best_fitness)\n        global_best_position = personal_best_positions[global_best_index].copy()\n        global_best_fitness = personal_best_fitness[global_best_index]\n        \n        search_range = ub - lb\n        center = global_best_position #Use global best as center for Cauchy\n\n        successes = 0\n        iterations = 0\n        trend = np.zeros(self.dim) #Initialize a trend vector\n\n        while self.budget > 0:\n            for i in range(self.swarm_size):\n                if self.budget <= 0:\n                    break\n\n                # Exploration phase: Random jump with a probability\n                if np.random.rand() < self.exploration_probability:\n                    new_position = np.random.uniform(lb, ub, size=self.dim)\n                else:\n                    # Bird Swarm Optimization part\n                    inertia = self.inertia_weight * swarm_velocities[i]\n                    cognitive = self.cognitive_coeff * np.random.rand(self.dim) * (personal_best_positions[i] - swarm_positions[i])\n                    \n                    # Social component: find best neighbor\n                    neighbors_indices = np.random.choice(self.swarm_size, size=self.neighborhood_size, replace=False)\n                    neighbors_fitness = personal_best_fitness[neighbors_indices]\n                    best_neighbor_index = neighbors_indices[np.argmin(neighbors_fitness)]\n                    social = self.social_coeff * np.random.rand(self.dim) * (personal_best_positions[best_neighbor_index] - swarm_positions[i])\n                    \n                    # Trend Following: Introduce the trend\n                    trend_component = self.trend_factor * trend\n                    swarm_velocities[i] = inertia + cognitive + social + trend_component\n                    \n                    # Update position\n                    new_position = swarm_positions[i] + swarm_velocities[i]\n\n                # Cauchy mutation with probability cauchy_probability\n                if np.random.rand() < self.cauchy_probability:\n                    mutation = center + search_range * np.random.standard_cauchy(size=self.dim) * self.cauchy_scale  # Scale Cauchy distribution\n                    new_position = mutation\n                \n                # Boundary handling\n                new_position = np.clip(new_position, lb, ub)\n                \n                new_fitness = func(new_position)\n                self.budget -= 1\n                iterations += 1\n                \n                if new_fitness < personal_best_fitness[i]:\n                    personal_best_fitness[i] = new_fitness\n                    personal_best_positions[i] = new_position.copy()\n                    successes += 1\n                    \n                    if new_fitness < global_best_fitness:\n                        #Update the trend based on movement towards the new global best\n                        trend = new_position - global_best_position \n                        global_best_fitness = new_fitness\n                        global_best_position = new_position.copy()\n                        center = global_best_position #Update center\n\n                swarm_positions[i] = new_position\n                \n            # Shrink search range\n            search_range *= self.shrink_factor\n            lb = np.clip(center - search_range/2, func.bounds.lb, func.bounds.ub)\n            ub = np.clip(center + search_range/2, func.bounds.lb, func.bounds.ub)\n\n            # Dynamic Parameter Control\n            self.inertia_weight = max(self.inertia_weight * self.inertia_weight_decay, self.min_inertia_weight)\n            self.cognitive_coeff *= self.cognitive_coeff_increase\n            self.social_coeff *= self.social_coeff_decrease\n            \n            # Adjust Cauchy scale based on success rate\n            if iterations > 0:\n                self.success_rate = successes / iterations\n                if self.success_rate > 0.2:\n                    self.cauchy_scale *= 1.1  # Increase scale if successful\n                else:\n                    self.cauchy_scale *= 0.9  # Decrease scale if not successful\n                successes = 0\n                iterations = 0\n\n            self.cauchy_scale = np.clip(self.cauchy_scale, 0.01, 1.0)  # Limit Cauchy scale\n        \n        # Local Search\n        for _ in range(min(self.local_search_iterations, self.budget)):\n             if self.budget <= 0:\n                 break\n\n             perturbation = np.random.uniform(-search_range/100, search_range/100, self.dim)\n             local_x = np.clip(global_best_position + perturbation, lb, ub)\n             local_f = func(local_x)\n             self.budget -=1\n\n             if local_f < global_best_fitness:\n                  global_best_fitness = local_f\n                  global_best_position = local_x\n\n\n        return global_best_fitness, global_best_position", "configspace": "", "generation": 11, "feedback": "The algorithm AdaptiveHybridBirdSwarmTrend scored 0.684 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["a59e4969-8f47-4293-8622-3e4ef39ee479"], "operator": "mutation", "metadata": {"llm_response": "# Description: A self-adjusting hybrid Bird Swarm Optimizer with adaptive Cauchy mutation and a local search fine-tuning step, enhanced with a trend-following mechanism for improved exploration and exploitation.\n# Code: \n```python\nimport numpy as np\n\nclass AdaptiveHybridBirdSwarmTrend:\n    def __init__(self, budget=10000, dim=10, swarm_size=20, inertia_weight=0.7, cognitive_coeff=1.4, social_coeff=1.4, neighborhood_size=3, shrink_factor=0.95, initial_cauchy_scale=0.1, local_search_iterations=5, cauchy_probability=0.2, exploration_probability=0.1, trend_factor=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = swarm_size\n        self.inertia_weight = inertia_weight\n        self.cognitive_coeff = cognitive_coeff\n        self.social_coeff = social_coeff\n        self.neighborhood_size = neighborhood_size\n        self.shrink_factor = shrink_factor\n        self.cauchy_scale = initial_cauchy_scale\n        self.local_search_iterations = local_search_iterations\n        self.cauchy_probability = cauchy_probability\n        self.exploration_probability = exploration_probability\n        self.success_rate = 0.5\n        self.inertia_weight_decay = 0.995  # Decay factor for inertia weight\n        self.cognitive_coeff_increase = 1.005  # Increase factor for cognitive coefficient\n        self.social_coeff_decrease = 0.995  # Decrease factor for social coefficient\n        self.min_inertia_weight = 0.4 #Minimum intertia weight value\n        self.trend_factor = trend_factor #Strength of the trend following\n\n    def __call__(self, func):\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        \n        # Initialize swarm positions and velocities\n        swarm_positions = np.random.uniform(lb, ub, size=(self.swarm_size, self.dim))\n        swarm_velocities = np.random.uniform(-abs(ub-lb)/10, abs(ub-lb)/10, size=(self.swarm_size, self.dim))\n        swarm_fitness = np.array([func(x) for x in swarm_positions])\n        self.budget -= self.swarm_size\n        \n        personal_best_positions = swarm_positions.copy()\n        personal_best_fitness = swarm_fitness.copy()\n        \n        global_best_index = np.argmin(personal_best_fitness)\n        global_best_position = personal_best_positions[global_best_index].copy()\n        global_best_fitness = personal_best_fitness[global_best_index]\n        \n        search_range = ub - lb\n        center = global_best_position #Use global best as center for Cauchy\n\n        successes = 0\n        iterations = 0\n        trend = np.zeros(self.dim) #Initialize a trend vector\n\n        while self.budget > 0:\n            for i in range(self.swarm_size):\n                if self.budget <= 0:\n                    break\n\n                # Exploration phase: Random jump with a probability\n                if np.random.rand() < self.exploration_probability:\n                    new_position = np.random.uniform(lb, ub, size=self.dim)\n                else:\n                    # Bird Swarm Optimization part\n                    inertia = self.inertia_weight * swarm_velocities[i]\n                    cognitive = self.cognitive_coeff * np.random.rand(self.dim) * (personal_best_positions[i] - swarm_positions[i])\n                    \n                    # Social component: find best neighbor\n                    neighbors_indices = np.random.choice(self.swarm_size, size=self.neighborhood_size, replace=False)\n                    neighbors_fitness = personal_best_fitness[neighbors_indices]\n                    best_neighbor_index = neighbors_indices[np.argmin(neighbors_fitness)]\n                    social = self.social_coeff * np.random.rand(self.dim) * (personal_best_positions[best_neighbor_index] - swarm_positions[i])\n                    \n                    # Trend Following: Introduce the trend\n                    trend_component = self.trend_factor * trend\n                    swarm_velocities[i] = inertia + cognitive + social + trend_component\n                    \n                    # Update position\n                    new_position = swarm_positions[i] + swarm_velocities[i]\n\n                # Cauchy mutation with probability cauchy_probability\n                if np.random.rand() < self.cauchy_probability:\n                    mutation = center + search_range * np.random.standard_cauchy(size=self.dim) * self.cauchy_scale  # Scale Cauchy distribution\n                    new_position = mutation\n                \n                # Boundary handling\n                new_position = np.clip(new_position, lb, ub)\n                \n                new_fitness = func(new_position)\n                self.budget -= 1\n                iterations += 1\n                \n                if new_fitness < personal_best_fitness[i]:\n                    personal_best_fitness[i] = new_fitness\n                    personal_best_positions[i] = new_position.copy()\n                    successes += 1\n                    \n                    if new_fitness < global_best_fitness:\n                        #Update the trend based on movement towards the new global best\n                        trend = new_position - global_best_position \n                        global_best_fitness = new_fitness\n                        global_best_position = new_position.copy()\n                        center = global_best_position #Update center\n\n                swarm_positions[i] = new_position\n                \n            # Shrink search range\n            search_range *= self.shrink_factor\n            lb = np.clip(center - search_range/2, func.bounds.lb, func.bounds.ub)\n            ub = np.clip(center + search_range/2, func.bounds.lb, func.bounds.ub)\n\n            # Dynamic Parameter Control\n            self.inertia_weight = max(self.inertia_weight * self.inertia_weight_decay, self.min_inertia_weight)\n            self.cognitive_coeff *= self.cognitive_coeff_increase\n            self.social_coeff *= self.social_coeff_decrease\n            \n            # Adjust Cauchy scale based on success rate\n            if iterations > 0:\n                self.success_rate = successes / iterations\n                if self.success_rate > 0.2:\n                    self.cauchy_scale *= 1.1  # Increase scale if successful\n                else:\n                    self.cauchy_scale *= 0.9  # Decrease scale if not successful\n                successes = 0\n                iterations = 0\n\n            self.cauchy_scale = np.clip(self.cauchy_scale, 0.01, 1.0)  # Limit Cauchy scale\n        \n        # Local Search\n        for _ in range(min(self.local_search_iterations, self.budget)):\n             if self.budget <= 0:\n                 break\n\n             perturbation = np.random.uniform(-search_range/100, search_range/100, self.dim)\n             local_x = np.clip(global_best_position + perturbation, lb, ub)\n             local_f = func(local_x)\n             self.budget -=1\n\n             if local_f < global_best_fitness:\n                  global_best_fitness = local_f\n                  global_best_position = local_x\n\n\n        return global_best_fitness, global_best_position\n```", "aucs": [0.20926110778497564, 0.3829762411783034, 0.8099097060011216, 0.9178293106667162, 0.84460998729302, 0.8547298120224951, 0.3331857907570892, 0.7631586658918674, 0.8296180656557524, 0.8295124018431148, 0.8987248065687685, 0.999501350778986, 0.2012405306709648, 0.8329814360794257, 0.7787825733512641, 0.855470093994334, 0.7081827803439742, 0.8932671730589723, 0.2060294670698105, 0.5395741868717624], "operator": "mutation", "theta_sampled": 0.007617985220121485, "reward": 0.0, "generation": 11}, "task_prompt": ""}
{"id": "eb6ee6ce-d853-48a9-aefd-40a62c23a4fd", "fitness": 0.0, "name": "AdaptiveDELevyBirdSwarm", "description": "Combines Adaptive Differential Evolution with Bird Swarm Optimization's local search and Cauchy mutation, enhanced with Lvy flight mutation for broader exploration and a shrinking search space.", "code": "import numpy as np\n\nclass AdaptiveDELevyBirdSwarm:\n    def __init__(self, budget=10000, dim=10, pop_size=40, F=0.5, CR=0.7, neighborhood_size=5, cauchy_probability=0.1, levy_probability=0.1, initial_scale=5.0):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F  # Mutation factor\n        self.CR = CR  # Crossover rate\n        self.lb = -5.0\n        self.ub = 5.0\n        self.stagnation_counter = 0\n        self.stagnation_threshold = budget // 10\n        self.best_fitness_history = []\n        self.neighborhood_size = neighborhood_size\n        self.cauchy_probability = cauchy_probability\n        self.cauchy_scale = 0.1\n        self.levy_probability = levy_probability\n        self.initial_scale = initial_scale\n        self.current_scale = initial_scale\n\n\n    def levy_flight(self, size, beta=1.5):\n        sigma = (np.math.gamma(1 + beta) * np.sin(np.pi * beta / 2) / (np.math.gamma((1 + beta) / 2) * beta * (2 ** ((beta - 1) / 2)))) ** (1 / beta)\n        u = np.random.normal(0, sigma, size)\n        v = np.random.normal(0, 1, size)\n        step = u / abs(v) ** (1 / beta)\n        return step\n\n    def __call__(self, func):\n        self.population = np.random.uniform(self.lb, self.ub, (self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n        self.eval_count = self.pop_size\n\n        while self.eval_count < self.budget:\n            for i in range(self.pop_size):\n                # Adaptive F and CR\n                adaptive_F = self.F * (1 + 0.1 * np.random.randn())\n                adaptive_CR = self.CR * (1 + 0.1 * np.random.randn())\n                adaptive_F = np.clip(adaptive_F, 0.1, 1.0)\n                adaptive_CR = np.clip(adaptive_CR, 0.1, 1.0)\n\n                # Mutation (DE part)\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = np.random.choice(idxs, 3, replace=False)\n                mutant = self.population[a] + adaptive_F * (self.population[b] - self.population[c])\n                mutant = np.clip(mutant, self.lb, self.ub)\n\n                # Crossover\n                trial = np.copy(self.population[i])\n                mask = np.random.rand(self.dim) < adaptive_CR\n                trial[mask] = mutant[mask]\n                trial = np.clip(trial, self.lb, self.ub)\n\n                # Bird Swarm Inspired Local Search and Cauchy/Levy Mutation\n                if np.random.rand() < 0.5: # Apply local search with 50% probability\n                    # Neighborhood Search\n                    neighbors_indices = np.random.choice(self.pop_size, size=self.neighborhood_size, replace=False)\n                    neighbors = self.population[neighbors_indices]\n                    neighbors_fitness = self.fitness[neighbors_indices]\n                    best_neighbor_index = neighbors_indices[np.argmin(neighbors_fitness)]\n                    best_neighbor = self.population[best_neighbor_index]\n\n                    # Move towards the best neighbor\n                    trial = trial + 0.1 * (best_neighbor - trial)\n                    trial = np.clip(trial, self.lb, self.ub)\n\n                    # Cauchy Mutation\n                    if np.random.rand() < self.cauchy_probability:\n                        mutation = trial + (self.ub - self.lb) * np.random.standard_cauchy(size=self.dim) * self.cauchy_scale\n                        mutation = np.clip(mutation, self.lb, self.ub)\n                        trial = mutation\n\n                    #Levy Flight Mutation\n                    if np.random.rand() < self.levy_probability:\n                        levy_steps = self.levy_flight(self.dim)\n                        mutation = trial + self.current_scale * levy_steps\n                        mutation = np.clip(mutation, self.lb, self.ub)\n                        trial = mutation\n\n\n                # Selection\n                f_trial = func(trial)\n                self.eval_count += 1\n\n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n            # Stagnation Check and Restart or Shrink\n            self.best_fitness_history.append(self.f_opt)\n            if len(self.best_fitness_history) > self.stagnation_threshold:\n                if self.best_fitness_history[-1] >= self.best_fitness_history[-self.stagnation_threshold]:\n                    self.stagnation_counter += 1\n                else:\n                    self.stagnation_counter = 0\n\n            if self.stagnation_counter >= self.stagnation_threshold:\n                # Option 1: Restart\n                #self.population = np.random.uniform(self.lb, self.ub, (self.pop_size, self.dim))\n\n                # Option 2: Shrink search space.\n                self.lb = self.x_opt - self.current_scale\n                self.ub = self.x_opt + self.current_scale\n                self.lb = np.clip(self.lb, -5.0, 5.0)\n                self.ub = np.clip(self.ub, -5.0, 5.0)\n                self.population = np.random.uniform(self.lb, self.ub, (self.pop_size, self.dim))\n\n                self.fitness = np.array([func(x) for x in self.population])\n                self.f_opt = np.min(self.fitness)\n                self.x_opt = self.population[np.argmin(self.fitness)]\n                self.eval_count += self.pop_size - (self.budget-self.eval_count) if self.eval_count + self.pop_size > self.budget else self.pop_size\n                self.stagnation_counter = 0\n                self.best_fitness_history = [self.f_opt]\n                self.current_scale *= 0.95 #Shrink the scale\n\n            if self.eval_count >= self.budget:\n                break\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 11, "feedback": "", "error": "Evaluation error: module 'numpy' has no attribute 'math'", "parent_ids": ["5b77f9fb-407a-4344-aa0a-a95d9f7e8690", "7f39ad48-10bd-4693-bf48-a08aea65c982"], "operator": "crossover", "metadata": {"llm_response": "# Description: Combines Adaptive Differential Evolution with Bird Swarm Optimization's local search and Cauchy mutation, enhanced with Lvy flight mutation for broader exploration and a shrinking search space.\n# Code: \n```python\nimport numpy as np\n\nclass AdaptiveDELevyBirdSwarm:\n    def __init__(self, budget=10000, dim=10, pop_size=40, F=0.5, CR=0.7, neighborhood_size=5, cauchy_probability=0.1, levy_probability=0.1, initial_scale=5.0):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F  # Mutation factor\n        self.CR = CR  # Crossover rate\n        self.lb = -5.0\n        self.ub = 5.0\n        self.stagnation_counter = 0\n        self.stagnation_threshold = budget // 10\n        self.best_fitness_history = []\n        self.neighborhood_size = neighborhood_size\n        self.cauchy_probability = cauchy_probability\n        self.cauchy_scale = 0.1\n        self.levy_probability = levy_probability\n        self.initial_scale = initial_scale\n        self.current_scale = initial_scale\n\n\n    def levy_flight(self, size, beta=1.5):\n        sigma = (np.math.gamma(1 + beta) * np.sin(np.pi * beta / 2) / (np.math.gamma((1 + beta) / 2) * beta * (2 ** ((beta - 1) / 2)))) ** (1 / beta)\n        u = np.random.normal(0, sigma, size)\n        v = np.random.normal(0, 1, size)\n        step = u / abs(v) ** (1 / beta)\n        return step\n\n    def __call__(self, func):\n        self.population = np.random.uniform(self.lb, self.ub, (self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n        self.eval_count = self.pop_size\n\n        while self.eval_count < self.budget:\n            for i in range(self.pop_size):\n                # Adaptive F and CR\n                adaptive_F = self.F * (1 + 0.1 * np.random.randn())\n                adaptive_CR = self.CR * (1 + 0.1 * np.random.randn())\n                adaptive_F = np.clip(adaptive_F, 0.1, 1.0)\n                adaptive_CR = np.clip(adaptive_CR, 0.1, 1.0)\n\n                # Mutation (DE part)\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = np.random.choice(idxs, 3, replace=False)\n                mutant = self.population[a] + adaptive_F * (self.population[b] - self.population[c])\n                mutant = np.clip(mutant, self.lb, self.ub)\n\n                # Crossover\n                trial = np.copy(self.population[i])\n                mask = np.random.rand(self.dim) < adaptive_CR\n                trial[mask] = mutant[mask]\n                trial = np.clip(trial, self.lb, self.ub)\n\n                # Bird Swarm Inspired Local Search and Cauchy/Levy Mutation\n                if np.random.rand() < 0.5: # Apply local search with 50% probability\n                    # Neighborhood Search\n                    neighbors_indices = np.random.choice(self.pop_size, size=self.neighborhood_size, replace=False)\n                    neighbors = self.population[neighbors_indices]\n                    neighbors_fitness = self.fitness[neighbors_indices]\n                    best_neighbor_index = neighbors_indices[np.argmin(neighbors_fitness)]\n                    best_neighbor = self.population[best_neighbor_index]\n\n                    # Move towards the best neighbor\n                    trial = trial + 0.1 * (best_neighbor - trial)\n                    trial = np.clip(trial, self.lb, self.ub)\n\n                    # Cauchy Mutation\n                    if np.random.rand() < self.cauchy_probability:\n                        mutation = trial + (self.ub - self.lb) * np.random.standard_cauchy(size=self.dim) * self.cauchy_scale\n                        mutation = np.clip(mutation, self.lb, self.ub)\n                        trial = mutation\n\n                    #Levy Flight Mutation\n                    if np.random.rand() < self.levy_probability:\n                        levy_steps = self.levy_flight(self.dim)\n                        mutation = trial + self.current_scale * levy_steps\n                        mutation = np.clip(mutation, self.lb, self.ub)\n                        trial = mutation\n\n\n                # Selection\n                f_trial = func(trial)\n                self.eval_count += 1\n\n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n            # Stagnation Check and Restart or Shrink\n            self.best_fitness_history.append(self.f_opt)\n            if len(self.best_fitness_history) > self.stagnation_threshold:\n                if self.best_fitness_history[-1] >= self.best_fitness_history[-self.stagnation_threshold]:\n                    self.stagnation_counter += 1\n                else:\n                    self.stagnation_counter = 0\n\n            if self.stagnation_counter >= self.stagnation_threshold:\n                # Option 1: Restart\n                #self.population = np.random.uniform(self.lb, self.ub, (self.pop_size, self.dim))\n\n                # Option 2: Shrink search space.\n                self.lb = self.x_opt - self.current_scale\n                self.ub = self.x_opt + self.current_scale\n                self.lb = np.clip(self.lb, -5.0, 5.0)\n                self.ub = np.clip(self.ub, -5.0, 5.0)\n                self.population = np.random.uniform(self.lb, self.ub, (self.pop_size, self.dim))\n\n                self.fitness = np.array([func(x) for x in self.population])\n                self.f_opt = np.min(self.fitness)\n                self.x_opt = self.population[np.argmin(self.fitness)]\n                self.eval_count += self.pop_size - (self.budget-self.eval_count) if self.eval_count + self.pop_size > self.budget else self.pop_size\n                self.stagnation_counter = 0\n                self.best_fitness_history = [self.f_opt]\n                self.current_scale *= 0.95 #Shrink the scale\n\n            if self.eval_count >= self.budget:\n                break\n        return self.f_opt, self.x_opt\n```", "operator": "crossover", "theta_sampled": 0.006929531045498483, "reward": 0.0, "generation": 11}, "task_prompt": ""}
{"id": "581b53de-aa9c-442f-8ea8-83195ee612a1", "fitness": 0.6933217671004159, "name": "AdaptiveDEBirdSwarmEnhanced", "description": "Combines Adaptive Differential Evolution with Bird Swarm Optimization's neighborhood search and Cauchy mutation, enhanced with a self-adaptive learning rate for local search and dynamic parameter adjustments.", "code": "import numpy as np\n\nclass AdaptiveDEBirdSwarmEnhanced:\n    def __init__(self, budget=10000, dim=10, pop_size=40, F=0.5, CR=0.7, neighborhood_size=5, cauchy_probability=0.1, initial_learning_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F  # Mutation factor\n        self.CR = CR  # Crossover rate\n        self.lb = -5.0\n        self.ub = 5.0\n        self.stagnation_counter = 0\n        self.stagnation_threshold = budget // 10\n        self.best_fitness_history = []\n        self.neighborhood_size = neighborhood_size\n        self.cauchy_probability = cauchy_probability\n        self.cauchy_scale = 0.1\n        self.learning_rate = initial_learning_rate  # Initial learning rate for local search\n        self.learning_rate_decay = 0.995 #Decay the learning rate slowly\n        self.min_learning_rate = 0.01\n\n    def __call__(self, func):\n        self.population = np.random.uniform(self.lb, self.ub, (self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n        self.eval_count = self.pop_size\n\n        while self.eval_count < self.budget:\n            for i in range(self.pop_size):\n                # Adaptive F and CR\n                adaptive_F = self.F * (1 + 0.1 * np.random.randn())\n                adaptive_CR = self.CR * (1 + 0.1 * np.random.randn())\n                adaptive_F = np.clip(adaptive_F, 0.1, 1.0)\n                adaptive_CR = np.clip(adaptive_CR, 0.1, 1.0)\n\n                # Mutation (DE part)\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = np.random.choice(idxs, 3, replace=False)\n                mutant = self.population[a] + adaptive_F * (self.population[b] - self.population[c])\n                mutant = np.clip(mutant, self.lb, self.ub)\n\n                # Crossover\n                trial = np.copy(self.population[i])\n                mask = np.random.rand(self.dim) < adaptive_CR\n                trial[mask] = mutant[mask]\n                trial = np.clip(trial, self.lb, self.ub)\n\n                # Bird Swarm Inspired Local Search and Cauchy Mutation\n                if np.random.rand() < 0.5: # Apply local search with 50% probability\n                    # Neighborhood Search\n                    neighbors_indices = np.random.choice(self.pop_size, size=self.neighborhood_size, replace=False)\n                    neighbors = self.population[neighbors_indices]\n                    neighbors_fitness = self.fitness[neighbors_indices]\n                    best_neighbor_index = neighbors_indices[np.argmin(neighbors_fitness)]\n                    best_neighbor = self.population[best_neighbor_index]\n\n                    # Move towards the best neighbor with adaptive learning rate\n                    trial = trial + self.learning_rate * (best_neighbor - trial)\n                    trial = np.clip(trial, self.lb, self.ub)\n\n                    # Cauchy Mutation\n                    if np.random.rand() < self.cauchy_probability:\n                        mutation = trial + (self.ub - self.lb) * np.random.standard_cauchy(size=self.dim) * self.cauchy_scale\n                        mutation = np.clip(mutation, self.lb, self.ub)\n                        trial = mutation\n\n                # Selection\n                f_trial = func(trial)\n                self.eval_count += 1\n\n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n            # Stagnation Check and Restart\n            self.best_fitness_history.append(self.f_opt)\n            if len(self.best_fitness_history) > self.stagnation_threshold:\n                if self.best_fitness_history[-1] >= self.best_fitness_history[-self.stagnation_threshold]:\n                    self.stagnation_counter += 1\n                else:\n                    self.stagnation_counter = 0\n\n            if self.stagnation_counter >= self.stagnation_threshold:\n                #Restart the population\n                self.population = np.random.uniform(self.lb, self.ub, (self.pop_size, self.dim))\n                self.fitness = np.array([func(x) for x in self.population])\n                self.f_opt = np.min(self.fitness)\n                self.x_opt = self.population[np.argmin(self.fitness)]\n                self.eval_count += self.pop_size - (self.budget-self.eval_count) if self.eval_count + self.pop_size > self.budget else self.pop_size\n                self.stagnation_counter = 0\n                self.best_fitness_history = [self.f_opt]\n                self.learning_rate = 0.1 # Re-initialize learning rate upon restart\n\n            #Learning rate decay\n            self.learning_rate = max(self.learning_rate * self.learning_rate_decay, self.min_learning_rate)\n\n            if self.eval_count >= self.budget:\n                break\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 11, "feedback": "The algorithm AdaptiveDEBirdSwarmEnhanced scored 0.693 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["5b77f9fb-407a-4344-aa0a-a95d9f7e8690", "7f39ad48-10bd-4693-bf48-a08aea65c982"], "operator": "crossover", "metadata": {"llm_response": "# Description: Combines Adaptive Differential Evolution with Bird Swarm Optimization's neighborhood search and Cauchy mutation, enhanced with a self-adaptive learning rate for local search and dynamic parameter adjustments.\n# Code: \n```python\nimport numpy as np\n\nclass AdaptiveDEBirdSwarmEnhanced:\n    def __init__(self, budget=10000, dim=10, pop_size=40, F=0.5, CR=0.7, neighborhood_size=5, cauchy_probability=0.1, initial_learning_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F  # Mutation factor\n        self.CR = CR  # Crossover rate\n        self.lb = -5.0\n        self.ub = 5.0\n        self.stagnation_counter = 0\n        self.stagnation_threshold = budget // 10\n        self.best_fitness_history = []\n        self.neighborhood_size = neighborhood_size\n        self.cauchy_probability = cauchy_probability\n        self.cauchy_scale = 0.1\n        self.learning_rate = initial_learning_rate  # Initial learning rate for local search\n        self.learning_rate_decay = 0.995 #Decay the learning rate slowly\n        self.min_learning_rate = 0.01\n\n    def __call__(self, func):\n        self.population = np.random.uniform(self.lb, self.ub, (self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n        self.eval_count = self.pop_size\n\n        while self.eval_count < self.budget:\n            for i in range(self.pop_size):\n                # Adaptive F and CR\n                adaptive_F = self.F * (1 + 0.1 * np.random.randn())\n                adaptive_CR = self.CR * (1 + 0.1 * np.random.randn())\n                adaptive_F = np.clip(adaptive_F, 0.1, 1.0)\n                adaptive_CR = np.clip(adaptive_CR, 0.1, 1.0)\n\n                # Mutation (DE part)\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = np.random.choice(idxs, 3, replace=False)\n                mutant = self.population[a] + adaptive_F * (self.population[b] - self.population[c])\n                mutant = np.clip(mutant, self.lb, self.ub)\n\n                # Crossover\n                trial = np.copy(self.population[i])\n                mask = np.random.rand(self.dim) < adaptive_CR\n                trial[mask] = mutant[mask]\n                trial = np.clip(trial, self.lb, self.ub)\n\n                # Bird Swarm Inspired Local Search and Cauchy Mutation\n                if np.random.rand() < 0.5: # Apply local search with 50% probability\n                    # Neighborhood Search\n                    neighbors_indices = np.random.choice(self.pop_size, size=self.neighborhood_size, replace=False)\n                    neighbors = self.population[neighbors_indices]\n                    neighbors_fitness = self.fitness[neighbors_indices]\n                    best_neighbor_index = neighbors_indices[np.argmin(neighbors_fitness)]\n                    best_neighbor = self.population[best_neighbor_index]\n\n                    # Move towards the best neighbor with adaptive learning rate\n                    trial = trial + self.learning_rate * (best_neighbor - trial)\n                    trial = np.clip(trial, self.lb, self.ub)\n\n                    # Cauchy Mutation\n                    if np.random.rand() < self.cauchy_probability:\n                        mutation = trial + (self.ub - self.lb) * np.random.standard_cauchy(size=self.dim) * self.cauchy_scale\n                        mutation = np.clip(mutation, self.lb, self.ub)\n                        trial = mutation\n\n                # Selection\n                f_trial = func(trial)\n                self.eval_count += 1\n\n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n            # Stagnation Check and Restart\n            self.best_fitness_history.append(self.f_opt)\n            if len(self.best_fitness_history) > self.stagnation_threshold:\n                if self.best_fitness_history[-1] >= self.best_fitness_history[-self.stagnation_threshold]:\n                    self.stagnation_counter += 1\n                else:\n                    self.stagnation_counter = 0\n\n            if self.stagnation_counter >= self.stagnation_threshold:\n                #Restart the population\n                self.population = np.random.uniform(self.lb, self.ub, (self.pop_size, self.dim))\n                self.fitness = np.array([func(x) for x in self.population])\n                self.f_opt = np.min(self.fitness)\n                self.x_opt = self.population[np.argmin(self.fitness)]\n                self.eval_count += self.pop_size - (self.budget-self.eval_count) if self.eval_count + self.pop_size > self.budget else self.pop_size\n                self.stagnation_counter = 0\n                self.best_fitness_history = [self.f_opt]\n                self.learning_rate = 0.1 # Re-initialize learning rate upon restart\n\n            #Learning rate decay\n            self.learning_rate = max(self.learning_rate * self.learning_rate_decay, self.min_learning_rate)\n\n            if self.eval_count >= self.budget:\n                break\n        return self.f_opt, self.x_opt\n```", "aucs": [0.2776209042163622, 0.5894587057927025, 0.628505171641873, 0.9033784062554172, 0.8047319381615463, 0.8337585879364601, 0.7207005312756325, 0.6830553005559075, 0.788423352331193, 0.7270712322823163, 0.8594921805539573, 0.9951172795994613, 0.41984162451556695, 0.7260344217026689, 0.8996908460641417, 0.8268594232207089, 0.5958019498275895, 0.858533776424424, 0.2291154562339618, 0.49924425341642686], "operator": "crossover", "theta_sampled": 0.0026433033324574457, "reward": 0.0, "generation": 11}, "task_prompt": ""}
{"id": "efd7a0bb-8eac-4ecc-97f1-ecbac924f8dd", "fitness": 0.0, "name": "LevyFlocking", "description": "Population-based algorithm using Lvy flights for exploration and a velocity-based attraction mechanism inspired by flocking behavior for exploitation.", "code": "import numpy as np\n\nclass LevyFlocking:\n    def __init__(self, budget=10000, dim=10, pop_size=20, levy_exponent=1.5, attraction_strength=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.levy_exponent = levy_exponent\n        self.attraction_strength = attraction_strength\n\n    def levy_flight(self, size):\n        sigma = (np.math.gamma(1 + self.levy_exponent) * np.sin(np.pi * self.levy_exponent / 2) / (np.math.gamma((1 + self.levy_exponent) / 2) * self.levy_exponent * 2**((self.levy_exponent - 1) / 2)))**(1 / self.levy_exponent)\n        u = np.random.normal(0, sigma, size=size)\n        v = np.random.normal(0, 1, size=size)\n        step = u / (np.abs(v)**(1 / self.levy_exponent))\n        return step\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        \n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        # Find initial best\n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index]\n        \n        velocities = np.zeros((self.pop_size, self.dim)) #Initialize velocities\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Lvy flight for exploration\n                levy_steps = self.levy_flight(self.dim)\n                new_position = population[i] + 0.01 * levy_steps * (func.bounds.ub - func.bounds.lb) # Scale the Lvy flight\n\n                # Flocking-inspired attraction to the best solution\n                direction_to_best = self.x_opt - population[i]\n                velocities[i] = velocities[i] + self.attraction_strength * direction_to_best\n\n                new_position = new_position + velocities[i]\n\n                # Clip to bounds\n                new_position = np.clip(new_position, func.bounds.lb, func.bounds.ub)\n\n                # Evaluate new position (if budget allows)\n                f = func(new_position)\n                self.budget -= 1\n\n                if f < fitness[i]:\n                    fitness[i] = f\n                    population[i] = new_position\n                    \n                    # Update global best\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = new_position\n\n                if self.budget <= 0:\n                    break\n                    \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 11, "feedback": "", "error": "Evaluation error: module 'numpy' has no attribute 'math'", "parent_ids": [], "operator": "random_new", "metadata": {"llm_response": "# Description: Population-based algorithm using Lvy flights for exploration and a velocity-based attraction mechanism inspired by flocking behavior for exploitation.\n\n# Code:\n```python\nimport numpy as np\n\nclass LevyFlocking:\n    def __init__(self, budget=10000, dim=10, pop_size=20, levy_exponent=1.5, attraction_strength=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.levy_exponent = levy_exponent\n        self.attraction_strength = attraction_strength\n\n    def levy_flight(self, size):\n        sigma = (np.math.gamma(1 + self.levy_exponent) * np.sin(np.pi * self.levy_exponent / 2) / (np.math.gamma((1 + self.levy_exponent) / 2) * self.levy_exponent * 2**((self.levy_exponent - 1) / 2)))**(1 / self.levy_exponent)\n        u = np.random.normal(0, sigma, size=size)\n        v = np.random.normal(0, 1, size=size)\n        step = u / (np.abs(v)**(1 / self.levy_exponent))\n        return step\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        \n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.budget -= self.pop_size\n\n        # Find initial best\n        best_index = np.argmin(fitness)\n        self.f_opt = fitness[best_index]\n        self.x_opt = population[best_index]\n        \n        velocities = np.zeros((self.pop_size, self.dim)) #Initialize velocities\n\n        while self.budget > 0:\n            for i in range(self.pop_size):\n                # Lvy flight for exploration\n                levy_steps = self.levy_flight(self.dim)\n                new_position = population[i] + 0.01 * levy_steps * (func.bounds.ub - func.bounds.lb) # Scale the Lvy flight\n\n                # Flocking-inspired attraction to the best solution\n                direction_to_best = self.x_opt - population[i]\n                velocities[i] = velocities[i] + self.attraction_strength * direction_to_best\n\n                new_position = new_position + velocities[i]\n\n                # Clip to bounds\n                new_position = np.clip(new_position, func.bounds.lb, func.bounds.ub)\n\n                # Evaluate new position (if budget allows)\n                f = func(new_position)\n                self.budget -= 1\n\n                if f < fitness[i]:\n                    fitness[i] = f\n                    population[i] = new_position\n                    \n                    # Update global best\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = new_position\n\n                if self.budget <= 0:\n                    break\n                    \n        return self.f_opt, self.x_opt\n```", "operator": "random_new", "theta_sampled": 0.006574198139169545, "reward": 0.0, "generation": 11}, "task_prompt": ""}
{"id": "96fb1b31-d724-4f19-a213-934f8bafc5fb", "fitness": 0.702822587726373, "name": "AdaptiveHybridDEBirdSwarmSelfAdaptivePop", "description": "Adaptive Differential Evolution with Bird Swarm Optimization, archive and a self-adaptive mechanism for population size.", "code": "import numpy as np\n\nclass AdaptiveHybridDEBirdSwarmSelfAdaptivePop:\n    def __init__(self, budget=10000, dim=10, pop_size_initial=40, F=0.5, CR=0.7, neighborhood_size=5, cauchy_probability=0.1, archive_size=10, pop_size_adapt_prob=0.1, pop_size_reduction_factor=0.9, pop_size_increase_factor=1.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size_initial\n        self.pop_size_initial = pop_size_initial\n        self.F = F  # Mutation factor\n        self.CR = CR  # Crossover rate\n        self.lb = -5.0\n        self.ub = 5.0\n        self.stagnation_counter = 0\n        self.stagnation_threshold = budget // 10\n        self.best_fitness_history = []\n        self.neighborhood_size = neighborhood_size\n        self.cauchy_probability = cauchy_probability\n        self.cauchy_scale = 0.1\n        self.archive_size = archive_size\n        self.archive = []\n        self.archive_fitness = []\n        self.eval_count = 0  # Initialize eval_count here\n        self.pop_size_adapt_prob = pop_size_adapt_prob\n        self.pop_size_reduction_factor = pop_size_reduction_factor\n        self.pop_size_increase_factor = pop_size_increase_factor\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(self.lb, self.ub, (self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.eval_count += self.pop_size\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n\n        # Initialize Archive\n        self.archive = []\n        self.archive_fitness = []\n        for i in range(min(self.archive_size, self.pop_size)):\n            self.archive.append(self.population[i].copy())\n            self.archive_fitness.append(self.fitness[i])\n\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.eval_count < self.budget:\n            for i in range(self.pop_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                # Adaptive F and CR\n                adaptive_F = self.F * (1 + 0.1 * np.random.randn())\n                adaptive_CR = self.CR * (1 + 0.1 * np.random.randn())\n                adaptive_F = np.clip(adaptive_F, 0.1, 1.0)\n                adaptive_CR = np.clip(adaptive_CR, 0.1, 1.0)\n\n                # Mutation (DE part)\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = np.random.choice(idxs, 3, replace=False)\n                mutant = self.population[a] + adaptive_F * (self.population[b] - self.population[c])\n                mutant = np.clip(mutant, self.lb, self.ub)\n\n                # Crossover\n                trial = np.copy(self.population[i])\n                mask = np.random.rand(self.dim) < adaptive_CR\n                trial[mask] = mutant[mask]\n                trial = np.clip(trial, self.lb, self.ub)\n\n                # Bird Swarm Inspired Local Search and Cauchy Mutation\n                if np.random.rand() < 0.5: # Apply local search with 50% probability\n                    # Neighborhood Search\n                    neighbors_indices = np.random.choice(self.pop_size, size=self.neighborhood_size, replace=False)\n                    neighbors = self.population[neighbors_indices]\n                    neighbors_fitness = self.fitness[neighbors_indices]\n                    best_neighbor_index = neighbors_indices[np.argmin(neighbors_fitness)]\n                    best_neighbor = self.population[best_neighbor_index]\n\n                    # Move towards the best neighbor\n                    trial = trial + 0.1 * (best_neighbor - trial)\n                    trial = np.clip(trial, self.lb, self.ub)\n\n                    # Cauchy Mutation\n                    if np.random.rand() < self.cauchy_probability:\n                        mutation = trial + (self.ub - self.lb) * np.random.standard_cauchy(size=self.dim) * self.cauchy_scale\n                        mutation = np.clip(mutation, self.lb, self.ub)\n                        trial = mutation\n\n                # Selection\n                f_trial = func(trial)\n                self.eval_count += 1\n\n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                \n                # Update Archive\n                if f_trial < np.max(self.archive_fitness):\n                    worst_index = np.argmax(self.archive_fitness)\n                    self.archive[worst_index] = trial.copy()\n                    self.archive_fitness[worst_index] = f_trial\n\n            # Stagnation Check and Restart\n            self.best_fitness_history.append(self.f_opt)\n            if len(self.best_fitness_history) > self.stagnation_threshold:\n                if self.best_fitness_history[-1] >= self.best_fitness_history[-self.stagnation_threshold]:\n                    self.stagnation_counter += 1\n                else:\n                    self.stagnation_counter = 0\n\n            if self.stagnation_counter >= self.stagnation_threshold:\n                #Restart the population, partially from archive\n                num_archive = min(self.pop_size // 2, len(self.archive))\n                new_pop = np.random.uniform(self.lb, self.ub, (self.pop_size - num_archive, self.dim))\n                \n                # Sort archive by fitness\n                sorted_indices = np.argsort(self.archive_fitness)\n                archive_subset = [self.archive[i] for i in sorted_indices[:num_archive]]\n                \n                self.population = np.concatenate((new_pop, np.array(archive_subset)), axis=0)\n                self.fitness = np.array([func(x) for x in self.population])\n                self.eval_count += self.pop_size - (self.budget-self.eval_count) if self.eval_count + self.pop_size > self.budget else self.pop_size\n                self.stagnation_counter = 0\n                self.best_fitness_history = [self.f_opt]\n                #Re-initialize Archive\n                self.archive = []\n                self.archive_fitness = []\n                for i in range(min(self.archive_size, self.pop_size)):\n                    self.archive.append(self.population[i].copy())\n                    self.archive_fitness.append(self.fitness[i])\n\n            # Population size adaptation\n            if np.random.rand() < self.pop_size_adapt_prob:\n                if np.random.rand() < 0.5:  # Reduce population size\n                    new_pop_size = int(self.pop_size * self.pop_size_reduction_factor)\n                    new_pop_size = max(10, new_pop_size)  # Ensure minimum pop size\n                    if new_pop_size < self.pop_size:\n                        # Keep the best individuals\n                        sorted_indices = np.argsort(self.fitness)\n                        self.population = self.population[sorted_indices[:new_pop_size]]\n                        self.fitness = self.fitness[sorted_indices[:new_pop_size]]\n                        self.pop_size = new_pop_size\n                        # Refill archive\n                        self.archive = []\n                        self.archive_fitness = []\n                        for i in range(min(self.archive_size, self.pop_size)):\n                            self.archive.append(self.population[i].copy())\n                            self.archive_fitness.append(self.fitness[i])\n                else:  # Increase population size\n                    new_pop_size = int(self.pop_size * self.pop_size_increase_factor)\n                    if self.eval_count + (new_pop_size - self.pop_size) <= self.budget:\n                         new_individuals = np.random.uniform(self.lb, self.ub, (new_pop_size - self.pop_size, self.dim))\n                         new_fitness = np.array([func(x) for x in new_individuals])\n                         self.eval_count += new_pop_size - self.pop_size\n\n                         self.population = np.concatenate((self.population, new_individuals), axis=0)\n                         self.fitness = np.concatenate((self.fitness, new_fitness))\n                         self.pop_size = new_pop_size\n                         #Refill Archive\n                         self.archive = []\n                         self.archive_fitness = []\n                         for i in range(min(self.archive_size, self.pop_size)):\n                             self.archive.append(self.population[i].copy())\n                             self.archive_fitness.append(self.fitness[i])\n                         \n                         self.f_opt = np.min(self.fitness)\n                         self.x_opt = self.population[np.argmin(self.fitness)]\n\n            if self.eval_count >= self.budget:\n                break\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 12, "feedback": "The algorithm AdaptiveHybridDEBirdSwarmSelfAdaptivePop scored 0.703 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["d25d99b5-3750-48d8-b413-d596dbc36df9"], "operator": "mutation", "metadata": {"llm_response": "# Description: Adaptive Differential Evolution with Bird Swarm Optimization, archive and a self-adaptive mechanism for population size.\n# Code:\n```python\nimport numpy as np\n\nclass AdaptiveHybridDEBirdSwarmSelfAdaptivePop:\n    def __init__(self, budget=10000, dim=10, pop_size_initial=40, F=0.5, CR=0.7, neighborhood_size=5, cauchy_probability=0.1, archive_size=10, pop_size_adapt_prob=0.1, pop_size_reduction_factor=0.9, pop_size_increase_factor=1.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size_initial\n        self.pop_size_initial = pop_size_initial\n        self.F = F  # Mutation factor\n        self.CR = CR  # Crossover rate\n        self.lb = -5.0\n        self.ub = 5.0\n        self.stagnation_counter = 0\n        self.stagnation_threshold = budget // 10\n        self.best_fitness_history = []\n        self.neighborhood_size = neighborhood_size\n        self.cauchy_probability = cauchy_probability\n        self.cauchy_scale = 0.1\n        self.archive_size = archive_size\n        self.archive = []\n        self.archive_fitness = []\n        self.eval_count = 0  # Initialize eval_count here\n        self.pop_size_adapt_prob = pop_size_adapt_prob\n        self.pop_size_reduction_factor = pop_size_reduction_factor\n        self.pop_size_increase_factor = pop_size_increase_factor\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(self.lb, self.ub, (self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.eval_count += self.pop_size\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n\n        # Initialize Archive\n        self.archive = []\n        self.archive_fitness = []\n        for i in range(min(self.archive_size, self.pop_size)):\n            self.archive.append(self.population[i].copy())\n            self.archive_fitness.append(self.fitness[i])\n\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.eval_count < self.budget:\n            for i in range(self.pop_size):\n                if self.eval_count >= self.budget:\n                    break\n\n                # Adaptive F and CR\n                adaptive_F = self.F * (1 + 0.1 * np.random.randn())\n                adaptive_CR = self.CR * (1 + 0.1 * np.random.randn())\n                adaptive_F = np.clip(adaptive_F, 0.1, 1.0)\n                adaptive_CR = np.clip(adaptive_CR, 0.1, 1.0)\n\n                # Mutation (DE part)\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = np.random.choice(idxs, 3, replace=False)\n                mutant = self.population[a] + adaptive_F * (self.population[b] - self.population[c])\n                mutant = np.clip(mutant, self.lb, self.ub)\n\n                # Crossover\n                trial = np.copy(self.population[i])\n                mask = np.random.rand(self.dim) < adaptive_CR\n                trial[mask] = mutant[mask]\n                trial = np.clip(trial, self.lb, self.ub)\n\n                # Bird Swarm Inspired Local Search and Cauchy Mutation\n                if np.random.rand() < 0.5: # Apply local search with 50% probability\n                    # Neighborhood Search\n                    neighbors_indices = np.random.choice(self.pop_size, size=self.neighborhood_size, replace=False)\n                    neighbors = self.population[neighbors_indices]\n                    neighbors_fitness = self.fitness[neighbors_indices]\n                    best_neighbor_index = neighbors_indices[np.argmin(neighbors_fitness)]\n                    best_neighbor = self.population[best_neighbor_index]\n\n                    # Move towards the best neighbor\n                    trial = trial + 0.1 * (best_neighbor - trial)\n                    trial = np.clip(trial, self.lb, self.ub)\n\n                    # Cauchy Mutation\n                    if np.random.rand() < self.cauchy_probability:\n                        mutation = trial + (self.ub - self.lb) * np.random.standard_cauchy(size=self.dim) * self.cauchy_scale\n                        mutation = np.clip(mutation, self.lb, self.ub)\n                        trial = mutation\n\n                # Selection\n                f_trial = func(trial)\n                self.eval_count += 1\n\n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                \n                # Update Archive\n                if f_trial < np.max(self.archive_fitness):\n                    worst_index = np.argmax(self.archive_fitness)\n                    self.archive[worst_index] = trial.copy()\n                    self.archive_fitness[worst_index] = f_trial\n\n            # Stagnation Check and Restart\n            self.best_fitness_history.append(self.f_opt)\n            if len(self.best_fitness_history) > self.stagnation_threshold:\n                if self.best_fitness_history[-1] >= self.best_fitness_history[-self.stagnation_threshold]:\n                    self.stagnation_counter += 1\n                else:\n                    self.stagnation_counter = 0\n\n            if self.stagnation_counter >= self.stagnation_threshold:\n                #Restart the population, partially from archive\n                num_archive = min(self.pop_size // 2, len(self.archive))\n                new_pop = np.random.uniform(self.lb, self.ub, (self.pop_size - num_archive, self.dim))\n                \n                # Sort archive by fitness\n                sorted_indices = np.argsort(self.archive_fitness)\n                archive_subset = [self.archive[i] for i in sorted_indices[:num_archive]]\n                \n                self.population = np.concatenate((new_pop, np.array(archive_subset)), axis=0)\n                self.fitness = np.array([func(x) for x in self.population])\n                self.eval_count += self.pop_size - (self.budget-self.eval_count) if self.eval_count + self.pop_size > self.budget else self.pop_size\n                self.stagnation_counter = 0\n                self.best_fitness_history = [self.f_opt]\n                #Re-initialize Archive\n                self.archive = []\n                self.archive_fitness = []\n                for i in range(min(self.archive_size, self.pop_size)):\n                    self.archive.append(self.population[i].copy())\n                    self.archive_fitness.append(self.fitness[i])\n\n            # Population size adaptation\n            if np.random.rand() < self.pop_size_adapt_prob:\n                if np.random.rand() < 0.5:  # Reduce population size\n                    new_pop_size = int(self.pop_size * self.pop_size_reduction_factor)\n                    new_pop_size = max(10, new_pop_size)  # Ensure minimum pop size\n                    if new_pop_size < self.pop_size:\n                        # Keep the best individuals\n                        sorted_indices = np.argsort(self.fitness)\n                        self.population = self.population[sorted_indices[:new_pop_size]]\n                        self.fitness = self.fitness[sorted_indices[:new_pop_size]]\n                        self.pop_size = new_pop_size\n                        # Refill archive\n                        self.archive = []\n                        self.archive_fitness = []\n                        for i in range(min(self.archive_size, self.pop_size)):\n                            self.archive.append(self.population[i].copy())\n                            self.archive_fitness.append(self.fitness[i])\n                else:  # Increase population size\n                    new_pop_size = int(self.pop_size * self.pop_size_increase_factor)\n                    if self.eval_count + (new_pop_size - self.pop_size) <= self.budget:\n                         new_individuals = np.random.uniform(self.lb, self.ub, (new_pop_size - self.pop_size, self.dim))\n                         new_fitness = np.array([func(x) for x in new_individuals])\n                         self.eval_count += new_pop_size - self.pop_size\n\n                         self.population = np.concatenate((self.population, new_individuals), axis=0)\n                         self.fitness = np.concatenate((self.fitness, new_fitness))\n                         self.pop_size = new_pop_size\n                         #Refill Archive\n                         self.archive = []\n                         self.archive_fitness = []\n                         for i in range(min(self.archive_size, self.pop_size)):\n                             self.archive.append(self.population[i].copy())\n                             self.archive_fitness.append(self.fitness[i])\n                         \n                         self.f_opt = np.min(self.fitness)\n                         self.x_opt = self.population[np.argmin(self.fitness)]\n\n            if self.eval_count >= self.budget:\n                break\n        return self.f_opt, self.x_opt\n```", "aucs": [0.28631840166514944, 0.6118464278064464, 0.7396404972657464, 0.8807269605618464, 0.7990903553335086, 0.7978547666657876, 0.6672712628370905, 0.8090558508486027, 0.7691932751639976, 0.7668008494348234, 0.8729706636437902, 0.9933207049847715, 0.3504216399891037, 0.7945319280747901, 0.8101114718130867, 0.8277491983607655, 0.6773084767011175, 0.8432983116212373, 0.2361598592663312, 0.5227808524894652], "operator": "mutation", "theta_sampled": 0.002197269166679549, "reward": 0.0, "generation": 12}, "task_prompt": ""}
{"id": "7c4bdc24-b3ec-4d88-8667-6bf7e5a1b544", "fitness": 0.6622302503460953, "name": "AdaptiveHybridDEBirdSwarmPlusPlus", "description": "Combines adaptive differential evolution, bird swarm optimization, Cauchy mutation, and success-history based adaptation of F/CR with dynamic population size adjustment.", "code": "import numpy as np\n\nclass AdaptiveHybridDEBirdSwarmPlusPlus:\n    def __init__(self, budget=10000, dim=10, pop_size=40, F=0.5, CR=0.7, neighborhood_size=5, cauchy_probability=0.1, success_history_size=10, stagnation_threshold_factor=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = int(np.clip(pop_size, 10, 100))  # Constrain pop_size\n        self.F = F  # Mutation factor\n        self.CR = CR  # Crossover rate\n        self.lb = -5.0\n        self.ub = 5.0\n        self.stagnation_counter = 0\n        self.stagnation_threshold = int(budget * stagnation_threshold_factor)\n        self.best_fitness_history = []\n        self.neighborhood_size = neighborhood_size\n        self.cauchy_probability = cauchy_probability\n        self.cauchy_scale = 0.1\n        self.success_history_size = success_history_size\n        self.success_f = []\n        self.success_cr = []\n        self.min_pop_size = 10\n        self.max_pop_size = 100\n        self.pop_resize_interval = budget // 20\n        self.global_best_fitness = np.inf\n        self.global_best_position = None\n        self.exploration_probability = 0.2\n\n    def __call__(self, func):\n        self.population = np.random.uniform(self.lb, self.ub, (self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n        self.eval_count = self.pop_size\n        self.generation = 0\n\n        if self.f_opt < self.global_best_fitness:\n            self.global_best_fitness = self.f_opt\n            self.global_best_position = self.x_opt.copy()\n\n        while self.eval_count < self.budget:\n            self.generation += 1\n\n            for i in range(self.pop_size):\n                # Adaptive F and CR using success history\n                if self.success_f and self.success_cr:\n                    adaptive_F = np.random.choice(self.success_f)\n                    adaptive_CR = np.random.choice(self.success_cr)\n                else:\n                    adaptive_F = self.F * (1 + 0.1 * np.random.randn())\n                    adaptive_CR = self.CR * (1 + 0.1 * np.random.randn())\n\n                adaptive_F = np.clip(adaptive_F, 0.1, 1.0)\n                adaptive_CR = np.clip(adaptive_CR, 0.1, 1.0)\n\n                # Mutation (DE part)\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = np.random.choice(idxs, 3, replace=False)\n                mutant = self.population[a] + adaptive_F * (self.population[b] - self.population[c])\n                mutant = np.clip(mutant, self.lb, self.ub)\n\n                # Crossover\n                trial = np.copy(self.population[i])\n                mask = np.random.rand(self.dim) < adaptive_CR\n                trial[mask] = mutant[mask]\n                trial = np.clip(trial, self.lb, self.ub)\n\n                # Bird Swarm Inspired Local Search and Cauchy Mutation\n                if np.random.rand() < 0.5: # Apply local search with 50% probability\n                    # Neighborhood Search\n                    neighbors_indices = np.random.choice(self.pop_size, size=self.neighborhood_size, replace=False)\n                    neighbors = self.population[neighbors_indices]\n                    neighbors_fitness = self.fitness[neighbors_indices]\n                    best_neighbor_index = neighbors_indices[np.argmin(neighbors_fitness)]\n                    best_neighbor = self.population[best_neighbor_index]\n\n                    # Move towards the best neighbor, but also consider global best\n                    if np.random.rand() < 0.5:\n                      trial = trial + 0.1 * (best_neighbor - trial) # Local exploitation\n                    else:\n                      trial = trial + 0.1 * (self.global_best_position - trial) # Global exploitation\n\n                    trial = np.clip(trial, self.lb, self.ub)\n\n                    # Cauchy Mutation\n                    if np.random.rand() < self.cauchy_probability:\n                        mutation = trial + (self.ub - self.lb) * np.random.standard_cauchy(size=self.dim) * self.cauchy_scale\n                        mutation = np.clip(mutation, self.lb, self.ub)\n                        trial = mutation\n\n                # Exploration: Random jump with probability exploration_probability\n                if np.random.rand() < self.exploration_probability:\n                    trial = np.random.uniform(self.lb, self.ub, self.dim)\n\n\n                # Selection\n                f_trial = func(trial)\n                self.eval_count += 1\n\n                if f_trial < self.fitness[i]:\n                    # Update success history\n                    self.success_f.append(adaptive_F)\n                    self.success_cr.append(adaptive_CR)\n                    if len(self.success_f) > self.success_history_size:\n                        self.success_f.pop(0)\n                        self.success_cr.pop(0)\n\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n                        if f_trial < self.global_best_fitness:\n                            self.global_best_fitness = f_trial\n                            self.global_best_position = trial.copy()\n\n\n\n            # Stagnation Check and Restart/Population Size Adjustment\n            self.best_fitness_history.append(self.f_opt)\n            if len(self.best_fitness_history) > self.stagnation_threshold:\n                if self.best_fitness_history[-1] >= self.best_fitness_history[-self.stagnation_threshold]:\n                    self.stagnation_counter += 1\n                else:\n                    self.stagnation_counter = 0\n\n            if self.stagnation_counter >= self.stagnation_threshold:\n                #Restart the population with possible population size adjustment\n                if self.generation % self.pop_resize_interval == 0:\n                    if np.random.rand() < 0.5: #shrink population\n                        self.pop_size = int(max(self.pop_size * 0.9, self.min_pop_size))\n                    else: #grow population\n                        self.pop_size = int(min(self.pop_size * 1.1, self.max_pop_size))\n\n                self.population = np.random.uniform(self.lb, self.ub, (self.pop_size, self.dim))\n                self.fitness = np.array([func(x) for x in self.population])\n                self.f_opt = np.min(self.fitness)\n                self.x_opt = self.population[np.argmin(self.fitness)]\n\n                if self.f_opt < self.global_best_fitness:\n                    self.global_best_fitness = self.f_opt\n                    self.global_best_position = self.x_opt.copy()\n\n                self.eval_count += self.pop_size - (self.budget-self.eval_count) if self.eval_count + self.pop_size > self.budget else self.pop_size\n                self.stagnation_counter = 0\n                self.best_fitness_history = [self.f_opt]\n\n            if self.eval_count >= self.budget:\n                break\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 12, "feedback": "The algorithm AdaptiveHybridDEBirdSwarmPlusPlus scored 0.662 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["5b77f9fb-407a-4344-aa0a-a95d9f7e8690", "3b387073-58a6-4ee0-9faf-ba6fbc4ee604"], "operator": "crossover", "metadata": {"llm_response": "# Description: Combines adaptive differential evolution, bird swarm optimization, Cauchy mutation, and success-history based adaptation of F/CR with dynamic population size adjustment.\n# Code: \n```python\nimport numpy as np\n\nclass AdaptiveHybridDEBirdSwarmPlusPlus:\n    def __init__(self, budget=10000, dim=10, pop_size=40, F=0.5, CR=0.7, neighborhood_size=5, cauchy_probability=0.1, success_history_size=10, stagnation_threshold_factor=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = int(np.clip(pop_size, 10, 100))  # Constrain pop_size\n        self.F = F  # Mutation factor\n        self.CR = CR  # Crossover rate\n        self.lb = -5.0\n        self.ub = 5.0\n        self.stagnation_counter = 0\n        self.stagnation_threshold = int(budget * stagnation_threshold_factor)\n        self.best_fitness_history = []\n        self.neighborhood_size = neighborhood_size\n        self.cauchy_probability = cauchy_probability\n        self.cauchy_scale = 0.1\n        self.success_history_size = success_history_size\n        self.success_f = []\n        self.success_cr = []\n        self.min_pop_size = 10\n        self.max_pop_size = 100\n        self.pop_resize_interval = budget // 20\n        self.global_best_fitness = np.inf\n        self.global_best_position = None\n        self.exploration_probability = 0.2\n\n    def __call__(self, func):\n        self.population = np.random.uniform(self.lb, self.ub, (self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n        self.eval_count = self.pop_size\n        self.generation = 0\n\n        if self.f_opt < self.global_best_fitness:\n            self.global_best_fitness = self.f_opt\n            self.global_best_position = self.x_opt.copy()\n\n        while self.eval_count < self.budget:\n            self.generation += 1\n\n            for i in range(self.pop_size):\n                # Adaptive F and CR using success history\n                if self.success_f and self.success_cr:\n                    adaptive_F = np.random.choice(self.success_f)\n                    adaptive_CR = np.random.choice(self.success_cr)\n                else:\n                    adaptive_F = self.F * (1 + 0.1 * np.random.randn())\n                    adaptive_CR = self.CR * (1 + 0.1 * np.random.randn())\n\n                adaptive_F = np.clip(adaptive_F, 0.1, 1.0)\n                adaptive_CR = np.clip(adaptive_CR, 0.1, 1.0)\n\n                # Mutation (DE part)\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = np.random.choice(idxs, 3, replace=False)\n                mutant = self.population[a] + adaptive_F * (self.population[b] - self.population[c])\n                mutant = np.clip(mutant, self.lb, self.ub)\n\n                # Crossover\n                trial = np.copy(self.population[i])\n                mask = np.random.rand(self.dim) < adaptive_CR\n                trial[mask] = mutant[mask]\n                trial = np.clip(trial, self.lb, self.ub)\n\n                # Bird Swarm Inspired Local Search and Cauchy Mutation\n                if np.random.rand() < 0.5: # Apply local search with 50% probability\n                    # Neighborhood Search\n                    neighbors_indices = np.random.choice(self.pop_size, size=self.neighborhood_size, replace=False)\n                    neighbors = self.population[neighbors_indices]\n                    neighbors_fitness = self.fitness[neighbors_indices]\n                    best_neighbor_index = neighbors_indices[np.argmin(neighbors_fitness)]\n                    best_neighbor = self.population[best_neighbor_index]\n\n                    # Move towards the best neighbor, but also consider global best\n                    if np.random.rand() < 0.5:\n                      trial = trial + 0.1 * (best_neighbor - trial) # Local exploitation\n                    else:\n                      trial = trial + 0.1 * (self.global_best_position - trial) # Global exploitation\n\n                    trial = np.clip(trial, self.lb, self.ub)\n\n                    # Cauchy Mutation\n                    if np.random.rand() < self.cauchy_probability:\n                        mutation = trial + (self.ub - self.lb) * np.random.standard_cauchy(size=self.dim) * self.cauchy_scale\n                        mutation = np.clip(mutation, self.lb, self.ub)\n                        trial = mutation\n\n                # Exploration: Random jump with probability exploration_probability\n                if np.random.rand() < self.exploration_probability:\n                    trial = np.random.uniform(self.lb, self.ub, self.dim)\n\n\n                # Selection\n                f_trial = func(trial)\n                self.eval_count += 1\n\n                if f_trial < self.fitness[i]:\n                    # Update success history\n                    self.success_f.append(adaptive_F)\n                    self.success_cr.append(adaptive_CR)\n                    if len(self.success_f) > self.success_history_size:\n                        self.success_f.pop(0)\n                        self.success_cr.pop(0)\n\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n                        if f_trial < self.global_best_fitness:\n                            self.global_best_fitness = f_trial\n                            self.global_best_position = trial.copy()\n\n\n\n            # Stagnation Check and Restart/Population Size Adjustment\n            self.best_fitness_history.append(self.f_opt)\n            if len(self.best_fitness_history) > self.stagnation_threshold:\n                if self.best_fitness_history[-1] >= self.best_fitness_history[-self.stagnation_threshold]:\n                    self.stagnation_counter += 1\n                else:\n                    self.stagnation_counter = 0\n\n            if self.stagnation_counter >= self.stagnation_threshold:\n                #Restart the population with possible population size adjustment\n                if self.generation % self.pop_resize_interval == 0:\n                    if np.random.rand() < 0.5: #shrink population\n                        self.pop_size = int(max(self.pop_size * 0.9, self.min_pop_size))\n                    else: #grow population\n                        self.pop_size = int(min(self.pop_size * 1.1, self.max_pop_size))\n\n                self.population = np.random.uniform(self.lb, self.ub, (self.pop_size, self.dim))\n                self.fitness = np.array([func(x) for x in self.population])\n                self.f_opt = np.min(self.fitness)\n                self.x_opt = self.population[np.argmin(self.fitness)]\n\n                if self.f_opt < self.global_best_fitness:\n                    self.global_best_fitness = self.f_opt\n                    self.global_best_position = self.x_opt.copy()\n\n                self.eval_count += self.pop_size - (self.budget-self.eval_count) if self.eval_count + self.pop_size > self.budget else self.pop_size\n                self.stagnation_counter = 0\n                self.best_fitness_history = [self.f_opt]\n\n            if self.eval_count >= self.budget:\n                break\n        return self.f_opt, self.x_opt\n```", "aucs": [0.20903441255893362, 0.5718621867639754, 0.6274297638961046, 0.8889572516596194, 0.7225374560191589, 0.7788194724722144, 0.6787567690073788, 0.7262241585438283, 0.7600916960112851, 0.7223162361530524, 0.7621552500944752, 0.9905427070318904, 0.3142174490600702, 0.6972586757257958, 0.8995418824265934, 0.7703579311281141, 0.5456658544710498, 0.8181718054134554, 0.2397666485045632, 0.5208973999803492], "operator": "crossover", "theta_sampled": 0.008608709950869767, "reward": 0.0, "generation": 12}, "task_prompt": ""}
{"id": "0e59a95d-3bfe-44bf-8597-2b73c24223a1", "fitness": 0.3501390578750637, "name": "AdaptiveDEShrink", "description": "Adaptive Differential Evolution with a self-adaptive Cauchy mutation and shrinking search space.", "code": "import numpy as np\n\nclass AdaptiveDEShrink:\n    def __init__(self, budget=10000, dim=10, pop_size=40, F=0.5, CR=0.7, cauchy_learning_rate=0.1, shrink_factor=0.99):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.lb = -5.0\n        self.ub = 5.0\n        self.eval_count = 0\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.cauchy_learning_rate = cauchy_learning_rate\n        self.shrink_factor = shrink_factor\n        self.search_range = self.ub - self.lb\n\n    def __call__(self, func):\n        self.population = np.random.uniform(self.lb, self.ub, (self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.eval_count += self.pop_size\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n\n        while self.eval_count < self.budget:\n            for i in range(self.pop_size):\n                # Adaptive F and CR\n                F_adaptive = self.F * (1 + 0.1 * np.random.randn())\n                CR_adaptive = self.CR * (1 + 0.1 * np.random.randn())\n                F_adaptive = np.clip(F_adaptive, 0.1, 1.0)\n                CR_adaptive = np.clip(CR_adaptive, 0.1, 1.0)\n\n                # Mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = np.random.choice(idxs, 3, replace=False)\n                mutant = self.population[a] + F_adaptive * (self.population[b] - self.population[c])\n                mutant = np.clip(mutant, self.lb, self.ub)\n\n                # Crossover\n                trial = np.copy(self.population[i])\n                mask = np.random.rand(self.dim) < CR_adaptive\n                trial[mask] = mutant[mask]\n                trial = np.clip(trial, self.lb, self.ub)\n\n                # Self-Adaptive Cauchy Mutation: Adapts based on current best fitness\n                cauchy_scale = self.cauchy_learning_rate * self.search_range * (1 + self.f_opt) # Scale Cauchy based on f_opt\n                cauchy_mutation = cauchy_scale * np.random.standard_cauchy(size=self.dim)\n                trial = trial + cauchy_mutation\n                trial = np.clip(trial, self.lb, self.ub)\n\n                # Selection\n                f_trial = func(trial)\n                self.eval_count += 1\n\n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n            # Shrink the search space\n            self.search_range *= self.shrink_factor\n            center = self.x_opt\n            self.lb = np.clip(center - self.search_range / 2, -5.0, 5.0)\n            self.ub = np.clip(center + self.search_range / 2, -5.0, 5.0)\n            \n            if self.eval_count >= self.budget:\n                break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 12, "feedback": "The algorithm AdaptiveDEShrink scored 0.350 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["5b77f9fb-407a-4344-aa0a-a95d9f7e8690"], "operator": "mutation", "metadata": {"llm_response": "# Description: Adaptive Differential Evolution with a self-adaptive Cauchy mutation and shrinking search space.\n# Code: \n```python\nimport numpy as np\n\nclass AdaptiveDEShrink:\n    def __init__(self, budget=10000, dim=10, pop_size=40, F=0.5, CR=0.7, cauchy_learning_rate=0.1, shrink_factor=0.99):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.lb = -5.0\n        self.ub = 5.0\n        self.eval_count = 0\n        self.f_opt = np.inf\n        self.x_opt = None\n        self.cauchy_learning_rate = cauchy_learning_rate\n        self.shrink_factor = shrink_factor\n        self.search_range = self.ub - self.lb\n\n    def __call__(self, func):\n        self.population = np.random.uniform(self.lb, self.ub, (self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.eval_count += self.pop_size\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n\n        while self.eval_count < self.budget:\n            for i in range(self.pop_size):\n                # Adaptive F and CR\n                F_adaptive = self.F * (1 + 0.1 * np.random.randn())\n                CR_adaptive = self.CR * (1 + 0.1 * np.random.randn())\n                F_adaptive = np.clip(F_adaptive, 0.1, 1.0)\n                CR_adaptive = np.clip(CR_adaptive, 0.1, 1.0)\n\n                # Mutation\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = np.random.choice(idxs, 3, replace=False)\n                mutant = self.population[a] + F_adaptive * (self.population[b] - self.population[c])\n                mutant = np.clip(mutant, self.lb, self.ub)\n\n                # Crossover\n                trial = np.copy(self.population[i])\n                mask = np.random.rand(self.dim) < CR_adaptive\n                trial[mask] = mutant[mask]\n                trial = np.clip(trial, self.lb, self.ub)\n\n                # Self-Adaptive Cauchy Mutation: Adapts based on current best fitness\n                cauchy_scale = self.cauchy_learning_rate * self.search_range * (1 + self.f_opt) # Scale Cauchy based on f_opt\n                cauchy_mutation = cauchy_scale * np.random.standard_cauchy(size=self.dim)\n                trial = trial + cauchy_mutation\n                trial = np.clip(trial, self.lb, self.ub)\n\n                # Selection\n                f_trial = func(trial)\n                self.eval_count += 1\n\n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n            # Shrink the search space\n            self.search_range *= self.shrink_factor\n            center = self.x_opt\n            self.lb = np.clip(center - self.search_range / 2, -5.0, 5.0)\n            self.ub = np.clip(center + self.search_range / 2, -5.0, 5.0)\n            \n            if self.eval_count >= self.budget:\n                break\n\n        return self.f_opt, self.x_opt\n```", "aucs": [0.14026889398413345, 0.2637993039523874, 0.3261372938264946, 0.36294593301887146, 0.25879494173359263, 0.3151710510605672, 0.2751145860187688, 0.2802459496508688, 0.2491628075584833, 0.21942199754719427, 0.33634664633947364, 0.9962356665099595, 0.2922893661455034, 0.2937183709470925, 0.6938152759572347, 0.34526755899147155, 0.2856969630762324, 0.39513726225068657, 0.18905807734188473, 0.48415321159037306], "operator": "mutation", "theta_sampled": 0.0026039599451966027, "reward": 0.0, "generation": 12}, "task_prompt": ""}
{"id": "aa97939d-0765-495d-bcfb-c9f578dcc8b3", "fitness": 0.29519105128666656, "name": "AntColonyOptimization", "description": "A population-based algorithm inspired by the foraging behavior of ants, utilizing pheromone trails and evaporation to guide search and dynamically adjust step sizes based on success.", "code": "import numpy as np\n\nclass AntColonyOptimization:\n    def __init__(self, budget=10000, dim=10, n_ants=20, evaporation_rate=0.1, alpha=1.0, beta=2.0, initial_step_size=1.0, step_size_reduction=0.95, step_size_increase=1.05):\n        self.budget = budget\n        self.dim = dim\n        self.n_ants = n_ants\n        self.evaporation_rate = evaporation_rate\n        self.alpha = alpha  # Pheromone influence\n        self.beta = beta   # Heuristic influence (fitness)\n        self.pheromone = np.ones(dim)  # Initialize pheromone trails for each dimension\n        self.positions = np.zeros((n_ants, dim))\n        self.fitness = np.zeros(n_ants)\n        self.initial_step_size = initial_step_size\n        self.step_size = np.full(dim, initial_step_size) # Initialize step size for each dimension\n        self.step_size_reduction = step_size_reduction\n        self.step_size_increase = step_size_increase\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        \n        # Initialize ant positions randomly\n        for i in range(self.n_ants):\n            self.positions[i] = np.random.uniform(func.bounds.lb, func.bounds.ub, self.dim)\n            self.fitness[i] = func(self.positions[i])\n            if self.fitness[i] < self.f_opt:\n                self.f_opt = self.fitness[i]\n                self.x_opt = self.positions[i]\n\n        eval_count = self.n_ants  # Account for initial evaluations\n\n        while eval_count < self.budget:\n            # Choose directions based on pheromone and fitness\n            probabilities = np.zeros((self.n_ants, self.dim))\n\n            for i in range(self.n_ants):\n                for j in range(self.dim):\n                    probabilities[i, j] = (self.pheromone[j]**self.alpha) / (self.step_size[j]**self.beta) # Heuristic: small step size are more interesting.\n                    \n            # Normalize probabilities for each ant\n            for i in range(self.n_ants):\n                probabilities[i] /= np.sum(probabilities[i])\n                \n            # Move ants\n            new_positions = np.copy(self.positions)\n            new_fitness = np.copy(self.fitness)\n            \n            for i in range(self.n_ants):\n                for j in range(self.dim):\n                    # Decide whether to move positive or negative with equal probability\n                    direction = np.random.choice([-1, 1])\n                    \n                    # Take a step in the chosen direction\n                    step = direction * self.step_size[j] * np.random.uniform(0,1)  # Scale step size\n\n                    new_positions[i, j] = self.positions[i, j] + step\n                    new_positions[i, j] = np.clip(new_positions[i, j], func.bounds.lb[j], func.bounds.ub[j])\n\n                f = func(new_positions[i])\n                eval_count += 1\n                if f < new_fitness[i]:\n                    new_fitness[i] = f\n\n                    # Update best solution if necessary\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = new_positions[i]\n            \n\n            # Update ant positions and fitness if the new positions are better\n            improved_indices = new_fitness < self.fitness\n            self.positions[improved_indices] = new_positions[improved_indices]\n            self.fitness[improved_indices] = new_fitness[improved_indices]\n            \n            # Update pheromone trails\n            self.pheromone *= (1 - self.evaporation_rate)  # Evaporation\n            \n            # Find the best ant\n            best_ant_index = np.argmin(self.fitness)\n            \n            # Deposit pheromone based on the best ant's position\n            for j in range(self.dim):\n                self.pheromone[j] += (1 / (1 + self.fitness[best_ant_index] ))  # Higher reward for better fitness\n            \n            # Adapt step sizes: increase for success, decrease for failure\n            for j in range(self.dim):\n                if np.any(improved_indices & (new_positions[:,j] != self.positions[:,j])): #if the dimension changed and led to an improvement, increase the step size\n                    self.step_size[j] *= self.step_size_increase\n                else: # reduce step size, if it did not lead to an improvement.\n                    self.step_size[j] *= self.step_size_reduction\n\n                # Ensure step size remains within reasonable bounds\n                self.step_size[j] = np.clip(self.step_size[j], 1e-6, (func.bounds.ub[j] - func.bounds.lb[j]) / 2) # bounds of at most half the search space size\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 12, "feedback": "The algorithm AntColonyOptimization scored 0.295 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": "random_new", "metadata": {"llm_response": "# Description: A population-based algorithm inspired by the foraging behavior of ants, utilizing pheromone trails and evaporation to guide search and dynamically adjust step sizes based on success.\n# Code:\n```python\nimport numpy as np\n\nclass AntColonyOptimization:\n    def __init__(self, budget=10000, dim=10, n_ants=20, evaporation_rate=0.1, alpha=1.0, beta=2.0, initial_step_size=1.0, step_size_reduction=0.95, step_size_increase=1.05):\n        self.budget = budget\n        self.dim = dim\n        self.n_ants = n_ants\n        self.evaporation_rate = evaporation_rate\n        self.alpha = alpha  # Pheromone influence\n        self.beta = beta   # Heuristic influence (fitness)\n        self.pheromone = np.ones(dim)  # Initialize pheromone trails for each dimension\n        self.positions = np.zeros((n_ants, dim))\n        self.fitness = np.zeros(n_ants)\n        self.initial_step_size = initial_step_size\n        self.step_size = np.full(dim, initial_step_size) # Initialize step size for each dimension\n        self.step_size_reduction = step_size_reduction\n        self.step_size_increase = step_size_increase\n\n    def __call__(self, func):\n        self.f_opt = np.inf\n        self.x_opt = None\n        \n        # Initialize ant positions randomly\n        for i in range(self.n_ants):\n            self.positions[i] = np.random.uniform(func.bounds.lb, func.bounds.ub, self.dim)\n            self.fitness[i] = func(self.positions[i])\n            if self.fitness[i] < self.f_opt:\n                self.f_opt = self.fitness[i]\n                self.x_opt = self.positions[i]\n\n        eval_count = self.n_ants  # Account for initial evaluations\n\n        while eval_count < self.budget:\n            # Choose directions based on pheromone and fitness\n            probabilities = np.zeros((self.n_ants, self.dim))\n\n            for i in range(self.n_ants):\n                for j in range(self.dim):\n                    probabilities[i, j] = (self.pheromone[j]**self.alpha) / (self.step_size[j]**self.beta) # Heuristic: small step size are more interesting.\n                    \n            # Normalize probabilities for each ant\n            for i in range(self.n_ants):\n                probabilities[i] /= np.sum(probabilities[i])\n                \n            # Move ants\n            new_positions = np.copy(self.positions)\n            new_fitness = np.copy(self.fitness)\n            \n            for i in range(self.n_ants):\n                for j in range(self.dim):\n                    # Decide whether to move positive or negative with equal probability\n                    direction = np.random.choice([-1, 1])\n                    \n                    # Take a step in the chosen direction\n                    step = direction * self.step_size[j] * np.random.uniform(0,1)  # Scale step size\n\n                    new_positions[i, j] = self.positions[i, j] + step\n                    new_positions[i, j] = np.clip(new_positions[i, j], func.bounds.lb[j], func.bounds.ub[j])\n\n                f = func(new_positions[i])\n                eval_count += 1\n                if f < new_fitness[i]:\n                    new_fitness[i] = f\n\n                    # Update best solution if necessary\n                    if f < self.f_opt:\n                        self.f_opt = f\n                        self.x_opt = new_positions[i]\n            \n\n            # Update ant positions and fitness if the new positions are better\n            improved_indices = new_fitness < self.fitness\n            self.positions[improved_indices] = new_positions[improved_indices]\n            self.fitness[improved_indices] = new_fitness[improved_indices]\n            \n            # Update pheromone trails\n            self.pheromone *= (1 - self.evaporation_rate)  # Evaporation\n            \n            # Find the best ant\n            best_ant_index = np.argmin(self.fitness)\n            \n            # Deposit pheromone based on the best ant's position\n            for j in range(self.dim):\n                self.pheromone[j] += (1 / (1 + self.fitness[best_ant_index] ))  # Higher reward for better fitness\n            \n            # Adapt step sizes: increase for success, decrease for failure\n            for j in range(self.dim):\n                if np.any(improved_indices & (new_positions[:,j] != self.positions[:,j])): #if the dimension changed and led to an improvement, increase the step size\n                    self.step_size[j] *= self.step_size_increase\n                else: # reduce step size, if it did not lead to an improvement.\n                    self.step_size[j] *= self.step_size_reduction\n\n                # Ensure step size remains within reasonable bounds\n                self.step_size[j] = np.clip(self.step_size[j], 1e-6, (func.bounds.ub[j] - func.bounds.lb[j]) / 2) # bounds of at most half the search space size\n        return self.f_opt, self.x_opt\n```", "aucs": [0.09602203696841627, 0.1912191738940613, 0.2928003979005078, 0.20880840040045423, 0.2120762797670499, 0.2788067802055443, 0.26364275358325207, 0.22894568343978572, 0.18520532679932544, 0.17587314943650367, 0.17896009847598182, 0.9996582169310545, 0.36222717512799274, 0.19927046475543497, 0.5292567463492408, 0.29902130642565594, 0.29006250161698943, 0.24173240418561703, 0.23272694078021738, 0.43750518869024535], "operator": "random_new", "theta_sampled": 0.0009595664520118792, "reward": 0.0, "generation": 12}, "task_prompt": ""}
{"id": "48dd5738-d007-4543-a007-c439dc30873d", "fitness": 0.0, "name": "OrthogonalAdaptiveOptimizer", "description": "Population-based algorithm with adaptive scaling based on fitness landscape exploration using orthogonal arrays.", "code": "import numpy as np\nfrom scipy.stats import norm\nfrom smt.sampling_methods import LHS\n\n\nclass OrthogonalAdaptiveOptimizer:\n    def __init__(self, budget=10000, dim=10, pop_size=50, oa_levels=3):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.oa_levels = oa_levels\n        self.population = None\n        self.fitness = None\n        self.best_fitness = np.inf\n        self.best_solution = None\n        self.lb = None\n        self.ub = None\n        self.oa = None\n        self.scale = 1.0\n\n    def initialize_population(self):\n        sampling = LHS(xlimits=np.array([self.lb, self.ub]).T, criterion=\"maximin\", random_state=42)\n        self.population = sampling(self.pop_size)\n        self.fitness = np.zeros(self.pop_size)\n\n    def generate_orthogonal_array(self, n_factors, n_levels):\n      # Approximating an OA.  Ideally, we'd use a library for this.\n      # This simplistic approach ensures it always creates an array, though it may not be perfectly orthogonal\n      oa = np.random.randint(0, n_levels, size=(n_factors * n_levels, n_factors))\n      return oa\n    \n    def evaluate_population(self, func):\n        for i in range(self.pop_size):\n            if self.fitness[i] == np.inf: # only evaluate if it hasn't been evaluated yet or was assigned infinity (to allow restarts)\n              if func.n_calls < self.budget:\n                self.fitness[i] = func(self.population[i])\n              else:\n                self.fitness[i] = np.inf # set to infinity if out of budget\n              \n              if self.fitness[i] < self.best_fitness:\n                  self.best_fitness = self.fitness[i]\n                  self.best_solution = self.population[i].copy()\n\n    def orthogonal_crossover(self):\n        oa_size = self.dim * self.oa_levels\n        if self.oa is None or self.oa.shape != (oa_size, self.dim):\n          self.oa = self.generate_orthogonal_array(self.dim, self.oa_levels)\n\n        for i in range(self.pop_size):\n            parent1_idx = np.random.randint(0, self.pop_size)\n            parent2_idx = np.random.randint(0, self.pop_size)\n\n            child = self.population[i].copy()\n            for j in range(self.dim):\n                oa_row_idx = np.random.randint(0, oa_size)\n                oa_level = self.oa[oa_row_idx, j]\n\n                if oa_level == 0:\n                    child[j] = self.population[parent1_idx][j]\n                elif oa_level == 1:\n                    child[j] = self.population[parent2_idx][j]\n                else: #oa_level == 2:\n                    child[j] = self.best_solution[j] # Incorporate the best solution to guide the search\n            \n            # Mutation (scale adaptive)\n            mutation_mask = np.random.rand(self.dim) < 0.1 # 10% mutation rate\n            child[mutation_mask] = child[mutation_mask] + self.scale * np.random.normal(0, 1, size=np.sum(mutation_mask))\n\n            child = np.clip(child, self.lb, self.ub)\n            \n            self.population[i] = child\n            self.fitness[i] = np.inf # Mark for re-evaluation after crossover/mutation\n\n\n    def adaptive_scaling(self):\n        # Adjust scaling based on fitness improvement\n        if self.best_fitness < self.previous_best_fitness:\n            self.scale *= 1.1 # Increase scale if improving\n        else:\n            self.scale *= 0.9 # Decrease scale if stagnating\n        self.scale = np.clip(self.scale, 0.01, 1.0) # Keep scale within reasonable bounds\n\n    def __call__(self, func):\n        self.lb = func.bounds.lb\n        self.ub = func.bounds.ub\n        self.initialize_population()\n        self.evaluate_population(func)\n        self.previous_best_fitness = np.inf # placeholder before any solutions are found\n\n        while func.n_calls < self.budget:\n            self.orthogonal_crossover()\n            self.evaluate_population(func)\n            self.adaptive_scaling()\n            self.previous_best_fitness = self.best_fitness\n\n        return self.best_fitness, self.best_solution", "configspace": "", "generation": 12, "feedback": "", "error": "Evaluation error: No module named 'smt'", "parent_ids": [], "operator": "random_new", "metadata": {"llm_response": "# Description: Population-based algorithm with adaptive scaling based on fitness landscape exploration using orthogonal arrays.\n# Code: \n```python\nimport numpy as np\nfrom scipy.stats import norm\nfrom smt.sampling_methods import LHS\n\n\nclass OrthogonalAdaptiveOptimizer:\n    def __init__(self, budget=10000, dim=10, pop_size=50, oa_levels=3):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.oa_levels = oa_levels\n        self.population = None\n        self.fitness = None\n        self.best_fitness = np.inf\n        self.best_solution = None\n        self.lb = None\n        self.ub = None\n        self.oa = None\n        self.scale = 1.0\n\n    def initialize_population(self):\n        sampling = LHS(xlimits=np.array([self.lb, self.ub]).T, criterion=\"maximin\", random_state=42)\n        self.population = sampling(self.pop_size)\n        self.fitness = np.zeros(self.pop_size)\n\n    def generate_orthogonal_array(self, n_factors, n_levels):\n      # Approximating an OA.  Ideally, we'd use a library for this.\n      # This simplistic approach ensures it always creates an array, though it may not be perfectly orthogonal\n      oa = np.random.randint(0, n_levels, size=(n_factors * n_levels, n_factors))\n      return oa\n    \n    def evaluate_population(self, func):\n        for i in range(self.pop_size):\n            if self.fitness[i] == np.inf: # only evaluate if it hasn't been evaluated yet or was assigned infinity (to allow restarts)\n              if func.n_calls < self.budget:\n                self.fitness[i] = func(self.population[i])\n              else:\n                self.fitness[i] = np.inf # set to infinity if out of budget\n              \n              if self.fitness[i] < self.best_fitness:\n                  self.best_fitness = self.fitness[i]\n                  self.best_solution = self.population[i].copy()\n\n    def orthogonal_crossover(self):\n        oa_size = self.dim * self.oa_levels\n        if self.oa is None or self.oa.shape != (oa_size, self.dim):\n          self.oa = self.generate_orthogonal_array(self.dim, self.oa_levels)\n\n        for i in range(self.pop_size):\n            parent1_idx = np.random.randint(0, self.pop_size)\n            parent2_idx = np.random.randint(0, self.pop_size)\n\n            child = self.population[i].copy()\n            for j in range(self.dim):\n                oa_row_idx = np.random.randint(0, oa_size)\n                oa_level = self.oa[oa_row_idx, j]\n\n                if oa_level == 0:\n                    child[j] = self.population[parent1_idx][j]\n                elif oa_level == 1:\n                    child[j] = self.population[parent2_idx][j]\n                else: #oa_level == 2:\n                    child[j] = self.best_solution[j] # Incorporate the best solution to guide the search\n            \n            # Mutation (scale adaptive)\n            mutation_mask = np.random.rand(self.dim) < 0.1 # 10% mutation rate\n            child[mutation_mask] = child[mutation_mask] + self.scale * np.random.normal(0, 1, size=np.sum(mutation_mask))\n\n            child = np.clip(child, self.lb, self.ub)\n            \n            self.population[i] = child\n            self.fitness[i] = np.inf # Mark for re-evaluation after crossover/mutation\n\n\n    def adaptive_scaling(self):\n        # Adjust scaling based on fitness improvement\n        if self.best_fitness < self.previous_best_fitness:\n            self.scale *= 1.1 # Increase scale if improving\n        else:\n            self.scale *= 0.9 # Decrease scale if stagnating\n        self.scale = np.clip(self.scale, 0.01, 1.0) # Keep scale within reasonable bounds\n\n    def __call__(self, func):\n        self.lb = func.bounds.lb\n        self.ub = func.bounds.ub\n        self.initialize_population()\n        self.evaluate_population(func)\n        self.previous_best_fitness = np.inf # placeholder before any solutions are found\n\n        while func.n_calls < self.budget:\n            self.orthogonal_crossover()\n            self.evaluate_population(func)\n            self.adaptive_scaling()\n            self.previous_best_fitness = self.best_fitness\n\n        return self.best_fitness, self.best_solution\n```", "operator": "random_new", "theta_sampled": 0.0004783275362710154, "reward": 0.0, "generation": 12}, "task_prompt": ""}
{"id": "12611198-b397-4ace-a480-1e8e579d54e3", "fitness": 0.0, "name": "AdaptiveHybridDEBirdSwarmPlusPlus", "description": "An adaptive hybrid algorithm combining Differential Evolution, Bird Swarm Optimization, success history adaptation, and dynamic population sizing with a self-adaptive Cauchy mutation strategy.", "code": "import numpy as np\n\nclass AdaptiveHybridDEBirdSwarmPlusPlus:\n    def __init__(self, budget=10000, dim=10, pop_size=40, F=0.5, CR=0.7, neighborhood_size=5, cauchy_probability=0.1, success_history_size=10, restart_probability=0.05):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = int(np.clip(pop_size, 10, 100))  # Constrain pop_size\n        self.F = F  # Mutation factor\n        self.CR = CR  # Crossover rate\n        self.lb = -5.0\n        self.ub = 5.0\n        self.stagnation_counter = 0\n        self.stagnation_threshold = budget // 10\n        self.best_fitness_history = []\n        self.neighborhood_size = neighborhood_size\n        self.cauchy_probability = cauchy_probability\n        self.cauchy_scale = 0.1\n        self.success_history_size = success_history_size\n        self.success_f = []\n        self.success_cr = []\n        self.min_pop_size = 10\n        self.max_pop_size = 100\n        self.pop_resize_interval = budget // 20\n        self.restart_probability = restart_probability\n        self.levy_exponent = 1.5 # Exponent for Levy flight\n        self.adaptive_cauchy_scale = 0.1  # Initial Cauchy scale\n\n\n    def levy_flight(self, size, beta=1.5):\n        sigma = (np.math.gamma(1 + beta) * np.sin(np.pi * beta / 2) / (np.math.gamma((1 + beta) / 2) * beta * (2 ** ((beta - 1) / 2)))) ** (1 / beta)\n        u = np.random.normal(0, sigma, size=size)\n        v = np.random.normal(0, 1, size=size)\n        step = u / abs(v) ** (1 / beta)\n        return step\n\n    def __call__(self, func):\n        self.population = np.random.uniform(self.lb, self.ub, (self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n        self.eval_count = self.pop_size\n        self.generation = 0\n\n        while self.eval_count < self.budget:\n            self.generation += 1\n\n            for i in range(self.pop_size):\n                # Adaptive F and CR using success history\n                if self.success_f and self.success_cr:\n                    adaptive_F = np.random.choice(self.success_f)\n                    adaptive_CR = np.random.choice(self.success_cr)\n                else:\n                    adaptive_F = self.F * (1 + 0.1 * np.random.randn())\n                    adaptive_CR = self.CR * (1 + 0.1 * np.random.randn())\n\n                adaptive_F = np.clip(adaptive_F, 0.1, 1.0)\n                adaptive_CR = np.clip(adaptive_CR, 0.1, 1.0)\n\n                # Mutation (DE part)\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = np.random.choice(idxs, 3, replace=False)\n                mutant = self.population[a] + adaptive_F * (self.population[b] - self.population[c])\n                mutant = np.clip(mutant, self.lb, self.ub)\n\n                # Crossover\n                trial = np.copy(self.population[i])\n                mask = np.random.rand(self.dim) < adaptive_CR\n                trial[mask] = mutant[mask]\n                trial = np.clip(trial, self.lb, self.ub)\n\n                # Bird Swarm Inspired Local Search\n                if np.random.rand() < 0.5:\n                    # Neighborhood Search\n                    neighbors_indices = np.random.choice(self.pop_size, size=self.neighborhood_size, replace=False)\n                    neighbors = self.population[neighbors_indices]\n                    neighbors_fitness = self.fitness[neighbors_indices]\n                    best_neighbor_index = neighbors_indices[np.argmin(neighbors_fitness)]\n                    best_neighbor = self.population[best_neighbor_index]\n\n                    # Move towards the best neighbor\n                    trial = trial + 0.1 * (best_neighbor - trial)\n                    trial = np.clip(trial, self.lb, self.ub)\n                    \n\n                # Self-Adaptive Cauchy Mutation with Levy Flight\n                if np.random.rand() < self.cauchy_probability:\n                     levy_steps = self.levy_flight(size=self.dim, beta=self.levy_exponent)\n                     mutation = trial + self.adaptive_cauchy_scale * levy_steps * np.random.standard_cauchy(size=self.dim)\n                     mutation = np.clip(mutation, self.lb, self.ub)\n                     trial = mutation\n\n\n                # Selection\n                f_trial = func(trial)\n                self.eval_count += 1\n\n                if f_trial < self.fitness[i]:\n                    # Update success history\n                    self.success_f.append(adaptive_F)\n                    self.success_cr.append(adaptive_CR)\n                    if len(self.success_f) > self.success_history_size:\n                        self.success_f.pop(0)\n                        self.success_cr.pop(0)\n\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                        # Adaptive Cauchy scaling: Reduce if finding better solution, increase otherwise\n                        self.adaptive_cauchy_scale *= 0.95\n                    else:\n                        self.adaptive_cauchy_scale = min(0.1, self.adaptive_cauchy_scale*1.05)\n\n\n            # Stagnation Check and Restart/Population Size Adjustment\n            self.best_fitness_history.append(self.f_opt)\n            if len(self.best_fitness_history) > self.stagnation_threshold:\n                if self.best_fitness_history[-1] >= self.best_fitness_history[-self.stagnation_threshold]:\n                    self.stagnation_counter += 1\n                else:\n                    self.stagnation_counter = 0\n\n            if self.stagnation_counter >= self.stagnation_threshold:\n                #Restart the population with possible population size adjustment\n                if self.generation % self.pop_resize_interval == 0:\n                    if np.random.rand() < 0.5: #shrink population\n                        self.pop_size = int(max(self.pop_size * 0.9, self.min_pop_size))\n                    else: #grow population\n                        self.pop_size = int(min(self.pop_size * 1.1, self.max_pop_size))\n\n                # Introduce a small chance of restarting even without stagnation\n                if np.random.rand() < self.restart_probability:\n                    self.population = np.random.uniform(self.lb, self.ub, (self.pop_size, self.dim))\n                    self.fitness = np.array([func(x) for x in self.population])\n                    self.f_opt = np.min(self.fitness)\n                    self.x_opt = self.population[np.argmin(self.fitness)]\n                    self.eval_count += self.pop_size - (self.budget-self.eval_count) if self.eval_count + self.pop_size > self.budget else self.pop_size\n                    self.stagnation_counter = 0\n                    self.best_fitness_history = [self.f_opt]\n                else: # If not restarting, just shuffle population\n                    np.random.shuffle(self.population)\n\n\n            if self.eval_count >= self.budget:\n                break\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 12, "feedback": "", "error": "Evaluation error: module 'numpy' has no attribute 'math'", "parent_ids": ["5b77f9fb-407a-4344-aa0a-a95d9f7e8690", "3b387073-58a6-4ee0-9faf-ba6fbc4ee604"], "operator": "crossover", "metadata": {"llm_response": "# Description: An adaptive hybrid algorithm combining Differential Evolution, Bird Swarm Optimization, success history adaptation, and dynamic population sizing with a self-adaptive Cauchy mutation strategy.\n# Code: \n```python\nimport numpy as np\n\nclass AdaptiveHybridDEBirdSwarmPlusPlus:\n    def __init__(self, budget=10000, dim=10, pop_size=40, F=0.5, CR=0.7, neighborhood_size=5, cauchy_probability=0.1, success_history_size=10, restart_probability=0.05):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = int(np.clip(pop_size, 10, 100))  # Constrain pop_size\n        self.F = F  # Mutation factor\n        self.CR = CR  # Crossover rate\n        self.lb = -5.0\n        self.ub = 5.0\n        self.stagnation_counter = 0\n        self.stagnation_threshold = budget // 10\n        self.best_fitness_history = []\n        self.neighborhood_size = neighborhood_size\n        self.cauchy_probability = cauchy_probability\n        self.cauchy_scale = 0.1\n        self.success_history_size = success_history_size\n        self.success_f = []\n        self.success_cr = []\n        self.min_pop_size = 10\n        self.max_pop_size = 100\n        self.pop_resize_interval = budget // 20\n        self.restart_probability = restart_probability\n        self.levy_exponent = 1.5 # Exponent for Levy flight\n        self.adaptive_cauchy_scale = 0.1  # Initial Cauchy scale\n\n\n    def levy_flight(self, size, beta=1.5):\n        sigma = (np.math.gamma(1 + beta) * np.sin(np.pi * beta / 2) / (np.math.gamma((1 + beta) / 2) * beta * (2 ** ((beta - 1) / 2)))) ** (1 / beta)\n        u = np.random.normal(0, sigma, size=size)\n        v = np.random.normal(0, 1, size=size)\n        step = u / abs(v) ** (1 / beta)\n        return step\n\n    def __call__(self, func):\n        self.population = np.random.uniform(self.lb, self.ub, (self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n        self.eval_count = self.pop_size\n        self.generation = 0\n\n        while self.eval_count < self.budget:\n            self.generation += 1\n\n            for i in range(self.pop_size):\n                # Adaptive F and CR using success history\n                if self.success_f and self.success_cr:\n                    adaptive_F = np.random.choice(self.success_f)\n                    adaptive_CR = np.random.choice(self.success_cr)\n                else:\n                    adaptive_F = self.F * (1 + 0.1 * np.random.randn())\n                    adaptive_CR = self.CR * (1 + 0.1 * np.random.randn())\n\n                adaptive_F = np.clip(adaptive_F, 0.1, 1.0)\n                adaptive_CR = np.clip(adaptive_CR, 0.1, 1.0)\n\n                # Mutation (DE part)\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = np.random.choice(idxs, 3, replace=False)\n                mutant = self.population[a] + adaptive_F * (self.population[b] - self.population[c])\n                mutant = np.clip(mutant, self.lb, self.ub)\n\n                # Crossover\n                trial = np.copy(self.population[i])\n                mask = np.random.rand(self.dim) < adaptive_CR\n                trial[mask] = mutant[mask]\n                trial = np.clip(trial, self.lb, self.ub)\n\n                # Bird Swarm Inspired Local Search\n                if np.random.rand() < 0.5:\n                    # Neighborhood Search\n                    neighbors_indices = np.random.choice(self.pop_size, size=self.neighborhood_size, replace=False)\n                    neighbors = self.population[neighbors_indices]\n                    neighbors_fitness = self.fitness[neighbors_indices]\n                    best_neighbor_index = neighbors_indices[np.argmin(neighbors_fitness)]\n                    best_neighbor = self.population[best_neighbor_index]\n\n                    # Move towards the best neighbor\n                    trial = trial + 0.1 * (best_neighbor - trial)\n                    trial = np.clip(trial, self.lb, self.ub)\n                    \n\n                # Self-Adaptive Cauchy Mutation with Levy Flight\n                if np.random.rand() < self.cauchy_probability:\n                     levy_steps = self.levy_flight(size=self.dim, beta=self.levy_exponent)\n                     mutation = trial + self.adaptive_cauchy_scale * levy_steps * np.random.standard_cauchy(size=self.dim)\n                     mutation = np.clip(mutation, self.lb, self.ub)\n                     trial = mutation\n\n\n                # Selection\n                f_trial = func(trial)\n                self.eval_count += 1\n\n                if f_trial < self.fitness[i]:\n                    # Update success history\n                    self.success_f.append(adaptive_F)\n                    self.success_cr.append(adaptive_CR)\n                    if len(self.success_f) > self.success_history_size:\n                        self.success_f.pop(0)\n                        self.success_cr.pop(0)\n\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n                        # Adaptive Cauchy scaling: Reduce if finding better solution, increase otherwise\n                        self.adaptive_cauchy_scale *= 0.95\n                    else:\n                        self.adaptive_cauchy_scale = min(0.1, self.adaptive_cauchy_scale*1.05)\n\n\n            # Stagnation Check and Restart/Population Size Adjustment\n            self.best_fitness_history.append(self.f_opt)\n            if len(self.best_fitness_history) > self.stagnation_threshold:\n                if self.best_fitness_history[-1] >= self.best_fitness_history[-self.stagnation_threshold]:\n                    self.stagnation_counter += 1\n                else:\n                    self.stagnation_counter = 0\n\n            if self.stagnation_counter >= self.stagnation_threshold:\n                #Restart the population with possible population size adjustment\n                if self.generation % self.pop_resize_interval == 0:\n                    if np.random.rand() < 0.5: #shrink population\n                        self.pop_size = int(max(self.pop_size * 0.9, self.min_pop_size))\n                    else: #grow population\n                        self.pop_size = int(min(self.pop_size * 1.1, self.max_pop_size))\n\n                # Introduce a small chance of restarting even without stagnation\n                if np.random.rand() < self.restart_probability:\n                    self.population = np.random.uniform(self.lb, self.ub, (self.pop_size, self.dim))\n                    self.fitness = np.array([func(x) for x in self.population])\n                    self.f_opt = np.min(self.fitness)\n                    self.x_opt = self.population[np.argmin(self.fitness)]\n                    self.eval_count += self.pop_size - (self.budget-self.eval_count) if self.eval_count + self.pop_size > self.budget else self.pop_size\n                    self.stagnation_counter = 0\n                    self.best_fitness_history = [self.f_opt]\n                else: # If not restarting, just shuffle population\n                    np.random.shuffle(self.population)\n\n\n            if self.eval_count >= self.budget:\n                break\n        return self.f_opt, self.x_opt\n```", "operator": "crossover", "theta_sampled": 0.0019978914841307016, "reward": 0.0, "generation": 12}, "task_prompt": ""}
{"id": "439d325b-cf2f-42ac-a1f2-b8926724ec8c", "fitness": 0.7054634974763413, "name": "AdaptiveHybridDEBirdSwarmPlusPlus", "description": "Combines adaptive DE with bird swarm's local search, stagnation handling with dynamic population resizing, and success-history based F/CR adaptation for enhanced exploration and exploitation.", "code": "import numpy as np\n\nclass AdaptiveHybridDEBirdSwarmPlusPlus:\n    def __init__(self, budget=10000, dim=10, pop_size=40, F=0.5, CR=0.7, neighborhood_size=5, cauchy_probability=0.1, success_history_size=10, p_local_search=0.5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = int(np.clip(pop_size, 10, 100))\n        self.F = F\n        self.CR = CR\n        self.lb = -5.0\n        self.ub = 5.0\n        self.stagnation_counter = 0\n        self.stagnation_threshold = budget // 10\n        self.best_fitness_history = []\n        self.neighborhood_size = neighborhood_size\n        self.cauchy_probability = cauchy_probability\n        self.cauchy_scale = 0.1\n        self.success_history_size = success_history_size\n        self.success_f = []\n        self.success_cr = []\n        self.min_pop_size = 10\n        self.max_pop_size = 100\n        self.pop_resize_interval = budget // 20\n        self.p_local_search = p_local_search  # Probability of applying local search\n        self.local_search_intensity = 0.1\n\n    def __call__(self, func):\n        self.population = np.random.uniform(self.lb, self.ub, (self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n        self.eval_count = self.pop_size\n        self.generation = 0\n\n        while self.eval_count < self.budget:\n            self.generation += 1\n\n            for i in range(self.pop_size):\n                # Adaptive F and CR using success history\n                if self.success_f and self.success_cr:\n                    adaptive_F = np.random.choice(self.success_f)\n                    adaptive_CR = np.random.choice(self.success_cr)\n                else:\n                    adaptive_F = self.F * (1 + 0.1 * np.random.randn())\n                    adaptive_CR = self.CR * (1 + 0.1 * np.random.randn())\n\n                adaptive_F = np.clip(adaptive_F, 0.1, 1.0)\n                adaptive_CR = np.clip(adaptive_CR, 0.1, 1.0)\n\n                # Mutation (DE part)\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = np.random.choice(idxs, 3, replace=False)\n                mutant = self.population[a] + adaptive_F * (self.population[b] - self.population[c])\n                mutant = np.clip(mutant, self.lb, self.ub)\n\n                # Crossover\n                trial = np.copy(self.population[i])\n                mask = np.random.rand(self.dim) < adaptive_CR\n                trial[mask] = mutant[mask]\n                trial = np.clip(trial, self.lb, self.ub)\n\n                # Bird Swarm Inspired Local Search and Cauchy Mutation\n                if np.random.rand() < self.p_local_search:\n                    # Neighborhood Search\n                    neighbors_indices = np.random.choice(self.pop_size, size=self.neighborhood_size, replace=False)\n                    neighbors = self.population[neighbors_indices]\n                    neighbors_fitness = self.fitness[neighbors_indices]\n                    best_neighbor_index = neighbors_indices[np.argmin(neighbors_fitness)]\n                    best_neighbor = self.population[best_neighbor_index]\n\n                    # Move towards the best neighbor\n                    trial = trial + self.local_search_intensity * (best_neighbor - trial)\n                    trial = np.clip(trial, self.lb, self.ub)\n\n                    # Cauchy Mutation\n                    if np.random.rand() < self.cauchy_probability:\n                        mutation = trial + (self.ub - self.lb) * np.random.standard_cauchy(size=self.dim) * self.cauchy_scale\n                        mutation = np.clip(mutation, self.lb, self.ub)\n                        trial = mutation\n\n                # Selection\n                f_trial = func(trial)\n                self.eval_count += 1\n\n                if f_trial < self.fitness[i]:\n                    # Update success history\n                    self.success_f.append(adaptive_F)\n                    self.success_cr.append(adaptive_CR)\n                    if len(self.success_f) > self.success_history_size:\n                        self.success_f.pop(0)\n                        self.success_cr.pop(0)\n\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n            # Stagnation Check and Restart/Population Size Adjustment\n            self.best_fitness_history.append(self.f_opt)\n            if len(self.best_fitness_history) > self.stagnation_threshold:\n                if self.best_fitness_history[-1] >= self.best_fitness_history[-self.stagnation_threshold]:\n                    self.stagnation_counter += 1\n                else:\n                    self.stagnation_counter = 0\n\n            if self.stagnation_counter >= self.stagnation_threshold:\n                #Restart the population with possible population size adjustment\n                if self.generation % self.pop_resize_interval == 0:\n                    if np.random.rand() < 0.5: #shrink population\n                        self.pop_size = int(max(self.pop_size * 0.9, self.min_pop_size))\n                    else: #grow population\n                        self.pop_size = int(min(self.pop_size * 1.1, self.max_pop_size))\n                self.population = np.random.uniform(self.lb, self.ub, (self.pop_size, self.dim))\n                self.fitness = np.array([func(x) for x in self.population])\n                self.f_opt = np.min(self.fitness)\n                self.x_opt = self.population[np.argmin(self.fitness)]\n                self.eval_count += self.pop_size - (self.budget-self.eval_count) if self.eval_count + self.pop_size > self.budget else self.pop_size\n                self.stagnation_counter = 0\n                self.best_fitness_history = [self.f_opt]\n\n            if self.eval_count >= self.budget:\n                break\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 12, "feedback": "The algorithm AdaptiveHybridDEBirdSwarmPlusPlus scored 0.705 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["5b77f9fb-407a-4344-aa0a-a95d9f7e8690", "3b387073-58a6-4ee0-9faf-ba6fbc4ee604"], "operator": "crossover", "metadata": {"llm_response": "# Description: Combines adaptive DE with bird swarm's local search, stagnation handling with dynamic population resizing, and success-history based F/CR adaptation for enhanced exploration and exploitation.\n# Code: \n```python\nimport numpy as np\n\nclass AdaptiveHybridDEBirdSwarmPlusPlus:\n    def __init__(self, budget=10000, dim=10, pop_size=40, F=0.5, CR=0.7, neighborhood_size=5, cauchy_probability=0.1, success_history_size=10, p_local_search=0.5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = int(np.clip(pop_size, 10, 100))\n        self.F = F\n        self.CR = CR\n        self.lb = -5.0\n        self.ub = 5.0\n        self.stagnation_counter = 0\n        self.stagnation_threshold = budget // 10\n        self.best_fitness_history = []\n        self.neighborhood_size = neighborhood_size\n        self.cauchy_probability = cauchy_probability\n        self.cauchy_scale = 0.1\n        self.success_history_size = success_history_size\n        self.success_f = []\n        self.success_cr = []\n        self.min_pop_size = 10\n        self.max_pop_size = 100\n        self.pop_resize_interval = budget // 20\n        self.p_local_search = p_local_search  # Probability of applying local search\n        self.local_search_intensity = 0.1\n\n    def __call__(self, func):\n        self.population = np.random.uniform(self.lb, self.ub, (self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n        self.eval_count = self.pop_size\n        self.generation = 0\n\n        while self.eval_count < self.budget:\n            self.generation += 1\n\n            for i in range(self.pop_size):\n                # Adaptive F and CR using success history\n                if self.success_f and self.success_cr:\n                    adaptive_F = np.random.choice(self.success_f)\n                    adaptive_CR = np.random.choice(self.success_cr)\n                else:\n                    adaptive_F = self.F * (1 + 0.1 * np.random.randn())\n                    adaptive_CR = self.CR * (1 + 0.1 * np.random.randn())\n\n                adaptive_F = np.clip(adaptive_F, 0.1, 1.0)\n                adaptive_CR = np.clip(adaptive_CR, 0.1, 1.0)\n\n                # Mutation (DE part)\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = np.random.choice(idxs, 3, replace=False)\n                mutant = self.population[a] + adaptive_F * (self.population[b] - self.population[c])\n                mutant = np.clip(mutant, self.lb, self.ub)\n\n                # Crossover\n                trial = np.copy(self.population[i])\n                mask = np.random.rand(self.dim) < adaptive_CR\n                trial[mask] = mutant[mask]\n                trial = np.clip(trial, self.lb, self.ub)\n\n                # Bird Swarm Inspired Local Search and Cauchy Mutation\n                if np.random.rand() < self.p_local_search:\n                    # Neighborhood Search\n                    neighbors_indices = np.random.choice(self.pop_size, size=self.neighborhood_size, replace=False)\n                    neighbors = self.population[neighbors_indices]\n                    neighbors_fitness = self.fitness[neighbors_indices]\n                    best_neighbor_index = neighbors_indices[np.argmin(neighbors_fitness)]\n                    best_neighbor = self.population[best_neighbor_index]\n\n                    # Move towards the best neighbor\n                    trial = trial + self.local_search_intensity * (best_neighbor - trial)\n                    trial = np.clip(trial, self.lb, self.ub)\n\n                    # Cauchy Mutation\n                    if np.random.rand() < self.cauchy_probability:\n                        mutation = trial + (self.ub - self.lb) * np.random.standard_cauchy(size=self.dim) * self.cauchy_scale\n                        mutation = np.clip(mutation, self.lb, self.ub)\n                        trial = mutation\n\n                # Selection\n                f_trial = func(trial)\n                self.eval_count += 1\n\n                if f_trial < self.fitness[i]:\n                    # Update success history\n                    self.success_f.append(adaptive_F)\n                    self.success_cr.append(adaptive_CR)\n                    if len(self.success_f) > self.success_history_size:\n                        self.success_f.pop(0)\n                        self.success_cr.pop(0)\n\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n            # Stagnation Check and Restart/Population Size Adjustment\n            self.best_fitness_history.append(self.f_opt)\n            if len(self.best_fitness_history) > self.stagnation_threshold:\n                if self.best_fitness_history[-1] >= self.best_fitness_history[-self.stagnation_threshold]:\n                    self.stagnation_counter += 1\n                else:\n                    self.stagnation_counter = 0\n\n            if self.stagnation_counter >= self.stagnation_threshold:\n                #Restart the population with possible population size adjustment\n                if self.generation % self.pop_resize_interval == 0:\n                    if np.random.rand() < 0.5: #shrink population\n                        self.pop_size = int(max(self.pop_size * 0.9, self.min_pop_size))\n                    else: #grow population\n                        self.pop_size = int(min(self.pop_size * 1.1, self.max_pop_size))\n                self.population = np.random.uniform(self.lb, self.ub, (self.pop_size, self.dim))\n                self.fitness = np.array([func(x) for x in self.population])\n                self.f_opt = np.min(self.fitness)\n                self.x_opt = self.population[np.argmin(self.fitness)]\n                self.eval_count += self.pop_size - (self.budget-self.eval_count) if self.eval_count + self.pop_size > self.budget else self.pop_size\n                self.stagnation_counter = 0\n                self.best_fitness_history = [self.f_opt]\n\n            if self.eval_count >= self.budget:\n                break\n        return self.f_opt, self.x_opt\n```", "aucs": [0.2678436960126386, 0.5072795182591894, 0.6793884097161278, 0.8661199574472185, 0.7989432758929969, 0.8376662504907789, 0.726097140049462, 0.6577073759280186, 0.8085203798402646, 0.7534797014551724, 0.8446577136407072, 0.9896012831061591, 0.31150383854554675, 0.7246449392470519, 0.9103810515472233, 0.8266985869117326, 0.6155276521842574, 0.8655564407474514, 0.5855931668194995, 0.5320595716853309], "operator": "crossover", "theta_sampled": 0.0023767252865301578, "reward": 0.0, "generation": 12}, "task_prompt": ""}
{"id": "53a7e5d8-b5fe-4ffe-bc23-7b3aec75b252", "fitness": 0.6302797528209554, "name": "AdaptiveHybridDEBirdSwarmPlusPlus", "description": "Combines Adaptive DE, Bird Swarm local search, Cauchy mutation, success history adaptation, and dynamic population size adjustment for enhanced exploration and exploitation.", "code": "import numpy as np\n\nclass AdaptiveHybridDEBirdSwarmPlusPlus:\n    def __init__(self, budget=10000, dim=10, pop_size=40, F=0.5, CR=0.7, neighborhood_size=5, cauchy_probability=0.1, success_history_size=10, p_neighbor=0.5, p_cauchy=0.5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = int(np.clip(pop_size, 10, 100))  # Constrain pop_size\n        self.F = F  # Mutation factor\n        self.CR = CR  # Crossover rate\n        self.lb = -5.0\n        self.ub = 5.0\n        self.stagnation_counter = 0\n        self.stagnation_threshold = budget // 10\n        self.best_fitness_history = []\n        self.neighborhood_size = neighborhood_size\n        self.cauchy_probability = cauchy_probability\n        self.cauchy_scale = 0.1\n        self.success_history_size = success_history_size\n        self.success_f = []\n        self.success_cr = []\n        self.min_pop_size = 10\n        self.max_pop_size = 100\n        self.pop_resize_interval = budget // 20\n        self.p_neighbor = p_neighbor # Probability to apply neighborhood search\n        self.p_cauchy = p_cauchy #Probability to apply cauchy mutation after neighbor search\n\n    def __call__(self, func):\n        self.population = np.random.uniform(self.lb, self.ub, (self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n        self.eval_count = self.pop_size\n        self.generation = 0\n\n        while self.eval_count < self.budget:\n            self.generation += 1\n\n            for i in range(self.pop_size):\n                # Adaptive F and CR using success history\n                if self.success_f and self.success_cr:\n                    adaptive_F = np.random.choice(self.success_f)\n                    adaptive_CR = np.random.choice(self.success_cr)\n                else:\n                    adaptive_F = self.F * (1 + 0.1 * np.random.randn())\n                    adaptive_CR = self.CR * (1 + 0.1 * np.random.randn())\n\n                adaptive_F = np.clip(adaptive_F, 0.1, 1.0)\n                adaptive_CR = np.clip(adaptive_CR, 0.1, 1.0)\n\n                # Mutation (DE part)\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = np.random.choice(idxs, 3, replace=False)\n                mutant = self.population[a] + adaptive_F * (self.population[b] - self.population[c])\n                mutant = np.clip(mutant, self.lb, self.ub)\n\n                # Crossover\n                trial = np.copy(self.population[i])\n                mask = np.random.rand(self.dim) < adaptive_CR\n                trial[mask] = mutant[mask]\n                trial = np.clip(trial, self.lb, self.ub)\n\n                # Bird Swarm Inspired Local Search and Cauchy Mutation\n                if np.random.rand() < self.p_neighbor: # Apply local search with probability p_neighbor\n                    # Neighborhood Search\n                    neighbors_indices = np.random.choice(self.pop_size, size=self.neighborhood_size, replace=False)\n                    neighbors = self.population[neighbors_indices]\n                    neighbors_fitness = self.fitness[neighbors_indices]\n                    best_neighbor_index = neighbors_indices[np.argmin(neighbors_fitness)]\n                    best_neighbor = self.population[best_neighbor_index]\n\n                    # Move towards the best neighbor\n                    trial = trial + 0.1 * (best_neighbor - trial)\n                    trial = np.clip(trial, self.lb, self.ub)\n\n                    # Cauchy Mutation\n                    if np.random.rand() < self.p_cauchy:\n                        mutation = trial + (self.ub - self.lb) * np.random.standard_cauchy(size=self.dim) * self.cauchy_scale\n                        mutation = np.clip(mutation, self.lb, self.ub)\n                        trial = mutation\n\n                # Selection\n                f_trial = func(trial)\n                self.eval_count += 1\n\n                if f_trial < self.fitness[i]:\n                    # Update success history\n                    self.success_f.append(adaptive_F)\n                    self.success_cr.append(adaptive_CR)\n                    if len(self.success_f) > self.success_history_size:\n                        self.success_f.pop(0)\n                        self.success_cr.pop(0)\n\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n            # Stagnation Check and Restart/Population Size Adjustment\n            self.best_fitness_history.append(self.f_opt)\n            if len(self.best_fitness_history) > self.stagnation_threshold:\n                if self.best_fitness_history[-1] >= self.best_fitness_history[-self.stagnation_threshold]:\n                    self.stagnation_counter += 1\n                else:\n                    self.stagnation_counter = 0\n\n            if self.stagnation_counter >= self.stagnation_threshold:\n                #Restart the population with possible population size adjustment\n                if self.generation % self.pop_resize_interval == 0:\n                    if np.random.rand() < 0.5: #shrink population\n                        self.pop_size = int(max(self.pop_size * 0.9, self.min_pop_size))\n                    else: #grow population\n                        self.pop_size = int(min(self.pop_size * 1.1, self.max_pop_size))\n\n                self.population = np.random.uniform(self.lb, self.ub, (self.pop_size, self.dim))\n                self.fitness = np.array([func(x) for x in self.population])\n                self.f_opt = np.min(self.fitness)\n                self.x_opt = self.population[np.argmin(self.fitness)]\n                self.eval_count += self.pop_size - (self.budget-self.eval_count) if self.eval_count + self.pop_size > self.budget else self.pop_size\n                self.stagnation_counter = 0\n                self.best_fitness_history = [self.f_opt]\n\n            if self.eval_count >= self.budget:\n                break\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 12, "feedback": "The algorithm AdaptiveHybridDEBirdSwarmPlusPlus scored 0.630 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["5b77f9fb-407a-4344-aa0a-a95d9f7e8690", "3b387073-58a6-4ee0-9faf-ba6fbc4ee604"], "operator": "crossover", "metadata": {"llm_response": "# Description: Combines Adaptive DE, Bird Swarm local search, Cauchy mutation, success history adaptation, and dynamic population size adjustment for enhanced exploration and exploitation.\n# Code: \n```python\nimport numpy as np\n\nclass AdaptiveHybridDEBirdSwarmPlusPlus:\n    def __init__(self, budget=10000, dim=10, pop_size=40, F=0.5, CR=0.7, neighborhood_size=5, cauchy_probability=0.1, success_history_size=10, p_neighbor=0.5, p_cauchy=0.5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = int(np.clip(pop_size, 10, 100))  # Constrain pop_size\n        self.F = F  # Mutation factor\n        self.CR = CR  # Crossover rate\n        self.lb = -5.0\n        self.ub = 5.0\n        self.stagnation_counter = 0\n        self.stagnation_threshold = budget // 10\n        self.best_fitness_history = []\n        self.neighborhood_size = neighborhood_size\n        self.cauchy_probability = cauchy_probability\n        self.cauchy_scale = 0.1\n        self.success_history_size = success_history_size\n        self.success_f = []\n        self.success_cr = []\n        self.min_pop_size = 10\n        self.max_pop_size = 100\n        self.pop_resize_interval = budget // 20\n        self.p_neighbor = p_neighbor # Probability to apply neighborhood search\n        self.p_cauchy = p_cauchy #Probability to apply cauchy mutation after neighbor search\n\n    def __call__(self, func):\n        self.population = np.random.uniform(self.lb, self.ub, (self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n        self.eval_count = self.pop_size\n        self.generation = 0\n\n        while self.eval_count < self.budget:\n            self.generation += 1\n\n            for i in range(self.pop_size):\n                # Adaptive F and CR using success history\n                if self.success_f and self.success_cr:\n                    adaptive_F = np.random.choice(self.success_f)\n                    adaptive_CR = np.random.choice(self.success_cr)\n                else:\n                    adaptive_F = self.F * (1 + 0.1 * np.random.randn())\n                    adaptive_CR = self.CR * (1 + 0.1 * np.random.randn())\n\n                adaptive_F = np.clip(adaptive_F, 0.1, 1.0)\n                adaptive_CR = np.clip(adaptive_CR, 0.1, 1.0)\n\n                # Mutation (DE part)\n                idxs = [idx for idx in range(self.pop_size) if idx != i]\n                a, b, c = np.random.choice(idxs, 3, replace=False)\n                mutant = self.population[a] + adaptive_F * (self.population[b] - self.population[c])\n                mutant = np.clip(mutant, self.lb, self.ub)\n\n                # Crossover\n                trial = np.copy(self.population[i])\n                mask = np.random.rand(self.dim) < adaptive_CR\n                trial[mask] = mutant[mask]\n                trial = np.clip(trial, self.lb, self.ub)\n\n                # Bird Swarm Inspired Local Search and Cauchy Mutation\n                if np.random.rand() < self.p_neighbor: # Apply local search with probability p_neighbor\n                    # Neighborhood Search\n                    neighbors_indices = np.random.choice(self.pop_size, size=self.neighborhood_size, replace=False)\n                    neighbors = self.population[neighbors_indices]\n                    neighbors_fitness = self.fitness[neighbors_indices]\n                    best_neighbor_index = neighbors_indices[np.argmin(neighbors_fitness)]\n                    best_neighbor = self.population[best_neighbor_index]\n\n                    # Move towards the best neighbor\n                    trial = trial + 0.1 * (best_neighbor - trial)\n                    trial = np.clip(trial, self.lb, self.ub)\n\n                    # Cauchy Mutation\n                    if np.random.rand() < self.p_cauchy:\n                        mutation = trial + (self.ub - self.lb) * np.random.standard_cauchy(size=self.dim) * self.cauchy_scale\n                        mutation = np.clip(mutation, self.lb, self.ub)\n                        trial = mutation\n\n                # Selection\n                f_trial = func(trial)\n                self.eval_count += 1\n\n                if f_trial < self.fitness[i]:\n                    # Update success history\n                    self.success_f.append(adaptive_F)\n                    self.success_cr.append(adaptive_CR)\n                    if len(self.success_f) > self.success_history_size:\n                        self.success_f.pop(0)\n                        self.success_cr.pop(0)\n\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n\n                    if f_trial < self.f_opt:\n                        self.f_opt = f_trial\n                        self.x_opt = trial\n\n            # Stagnation Check and Restart/Population Size Adjustment\n            self.best_fitness_history.append(self.f_opt)\n            if len(self.best_fitness_history) > self.stagnation_threshold:\n                if self.best_fitness_history[-1] >= self.best_fitness_history[-self.stagnation_threshold]:\n                    self.stagnation_counter += 1\n                else:\n                    self.stagnation_counter = 0\n\n            if self.stagnation_counter >= self.stagnation_threshold:\n                #Restart the population with possible population size adjustment\n                if self.generation % self.pop_resize_interval == 0:\n                    if np.random.rand() < 0.5: #shrink population\n                        self.pop_size = int(max(self.pop_size * 0.9, self.min_pop_size))\n                    else: #grow population\n                        self.pop_size = int(min(self.pop_size * 1.1, self.max_pop_size))\n\n                self.population = np.random.uniform(self.lb, self.ub, (self.pop_size, self.dim))\n                self.fitness = np.array([func(x) for x in self.population])\n                self.f_opt = np.min(self.fitness)\n                self.x_opt = self.population[np.argmin(self.fitness)]\n                self.eval_count += self.pop_size - (self.budget-self.eval_count) if self.eval_count + self.pop_size > self.budget else self.pop_size\n                self.stagnation_counter = 0\n                self.best_fitness_history = [self.f_opt]\n\n            if self.eval_count >= self.budget:\n                break\n        return self.f_opt, self.x_opt\n```", "aucs": [0.24150095797021853, 0.3962595322049338, 0.5611764090950098, 0.8484859689684865, 0.6445464794001874, 0.7495772573027808, 0.5971532243275373, 0.6381094373009188, 0.7362929076742748, 0.6182520688739234, 0.7801112645529478, 0.9989970463217337, 0.3437916376641339, 0.6422472352458681, 0.8858733029357011, 0.8030229218707992, 0.5376319703582357, 0.8494641249805113, 0.23127381359115018, 0.5018274957797573], "operator": "crossover", "theta_sampled": 0.005636254950023362, "reward": 0.0, "generation": 12}, "task_prompt": ""}
